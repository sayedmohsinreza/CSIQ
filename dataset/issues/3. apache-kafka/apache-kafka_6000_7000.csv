"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","6346","KAFKA-8002: Replica reassignment to new log dir may not complete if f…","…uture and current replicas segment files have different base offsets  This patch fixes a bug in log dir reassignment where Partition.maybeReplaceCurrentWithFutureReplica would compare the entire LogEndOffsetMetadata of each replica to determine whether the reassignment has completed. If the active segments of both replicas have different base segments (for example, if the current replica had previously been cleaned and the future replica rolled segments at different points), the reassignment will never complete. The fix is to compare only the LogEndOffsetMetadata.messageOffset for each replica. Tested with a unit test that simulates the compacted current replica case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-02-28T18:01:59Z","2019-03-04T07:57:37Z"
"","5646","KAFKA-7400: Compacted topic segments that precede the log start offse…","…t are not cleaned up  Currently we don't delete any log segments if the cleanup policy doesn't include delete. This patch changes the behavior to delete log segments that fully precede the log start offset even when deletion is not enabled. Regardless of whether delete is enabled, LogCleaner checks compact topics and LogManager checks non-compact topics. Tested with unit tests to verify that LogCleaner.cleanupOrSleep now cleans logs with cleanup.policy=compact and LogManager.cleanupLogs doesn't, and that Log.deleteOldSegments deletes segments that precede the start offset regardless of the cleanup policy.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2018-09-13T17:25:47Z","2018-09-21T20:34:00Z"
"","6030","KAFKA-7733: Reset the MockConsumer position according to its reset st…","…rategy if the position is behind the beginning offset  The poll method in MockConsumer will now use the configured reset strategy when it encounters a record with an offset below the beginning offset.   The poll method will also not clear the records list automatically. Letting the consumer position control which records are returned from poll is more similar to how the real consumer works, so e.g. a test can put a message in the consumer, poll for it, seek to the beginning and get the message out again. If tests need to delete the records entirely, they can still do so through a new method.  A few of the methods in MockConsumer were also returning mutable internal maps/lists. The regular consumer returns immutable copies, so I updated the MockConsumer to do the same.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","srdo","2018-12-13T17:24:50Z","2022-02-14T16:41:01Z"
"","6364","KAFKA-8002: Log dir reassignment stalls if future replica has differe…","…nt segment base offset  This patch fixes a bug in log dir reassignment where Partition.maybeReplaceCurrentWithFutureReplica would compare the entire LogEndOffsetMetadata of each replica to determine whether the reassignment has completed. If the active segments of both replicas have different base segments (for example, if the current replica had previously been cleaned and the future replica rolled segments at different points), the reassignment will never complete. The fix is to compare only the LogEndOffsetMetadata.messageOffset for each replica. Tested with a unit test that simulates the compacted current replica case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-03-04T08:02:23Z","2019-03-04T21:28:31Z"
"","5956","fix ProducerPerformance bug when numRecords > Integer.MAX, it will fa…","…ll into non-stop loop and send more message to broker, and Stat in PerfCallback method record will throw ArrayIndexOutOfBoundsException  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","seayoun","2018-11-28T11:00:03Z","2018-11-29T06:48:57Z"
"","5901","KAFKA-7616: Make MockConsumer only add entries to the partition map r…","…eturned by poll() if there are any records to return  The MockConsumer behaves unlike the real consumer in that it can return a non-empty ConsumerRecords from poll, that also has a count of 0. This change makes the MockConsumer only add partitions to the ConsumerRecords if there are records to return for those partitions.  A unit test in MockConsumerTest demonstrates the issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","srdo","2018-11-11T23:41:55Z","2018-11-20T17:48:53Z"
"","5881","KAFKA-5503 Idempotent producer ignores shutdown while fetching Produc…","…erId  Check running in `Sender.maybeWaitForProducerId`","closed","","layfe","2018-11-05T14:02:12Z","2019-01-02T23:00:41Z"
"","5727","KAFKA-7467: NoSuchElementException is raised because controlBatch is …","…empty  This patch adds checks before reading the first record of a control batch. If the batch is empty, it is treated as having already been cleaned. In the case of LogCleaner this means it is safe to discard. In the case of ProducerStateManager it means it shouldn't cause state to be stored because the relevant transaction has already been cleaned. In the case of Fetcher, it just preempts the check for an abort. In the case of GroupMetadataManager, it doesn't process the offset as a commit. The patch also adds isControl to the output of DumpLogSegments. Changes were tested with unit tests, except the DumpLogSegments change which was tested manually.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2018-10-02T15:29:50Z","2019-03-04T07:46:42Z"
"","6189","KAFKA-7740   Kafka Admin Client should be able to manage user/client …","…configurations for users and clients  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ghost","2019-01-24T00:22:23Z","2020-04-29T23:05:53Z"
"","6381","KAFKA-8053: Provide better error in kafka-topics.sh message when topi…","…c doesn't exist  This PR tries to address [KAFKA-8053](https://issues.apache.org/jira/browse/KAFKA-8053). It passes the option with which the user originally called the `kafka-topics.sh` tool to the `ensureTopicExists` method to try to produce better error message. It also handles the case when the user doesn't specify any topics at all (which is for example allowed for `--describe`).  If you prefer to address this differently, let me know and I can update the PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","scholzj","2019-03-06T16:18:53Z","2019-07-18T20:54:12Z"
"","5628","KAFKA-7392: Allow to specify subnet for Docker containers using stand…","…ard CIDR notation  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* I have manually verified the changes:  - I tested new help message is displayed ``` /ducker-ak up --help                             ducker-ak: a tool for running Apache Kafka tests inside Docker images.  Usage: ./ducker-ak [command] [options]  help|-h|--help     Display this help message  up [-n|--num-nodes NUM_NODES] [-f|--force] [docker-image]         [-C|--custom-ducktape DIR] [-s|--subnet CIDR]     Bring up a cluster with the specified amount of nodes (defaults to 14).     The docker image name defaults to ducker-ak.  If --force is specified, we will     attempt to bring up an image even some parameters are not valid.      If --custom-ducktape is specified, we will install the provided custom     ducktape source code directory before bringing up the nodes.  The provided     directory should be the ducktape git repo, not the ducktape installed module directory.      If --subnet is specified, default Docker subnet is overriden by given IP address and netmask,     using standard CIDR notation. For example: 192.168.1.5/24. ``` - Spinned up Docker containers with the new subnet argument and verified it is used by ducknet: ``` $ ./ducker-ak up -n 5 --subnet 192.168.1.5/24 ... $ docker network inspect ducknet                                             [     {         ""Name"": ""ducknet"",         ""Id"": ""75cadf2d29077b4bebb4bd5c1c03e0a542716992b656027e8728634a2d0bd3a5"",         ""Created"": ""2018-09-11T20:11:56.6093651Z"",         ""Scope"": ""local"",         ""Driver"": ""bridge"",         ""EnableIPv6"": false,         ""IPAM"": {             ""Driver"": ""default"",             ""Options"": {},             ""Config"": [                 {                     ""Subnet"": ""192.168.1.5/24""                 }             ]         }, ... ``` - Spinned up Docker containers without the new subnet argument and verified it is used default IP range allocated by docker is used: ``` $ ./ducker-ak up -n 2  ... $ docker network inspect ducknet [     {         ""Name"": ""ducknet"",         ""Id"": ""7ebb38cd99e8a3fb84a65fd4eaad52dbe37e28dd83b6f0b57d512f66222397bd"",         ""Created"": ""2018-09-09T12:22:25.6561649Z"",         ""Scope"": ""local"",         ""Driver"": ""bridge"",         ""EnableIPv6"": false,         ""IPAM"": {             ""Driver"": ""default"",             ""Options"": {},             ""Config"": [                 {                     ""Subnet"": ""172.24.0.0/16"",                     ""Gateway"": ""172.24.0.1""                 }             ]         }, ... ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asasvari","2018-09-09T12:23:50Z","2018-11-30T19:52:34Z"
"","6154","KAFKA-6833; Producer should await metadata for unknown partitions (#6…","…073)  This patch changes the behavior of KafkaProducer.waitOnMetadata to wait up to max.block.ms when the partition specified in the produce request is out of the range of partitions present in the metadata. This improves the user experience in the case when partitions are added to a topic and a client attempts to produce to one of the new partitions before the metadata has propagated to the brokers. Tested with unit tests.  Reviewers: Arjun Satish , Jason Gustafson   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2019-01-16T02:34:17Z","2019-01-24T02:27:10Z"
"","5507","KAFKA-6833: KafkaProducer throws 'Invalid partition given with record…","…' exception  Throw an UnknownTopicAndPartitionException, rather than a generic KafkaException, when trying to produce to a partition that isn't in the metadata. This could be caused be a delay in propagating metadata, so it should return a retriable error. Tested by expecting the specific exception class when fetching metadata for an out of range partition.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2018-08-14T20:19:03Z","2018-12-28T18:31:05Z"
"","6073","KAFKA-6833: KafkaProducer throws ""Invalid partition given with record…","…"" exception  This patch changes the behavior of KafkaProducer.waitOnMetadata to wait up to max.block.ms when the partition specified in the produce request is out of the range of partitions present in the metadata. This improves the user experience in the case when partitions are added to a topic and a client attempts to produce to one of the new partitions before the metadata has propagated to the brokers. Tested with unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2018-12-28T18:30:07Z","2019-01-09T02:05:14Z"
"","5706","KAFKA-6605 fix NPE in Flatten when optional Struct is null - backport…","… to 2.0  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mihbor","2018-09-27T20:38:55Z","2019-07-15T07:45:37Z"
"","5884","MINOR: updated documentation where RocksDBStore was being used as the…","… sample class for byte[] versus Bytes examples","closed","streams,","cfeduke","2018-11-06T06:38:11Z","2020-02-02T22:50:04Z"
"","5708","Minor: remove WorkerCoordinatorMetrics and instantiate the metrics in the constructor of WorkerCoordinator","WorkerCoordinatorMetrics is a private class but we don't ever use it after instantiating it. This PR change it to a private method with lambda expression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2018-09-28T03:55:47Z","2020-07-25T16:18:23Z"
"","6440","MINOR: resolve some of the core project compilier warnings","With this change this is the output of trying to generate the jar for the core project. ``` $ ./gradlew core:jar core/src/main/scala/kafka/admin/AdminUtils.scala:64: trait AdminUtilities in package admin is deprecated (since 1.1.0): This class is deprecated and will be replaced by kafka.zk.AdminZkClient. object AdminUtils extends Logging with AdminUtilities {                                        ^ core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala:52: class AdminClient in package admin is deprecated (since 0.11.0): This class is deprecated in favour of org.apache.kafka.clients.admin.AdminClient and it will be removed in a future release.   private def createAdminClient(opts: BrokerVersionCommandOptions): AdminClient = {                                                                     ^ core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala:58: object AdminClient in package admin is deprecated (since 0.11.0): This class is deprecated in favour of org.apache.kafka.clients.admin.AdminClient and it will be removed in a future release.     AdminClient.create(props)     ^ core/src/main/scala/kafka/tools/MirrorMaker.scala:197: class BaseConsumerRecord in package consumer is deprecated (since 0.11.0.0): This class has been deprecated and will be removed in a future release. Please use org.apache.kafka.clients.consumer.ConsumerRecord instead.     private def toBaseConsumerRecord(record: ConsumerRecord[Array[Byte], Array[Byte]]): BaseConsumerRecord =                                                                                         ^ core/src/main/scala/kafka/tools/MirrorMaker.scala:198: class BaseConsumerRecord in package consumer is deprecated (since 0.11.0.0): This class has been deprecated and will be removed in a future release. Please use org.apache.kafka.clients.consumer.ConsumerRecord instead.       BaseConsumerRecord(record.topic,       ^ core/src/main/scala/kafka/tools/MirrorMaker.scala:417: class BaseConsumerRecord in package consumer is deprecated (since 0.11.0.0): This class has been deprecated and will be removed in a future release. Please use org.apache.kafka.clients.consumer.ConsumerRecord instead.     def handle(record: BaseConsumerRecord): util.List[ProducerRecord[Array[Byte], Array[Byte]]]                        ^ core/src/main/scala/kafka/tools/MirrorMaker.scala:421: class BaseConsumerRecord in package consumer is deprecated (since 0.11.0.0): This class has been deprecated and will be removed in a future release. Please use org.apache.kafka.clients.consumer.ConsumerRecord instead.     override def handle(record: BaseConsumerRecord): util.List[ProducerRecord[Array[Byte], Array[Byte]]] = {                                 ^ 7 warnings found ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-03-14T00:15:37Z","2019-04-15T23:28:13Z"
"","5635","MINOR: Missing throttle time in OffsetsForLeaderEpoch response","With KIP-320, the OffsetsForLeaderEpoch API is intended to be used by consumers to detect log truncation. Therefore the new response schema should expose a field for the throttle time like all the other APIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-10T21:32:54Z","2018-09-11T16:08:23Z"
"","5492","KAFKA-7211: MM should handle TimeoutException in commitSync","With KIP-266 introduced, MirrorMaker should handle TimeoutException thrown in commitSync(). Besides, MM should only commit offsets for existsing topics.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-08-13T05:20:12Z","2018-09-04T20:58:10Z"
"","5623","KAFKA-7385: Fix log cleaner behavior when empty batches are retained","With idempotent producers, we may leave empty batches in the log during log compaction. When filtering the data, we keep track of state like `maxOffset` and `maxTimestamp` of filtered data. This patch ensures we maintain this state correctly for the case when only empty batches are left in `MemoryRecords#filterTo`. Without this patch, we did not initialize `maxOffset` in this edge case which led us to append data to the log with `maxOffset` = -1L, causing the append to fail and log cleaner to crash.","closed","","dhruvilshah3","2018-09-07T23:14:12Z","2018-09-09T01:04:08Z"
"","6428","KAFKA-7224: [WIP] Persistent Suppress [WIP]","WIP - no need to review. I'm just getting a copy of this onto github.  I'll call for reviews once I think it's ready.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-03-11T19:50:00Z","2020-04-30T05:20:13Z"
"","5793","MINOR: Update Streams Scala API for addition of Grouped","While working on the documentation updates I realized the Streams Scala API needs to get updated for the addition of `Grouped`  Added a test for `Grouped.scala` ran all `streams-scala` tests and `streams` tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","bbejeck","2018-10-12T15:17:34Z","2020-06-12T23:57:35Z"
"","6053","MINOR: code cleanup","While working on #6044, I did code cleanup on the side. Extracted parts of those cleanups into this PR for easier review. This PR should not change any behavior, but do:   - Java8 rewrites  - removed unused classed  - remove warnings  - break long lines that are hard to read into short lines  - some code re-formatting  - JavaDoc improvements  Only change is a dependency change, that I need for #6044, and will make reviewing PRs for KIP-258 simpler.","closed","streams,","mjsax","2018-12-20T14:18:36Z","2019-01-09T17:03:22Z"
"","5656","MINOR: cleanup some state store code","While prototyping KIP-258, I did some cleanup. Wanted to extract it into a separate PR.","closed","streams,","mjsax","2018-09-14T22:12:31Z","2018-09-18T21:19:45Z"
"","5908","KAFKA-7528: Standardize on Min/Avg/Max Kafka metrics' default value - NaN","While metrics like `Min`, `Avg` and `Max` make sense to respective use `Double.MAX_VALUE`, `0.0` and `Double.MIN_VALUE` as default values to ease computation logic, exposing those values makes reading them a bit misleading. For instance, how would you differentiate whether your `-avg` metric has a value of 0 because it was given samples of 0 or no samples were fed to it?  It makes sense to standardize on the output of these metrics with something that clearly denotes that no values have been recorded.","closed","","stanislavkozlovski","2018-11-12T18:51:09Z","2018-11-20T23:54:35Z"
"","6091","K7657 handling thread dead state change","While looking into KAFKA-7657, I found there are a few loopholes in this logic:  We kept a map of thread-name to thread-state and a global-thread state at the KafkaStreams instance-level, in addition to the instance state itself. stateLock is used when accessing the instance state, however when we are in the thread state change callback, we are accessing both the thread-states as well as the instance state at the same time in the callers of setState without a lock, which is vulnerable to concurrent multi-stream threads. The fix is a) introduce a threadStatesLock in addition to the stateLock, which should always be grabbed to modify the thread-states map before the stateLock for modifying the instance level; and we also defer the checking of the instance-level state inside the setState call.  When transiting to state.RUNNING, we check if all threads are either in RUNNING or DEAD state, this is because some threads maybe dead at the rebalance period but we should still proceed to RUNNING if the rest of threads are still transiting to RUNNING.  Added unit test for 2) above. Also simplified another test as a nit change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-01-04T22:44:13Z","2020-04-25T00:06:37Z"
"","6018","KAFKA-7657: Fixing thread state change to instance state change","While looking into KAFKA-7657, I found there are a few loopholes in this logic:  1. We kept a map of thread-name to thread-state and a global-thread state at the KafkaStreams instance-level, in addition to the instance state itself. `stateLock` is used when accessing the instance state, however when we are in the thread state change callback, we are accessing both the thread-states as well as the instance state at the same time in the callers of `setState` without a lock, which is vulnerable to concurrent multi-stream threads. The fix is a) introduce a `threadStatesLock` in addition to the `stateLock`, which should always be grabbed to modify the thread-states map before the `stateLock` for modifying the instance level; and we also defer the checking of the instance-level state inside the `setState` call.  2. When transiting to state.RUNNING, we check if all threads are either in RUNNING or DEAD state, this is because some threads maybe dead at the rebalance period but we should still proceed to RUNNING if the rest of threads are still transiting to RUNNING.  3. Added unit test for 2) above. Also simplified another test as a nit change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-12-09T23:21:36Z","2019-01-04T22:44:59Z"
"","5664","MINOR: Improve IntegrationTestUtils documentation","While I was writing [an example project on testing Kafka Streams](https://github.com/dongjinleekr/kafka-streams-example), I found that there are several problems with the documentation of `IntegrationTestUtils.`  - The documentation of `Time` parameter in `produceKeyValuesSynchronously` methods is missing. This parameter was added in commit de1b853, but documentation was omitted then. - Change parameter `enableTransactions` in `produceKeyValuesSynchronouslyWithTimestamp(String, Collection, Properties, Headers headers, Long, boolean)` into `enableTransactions`: for consistency with the other overloads. - Add Javadoc to undocumented methods, like `produceKeyValuesSynchronouslyWithTimestamp`, `waitUntilMinRecordsReceived`, etc. - Fix ordering: `waitUntilMinRecordsReceived` - Upper bound in ... -> Upper bound of ...  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2018-09-19T14:16:14Z","2019-01-25T07:17:50Z"
"","5954","MINOR: improve Streams checkstyle and code cleanup","While fixing the code to meet the new checkstyle rule, I updated all touched classed for Java8 and removed other warnings.","closed","streams,","mjsax","2018-11-27T23:12:45Z","2018-12-11T09:55:30Z"
"","6180","MINOR: Make use of metric name variables in SocketServer","While digging around the code, I saw that the `NetworkProcessorMetricTag` and `IdlePercentMetricName` values were duplicated in one spot. I decided it would be good to follow the variable-defining convention and create `SocketServerMetricsGroup` for the `""socket-server-metrics""` group","closed","","stanislavkozlovski","2019-01-20T21:40:23Z","2019-01-22T10:23:10Z"
"","6304","KAFKA-7961: Ignore assignment for un-subscribed partitions","Whenever the consumer coordinator sends a response that doesn't match the client consumer subscription, ignore the assignment and rejoin the group.  Testing strategy: create a mocked client that first sends an assignment response that doesn't match the client subscription followed by an assignment response that does match the client subscription.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-02-22T00:46:10Z","2019-02-25T17:59:35Z"
"","6110","KAFKA-7771:Group/Transaction coordinators should update assignment based on current partition count","Whenever a LeaderAndIsr request containing internal topics is received, the broker should need to try to update their partition count by recalculating it from Zookeeper.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2019-01-09T09:09:05Z","2019-07-23T03:55:04Z"
"","6473","KAFKA-8130:The consumer is not closed in GetOffsetShell, will exhausted socket channel when frequent calls","When use command ""bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic test --time -2 --broker-list 127.0.0.1:9092 --partitions 1"" frequently, or use code call kafka.tools.GetOffsetShell method more then socket limit. It will show us  ""Too many open files"" error.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","Mrart","2019-03-20T07:29:50Z","2019-07-19T07:51:47Z"
"","6426","KAFKA-7976; Update config before notifying controller of unclean leader update","When unclean leader election is enabled dynamically on brokers, we notify controller of the update before updating KafkaConfig. When processing this event, controller's decision to elect unclean leaders is based on the current KafkaConfig, so there is a small timing window when the controller may not elect unclean leader because `KafkaConfig` of the server was not yet updated. The PR fixes this timing window by using the existing `BrokerReconfigurable` interface used by other classes which  rely on the current value of `KafkaConfig`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-11T13:29:22Z","2019-03-12T09:47:06Z"
"","6217","MINOR: correctly set dev version","When the version was bumped to 2.1.1-SNAPSHOT, this one was missed, causing system tests to fail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-02-01T00:42:18Z","2019-02-04T17:58:18Z"
"","5819","MINOR: buffer should ignore caching","When the buffer size config is set to 0, Streams invokes `withCachingDisabled` in all registered stores.  Previously, we didn't expect this method to be called on the suppression buffer, but since it can be under valid circumstances, we should just ignore it rather than throwing an exception.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-19T20:28:49Z","2018-10-24T19:45:07Z"
"","6402","KAFKA-8066: Always close the sensors in Selector.close()","When shutting down the ReplicaFetcher thread, we may miss to unregister the sensor in selector.close(). When that happened, we will fail to start up the ReplicaFetcherThread with the same fetch id again because of the IllegalArgumentException in sensor registration. This issue will cause constant under replicated partitions in the cluster because the ReplicaFetchterThread is gone.  This patch addresses this issue by introducing a try-finally block in selector.close() so that we will always unregister the sensors in shutting down ReplicaFetcherThreads.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2019-03-08T06:43:57Z","2019-05-01T19:40:49Z"
"","5802","KAFKA-7509: Reduce unnecessary and misleading “configuration supplied but not known” warning messages in Connect","When running Connect, the logs contain quite a few warnings about:      The configuration '{}' was supplied but isn't a known config.  This occurs when Connect creates producers, consumers, and admin clients, because the AbstractConfig is logging unused configuration properties upon construction. It's complicated by the fact that the `Producer`, `Consumer`, and `AdminClient` all create in their constructors private instances of `ProducerConfig`, `ConsumerConfig`, and `AdminClientConfig`, respectively, and the unused properties are logged as warnings there. Because the `AbstractConfig` instances are created by the client components, Connect is not able to call the `ignore(String key)` method to suppress those log warnings.  Unfortunately, there are no arguments in the Producer, Consumer, or AdminClient public constructors to control whether the configs log these warning in their constructors. While that might be a possible change we want to make in the future, a simpler workaround is for Connect to be more hygienic and pass to the `Producer`, `Consumer`, and `AdminClient` constructors only  those configuration properties that the `ProducerConfig`, `ConsumerConfig`, and `AdminClientConfig` configdefs know about.  This PR makes this change for Connect.   If this approach is approved, we should consider how far back we want to port this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-10-15T22:20:57Z","2018-11-01T19:52:33Z"
"","6333","KAFKA-8012: Ensure partitionStates have not been removed before truncating.","When reproducing the flakiness of `DynamicBrokerReconfigurationTest#testThreadPoolResize` (KAFKA-7988) on my machine, I saw that failures correspond to an NPE in one or more replica fetcher threads. This happens when the replica fetcher manager simultaneously calls `removeFetcherForPartitions`, removing the corresponding partitionStates, while a replica fetcher thread attempts to truncate the same partition(s) in `truncateToHighWatermark`.  This change simply checks that the `partitionState` is not null first. Note that a similar guard exists in `truncateToEpochEndOffsets`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","colinhicks","2019-02-27T01:16:33Z","2019-03-01T10:11:26Z"
"","6407","KAFKA-7703: position() may return a wrong offset after seekToEnd","When poll is called which resets the offsets to the beginning, followed by a seekToEnd and a position, it could happen that the ""reset to earliest"" call in poll overrides the ""reset to latest"" initiated by seekToEnd in a very delicate way:  1. both request has been issued and returned to the client side (listOffsetResponse has happened) 2. in Fetcher.resetOffsetIfNeeded(TopicPartition, Long, OffsetData) the thread scheduler could prefer the heartbeat thread with the ""reset to earliest"" call, overriding the offset to the earliest and setting the SubscriptionState with that position. 3. The thread scheduler continues execution of the thread (application thread) with the ""reset to latest"" call and discards it as the ""reset to earliest"" already set the position - the wrong one. 4. The blocking position call returns with the earliest offset instead of the latest, despite it wasn't expected.  The fix makes the TopicPartitionState in SubscriptionState synchronized and starts to track the requested reset timestamp. With this we can precisely decide if the incoming offset reset is really what we want (by comparing the timestamp set when assigning for reset and the one that is actually used on seek). Therefore the latest initiated offset reset will happen only. Synchronization furthermore ensures that this is done in an atomic manner to avoid further similar bugs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-03-08T17:27:30Z","2019-05-30T11:32:07Z"
"","5918","KAFKA-7633: Allow Kafka Connect to access internal topics without cluster ACLs","When Kafka Connect does not have cluster ACLs to create topics, it fails to even access its internal topics which already exist. This was originally fixed in https://issues.apache.org/jira/browse/KAFKA-6250 #4247 by ignoring the cluster authorization error, but now Kafka 2.0 returns a different response code that corresponds to a different error. Add a patch to ignore this new error as well.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","arabelle","2018-11-15T22:32:39Z","2019-05-20T17:59:06Z"
"","6404","MINOR: Add log when the consumer does not send an offset commit due to not being part of an active group","When inspecting logs, it's hard to tell whether an `OffsetCommit` request was sent and [received a response that made it raise](https://github.com/apache/kafka/blob/027cbbaec521542f53274183daccc2073e91cfe9/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L843) a `CommitFailedException()` or whether the consumer did not send the request at all.  Since this is a rare case, we could add a log to say why this has happened. Another option is to allow the `CommitFailedException` class to accept a message in its constructor and explain the situation there. I'm not sure on the boundary of whether that counts as a public-facing change or not, so opted in for the log variant","closed","","stanislavkozlovski","2019-03-08T12:40:28Z","2019-05-21T10:30:53Z"
"","6127","MINOR: Need to have same wait as verify timeout broker upgrade downgrade","When I originally refactored the `streams_upgrade_test#upgrade_downgrade_brokers` test I removed the `wait` call which would wait for up 24 minutes for the `SmokeTestDriver` class to publish and verify all of its records.   Since most of the tests run in two minutes or less I set the `monitor_log` timeout to three minutes.  However, the `SmokeTestDriver#verify` method [allows up to six minutes](https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestDriver.java#L287) to consume all records before verifying the `monitor_log` timeout needs to be greater than 6 minutes.  I've set the timeout to 8 minutes.  Additionally, the steps needed to update the `streams_upgrade_test` should be documented as there are several components that need to get updated.  So I've documented those steps here on the test as a giant comment.  I ran a branch builder (just for the broker upgrade tests) http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-01-11--001.1547174767--bbejeck--MINOR_need_to_have_same_wait_as_verify_timeout_broker_upgrade_downgrade--4fec6e4/report.html and all tests passed  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-01-11T18:27:33Z","2019-01-11T22:25:40Z"
"","6279","MINOR: Make info logs for KafkaConsumer a bit more verbose","When debugging KafkaConsumer production issues, it's pretty useful to have log entries related to seeking and committed offset retrieval enabled by default. These are currently present, but only when debug logging is enabled. Change them to `info`.  Also included a minor code simplication and a slight improvement to an exception message.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-16T23:29:30Z","2019-02-17T19:39:53Z"
"","6305","Fix for KAFKA-7974: Avoid zombie AdminClient when node host isn't resolvable","When attempting to get topic list via KafkaAdminClient against a server that isn't resolvable, the worker thread can get killed as follows, leading to a zombie KafkaAdminClient:  ``` ERROR [kafka-admin-client-thread | adminclient-1] 2019-02-18 01:00:45,597 KafkaThread.java:51 - Uncaught exception in thread 'kafka-admin-client-thread | adminclient-1': java.lang.IllegalStateException: No entry found for connection 0     at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:330)     at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:134)     at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:921)     at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:287)     at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.sendEligibleCalls(KafkaAdminClient.java:898)     at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1113)     at java.lang.Thread.run(Thread.java:748) ```  It looks like cause is a bug in state handling between `NetworkClient` and `ClusterConnectionStates`: - `NetworkClient.ready()` invokes `this.initiateConnect()` as seen in the above stacktrace - `NetworkClient.initiateConnect()` invokes `ClusterConnectionStates.connecting()`, which internally invokes `ClientUtils.resolve()` to resolve the host when creating an entry for the connection. - If this host lookup fails, a `UnknownHostException` can be thrown back to `NetworkClient.initiateConnect()` and the connection entry is not created in `ClusterConnectionStates`. This exception doesn't currently get logged so this is a guess on my part. - `NetworkClient.initiateConnect()` catches the exception and attempts to call `ClusterConnectionStates.disconnected()`, which throws an `IllegalStateException` because no entry had yet been created due to the lookup failure. - This `IllegalStateException` ends up killing the worker thread and `KafkaAdminClient` gets stuck, never returning from `listTopics()`.  This PR includes a unit test which reproduces the original issue (matching stacktrace) and verifies the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nickbp","2019-02-22T01:35:15Z","2020-09-14T08:36:49Z"
"","6253","KAFKA-7897; Do not write epoch start offset for older message format versions","When an older message format is in use, we should disable the leader epoch cache so that we resort to truncation by high watermark. Previously we updated the cache for all versions when a broker became leader for a partition. This can cause large and unnecessary truncations after leader changes because we relied on the presence of _any_ cached epoch in order to tell whether to use the improved truncation logic possible with the OffsetsForLeaderEpoch API.  Note this is a simplified fix than what was merged to trunk in #6232 since the branches have diverged significantly. Rather than removing the epoch cache file, we guard usage of the cache with the record version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-11T17:57:22Z","2019-02-12T16:32:33Z"
"","6232","KAFKA-7897; Disable leader epoch cache when older message formats are used","When an older message format is in use, we should disable the leader epoch cache so that we resort to truncation by high watermark. Previously we updated the cache for all versions when a broker became leader for a partition. This can cause large and unnecessary truncations after leader changes because we relied on the presence of _any_ cached epoch in order to tell whether to use the improved truncation logic possible with the OffsetsForLeaderEpoch API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-05T01:47:52Z","2019-02-11T22:58:43Z"
"","6202","KAFKA-7837: Ensure we do not shrink ISR for offline partitions","When a partition is marked offline due to a failed disk, the leader is supposed to not shrink its ISR any more. In `ReplicaManager#maybeShrinkIsr`, we iterate through all non-offline partitions to shrink the ISR. If an ISR needs to shrink, we need to write the new ISR to ZK, which can take a bit of time. In this window, some partitions could now be marked as offline, but may not be picked up by the iterator. This is because the offline partition iterator used in `ReplicaManager#maybeShrinkIsr` is weakly consistent and does not necessarily reflect the updated offline state.  This patch fixes the above issue by iterating through all topic partitions and checking whether the partition has been made offline within the iteration. This helps ensure we detect a partition being made offline during the iteration and prevent ISRs from shrinking thereof.","closed","","dhruvilshah3","2019-01-25T23:18:33Z","2019-01-26T06:30:27Z"
"","5962","KAFKA-7610; Proactively timeout new group members if rebalance is delayed","When a consumer first joins a group, it doesn't have an assigned memberId. If the rebalance is delayed for some reason, the client may disconnect after a request timeout and retry. Since the client had not received its memberId, then we do not have a way to detect the retry and expire the previously generated member id. This can lead to unbounded growth in the size of the group until the rebalance has completed.  This patch fixes the problem by proactively completing all JoinGroup requests for new members after a timeout of 5 minutes. If the client is still around, we expect it to retry.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-11-28T18:56:25Z","2019-01-04T20:10:13Z"
"","6337","KAFKA-7925 - Create credentials only once for sun.security.jgss.native","When `sun.security.jgss.native=true`, we currently create a new server credential for every new client connection and add to the private credential set of the server's Subject. This is expensive and can result in an unbounded number of private credentials in the Subject used by the broker. The PR creates a credential immediately after login so that a single credential can be reused.  This was tested by Abhi who raised the issue. See https://issues.apache.org/jira/browse/KAFKA-7925 for details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-02-27T13:20:05Z","2019-12-12T09:56:31Z"
"","5887","Increase acceptable duration time for ReassignPartitionsClusterTest#shouldExecuteThrottledReassignment","We've seen this test fail in Jenkins (https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/) with 10400ms. Running locally 50 times, I had two instances where it took 8.2s and 9.3s. Since Jenkins is typically running on a slower machine, I think that it is reasonable to increase the acceptable duration here in order to reduce failed builds due to test flakiness.","closed","","stanislavkozlovski","2018-11-06T20:51:40Z","2019-02-10T19:39:34Z"
"","5859","MINOR: Fix a few blocking calls in PlaintextConsumerTest","We've been seeing some hanging builds recently (see KAFKA-7553). Consistently the culprit seems to be a test case in PlaintextConsumerTest. This patch doesn't fix the underlying issue, but it eliminates a few places where these test cases could block:  1. It replaces several calls to the deprecated `poll(long)` which can block indefinitely in the worst case with `poll(Duration)` which respects the timeout. 2. It also fixes a consume utility in `TestUtils` which can block for a long time depending on the number of records that are expected to be consumed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-30T20:39:54Z","2018-10-31T22:51:03Z"
"","5785","Resize buffer on OVERFLOW","We're seeing some BUFFER_OVERFLOW errors when using this library.  I might be mistaken, but as far as I can tell, the buffer isn't actually resized in the current implementation.  The Java documentation suggests that the buffer size should be added to the buffer's current position when resizing the buffer:  https://docs.oracle.com/javase/8/docs/api/javax/net/ssl/SSLEngine.html","closed","","gordonmessmer","2018-10-11T20:44:14Z","2022-05-31T21:49:16Z"
"","5551","KAFKA-7396 Materialized, Serialized, Joined, Consumed and Produced with implicit Serdes","We want to make sure that we always have a serde for all Materialized, Serialized, Joined, Consumed and Produced. For that we can make use of the implicit parameters in Scala.  KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-365%3A+Materialized%2C+Serialized%2C+Joined%2C+Consumed+and+Produced+with+implicit+Serde","closed","kip,","joan38","2018-08-22T01:00:41Z","2020-06-12T23:59:06Z"
"","6456","KAFKA-8118; Ensure ZooKeeper clients are closed in tests, fix verification","We verify that ZK clients are closed in tests since these can affect subsequent tests and that makes it hard to debug test failures. But because of changes to ZooKeeper client, we are now checking the wrong thread name. The thread name used now is `-EventThread` where `creatorThreadName` varies depending on the test. Fixed verification `ZooKeeperTestHarness` to check this format and fixed tests which were leaving ZK clients behind. Also added a test to make sure we can detect changes to the thread name when we update ZK clients in future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-17T16:08:36Z","2019-03-18T08:47:29Z"
"","6476","KAFKA-8106:Reducing the allocation and copying of ByteBuffer  when logValidator  do validation.","We suggest that reducing the allocation and copying of ByteBuffer when logValidator do validation when magic value to use is above 1 and no format conversion or value overwriting is required for compressed messages.And improved code is as follows. 1. Adding a class **SimplifiedDefaultRecord** implement class Record which define  various attributes of a message.  2. Adding Function **simplifiedreadFrom**() at class **DefaultRecord** .This function will not read data from DataInput to  ByteBuffer which need newly creating .**This will reduce the allocation and copying of ByteBuffer** when logValidator do validation .This will reduces GC frequency. We offer a simple read function to read data from **DataInput** whithout create ByteBuffer.Of course this opertion can not avoid deconmpression to data. 3. Adding Function **simplifiedIterator**() and **simplifiedCompressedIterator**() at class **DefaultRecordBatch**.This two functions will return iterator of instance belongs to class **SimplifiedDefaultRecord**. 4. Modify code of function **validateMessagesAndAssignOffsetsCompressed**() at class  LogValidator.      **After modifing code wich  reducing the allocation and copying of ByteBuffer**, the test performance is greatly improved, and the CPU's stable usage is below 60%. The following is a comparison of different code test performance under the same conditions. **Result of performance testing** Main config of Kafka: Single Message:1024B;TopicPartitions:200;linger.ms:1000ms, **1.Before modified code(Source code):** Network inflow rate:600M/s;CPU(%)(97%);production:25,000,000 messages/s **2.After modified code(remove allocation of ByteBuffer):** Network inflow rate:1G/s;CPU(%)(<60%);production:41,000,000 messages/s  **1.Before modified code(Source code) GC:** ![](https://i.loli.net/2019/05/07/5cd16df163ad3.png) **2.After modified code(remove allocation of ByteBuffer) GC:** ![](https://i.loli.net/2019/05/07/5cd16dae1dbc2.png)","closed","","Flowermin","2019-03-20T10:14:32Z","2019-05-08T12:49:36Z"
"","5595","KAFKA-7369; Handle retriable errors in AdminClient list groups API","We should retry when possible if ListGroups fails due to a retriable error (e.g. coordinator loading).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-31T19:47:07Z","2018-09-01T14:43:48Z"
"","5514","KAFKA-7296; Handle coordinator loading error in TxnOffsetCommit","We should check TxnOffsetCommit responses for the COORDINATOR_LOADING_IN_PROGRESS error code and retry if we see it. Additionally, if we encounter an abortable error, we need to ensure that pending transaction offset commits are cleared.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-15T22:09:12Z","2018-08-17T10:14:02Z"
"","5857","KAFKA-7481; Add upgrade/downgrade notes for 2.1.x","We seemed to be missing the usual rolling upgrade instructions so I've added them and emphasized the impact for downgrades after bumping the inter-broker protocol version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-30T18:59:44Z","2018-11-06T21:58:51Z"
"","6460","MINOR: Improve verification in flaky testPartitionReassignmentDuringDeleteTopic","We saw one failure in testPartitionReassignmentDuringDeleteTopic unit test couple of months ago. The investigation so far did not find any real failure. However, the test  checks whether `controller.kafkaController.controllerContext.partitionsBeingReassigned` contains partition being reassigned after verifying that reassignment znode does not exist anymore (via `ReassignPartitionsCommand.checkIfPartitionReassignmentSucceeded`). `controller.kafkaController.controllerContext.partitionsBeingReassigned` is updated after znode is deleted, so there is a very small chance that partitions still exists in `partitionsBeingReassigned`. This PR adds a waited check for `partitionsBeingReassigned`. ``` java.lang.AssertionError: Partition reassignment should fail 	at org.junit.Assert.fail(Assert.java:88) 	at org.junit.Assert.assertTrue(Assert.java:41) 	at org.junit.Assert.assertFalse(Assert.java:64) 	at kafka.admin.DeleteTopicTest.testPartitionReassignmentDuringDeleteTopic(DeleteTopicTest.scala:141) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2019-03-17T23:13:51Z","2019-03-19T01:48:02Z"
"","6008","MINOR: fix checkpoint write failure warning log","We saw a log statement in which the cause of the failure to write a checkpoint was not properly logged. This change logs the exception properly and also verifies the log message.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-12-06T18:42:27Z","2018-12-11T15:02:51Z"
"","6112","MINOR: Remove throwing exception if not found from describe topics","We recently improved the handling of the `InternalTopicManager` retries with #6085.  The `AdminClient` will throw an `InvalidTopicException` if the topic is not found.  We need to ignore that exception as when calling `AdminClient#describe` we may not have had a chance to create the topic yet, especially with the case of internal topics  I've created a new test asserting that when an `InvalidTopicException` is thrown when the topic is not found we continue on.  For system tests, I'll kick off the individual tests that have failed due to internal topics not yet created [StreamsUpgradeTest#upgrade_downgrade_brokers](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2249/) [StreamsBrokerCompatibilityTest](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2250/) EDIT removing StreamsNamedReparitionTopicTest - not affected by this  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-01-09T18:32:32Z","2019-01-18T17:37:45Z"
"","5772","Amend GetOffsetShell to support passing SSL properties","We need to be able to monitor the current offset and lag on SSL enabled Kafka clusters. This functionality has been added to other classes (e.g. ConfigCommand), but not this one. I have replicated the same syntax for the command line option.  Change has been tested by calling GetOffShell using both SSL (v1.1) and non-SSL (v0.10.2.1) clusters.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","narkedboy","2018-10-10T11:29:48Z","2020-02-02T21:16:15Z"
"","5672","MINOR: rename InternalProcessorContext.initialized","We named this method `initialized` because it marks the context as having been initialized. The downside is that it sounds more like a getter upon casual reading, which is weird since it's `void`.  Note also that the inverse method is already called `uninitialize`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-21T16:20:48Z","2018-09-21T20:48:41Z"
"","6048","MINOR: Increase timeout in transiently failing security test","We have been seeing some transient failures in the security system test. The timeout passed to `ConsoleConsumer` was set inconsistently with what was being enforced in `ProduceConsumeValidateTest` and was probably a bit too low (10 seconds). This patch drops the `ConsoleConsumer` timeout override so that the test relies exclusively on the 60 second timeout set by `ProduceConsumeValidateTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-12-19T01:08:49Z","2018-12-22T22:35:32Z"
"","5961","MINOR: improve state directory test","We get a failing Jenkins build with ``` java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertFalse(Assert.java:64) at org.junit.Assert.assertFalse(Assert.java:74) at org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldNotCreateBaseDirectory(StateDirectoryTest.java:363) ```  This is an attempt to stabilize the test plus some cleanup.","closed","streams,","mjsax","2018-11-28T18:08:25Z","2018-11-30T18:31:35Z"
"","6405","KAFKA-8070: Increase consumer startup timeout in system tests","We currently use 10 seconds as the timeout for ConsoleConsumer process to be started in ConsumerGroupCommandTest. For tests using SSL, this requires SSL keystores to be created first and then the process is started. Looking at successful test runs, it typically takes between 5 and 7 seconds to start SSL-enabled consumer process. But there have been several test failures that show `Consumer was too slow to start` in tests using SSL. The logs from the last two failures in ConsumerGroupCommand test had consumers which successfully started with SSL, but took ~13 seconds to log their first message. Hence changing the timeout to 20 seconds in system tests that check for consumer start up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-08T13:47:00Z","2019-03-08T16:57:59Z"
"","5890","KAFKA-7605; Retry async commit failures in integration test cases to fix flaky tests","We are seeing some timeouts in tests which depend on the `awaitCommitCallback` (e.g. `SaslMultiMechanismConsumerTest.testCoordinatorFailover`). After some investigation, we found that it is caused by a disconnect when attempting the async commit. To fix the problem, we have added simple retry logic to the test utility.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-11-08T00:13:56Z","2018-11-13T06:21:45Z"
"","5799","MINOR: Less restrictive assertion in flaky BufferPool test","We are routinely seeing CI failures from an assertion in `testBlockTimeout()`, which relies on the interaction between (de)allocations against a producer BufferPool, across multiple threads. I reproduced the failed assertion in my local build at a rate of 2% (N=100): ```java assertTrue(""available memory"" + pool.availableMemory(),      pool.availableMemory() >= 9 && pool.availableMemory() <= 10); ```  Here's a summary of the test history: - (https://github.com/apache/kafka/pull/1304) KAFKA-3648; maxTimeToBlock in BufferPool.allocate should be enforced   - Ensured timeout was properly checked and iterated during a blocked allocation.   - Added the test's deferred deallocations. - (https://github.com/apache/kafka/pull/2659) KAFKA-4840 : BufferPool errors can cause buffer pool to go into a bad state   - Ensured proper cleanup of blocked allocation requests given an exception.    - Added assertion in question.  This patch decreases the lower bound for expected available memory, as thread scheduling entails that a variable amount of deallocation happens by the point of assertion.  The patch also makes minor clarifications to test logic and comments. On my machine, it has a 100% passing rate (N=500).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","colinhicks","2018-10-15T14:33:29Z","2018-10-21T21:04:29Z"
"","6376","[KAFKA-8020] Consider making ThreadCache a TLRUCache","We are implementing an time-aware LRU Cache to supplant the NamedCache currently used by Kafka Streams.  The tests will be modified to check to see that entries which have their life time expired will be removed first.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ConcurrencyPractitioner","2019-03-06T04:03:20Z","2021-06-30T17:44:44Z"
"","5540","Small refactorings on KTable joins","WDYT?  @vvcephei  @guozhangwang","closed","","joan38","2018-08-20T22:35:45Z","2018-08-23T18:36:12Z"
"","6254","MINOR: drop dbAccessor reference on close","Very minor, but we should drop the reference to `dbAccessor` in the RocksDBStore after we close it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-02-11T18:53:53Z","2019-02-14T18:06:04Z"
"","6477","MINOR: Use https instead of http in links","Verified that the https links work.  I didn't update the license header in this PR since that touches so many files. Will file a separate one for that.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-03-20T12:32:03Z","2019-04-22T18:58:54Z"
"","5736","KAFKA-7476: Fix Date-based types in SchemaProjector","Various converters (AvroConverter and JsonConverter) produce a SchemaAndValue consisting of a logical schema type and a java.util.Date. This is a fix for SchemaProjector to properly handle the Date.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-10-03T18:15:57Z","2020-10-16T06:03:02Z"
"","5822","Fixed KAFKA-7382 - guarantee atleast one replica of partition to be alive during topic creation.","Validate that the replica assignment provided during topic creation contains at least one alive broker in the kafka cluster.  Reuse the method AdminZkClient.validateReplicaAssignment().   - Test for creating a topic with replica assignment with valid brokers. - Test for creating a topic with replica assignment with invalid brokers.","open","","sumannewton","2018-10-21T16:30:57Z","2019-03-11T19:43:06Z"
"","6237","MINOR: Close ZKDatabase","Using the EmbeddedKafkaBroker on Windows fills up the temp folder because the ZKDatabase is never closed and therefore the data directory can not be deleted (Windows can't delete files where an open file handle exists).","closed","tests,","ArloL","2019-02-07T08:41:55Z","2019-07-28T06:35:26Z"
"","6082","KAFKA-7781: Add validation check for retention.ms topic property.","Using AdminClient#alterConfigs, topic `retention.ms` property can be assigned to a value lesser than -1. This leads to inconsistency while describing the topic configuration. We should not allow values lesser than -1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-01-02T12:32:53Z","2019-01-29T05:03:40Z"
"","6427","MINOR: Better messaging for invalid fetch response","Users have reported (KAFKA-7565) that when consumer poll wake up is used, it is possible to receive fetch responses that don't match the copied topic partitions collection for the session when the fetch request was created.  This commit improves the error handling here by throwing an IllegalStateException instead of a NullPointerException. And by generating a message for the exception that includes a bit of more information.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-03-11T18:52:15Z","2019-03-13T21:34:13Z"
"","5711","Update README.md __ add maven settings.xml typical location","user maven settings location update  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","FuqiaoWang","2018-09-28T06:50:02Z","2018-12-05T09:10:58Z"
"","6097","KAFKA-5061 - Make default Worker Task client IDs distinct","Use the task ID to make the default client IDs used by Worker Tasks distinct and stable. This is avoids name conflicts on JMX MBeans and enables useful monitoring.  This implements [KIP-411](https://cwiki.apache.org/confluence/display/KAFKA/KIP-411%3A+Make+default+Kafka+Connect+worker+task+client+IDs+distinct), and so it should be merged only after that KIP is approved.  See: https://issues.apache.org/jira/browse/KAFKA-5061  This PR is an alternative to https://github.com/apache/kafka/pull/5775, avoiding the need for a new  configuration option.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","pdavidson100","2019-01-07T18:17:14Z","2022-01-13T14:16:45Z"
"","6457","KAFKA-8119; Ensure KafkaConfig listener accessors work during update","Use the same config to obtain all properties used in each KafkaConfig accessor method to ensure that validation doesn't fail if config was updated during the method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rajinisivaram","2019-03-17T17:04:49Z","2020-01-09T11:19:57Z"
"","6300","MINOR: Handle Metadata v0 all topics requests during parsing","Use of `MetadataRequest.isAllTopics` is not consistently defined for all versions of the api. For v0, it evaluates to false. This patch makes the behavior consistent for all versions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-21T16:46:39Z","2019-02-21T21:11:37Z"
"","5972","KAFKA-7832 Use automatic RPC generation in CreateTopics","Use KAFKA-7609 RPC generation in CreateTopics.  This was split off of KAFKA-7609","closed","","cmccabe","2018-11-29T18:21:03Z","2019-02-04T18:39:44Z"
"","6433","KAFKA-8094: Iterating over cache with get(key) is inefficient","Use concurrent data structure for the underlying cache in NamedCache, and iterate over it with subMap instead of many calls to get()  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-12T18:28:10Z","2019-03-19T15:51:29Z"
"","6395","KAFKA-8001: Reset future replica fetcher when local log becomes leader.","Upon becoming leader, the local replica can fail with `FENCED_LEADER_EPOCH` on its next fetch from the the future replica. In this condition, fetching is stalled until the next leader change. This patch avoids this scenario by removing then re-adding from `replicaAlterLogDirsManager` partitions for which both the local log is leader and a future replica exists.  The test case asserts that such a partition is reset for fetching with the new leader epoch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","colinhicks","2019-03-07T22:17:26Z","2020-02-28T22:26:46Z"
"","6197","MINOR: Upgrade ducktape to 0.7.5","Upgrade to the latest version of ducktape (0.7.5) Safe to upgrade branches that already have 0.7.1 (that is 1.0 and ahead)  Tested by running all system tests with version 0.7.5 of ducktape  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-01-24T22:38:30Z","2019-02-05T00:03:21Z"
"","6165","MINOR: upgrade to jdk8 8u202","Upgrade from 171 to 202. Unpack and install directly from a cached tgz rather than going via the installer deb from webupd8. The installer is still on 8u919 while we want 202.  Testing via kafka branch builder job  https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2305/","closed","","jarekr","2019-01-17T18:08:04Z","2019-01-25T06:21:27Z"
"","6021","MINOR: Update documentation for internal changelog when using table().","Updating the documentation for table operation because I believe it is incorrect.  In PR #5163 the table operation stopped disabling the changelog topic by default and instead moved that optimization to a configuration that is not enabled by default. This PR updates the documentation to reflect the change in behavior and point to the new configuration for optimization.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","cwildman","2018-12-10T23:13:26Z","2018-12-14T10:08:06Z"
"","5803","KAFKA-7396: document implicit Grouped","Updates the KIP-365 documentation in light of the replacement of Serialized with Grouped.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-15T22:27:36Z","2018-10-16T01:21:27Z"
"","6431","MINOR: Add verification step for Streams archetype to Jenkins build","Updates `./jenkins.sh` to build stream archetype and install it in local maven cache. Afterward, archetype is used to create a new maven project and maven project is compiled for verification.","closed","streams,","mjsax","2019-03-11T23:44:32Z","2019-03-22T00:59:55Z"
"","5517","MINOR: Update test to wait for final value to reduce flakiness updated test method for multiple keys","Updated two integration tests to use `IntegrationTestUtils#waitUntilFinalKeyValueRecordsReceived` to eliminate flaky test results.  Also, I updated `IntegrationTestUtils#waitUntilFinalKeyValueRecordsReceived` method to support having results with the same key present with different values.   For testing, I ran the current suite of streams tests ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-08-16T15:33:19Z","2018-08-17T19:14:30Z"
"","6111","MINOR: Add 2.1 version metadata upgrade","Updated the `test_metadata_upgrade` test.  To enable using the `2.1` version I needed to add config change to the `StreamsUpgradeTestJobRunnerService` to ensure the ductape passes proper args when starting the `StreamsUpgradeTest`    For testing, I ran the `test_metadata_upgrade` test and all versions now pass http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-01-09--001.1547049873--bbejeck--MINOR_add_2_1_version_metadata_upgrade--a450c68/report.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-01-09T16:11:44Z","2019-01-09T23:19:01Z"
"","6229","KAFKA-6786: Removing additional configs for StreamsBrokerDownResilienceTest","Updated iteration of KAFKA-6786. Currently a WIP as it is my first time contributing; want to make sure that we implemented this correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","tests,","sh-abhi","2019-02-03T15:13:33Z","2020-06-15T14:30:22Z"
"","6362","KAFKA-7273: extend Connect Converter to support headers","Update: See [KIP-440](https://cwiki.apache.org/confluence/display/KAFKA/KIP-440%3A+Extend+Connect+Converter+to+support+headers), approved.  Extending `Converter` interface to support headers in backwards-compatible way. Very similar to [Serializer](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/serialization/Serializer.java#L61-L63) and [Deserializer](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/serialization/Deserializer.java) interfaces.  A few questions I have to the contributors: - Do I need to update internal components like KafkaStatusBackingStore that use Converters? It doesn't look like they use headers in the moment, so I don't think it's necessary. - How about other Converter implementations like JsonConverter? Should they all be switched to the new interface method even if they don't use the headers? - ~~Is KIP required in this case? Converter _is_ a public interface, but the change is trivial and backwards-compatible.~~ Created  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","sap1ens","2019-03-04T04:35:02Z","2020-10-16T06:19:05Z"
"","6000","MINOR KAFKA-7705 : update java doc for delivery.timeout.ms","update KafkaProducer javadoc to put delivery.timeout.ms >= request.timeout.ms + linger.ms","closed","","hackerwin7","2018-12-05T09:11:50Z","2018-12-12T16:37:10Z"
"","5778","MINOR: Unmap index on close follow-up","Update comment, isMemoryMappedBufferClosed variable and handle `close` followed by `delete`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ijuma","2018-10-11T00:48:36Z","2018-12-07T00:55:06Z"
"","5520","KAFKA-7284: streams should unwrap fenced exception","Unwrap the ProducerFencedException in RecordCollectorImpl so it can be caught and converted to a TaskMigratedException instead of triggering a shutdown.  See also #5499 and #5513   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-08-16T21:35:00Z","2018-08-21T17:44:31Z"
"","5513","KAFKA-7284: streams should unwrap fenced exception","Unwrap the ProducerFencedException in RecordCollectorImpl so it can be caught and converted to a TaskMigratedException instead of triggering a shutdown.  See also #5499  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-08-15T20:29:25Z","2018-08-15T23:56:16Z"
"","5499","KAFKA-7284: streams should unwrap fenced exception","Unwrap the `ProducerFencedException` in `RecordCollectorImpl` so it can be caught and converted to a `TaskMigratedException`  instead of triggering a shutdown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-13T20:30:19Z","2018-08-14T00:29:33Z"
"","5767","KAFKA-7192: Wipe out state store if EOS is turned on and checkpoint file does not exist","Unified backport of PRs #5421 and #5430  Also compare backport to `0.11.0` #5641 and `1.0` #5657","closed","streams,","mjsax","2018-10-09T21:39:02Z","2018-11-19T23:21:08Z"
"","5657","KAFKA-7192: Wipe out state store if EOS is turned on and checkpoint file does not exist","Unified backport of PRs #5421 and #5430  Also compare backport to `0.11.0` #5641  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-09-15T00:38:19Z","2018-10-08T22:14:24Z"
"","5641","KAFKA-7192: Wipe out state store if EOS is turned on and checkpoint file does not exist","Unified backport of PRs #5421 and #5430","closed","streams,","mjsax","2018-09-12T00:56:52Z","2018-09-14T19:05:30Z"
"","6260","MINOR: add warning for long grace period for suppress","Unfortunately, the default grace period is 24 hours, soif people don't explicitly set the grace period before `Suppress.untilWindowCloses`, the suppression would appear to do nothing for a long time.  My advice is to always explicitly set the grace period when using final-results suppression, but there's no clean way to enforce it. Maybe a warning is good enough?   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-12T19:51:04Z","2019-02-18T16:00:03Z"
"","6350","MINOR - typo: pattten -> pattern","typo: pattten -> pattern  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","pierDipi","2019-03-01T10:03:57Z","2019-03-25T16:54:18Z"
"","5871","DONOTMERGE: Instrumentation to debug PR build failures","Trying to get to the bottom of all the jdk11 build failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-11-02T00:06:03Z","2018-11-14T06:58:24Z"
"","6471","MINOR: Update scoverage so bootstrapping build works with Gradle 5.2","Trying to bootstrap the build with the gradle command with Gradle 5.2 fails to apply the scoverage plugin. Upgrading to the most recent 2.x version of the plugin fixes this build issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2019-03-20T02:12:13Z","2020-01-31T20:52:21Z"
"","6045","KAFKA-7738 Track leader epochs in Metadata","Track the last seen partition epoch in the Metadata class. When handling metadata updates, check that the partition info being received is for the last seen epoch or a newer one. This prevents stale metadata from being loaded into the client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2018-12-17T20:39:54Z","2019-01-19T16:12:51Z"
"","5844","KAFKA-7548 : KafkaConsumer should not throw away already fetched data for paused partitions.","Today when we call KafkaConsumer.poll(), it will fetch data from Kafka asynchronously and is put in to a local buffer (completedFetches). If now we pause some TopicPartitions and call KafkaConsumer.poll(), we might throw away any buffered data that we might have in the local buffer for these TopicPartitions. Generally, if an application is calling pause on some TopicPartitions, it is likely to resume those TopicPartitions in near future, which would require KafkaConsumer to re-issue a fetch for the same data that it had buffered earlier for these TopicPartitions. This is a wasted effort from the application's point of view.  The current patch does not throw away the already buffered data for paused partitions, but stores it in a separate buffer (pausedCompletedFetchesPerTopicPartition). If we have data in this separate buffer for a recently resumed partition, it is given higher preference as compared to other partitions.   The patch also makes sure that if we have data in this separate buffer (pausedCompletedFetchesPerTopicPartition) for a recently resumed partition, it will not send out another fetch request for that TopicPartition until that buffered data has been processed. This guarantees that we would have at the most 1 completedFetch buffered for a paused TopicPartition.","closed","","MayureshGharat","2018-10-26T01:26:36Z","2019-08-02T00:12:50Z"
"","6137","[MINOR] Change the ""ignorable"" field for ControllerId in MetadataResponse schema","to use a boolean instead of the string ""true"". Every other `ignorable` field uses a boolean.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2019-01-13T22:07:50Z","2019-01-26T01:27:36Z"
"","6109","KAFKA-7801: TopicCommand should not be able to alter transaction topic partition count","To keep align with the way it handles the offset topic, TopicCommand should not be able to alter transaction topic partition count.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-01-09T05:57:23Z","2019-03-12T06:08:09Z"
"","6447","MINOR: capture result timestamps in Kafka Streams DSL tests","To enable proper testing of timestamp propagation in the DSL, this PR updates `MockProcessor` to capture the record timestamp.  All tests using `MockProcessor` need to be updated accordingly. This PR only sets most timestamps to zero for now. When we update the DSL semantics, those test will gradually be updated accordingly.  Some code cleanup on the side (Java8 rewrites, reformatting, getting rid of warnings.)","closed","kip,","mjsax","2019-03-14T22:04:27Z","2020-06-12T23:45:53Z"
"","5968","KAFKA-7455: Support JmxTool to connect to a secured RMI port.","To connect to a secured RMI port (enabling remote JMX with password authentication and SSL), JmxTool should pass an envionrment map that contains relevant certification entry.","closed","","murong00","2018-11-29T11:37:17Z","2019-05-06T12:22:54Z"
"","6131","HOTFIX: Compilation error in CommandLineUtils","This was broken by #6084. The syntax works with Scala 2.12, but not 2.11.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-01-11T21:32:30Z","2019-01-11T21:40:33Z"
"","6067","[KStream] remove topic prefix from consumer configuration to resolve unnecessary warning","This warning is emitted on every stream startup when using `topic.` or `public static String topicPrefix(final String topicProp)` properties :   ```java log.warn(""The configuration '{}' was supplied but isn't a known config."", key); ``` This fix will remove the warning","closed","","tchiotludo","2018-12-26T14:01:11Z","2021-07-10T20:24:30Z"
"","5944","MINOR: Catch NoRecordsException in testCommaSeparatedRegex() test","This test sometimes fails with ``` kafka.tools.MirrorMaker$NoRecordsException 	at kafka.tools.MirrorMaker$ConsumerWrapper.receive(MirrorMaker.scala:483) 	at kafka.tools.MirrorMakerIntegrationTest$$anonfun$testCommaSeparatedRegex$1.apply$mcZ$sp(MirrorMakerIntegrationTest.scala:92) 	at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:738) ``` It is a bit puzzling as to why this happens in the first place, as the producer call is synchronous and the consumer uses `auto.offset.reset=earliest`, so it should always return records. But the test originally caught `TimeoutException` with the comment `// this exception is thrown if no record is returned within a short timeout, so safe to ignore`, so I think that it is consistent to catch the other error when no records are returned such that we can retry until the test timeout is hit  Now we properly wait until the timeout of `TestUtils.waitUntilTrue` passes until failing the test.","closed","","stanislavkozlovski","2018-11-26T11:20:10Z","2018-12-08T17:43:44Z"
"","6330","KAFKA-7940: Address flakiness of CustomQuotaCallbackTest#testCustomQuotaCallback","This test has been seen to fail with: ``` java.lang.AssertionError: Too many quotaLimit calls Map(PRODUCE -> 1, FETCH -> 1, REQUEST -> 4) java.lang.AssertionError: Too many quotaLimit calls Map(PRODUCE -> 1, FETCH -> 1, REQUEST -> 3) java.lang.AssertionError: Too many quotaLimit calls Map(PRODUCE -> 1, FETCH -> 1, REQUEST -> 3) ```  All of which are in three test jobs where the following tests are seen to fail: https://jenkins.confluent.io/job/apache-kafka-test/job/2.2/28/ ``` kafka.api.ConsumerBounceTest.testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup kafka.api.CustomQuotaCallbackTest.testCustomQuotaCallback ```  https://jenkins.confluent.io/job/apache-kafka-test/job/2.2/15 ``` kafka.api.CustomQuotaCallbackTest.testCustomQuotaCallback ```  https://jenkins.confluent.io/job/apache-kafka-test/job/2.2/14/ ``` kafka.api.CustomQuotaCallbackTest.testCustomQuotaCallback kafka.server.DynamicBrokerReconfigurationTest.testAddRemoveSaslListeners ```  ### Reproduce Attempts I tried to reproduce this flaky test locally by running the `testCustomQuotaCallback` over and over again. First off, I had the issue where `GroupedUserQuotaCallback` would accumulate mutations and always fail. I thought it would be better if we reset the variables on every `tearDown()` call in `CustomQuotaCallbackTest.scala`  After changing that, I ran the test over and over again. I commented out the lines after line 107 (` assertTrue(s""Too many quotaLimit calls $quotaLimitCalls"", quotaLimitCalls(ClientQuotaType.REQUEST).get <= serverCount)`) since any changes afterwards are ruled out to have an impact.  A lot of red herrings encountered while debugging. I think I managed to pinpoint the cause of this to a race condition in ZooKeeper dynamic config change notifications. ``` Processing override for entityPath: users/group0_ with config: Map(request_percentage -> 1000.0, consumer_byte_rate -> 2000000, producer_byte_rate -> 1000000) 1551189456810 Processing override for entityPath: users/group0_ with config: Map(request_percentage -> 1000.0, consumer_byte_rate -> 2000000, producer_byte_rate -> 1000000) 1551189456810 in UserConfigHandler#processConfigChanges 1551189456811 in UserConfigHandler#processConfigChanges 1551189456811 In updateQuotaMetricConfigs 1551189456811} In updateQuotaMetricConfigs 1551189456811} In updateQuotaMetricConfigs 1551189456811} In updateQuotaMetricConfigs 1551189456811} In updateQuotaMetricConfigs 1551189456811} In updateQuotaMetricConfigs 1551189456811} Updating metric config allMetrics.asScala.filterKeys 1551189456811 In this 1551189456811} RESET THEM (quotaLimitCalls.values.foreach(_.set(0))) Updating metric config allMetrics.asScala.filterKeys 1551189456811 OUT UserConfigHandler#processConfigChanges 1551189456811 OUT UserConfigHandler#processConfigChanges 1551189456811 Asserting 1551189457588 ``` This is basically the trailing result of the `user.configureAndWaitForQuota` call. That test code verifies that the quotas are set on the metrics (https://github.com/apache/kafka/blob/2627a1be2cf5ef865fd1da0bc81680df9612655a/core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala#L297). The problem is that there is a race condition in between the code that updates said metrics - it subsequently calls `ClientQuotaManager#updateQuotaMetricConfigs()` (https://github.com/apache/kafka/blob/2627a1be2cf5ef865fd1da0bc81680df9612655a/core/src/main/scala/kafka/server/ClientQuotaManager.scala#L449) which also [goes on to call]((https://github.com/apache/kafka/blob/2627a1be2cf5ef865fd1da0bc81680df9612655a/core/src/main/scala/kafka/server/ClientQuotaManager.scala#L496) `quotaCallback.quotaLimit`, incrementing the `quotaLimitCalls` variable we check in the test. I found `UserConfigHandler#processConfigChanges()` to be the culprit.  ### Solution We basically need a way to guarantee that `UserConfigHandler#processConfigChanges()` has exited. If the test code's `user.configureAndWaitForQuota()` method had a way to do that it would be perfect. I, unfortunately, could not come up with a good way to address this.  I was thinking that we could correlate the expected `quotaLimitCalls`. I see they had three different kinds of values across tests (with Thread.sleep enabled): ``` resetting Map(PRODUCE -> 5, FETCH -> 5, REQUEST -> 18) resetting Map(PRODUCE -> 6, FETCH -> 6, REQUEST -> 20) resetting Map(PRODUCE -> 7, FETCH -> 7, REQUEST -> 22) ``` Part of those are populated from the test code's `user.configureAndWaitForQuota` (2 request `quota()` calls for each 1 produce, 1 fetch `quota()` call. I can't exactly tell the correlation though.  I brainstormed for a bit but could not figure out a smarter way to guarantee that this test passes unless we add a `Thread.sleep()`. We could potentially add a `waitUntilTrue(quotaLimitCalls(ClientQuotaType.REQUEST) >= 18)` and then reset but that seems even more hacky and error-prone.  Otherwise, we could simply not check `quotaLimitCalls(ClientQuotaType.REQUEST).get <= serverCount` or allow for some leeway there.","closed","","stanislavkozlovski","2019-02-26T15:40:27Z","2019-02-28T17:14:33Z"
"","5942","KAFKA-7670: Admin client testUnreachableBootstrapServer() flaky test","This test easily fails locally around once every 20-30 runs.  ``` java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment.  at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:262) at org.apache.kafka.clients.admin.KafkaAdminClientTest.testUnreachableBootstrapServer(KafkaAdminClientTest.java:277) at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298) at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292) ```  After spending considerate time debugging, I found out that the weakly-consistent iterator of `futureResponses` does not iterate through the two responses at all in the failing runs.  No elements are ever removed from `futureResponses` except in the `MockClient#reset()` method which I verified is never called. I think the cause is the weakly-consistent iterator of Java which states:  > they are guaranteed to traverse elements as they existed upon construction exactly once, **and may (but are not guaranteed to) reflect any modifications subsequent to construction.**  I'm still unsure how to solve this in the cleanest way. I'm opening this PR as a chance to discuss it with other people","closed","","stanislavkozlovski","2018-11-22T19:59:55Z","2019-02-18T11:30:34Z"
"","5589","KAFKA-7356 Added allDeps task to generate complete dependency report.","This task allows a user to examine the dependency list to confirm/deny use of a specific dependency. Running `$ gradle -q dependencies` in the root directory only lists the `rat` dependencies. Adding a custom section to _build.gradle_ allows for a complete listing of the dependencies from the command line.   ``` subprojects {     task allDeps(type: DependencyReportTask) {} } ```  To invoke: `$ gradle allDeps`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","alopresto","2018-08-29T21:34:25Z","2019-11-19T05:39:16Z"
"","6341","MINOR: Ignore streams-broker-upgrade-test","This should be cherry-picked to older branches as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-02-28T02:36:51Z","2020-04-24T23:44:50Z"
"","5868","MINOR: Avoid logging connector configuration in Connect framework","This should be backported as far back as possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-11-01T22:10:28Z","2020-10-16T06:21:20Z"
"","5870","MINOR: KStreams SuppressionDurabilityIntegrationTest should set StreamsConfig.STATE_CONFIG_DIR.","This sets StreamsConfig.STATED_DIR_CONFIG to temp directory in KStreams SuppressionDurabilityIntegrationTest, to match StreamsTestUtils.  This is a similar fix to #5826.","closed","","lbradstreet","2018-11-01T22:56:00Z","2018-11-05T19:22:57Z"
"","5826","MINOR: KStreams SuppressScenarioTest should set StreamsConfig.STATE_DIR_CONFIG.","This sets StreamsConfig.STATED_DIR_CONFIG in KStreams SuppressScenarioTest, as with StreamsTestUtils. I have deliberately avoided using StreamsTestUtils as this test sets bogus config parameters, but still fails if the default STATE_DIR_CONFIG does not exist.","closed","","lbradstreet","2018-10-22T17:20:28Z","2018-10-27T16:22:03Z"
"","5847","MINOR: KStreams SuppressionIntegrationTest should set StreamsConfig.STATE_CONFIG_DIR.","This sets StreamsConfig.STATE_DIR_CONFIG to temp directory in KStreams SuppressionIntegrationTest, to match StreamsTestUtils.  This is a similar fix to https://github.com/apache/kafka/pull/5826.","closed","","lbradstreet","2018-10-27T18:14:09Z","2019-05-07T02:05:02Z"
"","6011","[WIP] KAFKA-6794: Incremental partition reassignment","This pull request replaces the current partition reassignment strategy with an incremental one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-12-07T15:40:49Z","2020-04-30T09:18:21Z"
"","6032","KAFKA-7734: Metrics tags should use LinkedHashMap to guarantee ordering","This pull request replaces HashMap with LinkedHashMap to guarantee ordering of metrics tags.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","lambdaliu","2018-12-14T13:24:22Z","2019-01-04T11:21:11Z"
"","5922","KAFKA-6567: Remove KStreamWindowReducer","This pull request removes the final reference to `KStreamWindowReducer` and replaces it with `KStreamWindowAggregate`  Signed-off-by: Samuel Hawker   contribution is my original work and that I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","samuel-hawker","2018-11-17T00:28:10Z","2018-11-20T09:32:41Z"
"","5814","KAFKA-5876: IQ should throw different exceptions for different errors(KIP-216)","This pr would be discussed KIP-216 of the mailing list in parallel.  Any suggestion is welcome.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vitojeng","2018-10-18T03:09:27Z","2019-07-02T04:02:27Z"
"","6238","KAFKA-7893 Refactor ConsumerBounceTest to reuse functionality from BaseConsumerTest","This PR should help address the flakiness in the ConsumerBounceTest#testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup test (https://issues.apache.org/jira/browse/KAFKA-7965). I tested this locally and have verified it significantly reduces flakiness - 25/25 tests now pass. Running the test 25 times in trunk, I'd get `18/25` passes.  It does so by reusing the less-flaky consumer integration testing functionality inside `BaseConsumerTest`. Most notably, the test now makes use of the `ConsumerAssignmentPoller` class  - each consumer now polls non-stop rather than the more batch-oriented polling we had in `ConsumerBounceTest#waitForRebalance()`.","closed","tests,","stanislavkozlovski","2019-02-07T21:36:48Z","2019-04-05T20:08:05Z"
"","6188","KAFKA-7859: Use automatic RPC generation in LeaveGroups","This pr serves as a sub task of JIRA: https://issues.apache.org/jira/browse/KAFKA-7830 to eventually replace all the hand-coded request/response interfaces with Colin's new automated generation protocols. There are still some dependencies of this pr over https://github.com/apache/kafka/pull/5972, so we shall wait until it lands to continue address some minor issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-01-23T19:13:47Z","2019-01-31T18:39:49Z"
"","6105","MINOR: Put states in proper order, increase timeout for starting","This PR puts the states in the `stateListener.onChange` method in the correct order `newState, oldState`.  Updates to the new `KafkaStreams` states has exposed this bug.  Updated code for system test and ran [successfully](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2242/), and [test report](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-01-08--001.1546979269--bbejeck--MINOR_change_order_of_states_optimizable_test--2bf426a/report.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-01-08T21:28:11Z","2019-01-08T21:48:39Z"
"","5505","Test KAFKA-7019","This PR is opened solely to debug test failure for PR #5221. No need to review.","closed","","lindong28","2018-08-14T16:22:52Z","2018-08-15T00:10:43Z"
"","6147","KAFKA-6455: Extend CacheFlushListener to forward timestamp","This PR is of part of the overall KIP-258 story. It does not resolve KAFKA-6455, but prepares it. It's only internal changes so we can merge right away.","closed","kip,","mjsax","2019-01-15T19:45:57Z","2020-06-12T23:53:14Z"
"","5506","Test trunk","This PR is created solely to test whether all tests can pass in trunk. No need for review.","closed","","lindong28","2018-08-14T16:25:00Z","2018-08-15T00:10:40Z"
"","6463","KAFKA-8026: Fix flaky regex source integration test 1.0","This PR is an attempt to fix the RegexSourceIntegrationTest flakiness. My investigation did not reveal any issues, and I was unable to reproduce the failure locally. So I've taken some steps to eliminate what I think could be a race condition in the test.  This is a mirror image of #6459 and is the fix for `1.0`  1. Delete and create all topics before each test starts. 2. Give the streams application in each test a unique application ID 3.  Create a KafkaStreams instance local to each test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-18T13:09:56Z","2019-03-25T21:10:29Z"
"","6459","KAFKA-8026: Fix for flaky RegexSourceIntegrationTest","This PR is an attempt to fix the `RegexSourceIntegrationTest` flakiness.  My investigation did not reveal any issues, and I was unable to reproduce the failure locally.  So I've taken some steps to eliminate what I think could be a race condition in the test.  1. Delete and create all topics before each test starts. 2. Give the streams application in each test a unique application ID 3. Create a `KafkaStreams` instance local to each test.  If the Jenkins build is all green for this PR, I will create a separate PR for `1.0` and make the same fixes up from `2.0` to `trunk` I ran the entire suite of streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-03-17T22:18:56Z","2019-03-25T16:24:43Z"
"","6453","KAFKA-7502: Cleanup KTable materialization logic in a single place (filter)","This PR is a follow-up of #6174, which handles `doFilter` method.  cc/ @guozhangwang @bbejeck  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2019-03-16T10:29:13Z","2019-03-28T20:57:13Z"
"","5821","KAFKA-7235: Detect outdated control requests and bounced brokers using broker generation","This PR introduces the broker generation concept and leverage it to allow controller to detect fast bounced brokers and allow broker to reject outdated control requests.  It has the changes required to implement KIP-380:  [Common] - Refactor ZookeeperClient to expose the zookeeper `multi` request directly - Refactor KafkaZkClient to use MultiRequest instead of raw zookeeper transaciton - Atomically get creation transaction id (czxid) with broker znode creation and use it as broker epoch to identify broker gerneration across bounces - Introduce LeaderAndIsrRequest V2, UpdateMetadataRequest V5 and StopReplicaRequest V1 to include broker epoch in control requests and normalize their schemas to make it more memory efficient - Add STALE_BROKER_EPOCH error  [Broker] - Cache the current broker epoch after broker znode registration - Reject LeaderAndIsrRequest, UpdateMetadataRequest and StopReplicaRequest if the request's broker epoch < current broker epoch, and respond back with STALE_BROKER_EPOCH error  [Controller] - Cache/update broker epochs in `controllerContext.brokerEpochsCache` after reading from zk when processing `BrokerChange` event and `onControllerFailover` - Detect bounced brokers in `BrokerChange` event by comparing the broker epochs get from zk and cached broker epochs and trigger necessary state changes - Avoid sending out requests to dead brokers  [Test] - Add `BrokerEpochIntegrationTest` to test broker processing new versions of the control requests and rejecting requests with stale broker epoch - Add a test case in `ControllerIntegrationTest` to test controller detecting bounced brokers - Add test cases in `RequestResponseTest` to test seralization and de-seralization for new versions of the control requests - Add `ControlRequstTest` unit test to test control requests schemas normalization  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-10-20T07:38:39Z","2018-12-03T06:24:02Z"
"","5675","KAFKA-7430: Improve Transformer interface JavaDoc","This PR improves the JavaDoc of the transformer interface. See https://issues.apache.org/jira/browse/KAFKA-7430 for more details.","closed","","efeller","2018-09-21T19:02:13Z","2018-09-24T16:25:55Z"
"","5927","KAFKA-7632: Allow fine-grained configuration for compression","This PR implements [KIP-390: Add producer option to adjust compression level](https://cwiki.apache.org/confluence/display/KAFKA/KIP-390%3A+Add+producer+option+to+adjust+compression+level).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2018-11-18T22:11:23Z","2021-06-05T16:32:42Z"
"","5709","KAFKA-7406: Name join group repartition topics","This PR has the changes required to implement KIP-372.  1. Name repartition topics for Joins 2. Name repartition topics for Grouping operations 3. For optimizations where repartition nodes are reduced or merged the topic name of the **_first_** repartition node to be merged is used as the topic name for the new optimized repartition node. 4. Added the `Grouped` class 5. Deprecated the `Serialized` class 6. Updated Joined to accept a `name` parameter for naming repartition topics if required. 6. Updated KStream.groupBy, KStream.groupByKey, KTable.groupBy methods to accept a `Grouped` instance and deprecated methods taking a `Serialized` parameter.  Also, our approach when optimizing repartition topic is to **_always_** use the first repartition topic to get merged even if the user has elected to not name any repartition topics.  This approach should make the new topology compatible with existing applications.  However, an application reset may be required as all data may not have been consumed from existing repartition topics.  I've added tests for the following:  1. Verify that named repartition topics stay unchanged when topology changes. 2. Verify that user-specified repartition topic names are used over Streams generated names. 2. Verify that when optimization occurs, the new optimized repartition topic is the same name as the **_first_** replaced/merged repartition node. 3. The existing tests validate that when names for repartition topics are not provided the Streams generated names are used.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-28T04:20:44Z","2018-10-02T22:15:53Z"
"","5566","MINOR: Fix streams Scala peek recursive call","This PR fixes the previously recursive call of Streams Scala peek  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-08-24T00:18:42Z","2018-08-30T21:09:27Z"
"","6297","HOT_FIX: Change flag so plain RocksDB instance returned","This PR fixes the issue found in the soak testing cluster.  Once merged this will get cherry-picked to 2.2  I've added a test that fails without this fix  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-20T19:58:03Z","2019-02-21T00:12:05Z"
"","5601","KAFKA-7304 memory leakage in org.apache.kafka.common.network.Selector","This PR fixes potential memory leak when Selector is closed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tedyu","2018-09-01T22:47:17Z","2018-09-02T03:58:39Z"
"","5959","KAFKA-7671: Stream-Global Table join should not reset repartition flag","This PR fixes an issue reported from a user.  When we join a `KStream` with a `GlobalKTable` we should not reset the repartition flag as the stream may have previously changed its key, and the resulting stream could be used in an aggregation operation or join with another stream which may require a repartition for correct results.  I've added a test which fails without the fix.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-11-28T15:46:59Z","2018-11-29T20:10:42Z"
"","6029","MINOR:Start processor inside verify message","This PR fixes a flaky system test.    I ran six runs of branch builder, and each run was parameterized to repeat the test 25 times for a total of 150 runs.  All test runs passed.  1. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2122/ 2. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2123/ 3. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2124/ 4. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2128/ 5. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2129/ 6. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2130/   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-12-13T15:08:40Z","2018-12-14T16:28:32Z"
"","6220","KAFKA-7401: Fix inconsistent range exception on segment recovery","This PR fixes ""java.lang.IllegalArgumentException: inconsistent range"" which happens on broker startup after  unclean shutdown during log cleaning phase that creates swap files (in case where base offset < log start offset).  Added `testRecoveryAfterCrashAndIncrementedLogStartOffset` that reproduces Kafka-7401.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2019-02-01T23:00:59Z","2019-12-02T17:53:58Z"
"","6012","KAFKA-4850: Enable bloomfilters","This PR enables BloomFilters for RocksDB to speed up point lookups. The request for this has been around for some time - https://issues.apache.org/jira/browse/KAFKA-4850  For testing, I've done the following   1. Ran the standard streams suite of unit and integration tests 2. Kicked off [the simple benchmark test with bloom filters enabled  ](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2115/) 3. Kicked off [the simple benchmark test with bloom filters not enabled  ](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2116/) 4. Kicked off [streams system tests](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2114/)     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-12-07T15:47:45Z","2019-01-24T00:00:27Z"
"","5620","KAFKA-1880: Add support for checking binary/source compatibility","This PR aims to extend the gradle build with a task that can generate a report for API/ABI compatibility.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","viktorsomogyi","2018-09-07T09:26:44Z","2020-06-11T14:39:14Z"
"","5522","Minor: add valueChangingOperation and mergeNode to StreamsGraphNode#toString","This PR adds valueChangingOperation and mergeNode to StreamsGraphNode#toString  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-08-16T22:51:43Z","2018-08-17T23:14:00Z"
"","6087","Trogdor Improvements: execute Trogdor tasks directly and run the Trogdor worker in a separate process","This PR adds two features: 1) directly execute Trogdor tasks without setting up coordinators and agents, 2) make agents be able to execute shell commands as the worker in another process, so we have the flexibility of running workloads, such as non-Java clients, docker images, etc.  * Directly execute Trogdor tasks (feature 1).     * Add TrogdorLocalRunner that reads a task from the spec, executes the task, and reports results.     * Add two SPEC examples: consume-bench-spec.json and produce-bench-spec.json.     *  Add/update helper scripts.     *  Update TROGDOR.md  * Run the Trogdor worker in a separate process (feature 2).   * Users pass the worker command in spec.workerCommand, such as ""workerCommand"": [""python"", ""./producer.py""]   * RuntimeProcessWorker starts the worker in a process by executing `workercommand --spec TASK_SPEC`.   * RuntimeProcessWorker monitors the worker's stderr and stdout line by line, if the stdout line is a JSON value, it updates its status with the JSON value.     *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangxi","2019-01-04T12:52:46Z","2019-01-16T13:22:00Z"
"","6014","MINOR: Add unit for max latency in printf statement","This PR adds the unit for a metric.   The change was tested by running the main() method and observing the changed logs in the console.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-12-07T20:54:34Z","2018-12-12T16:28:59Z"
"","6065","added offsets and headers to consumer cli (KAFKA-7767)","This PR adds the capability to print partition/offset pairs per message, and its headers.  added configs: print.offsets=true|false (print partition/offset pair) print.headers=single_line|line_per_header|false (print headers in one line (to be able to grep on it) or in separate lines). headers.separator= (separate between each header, if print.headers==single_line) headers.kv.separator= (separate between each key and value).  https://issues.apache.org/jira/browse/KAFKA-7767","open","","berman7","2018-12-24T16:48:54Z","2018-12-24T16:49:34Z"
"","6265","KAFKA-7758: Reuse KGroupedStream/KGroupedTable with named repartition topics","This PR adds support for re-using a `KGroupedStream` or `KGroupedTable object after executing an aggregation operation with a named repartition topic.   `KGroupedStream` example ```java final KGroupedStream kGroupedStream = builder.stream(""topic"").selectKey((k, v) -> k).groupByKey(as(""grouping"")); kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(10L))).count().toStream().to(""output-one""); kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count().toStream().to(""output-two""); ``` `KGroupedTable` example ```java final KGroupedTable kGroupedTable = builder.table(""topic"").groupBy(KeyValue::pair, Grouped.as(""grouping""));  kGroupedTable.count().toStream().to(""output-count"");  kGroupedTable.reduce((v, v2) -> v2, (v, v2) -> v2).toStream().to(""output-reduce""); ``` This approach will not cause any compatibility issues for two reasons  1. Aggregations requiring repartitioning without naming the repartition topic maintain the same topology structure, which is the default mode today.  So by not reusing the repartition graph node, the numbering and structure of the topology remains the same. 2. Aggregations where the repartition topic _*is*_ named, it is not possible at the moment to re-use either the `KGroupedStream` or `KGroupedTable` object as Kafka Streams throws an `InvalidTopologyException` when building the topology. Hence you can't even deploy the application.  I've added unit tests for each case and ran our existing suite of streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-13T21:20:22Z","2019-02-18T01:09:35Z"
"","6118","KAFKA-7804: Update docs for topic-command related KIP-377","This PR adds a upgrade notes and changes examples to use the bootstrap-server.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-01-10T12:21:30Z","2019-02-04T21:49:54Z"
"","5864","KAFKA-7514: Add threads to ConsumeBenchWorker","This PR adds a new ConsumeBenchSpec field - ""consumerCount"". ""consumerCount"" will be spawned over ""consumerCount"" threads in the ConsumeBenchWorker. Since ""consumerCount"" will be 1 by default, these changes are backwards compatible  It's now questionable how existing fields such as ""targetMessagesPerSec"", ""maxMessages"", ""consumerGroup"" and ""activeTopics"" should work.  With ""activeTopics"", we need to decide whether they should be split over the consumers or not. I see 4 cases which I believe we should address like this:  1. Random group, subscribe to topics - N unique groups all subscribed to all topics 2. Specifed group, subscribe to topics - 1 group subscribed to all topics. Consumers share workload. 3. Random groups, assign partitions - X groups all subscribed to all partitions 4. Specified group, assign partitions - Throw an exception. Only one consumer can read from a specific partition within the context of a consumer group. It is then unclear how and whether at all we should split the partitions across the consumers. At this phase, I believe it's best to not support this.   I believe ""targetMessagesPerSec"", ""maxMessages"" should account for each consumer individually. This would ease implementation by a ton, too.   I haven't written tests yet since I want to flesh out the design first. Any feedback is appreciated","closed","","stanislavkozlovski","2018-10-31T23:23:11Z","2018-11-13T16:38:43Z"
"","5863","[KAFKA-7514] [WIP] consumer bench multiple threads","This PR adds a new ConsumeBenchSpec field - ""consumerCount"". ""consumerCount"" will be spawned over ""consumerCount"" threads in the ConsumeBenchWorker.  It's now questionable how existing fields such as ""targetMessagesPerSec"", ""maxMessages"", ""consumerGroup"" and ""activeTopics"" should work.  With ""activeTopics"", we need to decide whether they should be split over the consumers or not. I see 4 cases which I believe we should address like this:  Random group, subscribe to topics - N unique groups all subscribed to all topics Specific group, subscribe to topics - 1 group subscribed to all topics. Consumers share workload. Random groups, assign partitions - X groups all subscribed to all partitions Specific group, assign partitions - 1 group all subscribed to part of the partitions (split in a round-robin style) I believe ""targetMessagesPerSec"", ""maxMessages"" should account for each consumer individually. This would ease implementation too.","closed","","stanislavkozlovski","2018-10-31T23:21:34Z","2018-10-31T23:22:18Z"
"","5852","KAFKA-7564: Expose single task details in Trogdor","This PR adds a new ""/coordinator/tasks/{taskId}"" endpoint which fetches details for a single task","closed","","stanislavkozlovski","2018-10-29T12:44:56Z","2018-11-09T18:31:06Z"
"","6043","MINOR: Stabilization fixes broker down test trunk","This PR addresses a few issues with this system test flakiness. This PR is a cherry-picked duplicate of #6041 but for the trunk branch, hence I won't repeat the inline comments here.  1. Need to grab the monitor before a given operation to observe logs for signal 2. Relied too much on a timely rebalance and only sent a handful of messages. I've updated the test and ran it here https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2143/ parameterized for 15 repeats all passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-17T16:22:13Z","2018-12-20T01:53:11Z"
"","6042","MINOR: Fixes for broker down test stability 2.1","This PR addresses a few issues with this system test flakiness. This PR is a cherry-picked duplicate of #6041 but for the 2.1 branch, hence I won't repeat the inline comments here.  1. Need to grab the monitor before a given operation to observe logs for signal 2. Relied too much on a timely rebalance and only sent a handful of messages. I've updated the test and ran it here https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2142/ parameterized for 15 repeats all passed.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-17T16:16:55Z","2018-12-20T01:52:10Z"
"","6041","MINOR: Streams broker down flaky test","This PR addresses a few issues with this system test flakiness. I'll issue similar PRs for `2.1` and `trunk` as well.  1. Need to grab the monitor before a given operation to observe logs for signal 2. Relied too much on a timely rebalance and only sent a handful of messages.   I've updated the test and ran it here https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2141/ parameterized for 15 repeats all passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-17T15:38:41Z","2018-12-20T01:53:44Z"
"","6240","MINOR: improve stabilty of ProcessorStateManagerTest","This PR addressed the following test failure: ``` java.lang.AssertionError: Expected: a string starting with ""process-state-manager-test Failed to write offset checkpoint file to ["" but: was ""[AdminClient clientId=adminclient-874] Connection to node -1 (localhost/127.0.0.1:8080) could not be established. Broker may not be available."" ```","closed","streams,","mjsax","2019-02-08T01:58:59Z","2019-02-08T21:49:28Z"
"","5608","MINOR: Move common out of range handling into AbstractFetcherThread","This patch removes the duplication of the out of range handling between `ReplicaFetcherThread` and `ReplicaAlterLogDirsThread` and attempts to expose a cleaner API for extension. It also adds a mock implementation to facilitate testing and several new test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-04T17:14:19Z","2018-09-09T00:00:14Z"
"","6163","KAFKA-7641: Introduce ""group.max.size"" config to limit group sizes","This patch introduces a new config - ""consumer.group.max.size"", which caps the maximum size any consumer group can reach. It has a default value of Int.MAX_VALUE. Once a consumer group is of the maximum size, subsequent JoinGroup requests receive a MAX_SIZE_REACHED error.  In the case where the config is changed and a Coordinator broker with the new config loads an old group that is over the threshold, members are kicked out of the group and a rebalance is forced.  I have added two integration tests for both scenarios - a member joining an already-full group and a rolling restart with a new config  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2019-01-17T10:30:11Z","2019-02-02T03:56:40Z"
"","5689","KAFKA-7437; Persist leader epoch in offset commit metadata","This patch implements the changes described in KIP-320 for the persistence of leader epoch information in the offset commit protocol.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-25T00:42:40Z","2018-09-26T13:25:55Z"
"","5813","KAFKA-7567; Clean up internal metadata usage for consistency and extensibility","This patch has two objectives to improve metadata handling logic and testing:  1. We want to reduce dependence on the public object `Cluster` for internal metadata propagation since it is not easy to evolve. As an example, we need to propagate leader epochs from the metadata response to `Metadata`, but it is not straightforward to do this without exposing it in `PartitionInfo` since that is what `Cluster` uses internally. By doing this change, we are able to remove some redundant `Cluster` building logic.  2. We want to make the metadata handling in `MockClient` simpler and more consistent. Currently we have mix of metadata update mechanisms which are internally inconsistent with each other and do not match the implementation in `NetworkClient`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-18T00:00:44Z","2018-10-30T20:20:14Z"
"","5533","KAFKA-7313; StopReplicaRequest should attempt to remove future replica for the partition only if future replica exists","This patch fixes two issues:  1) Currently if a broker received StopReplicaRequest with delete=true for the same offline replica, the first StopRelicaRequest will show KafkaStorageException and the second StopRelicaRequest will show ReplicaNotAvailableException. This is because the first StopRelicaRequest will remove the mapping (tp -> ReplicaManager.OfflinePartition) from ReplicaManager.allPartitions before returning KafkaStorageException, thus the second StopRelicaRequest will not find this partition as offline.  This result appears to be inconsistent. And since the replica is already offline and broker will not be able to delete file for this replica, the StopReplicaRequest should fail without making any change and broker should still remember that this replica is offline.   2) Currently if broker receives StopReplicaRequest with delete=true, the broker will attempt to remove future replica for the partition, which will cause KafkaStorageException in the StopReplicaResponse if this replica does not have future replica. It is problematic to always return KafkaStorageException in the response if future replica does not exist.","closed","","lindong28","2018-08-20T07:06:26Z","2018-12-07T02:46:20Z"
"","5654","KAFKA-7414; Out of range errors should never be fatal for follower","This patch fixes the inconsistent handling of out of range errors in the replica fetcher. Previously we would raise a fatal error if the follower's offset is ahead of the leader's and unclean leader election is not enabled. The behavior was inconsistent depending on the message format. With KIP-101/KIP-279 and the new message format, upon becoming a follower, the replica would use leader epoch information to reconcile the end of the log with the leader and simply truncate. Additionally, with the old format, the check is not really bulletproof for detecting data loss since the unclean leader's end offset might have already caught up to the follower's offset at the time of its initial fetch or when it queries for the current log end offset.  With this patch, we simply skip the unclean leader election check and allow the needed truncation to occur. When the truncation offset is below the high watermark, a warning will be logged. This makes the behavior consistent for all message formats and removes a scenario in which an error on one partition can bring the broker down.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-14T18:07:47Z","2018-09-17T19:36:53Z"
"","5678","KAFKA-7415; Persist leader epoch and start offset on becoming a leader","This patch ensures that the leader epoch cache is updated when a broker becomes leader with the latest epoch and the log end offset as its starting offset. This guarantees that the leader will be able to provide the right truncation point even if the follower has data from leader epochs which the leader itself does not have. This situation can occur when there are back to back leader elections.  Additionally, we have made the following changes:  1. The leader epoch cache enforces monotonically increase epochs and starting offsets among its entry. Whenever a new entry is appended which violates requirement, we remove the conflicting entries from the cache. 2. Previously we returned an unknown epoch and offset if an epoch is queried which comes before the first entry in the cache. Now we return the smallest . For example, if the earliest entry in the cache is (epoch=5, startOffset=10), then a query for epoch 4 will return (epoch=4, endOffset=10). This ensures that followers (and consumers in KIP-320) can always determine where the correct starting point is for the active log range on the leader.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-21T23:31:52Z","2018-10-04T21:02:24Z"
"","6070","KAFKA-7773; Add end to end system test relying on verifiable consumer","This patch creates an `EndToEndTest` base class which relies on the verifiable consumer. This will ultimately replace `ProduceConsumeValidateTest` which depends on the console consumer. The advantage is that the verifiable consumer exposes more information to use for validation. It also allows for a nicer shutdown pattern. Rather than relying on the console consumer idle timeout, which requires a minimum wait time, we can halt consumption after we have reached the last acked offsets. This should be more reliable and faster. The downside is that the verifiable consumer only works with the new consumer, so we cannot yet convert the upgrade tests.  In this patch, I've converted only the replication tests and a flaky security test to use `EndToEndTest`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-12-28T00:53:44Z","2019-01-08T14:14:53Z"
"","5564","KAFKA-7333; Protocol changes for KIP-320","This patch contains the protocol updates needed for KIP-320 as well as some of the basic consumer APIs (e.g. `OffsetAndMetadata` and `ConsumerRecord`). The inter-broker format version has not been changed and the brokers will continue to use the current API versions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-23T20:13:10Z","2018-09-09T07:25:35Z"
"","5661","KAFKA-7395; Add fencing to replication protocol (KIP-320)","This patch contains the broker-side support for the fencing improvements from KIP-320. This includes the leader epoch validation in the ListOffsets, OffsetsForLeaderEpoch, and Fetch APIs as well as the changes needed in the fetcher threads to maintain and use the current leader epoch. The client changes from KIP-320 will be left for a follow-up.  One notable change worth mentioning is that we now require the read lock in `Partition` in order to read from the log or to query offsets. This is necessary to ensure the safety of the leader epoch validation. Additionally, we forward all leader epoch changes to the replica fetcher thread and go through the truncation phase. This is needed to ensure the fetcher always has the latest epoch and to guarantee that we cannot miss needed truncation if we missed an epoch change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-18T23:03:32Z","2018-10-05T20:25:09Z"
"","6187","KAFKA-7819: Improve RoundTripWorker","This patch changes the Trogdor RoundTripWorker to use a `long` field for maxMessages and makes the consumer group used unique","closed","","stanislavkozlovski","2019-01-23T14:47:10Z","2019-03-21T17:03:10Z"
"","6478","MINOR: Allow specifying auto.offset.reset in the ConsumeBenchSpec for Trogdor","This patch allows specifying `auto.offset.reset` for the Trogdor consumers, allowing benchmarks that start reading at the beginning of the log.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2019-03-20T20:27:29Z","2019-05-07T22:12:09Z"
"","6302","MINOR: Improve logging for alter log dirs","This patch adds several new log messages to provide more information about errors during log dir movement and to make it clear when each partition movement is finished.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-02-21T18:51:32Z","2019-03-04T07:46:37Z"
"","5812","MINOR: Ensure initial topic configs and updates are logged","This patch adds logging of topic config overrides during creation or during the handling of alter config requests. Also did some minor cleanup to avoid redundant validation logic when adding partitions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-17T20:20:02Z","2018-10-19T15:49:12Z"
"","6348","KAFKA-5511 ConfigKeyBuilder for ConfigDef","This patch adds ConfigKeyBuilder in order to improve readability in the future configuration definitions and avoid some parameters order related bugs.  Using the builder inside the all arguments `define` method we can reuse all previous written tests, which cover all the changes, in addition there are other unit tests, which match the `ConfigKey` properties.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pierDipi","2019-03-01T00:34:43Z","2020-06-29T06:28:05Z"
"","6385","MINOR: Improve logging around index files.","This patch adds additional DEBUG statements in AbstractIndex.scala, OffsetIndex.scala, and TimeIndex.scala. It also changes the logging on append from DEBUG to TRACE to make DEBUG logging less disruptive.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-03-07T01:01:09Z","2019-03-19T06:00:02Z"
"","6444","KAFKA-8102: Add an interval-based Trogdor transaction generator","This patch adds a TimeIntervalTransactionsGenerator class which enables the Trogdor ProduceBench worker to commit transactions based on a configurable millisecond time interval.  It also improves the Coordnator client's handling of a 409 response on CreateTask  Worth saying I tested these changes manually","closed","","stanislavkozlovski","2019-03-14T11:02:59Z","2019-03-25T16:58:12Z"
"","6216","Add some props to the log4j kafka appender to control security configs","This patch adds 2 props to the log4j kafka appender that get put directly into the sasl properties passed to the producer:     - ClientJaasConf: This property sets sasl.jaas.config     - SaslMechanim: This property sets sasl.mechanism  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2019-02-01T00:39:05Z","2019-02-12T04:05:42Z"
"","5573","Correct folder for package object scala","This package was not in the right folder compare to the package statement at the top of the file","closed","","joan38","2018-08-26T15:16:01Z","2018-08-28T09:35:15Z"
"","6384","KAFKA-8058: Fix ConnectClusterStateImpl.connectors() method","This makes the `ConnectClusterStateImpl.connectors()` method synchronous, whereas before it was implicitly asynchronous with no way to tell whether it had completed or not.  More detail can be found in the [Jira](https://issues.apache.org/jira/browse/KAFKA-8058).  ~Tested manually. Seems like a light enough change that even unit tests would be overkill, but if reviewers feel differently tests can be added.~ A unit test has also been added for the `ConnectClusterStateImpl` class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-03-07T00:22:50Z","2021-04-28T19:41:09Z"
"","5948","kafkatest: Make ZooKeeper log in ""DEBUG"" mode by default","This makes it consistent to Kafka's logs and will help debugging failing tests, especially in CI/CD environments","open","","stanislavkozlovski","2018-11-26T14:09:44Z","2019-01-09T14:12:36Z"
"","5619","Add a scala-friendlier KStream.transform() variant","This KStream.transform() variant restores the old capability of using scala-style Transformers that return (key, value) tuple instead of returning KeyValue instances.  This capability was present in the old Lightbend's kafka-streams-scala package and it seems that 2.0.0 API intended to have this capability, but it didn't work because of KAKFA-7250  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","redvasily","2018-09-07T09:15:35Z","2018-09-18T22:42:58Z"
"","5611","KAFKA-7321: Add a Maximum Log Compaction Lag (KIP-354)","This is to implement KIP-354. More detailed information can be found in KIP-354  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xiowu0","2018-09-04T22:00:15Z","2018-12-06T22:14:53Z"
"","5989","KAFKA-7693: Fix SequenceNumber overflow in client TransactionManager.","This is to fix KAFKA-7693. The bug is found during operating the Kafka cluster.  The problem is SequenceNumber is Int and should wrap around when it hit the int.MaxValue. The bug here is it doesn't wrap around and become negative and throw the Exception mentioned in the JIRA-7693.  Verified in the unitest and our daily testing environment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mingaliu","2018-12-03T05:27:35Z","2019-01-25T00:45:09Z"
"","5990","KAFKA-7692: Fix ProduceStateManager SequenceNumber overflow.","This is to fix Kafka 7692. The problem is at ProducerStateManager:Append(),  it has those lines: updatedEntry.addBatch(epoch, lastSeq, lastOffset, lastSeq - firstSeq, lastTimestamp) and  val firstOffset = lastOffset - (lastSeq - firstSeq) However, lastSeq can wrap around and smaller than firstSeq. This will cause some exception later on (described in the JIRA 7692)  Test: Verified with local unitest and in our daily operation.","closed","","mingaliu","2018-12-03T05:48:08Z","2019-01-25T00:28:02Z"
"","5591","KAFKA-7322: Fix race condition between log cleaner thread and log retention thread when topic cleanup policy is updated","This is to address issue described in KAFKA-7322.  To fix race condition between log retention and log cleaner when dynamically switching topic cleanup policy,  existing log cleaner inprogress map is used to prevent more than one thread working on the same topic partition.","closed","","xiowu0","2018-08-29T23:17:18Z","2018-09-23T04:32:40Z"
"","5494","[KAFKA-6923] Refactoring Serializer/Deserializer","This is the implementation of KIP-336.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-08-13T09:14:27Z","2018-09-23T08:11:54Z"
"","6177","KAFKA-7862 & KIP-345 part-1: Add static membership logic to JoinGroup protocol","This is the first diff for the implementation of JoinGroup logic for static membership. The goal of this diff contains:  1. Add `group.instance.id` to be unique identifier for consumer instances, provided by end user; 2. Modify group coordinator to accept JoinGroupRequest with/without static membership, refactor the logic for readability and code reusability. 3. Add client side support for incorporating static membership changes, including new config for `group.instance.id`, apply stream thread client id by default, and new join group exception handling. 4. Remove `internal.leave.on.close` config by checking whether `group.instance.id` is defined. Effectively speaking, only dynamic member will send LeaveGroupRequest while static membership expiration is only controlled through session timeout. 5. Increase max session timeout to 30 min for more user flexibility if they are inclined to tolerate partial unavailability than burdening rebalance. 6. Unit tests for each module changes, especially on the group coordinator logic. Crossing the possibilities like:   6.1 Dynamic/Static member   6.2 Known/Unknown member id   6.3 Group stable/unstable   6.4 Leader/Follower  The hope here is to merge this logic before 2.2 code freeze so that we (as Pinterest) could start experimenting on the core logic ASAP.  The rest of the 345 change will be broken down to 4 separate diffs:  1. Avoid kicking out members through rebalance.timeout, only do the kick out through session timeout. 2. Changes around LeaveGroup logic, including version bumping, broker logic, client logic, etc. 3. Admin client changes to add ability to batch remove static members 4. Deprecate group.initial.rebalance.delay  Let me know your thoughts @guozhangwang @hachikuji @stanislavkozlovski @MayureshGharat @kkonstantine @lindong28 @Ishiihara @shawnsnguyen , thanks!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-01-20T05:43:06Z","2019-05-13T06:41:57Z"
"","6015","MINOR: Fix failing test in ConsumeBenchTest:test_multiple_consumers_specifie…","This is the error message we're after: https://github.com/apache/kafka/blob/c05050346468cc27bcfb3a43bde90c47533a386c/tools/src/main/java/org/apache/kafka/trogdor/workload/ConsumeBenchWorker.java#L120  We apparently changed it midway through #5810 and forgot to update the test","closed","","stanislavkozlovski","2018-12-07T21:04:58Z","2018-12-08T17:28:00Z"
"","5724","KAFKA-7223: Make suppression buffer durable","This is Part 4 of suppression (durability) Part 1 was #5567 (the API) Part 2 was #5687 (the tests) Part 3 was #5693 (in-memory buffering)  Implement a changelog for the suppression buffer so that the buffer state may be recovered on restart or recovery. As of this PR, suppression is suitable for general usage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2018-10-02T06:23:47Z","2020-06-12T23:57:47Z"
"","5693","KAFKA-7223: In-Memory Suppression Buffering","This is Part 3 of suppression. Part 1 was #5567 (the API) Part 2 was #5687 (the tests)  Implement a non-durable in-memory buffering strategy for suppression. As of this changeset, the suppression API is fully functional.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2018-09-25T20:45:56Z","2020-06-12T23:58:46Z"
"","5687","KAFKA-7223: add tests in preparation for suppression","This is Part 2 of suppression.  Part 1 was https://github.com/apache/kafka/pull/5567  In an effort to control the scope of the review, this PR is just the tests for buffered suppression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-09-24T20:53:39Z","2018-09-25T23:05:12Z"
"","5842","KAFKA-7551:Refactor to create producer & consumer in the worker.","This is minor refactoring that brings in the creation of producer and consumer to the Worker. Currently, the consumer is created in the WorkerSinkTask. This should not affect any functionality and it just makes the code structure easier to understand.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-10-25T16:59:49Z","2020-10-16T06:21:20Z"
"","5594","MINOR: KafkaAdminClient code cleanup","This is just some minor cleanup of KafkaAdminClient, in particular Java 8 compatible code simplification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-08-31T13:14:24Z","2018-09-13T04:13:03Z"
"","5909","KAFKA-6958: Allow to define custom processor names with KStreams DSL","This is a WIP for the KIP :  https://cwiki.apache.org/confluence/display/KAFKA/KIP-307%3A+Allow+to+define+custom+processor+names+with+KStreams+DSL","closed","","fhussonnois","2018-11-12T22:40:53Z","2019-08-08T00:08:14Z"
"","5913","MINOR: Adding system test for named repartition topics","This is a system test for doing a rolling upgrade of a topology with a named repartition topic.  1. An initial Kafka Streams application is started on 3 nodes.  The topology has one operation forcing a repartition and the repartition topic is explicitly named. 2. Each node is started and processing of data is validated 3. Then one node is stopped (full stop is verified) 4. A property is set signaling the node to add operations to the topology _**before**_ the repartition node which forces a renumbering of all operators (except repartition node) 5. Restart the node and confirm processing records 6. Repeat the steps for the other 2 nodes completing the rolling upgrade   I ran two runs of the system test with 25 repeats in each run for a total of 50 test runs. All test runs passed  ### [First 25 test runs](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-11-13--001.1542119003--bbejeck--MINOR_create_system_test_for_named_repartition_topics--9bb2b95/report.html) ### [Second 25 test runs](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-11-13--001.1542123381--bbejeck--MINOR_create_system_test_for_named_repartition_topics--9bb2b95/report.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-11-14T16:05:35Z","2018-12-03T20:37:32Z"
"","5906","KAFKA-7617: Add authorization primitives to security page","This is a security page improvement that adds documentation about Kafka authorization primitives to the security page.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-11-12T13:54:07Z","2018-12-04T09:14:40Z"
"","5614","MINOR: Enable ignored upgrade system tests 2.0","This is a port of https://github.com/apache/kafka/pull/5605 for the 2.0 branch   Passing system test on branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1956/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-05T14:12:41Z","2018-09-06T22:41:17Z"
"","5613","MINOR: Update streams upgrade system tests 0.11.0.3","This is a port of https://github.com/apache/kafka/pull/5605 for the 11.3 branch  Passing system test on branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1953/  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-05T13:46:52Z","2018-09-06T20:06:36Z"
"","5609","MINOR:Update versions for streams upgrade tests 1.1","This is a port of https://github.com/apache/kafka/pull/5605 for the 1.1 branch    Passing system test on branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1950/ and   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-04T19:32:26Z","2018-09-10T21:24:04Z"
"","5933","DO NOT MERGE - KAFKA-1194 PARTIAL bug fix","This is a partial fix to the retention mechanism on Windows. It makes sure to close a segment file handles prior to deletion trial. Why is it partial? As it cannot work together will the log compaction policy.   Should work with a similar to the following configuration:  log.segment.bytes=1048576 log.retention.bytes= 5485760 log.retention.minutes = 1 log.retention.check.interval.minutes = 1 log.cleanup.policy = delete log.cleaner.enable = false # log.roll.ms = 10000 <= Can be specified but will not cause any failure without the above two lines of configuration (log.cleanup.policy, log.cleaner.enable)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kobihikri","2018-11-20T16:36:27Z","2018-11-26T12:29:03Z"
"","5912","MINOR: Add system test for optimization upgrades","This is a new system test testing for optimizing an existing topology.  This test takes the following steps  1. Start a Kafka Streams application that uses a `selectKey` then performs 3 `groupByKey()` operations and 1 `join` creating four repartition topics 2. Verify all instances start and process data 3. Stop all instances and verify stopped 4. For each stopped instance update the config for  `TOPOLOGY_OPTIMIZATION` to `all` then restart the instance and verify the instance has started successfully also verifying Kafka Streams reduced the number of repartition topics from 4 to 1 5. Verify that each instance is processing data from the `aggregation`, `reduce`, and `join` operation 6. Stop all instances and verify the shut down is complete.  For testing I ran two passes of the system test with 25 repeats for a total of 50 test runs.    All test runs passed ### [First 25 system test runs](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-11-14--001.1542165875--bbejeck--MINOR_create_system_test_for_rolling_upgrade_with_optimization--d161bf8/report.html) ### [Second 25 system test runs](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-11-14--001.1542170911--bbejeck--MINOR_create_system_test_for_rolling_upgrade_with_optimization--d161bf8/report.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-11-14T15:04:18Z","2018-11-27T21:07:35Z"
"","6017","KAFKA-6036: Follow-up; cleanup sendOldValues logic ForwardingCacheFlushListener","This is a follow-up PR from the previous PR https://github.com/apache/kafka/pull/5779, where KTabeSource always get old values from the store even if sendOldValues. It gets me to make a pass over all the KTable/KStreamXXX processor to push the `sendOldValues` at the callers in order to avoid unnecessary store reads.   1. More details: `ForwardingCacheFlushListener` and `TupleForwarder` both need sendOldValues as parameters.  a. For `ForwardingCacheFlushListener` it is not needed at all, since its callers `XXXCachedStore` already use the sendOldValues values passed from `TupleForwarder` to avoid getting old values from underlying stores.  b. For `TupleForwarder`, it actually only need to pass the boolean flag to the cached store; and then it does not need to keep it as its own variable since the cached store already respects the boolean to pass null or the actual value..  2. The only other minor bug I found from the pass in on KTableJoinMerge, where we always pass old values and ignores sendOldValues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-12-09T08:09:25Z","2018-12-09T23:33:18Z"
"","5549","MINOR: only split at first occurrence of '='","This is a fix to #5226 to account for config properties that have an equal char in the value.   Otherwise if there is one equal char in the value the following error occurs: ``` dictionary update sequence element #XX has length 3; 2 is required ```    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-08-21T20:15:26Z","2018-08-22T13:15:18Z"
"","6174","KAFKA-7502: Cleanup KTable materialization logic in a single place","This is a draft cleanup for KAFKA-7502. Here is the details:  1. Make `KTableKTableJoinNode` abstract, and define its child classes (`[NonMaterialized,Materialized]KTableKTableJoinNode`) instead: now, all materialization-related routines are separated into the other classes. 2. `KTableKTableJoinNodeBuilder#build` now instantiates `[NonMaterialized,Materialized]KTableKTableJoinNode` classes instead of `KTableKTableJoinNode`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2019-01-19T11:01:21Z","2019-03-15T01:42:08Z"
"","6204","KAFKA-3522: Replace RecordConverter with TimestampedBytesStore","This is a change to KIP-258 as announced on the mailing list.  - replace public interface `RecordConverter` with `TimestampedBytesStore` - extract value oldFormat -> newFormat conversion into static method - adds a new internal `RecordConverter` that uses new static method","closed","kip,","mjsax","2019-01-26T10:01:17Z","2020-06-12T23:52:17Z"
"","5596","KAFKA-7370: Enhance FileConfigProvider to read a dir","This is a backward compatible enhancement to augment FileConfigProvider to read from a directory, where the file names are the keys and the corresponding file contents are the values.  The former functionality of reading a Properties file is retained.  This will allow for easier integration with secret management systems where each secret is often written to an individual file, such as in some Docker and Kubernetes setups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-08-31T23:42:43Z","2018-09-17T22:35:24Z"
"","6122","KAFKA-7040: Ignore OffsetsForLeaderEpoch response if leader epoch changed while request in flight","This is a ""backport"" of https://github.com/apache/kafka/pull/6101 + relevant parts of https://github.com/apache/kafka/pull/5661 for 2.0 branch.  cc @hachikuji   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","apovzner","2019-01-11T00:48:28Z","2019-11-21T21:13:04Z"
"","6083","MINOR: Upgrade to Gradle 4.10.3","This includes a fix for an issue that required manual intervention during the 2.1.0 release: the unexpected existence of a .mapping file in the Maven repository.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-01-02T15:40:59Z","2019-01-03T02:07:47Z"
"","6326","MINOR: Increase produce timeout to 120 seconds","This gives more room to pass this test on systems with low resources running many parallel tests.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2019-02-25T22:48:37Z","2020-10-16T06:21:24Z"
"","5602","MINOR: Use annotationProcessor instead of compile for JMH annotation processor","This fixes the following Gradle warning:  > The following annotation processors were detected on the compile classpath: > 'org.openjdk.jmh.generators.BenchmarkProcessor'. Detecting annotation processors > on the compile classpath is deprecated and Gradle 5.0 will ignore them. Please add > them to the annotation processor path instead. If you did not intend to use annotation > processors, you can use the '-proc:none' compiler argument to ignore them.  With this change, the warning went away and `./jmh-benchmarks/jmh.sh` continues to work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-03T05:02:25Z","2018-09-11T06:46:29Z"
"","5883","KAFKA-7559: Correct standalone system tests to use the correct external file","This fixes the Connect standalone system tests. See branch builder: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2021/  **_This should be backported to the `2.0` branch_**, since that's when the tests were first modified to use the external property file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-11-05T18:52:31Z","2018-11-06T06:40:26Z"
"","6334","KAFKA-7990: Close streams at the end in KafkaStreamsTest","This fix is already in 2.1+ branches, but did not get into older branches.  Should be cherry-picked all the way to 0.10.2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-02-27T01:39:47Z","2019-02-27T06:29:46Z"
"","6115","KAFKA-7672 : force write checkpoint during StreamTask #suspend","This fix is aiming for #2 issue pointed out within https://issues.apache.org/jira/browse/KAFKA-7672 In the current setup, we do offset checkpoint file write when EOS is turned on during #suspend, which introduces the potential race condition during StateManager #closeSuspend call. To mitigate the problem, we attempt to always write checkpoint file in #suspend call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-01-10T04:08:00Z","2019-02-23T06:50:47Z"
"","5599","MINOR: Pass `--continue` to gradle in jenkins.sh","This ensures that the whole test suite is executed even if there are failures. It currently stops at a module boundary if there are any failures. There is a discussion to change the gradle default to stop after the first test failure:  https://github.com/gradle/gradle/issues/6513  `--continue` is recommended for CI in that discussion.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-01T16:16:04Z","2018-09-11T06:46:35Z"
"","5662","MINOR: Upgrade to Jackson 2.9.7","This contains important fixes:  https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.9.7  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-19T05:36:29Z","2018-09-25T08:20:41Z"
"","6262","KAFKA-7921: log at error level for missing source topic","This condition is a fatal error, so error level is warranted, to provide more context on why Streams shuts down.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-12T20:54:25Z","2019-02-13T20:10:57Z"
"","5924","MINOR: Update docs with out-dated context.schedule(...) examples","This commit updates the first parameter of example calls to context.schedule(...) in documentations from time durations in milliseconds of type long to time durations expressed as class java.time.Duration.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2018-11-17T22:30:29Z","2019-10-21T10:53:03Z"
"","6365","MINOR: Improve API docs of (flatT|t)ransform","This commit is a follow-up of pull request #5273.  - Adds and reformulates some parts - Removes parameters from links to methods in the javadocs   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-03-04T10:39:27Z","2019-10-21T10:53:08Z"
"","6104","KAFKA-7795 Produce a WARN log when duplicate config keys are found","This change simply adds some formatted error logging if duplicate keys are found when parsing a Kafka config file. Since we use Properties (which blindly accepts duplicates), the validation must happen when the Properties instance is loaded from a file.  No behavior changes here, just extra logging.  If I edit the broker config like:  ```diff diff --git a/config/server.properties b/config/server.properties index 898d8ebc28..593651fd33 100644 --- a/config/server.properties +++ b/config/server.properties @@ -133,4 +133,8 @@ zookeeper.connection.timeout.ms=6000  # The default value for this is 3 seconds.  # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.  # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. -group.initial.rebalance.delay.ms=0 \ No newline at end of file +group.initial.rebalance.delay.ms=0 + +zookeeper.connection.timeout.ms=7000 +zookeeper.connection.timeout.ms=8000 ```  It will produce the following error output at the top of the log: ``` [2019-01-10 10:54:28,679] WARN Duplicate config entries detected: 	Overwriting zookeeper.connection.timeout.ms = 6000 with 8000 	Overwriting zookeeper.connection.timeout.ms = 7000 with 8000  (org.apache.kafka.common.utils.Utils) ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mumrah","2019-01-08T20:30:38Z","2019-01-22T15:17:17Z"
"","5958","KAFKA-2607: Review Time interface and its usage","This change renames methods in Time interface to make apparent which underlying implementation is used in each case","open","","Al12","2018-11-28T15:01:45Z","2018-11-29T07:50:37Z"
"","6398","KAFKA-8037: Check deserialization at global state store restoration","This change aims to prevent records that cannot be deserialized from going into a global state store during restore. Currently such records are filtered during normal operations but will be processed during restore and will cause an exception when trying to access the value in the store. The change copies the logic from the GlobalStateUpdateTask and builds a list of deserializers to use during restoration.  The GlobalStateManagerImplTest was extended to cover the case that a StreamsException is expected when a record is processed that can't be deserialized with the default LogAndFailExceptionHandler. GlobalStateManagerImplLogAndContinueTest was added and contains one new test which uses the LogAndContinueExceptionHandler and verifies that a record can be ignored during restoration. Copying all tests is not ideal, but I found no easy way to override the DefaultExceptionHandler just for the one case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","pkleindl","2019-03-07T23:21:21Z","2020-01-09T22:16:46Z"
"","6394","KAFKA-8037: Check deserialization at global state store restoration","This change aims to prevent records that cannot be deserialized from going into a global state store during restore. Currently such records are filtered during normal operations but will be processed during restore and will cause an exception when trying to access the value in the store. The change copies the logic from the GlobalStateUpdateTask and builds a list of deserializers to use during restoration.  The GlobalStateManagerImplTest was extended to cover the case that a StreamsException is expected when a record is processed that can't be deserialized with the default LogAndFailExceptionHandler. GlobalStateManagerImplLogAndContinueTest was added and contains one new test which uses the LogAndContinueExceptionHandler and verifies that a record can be ignored during restoration. Copying all tests is not ideal, but I found no easy way to override the DefaultExceptionHandler just for the one case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pkleindl","2019-03-07T21:40:21Z","2019-03-07T23:18:40Z"
"","6289","KAFKA-7948: Feature to enable json field order retention in the JsonConverter","This change affects the JsonConverter.  Some json parsers have particular requirements for field order in a json message. While this is not a part of the json spec and shouldn't really be necessary it is a reality for parsers we have on our mainframe. I have made this a configuration setting with a default of 'none' to retain the current functionality as is while giving users the option of enabling field order retention.  3 new test methods have been added to verify the behaviour of the setting is as expected when not set, set to 'none' and set to 'retained'.   To engage the behaviour use: `json.field.order=retained`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","joncourt","2019-02-19T04:41:59Z","2019-02-20T07:43:21Z"
"","5660","KAFKA-4544: Add system tests for delegation token based authentication","This change adds some basic system tests for delegation token based authentication: - basic delegation token creation - producing with a delegation token - consuming with a delegation token - expiring a delegation token  - producing with an expired delegation token  New files: - delegation_tokens.py: a wrapper around kafka-delegation-tokens.sh  - executed in container where a secure Broker is running (taking advantage of automatic cleanup) - delegation_tokens_test.py: basic test to validate the lifecycle of a delegation token  Changes were made in the following file to extend their functionality: - config_property was updated to be able to configure Kafka brokers with delegation token related settings - jaas.conf template because a broker needs to support multiple login modules when delegation tokens are used - consule-consumer and verifiable_producer to override KAFKA_OPTS (to specify custom jaas.conf) and the client properties (to authenticate with delegation token).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-09-18T14:42:41Z","2018-12-03T06:00:49Z"
"","5598","MINOR: Introduce getPartitionAndReplicaOrException and use it in fetcher threads","This avoids the need to get `Option.get` and makes it clearer that the Replica and Partition associated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-01T16:04:24Z","2020-01-29T09:04:08Z"
"","5873","KAFKA-5054 : synchronized all the methods","This avoids inconsistent results while querying or storing the underlying storage","closed","streams,","lijubjohn","2018-11-02T09:45:23Z","2018-11-17T21:34:08Z"
"","6306","KAFKA-7938: Fix test flakiness in DeleteConsumerGroupsTest","This attempts to fix KAFKA-7938 and KAFKA-7946.   I removed two tests: - testDeleteWithShortInitialization basically didn't check the result and therefore always passed - testDeleteCmdWithShortInitialization has no way to enforce that initialization is indeed short, and therefore sometimes FAILED because the group would be created before the CMD tried to delete it.  I thought the tests had limited value relative to the effort of figuring out a way to make the timing work.  I also fixed testDeleteCmdNonEmptyGroup and testDeleteNonEmptyGroup so they will validate that the group both exists and is non-empty before starting the test itself. I also added some extra information for future debugging sessions :)  I ran the tests LOTS of times to validate, but with flaky tests, it is hard to tell :)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gwenshap","2019-02-22T02:52:38Z","2019-02-22T20:50:30Z"
"","5965","MINOR: Specify character encoding in NetworkTestUtils","This attempts to address the flaky test `SaslAuthenticatorTest.testCannotReauthenticateWithDifferentPrincipal()`   I was not able to reproduce locally even after 150 test runs in a loop, but given the error message: ``` org.junit.ComparisonFailure: expected: <[6QBJiMZ6o5AqbNAjDTDjWtQSa4alfuUWsYKIy2tt7dz5heDaWZlz21yr8Gl4uEJkQABQXeEL0UebdpufDb5k8SvReSK6wYwQ9huP-9]> but was:<[????ï¿½ï¿½ï¿½ï¿½????OAUTHBEARER]> ```  `????ï¿½ï¿½ï¿½ï¿½????` seems to mean invalid UTF-8.  This patch ensures that we specify a charset when writing out and reading in bytes. I'm not sure this fixes the problem but it seems likely.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2018-11-29T02:23:03Z","2018-12-08T17:36:15Z"
"","6225","MINOR: Add snap packaging.","This adds snap packaging (https://snapcraft.io) for Kafka.  [If this is considered more than a minor patch, I'm happy to open a JIRA ticket or post to the ml to discuss further, please advise. This is also my first PR into Kafka.. if packaging like this belongs elsewhere, please let me know where I should propose it.]  Strict confinement is used; all processes are run in an apparmor-restricted environment with access to the network and removable-media on the host. removable-media allows the use of mounted volumes for the log directory.  Kafka and zookeeper are installed as systemd services for a nice standalone, out-of-the-box, no-configuration-necessary experience. You install the snap and kafka is running on localhost. The zookeeper service can be disabled if an external zookeeper is used instead.  The snap build requires snapcraft 3.0 and multipass. So far, I've tested on Ubuntu 18.04.  Testing at a minimum would involve scripting an install of the snap on a VM and a smoke test, perhaps creating topics and running console producer/consumer.","closed","","cmars","2019-02-02T04:39:36Z","2019-09-10T17:11:28Z"
"","5537","KAFKA-5891: Adapts #4633 with schema tests","This adapts PR #4633 from @maver1ck with suggested unit test from @rhauch  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-08-20T21:56:41Z","2018-08-21T15:21:47Z"
"","6328","KAFKA-7918: Inline generic parameters Pt. III: in-memory window store","Third (and final) PR in series to inline the generic parameters of the following bytes stores:  [Pt. I] InMemoryKeyValueStore [Pt. II] RocksDBWindowStore [Pt. II] RocksDBSessionStore [Pt. II] MemoryLRUCache [Pt. II] MemoryNavigableLRUCache [x] InMemoryWindowStore  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-02-26T03:53:25Z","2019-03-01T01:39:48Z"
"","5510","MINOR: (re)add equals/hashCode to *Windows","These were removed in b3771ba22acad7870e38ff7f58820c5b50946787 because they were incorrect and believed unused.  In retrospect, they are in the public interface, so it might be better to just provide correct implementations.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-08-15T18:32:55Z","2018-08-20T23:34:45Z"
"","5644","MINOR: fix scala serde tests","These two commits are mutually incompatible, but they were both tested before either was merged, so the first time they were tested together was in trunk. https://github.com/apache/kafka/commit/9dac615d228c5b3464c6322aea9f9ce70f9ef37b https://github.com/apache/kafka/commit/acd3858ea69e676a7840d998240deb32aee62dc0  This commit fixes the build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-13T15:04:57Z","2018-09-13T15:30:37Z"
"","5986","KAFKA-6388: Recover from rolling an empty segment that already exists","There were several reported incidents where the log is rolled to a new segment with the same base offset as an active segment, causing KafkaException: Trying to roll a new log segment for topic partition X-N with start offset M while it already exists. From what I have investigated so far, this happens to an empty log segment where there is long idle time before the next append and somehow we get to a state where offsetIndex.isFull() returns true due to _maxEntries == 0. This PR recovers from the state where the active segment is empty and we try to roll to a new segment with the same offset: we delete segment and recreate it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-12-01T03:32:26Z","2018-12-05T22:49:19Z"
"","5903","MINOR: Remove unused IteratorTemplate","There seems to be no reason to keep this around since it is not used outside of testing and `AbstractIterator` is basically the same thing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-11-12T06:57:51Z","2018-11-12T21:42:29Z"
"","6377","KAFKA-8038 - Fix timing issue in SslTransportLayerTest.testCloseSsl","There is tiny timing window in the test where server has not marked the channel ready, but client has. Wait for server to mark the channel ready.  I wasn't able to recreate the failure with or without the fix. The fix is based on the stack trace in the JIRA.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-06T10:07:02Z","2019-03-06T17:33:26Z"
"","5702","KAFKA-7444: expose connector and task ID to SinkTasks","There is currently no good way for a SinkTask to use SinkUtils.consumerGroupId(connectorId), as there is no good way for a SinkTask to know it's connector ID. I've exposed the task ID as well. Currently, task ID is exposed via toString() methods and TaskState.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ryannedolan","2018-09-26T22:39:59Z","2021-06-16T17:01:15Z"
"","6418","KAFKA-7939: Fix timing issue in KafkaAdminClientTest.testCreateTopicsRetryBackoff","There is a small timing window where ```time.sleep(retryBackoff)``` will get executed before adminClient adds retry request to the queue.  Added a check to wait until the retry call added to the queue in AdminClient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-10T16:44:12Z","2019-03-11T14:21:05Z"
"","6101","KAFKA-7786: Ignore OffsetsForLeaderEpoch response if leader epoch changed while request in flight","There is a race condition in ReplicaFetcherThread, where we can update PartitionFetchState with the new leader epoch (same leader) before handling the OffsetsForLeaderEpoch response with FENCED_LEADER_EPOCH error which causes removing partition from partitionStates, which in turn causes no fetching until the next LeaderAndIsr.   Our system test kafkatest.tests.core.reassign_partitions_test.ReassignPartitionsTest.test_reassign_partitions.bounce_brokers=True.reassign_from_offset_zero=True failed 3 times due to this error in the last couple of months. Since this test is already able to test this condition, not adding any more tests.  Also added toString() implementation to PartitionData, because some log messages did not show useful info which I found while investigating the above system test failure.  cc @hachikuji who suggested the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2019-01-07T22:00:41Z","2019-01-09T19:09:49Z"
"","5766","MINOR: increase timeout for broker startup in 0.10.2 system tests","There has been some test flakiness in which this line times out just before the broker comes online.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-09T20:10:37Z","2018-11-05T20:51:59Z"
"","6078","MINOR: Fix some logging statements which can not correctly record the behaviors","There are some possible copy and paste errors in the log messages (The logging statement was copied from an old place to a new place, but the message wasn't changed to adapt to the function of the new place). So I modified some logging statements to make them record the run-time behaviors more accurately.","closed","connect,","ginolzh","2018-12-31T03:05:34Z","2019-03-04T23:57:45Z"
"","6063","MINOR: Flaky Optimization test due repartition topic creation failures","The topology optimization test was getting intermittent failures because of failures to create repartition topics on startup.  This PR Increased admin client retries   I kicked off the system test with 25 repeats, all passed http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-12-21--001.1545436859--bbejeck--MINOR_flaky_optimization_test_create_repartition_fails--6cd55e2/report.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-22T01:38:17Z","2018-12-22T06:43:56Z"
"","6288","KAFKA-7945: Calc refresh time correctly when token created in the past","The token refresh point could potentially be calculated incorrectly when the token was created far enough in the past, possibly raising an exception and disabling the client.  Typically this is not the case -- when the client invokes ""login"" on the JAAS LoginModule the token that is returned is not a token that has been ""sitting on the shelf"" for a while; it is generally a token that has just been created, so its start time is very close to ""now"".  The bug potentially exists for the case where the token has a start time far enough in the past such that the 80% point from that start time to the expiration time ends up being before the current time.  Even then, though, the minimum refresh time will generally kick in and cause the token to be refreshed after that minimum time.  The bug does exist for the special case where not only does the 80% point end up being calculated to be in the past, but also the remaining lifetime (the time between ""now"" and the expiration time) is small enough such that the minimum refresh time and the minimum buffer time at the end cannot both be accommodated.  For example, if the minimum refresh time is 1 minute and the minimum buffer time at the end is 5 minutes, but the token expires in less than 6 minutes -- then the bug condition can occur if the token start time is in the past.  Signed-off-by: Ron Dagostino","closed","","rondagostino","2019-02-19T03:22:09Z","2019-02-20T21:36:38Z"
"","5830","MINOR: Reduce ResetConsumerGroupOffsetTest flakiness","The tests in `kafka.admin.ResetConsumerGroupOffsetTest` sometimes fail with ``` java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available. ```  I think that it would be useful to have some retry mechanism in place to hop over transient errors","closed","","stanislavkozlovski","2018-10-23T14:37:34Z","2019-08-28T10:41:54Z"
"","6106","KAFKA-7799; Fix flaky test RestServerTest.testCORSEnabled","The test fails 100% of the time if `testOptionsDoesNotIncludeWadlOutput` is executed before `testCORSEnabled`. It seems the problem is the use of the system property. Perhaps there is some static caching somewhere.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","hachikuji","2019-01-09T01:40:44Z","2020-10-16T06:21:22Z"
"","5748","KAFKA-7484: fix suppression integration tests","The test data is based at time 0 (aka 1970), but the broker was started at time `now`. This caused the log cleaner to delete the test data if the tests took slightly too long, despite its generous default configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-10-05T17:12:33Z","2018-10-05T21:28:40Z"
"","6276","KAFKA-7799; Use httpcomponents-client in RestServerTest.","The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).  Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`. `sun.net.www.protocol.http.HttpURLConnection` drops restricted headers(`Host`, `Keep-Alive`, `Origin`, etc) based on static property `allowRestrictedHeaders`. This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.  So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`). To prevent this, we have to either make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible or do not rely on this system property at all.  This PR adds test dependency on `httpcomponents-client` which doesn't depend on `sun.net.http.allowRestrictedHeaders` system property. Thus none of existing tests should interfere with `RestServerTest`. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","avocader","2019-02-15T18:33:54Z","2019-03-04T12:34:37Z"
"","6277","KAFKA-7799; Use httpcomponents-client in RestServerTest.","The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).  Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`. `sun.net.www.protocol.http.HttpURLConnection` drops restricted headers(`Host`, `Keep-Alive`, `Origin`, etc) based on static property `allowRestrictedHeaders`. This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.  So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`). To prevent this, we have to either make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible or do not rely on this system property at all.  This PR adds test dependency on `httpcomponents-client` which doesn't depend on `sun.net.http.allowRestrictedHeaders` system property. Thus none of existing tests should interfere with `RestServerTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","avocader","2019-02-15T18:56:15Z","2019-03-04T12:34:21Z"
"","6236","KAFKA-7799: Use httpcomponents-client in RestServerTest.","The test `org.apache.kafka.connect.runtime.rest.RestServerTest#testCORSEnabled` assumes Jersey client can send restricted HTTP headers(`Origin`).  Jersey client uses `sun.net.www.protocol.http.HttpURLConnection`. `sun.net.www.protocol.http.HttpURLConnection` drops restricted headers(`Host`, `Keep-Alive`, `Origin`, etc) based on static property `allowRestrictedHeaders`. This property is initialized in a static block by reading Java system property `sun.net.http.allowRestrictedHeaders`.  So, if classloader loads `HttpURLConnection` before we set `sun.net.http.allowRestrictedHeaders=true`, then all subsequent changes of this system property won't take any effect(which happens if `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest` is executed before `RestServerTest`). To prevent this, we have to either make sure we set `sun.net.http.allowRestrictedHeaders=true` as early as possible or do not rely on this system property at all.  This PR adds test dependency on `httpcomponents-client` which doesn't depend on `sun.net.http.allowRestrictedHeaders` system property. Thus none of existing tests should interfere with `RestServerTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","avocader","2019-02-07T05:57:27Z","2019-02-15T00:00:20Z"
"","5485","KAFKA-7266: Fix MetricsTest.testMetrics flakiness by increasing batch…","The test `kafka.api.MetricsTest.testMetrics` has been failing intermittently in kafka builds. The particular failure is in the `MessageConversionsTimeMs` metric assertion: ``` java.lang.AssertionError: Message conversion time not recorded 0.0 ```  The test would intermittently fail because the conversion time took less than 1 millisecond. Such conversion time is reported as zero by the metric. There has been work done previously (https://github.com/apache/kafka/pull/4681) to combat the flakiness of the test and while it has improved it, the test still fails sometimes.  Locally, this test failed 5 times out of 25.  ### Changes Increase the record size and use compression - both will slow down message conversion enough to have it be above 1ms. Locally this test has not failed in 200 runs and counting","closed","","stanislavkozlovski","2018-08-09T12:52:03Z","2018-08-13T20:54:14Z"
"","5710","KAFKA-7223: internally provide full consumer record during restore","The Suppression buffer stores the full record context, not just the key and value,  so its changelog/restore loop will also need to preserve this information.  This change is a precondition to that, creating an option to register a  state restore callback to receive the full consumer record.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-28T04:44:20Z","2018-10-01T14:45:22Z"
"","6099","MINOR: Rejoin split ssl principal mapping rules","The string in the properties file is split on commas such that `ssl.principal.mapping.rules=RULE:^CN=(.*?),OU=ServiceUsers.*$/$1/` will create an array of `[""RULE:^CN=(.*?)"", ""OU=ServiceUsers.*$/$1/""]`. The Kafka broker then throws an illegal argument exception on startup because these rules are invalid. This pull request merges strings back together such that all rules either begin with `RULE:` or `DEFAULT`.  The new code was tested by adding two new functions to `SslPrincipalMapperTest` that mimic the existing tests, but with string lists as they would be read from the properties file as mentioned above.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ryannatesmith","2019-01-07T19:42:54Z","2019-01-21T06:31:37Z"
"","5671","MINOR: don't log config during unit tests","The StreamsConfig class by default prints its configuration values during construction.  This is valuable at runtime as a record of the actual configuration, but during unit testing, it results in excessive output.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-21T16:15:55Z","2018-10-01T14:44:33Z"
"","5914","KAFKA-7620:  Fix restart logic for TTLs in WorkerConfigTransformer","The restart logic for TTLs in `WorkerConfigTransformer` was broken when trying to make it toggle-able.   Accessing the toggle through the `Herder` causes the same code to be called recursively.  This fix just accesses the toggle by simply looking in the properties map that is passed to `WorkerConfigTransformer`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-11-14T22:53:25Z","2020-10-16T06:31:22Z"
"","5896","MINOR: Remove redundant SuppressIntegrationTests","The removed tests have counterparts covered by SuppressScenarioTest using the TopologyTestDriver.  This will speed up the build and improve stability in the CPU-constrained Jenkins environment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-08T23:16:11Z","2018-11-15T22:15:39Z"
"","5876","KAFKA-7509: Avoid passing most non-applicable properties to producer, consumer, and admin client","The producer, consumer, and admin client log properties that are supplied but unused by the producer. Previously, Connect would pass many of its worker configuration properties into the producer, consumer, and admin client used for internal topics, resulting in lots of log warnings about unused config properties.  With this change, Connect attempts to filter out the worker’s configuration properties that are not applicable to the producer, consumer, or admin client used for _internal_ topics. (Connect is already including only producer and consumer properties when creating those clients for connectors, since those properties are prefixed in the worker config.)  For the most part, this is relatively straightforward, since there are some top-level worker-specific properties that can be removed, and most extension-specific properties have Connect-specific properties. Unfortunately, the REST extension is the only type of connect extension that uses unprefixed properties from the worker config, so any it is not possible to remove those from the properties passed to the producer, consumer, and admin clients. Hopefully, REST extensions are prevalant yet, and this will mean most users may not see any warnings about unused properties in the producer, consumer, and admin client.  Removing the Connect worker configs is one step. The other is to remove the any properties for the producer that are specific to the consumer and admin client. Likewise, for the consumer we have to remove any properties that are specific to the producer and admin client, and for the admin client remove any properties that are specific to the producer and consumer. Note that any property that is unknown (e.g., properties for REST extension, interceptors, metric reporters, serdes, partitioners, etc.) must be passed to the producer, consumer, and admin client. All of these — except for the REST extension properties — should be used by both the producer and consumer. But, since the admin client only supports metric reporters, any properties for interceptors, serdes, partitioners and REST extension will also be logged as unused. Connect configures the serdes for the producers, so we're left with any custom properties for interceptors and partitioners still getting passed to the AdminClient, but this is about the best we can do at this point.  All of this filtering logic was added to the `ConnectUtils` class, allowing the logic to be easily unit tested and minimize changes to other existing code. All changes are limited to Kafka Connect, and will work with all client and Connect extensions (passing them to the clients if they are unknown). Also, note that when configuring the producers and consumers for connectors, Connect only uses the properties that begin with the `producer.` and `consumer.` prefixes, respectively.  This supersedes #5867 and #5802.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","rhauch","2018-11-02T23:51:20Z","2022-04-05T01:29:15Z"
"","5889","KAFKA-7604; Fix flaky unit test `testRebalanceAfterTopicUnavailableWithPatternSubscribe`","The problem is the concurrent metadata updates in the foreground and in the heartbeat thread. I changed the code to use `ConsumerNetworkClient.poll`, which enforces mutual exclusion when accessing the underlying client. I was seeing a failure rate of about 1/250 before this fix and I was not able to reproduce it with the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-11-07T22:47:18Z","2018-11-08T13:37:06Z"
"","6182","KAFKA-7844: Use regular subproject for generator to fix *All targets","The presence of the buildSrc subproject is causing problems when we try to run installAll, jarAll, and the other ""all"" targets. It's easier just to make the generator code a regular subproject and use the JavaExec gradle task to run the code. This also makes it more straightforward to run the generator unit tests.  Continuation from https://github.com/apache/kafka/pull/6169 with an added commit to exclude the `integrationTest` for generator, as it would break when `./gradlew integrationTest` got ran.","closed","","stanislavkozlovski","2019-01-21T16:15:21Z","2019-01-22T05:06:37Z"
"","6169","KAFKA-7844: Fix installAll, jarAll, etc. targets","The presence of the buildSrc subproject is causing problems when we try to run installAll, jarAll, and the other ""all"" targets.  It's easier just to make the generator code a regular subproject and use the JavaExec gradle task to run the code.  This also makes it more straightforward to run the generator unit tests.","closed","","cmccabe","2019-01-18T20:56:27Z","2019-05-20T19:03:53Z"
"","5663","KAFKA-7149 Reduce assignment data size","The PR contains the changes related to Assignment Info re-design where the TopicPartitions are replaced with TaskIDs and GZIP compression is being done on assignmentInfo to reduce the assignment size in version 4.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","brary","2018-09-19T07:53:56Z","2019-01-17T14:09:56Z"
"","6162","KAFKA-7149 Reduce assignment data size","The PR contains the changes related to Assignment Info re-design where the TopicPartitions are replaced with TaskIDs and GZIP compression is being done on assignmentInfo to reduce the assignment size in version 4.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","brary","2019-01-17T07:52:17Z","2019-06-13T20:50:42Z"
"","5683","KAFKA-7433: Introduce broker options in TopicCommand to use AdminClient (KIP-377)","The PR adds --bootstrap-server and --admin.config options to TopicCommand and implements an alternative, AdminClient based way of topic management.  As testing I've duplicated the existing tests and made them working with the AdminClient options.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-09-24T10:10:22Z","2019-02-04T18:15:08Z"
"","5720","KAFKA-6914: Set parent classloader of DelegatingClassLoader same as the worker's","The parent classloader of the DelegatingClassLoader and therefore the classloading scheme used by Connect does not have to be fixed to the System classloader.  Setting it the same as the one that was used to load the DelegatingClassLoader class itself is more flexible and, while in most cases will result in the System classloader to be used, it will also work in othr managed environments that control classloading differently (OSGi, and others).  The fix is minimal and the mainstream use is tested via system tests.","closed","connect,","kkonstantine","2018-10-01T19:08:47Z","2020-10-16T06:03:01Z"
"","6467","KAFKA-7847: KIP-421 - Support for resolving externalized secrets in AbstractConfig","The new constructor is similar to an existing constructor, with a flag enableAutoResolution to enable/disable automatic resolution of indirect variable. The originals configurations will contain both the config provider configs as well as configuration properties. The constructor will first instantiate the ConfigProviders using the config provider configs, then it will find all variables in the values of the originals configurations, attempt to resolve the variables using the named ConfigProviders, and then do the normal parsing and validation of the configurations. If ConfigProvider is not provided in the originals, the constructor will skip the variable substitution step and will simply validate and parse the supplied configuration.    ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)  Reviewer: @rhauch @rajinisivaram Please could you'll review this PR.","closed","connect,","tadsul","2019-03-18T22:57:02Z","2020-10-16T06:19:05Z"
"","6316","MINOR FIX: refactor naming of client metadata in StreamsPartitionAssignor","The name `clientsMetadata` makes the code readability harder since it collides with individual record name `clientMetadata`. Renaming to `clientMetadataMap` is a much better option IMO.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-02-23T23:39:32Z","2019-02-27T03:13:45Z"
"","5867","KAFKA-7509: Logging unused configs as DEBUG rather than WARN","The KafkaProducer, KafkaConsumer, and KafkaAdminClient all call `config.logUnused()` to log all of the unused configurations. These can be very misleading, because some conventional configuration properties for custom interceptors, key or value (de)serializers, metrics reporters, and partitioner are not actually defined by ConfigDefs and are thus all considered unused and logged as warnings.  Also, some client applications (such as Connect) do not determine the subset of properties that can be passed to one of these clients, since there are these custom extensions. This causes extra properties to be logged as warnings, which is concerning for new users.  These unused configurations should be logged at DEBUG level instead to reduce the number of unexpected warnings in many client application logs.  ### Backporting This is a usability improvement that affects only this particular log message. As such, it may or may not be something that should be backported. Advice from committers would be appreciated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-11-01T19:51:50Z","2018-11-02T23:53:45Z"
"","5934","MINOR: increase system test kafka start timeout","The Kafka Streams system tests fail with some regularity due to a timeout starting the broker.  The initial start is quite quick, but many of our tests involve stopping and restarting nodes with data already loaded, and also while processing is ongoing.  Under these conditions, it seems to be normal for the broker to take about 25 seconds to start, which makes the 30 second timeout pretty close for comfort. I have seen many test failures in which the broker successfully started within a couple of seconds after the tests timed out and already initiated the failure/shut-down sequence.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-21T01:31:12Z","2018-11-21T19:51:32Z"
"","6155","MINOR: Fix Javadoc of KafkaConsumer","The Javadoc is using Properties.put which should never be used because it allows putting non-strings into a Properties object which is designed to only handle strings. This method (as well as get) are used internally a lot but this just changes the Javadoc as to not encourage bad usage for users of the API.  Two other minor fixes so the examples actually work  No JIRA issue because it's a trivial javadoc fix, no tests because javadoc only patch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lfrancke","2019-01-16T10:19:04Z","2019-01-20T14:20:48Z"
"","5798","KAFKA-7412: onComplete should not reassign `metadata` variable","The Java doc for `InterceptorCallback#onComplete` says that exactly one of the arguments(metadata and exception) will be non-null. However, the commitment will be broken when TimeoutException is encountered since the code reassigns a new-created RecordMetadata object to variable `metadata`.  The solution is to leave `metadata1` unchanged and pass a new RecordMetadata instance to `onAcknowledgement`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-10-15T09:25:11Z","2018-11-09T00:58:15Z"
"","6373","KAFKA-7027: Add an overload build method in scala","The Java API can pass a Properties object to StreamsBuilder#build, to allow, e.g., topology optimization, while the Scala API does not yet. The latter only delegates the work to the underlying Java implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","massimosiani","2019-03-05T11:06:02Z","2020-06-12T23:48:09Z"
"","6176","MINOR: Handle case where connector status endpoints returns 404","The integration test framework for Connect provides a `org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster#connectorStatus` method that checks the status of a Connector via the REST endpoint and deserializes the response into a `ConnectorStateInfo` object. If the connector creation request has just been finished, and task creation delayed for some reason (for example, rebalancing is ongoing), the REST API returns a 404 on the `connectors/{connectorName}/status` endpoint, and causes the test to fail with the exception:  ``` [2019-01-19 03:47:59,289] (Test worker) ERROR Could not read connector state (org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster:178) java.io.FileNotFoundException: http://localhost:38625/connectors/simple-conn/status 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1909) 	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1509) 	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.executeGet(EmbeddedConnectCluster.java:223) 	at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.connectorStatus(EmbeddedConnectCluster.java:176) 	at org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.lambda$testProduceConsumeConnector$1(ExampleConnectIntegrationTest.java:117) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:355) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:342) 	at org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testProduceConsumeConnector(ExampleConnectIntegrationTest.java:117) ```  This fix handles the 404 case instead of simply re-throwing the Exception.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2019-01-19T20:48:00Z","2020-10-16T06:21:23Z"
"","6080","KAFKA-7243: Add unit integration tests to validate metrics in Kafka Streams","The goal of this task is to implement an integration test for the kafka stream metrics. We have to check 2 things:     1. After streams application are started, all metrics from different levels (thread, task, processor, store, cache) are correctly created and displaying recorded values.     2. When streams application are shutdown, all metrics are correctly de-registered and removed.","closed","streams,","khaireddine120","2018-12-31T08:28:21Z","2019-03-22T07:37:13Z"
"","5888","MINOR: Remove unused abstract function in test class","The function `setup_producer_and_consumer` is unused in the framework, which incorrectly suggests subclasses should implement it. It is not required or even referenced by the framework, so the requirement should be removed.","closed","","cyrusv","2018-11-07T19:32:59Z","2018-11-13T17:49:34Z"
"","6047","Instance level metrics for Streams","The following metrics are added at the per-instance level (before we do not have any instance level metrics, and the most coarsen gained is per-thread-level):  * application-id (static gauge) * state-string-name (dynamic gauge) * version / commit-id (static gauge, from appInfo) * rebalance-total / rebalance-latency-avg / rebalance-latency-max (count/avg/max) * last-rebalance-time (dynamic gauge)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-12-18T01:56:45Z","2020-04-25T00:06:50Z"
"","5575","KAFKA-7165: Retry the BrokerInfo registration into ZooKeeper","The following is a proposal when the ZooKeeper session has changed and the Broker tries to register himself into ZooKeeper.  Currently, this throws a `NodeExistsException` since a Broker with the same id got registered previously into Zookeeper.  **Assuming Broker id = 1** - If a Broker using this patch retries to register himself into ZooKeeper and the previous ZooKeeper session was not the one that registers the Broker (another Broker with the same id did), the Broker won't be allowed to register and a `NodeExistsException` will be generated. - If a Broker using this patch retries to register himself into ZooKeeper and the previous ZooKeeper session was the one that registers the Broker, the ephemeral node will be deleted from ZooKeeper and re-created by the Broker. - If a Broker **not** using this patch retries to register himself into ZooKeeper and the previous ZooKeeper session was not the one that registers the Broker (another Broker with the same id did), the Broker won't be allowed to register and a `NodeExistsException` will be generated.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonathansantilli","2018-08-26T19:46:50Z","2019-01-05T02:16:20Z"
"","5917","Making sure to close segment handlers prior to deletion","The fix is very simple in nature: Make sure to close all file handlers prior to deletion. As the closeHandles() method already calls safeForceUnmap, once you arrive there the buffer might already be cleaned. Thus, the added null check.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kobihikri","2018-11-15T13:45:56Z","2018-11-15T14:39:52Z"
"","6128","MINOR: clarify the record selection algorithm and stream-time definition","The existing javadoc for PartitionGroup is a little confusing. It's relatively important for these concepts to be clear, since they form the basis for stream-time in Kafka Streams.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-11T18:52:24Z","2019-01-13T19:43:49Z"
"","5916","MINOR: Bump documentation version to 2.1","The documentation version of 2.1.0 RC1 is still at 2.0. Updated it to 2.1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-11-15T06:57:27Z","2018-11-15T19:24:52Z"
"","6464","MINOR: Mention in configuration of broker setting log.retention.ms that -1 disables retention by time","The docs around retention were [updated](https://github.com/apache/kafka/commit/f3ed56b21f6ac1e47337983b46aca1af88337e9e) a while ago to reflect that -1 disables time limits entirely. That sentence should also be added to the broker level setting log.retention.ms as this is pulled down into the topic level setting retention.ms if no value is explicitly specified.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-03-18T18:59:55Z","2019-04-15T14:37:46Z"
"","5738","KAFKA-7478: Reduce default logging verbosity in OAuthBearerLoginModule","The default `OAuthBearerLoginModule` is too noisy. My reasoning is that:  - Successful logins should be shown in `DEBUG` - DEBUG should not be too noisy either - Login aborted should be in `DEBUG` since authentication failures are always logged","closed","","stanislavkozlovski","2018-10-04T09:07:33Z","2018-10-08T21:23:56Z"
"","6375","MINOR: Fixed docs regarding caching default","The current docs are out of date regarding the default caching state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-05T19:06:11Z","2019-03-15T23:52:01Z"
"","6205","MINOR: Add information to command line doc of producer performance tool","The command line producer performance tool has a the required parameter _--throughput_ which controls the target number of messages to be sent per second. If no throttling is desired this can be set to a [negative number ](https://github.com/apache/kafka/blob/d0e436c471ba4122ddcc0f7a1624546f97c4a517/tools/src/main/java/org/apache/kafka/tools/ThroughputThrottler.java#L70) - but this is not specified in the command line help text.  No jira because this is a minor fix. No added tests, as this just changes a string variable, no actual code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-01-26T14:28:21Z","2019-02-07T17:00:57Z"
"","6266","MINOR: fix bypasses in ChangeLogging stores","The change-logging stores should not bypass methods in underlying stores.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-13T22:48:19Z","2019-02-14T20:03:12Z"
"","5576","KAFKA-7347; Return not leader error OffsetsForLeaderEpoch requests to non-replicas","The broker should return NOT_LEADER_FOR_PARTITION for OffsetsForLeaderEpoch requests against non-replicas instead of UNKNOWN_TOPIC_OR_PARTITION. This patch also fixes a minor bug in the handling of ListOffsets request using the DEBUG replica id. We should return UNKNOWN_TOPIC_OR_PARTITION if the topic doesn't exist.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-26T22:20:46Z","2018-08-28T00:39:45Z"
"","6100","KAFKA-7768: Use absolute paths for javadoc","The breakage observed in KAFKA-7768 was only when following a link from a page with a non-versioned URL.  When starting from  a versioned url, the number of `../` up-paths was correct, so inserting the version actually breaks it. To avoid this situation, this PR switches to using absolute paths in conjunction with the version number, as other links in the docs do.  See also #6094   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-07T20:58:05Z","2019-01-07T22:12:14Z"
"","5582","KIP-368: Allow SASL Connections to Periodically Re-Authenticate","The adoption of KIP-255: OAuth Authentication via SASL/OAUTHBEARER in release 2.0.0 creates the possibility of using information in the bearer token to make authorization decisions. Unfortunately, however, Kafka connections are long-lived, so there is no ability to change the bearer token associated with a particular connection. Allowing SASL connections to periodically re-authenticate would resolve this. In addition to this motivation there are two others that are security-related. First, to eliminate access to Kafka for connected clients, the current requirement is to remove all authorizations (i.e. remove all ACLs). This is necessary because of the long-lived nature of the connections. It is operationally simpler to shut off access at the point of authentication, and with the release of KIP-86: Configurable SASL Callback Handlers it is going to become more and more likely that installations will authenticate users against external directories (e.g. via LDAP). The ability to stop Kafka access by simply disabling an account in an LDAP directory (for example) is desirable. The second motivating factor for re-authentication related to security is that the use of short-lived tokens is a common OAuth security recommendation, but issuing a short-lived token to a Kafka client (or a broker when OAUTHBEARER is the inter-broker protocol) currently has no benefit because once a client is connected to a broker the client is never challenged again and the connection may remain intact beyond the token expiration time (and may remain intact indefinitely under perfect circumstances). This KIP proposes adding the ability for clients (and brokers when OAUTHBEARER is the inter-broker protocol) to re-authenticate their connections to brokers and for brokers to close connections that continue to use expired sessions.  Signed-off-by: Ron Dagostino","closed","","rondagostino","2018-08-28T14:25:35Z","2018-10-26T22:18:15Z"
"","6046","MINOR: Remove sleep calls and ignore annotation from streams upgrade test","The `StreamsUpgradeTest::test_upgrade_downgrade_brokers`  used sleep calls in the test which led to flaky test performance and as a result, we placed an `@ignore` annotation on the test.  This PR uses log events instead of the `sleep` calls hence we can now remove the `@ignore` setting.  Ran a branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2146/ which passed with all tests enabled.  Kicked off another branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2147/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-17T22:32:01Z","2019-01-07T07:03:55Z"
"","6244","MINOR: Add all topics created check streams broker bounce test (2.2)","The `StreamsBrokerBounceTest.test_broker_type_bounce` experienced what looked like a transient failure. After looking over this test and failure, it seems like it is vulnerable to timing error that streams will start before the kafka service creates all topics.  ``` org.apache.kafka.streams.errors.TopologyException: Invalid topology: stream-thread [SmokeTest-44232843-7798-4a19-b0a8-56deedd866e6-StreamThread-1-consumer] Topic not found: sum         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor$CopartitionedTopicsValidator.validate(StreamsPartitionAssignor.java:923)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.ensureCopartitioning(StreamsPartitionAssignor.java:902)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:468)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:419)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:592)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:94)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:544)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:527)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:894)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:874) ``` After making the changes, I kicked off a [branch builder with five repeats](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2335/)  Note that the reason I chose five repeats is the test uses a matrix and generate eight versions of the test and each test lasts about 5 minutes. There will be mirrored PRs for `2.0`, `2.1` and `trunk`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-02-08T15:30:24Z","2019-02-20T17:47:15Z"
"","6243","MINOR: Add all topics created check streams broker bounce test (trunk)","The `StreamsBrokerBounceTest.test_broker_type_bounce` experienced what looked like a transient failure. After looking over this test and failure, it seems like it is vulnerable to timing error that streams will start before the kafka service creates all topics.  ``` org.apache.kafka.streams.errors.TopologyException: Invalid topology: stream-thread [SmokeTest-44232843-7798-4a19-b0a8-56deedd866e6-StreamThread-1-consumer] Topic not found: sum         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor$CopartitionedTopicsValidator.validate(StreamsPartitionAssignor.java:923)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.ensureCopartitioning(StreamsPartitionAssignor.java:902)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:468)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:419)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:592)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:94)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:544)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:527)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:894)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:874) ``` After making the changes, I kicked off a [branch builder with five repeats](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-02-07--001.1549591092--bbejeck--MINOR_add_all_topics_created_check_StreamsBrokerBounceTest--b30c8e4/report.html)  Note that the reason I chose five repeats is the test uses a matrix and generate eight versions of the test and each test lasts about 5 minutes. There will be mirrored PRs for `2.0`, `2.1` and `2.2`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-08T14:59:13Z","2019-02-20T17:45:45Z"
"","6242","MINOR: Add all topics created check streams broker bounce test (2.1)","The `StreamsBrokerBounceTest.test_broker_type_bounce` experienced what looked like a transient failure. After looking over this test and failure, it seems like it is vulnerable to timing error that streams will start before the kafka service creates all topics.  ``` org.apache.kafka.streams.errors.TopologyException: Invalid topology: stream-thread [SmokeTest-44232843-7798-4a19-b0a8-56deedd866e6-StreamThread-1-consumer] Topic not found: sum         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor$CopartitionedTopicsValidator.validate(StreamsPartitionAssignor.java:923)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.ensureCopartitioning(StreamsPartitionAssignor.java:902)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:468)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:419)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:592)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:94)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:544)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:527)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:894)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:874) ``` After making the changes, I kicked off a [branch builder with five repeats](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-02-07--001.1549590732--bbejeck--MINOR_add_all_topics_created_check_StreamsBrokerBounceTest_2_1--0b0d239/report.html)  Note that the reason I chose five repeats is the test uses a matrix and generate eight versions of the test and each test lasts about 5 minutes. There will be mirrored PRs for `2.0`, `2.2` and `trunk`.","closed","","bbejeck","2019-02-08T14:52:02Z","2019-02-21T20:59:21Z"
"","6241","MINOR: Add check all topics created check streams broker bounce test (2.0)","The `StreamsBrokerBounceTest.test_broker_type_bounce` experienced what looked like a transient failure.  After looking over this test and failure, it seems like it is vulnerable to timing error that streams will start before the kafka service creates all topics.  ``` org.apache.kafka.streams.errors.TopologyException: Invalid topology: stream-thread [SmokeTest-44232843-7798-4a19-b0a8-56deedd866e6-StreamThread-1-consumer] Topic not found: sum         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor$CopartitionedTopicsValidator.validate(StreamsPartitionAssignor.java:923)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.ensureCopartitioning(StreamsPartitionAssignor.java:902)         at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:468)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:419)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:592)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:94)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:544)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:527)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:894)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:874) ``` After making the changes, I kicked off a [branch builder with five repeats](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-02-07--001.1549590843--bbejeck--MINOR_add_all_topics_created_check_StreamsBrokerBounceTest_2_0--b09424a/report.html)  Note that the reason I chose five repeats is the test uses a matrix and generate eight versions of the test and each test lasts about 5 minutes. There will be mirrored PRs for `2.1` and `trunk`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-02-08T13:53:31Z","2019-02-20T21:40:42Z"
"","5666","MINOR: Remove ignored tests that hang, added new versions for EOS tests","The `streams_broker_compatibility_test::test_timeout_on_pre_010_brokers` test is ignored.  The test uses broker version `0.9 latest` and `0.8 latest`.  At one point Streams would throw a `BrokerNotFoundException`, but that exception is no longer thrown so this will hang forever, thus should just be removed.   Also added new versions to `test_compatible_brokers_eos_disabled` for consistency.  For testing ran branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1964/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-19T20:56:00Z","2018-09-19T22:05:40Z"
"","6370","KAFKA-8025: Update regex to allow any chars after "":""","The `RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest` had a couple of flaky failures.  From looking at the error message ``` Expected: a string matching the pattern 'Unexpected method call DBOptions\.baseBackgroundCompactions((.* 14:06:14     *)*):' 14:06:14          but: was ""Unexpected method call DBOptions.baseBackgroundCompactions():\n    DBOptions.close(): expected: 3, actual: 0"" ``` It looks like there are occasionally extra characters after the `:` so the expected pattern was expanded to include any characters after `:`  I updated the test and ran the streams test suite.  I ran the updated test locally 1000 times and all tests passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-04T23:08:33Z","2019-04-17T18:49:07Z"
"","5947","kafkatest: Match and compare `*-SNAPSHOT` versions in KafkaVersionTest","The `KafkaVersionTest` consistently fails with the following error: ``` [WARNING - 2018-11-25 16:41:50,917 - util - is_version - lineno:71]: : kafka version mismatch: expected ['2.1.1-SNAPSHOT'], actual set([u'2.1.1']), ps line 18996 ?   ``` We should extend the version check to support snapshot versions","closed","","stanislavkozlovski","2018-11-26T13:34:26Z","2018-11-28T10:17:42Z"
"","5841","MINOR: Increase low timeouts to help with test flakiness","The `InternalTopicIntegrationTest` and `GlobalThreadShutDownOrderTest` use relatively strict timeouts of 5 seconds to check the intermediate state of the tests.  The test failures observed in these two tests were not about the final output but asserting the embedded broker sent messages within the given timeframe.  I ran the existing Streams test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-25T16:52:55Z","2018-11-05T02:56:56Z"
"","5528","MINOR: Change order of Property Check To Avoid NPE","The `InternalStreamsBuilder#maybePerformOptimizations` should change the order of how it checks for `Topology_OPTIMIZATION` to avoid an NPE.   For testing, I added a test that results in an NPE without the provided fix and ran the existing set of streams tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-08-17T21:47:25Z","2018-08-17T22:44:09Z"
"","5995","MINOR: Update command line options in Authorization and ACLs documentation chapter","The [_Command Line Interface_](http://kafka.apache.org/documentation/#security_authz_cli) section of the _Authorization and ACLs_ chapter seems to be missing some of the new options available in Kafka 2.1 and later. This PR updates this section:  * Add Transactional ID * Add Delegation Token * Update supported operations for `--operations` options * Add Idempotent option  This is documentation change only. No tests were updated. Local webserver was used to verify the changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","scholzj","2018-12-03T23:15:28Z","2019-01-08T16:45:04Z"
"","5862","Merge pull request #2 from apache/trunk","test1 - merge  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aka007","2018-10-31T23:11:05Z","2018-10-31T23:11:42Z"
"","6383","KAFKA-8039 - Use MockTime in fast reauth test to avoid transient failures","Test uses 100ms as `connections.max.reauth.ms` and checks that a second reauthentication doesn't occur within the hard-coded 1 second minimum interval. But since the interval is small, we cannot guarantee that the time between the two checks is not higher than 1 second. Change the test to use MockTime so that we can control the time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-06T18:08:09Z","2019-03-07T13:22:17Z"
"","6391","KAFKA-7980 - Fix timing issue in SocketServerTest.testConnectionRateLimit","Test currently checks that there were at least 5 polls when 5 connections are established with connectionQueueSize=1. But we could be doing the check just after the 5th connection before the 5th poll, so updated the check to verify that there were at least 4 polls.  I could not recreate the failure with or without the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-07T11:31:56Z","2019-03-08T14:37:24Z"
"","6458","MINOR: fix flaky QueryableStateIntegrationTest","Test `#concurrentAccesses()` fails regularly on `0.11.0` branch, so I compared the test code with current `trunk` version and back ported all changes. Hope this will stabilize the test.","closed","streams,","mjsax","2019-03-17T19:41:18Z","2019-03-26T18:32:21Z"
"","5718","KAFKA-7460: Fix Connect Values converter date format pattern","Switches to normal year format instead of week date years and day of month instead of day of year.  This is directly from #4820, but separated into a different JIRA/PR to keep the fixes independent. Original authorship should be maintained in the commit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-09-30T23:53:56Z","2018-10-01T02:53:54Z"
"","6013","MINOR: Switch anonymous classes to lambda expressions in tools module","Switch to lambda when ever possible instead of old anonymous way  in tools module  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mrsrinivas","2018-12-07T17:20:56Z","2018-12-21T08:58:49Z"
"","5647","KAFKA-7367: Ensure stateless topologies don't require disk access","Streams should not eagerly create the state store directory, since in general topologies may not need state stores.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-09-13T18:35:07Z","2018-11-14T20:55:38Z"
"","5792","MINOR: fix non-deterministic streams-scala tests","Stop using current system time by default, as it introduces non-determinism.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-12T15:12:44Z","2018-10-16T00:48:04Z"
"","6148","MINOR: Start Connect REST server in standalone mode to match distributed mode (KAFKA-7503 follow-up)","Start the Rest server in the standalone mode similar to how it's done for distributed mode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mageshn","2019-01-15T20:00:21Z","2019-01-17T07:00:38Z"
"","6378","KAFKA-7979 - Clean up threads and increase timeout in PartitionTest","Stack trace generated from the test failure shows that the test failed even though threads were runnable and making progress, indicating that the timeout may be too small when test machine is slow. Increasing timeout from 10 to 15 seconds, consistent with the default wait in other tests. Thread dump also showed a lot of left over threads from other tests, so added clean up of those as well.  I wasn't able to recreate the failure with or without the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-06T10:31:41Z","2019-03-06T14:13:33Z"
"","6059","KAFKA-7766: Fail fast PR builds","Split the Gradle invocation in the jenkins.sh script into two commands so we can fail fast for validation checks such as compile errors and checkstyle errors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2018-12-21T14:39:03Z","2019-02-02T20:22:57Z"
"","5650","KAFKA-6647 KafkaStreams.cleanUp creates .lock file in directory it tries to clean","Specify StandardOpenOption#DELETE_ON_CLOSE when creating the FileChannel.  Move lock file up one level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","tedyu","2018-09-14T00:09:29Z","2019-05-14T06:31:19Z"
"","5957","MINOR: Support long maxMessages in Trogdor consume/produce bench workers","Sometimes we might want to run intense longer-running benchmarks with Trogdor. We are currently limited to `2147483647`. If we use 100 bytes per message, this limits us to 2.4MB/s of throughput a day. (`((2147483647 * 100) / 1000000 / 86400)`) We might want to have longer running tasks than a single day as well, so I think it makes sense to bump this to a long.  cc @cmccabe","closed","","stanislavkozlovski","2018-11-28T14:53:51Z","2018-11-28T17:13:22Z"
"","6028","MINOR: improve resilience of Streams test producers","Some Streams system tests have failed during the setup phase due to the producer having retries disabled and getting some  transient error from the broker.  This patch adds a `retries` parameter to the VerifiableProducer (default unchanged), and sets `retries` to 10 for Streams tests.  It also sets `acks` equal to the number of brokers for Streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-12-12T20:42:23Z","2019-01-04T22:04:11Z"
"","5704","KAFKA-7449 Forward topic from console consumer to deserializer","Some deserializer needs the topic name to be able to correctly deserialize the payload of the message. Console consumer works great with Deserializer however it calls deserializer with topic set as null. This breaks the API and the topic information is available in the ConsumerRecord.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mchataigner","2018-09-27T14:22:47Z","2018-11-29T02:23:30Z"
"","6442","make sure replica.fetch.max.bytes will always >= to max.message.bytes","so when the max.message.bytes is bigger than replica.fetch.max.bytes will cause the replica fetch blocked. This will leads to only the leader partition is available, and change the replica.fetch.max.bytes later on might cause the data loss.","open","","lisa2lisa","2019-03-14T09:02:10Z","2019-03-20T05:20:04Z"
"","6321","Critical: make sure replica.fetch.max.bytes will always >= to max.message.bytes","so when increase the when the max.message.bytes is bigger than replica.fetch.max.bytes will cause the replica fetch blocked.  This will leads to only one Isa available, and changing the config might cause data loss.","closed","","lisa2lisa","2019-02-25T14:55:27Z","2019-03-13T10:20:01Z"
"","6142","[KAFKA-7818] Make the KafkaConsumer AutoCloseable","So we can use Java's try-with-resources functionality: https://jira.apache.org/jira/browse/KAFKA-7818  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","Fokko","2019-01-14T12:54:37Z","2019-01-14T23:19:57Z"
"","5550","Get state store from Materialized","So I have this use case I'm not too sure how to address: ```scala val stateStore = ""store"" val materialized = Materialized.as[String, Long, ByteArrayKeyValueStore](stateStore) ... table1.join(table2, materialized)((a, b) => a + b)  val testDriver = createTestDriver(builder) ... testDriver.getKeyValueStore[String, Long](stateStore).get(""1"") shouldBe 2 ```  To me it sounds like we should be able to get the store from/with the `Materialized` object. It's much better typed than a `String`: ```scala val materialized = Materialized.as[String, Long, ByteArrayKeyValueStore](stateStore) ... table1.join(table2, materialized)((a, b) => a + b)  val testDriver = createTestDriver(builder) ... testDriver.getKeyValueStore(materialized).get(""1"") shouldBe 2 ```  I'm not sure if that's the way to do it exactly so feel free to comment.","open","","joan38","2018-08-22T00:09:00Z","2018-08-28T22:19:51Z"
"","6033","MINOR: Gracefully close the consumer in ConsumeBenchWorker","Since WorkerUtils.abort propagates the exception after handling it, any exceptions raised will not close the consumer properly","open","","stanislavkozlovski","2018-12-14T14:23:38Z","2020-02-06T19:16:39Z"
"","6299","MINOR: Subscribe/assign calls should be logged at info level","Since we are logging offset resets and such, it makes sense to ensure we know what topics/partitions are current subscribed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-21T16:32:19Z","2019-02-21T21:03:35Z"
"","6307","KAFKA-7937: Fix Flaky Test ResetConsumerGroupOffsetTest.testResetOffsetsNotExistingGroup","Since the test fails sometimes on lack of coordinator, I'm giving it a bit more attempts to find it.  I admit that I haven't been able to actually reproduce this failure, so I'm only hoping this fixes it. But it doesn't fail more often than it used to (on my machine)  Fixing on 2.2 because the intent is to fix enough flakes to allow for a clean release.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gwenshap","2019-02-22T05:54:08Z","2019-02-22T19:21:12Z"
"","6245","MINOR: Extended Scala serdes implicit conversions with optional custom naming","Since custom state store and repartition topic naming in DSL API is now possible in Kafka 2.1.0, this proposed feature enhance implicit conversions for the Scala API making it possible to call (assuming `val stream: KGroupedStream[K, V]`:  `stream.aggregate(initializer)(aggregateFunc)(""customNameTopic"")`  instead of: `stream.aggregate(initializer)(aggregateFunc)(Materialized.as[K, V, ByteArrayWindowStore](""customStateStoreName"")`  This change is backward compatible, that said: `stream.aggregate(initializer)(aggregateFunc)` also works with default naming convention.  These implicit conversions are propagated for `Materialized`, `Grouped` and `Joined` as well.","closed","","mowczare","2019-02-08T17:40:09Z","2019-02-12T13:18:07Z"
"","6267","KAFKA-7930: topic is not internal if explicitly listed in args","Simplest fix: topic is not internal if explicitly listed in --input-topics or --intermediate-topics.","closed","tools,","muradm","2019-02-14T06:34:40Z","2019-02-21T07:38:56Z"
"","6057","MINOR: code cleanup","Similar to #6054","closed","streams,","mjsax","2018-12-20T14:51:54Z","2019-01-09T14:04:58Z"
"","6056","MINOR: code cleanup","Similar to #6053","closed","streams,","mjsax","2018-12-20T14:46:24Z","2019-01-08T21:33:00Z"
"","6054","MINOR: code cleanup","Similar PR as #6053","closed","streams,","mjsax","2018-12-20T14:31:04Z","2019-01-14T21:36:41Z"
"","6397","MINOR: avoid unnecessary collection copies between java and scala in metadata cache","Signed-off-by: radai-rosenblatt   a profiler run on one of our brokers under load showed that ~11% of the global cpu time for the entire broker was taken by these 3 .map() calls:  5.36% in replicas.asScala.map(\_.toInt) (line 79) 2.31% in offlineReplicas.asScala.map(\_.toInt) (line 81) 3.58% in isr.asScala.map(\_.toInt) (line 97)  for a total of 11.25%","closed","","radai-rosenblatt","2019-03-07T23:07:20Z","2019-03-22T15:57:37Z"
"","6206","MINOR: always include full tp for fetch errors to help narrow down the specific broker they came from","Signed-off-by: radai-rosenblatt   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","radai-rosenblatt","2019-01-27T01:18:25Z","2019-02-10T08:45:58Z"
"","5729","KAFKA-7475 - capture remote address on connection authetication errors, and log it","Signed-off-by: radai-rosenblatt   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","radai-rosenblatt","2018-10-02T20:35:11Z","2018-10-11T12:32:05Z"
"","6462","KAFKA-8121; Shutdown ZK client expiry handler earlier during close","Shutdown session expiry thread prior to closing ZooKeeper client to ensure that new clients are not created by the expiry thread and left active when returning from `ZooKeeperClient.close()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-18T12:19:32Z","2019-03-18T18:07:24Z"
"","6004","MINOR: Safe string conversion to avoid NPEs","Should be ported back to 2.0","closed","","cyrusv","2018-12-05T18:05:57Z","2018-12-05T21:30:21Z"
"","5843","MINOR: Bump timeout for sending records","Set timeout to be consistent with changes to `2.0` #5841  when merged we should cherry-pick to `2.1`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-25T17:34:53Z","2018-11-05T02:56:20Z"
"","5552","KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER","Set empty extensions if null is passed in.  Signed-off-by: Ron Dagostino","closed","","rondagostino","2018-08-22T02:08:10Z","2018-08-29T10:16:30Z"
"","5783","KAFKA-4453: Separating controller connections and requests from the data plane (KIP-291)","Separating controller connections and requests from the data plane (KIP-291)  - Tested that there are no dedicated control plane endpoints or threads when control.plane.listener.name is not set. - Tested that the controller requests are handled by the control plane threads when control.plane.listener.name is set  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-10-11T18:33:00Z","2018-11-16T20:16:44Z"
"","5531","KAFKA-7311: Reset next batch expiry time on each poll loop","Sender/RecordAccumulator never resets the next batch expiry time. Its always computed as the min of the current value and the expiry time for all batches being processed. This means that its always set to the expiry time of the first batch, and once that time has passed Sender starts spinning on epoll with a timeout of 0, which consumes a lot of CPU. This patch updates Sender to reset the next batch expiry time on each poll loop so that a new value reflecting the expiry time for the current set of batches is computed.  cc @guozhangwang @hachikuji @ijuma  Hope you folks are the right people to review.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2018-08-19T00:56:18Z","2018-08-21T21:34:50Z"
"","6390","KAFKA-7288 - Make sure no bytes buffered when relying on idle timeout in channel close test","SelectorTest.testCloseConnectionInClosingState sends and receives messages to get the channel into a state with staged receives and then waits for idle timeout to close the channel. When run with SSL, the channel may have buffered bytes that prevent the channel from being closed. Updated the test to wait until there are no buffered bytes as well. I have left the number of retries to achieve this state at 100, since my local runs always succeed the first time.  I wasn't able to recreate the failure with or without the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-07T10:44:34Z","2019-03-07T18:09:51Z"
"","6235","KAFKA-7540; Retry coordinator lookup to fix transient failure","Seems like a good idea to retry FindCoordinator requests in test cases. If lookup fails, then we get the `ArrayIndexOutOfBoundsException`. At a minimum, this should get us to a better indication of the problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-07T00:30:34Z","2019-02-07T10:00:58Z"
"","6215","KAFKA-7890: Invalidate ClusterConnectionState cache for a broker if the hostname of the broker changes.","See https://issues.apache.org/jira/browse/KAFKA-7890 for additional details.  - Added a new unit test where the hostname changes. - Also tested in production  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","markcho","2019-01-31T21:37:13Z","2019-02-05T12:29:55Z"
"","5946","KAFKA-7443: OffsetOutOfRangeException in restoring state store from changelog topic when start offset of local checkpoint is smaller than that of changelog topic","See https://issues.apache.org/jira/browse/KAFKA-7443 for details. The fix set this state partition to ""NO_CHECKPOINT"" when the offset in local checkpoint file has expired and older than the current start offset of changelog topic, thus making this task to restore local state from the current beginning offset of changelog topic, avoiding falling into the infinite loop caused by this exception.","closed","streams,","linyli001","2018-11-26T13:21:22Z","2018-12-14T02:16:53Z"
"","5943","KAFKA-7389: Enable spotBugs with Java 11 and disable false positive warnings","See https://github.com/spotbugs/spotbugs/issues/756 for details on the false positives affecting try with resources. An example is:  > RCN | Nullcheck of fc at line 629 of value previously dereferenced in > org.apache.kafka.common.utils.Utils.readFileAsString(String, Charset)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-11-24T17:20:20Z","2018-11-30T14:55:59Z"
"","5743","KAFKA-3816: Add MDC logging to Connect runtime","See https://cwiki.apache.org/confluence/display/KAFKA/KIP-449%3A+Add+connector+contexts+to+Connect+worker+logs  Added LoggingContext as a simple mechanism to set and unset Mapped Diagnostic Contexts (MDC) in the loggers to provide for each thread useful parameters that can be used within the logging configuration. MDC avoids having to modify lots of log statements, since the parameters are available to all log statements issued by the thread, no matter what class makes those calls.  The design intentionally minimizes the number of changes to any existing classes, and does not use Java 8 features so it can be easily backported if desired, although per this KIP it will be applied initially only in AK 2.3 and later and must be enabled via the Log4J configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-10-04T22:53:56Z","2020-10-16T06:03:02Z"
"","6284","KAFKA-6755: Allow literal value for MaskField SMT","See [KIP-437](https://cwiki.apache.org/confluence/display/KAFKA/KIP-437%3A+Custom+replacement+for+MaskField+SMT), which has been adopted.  ### Goal Allow literal value for MaskField SMT instead of null-masking for masking out: - IP address,  - SSN - other personally identifiable information (PII)  ### Details The existing `org.apache.kafka.connect.transforms.MaskField` SMT always uses the null value for field masking. This PR introduces additional config param `replacement`, which will be applied to all fields in `fields` list. In such a way you can mask any string or numeric values with any literal of your choice.  _example config:_ ``` transforms=SSNMask,IPMask transforms.SSNMask.type=org.apache.kafka.connect.transforms.MaskField$Value transforms.SSNMask.fields=ssn transforms.SSNMask.replacement=***-***-**** transforms.IPMask.type=org.apache.kafka.connect.transforms.MaskField$Value transforms.IPMask.fields=ipAddress transforms.IPMask.replacement=xxx.xxx.xxx.xxx ``` ### Restrictions: - only numeric or string values are supported (no Dates, Booleans, lists, maps) - literal value in `replacement` should be convertable to the type(s) of the `fields` passed. Be aware of such situations:  _if the replacement cannot be converted to some of the fields passed, the exception will be thrown_ ``` transforms=FieldMask transforms.FieldMask.type=org.apache.kafka.connect.transforms.MaskField$Value transforms.FieldMask.fields=ssn,age transforms.FieldMask.replacement=string_replacement ```  _if the replacement can be converted to all of the fields types, but fields types are different, replacement will be converted to the desired types_: ``` transforms=FieldMask transforms.FieldMask.type=org.apache.kafka.connect.transforms.MaskField$Value transforms.FieldMask.fields=name,age transforms.FieldMask.replacement=12 ```  Masking lists, maps and Booleans with custom value seems useless. Masking dates is not very flexible in `org.apache.kafka.connect.data.Values`. Custom parser is also a needless solution, until it is not required by the user.  ### Testing - Unit tests are provided, custom doc testing was held  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","Nimfadora","2019-02-18T14:19:01Z","2020-05-24T14:22:41Z"
"","6368","MINOR: Log exception thrown by consumer.poll() in VerifiableConsumer","SecurityTest.test_client_ssl_endpoint_validation_failure is failing because it greps for 'SSLHandshakeException in the consumer and producer log files. With the fix for KAKFA-7773, the test uses the VerifiableConsumer instead of the ConsoleConsumer, which does not log the exception stack trace to the service log. This patch catches exceptions in the VerifiableConsumer and logs them in order to fix the test. Tested by running the test locally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-03-04T21:09:00Z","2019-03-06T02:12:47Z"
"","6327","KAFKA-7918: Inline generic parameters Pt. II: RocksDB Bytes Store and Memory LRU Caches","Second PR in series to inline the generic parameters of the following bytes stores:  [ Pt. I] InMemoryKeyValueStore [x] RocksDBWindowStore [x] RocksDBSessionStore [x] MemoryLRUCache [x] MemoryNavigableLRUCache [  ] InMemoryWindowStore  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-02-26T00:15:12Z","2019-02-27T15:08:32Z"
"","5747","KAFKA-7477: Improve Streams close timeout semantics","Second part of [KIP-358](https://cwiki.apache.org/confluence/display/KAFKA/KIP-358%3A+Migrate+Streams+API+to+Duration+instead+of+long+ms+times).  This changes based on [previous PR discussion](https://github.com/apache/kafka/pull/5682#discussion_r221473451).  Default `close` timeout is `30 seconds`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","nizhikov","2018-10-05T12:30:50Z","2018-10-09T18:38:29Z"
"","5530","KAFKA-7524: Recommend Scala 2.12 and use it for development","Scala 2.12 has better support for newer Java versions and includes additional compiler warnings that are helpful during development. In addition, Scala 2.11 hasn't been supported by the Scala community for a long time, the soon to be released Spark 2.4.0 will finally support Scala 2.12 (this was the main reason preventing many from upgrading to Scala 2.12) and Scala 2.13 is at the RC stage. It's time to start recommending the Scala 2.12 build as we prepare support for Scala 2.13 and start thinking about removing support for Scala 2.11.  In the meantime, Jenkins will continue to build all supported Scala versions (including Scala 2.11) so the PR and trunk jobs will fail if people accidentally use methods introduced in Scala 2.12.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-08-18T14:49:20Z","2018-10-28T18:31:43Z"
"","6185","MINOR: fix race condition in KafkaStreamsTest","Saw failing build: https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk11/detail/kafka-trunk-jdk11/235/tests/ ``` java.lang.AssertionError: expected:<6> but was:<5> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.junit.Assert.assertEquals(Assert.java:631) at org.apache.kafka.streams.KafkaStreamsTest.testStateOneThreadDeadButRebalanceFinish(KafkaStreamsTest.java:212) ```  Side ""cleanup"" to revert #6178 (was a try to stabilize the build, but did not resolve the issue) \cc @ijuma","closed","streams,","mjsax","2019-01-22T22:11:39Z","2019-01-24T07:21:19Z"
"","6006","KAFKA-6388: Recover from rolling an empty segment that already exists (branch 1.1)","Same as https://github.com/apache/kafka/pull/5986 but for AK 1.1.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-12-06T00:23:00Z","2018-12-06T18:17:50Z"
"","6055","MINOR: code cleanup","Same as #6054","closed","streams,","mjsax","2018-12-20T14:38:57Z","2019-01-09T14:03:34Z"
"","6329","KAFKA-1194: Fix renaming open files on Windows","Running Kafka on Windows results in errors relating to renaming files that are locked (still mapped) by running Kafka. This patch unmmap()s and mmap()s a file before and after the actual renameTo() respectively.  Patch should have no effect on Linux.","open","","robertbraeutigam","2019-02-26T13:27:42Z","2022-07-14T15:59:21Z"
"","6310","MINOR: Enable capture of full stack trace in StreamTask#process","Right now in `StreamTask#process` when we catch a `KafkaException`, only the message is printed in the log statement which makes debugging errors difficult.  This PR will capture the full stack trace of the exception and add it to the log statement.  I ran the existing streams test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-22T19:10:55Z","2019-02-23T20:11:13Z"
"","6050","MINOR: Add test demonstrating re-use of KGroupedStream with Optimizations enabled","Right now if a repartition is required and users choose to name the repartition topic for an aggregation i.e `kGroupedStream = builder.stream(""topic"").selectKey((k, v) -> k).groupByKey(Grouped.as(""grouping""));` The resulting `KGroupedStream` can't be reused with optimizations are **disabled**, as Streams will attempt to create two repartiton topics with the same name.  However, if optimizations **_are enabled_** then the resulting `KGroupedStream` **can** be re-used For example the following will work if optimizations are enabled. ``` kGroupedStream = builder.stream(""topic"")                              .selectKey((k, v) -> k)                              .groupByKey(Grouped.as(""grouping""));  kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(10L))).count(); kGroupedStream.windowedBy(TimeWindows.of(Duration.ofMillis(30L))).count(); ```   This PR provides a unit test proving as much.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-19T16:54:47Z","2018-12-22T06:56:33Z"
"","6335","KAFKA-8007: Avoid copying on fetch in InMemoryWindowStore","Rewrote the InMemoryWindowStore implementation by moving the work of a fetch to the iterator, and cleaned up the iterators as well.  Blocked by [KAFKA-7918](https://github.com/apache/kafka/pull/6328)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-02-27T06:34:49Z","2019-03-06T19:02:49Z"
"","6420","KAFKA-3881: use plain topic tag in Fetcher metrics","Reverts topic.replace conversion of dots to underbars in topic values for Fetcher metrics, so topic values are consistent across metrics.  The original `topic.replace` code was introduced in https://github.com/apache/kafka/pull/939 following a pattern where the topic value would be prefixed with ""topic-"" and dots would be converted to underbars. This pattern was then partially backed out, leaving the dots conversion by mistake.  Note that all other metric collections use the topic value as-is without conversion, so this change should align topic values from Fetcher with other metric topic values. The failure to align only happens when the topic value has a dot, so is understandably hard to find otherwise.","open","","dbrinegar","2019-03-11T00:54:35Z","2019-03-16T22:53:00Z"
"","6090","Revert ""KAFKA-7657: Fixing thread state change to instance state change""","Reverts apache/kafka#6018 because it lacks one final commit.","closed","","guozhangwang","2019-01-04T22:40:53Z","2019-09-13T05:36:40Z"
"","6369","Minor resolve streams scala warnings","Resolves the compiler warnings when building streams-scala.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-03-04T21:17:14Z","2019-03-08T15:18:45Z"
"","5875","KAFKA-7576: Fix shutdown of replica fetcher threads","ReplicaFetcherThread.shutdown attempts to close the fetcher's Selector while the thread is running. This in unsafe and can result in `Selector.close()` failing with an exception. The exception is caught and logged at debug level, but this can lead to socket leak if the shutdown is due to dynamic config update rather than broker shutdown. This PR changes the shutdown logic to close Selector after the replica fetcher thread is shutdown, with a wakeup() and flag to terminate blocking sends first.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-11-02T23:07:52Z","2018-11-16T01:27:30Z"
"","5653","KAFKA-7413: Replace slave terminology with follower in website","Replace slave terminology with follower in website  Issue [KAFKA-7413](https://issues.apache.org/jira/browse/KAFKA-7413)","closed","","satybald","2018-09-14T11:31:53Z","2018-10-29T05:12:44Z"
"","6274","KAFKA-7935: UNSUPPORTED_COMPRESSION_TYPE if ReplicaManager.getLogConfig returns None","Replace `forall` with `exists`.  Added a unit test to `KafkaApisTest` that failed before the change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-15T09:12:14Z","2019-02-18T18:19:16Z"
"","5583","KAFKA-7353; Connect logs 'this' for anonymous inner classes","Replace 'this' reference in anonymous inner class logs to out class's 'this'  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kevin-laff","2018-08-28T20:27:36Z","2018-09-06T03:17:49Z"
"","5759","KAFKA-7277: default implementation for new window store overloads","Removes the necessity for every store implementation to provide identical, trivial overrides.  Also, helps ensure that stores won't forget to validate the `Instant` inputs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-08T16:58:04Z","2018-10-11T04:27:23Z"
"","6331","MINOR: Remove types from caching stores","Removes all type information (including Serdes) from caching stores and move this ""one level up"" (ie, into metered stores).  Impact:  - caching stores returns `` on flush  - flush callback must deserialize the data before forwarded to downstream operator  `TupleForwarded` registered ""typed"" `FlushListener` on ""metered stores"" now (not on the caching store), and ""metered stores"" registers a `byte[]`-`FlushListener` on the caching store. When caching store evicts and call the listener, the ""metered stores"" deserializes the bytes, and fires the original ""typed"" `FlushListener`.  This will unblock KIP-258 PR #6152 (ie, the open discussion about flushing...)   Call for review @guozhangwang @bbejeck @vvcephei @ableegoldman    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-02-26T20:18:57Z","2019-02-28T17:34:28Z"
"","6291","Update kafka-run-class.bat","Removed quotes from LogDir variable generation as there are additional quotes in Line 127.. This caused problems when those batch files are invoked from a path that contains space characters.","closed","","skombijohn","2019-02-19T12:01:01Z","2019-02-27T08:33:08Z"
"","5686","MINOR: Remove duplicate word","Removed a duplicated `provides` word  Since this is the removal of a single  word, visual inspection is enough  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-24T17:02:12Z","2018-09-24T17:55:23Z"
"","5605","MINOR: Enable ignored upgrade system tests - trunk","Removed `ignore` annotations from the upgrade tests.  This PR includes the following changes for updating the upgrade tests:  1. Uploaded new versions `0.10.2.2`, `0.11.0.3`, `1.0.2`, `1.1.1`, and `2.0.0` (in the associated scala versions) to kafka-packages 2. Update versions in `version.py`, `Dockerfile`, `base.sh` 3. Added new versions to `StreamsUpgradeTest.test_upgrade_downgrade_brokers` *_including_* version `2.0.0`  4. Added new versions `StreamsUpgradeTest.test_simple_upgrade_downgrade` test *_excluding_* version `2.0.0`      1. Version `2.0.0` is excluded from the streams upgrade/downgrade test as `StreamsConfig` needs an update for the new version, requiring a KIP. Once the community votes the KIP in, a minor follow-up PR can be pushed to add the `2.0.0` version to the upgrade test. 5. Fixed minor bug in `kafka-run-class.sh` for classpath in upgrade/downgrade tests across versions.  Follow on PRs for `0.10.2x`, `0.11.0x`, `1.0.x`, `1.1.x`, and `2.0.x` will be pushed soon with the same updates required for the specific version.  For testing, the following https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1942/ branch builder was executed and passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-03T20:19:23Z","2018-09-13T20:46:48Z"
"","5699","KAFKA-7429: Enable key/truststore update with same filename/password","Reload SSL trust stores and keystores on AlterConfigsRequest from the admin client if the file was modified, even if the file name and password haven't changed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-09-26T18:34:37Z","2018-10-02T19:57:45Z"
"","5529","KAFKA-7308: Fix rat and checkstyle config for Java 11 support","Relative paths in Gradle break when the Gradle daemon is used unless `user.dir` can be changed while the process is running. Java 11 disallows this, so we use project paths instead.  Verified that `rat` and `checkstyle` work with Java 11 after these changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-08-18T03:39:26Z","2018-08-18T15:18:32Z"
"","5607","MINOR:Updates for new versions and upgrade tests 1.0","Related to PR https://github.com/apache/kafka/pull/5605 this PR is for system test updates for new versions in branch 1.0  System test ran and passed https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1947/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-04T16:09:41Z","2018-09-06T17:08:48Z"
"","5834","refs KAFKA-7510, preventing data being leaked to logs by default","Refs KAFKA-7510 (https://issues.apache.org/jira/browse/KAFKA-7510).  This is a bare minimum implementation to get data out of the logs by default as explained in the Jira ticket. I've tried to keep the formatting consistent with the existing formatting.  A further follow up PR as part of the wider KAFKA-7510 discussion can add unit tests asserting that data is not present at ERROR log level.  Note I do get flickering tests but they look to be unrelated and non deterministic due to the parallelism although they do appear to be mostly be from `RocksDBWindowsStoreTest` when I do get failures.  ``` org.apache.kafka.streams.state.StoresTest > shouldCreateRocksDbWindowStore PASSED  org.apache.kafka.streams.state.StoresTest > shouldThrowIfILruMapStoreNameIsNull PASSED  org.apache.kafka.streams.state.StoresTest > shouldThrowIfIPersistentSessionStoreStoreNameIsNull PASSED  org.apache.kafka.streams.state.internals.RocksDBWindowStoreTest > testRestore SKIPPED  > Task :streams:test FAILED  FAILURE: Build failed with an exception.  * What went wrong: Execution failed for task ':streams:test'. > Process 'Gradle Test Executor 103' finished with non-zero exit value 134   This problem might be caused by incorrect test process configuration.   Please refer to the test execution section in the user guide at https://docs.gradle.org/4.10.2/userguide/java_plugin.html#sec:test_execution ```  Note I've also yet to find out the `gradle` task to publish the artifact to my local maven repository to test in my app, `maven-publish` plugin doesn't look to be enabled and not been able to dig in to the build scripts enough to get it to work. Using `assembly` and manually adding the JAR to the classpath to test is non trivial with my build pipeline/app deployment.","closed","","forficate","2018-10-24T03:16:53Z","2019-01-04T12:02:16Z"
"","6270","KAFKA-7736: Consolidate Map usages in TransactionManager","Refactors the various maps used in TransactionManager into one map to simplify bookkeeping of inflight batches, offsets and sequence numbers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-02-14T14:59:39Z","2019-03-19T09:17:31Z"
"","6271","MINOR: Refactor topic check to make sure all topics exist by name vs doing a topic count","Refactor the topic check to match the one done in PRs #6241, #6242, #6243, and #6243  The check verifies all topics exist by name vs. doing a sum of all topics found on the brokers    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-15T00:22:46Z","2019-02-21T01:03:05Z"
"","5779","KAFKA-6036: Local Materialization for Source KTable","Refactor the materialization for source KTables in the way that:  1. If Materialized.as(queryableName) is specified, materialize; 2. If the downstream operator requires to fetch from this KTable via ValueGetters, materialize; 3. If the downstream operator requires to send old values, materialize.  Otherwise do not materialize the KTable. E.g. `builder.table(""topic"").filter().toStream().to(""topic"")` would not create any state stores.  There's a couple of minor changes along with PR as well:  1. KTableImpl's `queryableStoreName` and `isQueryable` are merged into `queryableStoreName` only, and if it is null it means not queryable. As long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).  To achieve this, splitted `MaterializedInternal.storeName()` and `MaterializedInternal.queryableName()`. The former can be internally generated and will not be exposed to users. QueryableName can be modified to set to the internal store name if we decide to materialize it during the DSL parsing / physical topology generation phase. And only if queryableName is specified the corresponding KTable is determined to be materialized.  2. Found some overlapping unit tests among `KTableImplTest`, and `KTableXXTest`, removed them.  3. There are a few typing bugs found along the way, fixed them as well.  -----------------------  This PR is an illustration of experimenting a poc towards logical materializations.  Today we've logically materialized the KTable for filter / mapValues / transformValues if queryableName is not specified via Materialized, but whenever users specify queryableName we will still always materialize. My original goal is to also consider logically materialize for queryable stores, but when implementing it via a wrapped store to apply the transformations on the fly I realized it is tougher than I thought, because we not only need to support `fetch` or `get`, but also needs to support range queries, `approximateNumEntries`, and `isOpen` etc as well, which are not efficient to support. So in the end I'd suggest we still stick with the rule of always materializing if queryableName is specified, and only consider logical materialization otherwise.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-10-11T02:02:49Z","2020-04-25T00:06:55Z"
"","6255","KAFKA-7916: Unify store wrapping code for clarity","Refactor internal store wrapping for improved maintainability.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-11T19:11:03Z","2019-02-14T16:56:21Z"
"","5805","KAFKA-7513: Fix timing issue in SaslAuthenticatorFailureDelayTest","Reduce tick interval of the mock timer and avoid large timer increments to avoid hitting idle expiry on the client-side before delayed close is processed by the server. Also reduce poll interval in the server to make the test complete faster (since delayed close is only processed when poll returns).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-16T10:59:05Z","2018-10-16T16:15:22Z"
"","6448","KAFKA-7652: Restrict range of fetch/findSessions in cache","Reduce the total key space cache iterators have to search for segmented byte stores by wrapping several single-segment iterators.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-15T00:39:17Z","2020-06-26T22:39:23Z"
"","5929","KAFKA-7655 Metadata spamming requests from Kafka Streams under some circumstances, potential DOS","Re-validate and make sure the topic either exists or it's gone by using a delay.  There is a bug in the InternalTopicManager that makes the client believe that a topic exists even though it doesn't, it occurs mostly in those few seconds between when a topic is marked for deletion and when it is actually deleted. In that timespan, the Broker gives inconsistent information, first it hides the topic but then it refuses to create a new one therefore the client believes the topic was existing already and it starts polling for metadata.  The consequence is that the client goes into a loop where it polls for topic metadata and if this is done by many threads it can take down a small cluster or degrade greatly its performances.  The real life scenario is probably a reset gone wrong. Reproducing the issue is fairly simple, these are the steps:  Stop a Kafka streams application Delete one of its changelog and the local store Restart the application immediately after the topic delete You will see the Kafka streams application hanging after the bootstrap saying something like: INFO  Metadata - Cluster ID: xxxx    I am attaching a patch that fixes the issue client side but my personal opinion is that this should be tackled on the broker as well, metadata requests seem expensive and it would be easy to craft a DDOS that can potentially take down an entire cluster in seconds just by flooding the brokers with metadata requests.  The patch kicks in only when a topic that wasn't existing in the first call to getNumPartitions triggers a TopicExistsException. When this happens it forces the re-validation of the topic and if it still looks like doesn't exists plan a retry with some delay, to give the broker the necessary time to sort it out.  I think this patch makes sense beside the above mentioned use case where a topic it's not existing, because, even if the topic was actually created, the client should not blindly trust it and should still re-validate it by checking the number of partitions. IE: a topic can be created automatically by the first request and then it would have the default partitions rather than the expected ones.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Pasvaz","2018-11-19T15:32:51Z","2018-12-14T09:11:11Z"
"","6198","MINOR: release.py: fix some compatibility problems.","Rather than using sed, use built-in Python regular expressions to strip the SNAPSHOT expression from the pom.xml files.  Sed has different flags on different platforms, such as Linux.  Using Python directly here is more compatible, as well as being more efficient, and not requiring an rm command afterwards.  When running release_notes.py, use the current Python interpreter. This is needed to prevent attempting to run release_notes.py with Python 3 on some systems.  release_notes.py will not (yet) work with Python 3.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cmccabe","2019-01-24T23:28:32Z","2019-05-20T19:04:36Z"
"","5632","MINOR: Fix quota_test failure in 2.0 due to missing kafka 1.1.0","QuotaTest.test_quota with old_broker_throttling_behavior==true fails on 2.0 branch because it needs kafka 1.1.0 (V_1_1_0). However, we don't install kafka 1.1.0 for system tests in 2.0, only 1.1.1.   This PR installs kafka 1.1.0, to be consistent with trunk. And also uses LATEST_1_1, instead of V_1_1_0, in quota test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-09-10T03:56:55Z","2018-09-15T16:43:20Z"
"","5587","MINOR: Next round of fetcher thread consolidation","Pull the epoch request build logic up to `AbstractFetcherThread`. Also get rid of the `FetchRequest` indirection.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-29T17:47:26Z","2018-09-07T18:49:23Z"
"","6228","Merge pull request #2 from apache/trunk","pull  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hejiefang","2019-02-03T01:41:07Z","2019-02-03T01:43:59Z"
"","6075","Merge pull request #2 from apache/trunk","pull  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hejiefang","2018-12-29T01:52:10Z","2018-12-29T02:16:21Z"
"","5682","KAFKA-7277: Migrate Streams API to Duration instead of longMs times","Public API changed according to [KIP-358](https://cwiki.apache.org/confluence/display/KAFKA/KIP-358%3A+Migrate+Streams+API+to+Duration+instead+of+long+ms+times)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","nizhikov","2018-09-22T07:20:23Z","2018-10-04T20:51:40Z"
"","6340","KAFKA-8013: Avoid underflow when reading a Struct from a partially correct buffer","Protocol compatibility can be facilitated if a Struct, that has been defined as an extension of a previous Struct by adding fields at the end of the older version, can read a message of an older version by ignoring the absence of the missing new fields. Reading the missing fields should be allowed by the definition of these fields (they have to be nullable).  * Tested by adding unit tests around Schema.read in both directions   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-02-28T01:14:13Z","2020-10-16T06:19:04Z"
"","5809","MINOR: Prohibit setting StreamsConfig commit.interval.ms to a negative value","Prohibit setting StreamsConfig `commit.interval.ms` to a negative value to avoid possible ambiguity of what it indicates.  So far, setting the property to a negative value can be used to turn off periodic offset-commit by `StreamThread`. With this change, that will no longer work. Instead of using a negative value, `LONG.MAX_VALUE` can be used to achieve the almost same thing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","occho","2018-10-17T03:00:47Z","2018-11-12T07:19:34Z"
"","5939","KAFKA-7667: Synchronous send support for kafka performance producer java application","ProducerPerformance java application now supports synchronous blocking sends to produce messages to kafka brokers.  Till now, performance producer application supported only asynchronous sends with a callback function registered. this would be enough when trying to load the kafka brokers without bothering much on the replication & producer responses.  But when kafka brokers replication capacity has to be loaded, asynchronous sends would not suffice. So synchronous blocking send calls are now supported with all the existing metrics calculation of producer performance.  A boolean command line flag --synchronous-send has been added to enable/disable this feature. Blocking .get() calls are now done upon each send, and as of this commit ResultMetadata is not being used or logged. A new function has been introduced in the Stats class to update and get the iteration counter.   Tests done:  Comparison tests were done with and without synchronous sends.  below were the configurations of kafka cluster: No of brokers: 3 CPU per broker: 10 cores Memory : 32GB Topic partitions : 1 replication factor : 3 min.insync.replicas: 1 (default). producer acks level : 1(default),  response after only leader write.  Similar configurations were given for performance producer with & without synchronous sends enabled. It was observed that, synchronous producer was producing records approximately 5 times slower than asynchronous producer but with guarantee of replication before subsequent sends.  So this clearly indicates the impact of replication in brokers. And these tests would help optimise resources in  kafka broker from replication performance point of view.  Attached results snapshot of performance producer run with/without synchronous send flag enabled.","open","","kaushiksrinivas","2018-11-22T09:46:43Z","2018-12-16T05:53:14Z"
"","6339","Embedded ConfigDef Validator issue","private static embeddedValidator should return an Anonymous Object instead of lambda  Using the lambda expression will cause issues with: toRst() toEnrichedRst() toHTMLTable()  The lambda expression has no appropriate toString() override resulting in RST or HTML like:  ```  * Valid Values: org.apache.kafka.common.config.ConfigDef$$Lambda$27/785447854@29176cc1```","closed","","chasewalden","2019-02-27T20:12:51Z","2019-04-29T19:55:30Z"
"","6419","KAFKA-7858: Replace JoinGroup request/response with automated protocol","Prioritizing this migration because we have blocking feature from KIP-345 part 1: https://github.com/apache/kafka/pull/6177  Upgrade join group protocols could ease the process of adding group instance id towards JoinGroupResponse.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-03-10T18:52:50Z","2019-03-18T21:36:08Z"
"","5707","MINOR: Switch to use AWS spot instance to save 76% in cost","Pricing for m3.xlarge: On-Demand is at $0.266. Reserved is at about $0.16 (40% discount). And Spot is at $0.0627 (76% discount relative to On-Demand, or 60% discount relative to Reserved). Insignificant fluctuation in the past 3 months.  Ran on branch builder and works as expected -- each worker is created using spot instances (https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1982/console)  This can be safely backported to 0.10.2 (tested using https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1983/)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","maxzheng","2018-09-27T21:27:53Z","2018-10-05T17:23:18Z"
"","5590","MINOR: single Jackson serde for PageViewTypedDemo","Previously, we depicted creating a Jackson serde for every pojo class, which becomes a burden in practice. There are many ways to avoid this and just have a single serde, so we've decided to model this design choice instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-29T22:00:48Z","2018-09-04T16:13:57Z"
"","6336","KAFKA-7912: Support concurrent access in InMemoryKeyValueStore","Previously the InMemoryKeyValue store would throw a ConcurrentModificationException if the store was modified beneath an open iterator. The TreeMap implementation was swapped with a ConcurrentSkipListMap for similar performance while supporting concurrent access.  Added one test to AbstractKeyValueStoreTest, no existing tests caught this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-02-27T07:42:20Z","2019-02-28T22:47:51Z"
"","6062","MINOR: standby task test throughput too low 2.0","Previous PR #6043 reduced throughput for VerifiableProducer in base class, but the streams_standby_replica_test needs higher throughput for consumer to complete verification in 60 seconds. Same update as #6060 and #6061   Kicked off branch builder with 25 repeats https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2203/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-21T23:35:01Z","2018-12-22T06:41:29Z"
"","6061","MINOR: Increase throughput too slow for consumer to read within timeout","Previous PR #6043 reduced throughput for VerifiableProducer in base class, but the streams_standby_replica_test needs higher throughput for consumer to complete verification in 60 seconds. Same update as #6060  For testing kicked off branch builder with 25 repeats https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2202/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-21T23:31:14Z","2018-12-22T06:35:17Z"
"","6060","MINOR: Increase throughput for VerifiableProducer in test","Previous PR #6043 reduced throughput for `VerifiableProducer` in base class, but the `streams_standby_replica_test` needs higher throughput for consumer to complete verification in 60 seconds  Update system test and kicked off branch builder with 25 repeats https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2201/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-12-21T23:27:16Z","2018-12-22T06:32:20Z"
"","6360","KAFKA-7312: Change broker port used in testMinimumRequestTimeouts and testForceClose","Port 22 is used by ssh, which causes the AdminClient to throw an OOM:  > java.lang.OutOfMemoryError: Java heap space > 	at java.nio.HeapByteBuffer.(HeapByteBuffer.java:57) > 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) > 	at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30) > 	at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:112) > 	at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:424) > 	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:385) > 	at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:640) > 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:561) > 	at org.apache.kafka.common.network.Selector.poll(Selector.java:472) > 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:535) > 	at org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.run(KafkaAdminClient.java:1140) > 	at java.lang.Thread.run(Thread.java:748) >  >   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","omkreddy","2019-03-03T07:37:28Z","2019-03-05T04:17:03Z"
"","5899","MINOR: Replace deprecated consumer poll() and close() calls in Scala code","Plus, minor code cleanup.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-11-11T07:11:04Z","2018-11-12T15:36:57Z"
"","5545","MINOR: Fixed couple of warnings","Plus some test coverage in ACL command  Co-authored-by: Mickael Maison  Co-authored-by: Gantigmaa Selenge    *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tinaselenge","2018-08-21T11:34:11Z","2018-08-21T13:32:22Z"
"","5498","KAFKA-7283: Enable lazy mmap on index files and skip sanity check for segments below recovery point","Per the KIP-263 discussion, we think we can improve broker restart time by avoiding performing costly disk operations when sanity checking index files for segments below recovery point on broker startup.  This PR includes the following changes: 1. Mmap the index file and populate fields of the index file on-demand rather than performing costly disk operations when creating the index object on broker startup. 2. Skip sanity checks on the time index and offset index of segments.    1) For segment with offset below the flushed point (recovery point), these segments are safely flushed so we don't need to sanity check the index files. if there are indeed data corruption on disk, given that we don't sanity check the segment file, sanity checking only the indexes adds little benefit.    2) For segment with offset above the flushed point (recovery point), we will recover these segments in `recoveryLog()` (Log.scala) in any case so sanity checking the index files for these segments is redundant.  We did experiments on a cluster with 15 brokers, each of which has ~3k segments (and there are 31.8k partitions with RF=3 which are evenly distributed across brokers; total bytes-in-rate is around 400 MBps). The results show that rolling bounce time reduces from 135 minutes to 55 minutes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-08-13T20:10:39Z","2019-02-28T17:23:40Z"
"","5858","KAFKA-7572: Producer should not send requests with negative partition id","Partition id should never be a negative value. This commit will make debug easier, when custom Partitioner generate an invalid negative partition id.","closed","","ghost","2018-10-30T20:08:46Z","2022-02-04T01:41:49Z"
"","6170","KAFKA-3522: Generalize Segments","Part of KIP-258: In order to support old and new segments (ie, without timestamps and with timestamps), the code is restructured:  - splitting `Segment` into interface `Segment` and `KeyValueSegment` class - splitting `Segments` into `AbstractSegments ` and `KeyValueSegments`  This is no functional change. This is also completely internal and we can merge before the KIP is accepted.","closed","kip,","mjsax","2019-01-18T21:32:42Z","2020-06-12T23:52:36Z"
"","6149","KAFKA-3522: Add RocksDBTimestampedStore","Part of KIP-258. adds only internal classes (we can merge right away).","closed","kip,","mjsax","2019-01-15T22:55:48Z","2020-06-12T23:53:06Z"
"","6179","KAFAK-3522: Add TopologyTestDriver unit tests","Part of KIP-258.   - For backward compatibility, we need to return the correct store via IQ API - Add additional unit test for #6175","closed","streams,","mjsax","2019-01-20T20:12:00Z","2019-05-16T15:16:07Z"
"","6173","KAFKA-3522: Add TimestampedWindowStore builder/runtime classes","Part of KIP-258.  This PR adds  - TimestampedWindowStore - MeteredTimestampedWindowStore - ChangeLoggingTimestampedWindowByteStore - TimestampedWindowStoreBuilder","closed","kip,","mjsax","2019-01-19T01:33:33Z","2020-06-12T23:48:26Z"
"","6152","KAFKA-3522: Add TimestampedKeyValueStore builder/runtime classes","Part of KIP-258.  This is mostly internal changes, however depends on public interfaces that are added via #6151). This PR contains some duplication to #6151 to make it compile:  - KeyValueWithTimestampStore  - ValueAndTimestamp  - ValueAndTimestampSerializer / ValueAndTimestampDeserializer  Also some refactoring for the three caching stores to allow fewer code for newly added session store. Similar for logging store.  The actual change of this PR is to add  - KeyValueWithTimestampStoreBuilder  - MeteredKeyValueWithTimestampStore  - CachingKeyValueWithTimestampStore  - ChangeLoggingKeyValueWithTimestampStore","closed","kip,","mjsax","2019-01-16T00:53:11Z","2020-06-12T23:48:33Z"
"","6150","KAFKA-3522: Add internal RecordConverter interface","Part of KIP-258.  Adding the proposed `RecordConverter` interface.  The `RecordConverter` is responsible to convert `ConsumerRecords` from a changelog topic into `` key-value-pairs that are put into the stores. This is required to move the timestamp from the `ConsumerRecord` into the value-part in the store. The default implementation only maps key-value to key-value, ie, it's a no-op and this PR does not change any behavior.","closed","kip,","mjsax","2019-01-15T23:14:38Z","2020-06-12T23:52:59Z"
"","6186","KAFKA-3522: Add RocksDBTimestampedSegmentedBytesStore","Part of KIP-258.","closed","kip,","mjsax","2019-01-23T06:23:54Z","2020-06-12T23:52:23Z"
"","6151","KAFKA-3522: Add in-memory TimestampedKeyValueStore","Part of KIP-258.","closed","kip,","mjsax","2019-01-16T00:13:54Z","2020-06-12T23:52:50Z"
"","5567","KAFKA-7223: Suppress API with only immediate emit","Part 1 of the suppression API.  * add the DSL `suppress` method and config objects * add the processor, but only in ""identity"" mode   (i.e., it will forward only if the suppression spec says to forward immediately) * add tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-24T22:33:01Z","2018-09-24T20:50:58Z"
"","6210","KAFKA-7885: TopologyDescription violates equals-hashCode contract.","Overwrote hash code implementation in StaticTopicNameExtractor. Also overwrote `equals` to pass checkstyle. Added a unit test confirming equals-hashCode contract.","closed","streams,","jCalamari","2019-01-30T19:43:59Z","2020-04-15T22:50:40Z"
"","5636","MINOR: Use LATEST_1_1 instead of V_1_1_0 in quota_test","Opening a separate PR against trunk for changes in quota_test.py to use latest 1.1 release, so that it could also be merged to previous branches. The original PR was against 2.0 which also added installing 1.1:: https://github.com/apache/kafka/pull/5632. Will decide separately which bug releases we should install for system tests in 2.0 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-09-11T00:00:54Z","2018-09-13T14:17:02Z"
"","5768","MINOR: Refactor code for restoring tasks","Only StreamTasks can be restored. Thus, moving restoring logic into `AssignedStreamTasks` class  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-10-09T22:20:36Z","2018-11-23T20:32:40Z"
"","6399","[KAFKA-3729] Auto-configure non-default SerDes passed alongside the topology builder","Only default serdes provided through configs are auto-configured today. But we could auto-configure other serdes passed alongside the topology builder as well.  Existing test suite  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2019-03-07T23:22:54Z","2019-03-18T03:04:44Z"
"","6461","[KAFKA-3729] Auto-configure non-default SerDes passed alongside the topology builder","Only default serdes provided through configs are auto-configured today. But we could auto-configure other serdes passed alongside the topology builder as well.  Added new tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2019-03-18T03:04:21Z","2019-04-20T18:30:55Z"
"","6223","MINOR: trogdor README code block formatting fix","One of the code blocks wasn't formatted correctly. Also added some backticks in relevant places and a small phrasing change to make readme easier to follow.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gracechang","2019-02-02T01:01:25Z","2019-05-12T22:13:25Z"
"","5998","KAFKA-7704: MaxLag.Replica metric is reported incorrectly","On the follower side, for the empty `LogAppendInfo` retrieved from the leader, fetcherLagStats set the wrong lag for fetcherLagStats due to `nextOffset` is zero in this case where it actually means no lagging, so the lag should be set to 0 if `nextOffset` is 0 or `logAppendInfo.lastOffset` is -1.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-12-04T03:18:06Z","2018-12-14T02:30:09Z"
"","5733","KAFKA-7462: Make token optional for OAuthBearerLoginModule","OAuthBearerLoginModule is used both on the server-side and client-side (similar to login modules for other mechanisms). OAUTHBEARER tokens are client credentials used only on the client-side to authenticate with servers, but the current implementation requires tokens to be provided on the server-side even if OAUTHBEARER is not used for inter-broker communication. Tokens should be optional for server-side login context to allow brokers to be configured without a token when OAUTHBEARER is not used for inter-broker communication.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-03T13:19:10Z","2018-10-08T09:20:19Z"
"","5534","Minor: set task to null at the end of shouldWrapProducerFencedExceptionWithTaskMigragedExceptionInSuspendWhenCommitting","Noticed the following in test output: ``` org.apache.kafka.streams.processor.internals.StreamTaskTest > shouldWrapProducerFencedExceptionWithTaskMigragedExceptionInSuspendWhenCommitting STANDARD_OUT     [2018-08-20 17:30:10,706] ERROR task [0_0] Could not close task due to the following error: (org.apache.kafka.streams.processor.internals.StreamTask:660)     java.lang.NullPointerException       at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.close(RecordCollectorImpl.java:252)       at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:538)       at org.apache.kafka.streams.processor.internals.StreamTask.close(StreamTask.java:656)       at org.apache.kafka.streams.processor.internals.StreamTaskTest.cleanup(StreamTaskTest.java:176) ``` This PR adds null check for producer in RecordCollectorImpl#close . ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-08-20T17:46:13Z","2018-08-21T20:40:52Z"
"","6167","MINOR: Remove the InvalidTopicException handling in InternalTopicManager","Note we can only remove this handling in 2.2 but not in 2.1 since https://github.com/apache/kafka/pull/6124 is only in 2.2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-01-18T17:51:19Z","2019-01-18T19:37:00Z"
"","5749","KAFKA-7415; Persist leader epoch and start offset on becoming a leader","Note this is a backport of #5678 for 1.1  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-05T18:19:39Z","2018-10-05T22:01:31Z"
"","5511","[MINOR] Improve docs by adding ToC links to Monitoring","My top 2 reasons for visiting the Kafka docs are to: - View configurations - View metrics   This PR aims to improve the user experience for viewing metrics: - Add href links to the `Monitoring` section of the Table of Contents so users do not need to scroll or Ctrl/Cmd-F to find specific metric details (Monitoring section has grown large as more component & metrics are added)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2018-08-15T20:19:27Z","2018-11-28T06:45:41Z"
"","6355","KAFKA-7963: Extract hard-coded Streams metric name strings to centralized place","Moved hard-coded 'expired-window-record-drop' and 'late-record-drop' to static Strings in StreamsMetricsImpl  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-02T00:23:47Z","2019-03-15T23:57:02Z"
"","6026","KAFKA-7723: Support override kafka connect worker api configuration with rest api","More details https://issues.apache.org/jira/browse/KAFKA-7723  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sweat123","2018-12-12T03:40:53Z","2019-03-20T15:18:12Z"
"","6074","Replace mockito-core by mockito-scala","Mockito now has a Scala version that improves the API and solves longstanding issues, I've made the required changes to use the scala version and new API","closed","","bbonanno","2018-12-28T19:22:19Z","2019-03-04T15:32:33Z"
"","6166","KAFKA-7839 Add quotes to CLASSPATH on Java start","Missing quotes around CLASSPATH break kafka-run-class.sh when CLASSPATH contains only a single component with wildcards. For detailed description of the problem see Jira ticket.  This (trivial) contribution is my original work and I license it to the project under the project's open source license.","open","","rwunderer","2019-01-18T08:45:39Z","2019-12-16T18:29:54Z"
"","5692","[KStreams docs] Wording fix","Minor wording fix; not sure regarding polices whether that needs a JIRA issue?","closed","","gunnarmorling","2018-09-25T19:51:20Z","2020-02-20T11:35:28Z"
"","5755","MINOR: Missing punctuation marks in quickstart","Minor fix for missing punctuation marks in the quickstart.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vitojeng","2018-10-07T15:03:12Z","2019-07-02T04:02:15Z"
"","6221","KAFKA-7831; Do not modify subscription state from background thread","Metadata may be updated from the background thread, so we need to protect access to SubscriptionState. This patch restructures the metadata handling so that we only check pattern subscriptions in the foreground. Additionally, it improves the following:  1. SubscriptionState is now the source of truth for the topics that will be fetched. We had a lot of messy logic previously to try and keep the the topic set in Metadata consistent with the subscription, so this simplifies the logic. 2. The metadata needs for the producer and consumer are quite different, so it made sense to separate the custom logic into separate extensions of Metadata. For example, only the producer requires topic expiration. 3. We've always had an edge case in which a metadata change with an inflight request may cause us to effectively miss an expected update. This patch implements a separate version inside Metadata which is bumped when the needed topics changes. 4. This patch removes the MetadataListener, which was the cause of https://issues.apache.org/jira/browse/KAFKA-7764.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-02T00:15:46Z","2019-03-08T00:29:20Z"
"","5674","MINOR: fix build from f1f71921","Merging https://github.com/apache/kafka/commit/f1f719211e5f28fe5163e65dba899b1da796a8e0 broke the scala 2.12 build. This fixes it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-21T18:07:44Z","2018-09-21T23:00:06Z"
"","5780","MINOR: Refactor Log layer","Log segments are an implementation detail that should ideally not be leaked to components outside log. This patch makes such Log APIs private and provides alternate APIs where necessary.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-10-11T02:05:08Z","2019-03-07T01:22:32Z"
"","5949","MINOR: Fix flaky TransactionsTest","Log loading takes longer than 60s when the broker is bounced. Increasing the timeout to 120s.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-11-26T17:35:17Z","2019-02-10T05:36:25Z"
"","6292","KAFKA-7933: Switch from persistent to in-memory store in KTableKTablLeftJoinTest","Local test runtime drops from 15 sec to about 1 second for `shouldNotThrowIllegalStateExceptionWhenMultiCacheEvictions()`","closed","streams,","mjsax","2019-02-19T17:09:03Z","2019-02-21T07:18:03Z"
"","6022","KAFKA-7719: Improve fairness in SocketServer processors (KIP-402)","Limit the number of new connections processed in each iteration in `SocketServer` on each `Processor`. Block `Acceptor` if the connection queue is full on all Processors. Added a metric to track accept idle time percent. See KIP-402 for details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-11T11:18:55Z","2019-02-01T14:02:26Z"
"","6280","MINOR: Adjust Streams parameter hint on TimeoutException","KIP-91 was included in Kafka 2.1.0, so we should mention `delivery.timeout.ms` in the hint which is more likely to help.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ijuma","2019-02-17T04:12:44Z","2019-07-03T04:14:10Z"
"","5731","KAFKA-7223: Add name config to Suppressed","KIP-372 (allow naming all internal topics) was designed and developed concurrently with suppression.  Since suppression introduces a new internal topic, it also needs to be nameable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-03T00:38:08Z","2018-10-04T15:35:38Z"
"","5667","KAFKA-7391 Introduce close(Duration) to Producer and AdminClient inst…","KIP-367: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070496  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-09-20T09:41:57Z","2019-01-15T16:48:33Z"
"","5794","MINOR: Streams Update for KIP-330 / KIP-356","KIP-330 / 356 are very trivial changes so I merged them together in one PR.  A couple of other changes:  * bump up the version number, and use templates in the upgrade section for the latest version. * reordered some of the API changes for 2.0 to after the 2.1 section.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","guozhangwang","2018-10-12T17:58:04Z","2020-06-12T23:57:29Z"
"","5921","KAFKA-4453 : Added code to separate controller connections and requests from the data plane","KIP-291 Implementation : Added code to separate controller connections and requests from the data plane.  - Tested with local deployment that the controller request are handled by the control plane and other requests are handled by the data plane. - Also added unit tests in order to test the functionality.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","MayureshGharat","2018-11-16T18:36:15Z","2019-01-15T02:02:15Z"
"","5631","MINOR: Mark new Scala streams tests as integration tests (KIP-270 follow-up)","KIP-270 added some integration tests, but they were not marked with the appropriate annotation and bloat the `unitTest` target. This annotates them properly so we get faster unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-09-10T01:37:04Z","2018-09-10T16:53:03Z"
"","5723","KAFKA-7468 No need to startup the kafkaScheduer in KafkaController when auto leader rebalance is false","KafkaController has its inner member KafkaScheduler instance to schedule auto leader rebalance tasks. But when the auto rebalance config set false, no need to startup the kafkaScheduler, because that created a non-used ScheduledThreadPoolExecutor with one thread.","open","","xiaoxiaogua","2018-10-02T03:59:30Z","2018-10-02T03:59:30Z"
"","6141","MINOR: Fix typo in security.html","kafka-delegation-tokens.sh is the name of the script  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2019-01-14T11:03:52Z","2019-01-14T16:04:26Z"
"","6298","KAFKA-7959: Delete leader epoch cache files with old message format versions","KAFKA-7897 fixed a critical bug where replica followers would inadequately use the leader epoch cache for truncating their logs upon becoming a follower. The root of the issue was that a regression in KAFKA-7415 caused the leader epoch cache to be populated upon becoming a leader, even if the message format was older and did not support epoch caches. This resulted in very sparsely populated caches which the brokers would make use of when becoming a follower, resulting in huge log truncations.  KAFKA-7897 fixed that problem by not updating the leader epoch cache if the message format does not support it. It was merged all the way back to 1.1 but due to significant branch divergence the patches for 2.0 and below were simplified. As said in the commit: > Note this is a simplified fix than what was merged to trunk in #6232 since the branches have diverged significantly. Rather than removing the epoch cache file, we guard usage of the cache with the record version.  Due to the previous problem, we still had the sparsely populated epoch cache file present. This results in the same bug being hit at a different time. When the message format gets upgraded to support the leader epoch cache, brokers start to make use of it. This results in the same large truncations we saw in KAFKA-7897.  The key difference is that the patches for 2.1 and trunk deleted the non-empty leader epoch cache files if the log message format did not support it. We should update the earlier versions to do the same thing. That way, users that have upgraded to 2.0.1 but are still using the old message formats/protocol will have their epochs cleaned up on the first roll that upgrades the `inter.broker.protocol.version`","closed","","stanislavkozlovski","2019-02-21T09:26:45Z","2019-02-22T22:56:09Z"
"","5973","KAFKA-7650: make auto.create.topics.enable dynamically configurable","KAFKA-7650: make auto.create.topics.enable dynamically configurable  tested auto.create.topics.enable can be dynamically configured after change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","xiowu0","2018-11-29T21:36:19Z","2018-12-13T05:10:35Z"
"","5902","Kafka_7615","KAFKA-7615  Currently mirrormaker only supports same topic name in source and destination broker. Support for different topic names in source and destination brokers is needed.  MirrorData from topicA to topicB","closed","","adeetikaushal","2018-11-12T05:48:39Z","2022-02-10T16:19:51Z"
"","5853","KAFKA-7561: Increase stop_timeout_sec to make ConsoleConsumerTest pass","KAFKA-7561: Increase stop_timeout_sec to make ConsoleConsumerTest pass https://issues.apache.org/jira/browse/KAFKA-7561 Tests are failing in ducktape for the ConsoleConsumerTest. While the test itself is passing,  it ends up in a failed state due to the consumer failing to shutdown in time. This is blocking the 2.1 release currently and I would like to get it into the 2.1 branch.  However I'm not sure why the consumer is taking extra time to shut down. Viewing the consumer logs did not show a large amount of time between starting and finishing shutdown in the passing case. I believe there is some extra time being introduced establishing the connection to the docker container.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2018-10-30T04:43:26Z","2018-11-03T17:06:18Z"
"","5694","KAFKA-7441; Allow LogCleanerManager.resumeCleaning() to be used concurrently","KAFKA-7441; Allow LogCleanerManager.resumeCleaning() to be used concurrently  The following race condition can happen: 1) log retention set a topic partition to paused state 2) topic deletion come and see it is already in paused state and proceed 3) topic deletion removed the paused state 4)log retention tries to resume the same topic partition from a NONE state and throw out an exception.   In order to fix this situation, we allow a topic partition to be paused multiple times. Two unit tests are added to verify the fix.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","xiowu0","2018-09-26T00:48:11Z","2018-10-05T00:06:05Z"
"","5651","KAFKA-7409: Validate topic configs prior to topic creation","KAFKA-7409: Validate topic configs prior to topic creation https://issues.apache.org/jira/browse/KAFKA-7409  Values for `message.format.version` and `log.message.format.version` should be verfied before topic creation or config change.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-09-14T01:28:31Z","2018-09-28T05:39:45Z"
"","5839","KAFKA-7402: Kafka Streams should implement AutoCloseable where approp…","KAFKA-7402: Kafka Streams should implement AutoCloseable where appropriate. KIP 376: https://cwiki.apache.org/confluence/display/KAFKA/KIP-376%3A+Implement+AutoClosable+on+appropriate+classes+that+want+to+be+used+in+a+try-with-resource+statement  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shunge","2018-10-25T08:35:06Z","2018-11-16T23:59:00Z"
"","5758","KAFKA-6448 Mx4jLoader#props.getBoolean(""kafka_mx4jenable"", false) conflict with the annotation","KAFKA-6448 Mx4jLoader#props.getBoolean(""kafka_mx4jenable"", false) conflict with the annotation   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","piaosama","2018-10-08T01:43:56Z","2018-10-08T01:43:56Z"
"","5807","KAFKA-7501: fix producer batch double deallocation when receiving message too large error on expired batch","Kafka will try to deallocate a batch twice and throw IllegalStateException when a ""MESSAGE_TOO_LARGE"" response arrived after the inflight batch is expired. This patch fixes the issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xiowu0","2018-10-16T18:01:20Z","2018-10-22T17:44:38Z"
"","6039","MINOR: Update README.md","Kafka requires Gradle 5+  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-12-17T11:39:40Z","2018-12-18T00:30:14Z"
"","6430","MINOR: Use Java 8 lambdas in KStreamImplTest","Just a minor cleanup to use Java 8 lambdas vs anonymous classes in this test.  I ran all tests in the streams test suite ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-11T22:38:18Z","2019-03-12T20:35:26Z"
"","6248","MINOR: Update JUnit to 4.13 and annotate log cleaner integration test","JUnit 4.13 fixes the issue where `Category` and `Parameterized` annotations could not be used together. It also deprecates `ExpectedException` and `assertThat`. Given this, we: - Replace `ExpectedException` with the newly introduced `assertThrows`. - Replace `Assert.assertThat` with `MatcherAssert.assertThat`. - Annotate `AbstractLogCleanerIntegrationTest` with `IntegrationTest` category.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-10T08:06:02Z","2019-02-12T06:06:27Z"
"","5570","KAFKA-7310: Fix testUnsupportedCiphers to work with Java 11","Java 11 supports TLS 1.3 which has different cipher names than previous TLS versions so the simplistic way of choosing ciphers is not guaranteed to work. Fix it by configuring the context to use TLS 1.2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-08-25T07:19:46Z","2018-09-11T06:46:38Z"
"","5586","KAFKA-7338: Specify AES-128 default encryption type for Kerberos tests","Java 11 enables `aes128-cts-hmac-sha256-128` and `aes256-cts-hmac-sha384-192` by default. These are not supported in earlier versions of Java and not supported by Apache DS libraries used by MiniKdc. To ensure that the default kerberos configuration used by Kafka integration and system tests work with all versions of Java 8 and above, configure `default_tkt_enctypes` and `default_tgs_enctypes` to use `aes128-cts-hmac-sha1-96`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-29T14:28:54Z","2018-08-30T15:19:34Z"
"","5568","KAFKA-7309: Upgrade Jacoco for Java 11 support","Jacoco 0.8.2 adds Java 11 support:  https://github.com/jacoco/jacoco/releases/tag/v0.8.2  Java 11 RC1 is out so it would be good for us to get a working CI build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-08-25T05:16:58Z","2018-09-11T06:46:42Z"
"","6038","KAFKA-7717: Optimize EndToEndLatency tool for better arguments description.","It's better to use `OptionParser` for arguments description which can avoid ambiguous description.","open","","murong00","2018-12-17T10:09:48Z","2018-12-17T10:09:48Z"
"","6143","[WIP] KAFKA-7805: Ducktape should use --bootstrap-server on topic creation","It was tested by running the sanity checks in Cloudera's infrastructure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-01-14T15:00:06Z","2020-04-30T09:24:42Z"
"","6379","MINOR: Update Zookeeper version in documentation to 3.4.13 which is currently used by Kafka","It seems that the current version of the [documentation](http://kafka.apache.org/documentation/#zkversion) still references Zookeeper 3.4.9 as the current version. But we are now using Zookeeper 3.4.13. This PR updates the version to 3.4.13.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","scholzj","2019-03-06T13:42:00Z","2019-12-19T15:32:46Z"
"","5837","fix startup on windows","It seems a tipo was made on a avariable for the log dir with commit 81e789ae3dc6ea8369db181c5aef440491d74f19. Then Windows tries to access by default the /log directory which of cause not exists. @klesta490 Is it ok so? Or was the tilde ~ intentional?  To test:  On Windows with the a fresh downloaded Kafka, adapt the properties: * dataDir in config/zookeeper.properties with what you want, windows-compatible * log.dirs in config/server.properties with what you want, windows-compatible and executes: bin\windows\zookeeper-server-start.bat config\zookeeper.properties","closed","","florianhof","2018-10-24T13:45:41Z","2018-10-28T16:58:37Z"
"","5885","KAFKA-7597: Make Trogdor ProduceBenchWorker support transactions","It now accepts a new ""messagesPerTransaction"" field, which, if present, will enable transactional functionality in the bench worker. The producer will open N transactions with X messages each (bounded by the mandatory ""maxMessages"" field)","closed","","stanislavkozlovski","2018-11-06T13:58:56Z","2018-11-27T20:49:54Z"
"","5945","This is a partial fix to the retention mechanism on Windows.","It makes sure to close a segment file handles prior to deletion trial. Why is it partial? As it cannot work together will the log compaction policy.  Should work with a similar to the following configuration:  log.segment.bytes=1048576 log.retention.bytes= 5485760 log.retention.minutes = 1 log.retention.check.interval.minutes = 1 log.cleanup.policy = delete log.cleaner.enable = false  log.roll.ms = 10000 <= Can be specified but will not cause any failure without the above two lines of configuration (log.cleanup.policy, log.cleaner.enable)  * The rationale behind providing this fix is that even while it doesn't solve **all the windows platform related problems**, it does **enable the usage of the retention mechanism** (i.e. for windows as well) - thus providing a **more** cross platform deployment capabilities.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kobihikri","2018-11-26T12:45:56Z","2018-12-02T14:30:33Z"
"","6443","KAFKA-8091; Remove unsafe produce from dynamic listener update test","It is difficult to guarantee that a record produced before listener is removed won't be consumed after listener is removed since consumer sends a fetch request in advance. Hence removing the extra produce from the test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-14T10:08:21Z","2019-03-14T19:29:34Z"
"","6309","MINOR: Change Trogdor agent's cleanup executor to a cached thread pool","It is best to use a growing thread pool for worker cleanups. This lets us ensure that we close workers as fast as possible and not get slowed down on blocking cleanups.","closed","","stanislavkozlovski","2019-02-22T11:58:01Z","2019-03-12T15:31:51Z"
"","5879","[KAFKA-7380] Global thread restore blocks KafkaStreams#start()","It has been noted that KafkaStreams#start()'s behavior is somewhat different from what is described in its description. When global stores has not finished restoring yet, we are forced to wait until they are done. To fix this, StreamThreads will be put into wait status until the global stores has finished restoring, then  it will resume.","closed","streams,","ConcurrencyPractitioner","2018-11-04T17:58:42Z","2019-01-02T03:13:56Z"
"","5703","expose full list of offsets requested via OffsetFetchRequest","It does not make much sense to have an internal implementation which can request the full list of Topic offsets and do not expose it. I added methods similar to KafkaConsumer.committed with Collection of topics as argument  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","h0nIg","2018-09-27T11:13:53Z","2018-09-27T16:50:06Z"
"","6036","MINOR: Include additional detail in fetch error message","Issues such as KAFKA-7656 would be easier to debug if the error message included additional detail about the fetch request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-12-15T12:09:09Z","2018-12-17T23:51:02Z"
"","5559","1.1","issues ? Xmlipcregsvc-> 172.18.58.184:60686 (CLOSE_WAIT) has many such ports to close waiting, which is the application connection side.Why wait?Memory nor recycling three services are 2 nuclear 4 gb of memory, this before is kafka3G, found that memory, the heap memory, and then I will limit kfaka up to 2 g, but the master node to run after a period of time, and submitted to the heap memory and heap memory leak, I free -m looked at it and really have 100 MB of memory, I don't know where memory use, kafka made up 80% of the process of memory, CPU by more than 100%, what reason is this?The configuration parameters have been checked with the official website. The default is not acceptable.  XmlIpcRegSvc->172.18.58.184:60686 (CLOSE_WAIT) 有很多这个样的端口关闭等待，这是应用连接端。为什么一直等待呢？内存也没有回收 我3台服务是 2核 4G 内存，这之前给的是kafka3G,发现内存没了，报堆外内存溢出，然后我就限制kfaka最大为2G，但主节点跑一段时间后，又报堆内存溢出和堆外内存溢出，我free -m看了一下，内存确实还有100MB了，不知内存用在那里，kafka 这个进程暂用完了 80%的内存，cpu 100%多了，这是什么原因呢?配置参数和官网核对了一下，全用默认的也不行，  ` 1772 liandong  20   0 6398984 2.146g  16112 S 101.3 58.0  93:59.72 /usr/local/jdk1.8/bin/java -Xmx2G -Xms1G -server -XX:+UseG1GC -XX:+HeapDumpOnOutOfMemoryError -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -XX:MaxDirectMemorySize=512m -Xloggc:/data/kafka/bin/../logs/kafkaSer+...`  kafka server.log log `[2018-08-23 07:56:11,788] INFO [GroupCoordinator 0]: Stabilized group consumer.web.log generation 268 (__consumer_offsets-24) (kafka.coordinator.group.GroupCoordinator) [2018-08-23 07:56:12,054] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.OutOfMemoryError: Java heap space [2018-08-23 07:56:13,846] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.OutOfMemoryError: Java heap space [2018-08-23 07:56:15,673] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.OutOfMemoryError: Direct buffer memory   at java.nio.Bits.reserveMemory(Bits.java:694)   at java.nio.DirectByteBuffer.(DirectByteBuffer.java:123)   at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311)   at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:241)   at sun.nio.ch.IOUtil.read(IOUtil.java:195)   at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)   at org.apache.kafka.common.network.PlaintextTransportLayer.read(PlaintextTransportLayer.java:104)   at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:145)   at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:93)   at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:235)   at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:196)   at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:557)   at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:495)   at org.apache.kafka.common.network.Selector.poll(Selector.java:424)   at kafka.network.Processor.poll(SocketServer.scala:628)   at kafka.network.Processor.run(SocketServer.scala:545)   at java.lang.Thread.run(Thread.java:748) [2018-08-23 07:56:16,379] ERROR Processor got uncaught exception. (kafka.network.Processor) java.lang.OutOfMemoryError: Java heap space`   172.18.58.184:speedtrace (CLOSE_WAIT)   172.18.58.184 是 kafka client connect lsof -i | grep java  `java    1772 liandong   83u  IPv4 7990697      0t0  TCP *:36145 (LISTEN) java    1772 liandong   84u  IPv4 7990698      0t0  TCP *:9099 (LISTEN) java    1772 liandong   85u  IPv4 7990701      0t0  TCP *:40745 (LISTEN) java    1772 liandong  100u  IPv4 7990709      0t0  TCP prod_data_kafka_2:44688->prod_data_zk:eforward (ESTABLISHED) java    1772 liandong  193u  IPv4 7989816      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc (LISTEN) java    1772 liandong  224u  IPv4 8019955      0t0  TCP prod_data_kafka_2:9099->172.18.58.184:47430 (ESTABLISHED) java    1772 liandong  228u  IPv4 8018733      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:33032 (CLOSE_WAIT) java    1772 liandong  229u  IPv4 7990859      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:51334 (ESTABLISHED) java    1772 liandong  230u  IPv4 8022506      0t0  TCP prod_data_kafka_2:36145->172.18.58.184:46112 (ESTABLISHED) java    1772 liandong  235u  IPv4 7989829      0t0  TCP prod_data_kafka_2:32976->prod_data_kafka_1:XmlIpcRegSvc (ESTABLISHED) java    1772 liandong  236u  IPv4 8022224      0t0  TCP prod_data_kafka_2:36145->172.18.58.184:46024 (ESTABLISHED) java    1772 liandong  243u  IPv4 7998548      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->prod_data_kafka_3:39816 (ESTABLISHED) java    1772 liandong  247u  IPv4 7998555      0t0  TCP prod_data_kafka_2:33206->prod_data_kafka_3:XmlIpcRegSvc (ESTABLISHED) java    1772 liandong  248u  IPv4 8017061      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:60686 (CLOSE_WAIT) java    1772 liandong  251u  IPv4 7999481      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->prod_data_kafka_1:48914 (ESTABLISHED) java    1772 liandong  254u  IPv4 8016659      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:60920 (CLOSE_WAIT) java    1772 liandong  255u  IPv4 8009660      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:59356 (ESTABLISHED) java    1772 liandong  256u  IPv4 8017062      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:60700 (ESTABLISHED) java    1772 liandong  257u  IPv4 8022398      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:33626 (ESTABLISHED) java    1772 liandong  259u  IPv4 8019887      0t0  TCP prod_data_kafka_2:XmlIpcRegSvc->172.18.58.184:speedtrace (CLOSE_WAIT) ` gc log `2018-08-22T19:01:45.014+0800: 31537.291: [GC pause (G1 Evacuation Pause) (young) (initial-mark), 0.0147456 secs]    [Parallel Time: 12.9 ms, GC Workers: 2]       [GC Worker Start (ms): Min: 31537291.3, Avg: 31537291.3, Max: 31537291.3, Diff: 0.0]       [Ext Root Scanning (ms): Min: 1.8, Avg: 1.9, Max: 1.9, Diff: 0.1, Sum: 3.8]       [Update RS (ms): Min: 1.9, Avg: 1.9, Max: 1.9, Diff: 0.0, Sum: 3.9]          [Processed Buffers: Min: 14, Avg: 15.5, Max: 17, Diff: 3, Sum: 31]       [Scan RS (ms): Min: 4.5, Avg: 4.5, Max: 4.5, Diff: 0.0, Sum: 9.0]       [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [Object Copy (ms): Min: 4.1, Avg: 4.2, Max: 4.2, Diff: 0.1, Sum: 8.3]       [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Termination Attempts: Min: 4, Avg: 4.0, Max: 4, Diff: 0, Sum: 8]       [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [GC Worker Total (ms): Min: 12.5, Avg: 12.5, Max: 12.5, Diff: 0.0, Sum: 25.1]       [GC Worker End (ms): Min: 31537303.8, Avg: 31537303.8, Max: 31537303.8, Diff: 0.0]    [Code Root Fixup: 0.1 ms]    [Code Root Purge: 0.0 ms]    [Clear CT: 0.3 ms]    [Other: 1.4 ms]       [Choose CSet: 0.0 ms]       [Ref Proc: 0.2 ms]       [Ref Enq: 0.0 ms]       [Redirty Cards: 0.1 ms]       [Humongous Register: 0.0 ms]       [Humongous Reclaim: 0.1 ms]       [Free CSet: 0.6 ms]    [Eden: 781.0M(781.0M)->0.0B(781.0M) Survivors: 3072.0K->3072.0K Heap: 2106.0M(2347.0M)->1325.2M(2347.0M)]  [Times: user=0.03 sys=0.00, real=0.02 secs]  2018-08-22T19:01:45.029+0800: 31537.306: [GC concurrent-root-region-scan-start] 2018-08-22T19:01:45.039+0800: 31537.315: [GC concurrent-root-region-scan-end, 0.0098860 secs] 2018-08-22T19:01:45.039+0800: 31537.315: [GC concurrent-mark-start] 2018-08-22T19:01:45.111+0800: 31537.388: [GC concurrent-mark-end, 0.0721221 secs] 2018-08-22T19:01:45.111+0800: 31537.388: [GC remark 2018-08-22T19:01:45.111+0800: 31537.388: [Finalize Marking, 0.0002506 secs] 2018-08-22T19:01:45.111+0800: 31537.388: [GC ref-proc, 0.0008536 secs] 2018-08-22T19:01:45.112+0800: 31537.389: [Unloading, 0.0159521 secs], 0.0264459 secs]  [Times: user=0.05 sys=0.00, real=0.03 secs]  2018-08-22T19:01:45.139+0800: 31537.415: [GC cleanup 1339M->1339M(2347M), 0.0026152 secs]  [Times: user=0.00 sys=0.00, real=0.00 secs]  2018-08-22T19:01:48.222+0800: 31540.499: [GC pause (G1 Evacuation Pause) (young), 0.0141944 secs]    [Parallel Time: 12.6 ms, GC Workers: 2]       [GC Worker Start (ms): Min: 31540499.4, Avg: 31540499.4, Max: 31540499.4, Diff: 0.0]       [Ext Root Scanning (ms): Min: 1.4, Avg: 1.4, Max: 1.4, Diff: 0.1, Sum: 2.8]       [Update RS (ms): Min: 2.3, Avg: 2.3, Max: 2.3, Diff: 0.1, Sum: 4.6]          [Processed Buffers: Min: 11, Avg: 17.0, Max: 23, Diff: 12, Sum: 34]       [Scan RS (ms): Min: 4.4, Avg: 4.5, Max: 4.5, Diff: 0.1, Sum: 8.9]       [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [Object Copy (ms): Min: 4.2, Avg: 4.3, Max: 4.3, Diff: 0.1, Sum: 8.5]       [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Termination Attempts: Min: 1, Avg: 2.5, Max: 4, Diff: 3, Sum: 5]       [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]       [GC Worker Total (ms): Min: 12.5, Avg: 12.5, Max: 12.5, Diff: 0.0, Sum: 24.9]       [GC Worker End (ms): Min: 31540511.9, Avg: 31540511.9, Max: 31540511.9, Diff: 0.0]    [Code Root Fixup: 0.1 ms]    [Code Root Purge: 0.0 ms]    [Clear CT: 0.3 ms]    [Other: 1.3 ms]       [Choose CSet: 0.0 ms]       [Ref Proc: 0.2 ms]       [Ref Enq: 0.0 ms]       [Redirty Cards: 0.1 ms]       [Humongous Register: 0.1 ms]       [Humongous Reclaim: 0.1 ms]       [Free CSet: 0.6 ms]    [Eden: 781.0M(781.0M)->0.0B(780.0M) Survivors: 3072.0K->3072.0K Heap: 2106.2M(2347.0M)->1325.2M(2347.0M)]  [Times: user=0.02 sys=0.00, real=0.01 secs]  2018-08-22T19:01:51.373+0800: 31543.650: [GC pause (G1 Evacuation Pause) (young), 0.0146431 secs]    [Parallel Time: 13.1 ms, GC Workers: 2]       [GC Worker Start (ms): Min: 31543649.9, Avg: 31543649.9, Max: 31543649.9, Diff: 0.0]       [Ext Root Scanning (ms): Min: 1.4, Avg: 1.4, Max: 1.5, Diff: 0.1, Sum: 2.8]       [Update RS (ms): Min: 2.4, Avg: 2.4, Max: 2.5, Diff: 0.1, Sum: 4.8]          [Processed Buffers: Min: 8, Avg: 17.5, Max: 27, Diff: 19, Sum: 35]       [Scan RS (ms): Min: 4.5, Avg: 4.6, Max: 4.7, Diff: 0.2, Sum: 9.2]       [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [Object Copy (ms): Min: 4.4, Avg: 4.4, Max: 4.5, Diff: 0.1, Sum: 8.9]       [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Termination Attempts: Min: 1, Avg: 1.0, Max: 1, Diff: 0, Sum: 2]       [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [GC Worker Total (ms): Min: 13.0, Avg: 13.0, Max: 13.0, Diff: 0.0, Sum: 25.9]       [GC Worker End (ms): Min: 31543662.8, Avg: 31543662.8, Max: 31543662.9, Diff: 0.0]    [Code Root Fixup: 0.1 ms]    [Code Root Purge: 0.0 ms]    [Clear CT: 0.4 ms]    [Other: 1.2 ms]       [Choose CSet: 0.0 ms]       [Ref Proc: 0.1 ms]       [Ref Enq: 0.0 ms]       [Redirty Cards: 0.1 ms]       [Humongous Register: 0.1 ms]       [Humongous Reclaim: 0.1 ms]       [Free CSet: 0.6 ms]    [Eden: 780.0M(780.0M)->0.0B(780.0M) Survivors: 3072.0K->3072.0K Heap: 2105.2M(2347.0M)->1325.3M(2347.0M)]  [Times: user=0.02 sys=0.00, real=0.02 secs] `","closed","","liangrui1988","2018-08-23T02:01:50Z","2018-08-23T08:27:37Z"
"","5797","MINOR: Update reverse lookup test to work when ipv6 not enabled","ipv6 address is not returned by InetAddress.getAllByName if ipv6 not enabled (e.g. in a docker container). Update test to work with this environment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-14T09:22:22Z","2018-10-15T07:59:21Z"
"","6140","HOTFIX: Fix Properties.putAll compiler error when compiling with Java 11","Introduced by https://github.com/apache/kafka/pull/5921.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-01-14T08:54:01Z","2019-01-14T12:24:26Z"
"","5848","KAFKA-7557: optimize LogManager.truncateFullyAndStartAt()","Instead of calling `deleteSnapshotsAfterRecoveryPointCheckpoint` for `allLogs`, a possible optimization could be invoking it only for the logs being truncated.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-10-28T01:06:30Z","2018-11-12T17:02:45Z"
"","5588","MINOR:Use StoreBuilder.name() for node name","Initially, I wanted to make sure the name for the `StateStoreNode` was unique.  But we ensure name uniqueness in with following steps 1. we create store names by appending a number from an incrementing counter 2. if an attempt is made to add a state store with an existing name a TopologyException is thrown.  With that in mind, it will be more clear to use the `StoreBuilder.name()` instead.   For testing, I ran the existing streams unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-08-29T20:28:56Z","2018-08-30T13:24:26Z"
"","5937","KAFKA-7666: KIP-391 Producing with Offsets for Cluster Replication","initial commit  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   first implementation of https://cwiki.apache.org/confluence/display/KAFKA/KIP-391%3A+Allow+Producing+with+Offsets+for+Cluster+Replication  current limitation - a Producer can only be used to send Records with or without offsets, it cannot mix  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","edoardocomar","2018-11-21T17:20:24Z","2018-11-26T10:50:46Z"
"","5840","MINOR: Increase timeout and add more context to error message","Increasing timeout in the test, added more context to the error message to help diagnose future failures   For testing, I ran the existing suite of streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-25T15:36:19Z","2018-10-26T23:10:38Z"
"","5639","MINOR: increase number of unique keys for Streams EOS system test","Increasing the number of unique keys, to increase likelihood that the test exposes KAFKA-7192  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-09-11T22:28:49Z","2018-09-11T22:34:03Z"
"","5640","MINOR: increase number of unique keys for Streams EOS system test","Increasing the number of unique keys, to increase likelihood that the test exposes KAFKA-7192","closed","streams,","mjsax","2018-09-11T22:33:47Z","2018-09-20T00:19:27Z"
"","5860","MINOR: Add logging to Connect SMTs","Includes Update to ConnectRecord string representation to give visibility into schemas, useful in SMT tracing","closed","connect,","cyrusv","2018-10-31T16:40:53Z","2020-10-16T06:03:02Z"
"","5741","MINOR: fix generic type of ProcessorParameters","In unrelated recent work, I noticed some warnings about the missing type parameters on ProcessorParameters.  While investigating it, it seems like there was a bug in the creation of repartition topics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-04T17:52:03Z","2018-10-05T14:22:28Z"
"","5923","KAFKA-7536: Initialize TopologyTestDriver with non-null topic","In TopologyTestDriver constructor set non-null topic; and in unit test intentionally turn on caching to verify this case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-11-17T22:22:04Z","2020-04-25T00:06:07Z"
"","6120","MINOR: Fix Kafka Log Implementation Image","In the old image, there was Message 34477849968 ~ 35551592051 in `topic/82796232652.kafka` Segment File. In the new image, changed this to Message 82796232652 ~ 83869974631.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","shlee322","2019-01-10T14:14:21Z","2019-01-10T14:14:21Z"
"","6194","KAFKA-7855: Kafka Streams Maven Archetype quickstart fails to compile out of the box","In the LineSplit.java example, the untyped KStream is resolved to `KStream`. `value.split(..)` then  fails to build, because value is of type Object.   Other possible solutions would be to add the type to the builder: `builder.stream(...).flatmap(..).to(...)`, or `builder.stream(..., Consumed.with(Serdes.String(), Serdes.String()))`.  This change matches the code in the [tutorial](http://kafka.apache.org/21/documentation/streams/tutorial).  WIP: It should be tested during compile that the Java resource files compiles. However, I haven't found any way to achieve this. Feedback appreciated!   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","aurlien","2019-01-24T14:37:48Z","2019-03-16T02:57:06Z"
"","6002","KAFKA-7707: remove  the code never execute","in the BufferPool,the waiters is locked by ReentrantLock,and the waiters add Condition all within the lock,and the waiters remove also within the lock.in the waiters there is only one   Condition instance. ![image](https://user-images.githubusercontent.com/9736914/49506799-ea662100-f8b9-11e8-9fd8-12e99eb8b079.png) and in the finally we have remove the waiters's condition,so in the finally, we use the  ```   java finally {             // signal any additional waiters if there is more memory left             // over for them             try {                 if (!(this.nonPooledAvailableMemory == 0 && this.free.isEmpty()) && !this.waiters.isEmpty())                     this.waiters.peekFirst().signal();             } finally {                 // Another finally... otherwise find bugs complains                 lock.unlock();             }         } ```  can modify like  ```   java finally {             lock.unlock();         } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huangyiminghappy","2018-12-05T10:20:33Z","2018-12-24T07:36:17Z"
"","6338","KAFKA-8011: Fix for race condition causing ConcurrentModificationException","In the `RegexSourceIntegrationTest#testRegexMatchesTopicsAWhenCreated()` and `RegexSourceIntegrationTest#testRegexMatchesTopicsAWhenDeleted()` a race condition exists where the `ConsumerRebalanceListener` in the test modifies the list of subscribed topics when the condition for the test success is comparing the same array instance against expected values.  This PR should fix this race condition by using a `CopyOnWriteArrayList` which guarantees safe traversal of the list even when a concurrent modification is happening.    Using the `CopyOnWriteArrayList` should not impact performance negatively as the number of traversals, a result of using `ArrayList.equals()`, far outnumber (`TestUtils.waitForCondition()` checks for a successful result every`100ms`) the possible modifications as there will be at most one topic name added/removed during the test.  For testing, I updated the `RegexSourceIntegrationTest`integration test and ran the suite of streams tests.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-02-27T18:20:28Z","2019-03-01T00:32:05Z"
"","5953","KAFKA-7660: fix parentSensors memory leak","In StreamsMetricsImpl, the parentSensors map was keeping references to Sensors after the sensors themselves had been removed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-11-27T21:58:10Z","2018-11-30T14:17:33Z"
"","5735","MINOR: Increase JMX tool start timeouts in tests","In some tests, the check monitoring the JMX tool log output doesn’t quite wait long enough before failing. Increasing the timeout from 10 to 20 seconds.  This should be easily merged back to the `0.10.2` branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-10-03T14:56:42Z","2018-10-05T00:33:40Z"
"","6218","KAFKA-7956 In ShutdownableThread, immediately complete the shutdown if the thread has not been started","In some test cases it's desirable to instantiate a subclass of `ShutdownableThread` without starting it. Since most subclasses of `ShutdownableThread` put cleanup logic in `ShutdownableThread.shutdown()`, being able to call `shutdown()` on the non-running thread would be useful.  This change allows us to avoid blocking in `ShutdownableThread.shutdown()` if the thread's `run()` method has not been called. We also add a check that `initiateShutdown()` was called before `awaitShutdown()`, to protect against the case where a user calls `awaitShutdown()` before the thread has been started, and unexpectedly is not blocked on the thread shutting down.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2019-02-01T19:19:10Z","2019-02-26T17:55:39Z"
"","5670","MINOR: Fix broken link in security.html","In section ""Authentication using SASL/Kerberos"" the link to Configuring Kafka Clients is broken  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-09-21T15:42:06Z","2018-09-26T09:20:12Z"
"","5740","MINOR: clarify usage of stateful processor node","In recent PRs, we have been confused about the proper usage of StatefulProcessorNode (#5731 , #5737 )  This change disambiguates it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-04T17:34:33Z","2018-10-05T17:25:20Z"
"","6157","KAFKA-7829; Javadoc should show that AdminClient.alterReplicaLogDirs() is supported in Kafka 1.1.0 or later","In ReassignPartitionsCommand, the --reassignment-json-file option says ""...If absolute log directory path is specified, it is currently required that the replica has not already been created on that broker..."". This is inaccurate since we support moving existing replicas to new log dirs.  In addition, the Javadoc of AdminClient.alterReplicaLogDirs(...) should show the API is supported by brokers with version 1.1.0 or later.","closed","","lindong28","2019-01-16T18:37:06Z","2019-01-17T23:30:17Z"
"","6313","MINOR: Refactor replica log dir fetching for improved logging","In order to debug problems with log directory reassignments, it is helpful to know when the fetcher thread begins moving a particular partition. This patch refactors the fetch logic so that we stick to a selected partition as long as it is available and log a message when a different partition is selected.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-02-22T21:27:39Z","2019-02-26T16:38:33Z"
"","6388","KAFKA-8061: Handle concurrent ProducerId reset and call to Sender thread shutdown","In KAFKA-5503, we have added a check  for `running` flag in the loop inside maybeWaitForProducerId.  This is to handle concurrent call to Sender close(), while we attempt to get the ProducerId. This avoids blocking indefinitely when the producer is shutting down.  This created a corner case, where Sender thread gets blocked, if we had concurrent producerId reset and call to Sender thread close. The proposed fix is to check the forceClose flag in the loop inside maybeWaitForProducerId.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-07T09:40:28Z","2019-03-07T22:28:22Z"
"","6084","KAFKA-6627: Prevent config default values overriding ones specified through --producer-property on command line.","In ConsoleProducer, extraProducerProps (options specified in --producer-property) are applied first, then overriden (thus ignored) by the values returned by command line parser (default values are used for ones not specified from command line). This patch fixes it so that it doesn't override the existing value set by --producer-property if it is not explicitly specified.  The contribution is my original work and I license the work to the project under the project's open source license.      Signed-off-by: Kan Li   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* Testing strategy: For producer, print out acks value in KafkaProducer.configureAcks, start producer using --producer-property aks=all, then observe the difference with and without the patch. For consumer, print out isolationLevel in KafkaConsumer, start consumer using  --consumer-property isolation.level=read_committed, then observe the difference with and without the patch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","likan999","2019-01-03T02:13:35Z","2019-01-11T21:44:47Z"
"","6246","KAFKA-7864; validate partitions are 0-based","in AdminZkClient.validateTopicCreate(), check the partition ids are consecutive and 0-based","closed","","ctchenn","2019-02-08T19:55:33Z","2019-02-22T21:26:51Z"
"","6468","KAFKA-8062: Do not remore StateListener when shutting down stream thread","In a previous commit https://github.com/apache/kafka/pull/6091, we've fixed a couple of edge cases and hence do not need to remove state listener anymore (before that we removed the state listener intentionally to avoid some race conditions, which has been gone for now).  Not removing the state listener would automatically fix the issue that when threads are shutting down due to error code, the instance-level state will not be transited to ERROR, and then eventually to NOT_RUNNING.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-03-18T23:00:57Z","2020-04-24T23:59:50Z"
"","5750","MINOR: Ensure consumers are closed in DynamicBrokerReconfigurationTest","In `ConsumerBuilder.build`, if `awaitInitialPositions` raises an exception, the consumer will not be closed properly. We should add the consumer instance to the `consumers` collection immediately after construction.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-05T18:25:45Z","2018-10-05T18:31:06Z"
"","5828","MINOR: remove duplicated code in ConsumerPerformance","In `consume` method, the consumer subscribes the topic as expected, so there is no need to do the same thing ahead of this method call.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-10-23T00:57:00Z","2018-10-28T04:46:00Z"
"","6138","[KAFKA-7024] Rocksdb state directory should be created before opening…","In ```RocksDBStore.openDB``` we call  ```Files.createDirectories(dir.getParentFile().toPath()); ``` ```return RocksDB.open(options, dir.getAbsolutePath());  ```  We would also add the absolute file path as well to avoid the extra logging.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ConcurrencyPractitioner","2019-01-14T00:02:12Z","2019-01-18T17:34:57Z"
"","5928","MINOR: fix failing Streams system tests","In #5850 the output that prints the current Kafka version was reformatted (space before `:` was removed):  Old format: ``` Kafka version : 2.2.0-SNAPSHOT ```  New format: ``` Kafka version: 2.2.0-SNAPSHOT ```  This breaks some Streams system tests that grep the logs for this line.","closed","streams,","mjsax","2018-11-19T04:09:47Z","2018-11-20T17:46:05Z"
"","5536","MINOR: restructure Windows to favor immutable implementation","In #5510, I added an unsafe (but probably fine in practice) equals/hashCode to the Windows hierarchy.  I believe that this approach is better since: * It includes no non-deprecated mutable state in equals/hashCode * It avoids the subclass-return-type builder issues * It restricts the public interface to only the things that need to be known about all Windows. That is, all Windows need to offer a `gracePeriodMs()`. They do *not* all need to offer a builder method to set it. See `UnlimitedWindows`, for which ""grace period"" is nonsense. It does not need to allow setting it, and it can return `0`, which is always correct. In contrast, consider `UnlimitedWindows#until(long)`, which has to throw a runtime exception because the interface unnecessarily requires this method to be implemented.  In general, this PR sets a direction in which, once we drop the deprecated members from `Windows`, all the members will be abstract, and `Windows` itself will basically be an interface, which offers a much cleaner and more consistent implementation target.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-08-20T20:30:54Z","2018-08-24T14:35:33Z"
"","5877","KAFKA-6774: Improve the default group id behavior in KafkaConsumer (KIP-289)","Improve the default group id behavior by * changing the default consumer group to null, where no offset commit or fetch, or group management operations are allowed * deprecate the use of empty (`""""`) consumer group on the client   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-11-04T05:23:24Z","2018-11-16T08:58:57Z"
"","6068","MINOR: Improve exception messages in FileChannelRecordBatch","Improve exception message in `FileChannelRecordBatch.loadBatchWithSize`. The original description was removed as part of 81f0c1e8, reuse the description that was present before.  Replace `channel` by `fileRecords` in potentially thrown KafkaException descriptions when loading/writing `FileChannelRecordBatch`. This makes exception messages more readable (channel only shows an object hashcode, fileRecords shows the path of the file being read and start/end positions in the file).  ### Context  If loading a Kafka segment throws an `IOException` (from `FileChannel.read` in [`Utils.readFully`](https://github.com/apache/kafka/blob/4616c0aaff5766b6305baeed521efdfaae0094e8/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L936)), the exception is wrapped into a `KafkaException`. If this happens when loading/recovering a segment at start time, Kafka will shutdown and the current exception message does not help to pinpoint the origin of the issue.  Example stacktrace: (from a broker running 1.1.0) ``` [2018-12-24 23:17:44,727] ERROR {main} There was an error in one of the threads during logs loading: org.apache.kafka.common.KafkaException: java.io.IOException: Input/output error (kafka.log.LogManager) [2018-12-24 23:17:44,732] ERROR {main} [KafkaServer id=172329974] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer) org.apache.kafka.common.KafkaException: java.io.IOException: Input/output error         at org.apache.kafka.common.record.FileLogInputStream$FileChannelRecordBatch.loadBatchWithSize(FileLogInputStream.java:214)         ...         at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Input/output error         at sun.nio.ch.FileDispatcherImpl.pread0(Native Method)         at sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)         at sun.nio.ch.IOUtil.read(IOUtil.java:197)         at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)         at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)         at org.apache.kafka.common.utils.Utils.readFully(Utils.java:831)         at org.apache.kafka.common.utils.Utils.readFullyOrFail(Utils.java:804)         at org.apache.kafka.common.record.FileLogInputStream$FileChannelRecordBatch.loadBatchWithSize(FileLogInputStream.java:210)         ... 28 more ```  After applying the proposed changes: ``` org.apache.kafka.common.KafkaException: Failed to load record batch at position 12868 from FileRecords(file= /usr/local/var/lib/kafka-logs/__consumer_offsets-9/00000000000000000000.log, start=0, end=2147483647) 	at org.apache.kafka.common.record.FileLogInputStream$FileChannelRecordBatch.loadBatchWithSize(FileLogInputStream.java:218) 	... 	at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Input/output error         at sun.nio.ch.FileDispatcherImpl.pread0(Native Method)         at sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)         at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)         at sun.nio.ch.IOUtil.read(IOUtil.java:197)         at sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)         at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)         at org.apache.kafka.common.utils.Utils.readFully(Utils.java:831)         at org.apache.kafka.common.utils.Utils.readFullyOrFail(Utils.java:804)         at org.apache.kafka.common.record.FileLogInputStream$FileChannelRecordBatch.loadBatchWithSize(FileLogInputStream.java:213) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","flavray","2018-12-26T23:26:16Z","2018-12-28T21:58:06Z"
"","5542","KAFKA-7320: Add consumer configuration to disable auto topic creation","Implements KIP-361 to provide a consumer configuration to specify whether subscribing or assigning a non-existent topic would result in it being automatically created or not.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-08-21T03:13:08Z","2019-05-08T17:45:45Z"
"","6239","KAFKA-4730: Streams does not have an in-memory windowed store","Implemented an in-memory window store allowing for range queries. A finite retention period defines how long records will be kept, ie the window of time for fetching, and the grace period defines the window within which late-arriving data may still be written to the store.  Unit tests were written to test the functionality of the window store, including its insert/update/delete and fetch operations. Single-record, all records, and range fetch were tested, for both time ranges and key ranges. The logging and metrics for late-arriving (dropped)records were tested as well as the ability to restore from a changelog.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ableegoldman","2019-02-08T01:30:57Z","2019-02-21T03:09:51Z"
"","6295","KIP-382: MirrorMaker 2.0","Implementation of [KIP-382 ""MirrorMaker 2.0""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0) **_(approved)_** and [KIP-416 ""Notify SourceTask of ACK'd offsets, metadata""](https://cwiki.apache.org/confluence/display/KAFKA/KIP-416%3A+Notify+SourceTask+of+ACK%27d+offsets%2C+metadata) **_(not yet approved)_**.   Depends on #6171   Quick start:  1. create MM2 configuration file. 2. launch with `./bin/connect-mirror-maker.sh mm2.properties`  Sample configuration file:      clusters = one, two, three, four     one.bootstrap.servers = ...     two.bootstrap.servers = ...     ...     one->two.enabled = true # false by default     two->one.enabled = true     ...     three->four.topics = topic1, topic2 # .* by default     ...     sync.topic.acls.enabled = false # disable for non-secure clusters  The following features of the KIP are deferred for now:  - MirrorSinkConnector/Task -- not used by the MirrorMaker driver, but may be useful to run on a Connect cluster  - ""legacy mode"" script -- per KIP, not part of first release","closed","","ryannedolan","2019-02-20T16:13:48Z","2022-01-03T16:33:16Z"
"","6009","KAFKA-7321: Add a Maximum Log Compaction Lag (KIP-354)","Implement the change described in KIP-354 Added unit tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xiowu0","2018-12-06T22:17:44Z","2019-05-13T16:17:14Z"
"","6261","KAFKA-7656: Disallow negative max bytes in fetch request","Implement a new Validator that  1. Returns an INVALID_REQUEST for all of the fetch partitions if the fetch request has a negative max bytes. 2. Returns an INVALID_REQUEST for any fetch partion that has a negative max bytes.  Extended the unittest FetchRequestTest to cover this scenario.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-02-12T20:00:39Z","2019-10-08T23:47:22Z"
"","5765","MINOR: update test_broker_type_bounce_at_start to match trunk","Ignore a flaky test in 0.10.2 that is also ignored in trunk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-09T19:55:12Z","2018-11-05T20:38:16Z"
"","5992","KAFKA-7698: Kafka Broker fail to start: ProducerFencedException throw…","If ValidationType is None, also skip the check in appendEndTxnMarker (similar to append).  Verified with existing unitest and our daily operation.","closed","","mingaliu","2018-12-03T18:27:42Z","2019-12-10T21:30:55Z"
"","5894","MINOR: Optimization should only run with more than one repartition topic","If there is only one repartition topic as the result of a key-change operation, then the optimization should not take place as the resulting sub-topology will have the same structure.  While not strictly an issue I'm very much in favor of not performing unnecessary operations.   I've added a test which fails without the update to the optimization.  The existing test suite ensures all other code not impacted by the change.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage, and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-11-08T19:40:00Z","2018-11-21T20:38:03Z"
"","5518","KAFKA-7298; Raise UnknownProducerIdException if next sequence number is unknown","If the only producer state left in the log is a transaction marker, then we do not know the next expected sequence number. This can happen if there is a call to DeleteRecords which arrives prior to the writing of the marker. Currently we raise an OutOfOrderSequence error when this happens, but this is treated as a fatal error by the producer. Raising UnknownProducerId instead allows the producer to check for truncation using the last acknowledged sequence number and reset if possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-16T16:30:24Z","2018-08-20T16:00:22Z"
"","5577","Update InFlightRequests.java","if not iff  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chaplinthink","2018-08-27T09:15:31Z","2020-11-04T06:06:46Z"
"","6345","state.cleanup.delay.ms default is 600,000 ms (10 minutes).","If I'm counting my zeros correctly, I believe the default cleanup delay is 600_000ms, not 6_000_000 as written in the docs.  Default value here: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java#L681  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cwildman","2019-02-28T15:19:24Z","2019-02-28T17:29:49Z"
"","5557","KAFKA-7128: Follower has to catch up to offset within current leader epoch to join ISR","If follower is not in ISR, it has to fetch up to start offset of the current leader epoch. Added unit test to verify this behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-08-22T18:58:28Z","2018-08-28T17:45:01Z"
"","6290","MINOR: cleanup deprectaion annotations","If deprecated interface methods are inherited, the `@Deprication` tag should be used (instead on suppressing the deprecation warning).  Also adding comments to better explain when stuff will be fixed.  Also adding comments about deprecated ReadOnlyWindowStore methods, that we keep for WindowStore though (to avoid method removal because they are deprecated in parent interface).  Some additional side cleanup","closed","streams,","mjsax","2019-02-19T04:42:08Z","2019-03-09T03:38:30Z"
"","5500","KAFKA-7286: Avoid getting stuck loading large metadata records","If a group metadata record size is higher than offsets.load.buffer.size, loading offsets and group metadata from __consumer_offsets would hang forever.  This was due to a buffer being too small to fit any message bigger than the maximum configuration. If this happens, temporarily use a buffer that can fit the large records and move on.","closed","","flavray","2018-08-13T23:18:32Z","2018-09-09T07:05:49Z"
"","5849","MINOR: Improve ReplicationQuotasTest#shouldThrottleOldSegments() test…","I've seen this test fail with ``` java.lang.AssertionError: Throttled replication of 6352ms should be < 6000ms ```  I think a possible cause to that might be that it starts counting the time it took for replication before the replication itself has started. `createServer()` initializes ZK and other systems before it starts up the replication thread.  I ran the test 25 times locally both ways. Average `throttledTook` before the change: 5341.75 Mean `throttledTook` after the change: 5256.92  Note that those are the results from `./gradlew core:test --tests kafka.server.ReplicationQuotasTest.shouldThrottleOldSegments`. I've noticed that if I run the whole test class `ReplicationQuotasTest`, the `throttledTook` is close ~4100","closed","","stanislavkozlovski","2018-10-28T11:01:36Z","2018-10-31T14:48:54Z"
"","6144","MINOR: Add log entry for KafkaException","I've observed several reports of sudden unexpected streamthread shutdown with the log entry like:  ``` State transition from PENDING_SHUTDOWN to DEAD ```  but there is no related error logs before this line at all. I suspect this is because we intentionally do not log for KafkaException and there's some edge cases where we miss internally and hence caused this. I'm adding the ERROR level log entry here in order to reveal more information in case I saw this again in the future.","closed","","guozhangwang","2019-01-15T02:59:07Z","2019-01-15T18:19:19Z"
"","5732","KAFKA-6971 Passing in help flag to kafka-console-producer should print arg options","I've added this small fix to print all options and descriptions of console consumer and producer without parameters i.e. ""console-producer"" and with --help option i.e. ""console-producer --help"" I did it without unit tests because it uses `System.exit(1)` after printing message and tests will be always red.","closed","","kochunstvo","2018-10-03T10:32:37Z","2018-11-27T04:03:59Z"
"","5677","MINOR: demonstrate migrating Produced config to an interface","I was wondering about the impact of changing our config objects to be interfaces on the public side. This is just a sketch of the idea for discussion.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-21T22:43:51Z","2018-09-24T15:25:44Z"
"","6445","MINOR: Update Trogdor ConnectionStressWorker status at the end of execution","I was testing this worker locally and found that it would not report correct numbers after it shutting down due to the `durationMs` passing.  Status before patch: ``` {""tasks"":{""connect"":{""state"":""DONE"",""spec"":{""class"":""org.apache.kafka.trogdor.workload.ConnectionStressSpec"",""startMs"":1552576318156,""durationMs"":60000,""clientNode"":[""node0""],""bootstrapServers"":""localhost:9092"",""targetConnectionsPerSec"":100,""numThreads"":10,""action"":""CONNECT""},""startedMs"":1552576318159,""doneMs"":1552576378366,""cancelled"":false,""status"":{""totalConnections"":4021,""totalFailedConnections"":0,""connectsPerSec"":100.19186205865498}}  ""connect2"":{""state"":""RUNNING"",""spec"":{""class"":""org.apache.kafka.trogdor.workload.ConnectionStressSpec"",""startMs"":1552576480870,""durationMs"":60000,""clientNode"":[""node0""],""bootstrapServers"":""localhost:9092"",""targetConnectionsPerSec"":200,""numThreads"":10,""action"":""CONNECT""},""startedMs"":1552576480870,""status"":{""totalConnections"":8021,""totalFailedConnections"":0,""connectsPerSec"":200.3797246995928}}}} ```  Status after patch: ``` ""tasks"":{""connect_newer1"":{""state"":""DONE"",""spec"":{""class"":""org.apache.kafka.trogdor.workload.ConnectionStressSpec"",""startMs"":1552579728411,""durationMs"":60000,""clientNode"":[""node0""],""bootstrapServers"":""localhost:9092"",""targetConnectionsPerSec"":100,""numThreads"":10,""action"":""CONNECT""},""startedMs"":1552579728412,""doneMs"":1552579788645,""cancelled"":false,""status"":{""totalConnections"":6010,""totalFailedConnections"":0,""connectsPerSec"":100.19004434368019}}  ""connect_newer2"":{""state"":""DONE"",""spec"":{""class"":""org.apache.kafka.trogdor.workload.ConnectionStressSpec"",""startMs"":1552579472885,""durationMs"":60000,""clientNode"":[""node0""],""bootstrapServers"":""localhost:9092"",""targetConnectionsPerSec"":200,""numThreads"":10,""action"":""CONNECT""},""startedMs"":1552579472891,""doneMs"":1552579533114,""cancelled"":false,""status"":{""totalConnections"":11980,""totalFailedConnections"":0,""connectsPerSec"":200.31434973079624}}}} ```","closed","","stanislavkozlovski","2019-03-14T16:21:49Z","2019-03-15T16:53:22Z"
"","6438","MINOR: Improve Trogdor external command worker docs","I was exploring the code for the new external command worker and saw that it wasn't documented.  This change also contains some very minor code style improvements","closed","","stanislavkozlovski","2019-03-13T12:24:14Z","2019-06-06T17:04:06Z"
"","5764","DOCS: Better clarify topic-configured min.isr vs cluster-configured min.isr","I think this better clarifies the misunderstanding in https://issues.apache.org/jira/browse/KAFKA-7474 cc  @joel-hamill (reported the issue) @ciudilo (reported the issue)  @hachikuji @vahidhashemian (reviewed the previous https://github.com/apache/kafka/pull/3035)","open","","stanislavkozlovski","2018-10-09T14:25:42Z","2018-10-21T11:30:10Z"
"","5838","KAFKA-3932 - Consumer fails to consume in a round robin fashion","I think the issue is the statement in [PIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-41%3A+KafkaConsumer+Max+Records): ""As before, we'd keep track of **which partition we left off** at so that the next iteration **would begin there**."" I think it should **NOT** use the last partition in the next iteration; **it should pick the next one instead**.  The simplest solution to impose the order to pick the next one is to use the order the consumer.internals.Fetcher receives the partition messages, as determined by **completedFetches** queue in that class. To avoid parsing the partition messages repeatedly. we can **save those parsed fetches to a list and maintain the next partition to get messages there.**   @hachikuji, @lindong28, @guozhangwang, @ijuma and others, please review.","closed","","chienhsingwu","2018-10-24T17:57:13Z","2019-05-21T13:14:27Z"
"","5926","MINOR: add unit test for Utils.murmur2","I recently needed to port the `Utils.murmur` hash function to Go for a project I'm working on. I realized there were no definite test cases for the Java implementation that I could use to tell me whether my ported implementation was correct and returned the same results as the code in this project. And although there is test coverage of this hash function by way of a caller in the `testKeyPartitionIsStable` unit in `DefaultPartitionerTest`, the granularity of the test there is not sufficient for any developer to write a reliable port in another language of choice.  Fortunately, I found someone who ran into the same trouble as me:  https://stackoverflow.com/questions/48582589/porting-kafkas-murmur2-implementation-to-go  who provided test cases with their solution. I took the test cases from the above and codified them as unit test cases for this hash function in `UtilsTest`. In this way, any developer writing a custom Kafka client or server in any language can look at these reference cases to confirm the correctness of their implementations with absolute confidence that keys hashed with this murmur function (modulo some number of partitions) land on the right partition.  An open question to the maintainers (and to anyone with expertise in non-cryptographic hashing): do the cases I've given add enough surface area to cover the complete inner workings of this function's mappings?","closed","","natemurthy","2018-11-18T19:29:45Z","2019-06-25T05:43:12Z"
"","6208","MINOR: fix checkstyle suppressions for generated RPC code to work on Windows","I have test this PR on my windows box. The build of kafka works well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2019-01-29T12:11:37Z","2019-02-01T00:03:35Z"
"","5645","docs: Fix upgrade.mode references","I guess upgrade.from is what is meant.","closed","","hasegeli","2018-09-13T17:19:50Z","2020-10-20T14:18:23Z"
"","6227","MINOR: Remove unused StreamsGraphNode#repartitionRequired","I found this defect while inspecting [KAFKA-7293: Merge followed by groupByKey/join might violate co-partitioning](https://issues.apache.org/jira/browse/KAFKA-7293); This flag is never used now. Instead, `KStreamImpl#repartitionRequired` is now covering its functionality.  cc/ @mjsax   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2019-02-02T14:29:40Z","2019-02-26T15:23:15Z"
"","5648","MINOR: react to a null task with a rebalance","I found a system test failure in which `task.isClosed()` threw an NPE.  This change simply catches that condition and initiates a rebalance to try and recover.  An alternative approach would be to track down the root cause if we really don't expect the task to be missing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-13T20:02:21Z","2018-09-14T20:38:54Z"
"","5739","MINOR: Fix typo in Resource in org.apache.kafka.trogdor","I fixed 2 typos in   org.apache.kafka.trogdor.agent.Agent org.apache.kafka.trogdor.coordinator.Coordinator  There were missing ""c"" in Resource, as you can see in commit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","matus-cuper","2018-10-04T13:12:12Z","2018-10-06T17:36:03Z"
"","6392","Slight change on server.properties documentation","I didn't find any `java.net.InetAddress.getCanonicalHostName()` usage in the code. But I find `java.net.InetAddress.getLocalHost.getCanonicalHostName` in  `https://github.com/apache/kafka/blob/4b29487fa9d3d4ff8adaaaa6204db796eccd3a68/core/src/main/scala/kafka/server/KafkaServer.scala`. So, if the internal kafka use the localhost value, I think I should change the documentation to appropriate value.  I dont think that I changed anything, therefore, no need to change any test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","LIQRGV","2019-03-07T12:25:16Z","2019-11-14T17:31:07Z"
"","5541","Serdes mandatory for Materialized","I can't see a case where `Serde`s would not be given to a `Materialized` except: - When the default one is (by accident? by chance?) the right Serde. (If we refactor it will still compile and if we are lucky to have tests it might be caught) - We are doing a `count` or `reduce` and therefore the value `Serde` will be set for us.  In any case it's: - Semantically weird to say that we will store data but we don't tell how to serialize in that storage. - Leading to a lot of runtime errors that could have been caught at compile time.  Let me know your thoughts. @vvcephei  @guozhangwang  @ijuma","closed","","joan38","2018-08-20T22:55:24Z","2018-08-23T18:36:12Z"
"","5856","MINOR: Add DEBUG level logs for successful/failed authentications wit…","I believe that it'll be useful for debugging purposes to know which IP addresses are consistently failing authentication. Applications that consistently fail authentication can introduce CPU pressure to the broker and as such, I think it is useful for Ops teams to have the needed information to quickly block.  I've left these logs on `DEBUG` level since I presume that most common cases won't require to know this information. I'm sure we shouldn't log successful auths in `INFO` but I'm hesitant about unsuccessful auths  cc @rajinisivaram @ijuma @rondagostino","closed","","stanislavkozlovski","2018-10-30T17:55:07Z","2019-01-09T17:32:03Z"
"","6472","KAFKA-8125: Skip doing assignment when topic existed","https://issues.apache.org/jira/browse/KAFKA-8125  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-03-20T02:49:11Z","2020-05-02T15:09:21Z"
"","6308","KAFKA-7962: Avoid NPE for StickyAssignor","https://issues.apache.org/jira/browse/KAFKA-7962  Consumer using StickyAssignor throws NullPointerException if a subscribed topic was removed.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-02-22T08:22:26Z","2019-03-01T01:41:37Z"
"","6139","KAFKA-7813: JmxTool throws NPE when --object-name is omitted","https://issues.apache.org/jira/browse/KAFKA-7813  Running the JMX tool without --object-name parameter, results in a NullPointerException.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-01-14T01:55:17Z","2019-03-18T01:53:11Z"
"","6103","KAFKA-7790: Fix Bugs in Trogdor Task Expiration","https://issues.apache.org/jira/browse/KAFKA-7790  ### Changes: * The Trogdor Coordinator now overwrites a task's `startMs` to the time it received it if `startMs` is in the past. * The Trogdor Agent now correctly expires a task after the expiry time (`startMs + durationMs`) passes. Previously, it would ignore `startMs` and expire after `durationMs` milliseconds of local start of the task.","closed","","stanislavkozlovski","2019-01-08T10:35:54Z","2019-01-11T21:38:01Z"
"","6081","KAFKA-7779: Avoid unnecessary loop iteration in leastLoadedNode","https://issues.apache.org/jira/browse/KAFKA-7779  In NetworkClient.leastLoadedNode, it invokes `isReady` to  check if an established connection exists for the given node. `isReady` checks whether metadata needs to be updated also which wants to make metadata request first priority. However, if the to-be-sent request is metadata request, then we do not have to check this otherwise the loop in `leastLoadedNode` will do a complete iteration until the final node is selected. That's not performance efficient for a large cluster.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-01-02T09:27:10Z","2019-04-29T14:53:25Z"
"","5935","KAFKA-7665: Replace BaseConsumerRecord with ConsumerRecord in MM","https://issues.apache.org/jira/browse/KAFKA-7665  In MirrorMaker, the fix will replace BaseConsumerRecord with ConsumerRecord.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-11-21T06:48:30Z","2022-02-10T16:19:40Z"
"","5810","KAFKA-7515: Trogdor - Add Consumer Group Benchmark Specification","https://issues.apache.org/jira/browse/KAFKA-7515   ### Changes * Add new `consumerGroup` field to `ConsumeBenchSpec`. * Changes the `activeTopics` field format in `ConsumeBenchSpec`.     * `activeTopics` is a list of strings and now supports three notations for each value         * 'foo' - denotes a topic name 'foo'         * **single-range notation** 'foo[1-2] - gets expanded to two topics - 'foo1' and 'foo2'          * **double-range notation** 'foo[1-2][1-2] - gets expanded to two topics with two partitions each. topic 'foo1' with partitions 1 and 2, topic 'foo2' with partitions 1 and 2   This ConsumeBenchWorker now supports three cases: 1. When we want to manually assign partitions and use a random, new consumer group. (activeTopics contains at least one value with the new double-range notation (e.g foo[1-2][1-2]), consumerGroup is undefined)  - consumer uses the specific partitions (and all partitions for topics who did not have in a second range) via KafkaConsumer#assign() 2. When we want to have dynamic partition assignment via an existing consumer group's (activeTopics **does not contain any** double range notations, consumerGroup is specified) - KafkaConsumer#subscribe() 3. When we want to manually assign partitions but track offsets via an existing consumer group (activeTopics contains at least one value with the new double-range notation, consumerGroup is specified) - KafkaConsumer#assign()","closed","","stanislavkozlovski","2018-10-17T09:13:25Z","2018-10-29T17:51:08Z"
"","5770","KAFKA-7442: forceUnmap mmap on linux when index resize","https://issues.apache.org/jira/browse/KAFKA-7442  In AbstractIndex#resize, `safeForceUnmap` should also be invoked for Linux.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2018-10-10T03:17:58Z","2018-11-28T04:56:44Z"
"","5584","KAFKA-7354: Fix IdlePercent and NetworkProcessorAvgIdlePercent metric","https://issues.apache.org/jira/browse/KAFKA-7354  Currently, MBean `kafka.network:type=Processor,name=IdlePercent,networkProcessor=*` and `afka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent` could be greater than 1. However, these two values represent a percentage which should not exceed 1.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-08-29T01:41:41Z","2018-08-29T15:32:42Z"
"","5579","KAFKA-7326: KStream.print() should flush on each line for PrintStream","https://issues.apache.org/jira/browse/KAFKA-7326  PrintForeachAction should flush for PrintStream after printing the line.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","huxihx","2018-08-28T05:57:13Z","2018-11-13T06:34:59Z"
"","5527","KAFKA-3705: Added a foreignKeyJoin implementation for KTable.","https://issues.apache.org/jira/browse/KAFKA-3705  Foreign Key Join: ================================ Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.  Design Philosophy: ================================ The intent of this design was to build a totally encapsulated function that operates very similarly to the regular join function. No further work is required by the user to obtain their foreignKeyJoin results after calling the function. That being said, there is increased cost in some of the topology components, especially due to resolving out-of-order arrival due to foreign key changes. I would appreciate any and all feedback on this approach, as my understanding of the Kafka Streams DSL is to provide higher level functionality without requiring the users to know exactly what's going on under the hood.   Some points of note: 1) Requires an additional materialized State Store for the prefixScanning of the repartitioned CombinedKey events.  2) ReadOnlyKeyValueStore interface was modified to contain prefixScan. This requires that all implementations support this, but follows an existing precedent where some store functions are already stubbed out with exceptions.  3) Currently limited to Inner Join (can do more join logic in future - just limiting the focus of this KIP).  4) Application Reset does not seem to delete the new internal topics that I have added. (only tested with Kafka 1.0).  5) Only works with identical number of input partitions at the moment, though it may be possible to get it working with KTables of varying input partition count.  Testing: ================================ Testing is covered by a two integration tests that exercises the foreign key join.  The first test exercises the out-of-order resolution and partitioning strategies by running three streams instances on three partitions. This demonstrates the scalability of the proposed solution.  *important* The second test (KTableKTableForeignKeyInnerJoinMultiIntegrationTest) attempts to join using foreign key twice. This results in a NullPointerException regarding a missing task, and must be resolved before committing this.","closed","kip,","bellemare","2018-08-17T18:27:07Z","2020-06-12T23:59:17Z"
"","6161","KAFKA-7652: Part II; Add single-point query for SessionStore and use for flushing / getter","https://github.com/apache/kafka/pull/2972 tried to fix a bug about flushing operation, but it was not complete, since `findSessions(key, earliestEnd, latestStart)` does not guarantee to only return a single entry since its semantics are to return any sessions whose end > earliestEnd and whose start < latestStart.  I've tried various ways to fix it completely and I ended up having to add a single-point query to the public ReadOnlySessionStore API for the exact needed semantics. It is used for flushing to read the old values (otherwise the wrong old values will be sent downstreams, hence it is a correctness issue) and also for getting the value for value-getters (it is for perf only).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-01-17T07:39:53Z","2019-01-31T01:31:32Z"
"","5777","MINOR: Adjust test params pursuant to KAFKA-4514.","https://github.com/apache/kafka/pull/2267 Introduced support for Zstandard compression. The relevant test expects values for `num_nodes` and `num_producers` based on the (now-incremented) count of compression types.  Passed the affected, previously-failing test: `ducker-ak test tests/kafkatest/tests/client/compression_test.py`","closed","","colinhicks","2018-10-10T21:36:02Z","2018-10-11T16:13:55Z"
"","6183","HOTFIX: fix checkstyle issue","hotfix for `2.1` branch","closed","core,","mjsax","2019-01-21T20:44:19Z","2019-01-22T00:11:16Z"
"","5719","MINOR: Add test case. Incomplete rack assignment in metadatas.","Hi~. I add test case. About incomplete rack assignment in metadatas. Thank you.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","AndersonChoi","2018-10-01T02:57:17Z","2018-10-03T06:48:26Z"
"","5715","MINOR: Update Scala to 2.12.7, lz4-java to 1.5 and others","Highlights: * 10% compilation speed improvement in Scala 2.12.7: https://www.scala-lang.org/news/2.12.7 * 10% better decompression speed in lz4 1.8.2 (lz4-java 1.5.0 includes lz4 1.8.3): https://github.com/lz4/lz4-java/blob/master/CHANGES.md#150 https://github.com/lz4/lz4/releases  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-29T02:04:31Z","2018-09-30T05:32:10Z"
"","5553","WIP - KAFKA-3821 Allowing source tasks to produce offset without emitting r…","Hi @rhauch, @ewencp et al., here's a proposal for [KAFKA-3821](https://issues.apache.org/jira/browse/KAFKA-3821) based on my latest comment on the JIRA issue. It'd need more polishing and testing, but I wanted to demonstrate the idea and see whether you agree with it before spending more time on it.  I think it's the simplest way to let source connectors produce offsets without emitting messages and without creating an awkward inheritance relationship of `SourceRecord` and a sub-class just carrying offset information. Please see [this comment](https://issues.apache.org/jira/browse/KAFKA-3821?focusedCommentId=16564900&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16564900) on the JIRA issue for the two use cases of this new functionality.  Looking forward to learn about your toughts. Thanks!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","gunnarmorling","2018-08-22T09:25:54Z","2020-03-22T06:23:45Z"
"","6409","KAFKA-6958: Add new NamedOperation interface to enforce consistency i…","Hi @mjsax @bbejeck   This is the first PR for the KIP-307. Thanks a lot for the review.","closed","kip,","fhussonnois","2019-03-08T19:12:56Z","2020-06-12T23:46:35Z"
"","6410","KAFKA-6958: Allow to name operation using parameter classes","Hi @mjsax @bbejeck   This is the 2nd PR for the KIP-307.   NOTE : [PR 6409](https://github.com/apache/kafka/pull/6409 ) should be merge first  Thanks a lot for the review.","closed","kip,","fhussonnois","2019-03-08T19:14:44Z","2020-06-12T23:46:29Z"
"","6413","KAFKA-6958: Overload methods for group and windowed stream to allow to name operation name using the new Named class","Hi @mjsax @bbejeck  This is the last PR for the KIP-307.  NOTE : [PR 6412](https://github.com/apache/kafka/pull/6412) should be merge first  Thanks a lot for the review.","closed","kip,","fhussonnois","2019-03-08T19:24:05Z","2020-06-12T23:46:09Z"
"","6412","KAFKA-6958: Overload KTable methods to allow to name operation name using the new Named class","Hi @mjsax @bbejeck  This is the 4th PR for the KIP-307. NOTE :[PR 6411](https://github.com/apache/kafka/pull/6411) should be merged first  Thanks a lot for the review.","closed","kip,","fhussonnois","2019-03-08T19:20:02Z","2020-06-12T23:46:15Z"
"","6411","KAFKA-6958: Overload KStream methods to allow to name operation name  using the new Named class","Hi @mjsax @bbejeck  This is the 3rd PR for the KIP-307.  NOTE : [PR 6410](https://github.com/apache/kafka/pull/6410) should be merge first  Thanks a lot for the review.","closed","kip,","fhussonnois","2019-03-08T19:16:48Z","2020-06-12T23:46:23Z"
"","6058","KAFKA-7824: Require member.id for initial join group request","Hey there,  this is my draft implementation of [KIP-394](https://cwiki.apache.org/confluence/display/KAFKA/KIP-394%3A+Require+member.id+for+initial+join+group+request), requiring member id when the new join group request was received. The current blocker was on the concurrent unit tests, and I figure it would be good to post the implementation at the same time to have some parallel review.  Basically the major refactor is that we extract all the unknown member handling logic as a separate function called `doUnknownJoinGroup` , and define empty MemberMetadata struct called `unknownMemberMetadata ` which will be inserted into the members list when we receive a valid unknown member join group request. This mock object will be evicted after the rebalance timeout.  Let me know if the high level implementation makes sense here when you got time @hachikuji @guozhangwang @MayureshGharat @mjsax @stanislavkozlovski   Also we created a separate JIRA: https://issues.apache.org/jira/browse/KAFKA-7824 for this PR, since 7610 is closed with another PR.  Thanks a lot!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2018-12-21T06:31:08Z","2019-01-15T21:44:05Z"
"","6343","Perfect system to get nanosecond logic.","Here's a description of System.nanoTime():     /**      * Returns the current value of the running Java Virtual Machine's      * high-resolution time source, in nanoseconds.      *      * This method can only be used to measure elapsed time and is      * not related to any other notion of system or wall-clock time.      * The value returned represents nanoseconds since some fixed but      * arbitrary origin time (perhaps in the future, so values      * may be negative).  The same origin is used by all invocations of      * this method in an instance of a Java virtual machine; other      * virtual machine instances are likely to use a different origin.      *      * This method provides nanosecond precision, but not necessarily      * nanosecond resolution (that is, how frequently the value changes)      * - no guarantees are made except that the resolution is at least as      * good as that of {@link #currentTimeMillis()}.      *      * Differences in successive calls that span greater than      * approximately 292 years (263 nanoseconds) will not      * correctly compute elapsed time due to numerical overflow.      *      * The values returned by this method become meaningful only when      * the difference between two such values, obtained within the same      * instance of a Java virtual machine, is computed.      *      *  For example, to measure how long some code takes to execute:      *   {@code      * long startTime = System.nanoTime();      * // ... the code being measured ...      * long estimatedTime = System.nanoTime() - startTime;}      *      * To compare two nanoTime values      *   {@code      * long t0 = System.nanoTime();      * ...      * long t1 = System.nanoTime();}      *      * one should use {@code t1 - t0 < 0}, not {@code t1 < t0},      * because of the possibility of numerical overflow.      *      * @return the current value of the running Java Virtual Machine's      *         high-resolution time source, in nanoseconds      * @since 1.5      */     public static native long nanoTime();  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","yahan-lee","2019-02-28T07:48:02Z","2019-03-05T01:19:15Z"
"","6256","KAFKA-7920: Do not permit zstd use until inter.broker.protocol.version is updated to 2.1","Here is the fix. Since `inter.broker.protocol.version` is defined in `KafkaConfig` (not `LogConfig`), I concluded that `ProduceRequest` validation routine is the best place for check. (I first thought that `LogValidator` would be the right place, but it required too many parameter passing. So I rejected that approach.)  Here are the details:  1. Add `Optional interBrokerProtocolVersion` to `ProduceRequest#validateRecords.` 2. Disallow zstd compressed `MemoryRecords` if `inter.broker.protocol.version` < 2.1 (i.e., `ApiVersion.id` = 17). 3. Add `ProduceRequest#validateRecords(short, MemoryRecords)` for backward compatibility. 4. Fix a typo: ""note allowed to"" → ""not allowed to""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-02-12T12:05:10Z","2019-02-20T16:44:05Z"
"","6323","KAFKA-7996: KafkaStreams does not pass timeout when closing Producer","Here is the draft fix. The approach is simple - it adds a new parameter for `StreamThread.TaskCreator` and `RecordCollectorImpl` to denote the close wait duration; I found two `Producer#close()` in streams module but if there is omitted one, don't hesitate to give me a comment.  Since it introduces a new config, `close.wait.ms`, it needs a KIP. Isn't it?  cc/ @mjsax @bbejeck  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2019-02-25T16:00:18Z","2019-11-25T01:06:14Z"
"","6093","[KAFKA-6989] Add asynchronous processing to Processor API","Here is the design doc for KAFKA-6989: https://cwiki.apache.org/confluence/display/KAFKA/KIP-408%3A+Add+Asynchronous+Processing+To+Kafka+Streams  The implementation details and changes can be found here.","closed","","ConcurrencyPractitioner","2019-01-06T21:06:20Z","2019-10-20T22:34:00Z"
"","5581","MINOR: Add verifiable_producer.properties template file","Having the settings for VerifiableProducer be split up in a separate template file makes them clearer, easier to edit, easier to extend and keeps them consistent with the other implementations (e.g `console_consumer`)  /* Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes. */  Tested it locally by running the system tests","closed","","stanislavkozlovski","2018-08-28T13:40:21Z","2018-08-30T15:49:01Z"
"","6353","KAFKA-8006: Guard calls to init and close from global processor","Guards against users calling init() and close() on global stores accessed via global processor  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-01T23:07:26Z","2019-03-14T18:44:36Z"
"","5964","MINOR: Bump Gradle version to 5.0","Gradle 5.0 was released on 26 November. See the release notes for enhancements and fixes: https://docs.gradle.org/5.0/release-notes.html.  A notable bugfix ensures that Javadoc artifacts are cleaned between builds.  The upgraded wrapper was spot-checked against the commands in the README and the README was updated not to use removed system property `-Dtest.single`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","colinhicks","2018-11-29T00:30:39Z","2018-11-30T00:30:50Z"
"","5865","KAFKA-7420: Global store surrounded by read only implementation","Global store surrounded by read-only `KeyValueStore` implementation. All methods that try to write into Store will throw `UnsupportedOperationException`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","nizhikov","2018-11-01T13:25:06Z","2018-12-05T20:05:46Z"
"","6031","KAFKA-1714: Fix gradle wrapper bootstrapping","Given we need to follow the Apache rule of not checking any binaries into the source code, Kafka has always had a bit of a tricky Gradle bootstrap. Using ./gradlew as users expect doesn’t work and a local and compatible version of Gradle was required to generate the wrapper first.  This patch changes the behavior of the wrapper task to instead generate a gradlew script that can bootstrap the jar itself. Additionally it adds a license, removes the bat script, and handles retries.  The documentation in the readme was also updated.  Going forward patches that upgrade gradle should run `gradle wrapper` before checking in the change.  With this change users using ./gradlew can be sure they are always building with the correct version of Gradle.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","granthenke","2018-12-13T18:10:24Z","2019-11-21T16:06:06Z"
"","5617","KAFKA-7373: Allow GetOffsetShell command to accept a configurations file","GetOffsetShell doesn't provide a mechanism to provide additional configuration for the underlying KafkaConsumer as does the `ConsoleConsumer`. This leaves it unable to connect to a broker using SSL.  This PR allows it to accept a client configuration file, subsequently allowing it to provide SSL configurations and connect to a broker.  I tested this manually. Trying to connect to a broker's SSL listener raised an out of memory error for me. After passing in the appropriate configurations via a config file, it connected successfully","closed","","stanislavkozlovski","2018-09-06T15:17:58Z","2018-09-06T15:34:36Z"
"","5676","HOT-FIX: Fix broken links","Found several broken links in the docs, mostly in `streams-concepts`  For testing, I ran kafka-site locally with these changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-09-21T19:40:37Z","2018-10-04T01:27:20Z"
"","6282","*DO NOT MERGE* Work In Progress For Bug #1194","For some reason, running the tests locally I am receiving different failed tests than on the build server. As I would like to address all failing tests on the build server, I need to trigger a build.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kobihikri","2019-02-17T14:43:22Z","2019-02-20T14:44:24Z"
"","6281","*DO NOT MERGE* - An attempt at https://issues.apache.org/jira/browse/KAFKA-1194","For some reason, running the tests locally I am receiving different failed tests than on the build server. As I would like to address all failing tests on the build server, I need to  trigger a build.","closed","","kobihikri","2019-02-17T11:20:58Z","2019-02-17T14:42:13Z"
"","6201","getConfiguredInstance may return a confusing error","For getConfiguredInstance, if there are classloader differences for the two classes such as in the following error output, this method puts out an error message than makes no sense on the face of it. It looks like Kafka is conflicting with itself.  eg. org.apache.kafka.common.KafkaException: org.apache.kafka.common.serialization.ByteArraySerializer is not an instance of org.apache.kafka.common.serialization.Serializer  The proposed changes indicate that the problem could be the result of classloader differences allowing someone to look in the appropriate direction to resolve their issue.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lonerzzz","2019-01-25T21:02:51Z","2019-11-21T21:13:03Z"
"","5615","KAFKA-7382 We shoud guarantee at lest one replica of partition should be alive w…","For example：I have brokers: 1,2,3,4,5. I create a new topic by command: `sh kafka-topics.sh --create --topic replicaserror --zookeeper localhost:2181 --replica-assignment 11:12:13,12:13:14,14:15:11,14:12:11,13:14:11 ` Then kafkaController will process this,after partitionStateMachine and replicaStateMachine handle state change,topic metadatas and state will be strange,partitions is on NewPartition and replicas is on OnlineReplica.  Next wo can not delete this topic(bacase state change illegal ),This will cause a number of problems.So i think wo shoud check replicas assignment when create or update topic.","open","","zzsmdfj","2018-09-06T09:44:33Z","2018-09-06T09:46:54Z"
"","5897","MINOR: Fix connect-0.11.0.x system-test service","For connect test services, changes directory before starting up.  If we don't change directory to persistent root, connect starts from the user home directory (`~/`). This is problematic only in connect version 0.11.0.x because of a known [classloader failure](https://github.com/apache/kafka/commit/17aaff3606393b42d4e8ef5299141b5bb21300b0#diff-b46eaec4e7101d2e92e71f3f871f3669). It tries to search through the current working directory and if it runs into broken symbolic links, causes connect to throw an exception and stop looking for plugins altogether. This is happening in a downstream `ducktape` test that is using this service running 0.11.0.x.  `kafkatest` connect tests (with change): PASSING  `ducktape` failing test (without change): FAILING `ducktape` fixed test (with change): PASSING  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","brianbushree","2018-11-10T00:36:26Z","2020-10-16T06:03:03Z"
"","6356","KAFKA-3522: add missing guards for TimestampedXxxStore","Follow up to #6152 and #6173","closed","kip,","mjsax","2019-03-02T02:33:15Z","2020-06-12T23:48:39Z"
"","5915","KAFKA-7192: Wipe out state store if EOS is turned on and checkpoint file does not exist","Follow up PR to #5657. Fixes buggy back port.","closed","streams,","mjsax","2018-11-15T03:40:06Z","2018-11-16T18:55:17Z"
"","5960","KAFKA-6988: Improve Kafka classpath length by using all files in dir instead of adding one by one","Fixing Windows too long command due to classpath in case kafka is installed in a directory that has large length, also shorten the Unix classpath as it can have similar problem on docker  the contribution is my original work and I license the work to the project under the project's open source license.","open","","lkgendev","2018-11-28T16:51:37Z","2019-01-18T04:35:37Z"
"","5760","MINOR: Fix small spelling error","Fixing a small spelling error in Javadoc for `StatefulProcessorNode`  For testing, I ran the streams test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-08T20:33:10Z","2018-10-08T21:57:00Z"
"","5955","MINOR: Fix kafkatest/__init__.py to use dev0 instead of SNAPSHOT","Fixes the system tests.","closed","","lbradstreet","2018-11-28T01:03:14Z","2018-11-28T01:31:18Z"
"","6414","HOT_FIX: Changes needed to correct cherry-pick","Fixes needed to correct changes from cherry-pick to `2.1`  I ran all streams tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-08T19:43:02Z","2019-03-09T02:35:11Z"
"","5737","MINOR: Fix bug where StoreBuilder added after connecting store with processor","Fixes bug found in https://github.com/apache/kafka/pull/5731 where we add a state store to the processor before adding the state store to the `InternalTopologyBuilder`  I've added a test which fails without this fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-10-03T20:54:36Z","2018-10-04T02:20:04Z"
"","5811","HOTFIX: Fix compilation issue in streams","Fixed the compilation issue which raised when cleaning up overlapping KAFKA-7080 and KAFKA-7222 KIP changes.  - not sure whether to remove Windows#segments() method so kept it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2018-10-17T11:35:04Z","2018-10-17T11:45:28Z"
"","5878","MINOR: Various javadoc improvement in clients and connect","Fixed formatting issues and added links in a few classes' javadocs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-11-04T15:20:02Z","2018-11-23T05:20:30Z"
"","5532","MINOR: Improved configurations formatting","Fixed a few formatting issues in the configurations  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-08-19T16:46:42Z","2018-08-21T18:39:55Z"
"","6184","KAFKA-7856: Fix vulnerability CWE-331","fix vulnerability pointed by CWE. Cryptographic Issues by Insufficient Entropy (CWE-331 - Flaw medium,SANS TOP 25) This change should be promoted to new versions.","closed","","tbaldo","2019-01-22T14:32:30Z","2019-01-24T22:48:03Z"
"","6251","KAFKA-7909: Ensure timely rebalance completion after pending members rejoin or fail","Fix the following situations, where pending members (one that has a member-id, and has requested to join a group) can cause rebalance operations to fail:   - In AbstractCoordinator, a pending consumer should be allowed to leave. - A rebalance operation must successfully complete if a pending member either joins or times out. - During a rebalance operation, a pending member must be able to leave a group.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2019-02-11T05:51:03Z","2019-02-21T19:20:06Z"
"","6415","KAFKA-7288; Fix check in SelectorTest to wait for no buffered bytes","Fix the check added in PR #6390 to actually wait for no buffered bytes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-08T21:03:10Z","2019-03-09T08:09:57Z"
"","5988","MINOR: improve QueryableStateIntegrationTest","Fix test `Comparators`.","closed","streams,","mjsax","2018-12-02T09:58:03Z","2018-12-03T07:08:35Z"
"","5987","MINOR: improve QueryableStateIntegrationTest","Fix test `Comparators` plus Java8 cleanup","closed","streams,","mjsax","2018-12-02T09:56:58Z","2018-12-03T07:08:19Z"
"","5489","KAFKA-7225: Corrected system tests by generating external properties file","Fix system tests from earlier #5445 by moving to the `ConnectSystemBase` class the creation & cleanup of a file that can be used as externalized secrets in connector configs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-08-10T23:39:37Z","2020-10-16T06:03:01Z"
"","5604","DO-NOT-MERGE KAFKA-1194: Alternative fix","Fix of KAFKA-1194 brought down to the minimal amount of code change. Related to #5603 In testing  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","simplesteph","2018-09-03T15:58:19Z","2018-09-12T11:55:38Z"
"","6435","MINOR: Fix JavaDocs warnings","Fix JavaDocs Warning: ``` > Task :streams:javadoc 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:158: warning - invalid usage of tag < 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:164: warning - invalid usage of tag < 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:164: warning - invalid usage of tag > 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:169: warning - invalid usage of tag > 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:174: warning - invalid usage of tag > 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java:174: warning - invalid usage of tag < 11:19:51 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java:105: warning - invalid usage of tag > 11:19:52 7 warnings ```","closed","core,","mjsax","2019-03-12T20:10:35Z","2019-03-13T16:33:24Z"
"","6020","KAFKA-7580: Allow running Streams unit test as root","Fix for Unit Test 'shouldThrowProcessorStateExceptionOnOpeningReadOnlyDir' fails when run as root user(#KAFKA-7580) Refer https://issues.apache.org/jira/browse/KAFKA-7580  gradle unitTest passes successfully with message ""BUILD SUCCESSFUL""","closed","streams,","sarveshtamba","2018-12-10T09:34:18Z","2019-01-04T05:58:42Z"
"","6096","KAFKA-7789: Increase size of RSA keys used by TestSslUtils","Fix [KAFKA-7789](https://issues.apache.org/jira/browse/KAFKA-7789) by increasing the key size for the RSA keys generated for the tests.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","tombentley","2019-01-07T13:50:45Z","2019-01-16T11:07:46Z"
"","6293","KAFKA-7918: Inline generic parameters Pt. I: in-memory key-value store","First PR in series to inline the generic parameters of the following bytes stores:  [x] InMemoryKeyValueStore  [  ] RocksDBWindowStore  [  ] RocksDBSessionStore  [  ] MemoryLRUCache  [  ] MemoryNavigableLRUCache  [  ] (awaiting merge) InMemoryWindowStore  A number of tests took advantage of the generic InMemoryKeyValueStore and had to be reworked somewhat -- this PR covers everything related to the in-memory key-value store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-02-20T00:33:27Z","2019-02-26T03:59:24Z"
"","5625","KAFKA-5887: Replace findBugs with spotBugs and upgrade to Gradle 4.10","findBugs is abandoned, it doesn't work with Java 9 and the Gradle plugin will be deprecated in Gradle 5.0: https://github.com/gradle/gradle/pull/6664  spotBugs is actively maintained and it supports Java 8, 9 and 10. Java 11 is not supported yet, but it's likely to happen soon.  Also fixed a file leak in Connect identified by spotbugs.  Manually tested spotBugsMain, jarAll and importing kafka in IntelliJ and running a build in the IDE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-08T17:05:23Z","2018-09-11T06:46:29Z"
"","6135","Minor: Prevent resource leak in tests","Few minor improvements: * Wrapped KafkaConsumer with try-with-resources to make sure than stream is closed after test completion despite the test status (success/failure); * Removed unused private method; * added SupressWarning where needed to address compiler warnings; * use static imports across all asserts for consistency.","open","","OleksiiBondar","2019-01-12T08:40:16Z","2019-01-12T08:40:56Z"
"","5835","KAFKA-7535:KafkaConsumer doesn't report records-lag if isolation.level is read_committed","FetchResponse should return the partitionData's lastStabeleOffset  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2018-10-24T06:24:15Z","2019-01-04T11:20:18Z"
"","5761","MINOR: flaky test improvement","extract a more resilient testing pattern from later versions of the test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-08T23:53:57Z","2018-10-11T18:44:06Z"
"","6342","KAFKA-8014: Extend Connect integration tests to add and remove workers dynamically","Extend Connect's integration test harness to add the capability to add and remove workers dynamically as well as discover the set of active workers at any point during an integration test.   The current integration tests are extended to test the new capabilities.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-02-28T05:41:47Z","2020-10-16T05:55:11Z"
"","5516","KAFKA-7503: Connect integration test harness","Expose a programmatic way to bring up a Kafka and Zk cluster through Java API to facilitate integration tests for framework level changes in Kafka Connect. The Kafka classes would be similar to KafkaEmbedded in streams. The new classes would reuse the kafka.server.KafkaServer classes from :core, and provide a simple interface to bring up brokers in integration tests.   Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-08-16T05:17:33Z","2020-10-16T06:03:01Z"
"","6203","KAFKA-7873: Always seek to beginning in KafkaBasedLog","Explicitly seek KafkaBasedLog’s consumer to the beginning of the topic partitions, rather than potentially use committed offsets (which would be unexpected) if group.id is set or rely upon `auto.offset.reset=earliest` if the group.id is null.  This should not change existing behavior but should remove some potential issues introduced with KIP-287 if `group.id` is not set in the consumer configurations. Note that even if `group.id` is set, we still always want to consume from the beginning.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-01-26T00:23:14Z","2020-10-16T06:21:23Z"
"","5558","KAFKA-5445-1: document exceptions for create, list, delete, describe","Exceptions are processed internally in KafkaAdminClient without throwing them to the client code, hence the documentation of the exception is done in unusual way.  The PR is the follow up for #4000 which is gonna be split in 4 different PRs.","open","","adyach","2018-08-22T21:12:50Z","2019-02-26T17:09:58Z"
"","6325","KAFKA-7895: fix stream-time reckoning for Suppress (2.1) (#6286)","Even within a Task, different Processors have different perceptions of time, due to record caching on stores and in suppression itself, and in general, due to any processor logic that may hold onto records arbitrarily and emit them later. Thanks to this, we can't rely on the whole task existing in the same ""instant"" of stream-time. The solution is for each processor node that cares about stream-time to track it independently.  On the side, backporting some internally-facing code maintainability updates  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-25T19:30:33Z","2019-03-05T19:39:14Z"
"","5504","KAFKA-7288: Fix for SslSelectorTest.testCloseConnectionInClosingState","Ensure that sends are completed before waiting for channel to be closed based on idle expiry, since channel will not be expired if added to ready keys in the next poll as a result of pending sends.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-14T12:14:27Z","2018-08-15T08:05:31Z"
"","6374","KAFKA-7976 - Fix DynamicBrokerReconfigurationTest.testUncleanLeaderElectionEnable","Ensure that controller is not shutdown in the test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-05T12:06:03Z","2019-03-06T09:55:06Z"
"","6023","KAFKA-7712: Remove channel from Selector before propagating exception","Ensure that channel and selection keys are removed from `Selector` collections before propagating connect exceptions. They are currently cleared on the next `poll()`, but we can't ensure that callers (NetworkClient for example) wont try to connect again before the next `poll` and hence we should clear the collections before re-throwing exceptions from `connect()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-11T15:44:33Z","2018-12-12T18:50:57Z"
"","5695","MINOR: Fix LogDirFailureTest flake","Ensure that `TestUtils.waitUntilTrue(..)` is blocked on both send completed and a new leader being assigned  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2018-09-26T00:56:41Z","2018-10-08T21:10:05Z"
"","5950","MINOR: Fix handling of dummy record in EndToEndLatency tool","EndToEndLatency tool produces a dummy record in case the topic does not exist. This behavior was introduced in this PR https://github.com/apache/kafka/pull/5319  as part of updating the tool to use latest consumer API. However, if we run the tool with producer acks == 1, the high watermark may not be updated before we reset consumer offsets to latest. In rare cases when this happens, the tool will throw an exception in the for loop where the consumer will unexpectedly consume the dummy record. As a result, we occasionally see Benchmark.test_end_to_end_latency system test failures.  This PR checks if topic exists, and creates the topic using AdminClient if it does not exist.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-11-27T05:58:37Z","2018-11-30T06:23:21Z"
"","6296","Enable pending reassignments cancellation/rollback","Enable pending reassignments (reassignments still in-flight in /admin/reassign_partitions) cancellation/rollback.  Please see https://cwiki.apache.org/confluence/display/KAFKA/KIP-236%3A+Interruptible+Partition+Reassignment  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sql888","2019-02-20T19:46:54Z","2019-03-15T16:32:16Z"
"","6249","MINOR: Improve PlainSaslServer error message for empty tokens","Empty username or password would result in the ""expected 3 tokens"" error instead of ""username not specified"" or ""password not specified"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-10T09:34:18Z","2019-02-12T19:58:29Z"
"","5846","MINOR: Update zstd, easymock, powermock, zkclient and build plugins","EasyMock 4.0.x includes a change that relies on the caller for inferring the return type of mock creator methods. Updated a number of Scala tests for compilation and execution to succeed.  The versions of EasyMock and PowerMock in this PR include full support for Java 11.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-27T17:12:12Z","2018-11-30T14:58:01Z"
"","6403","KAFKA-6188; Fix windows clean log fail caused shut down","During debugging, I find that delete or rename a file before close it is illegal on windows, so I changed the function `deleteSegment` to add a direct close handle operation.  I have manually test it on windows, the change seems very straightforward, please review it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","theidexisted","2019-03-08T09:31:08Z","2021-07-12T08:06:37Z"
"","5519","KAFKA-7186: Avoid re-instantiating UpdateMetadataReuqest and struct objects to reduce controller memory usage","During controller failover and broker changes, it sends out UpdateMetadataRequest to all brokers in the cluster containing the states for all partitions and live brokers. The current implementation will instantiate the UpdateMetadataRequest object and its serialized form (Struct) for `# of brokers` times, which causes OOM if the memory exceeds the configure JVM heap size before GC kicks in. We have seen this issue in the production environment for multiple times.   For example, if we have 100 brokers in the cluster and each broker is the leader of 2k partitions, the extra memory usage introduced by controller trying to send out UpdateMetadataRequest is around:  ```  * <# of brokers> *  = 250B * 100 * 200k = 5GB ```  This PR avoids the recreation of UpdateMetadataReuqest and struct objects if the same builder object is used to build the request. This can significantly reduce memory footprint in Controller based on the above calculation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-08-16T18:35:48Z","2020-04-29T11:18:11Z"
"","5976","KAFKA-7687: Print batch level information in DumpLogSegments","DumpLogSegment should be able to print batch level information when deep-iteration is specified  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-11-30T00:50:45Z","2018-12-04T17:04:40Z"
"","6145","MINOR: Log partition info when creating new request batch in controller","Due to the missing `$`, the name was being logged instead of the value.","closed","","jaceklaskowski","2019-01-15T10:11:03Z","2019-01-19T04:55:23Z"
"","5539","Fix Streams Scala foreach recursive call","Due to lack of conversion to kstream Predicate, existing foreach method in KStream.scala would result in StackOverflowError.  This PR fixes the bug and adds testing for it.","closed","streams,","joan38","2018-08-20T22:30:22Z","2018-08-29T17:22:56Z"
"","5538","KAFKA-7316 Fix Streams Scala filter recursive call","Due to lack of conversion to kstream Predicate, existing filter method in KTable.scala would result in StackOverflowError.  This PR fixes the bug and adds testing for it.","closed","streams,","joan38","2018-08-20T22:28:49Z","2018-08-23T16:28:34Z"
"","5543","KAFKA-7316 Use of filter method in KTable.scala may result in StackOverflowError","Due to lack of conversion to kstream Predicate, existing filter method in KTable.scala would result in StackOverflowError.  This PR fixes the bug and adds calls in StreamToTableJoinScalaIntegrationTestImplicitSerdes.testShouldCountClicksPerRegion to prevent regression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-08-21T04:50:30Z","2018-08-24T00:19:15Z"
"","6132","MINOR: Add -f option to ducker-ak down.","ducker-ak down fails when there are another ducker-ak test running. This commits allows user to forcefully shutdown existing ones. The behavior doesn't change if -f is not supplied.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  The contribution is my original work and I license the work to the project under the project's open source license.  Signed-off-by: Kan Li likan@uber.com  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","likan999","2019-01-12T00:00:40Z","2019-05-24T17:30:45Z"
"","6252","KAFKA-7915: Don't return sensitive authentication errors to clients","Don't return error messages from `SaslException` to clients. Error messages to be returned to clients to aid debugging must be thrown as AuthenticationExceptions. This is a fix for a regression from KAFKA-7352 which is not yet in any release (will be in 2.2.0).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-02-11T15:46:08Z","2019-02-11T22:23:55Z"
"","5509","KAFKA-7119: Handle transient Kerberos errors on server side","Don't report retriable Kerberos errors on the server-side as authentication failures to clients.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-15T09:08:32Z","2018-08-16T08:02:25Z"
"","5790","MINOR: doc changes for KIP-372","Documentation changes for KIP-372  - Adding `Grouped` - Deprecating `Serialized` - Adding `withNamed` to `Joined` - Naming repartition topics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-11T23:07:40Z","2018-10-16T02:02:47Z"
"","5789","MINOR: Doc changes for KIP-312","Documentation changes for adding overloaded `StreamsBuilder(java.util.Properties props)` method in KIP-312    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-11T23:05:37Z","2018-10-16T04:26:19Z"
"","5788","MINOR: Doc changes for KIP-324","Documentation changes for adding KIP-324 `AdminClient.metrics()`    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-11T23:04:01Z","2018-10-16T01:56:47Z"
"","6126","KAFKA-7741: streams-scala - document dependency workaround","Document the workaround detailed in https://issues.apache.org/jira/browse/KAFKA-7741  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-11T17:24:31Z","2019-02-11T18:23:52Z"
"","6125","KAFKA-7741: streams-scala - document dependency workaround","Document the workaround detailed in https://issues.apache.org/jira/browse/KAFKA-7741  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-11T17:21:45Z","2019-01-12T01:16:44Z"
"","6024","KAFKA-7223: document suppression buffer metrics","Document the new metrics added in #5795   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-12-11T17:21:20Z","2018-12-13T02:22:55Z"
"","6354","KAFKA-8018: Flaky Test SaslSslAdminClientIntegrationTest#testLegacyAclOpsNeverAffectOrReturnPrefixed","Disable forceSync in EmbeddedZookeeper. Increase ZK tick to allow longer maxSessionTimeout in tests. Increase ZK client session timeout in tests. Handle transient ZK session expiration exception in test utils for createTopic.","closed","tests,","junrao","2019-03-01T23:38:28Z","2019-03-07T17:39:49Z"
"","5691","KAFKA-7439: Replace EasyMock and PowerMock with Mockito in clients module","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java release cadence, it's a problem to depend on libraries that do bytecode manipulation and are not actively maintained. In addition, Mockito is also easier to use.  While updating the tests, I attempted to go from failing test to passing test. In cases where the updated test passed on the first attempt, I artificially broke it to ensure the test was still doing its job.  I included a few improvements that were helpful while making these changes:  1. Better exception if there are no nodes in `leastLoadedNodes` 2. Always close the producer in `KafkaProducerTest` 3. requestsInFlight producer metric should not hold a reference to `Sender`  Finally, `Metadata` is no longer final so that we don't need `PowerMock` to mock it. It's an internal class, so it's OK.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-25T15:41:02Z","2018-10-10T18:05:52Z"
"","6436","MINOR: gradle-related version upgrades","details:  * gradle:                  5.1.1  -->>  5.4.1  * grgit:                   1.9.3  -->>  3.1.1 (breaking change release: artifact name is changed; also, Grgit.open' usage is slightly refactored)  * gradle-versions-plugin:  0.20.0 -->> 0.21.0  * shadow:                  4.0.3  -->>  4.0.4  * spotless-plugin-gradle: 3.17.0  -->> 3.23.0  * checkstyle:                8.10 -->> 8.20  * spotbugs:                 3.1.8 -->> 3.1.12  * jacoco:                   0.8.2 -->> 0.8.3  related release notes:  * gradle:        ** https://github.com/gradle/gradle/releases/tag/v5.2.0        ** https://github.com/gradle/gradle/releases/tag/v5.2.1        ** https://github.com/gradle/gradle/releases/tag/v5.3.0        ** https://github.com/gradle/gradle/releases/tag/v5.3.1        ** https://github.com/gradle/gradle/releases/tag/v5.4.0        ** https://github.com/gradle/gradle/releases/tag/v5.4.1  * grgit:        ** https://github.com/ajoberstar/grgit/releases/tag/2.0.0        ** https://github.com/ajoberstar/grgit/releases/tag/3.0.0        ** https://github.com/ajoberstar/grgit/releases/tag/3.1.0  * gradle-versions-plugin:        ** https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.21.0  * shadow:        ** https://github.com/johnrengelman/shadow/releases/tag/4.0.4  * spotless-plugin-gradle:        ** https://github.com/diffplug/spotless/blob/gradle/3.23.0/plugin-gradle/CHANGES.md  * checkstyle:        ** http://checkstyle.sourceforge.net/releasenotes.html#Release_8.20  * spotbugs:        ** https://github.com/spotbugs/spotbugs/blob/3.1.12/CHANGELOG.md  * jacoco:        ** https://github.com/jacoco/jacoco/releases/tag/v0.8.3  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","dejan2609","2019-03-13T09:35:38Z","2019-04-29T15:39:08Z"
"","6332","MINOR: gradle and gradle plugins version upgrades","details:  * gradle:                  5.1.1  -->>  5.2.1  * grgit:                   1.9.3  -->>  3.0.0 (breaking change release, 'Grgit.open' usage is refactored)  * ~~gradle-scoverage:        2.5.0  -->>  3.0.0~~ `(opted out, see comments below)`  * shadow:                  4.0.3  -->>  4.0.4  * ~~spotless-plugin-gradle: 3.17.0  -->> 3.18.0~~ `(opted out, see comments below)`  * spotbugs-gradle-plugin:  1.6.9  -->>  1.6.10  related links:  * https://github.com/gradle/gradle/releases/tag/v5.2.0, https://github.com/gradle/gradle/releases/tag/v5.2.1  * https://github.com/ajoberstar/grgit/releases/tag/3.0.0  * https://github.com/scoverage/gradle-scoverage/releases/tag/3.0.0  * https://github.com/johnrengelman/shadow/releases/tag/4.0.4  * https://github.com/diffplug/spotless/blob/gradle/3.18.0/CHANGES.md  * https://github.com/spotbugs/spotbugs-gradle-plugin/blob/1.6.10/CHANGELOG.md#1610---2019-02-18","closed","","dejan2609","2019-02-26T21:41:49Z","2019-03-13T09:44:21Z"
"","6098","KAFKA-7160 Add check for group ID length","Description: validate group id length is <= 265 byte  Testing: added 2 test cases to validate behavior of using group-id = 265 [border] & group-id > 265  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rosama86","2019-01-07T19:27:22Z","2019-03-04T12:51:44Z"
"","5621","KAFKA-7351 [WIP] return true for non existant ids in state map","Description Handle case when id of node does not exist in ClusterConnectionStates  Testing Added unit test for specific case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mutdmour","2018-09-07T16:36:06Z","2018-09-08T06:13:19Z"
"","6200","[WIP] demonstrating my thoughts on #6192","demonstrating my thoughts on #6192  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2019-01-25T00:16:39Z","2019-01-25T19:22:03Z"
"","5999","KAFKA-7697: Process DelayedFetch without holding leaderIsrUpdateLock","Delayed fetch operations acquire leaderIsrUpdate read lock of one or more Partitions from the fetch request when attempting to complete the fetch operation. When attempting to complete delayed fetch after appending new records, completion should be attempted only after releasing the leaderIsrUpdate of the Partition to which records were appended. Otherwise, waiting writers (e.g. to check if ISR needs to be shrinked) can cause deadlocks in request handler threads when trying to acquire lock of a different partition while holding on to lock of one partition.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-04T13:46:41Z","2018-12-05T09:05:27Z"
"","5845","MINOR: Remove unused commitSync in ConsoleConsumer","Dead code is confusing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-26T15:52:00Z","2018-10-26T19:19:02Z"
"","5882","MINOR: Modify Connect service's startup timeout to be passed via the init","Currently, the startup timeout is hardcoded to be 60 seconds in Connect's test service. Modifying it to be passable via init. This can safely be backported as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-11-05T18:18:30Z","2020-10-16T06:21:20Z"
"","5554","kafkatest: Make JAAS config configurable via template variables","Currently, the only way to add a new variable to the `jaas.conf` template file is to directly edit the path the config is constructed by adding new keyword arguments. This wasn't necessarily a big problem, since you'd only need edit the `security_config.py` file as JAAS settings should come from the security settings.  Now, with the addition of [KIP-342](https://cwiki.apache.org/confluence/display/KAFKA/KIP-342%3A+Add+support+for+Custom+SASL+extensions+in+OAuthBearer+authentication), the OAuthBearer JAAS config supports arbitrary values in the form of SASL extensions.  I thought it would be nice if we could make adding arbitrary values to the JAAS config in tests as painless as possible for future tests.  ### Future work The same problem to a lesser extent exists in the `.properties` template files for consumer/producer.  That one can easily be worked around, since ducktape attaches a class' variables to the template file (https://github.com/confluentinc/ducktape/blob/d7226d83aa11538765d352f69bbf75e0c27e6304/ducktape/template.py#L34) We can see this being used in multiple tests: * https://github.com/apache/kafka/blob/3511e904effbfb74fa54c26467477863b7f730c0/tests/kafkatest/services/mirror_maker.py#L81 - variables used in `mirror_maker_consumer.properties` * https://github.com/apache/kafka/blob/3511e904effbfb74fa54c26467477863b7f730c0/tests/kafkatest/services/mirror_maker.py#L78 - variable used in `mirror_maker_producer.properties`  I find it would be better if config variables were set explicitly and would not require the developer to read through the framework code to understand how they're populated. So future work might make all sorts of config variables passable through constructors. (Maybe we could have a single class encapsulating all variables, then, as having producer/consumer constructors accept `jaas_override_variables` is a bit non-ideal)","closed","","stanislavkozlovski","2018-08-22T10:08:17Z","2018-08-23T21:55:41Z"
"","5515","KAFKA-7299: Batch LeaderAndIsr requests for AutoLeaderRebalance","Currently, in KafkaController.checkAndTriggerAutoLeaderRebalance(), we call onPreferredReplicaElection() one partition at a time. This means that the controller will be sending LeaderAndIsrRequest one partition at a time. It would be more efficient to call onPreferredReplicaElection() for a batch of partitions to reduce the number of LeaderAndIsrRequests.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-08-16T02:31:59Z","2018-08-16T21:54:59Z"
"","5940","KAFKA-4493: Throw InvalidTransportLayerException when a plaintext client receives a TLS response","Currently, if a client if misconfigured to not use `ssl` and conencts to a broker with SSL listeners, the broker responds with a TLS Alert response. The client interprets the first 4 TLS alert response bytes as the size of the response, resulting in it trying to allocate `352518912` bytes. This either raises an `OutOfMemory` exception or stalls on the `allocate()` call. Either way, we should fatally throw a more clear exception so the user can know what is wrong. I personally spent more time than I'm comfortable sharing debugging this issue and I'd like to see it improved.  ### Implementation On the first read of a `PlaintextTransportLayer` channel, we check against the pre-defined TLS alert headers, which consist of the alert byte `0x15` and the TLS version `0301`,`0302`, `0303`.  I believe that it should be alright to close such responses since we would never expect to receive a 3mb response on the first response in a newly-established connection since we should at least verify `API_VERSIONS` first.  I'm not entirely certain whether this warrants creation of a KIP since we are raising a new exception - `InvalidTransportLayerException`. That is not publicly exposed, right?  cc @rajinisivaram for review","open","","stanislavkozlovski","2018-11-22T11:51:02Z","2020-05-28T16:36:32Z"
"","6051","KAFKA-7759: Disable WADL output on OPTIONS method in Connect REST.","Currently, Connect REST endpoint replies to OPTIONS request with verbose WADL information, which could be used for an attack. This was never documented or intended to expose. More discussion is [here]  (https://lists.apache.org/thread.html/84eb4538397ae4544d20c072c936d9a31f22f429a0891cbb7d8e2296@%3Cdev.kafka.apache.org%3E)  Added unit tests in RestServerTest, which asserts that calling `OPTIONS` on `/connectors` replies with a list of supported HTTP methods, with no WADL information.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","avocader","2018-12-19T23:51:59Z","2020-10-16T06:21:21Z"
"","6025","KAFKA-7715: Added a configuration parameter to Connect which disables WADL output for OPTIONS method.","Currently, Connect REST endpoint replies to `OPTIONS` request with verbose WADL information, which could be used for an attack. It's not recommended for the production system to expose that information, but for the backward-compatibility reasons, it may still be available by default, with a possibility to turn it off by setting `rest.wadl.enable=false`.  Added unit tests in `RestServerTest`, which asserts that `Content-type` is either `application/vnd.sun.wadl+xml` if `rest.wadl.enable=true` or `rest.wadl.enable` is not set;  or `text/plain` otherwise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","avocader","2018-12-12T01:49:33Z","2020-10-16T06:21:21Z"
"","6066","KAFKA-7763: commitTransaction and abortTransaction should not wait endless when brokers are down","Currently, commitTransaction and abortTransaction invoke Latch#await to wait for the request completion. When brokers are not available, these calls await forever and producer never ends. This fix replaces it with the timed equivalent await(timeout, unit). When timeout occurred, producer throws a TimeoutException.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-12-26T03:30:03Z","2019-02-21T22:18:37Z"
"","5622","KAFKA-7386: streams-scala should not cache serdes","Currently, `scala.Serdes.String`, for example, invokes Serdes.String() once and caches the result.  However, the implementation of the String serde has a non-empty configure method that is variant in whether it's used as a key or value serde. So we won't get correct execution if we create one serde and use it for both keys and values.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-09-07T17:21:48Z","2018-09-13T05:45:44Z"
"","6124","KAFKA-7808: AdminClient#describeTopics should not throw InvalidTopicException if topic name is not found","Currently, `AdminClient#describeTopics` is called in `WorkerUtils#[getMatchingTopicPartitions,verifyTopics]` only. However, `WorkerUtils#getMatchingTopicPartitions` does not being called by anyone - so it should be removed.  1. Update `KafkaAdminClient#describeTopics` to throw `UnknownTopicOrPartitionException`. 2. Change the Exception message thrown by `MockAdminClient#describeTopics`: for consistency with `KafkaAdminClient`. 3. Remove unused method: `WorkerUtils#getMatchingTopicPartitions`. 4. Add @throws description on `WorkerUtils#verifyTopics`: when `UnknownTopicOrPartitionException` is thrown. 5. Expand javadoc on `WorkerUtils#createTopics(Logger, AdminClient, Map, boolean)`: the only method which calls `WorkerUtils#verifyTopics`.  Since the `Throwable` instance thrown by `WorkerUtils#createTopics(Logger, AdminClient, Map, boolean)` is caught by `[ProduceBenchWorker,RoundTripWorker].Prepare`, we don't need more cleanup.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-01-11T14:38:54Z","2019-01-14T17:27:51Z"
"","5869","KAFKA-7537: Avoid sending full UpdateMetadataRequest to existing brokers in the cluster on broker changes to reduce controller memory footprint","Currently when brokers join/leave the cluster without any partition states changes, controller will send out UpdateMetadataRequests containing the states of all partitions to all brokers. But for existing brokers in the cluster, the metadata diff between controller and the broker should only be the ""live_brokers""/""offline_replicas"" info. Only the brokers with empty metadata cache need the full UpdateMetadataRequest. Sending the full UpdateMetadataRequest to all brokers can place nonnegligible memory pressure on the controller side, especially for large clusters with many brokers and a large number of partitions.  Let's say in total we have N brokers, M partitions in the cluster and we want to add 1 brand new broker in the cluster. With RF=2, the memory footprint per partition in the UpdateMetadataRequest is ~200 Bytes. In the current controller implementation, if each of the N RequestSendThreads serializes and sends out the UpdateMetadataRequest at roughly the same time (which is very likely the case), we will end up using (N+1)*M*200B memory. However, we only need to send out one full UpdateMetadataReuqest in this case. More detail can be found in the jira ticket KAFKA-7537.  This PR avoids sending out full UpdateMetadataReuqest in the following scenarios: 1. On broker startup, send out full UpdateMetadataRequest to newly added brokers and only send out UpdateMetadataReuqest with empty partition states to existing brokers. 2. On broker failure, if it doesn't require leader election, only include the states of partitions that are hosted by the dead broker(s) in the UpdateMetadataReuqest instead of including all partition states.  _Note that after partition state change and replica state change, UpdateMetadataReuqests still need to be sent and this behavior remains unchanged after this PR._  This PR also introduces a minor optimization in the MetadataCache update to avoid copying the previous partition states upon receiving UpdateMetadataRequest with no partition states.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-11-01T22:52:59Z","2018-11-06T23:28:54Z"
"","5886","KAFKA-7560; PushHttpMetricsReporter should not convert metric value to double","Currently PushHttpMetricsReporter will convert value from KafkaMetric.metricValue() to double. This will not work for non-numerical metrics such as version in AppInfoParser whose value can be string. This has caused issue for PushHttpMetricsReporter which in turn caused system test kafkatest.tests.client.quota_test.QuotaTest.test_quota to fail.    Since we allow metric value to be object, PushHttpMetricsReporter should also read metric value as object and pass it to the http server.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-11-06T19:25:50Z","2018-12-07T02:46:00Z"
"","5832","MINOR: Fix undefined variable in Connect test","Corrects an error in the system tests: ``` 07:55:45 [ERROR:2018-10-23 07:55:45,738]: Failed to import kafkatest.tests.connect.connect_test, which may indicate a broken test that cannot be loaded: NameError: name 'EXTERNAL_CONFIGS_FILE' is not defined ```  The constant is defined in the [services/connect.py](https://github.com/apache/kafka/blob/trunk/tests/kafkatest/services/connect.py#L43) file in the `ConnectServiceBase` class, but the problem is in the [tests/connect/connect_test.py](https://github.com/apache/kafka/blob/trunk/tests/kafkatest/tests/connect/connect_test.py#L50) `ConnectStandaloneFileTest`, which does *not* extend the `ConnectServiceBase class`. Suggestions welcome to be able to reuse that variable without duplicating the literal (as in this PR).  System test run with this PR: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2004/  If approved, this should be merged as far back as the `2.0` branch.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-10-23T15:30:16Z","2018-10-24T20:19:55Z"
"","6439","KAFKA-8093: fixed some javadoc errors","corrected link to Oracle documentation according to https://discuss.gradle.org/t/javadoc-task-link-not-linking-external-javadocs/25264/3 Question: Should the link really go to 7?  If I run ""aggregatedJavadoc"" locally I get ERRORs because of exclude ""**/internals/**"":  > Task :aggregatedJavadoc /Users/patrik.kleindl/IdeaProjects/kafka_pkleindl/streams/test-utils/src/main/java/org/apache/kafka/streams/processor/MockProcessorContext.java:31: error: cannot find symbol import org.apache.kafka.streams.internals.QuietStreamsConfig;                                          ^   symbol:   class QuietStreamsConfig   location: package org.apache.kafka.streams.internals /Users/patrik.kleindl/IdeaProjects/kafka_pkleindl/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:43: error: cannot find symbol import org.apache.kafka.streams.internals.KeyValueStoreFacade;                                          ^   symbol:   class KeyValueStoreFacade   location: package org.apache.kafka.streams.internals /Users/patrik.kleindl/IdeaProjects/kafka_pkleindl/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:44: error: cannot find symbol import org.apache.kafka.streams.internals.QuietStreamsConfig;                                          ^   symbol:   class QuietStreamsConfig   location: package org.apache.kafka.streams.internals /Users/patrik.kleindl/IdeaProjects/kafka_pkleindl/streams/test-utils/src/main/java/org/apache/kafka/streams/TopologyTestDriver.java:45: error: cannot find symbol import org.apache.kafka.streams.internals.WindowStoreFacade;                                          ^   symbol:   class WindowStoreFacade   location: package org.apache.kafka.streams.internals 4 errors  I guess the problem is that this excludes everything in the path of internals (including test-utils), maybe this was only meant for stuff inside streams?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pkleindl","2019-03-13T23:16:38Z","2019-05-23T21:35:15Z"
"","6069","KAFKA-7772: Adjust log levels of classes in Connect via JMX","Consolidating ways to adjust log levels in AK. This change provides a common way to modify log levels of running Kafka brokers and Kafka Connect workers via a JMX interface.  Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-12-27T23:07:54Z","2020-10-16T06:21:22Z"
"","5746","MINOR: More docs on Connector#stop","Connector#stop/Connector#start can be invoked by PAUSE/RESUME, but ConnectorTask#stop/ConnectorTask#start can't. We should doc that for Connector.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-10-05T08:14:21Z","2020-03-17T08:59:27Z"
"","5659","MINOR: remove idempotent statement","Compare https://github.com/apache/kafka/pull/5657/files#r218147757  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-09-17T23:14:50Z","2018-09-20T00:17:32Z"
"","6233","KAFKA-7902: Replace original loginContext if SASL/OAUTHBEARER refresh login fails","Code fix included.  Will add a test case ASAP -- hopefully today.  It is possible for a Java SASL/OAUTHBEARER client (either a non-broker producer/consumer client or a broker when acting as an inter-broker client) to end up in a state where it cannot connect to a new broker (or, if re-authentication as implemented by KIP-368 and merged for v2.2.0 were to be deployed and enabled, to be unable to re-authenticate). The error message looks like this:  `Connection to node 1 failed authentication due to: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: Unable to find OAuth Bearer token in Subject's private credentials (size=2) [Caused by java.io.IOException: Unable to find OAuth Bearer token in Subject's private credentials (size=2)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTHENTICATION_FAILED state.`  The root cause of the problem begins at this point in the code:  https://github.com/apache/kafka/blob/2.0/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/expiring/ExpiringCredentialRefreshingLogin.java#L378:  The `loginContext` field doesn't get replaced with the old version stored away in the `optionalLoginContextToLogout` variable if/when the `loginContext.login()` call on line 381 throws an exception. This is an unusual event – the OAuth authorization server must be unavailable at the moment when the token refresh occurs – but when it does happen it puts the refresher thread instance in an invalid state because now its `loginContext` field represents the one that failed instead of the original one, which is now lost. The current `loginContext` can't be logged out – it will throw an `InvalidStateException` if that is attempted because there is no token associated with it – and the token associated with the login context that was lost can never be logged out and removed from the Subject's private credentials (because we don't retain a reference to it). The net effect is that we end up with an extra token on the Subject's private credentials, which eventually results in the exception mentioned above when the client tries to authenticate to a broker.  So the chain of events is:  1.  login failure upon token refresh causes the refresher thread's login context field to be incorrect, and the existing token on the Subject's private credentials will never be logged out/removed 2. retry occurs in 10 seconds, potentially repeatedly until the authorization server is back online 3. login succeeds, adding a second token to the Subject's private credentials (logout is then called on the login context set incorrectly in the most recent failure – e.g. in step 1 – which results in an exception, but this is not the real issue – it is the 2 tokens on the Subject's private credentials that is the issue) 4. At this point we now have 2 tokens on the Subject, and then at some point in the future the client tries to make a new connection, it sees the 2 tokens and throws an exception – BOOM! The client is now unable to connect (or re-authenticate if applicable) going forward.","closed","","rondagostino","2019-02-06T15:41:33Z","2021-09-14T17:05:03Z"
"","5526","MINOR: fixing broken link in protocol docs","Co-authored-by: Mickael Maison  Co-authored-by: Gantigmaa Selenge    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tinaselenge","2018-08-17T14:00:12Z","2018-08-24T21:03:59Z"
"","5898","NONE - Closed in favor of PR #5910","Closed in favor of [PR #5910](https://github.com/apache/kafka/pull/5910)","closed","","mrsrinivas","2018-11-11T06:44:39Z","2018-11-26T05:40:11Z"
"","6146","MINOR: Update usage of deprecated API","close(long, TimeUnit) was deprecated and replaced with close(Duration);","closed","streams,","mjsax","2019-01-15T17:33:41Z","2019-01-29T19:15:48Z"
"","5486","MINOR: Clean up to avoid errors in dynamic broker config tests","Clean up cached instances in dynamic broker config on broker shutdown to avoid spurious session expiry errors in tests which restart broker.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-09T13:11:06Z","2018-08-14T16:14:49Z"
"","6195","MINOR: clarify why suppress can sometimes drop tombstones","Clarify when and why suppress would drop tombstone records. No behavioral change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-24T14:57:36Z","2019-01-25T22:19:04Z"
"","5993","KAFKA-7678: Avoid NPE when closing the RecordCollector","Check if the Producer is not null to avoid calling **producer.close()** on a **null** value.","closed","streams,","jonathansantilli","2018-12-03T22:04:31Z","2018-12-10T08:51:57Z"
"","5866","KAFKA-7538: Reduce lock contention for Partition ISR lock","Check for ISR updates using ISR read lock and acquire ISR write lock only if ISR needs to be updated. This avoids lock contention between request handler threads processing log appends on the leader holding the ISR read lock and request handler threads processing replica fetch requests that check/update ISR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-11-01T16:03:20Z","2020-01-14T14:51:31Z"
"","5630","KAFKA-7388 equal sign in property value for password","Changing limit of split to 2 values, to only split to key and value. if split and value is empty, also throw error.   * Added test to check feature with delimiter (equal sign) in different pos * deleted test that enforced no multiple equal signs in properties  * added tests for password overrides with symbols  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mutdmour","2018-09-09T17:02:44Z","2018-09-18T08:36:00Z"
"","5930","KAFKA-7446: Fix the duration and instant validation messages","Changes made as part of this PR for [KAFKA-7446](https://issues.apache.org/jira/browse/KAFKA-7446)  - Improved error message for better readability  - Corrected java documentation on `AdvanceInterval` check.  Core and Streams modules' JUnit tests are successful in local but  tests related to file system is failed since I am running on Windows.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mrsrinivas","2018-11-19T18:12:07Z","2018-12-04T06:59:55Z"
"","5910","KIP-374: Add the missing '--help' option to Kafka commands (KAFKA-7418)","Changes made as part of this [KIP-374](https://cwiki.apache.org/confluence/x/FgSQBQ) and [KAFKA-7418](https://issues.apache.org/jira/browse/KAFKA-7418)  - Checking for empty args or help option in command file to print Usage  - Added new class to enforce help option to all commands  - Refactored few lines (ex `PreferredReplicaLeaderElectionCommand`) to    make use of `CommandDefaultOptions` attributes.  - Made the changes in help text wordings  Run the unit tests in local(Windows) few Linux friendly tests are failing but  not any functionality, verified `--help` and no option response by running  Scala classes, since those all are having `main` method.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mrsrinivas","2018-11-13T17:02:31Z","2019-01-10T09:16:25Z"
"","6040","KAFKA-7691; Encypt-then-MAC Delegation token metadata","Change-Id: I857d45933b5411b8f55f93ac79e36db86d334d73  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* This is an implementation of [KIP-395: Encypt-then-MAC Delegation token metadata](https://cwiki.apache.org/confluence/display/KAFKA/KIP-395%3A+Encypt-then-MAC+Delegation+token+metadata).  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asasvari","2018-12-17T13:47:42Z","2018-12-18T10:47:43Z"
"","5905","KAFKA-7618: Fix /coordinator/tasks parameters to accept long values","cc @cmccabe","closed","","stanislavkozlovski","2018-11-12T13:36:37Z","2018-11-13T16:35:04Z"
"","5833","KAFKA-7534: Error in flush calling close may prevent underlying store  from closing","Calling the `CachingKeyValueStore#close()` method first calls `CachingKeyValueStore.flush()`.  If there is an exception thrown during the `flush` call, the underlying store is not closed.  Subsequently, another task can't open the RocksDB store and receives a `No locks available` exception.  I added a unit test that fails with the proposed fix.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-10-24T00:34:27Z","2018-10-26T23:08:14Z"
"","5963","MINOR: avoid miss-leading log statement for KafkaStreams#close()","Calling `KafkaStream#close()` should not call deprecated `#close()` because this will log miss leading log statement about using a deprecated method.","closed","streams,","mjsax","2018-11-28T21:07:26Z","2018-11-30T01:51:25Z"
"","6089","MINOR: Use functional patterns in PartitionStates","By introducing some functional apis, we can eliminate some filtering and collection boilerplate. This also removes the often unnecessary copy in `PartitionStates.partitionStates()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-01-04T17:49:53Z","2019-01-05T11:52:03Z"
"","5580","git first commit message","by chenxiaolei  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chenxiaolei8","2018-08-28T09:51:00Z","2018-09-01T15:06:00Z"
"","5816","KAFKA-7517: Add whitelist to Range config validator. Ensure retention bytes cannot be 0","By adding a whitelist option to the `Range` config validator we can ensure that certain configs can still use their special value of -1 while not being less than a certain value","open","","stanislavkozlovski","2018-10-18T13:16:39Z","2019-07-13T21:33:27Z"
"","5920","KAFKA-7639: Read one request at a time from socket to avoid OOM","Broker's Selector currently reads all requests available on the socket when the socket is ready for read. These are queued up as staged receives. This can result in excessive memory usage when socket read buffer size is large. We mute the channel and stop reading any more data until all the staged requests are processed. This behaviour is slightly inconsistent since for the initial read we drain the socket buffer, allowing it to get filled up again, but if data arrives slighly after the initial read, then we dont read from the socket buffer until pending requests are processed.   To avoid holding onto requests for longer than required, this PR removes staged receives and reads one request at a time even if more data is available in the socket buffer. This is especially useful for produce requests which may be large and may take long to process. Additional data read from the socket for SSL is limited to the pre-allocated intermediate SSL buffers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-11-16T17:37:32Z","2020-01-13T09:43:11Z"
"","6222","KAFKA-3522: Remove TimestampedByteStore from public API","Because KIP-258 slips, this PR moves `TimestampedByteStore` into internal package to not leak public API changes in 2.2 release. We will move it back, after 2.2 branch it cut.","closed","kip,","mjsax","2019-02-02T00:35:46Z","2020-06-12T23:52:10Z"
"","5984","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953) (see also 2.1: #5979, 2.0: #5980, 1.1: #5981, 1.0: #5982, 0.11: #5983  )  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-30T23:39:32Z","2018-12-07T17:44:02Z"
"","5983","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953) (see also 2.1: #5979, 2.0: #5980, 1.1: #5981, 1.0: #5982  )  It looks like we already had the parentSensors fix in 0.11.0 and lost it in 2.0. The change in this PR just tidies it up a little for consistency with later branches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-30T23:28:01Z","2018-12-07T17:43:52Z"
"","5982","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953) (see also 2.1: #5979, 2.0: #5980, 1.1: #5981  )  It looks like we already had the parentSensors fix in 1.0 and lost it in 2.0. The change in this PR just tidies it up a little for consistency with later branches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-30T23:26:26Z","2018-12-07T17:43:39Z"
"","5981","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953) (see also 2.1: #5979, 2.0: #5980 )  It looks like we already had the parentSensors fix in 1.1 and lost it in 2.0. The change in this PR just tidies it up a little for consistency with later branches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-11-30T23:03:29Z","2018-12-07T17:43:33Z"
"","5980","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953) (see also 2.1: #5979)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-11-30T22:27:09Z","2018-12-04T21:53:37Z"
"","5979","KAFKA-7660: fix streams and Metrics memory leaks","Backport two memory-leak fixes (#5974 and #5953)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-11-30T21:49:55Z","2018-12-04T21:53:11Z"
"","5904","Not log retriable exceptions as errors","Background: I've spotted tons of kafka related errors in logs, after investigation I found out that those are harmless as being retried.  Hence I propose to not log retriable exceptions as errors.  Examples of what I've see in logs:  * Offset commit failed on partition .. at offset ..: The request timed out. * Offset commit failed on partition .. at offset ..: The coordinator is loading and hence can't process requests. * Offset commit failed on partition .. at offset ..: This is not the correct coordinator. * Offset commit failed on partition .. at offset ..: This server does not host this topic-partition.","closed","","t3hnar","2018-11-12T11:37:46Z","2019-04-25T17:10:42Z"
"","5612","MINOR: fix Streams EOS tests","Back porting #5501 broke some tests","closed","","mjsax","2018-09-04T23:16:39Z","2018-09-05T18:03:29Z"
"","6119","KAFKA-7811: Avoid unnecessary lock acquire when KafkaConsumer commits offsets","Avoid unnecessary lock acquire when KafkaConsumer commits offsets.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-01-10T13:51:39Z","2019-02-15T02:12:01Z"
"","6207","KAFKA-7741: Reword Streams dependency workaround docs","Avoid mentioning unreleased versions in the docs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-28T01:03:11Z","2019-02-11T18:26:08Z"
"","5836","MINOR: Reduce intermittent test failures for testMarksPartitionsAsOfflineAndPopulatesUncleanableMetrics and make log cleaner tests more efficient","As seen in https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/239/testReport/junit/kafka.log/LogCleanerIntegrationTest/testMarksPartitionsAsOfflineAndPopulatesUncleanableMetrics/  This test sometimes fails because of passing the 15 second timeout. Inspecting the error message from the build failure, we see that this timeout happens in the `writeDups()` calls which call `roll()`. ``` [2018-10-23 15:18:51,018] ERROR Error while flushing log for log-1 in dir /tmp/kafka-8190355063195903574 with offset 74 (kafka.server.LogDirFailureChannel:76) java.nio.channels.ClosedByInterruptException ... 	at kafka.log.Log.roll(Log.scala:1550) ... 	at kafka.log.AbstractLogCleanerIntegrationTest.writeDups(AbstractLogCleanerIntegrationTest.scala:132) ... ```  After investigating, I saw that this test would call `Log#roll()` around 60 times every run. Increasing the `segmentSize` config to `5120` reduces the `Log#roll()` calls to 4 per test. I saw that most other LogCleaner tests also call `roll()` ~90 times, so I've changed the default to be `5120`. I've also made the one test which requires a smaller segmentSize to set it via the args","closed","","stanislavkozlovski","2018-10-24T09:35:19Z","2018-10-27T19:31:31Z"
"","5831","KAFKA-7532: Fix controller log for list of shutting down brokers","As reported in [KAFKA-7532](https://issues.apache.org/jira/browse/KAFKA-7532) - this line prints out  ``` [2018-10-23 12:19:59,977] INFO [Controller id=0] Removed ArrayBuffer() from list of shutting down brokers. (kafka.controller.KafkaController) ```  This change seems to be present in all versions even as back as `0.9.0`. I think it might be worth it to merge to `1.0` (pre-2.0 uses `"""".format()`)","closed","","stanislavkozlovski","2018-10-23T15:25:15Z","2018-10-26T13:50:07Z"
"","5562","KIP-363 Make FunctionConversions deprecated","As pointed out in this comment https://github.com/apache/kafka/pull/5539#discussion_r212380648 the object `FunctionConversions` is only of internal use and therefore should be private to the lib only so that we can do changes without going through KIP like this one.  KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-363%3A+Make+FunctionConversions+private  @guozhangwang @vvcephei @mjsax @debasishg and whoever wants to join the party 😄","closed","streams,","joan38","2018-08-23T18:43:14Z","2018-09-19T16:10:16Z"
"","5855","KAFKA-7568; Return leader epoch in ListOffsets response","As part of KIP-320, the ListOffsets API should return the leader epoch of any fetched offset. We either get this epoch from the log itself for a timestamp query or from the epoch cache if we are searching the earliest or latest offset in the log. When handling queries for the latest offset, we have elected to choose the current leader epoch, which is consistent with other handling (e.g. OffsetsForTimes).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-30T07:04:41Z","2018-11-01T21:34:29Z"
"","5634","KAFKA-7394; OffsetsForLeaderEpoch supports topic describe access","As part of KIP-320, allow OffsetsForLeaderEpoch requests with Topic Describe permission.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-09-10T18:34:18Z","2018-09-13T06:34:42Z"
"","5925","KAFKA-7549: Old ProduceRequest with zstd compression does not return error to client","As of current version (2.1.0), zstd-related validations are located in following spots:  1. **ProduceRequest**      - `MemoryRecordsBuilder`: can't create `MemoryRecords` with magic < 2 (`IllegalArgumentException`)     - `ProduceRequest.Builder`: can't create `ProduceRequest` with api version below 7 (`InvalidRecordException`)  2. **FetchRequest**      - `KafkaApis#handleFetchRequest`: Returns `FetchResponse` w/ `Errors#UNSUPPORTED_COMPRESSION_TYPE` if ...         1. `FetchRequest` w/ API version < 10 is delivered to a zstd-compressed topic.         2. Down-conversion failure: `LazyDownConversionRecords#makeNext` → `RecordsUtil#downConvert` throws `UnsupportedCompressionTypeException`.  3. **Etc**      - `AbstractLegacyRecordBatch.DeepRecordsIterator`: A boilerplate validation for legacy record batches.  **In short, there is no broker-side validation for `ProduceRequest` w/ zstd compressed records.** This PR compensates this hole.  There is a reason why this validation can't be located in other class, e.g., `LogValidator`: it can't see the API version of `ProduceRequest.` The only method that can check both of `CompressionType` and API version is `KafkaApis#handleProduceRequest`; it's why.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2018-11-18T17:31:08Z","2018-12-10T17:45:19Z"
"","6396","KAFKA-8040: Streams handle initTransactions timeout","As of 2.0, Producer.initTransactions may throw a TimeoutException. Streams should log an explanatory error and throw a StreamsException to signal an intentional graceful shutdown in the face of an error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-03-07T22:59:28Z","2019-03-07T23:02:30Z"
"","6416","KAFKA-8040: Streams handle initTransactions timeout","As of 2.0, Producer.initTransactions may throw a TimeoutException, which is retriable. Streams should retry instead of crashing when we encounter this exception  Reviewers: Guozhang Wang , Matthias J. Sax ,  Bill Bejeck   Cherry-pick of #6372  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-03-08T22:36:04Z","2019-03-11T22:22:00Z"
"","6372","KAFKA-8040: Streams retry initTransactions on timeout","As of 2.0, `Producer.initTransactions` may throw a TimeoutException, which is retriable. Streams should retry instead of crashing when we encounter this exception.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-03-05T02:33:09Z","2019-03-08T17:01:56Z"
"","6121","KAFKA-7741: Streams exclude javax dependency","As documented in https://issues.apache.org/jira/browse/KAFKA-7741, the javax dependency we receive transitively from connect is incompatible with SBT builds.  Streams doesn't use the portion of Connect that needs the dependency, so we can fix the builds by simply excluding it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-01-10T20:15:47Z","2019-01-11T00:23:49Z"
"","5512","KAFKA-7269: Add docs for KStream.merge","As discussed in KAFKA-7269","closed","docs,","lucapette","2018-08-15T20:22:34Z","2018-08-29T17:04:20Z"
"","6193","[WIP] KIP-81 Bound Fetch memory usage in the consumer","As discussed in https://github.com/apache/kafka/pull/4934#discussion_r236339139, this is a PoC implementation for KIP-81. This does not rely on tagging Node to determine which responses should be allocated in the MemoryPool.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-01-24T10:26:52Z","2022-02-09T13:46:54Z"
"","6130","KAFKA-7792: Add simple /agent/uptime and /coordinator/uptime health check endpoints","As described in https://issues.apache.org/jira/browse/KAFKA-7792, Trogdor should have a simple uptime function.  ### Changes This PR adds two new endpoints, one for the Trogdor Agent and one for the Trogdor Coordinator. `GET /agent/uptime` `GET /coordinator/uptime`  Both return a 200 status code and a JSON response informing the user how long the agent/coordinator has been running for: ``` {""serverStartMs"": 1, ""nowMs"": 2} ```  It is worth noting that the `/coordinator/uptime` is a bit superfluous, as the `/coordinator/status` endpoint returns a JSON containing the time the coordinator started - `{""serverStartMs"": 1547234982334}`. The main motivation behind this PR is that the `/agent/status` is a potentially heavyweight call. I added the `/coordinator/uptime` for consistency","closed","","stanislavkozlovski","2019-01-11T19:30:55Z","2019-01-15T19:52:49Z"
"","6123","MINOR: Update Trogdor StringExpander regex to handle epilogue","As @yangxi pointed out, having the lazy quantifier `?` in the epilogue group of the regex `(.*?)` would mean that strings after the range quantifier would not get caught. e.g `""aaa[1-2]aa"" => ""aaa1, aaa2""`  Also see/play-around with https://regex101.com/r/WGIT7W/1  I also removed the lazy quantifier in the beginning - no strong opinion there - but I'd somehow prefer if the regex would pick the right-most range expression when using nested expressions. Removing the lazy quantifier in the beginning ensures that. Seems more natural.  cc @cmccabe for review","closed","","stanislavkozlovski","2019-01-11T06:20:18Z","2019-01-15T04:49:24Z"
"","5801","STREAMS: Multiple topics extractor proposal","Approach of adding possibility to extract several topics for flexible message routing.  I didn't find ""nice"" way how to apply such feature and keep all the changes backward compatible so I decided to ask if this change is acceptable via creating a PR. If it's OK - I'll finish all needed stuff (documentation and tests)  This would be awesome change, which will allow to chose several topics for single message dynamically based on key/value or application configuration.  Please let me know should I go further with this change and polish it.","open","kip,","tdanylchuk","2018-10-15T19:13:37Z","2020-06-12T23:03:03Z"
"","6114","MINOR: Put state args in correct order named repartition test","Another system test that needs to be updated with states in the correct order  Kicked off another branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2252/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-01-10T01:52:38Z","2019-01-14T01:12:51Z"
"","6231","MINOR: add test for StreamsSmokeTestDriver","Also, add more output for debuggability  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-04T23:50:39Z","2019-02-15T16:25:17Z"
"","5907","Trogdor: Add Task State filter to /coordinator/tasks endpoint","Also ensures that the `/tasks/{taskId}` endpoint returns a semantically-correct 404 status code. It would previously return 500 since the `NotFoundException` was not handled anywhere.  cc @cmccabe","closed","","stanislavkozlovski","2018-11-12T15:10:38Z","2018-11-27T00:50:15Z"
"","6156","KAFKA-7828: Execute Trogdor tasks with external commands","Allow the Trogdor agent to execute Trogdor tasks with external commands such as a python Kafka client.  + ExternalCommandSpec describes the external command and its workload. + ExternalCommandWorker uses a external process to execute the command.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangxi","2019-01-16T13:16:53Z","2019-04-01T18:58:24Z"
"","6219","KAFKA-7828: Add ExternalCommandWorker to Trogdor","Allow the Trogdor agent to execute external commands.  The agent communicates with the external commands via stdin, stdout, and stderr.  Based on a patch by Xi Yang","closed","","cmccabe","2019-02-01T19:19:33Z","2019-05-20T18:54:25Z"
"","5978","FailoverAssignor: All partitions will be assigned to one consumer, while others consumer are failover.","All partitions will be assigned to one consumer, while others consumer are failover.     For example, suppose there are two consumers C0 and C1, two topics t0 and t1, and each topic has 3 partitions,   resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2.     The assignment will be:   C0: [t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2]   C1: []     When C0 crashes , the result of the assignment will be   C1: [t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2]    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoyJava","2018-11-30T09:55:18Z","2018-12-07T12:40:11Z"
"","6010","FailoverAssignor: All partitions will be assigned to one consumer, while others consumer are failover.","All partitions will be assigned to one consumer, while others consumer are failover.  For example, suppose there are two consumers C0 and C1, two topics t0 and t1, and each topic has 3 partitions, resulting in partitions t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2.  The assignment will be: C0: [t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2] C1: []  When C0 crashes , the result of the assignment will be C1: [t0p0, t0p1, t0p2, t1p0, t1p1, and t1p2] ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","JoyJava","2018-12-07T12:41:14Z","2019-09-24T07:57:19Z"
"","6371","KAFKA-7747 Check for truncation after leader changes","After the client detects a leader change we need to check the offset of the current leader for truncation.     TODO expand on this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2019-03-05T00:29:50Z","2019-04-21T23:24:19Z"
"","6401","KAFKA-8069: Setting expireTimestamp to None if it is the default value after loading v1 offset records from __consumer_offsets","After the 2.1 release, if the broker hasn't been upgrade to the latest inter-broker protocol version, the committed offsets stored in the __consumer_offset topic will get cleaned up way earlier than it should be when the offsets are loaded back from the __consumer_offset topic in GroupCoordinator, which will happen during leadership transition or after broker bounce. This patch fixes the bug by setting expireTimestamp to None if it is the default value after loading v1 offset records from __consumer_offsets.  Details for the bug can be found in https://issues.apache.org/jira/browse/KAFKA-8069  The bug can be reproduced by starting a broker with inter-broker protocol version = 1.0 and a 2.*/1.*/0.11.* consumer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2019-03-08T06:39:19Z","2019-03-08T17:04:05Z"
"","6019","KAFKA-7708: Fixed KTable tests using KStream API in scala tests","After reading https://issues.apache.org/jira/browse/KAFKA-7708 I can confirm KTable scala tests are using KStream API.  I propose this simple PR. I change all builder.stream to builder.table and modify KTable tests accordingly.  KTable scala tests were introduced in: #5502","closed","","lodamar","2018-12-10T09:12:51Z","2018-12-11T14:14:09Z"
"","6007","Fixed KTable tests that are wrongly using KStream API","After reading https://issues.apache.org/jira/browse/KAFKA-7708 I can confirm **KTable** scala tests are using KStream API.  I propose this simple PR. I change all builder._stream_ to builder._table_ and modify **KTable** tests accordingly.  **KTable** scala tests were introduced in: #5502","closed","","lodamar","2018-12-06T10:04:25Z","2018-12-10T09:11:15Z"
"","5649","MINOR: increase the timeout for the streams broker resilience test","After observing what seems to be a transient system test failure related to recovery just being a little slower than expected, I'm recommending to increase the timeout to the same value we use for trunk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-13T21:19:09Z","2018-09-14T20:48:49Z"
"","5717","KAFKA-7459: Use thread-safe Pool instead of non-thread-safe mutable.HashMap for requestRateInternal","After KAFKA-6514, we add API version as a tag for the RequestsPerSec metric but in the implementation, we use the non-threadsafe mutable.HashMap to store the version -> metric mapping without any protection (https://github.com/apache/kafka/pull/4506/files#diff-d0332a0ff31df50afce3809d90505b25R357 ). This can mess up the data structure and cause unexpected behavior (https://github.com/scala/bug/issues/10436 ). This PR changes requestRateInternal to use the thread-safe Pool instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-09-29T08:36:49Z","2018-10-01T17:07:54Z"
"","5808","KAFKA-7464: catch exceptions in ""leaderEndpoint.close()"" when shutting down ReplicaFetcherThread","After KAFKA-6051, we close leaderEndPoint in replica fetcher thread initiateShutdown to try to preempt in-progress fetch request and accelerate repica fetcher thread shutdown. However, leaderEndpoint can throw an Exception when the replica fetcher thread is still actively fetching, which can cause ReplicaManager to fail to shutdown cleanly. This PR catches the exceptions thrown in ""leaderEndpoint.close()"" instead of letting it throw up in the call stack.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-10-16T20:28:50Z","2018-10-20T22:18:27Z"
"","5642","MINOR: Update minimum required Gradle version","After commit f123d2f18c55b1cf2edb452aeb87e6ad0743c292, the minimum required gradle version changed to 4.7  This is due to the use of `isJava11Compatible()` in build.gradle. It was introduced in version 4.7 (https://github.com/gradle/gradle/blob/master/subprojects/base-services/src/main/java/org/gradle/api/JavaVersion.java#L172-L180)  cc @ijuma @lindong28","closed","","stanislavkozlovski","2018-09-12T09:59:47Z","2018-09-13T14:38:54Z"
"","5991","KAFKA-2334 Guard against non-monotonic offsets in the client","After a recent leader election, the leaders high-water mark might lag behind the offset at the beginning of the new epoch (as well as the previous leader's HW). This can lead to offsets going backwards from a client perspective, which is confusing and leads to strange behavior in some clients.  This change causes Partition#fetchOffsetForTimestamp to throw an exception to indicate the offsets are not yet available from the leader. For new clients, a new OFFSET_NOT_AVAILABLE error is added. For existing clients, a LEADER_NOT_AVAILABLE is thrown.  This is an implementation of [KIP-207](https://cwiki.apache.org/confluence/display/KAFKA/KIP-207%3A+Offsets+returned+by+ListOffsetsResponse+should+be+monotonically+increasing+even+during+a+partition+leader+change)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2018-12-03T18:11:00Z","2018-12-14T21:53:04Z"
"","5606","MINOR: Update README to mention the minimum required gradle version to 4.6","After #5602 we need Gradle 4.6 as minimum required version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-04T08:51:27Z","2018-09-04T13:03:28Z"
"","6034","KAFKA-7730: Limit number of active connections per listener in brokers (KIP-402)","Adds a new listener config `max.connections` to limit the number of active connections on each listener. The config may be prefixed with listener prefix. This limit may be dynamically reconfigured without restarting the broker.  This is one of the PRs for KIP-402 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-402%3A+Improve+fairness+in+SocketServer+processors). Note that this is currently built on top of PR #6022   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-14T16:31:57Z","2019-03-14T23:21:02Z"
"","5730","MINOR KAFKA-7406: Follow up and address final comments","Addressing final comments from KAFKA-7406  I used the existing tests for verification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","bbejeck","2018-10-02T21:42:01Z","2020-06-12T23:58:33Z"
"","6312","KAFKA-7938: Fix test flakiness in DeleteConsumerGroupsTest","Address the comments on PR-6307. Sorry for new PR, but one of the comments was to move the PR to another branch.  **** This attempts to fix KAFKA-7938 and KAFKA-7946.  I removed two tests:  * testDeleteWithShortInitialization basically didn't check the result and therefore always passed * testDeleteCmdWithShortInitialization has no way to enforce that initialization is indeed short, and therefore sometimes FAILED because the group would be created before the CMD tried to delete it.  I thought the tests had limited value relative to the effort of figuring out a way to make the timing work.  I also fixed testDeleteCmdNonEmptyGroup and testDeleteNonEmptyGroup so they will validate that the group both exists and is non-empty before starting the test itself. I also added some extra information for future debugging sessions :)  I ran the tests LOTS of times to validate, but with flaky tests, it is hard to tell :)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gwenshap","2019-02-22T20:49:44Z","2019-02-23T17:39:06Z"
"","6311","KAFKA-7937: Fix Flaky Test ResetConsumerGroupOffsetTest.testResetOffs…","Address the comments on PR-6307. Sorry for new PR, but one of the comments was to move the PR to another branch.  *** Since the test fails sometimes on lack of coordinator, I'm giving it a bit more attempts to find it.  I admit that I haven't been able to actually reproduce this failure, so I'm only hoping this fixes it. But it doesn't fail more often than it used to (on my machine)  Fixing on 2.2 because the intent is to fix enough flakes to allow for a clean release.","closed","","gwenshap","2019-02-22T19:20:13Z","2019-02-24T18:03:47Z"
"","6077","KAFKA-7461: Add tests for logical types","Added testing of logical types for Kafka Connect in support of KIP-145 features. Added tests for Boolean, Time, Date and Timestamp, including the valid conversions.  The area of ISO8601 strings is a bit of a mess because the tokenizer is not compatible with that format, and a subsequent JIRA will be needed to fix that.  A few small fixes as well as creating test cases, but they're clearly just corrections such as using 0 to mean January (java.util.Calendar uses zero-based month numbers).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","AndrewJSchofield","2018-12-30T21:14:06Z","2019-01-14T23:43:04Z"
"","6357","KAFKA-7950: Kafka tools GetOffsetShell -time description","Added additional description for the ""time"" parameter for GetOffsetShell which adds "" No offset is returned if timestamp provided is greater than recently committed record timestamp."" in the description.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kartikvk1996","2019-03-02T13:59:43Z","2019-03-07T16:27:15Z"
"","6171","KIP-416: Notify SourceTask of ACK'd offsets, metadata","Added a hook for producer ACKs. See:  https://cwiki.apache.org/confluence/display/KAFKA/KIP-416%3A+Notify+SourceTask+of+ACK%27d+offsets%2C+metadata","closed","","ryannedolan","2019-01-18T21:43:45Z","2019-09-18T14:31:25Z"
"","5795","KAFKA-7223: Suppression Buffer Metrics","Add the final batch of metrics from KIP-328  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-12T20:23:40Z","2018-11-27T21:07:22Z"
"","6175","KAFKA-3522: Add public interfaces for timestamped stores","Add public interfaces for KIP-258 (key-value and window-store).  - `TimestampedBytesStore` is omitted because it's covered via #6204 - `Stores` is omitted, because we need to add corresponding store builder classed first - new session stores are not added yet  Additionally, adds some internal ""facade"" classes to allow accessing TimestampedStores with non-timestamped interfaces. This is part of the backward compatibility story of the KIP.  - `ReadOnlyXxxFace` classed are part of main module because they will be use for IQ backward compatibility story in a follow up PR - For `TopologyTestDriver` we need to give read/write access and add corresponding read/write facade classes","closed","kip,","mjsax","2019-01-19T18:41:36Z","2020-06-12T23:52:29Z"
"","5742","KAFKA-7223: Add late-record metrics","Add late record metrics, as specified in KIP-328  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-04T20:31:59Z","2018-10-12T16:34:22Z"
"","5561","KAFKA-7187 MockConsumer can fetch offsets based on timestamps","Add implementation for the MockConsumer, previously unexisting, for fetching offsets based on provided timestamps.  The unit test case was improved to include the testing of the fetching offsets based on timestamps.  ### Committer Checklist (excluded from commit message) - [ x ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ x ] Verify documentation (including upgrade notes)","open","","nucatus","2018-08-23T15:56:54Z","2020-11-04T07:15:36Z"
"","5560","KAFKA-7187 MockConsumer can fetch offsets based on timestamps","Add implementation for the MockConsumer, previously unexisting, for fetching offsets based on provided timestamps.  The unit test case was improved to include the testing of the fetching offsets based on timestamps.  ### Committer Checklist (excluded from commit message) - [ x ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ x ] Verify documentation (including upgrade notes)","closed","","nucatus","2018-08-23T15:39:37Z","2018-08-23T15:53:22Z"
"","5787","KAFKA-7223: Suppression documentation","Add documentation for KIP-328: Suppression  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-10-11T22:43:40Z","2018-10-16T04:05:16Z"
"","5775","KAFKA-5061: Add option to make client.id values unique by appending a task id suffix","Add configuration option to append the task id to the client.id used by producer or consumer instances, allowing us to ensure all client.ids used within a worker process are distinct. This is necessary to avoid name conflicts on JMX mbeans and allows proper monitoring.   See https://issues.apache.org/jira/browse/KAFKA-5061  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pdavidson100","2018-10-10T20:27:00Z","2019-02-21T15:36:46Z"
"","5784","KAFKA-7498: Remove references from `common.requests` to `clients`","Add `CreatePartitionsRequest.PartitionDetails` similar to `CreateTopicsRequest.TopicDetails` to avoid references from `common.requests` to `clients`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-11T19:40:27Z","2018-10-15T12:21:16Z"
"","5603","KAFKA-1194: Fixes for Windows","Adapt changes from #4431 to existing codebase  Original contributors @nxmbriggs404 and @GeorgeCGV  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","simplesteph","2018-09-03T06:50:30Z","2019-01-23T07:14:27Z"
"","5565","MINOR: replace deprecated remove with delete","According to rocksDB commit history, it should be a pure renaming and no perf / correctness implications at all. But still good to remove deprecated API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-08-23T23:46:16Z","2018-08-24T15:53:58Z"
"","6287","MINOR: Add 2 minute timeout to KTableKTableLeftJoinTest","According to KAFKA-7933, this test sometimes takes over an hour. We add a 2 minute timeout to fail faster until we find the problem. Another benefit is that we now collect logs when a test fails (since ed3071231).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ijuma","2019-02-19T02:00:41Z","2019-02-21T09:16:31Z"
"","5752","HOTFIX: Compilation error in GroupMetadataManagerTest","Accidentally broke after merging KAFKA-7395.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-05T20:33:15Z","2018-10-05T20:38:33Z"
"","6181","KAFKA-7840: Documentation for cleanup.policy is out of date","A trivial documentation fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-01-21T12:56:56Z","2019-01-24T13:13:31Z"
"","5655","MINOR: log and fail on missing task in Streams","A system test failed unexpectedly when Streams attempted to process a record for which it had no task. It's not clear how this happened, and the logs don't contain enough information, so we'll just log more information and still fail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-09-14T21:20:01Z","2018-09-17T16:23:46Z"
"","6469","Fix KafkaProducer#flush() behavior","A produce batch is split into multiple smaller batches when the client recieves Erros.MESSAGE_TOO_LARGE code from the broker. The future's returned as part of KafkaProducer#send() API are chained to make sure that they aren't evaluated until the newly created small batches are sent to the server. This chaining behavior is lacking in KafkaProducer#flush() which breaks the API contract that all the previous futures will be in done state when the flush is complete.","closed","","viswamy","2019-03-19T00:10:46Z","2019-04-01T17:38:46Z"
"","5851","MINOR: Introduce KafkaChannel.newRequestContext","A minor clean-up that I noticed while looking at the reauthentication PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-28T17:51:58Z","2019-09-13T05:36:40Z"
"","5974","KAFKA-7660: Fix child sensor memory leak","A heap dump provided by Patrik Kleindl in https://issues.apache.org/jira/browse/KAFKA-7660 identifies the childrenSensors map in Metrics as keeping references to sensors alive after they have been removed.  This PR fixes it and adds a test to be sure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-11-29T22:07:52Z","2018-11-30T20:36:01Z"
"","6429","MINOR: Avoid double null check in KStream::transform()","A call to KStream::transform() checked twice the parameter transformerSupplier for null. The double null check occurred because KStream::transform() is expressed in terms of KStream::flatTransform() and both methods performed the null check.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-03-11T20:04:50Z","2019-10-21T10:53:15Z"
"","6455","MINOR: Retain public constructors of classes from public API","`TopicDescription` and `ConsumerGroupDescription`  in `org.apache.kafka.clients.admin.` are part of the public API, so we should retain the existing public constructor. Changed the new constructor with authorized operations to be package-private to avoid maintaining more public constructors since we only expect admin client to use this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-16T21:01:26Z","2019-03-18T08:51:51Z"
"","6317","MINOR: fix release.py script","`release.py` does not compute docs-version string correctly.  We should use `https` instead of `http` in links.  This should to into `2.2` branch, too","closed","","mjsax","2019-02-24T00:08:48Z","2019-03-01T00:20:16Z"
"","6164","KAFKA-5488: A method-chaining way to build branches for topology","`KStream.branch` method uses varargs to supply predicates and returns array of streams ('[Each stream in the result array corresponds position-wise (index) to the predicate in the supplied predicates](http://home.apache.org/~ijuma/kafka-0.10.0.1-rc1/javadoc/org/apache/kafka/streams/kstream/KStream.html)').   This is poor API design that makes building branches very inconvenient because of 'impedance mismatch' between arrays and generics in Java language.   * In general, the code  have low cohesion: we need to define predicates in one place, and respective stream processors in another place of code.     * If the number of predicates is predefined, this method forces us to use 'magic numbers' to extract the right branch from the result (see examples [here](https://stackoverflow.com/questions/48950580/kafka-streams-send-on-different-topics-depending-on-streams-data)).    * If we need to build branches dynamically (e. g. one branch per enum value) we inevitably have to deal with 'generic arrays' and 'unchecked typecasts'.   The proposed class `KafkaStreamBrancher` introduces new standard way to build branches on top of KStream.   Instead of   ```java KStream source_o365_user_activity = builder.stream(""source""); KStream[] branches = source_o365_user_activity.branch(        (key, value) -> value.contains(""A""),       (key, value) -> value.contains(""B""),       (key, value) -> true      );  branches[0].to(""A""); branches[1].to(""B""); branches[2].to(""C""); ```  we can use  ```java new KafkaStreamsBrancher()    .branch((key, value) -> value.contains(""A""), ks->ks.to(""A""))    .branch((key, value) -> value.contains(""B""), ks->ks.to(""B""))    //default branch should not necessarily be defined in the end!    .defaultBranch(ks->ks.to(""C""))    .onTopOf(builder.stream(""source"")) ```    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","inponomarev","2019-01-17T15:07:50Z","2020-06-12T23:52:42Z"
"","5618","MINOR: Insure that KafkaStreams client is closed if test fails","`KafkaStreams` instances must be closed even if we did not call `#start()`.  Some additional Java8 cleanup.  Call for review any of @guozhangwang @bbejeck @vvcephei","closed","streams,","mjsax","2018-09-06T22:17:15Z","2018-09-13T20:04:13Z"
"","6367","MINOR: list-topics should not require topic param","`kafka.list_topics(...)` should not require a topic parameter  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-03-04T18:20:55Z","2019-03-22T18:52:03Z"
"","5627","KAFKA-7044: Fix Fetcher.fetchOffsetsByTimes and NPE in describe consumer group","`kafka-consumer-groups --describe --group ...` can result in NullPointerException for two reasons: 1)  Fetcher.fetchOffsetsByTimes() may return too early, without sending list offsets request for topic partitions that are not in cached metadata. 2) `ConsumerGroupCommand.getLogEndOffsets()` and `getLogStartOffsets()` assumed that endOffsets()/beginningOffsets() which eventually call Fetcher.fetchOffsetsByTimes(), would return a map with all the topic partitions passed to endOffsets()/beginningOffsets() and that values are not null. Because of (1), null values were possible if some of the topic partitions were already known (in metadata cache) and some not (metadata cache did not have entries for some of the topic partitions). However, even with fixing (1), endOffsets()/beginningOffsets() may return a map with some topic partitions missing, when list offset request returns a non-retriable error. This happens in corner cases such as message format on broker is before 0.10, or maybe in cases of some other errors.   Testing: -- added unit test to verify fix in Fetcher.fetchOffsetsByTimes()  -- did some manual testing with `kafka-consumer-groups --describe`, causing NPE. Was not able to reproduce any NPE cases with DescribeConsumerGroupTest.scala,  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-09-09T08:02:04Z","2018-09-11T17:10:43Z"
"","5502","KAFKA-7301: Fix streams Scala join ambiguous overload","`join` in the Scala streams API is currently unusable in `2.0.0` as reported by @mowczare: https://github.com/apache/kafka/pull/5019#issuecomment-412632207  This due to an overload of it with the same signature in the first curried parameter. See compiler issue that didn't catch it: https://issues.scala-lang.org/browse/SI-2628  I don't see many options here: - Keeping only one `join` with the `Materialized` as implicit like we did with `aggregate`... (this PR currently implements) - Renaming the materialized `join` to something like `joinMat`. - Uncurrying one of the 2 or both `join` and loosing the type inference.  I will add all the needed tests once we agree on an option.  The current workarounds are: - Monkey patching KTable.scala in your code with the implementation in this PR. - Rolling back to https://github.com/lightbend/kafka-streams-scala  @debasishg @ijuma @guozhangwang @mowczare  Thanks","closed","streams,","joan38","2018-08-14T00:09:38Z","2018-08-23T16:27:44Z"
"","5725","KAFKA-7075: Allow Topology#addGlobalStore to add a window store","`InternalTopologyBuilder#addGlobalStore` has no tests for stores other than `KeyValueStore`. This PR adds a new unit test to show that a StoreBuilder can be used in InternalTopologyBuilder#addGlobalStore. This required creating two new mocks, `MockWindowStore` and `MockWindowStoreBuilder`, modelled on `MockKeyValueStore` and `MockKeyValueStoreBuilder`, respectively.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","lmontrieux","2018-10-02T09:24:21Z","2020-02-07T18:01:55Z"
"","6425","KAFKA-8091; Wait for processor shutdown before testing removed listeners","`DynamicBrokerReconfigurationTest.testAddRemoveSaslListeners` removes a listener, waits for the config to be propagated to all brokers and then validates that connections to the removed listener fail. But there is a small timing window between config update and Processor shutdown. Before validating that connections to a removed listener fail, this PR waits for all metrics of the removed listener to be deleted, ensuring that the Processors of the listener have been shutdown.  Ran the test with the fix 1000 times over the weekend without any failures. It failed after ~200 runs without the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-11T11:39:14Z","2019-03-11T18:43:47Z"
"","5829","MINOR: Make ControllerIntegrationTest less flaky","`ControllerIntegrationTest#waitUntilControllerEpoch` sometimes fails with the following error: ``` java.util.NoSuchElementException: None.get 	at scala.None$.get(Option.scala:347) 	at scala.None$.get(Option.scala:345) 	at kafka.controller.ControllerIntegrationTest$$anonfun$waitUntilControllerEpoch$1.apply$mcZ$sp(ControllerIntegrationTest.scala:312) 	at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:779) 	at kafka.controller.ControllerIntegrationTest.waitUntilControllerEpoch(ControllerIntegrationTest.scala:312) 	at kafka.controller.ControllerIntegrationTest.testEmptyCluster(ControllerIntegrationTest.scala:51) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 	at  ```  I think that ensuring the controllerEpoch is defined (by implicitly retrying) is a way to remove this error and fix cases where something transient has caused the method to return `None`.","closed","","stanislavkozlovski","2018-10-23T13:13:00Z","2018-10-24T20:24:10Z"
"","5555","MINOR separate throughput sensors for commit, process, and punctuate ops","`commitTimeSensor`, `processTimeSensor`, and `punctuateTimeSensor` are being used like `timeSensor.record(latencyTime / numberOfOps)`, which measures the average latency for a batch of operations, but rate and total metrics were not being computed correctly. This PR adds a throughput sensor for each of commit, process, and punctuate operations which is used like `thoughputSensor.record(numberOfOps)`.  Unfortunately I don't have time right now to add tests, but because I already had this change started I wanted to submit a PR so it wouldn't get forgotten about.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lendle","2018-08-22T16:34:34Z","2019-02-19T19:40:39Z"
"","5571","KIP-365: Make KStream.process processorSupplier parameter call by name","``` def process(processorSupplier: () => Processor[K, V], stateStoreNames: String*) ``` should be: ``` def process(processorSupplier: => Processor[K, V], stateStoreNames: String*) ```  KIP: https://cwiki.apache.org/confluence/pages/resumedraft.action?draftId=89069981&draftShareId=a21bcef4-c388-4663-9c6f-9ccceee4cb30","closed","","joan38","2018-08-25T17:16:58Z","2018-08-27T23:16:31Z"
"","6211","MINOR: Increase timeout for flaky test","_**This is a WIP. Please do not merge.**_  This test was occasionally failing with timeout issues.  Even though on a laptop the test takes 5-6 seconds to setup connectors, on slow VMs, it might need more than the 15 seconds we were allowing it.  Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2019-01-31T03:59:02Z","2019-02-18T06:36:13Z"
"","6318","KAFKA-7992: Introduce start-time-ms metric","[KIP-436](https://cwiki.apache.org/confluence/display/KAFKA/KIP-436%3A+Add+a+metric+indicating+start+time) [KAFKA-7992](https://issues.apache.org/jira/browse/KAFKA-7992)  This PR exposes a new `start-time-ms` metric aross all components  Also removes the unused scala `AppInfo` class","closed","","stanislavkozlovski","2019-02-24T07:49:17Z","2019-05-01T15:58:03Z"
"","6224","KAFKA-7236: Add --under-min-isr option to describe topics command (KIP-351)","[KIP-351](https://cwiki.apache.org/confluence/display/KAFKA/KIP-351%3A+Add+--under-min-isr+option+to+describe+topics+command)  - Add `--under-min-isr` option to `TopicCommand` to report partitions under the configured `min.insync.replicas` value  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2019-02-02T01:18:12Z","2019-02-12T00:57:18Z"
"","6129","KAFKA-5117: Stop resolving externalized configs in Connect REST API","[KIP-297](https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations#KIP-297:ExternalizingSecretsforConnectConfigurations-PublicInterfaces) introduced the `ConfigProvider` mechanism, which was primarily intended for externalizing secrets provided in connector configurations. However, when querying the Connect REST API for the configuration of a connector or its tasks, those secrets are still exposed. The changes here prevent the Connect REST API from ever exposing resolved configurations in order to address that. @rhauch has given a more thorough writeup of the thinking behind this in [KAFKA-5117](https://issues.apache.org/jira/browse/KAFKA-5117)  Tested and verified manually. If these changes are approved unit tests can be added to prevent a regression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-01-11T18:53:02Z","2020-10-16T06:21:22Z"
"","6049","KAFKA-7755 Turn update inet addresses","[KAFKA-7755](https://issues.apache.org/jira/browse/KAFKA-7755)  Turn update resolved addresses in case that node address maybe changed in DNS latter.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hackerwin7","2018-12-19T10:21:16Z","2019-01-08T02:30:15Z"
"","5681","KAFKA-7431 : Clean up connect unit tests","[KAFKA-7431](https://issues.apache.org/jira/browse/KAFKA-7431)  Changes made to improve the code readability:  - Removed `throws Exception` from the place where there won't be an  exception  - Removed type arguments where those can be inferred explicitly by compiler  - Rewritten Anonymous classes to Java 8 with lambdas  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mrsrinivas","2018-09-22T07:06:20Z","2018-11-08T05:47:23Z"
"","5932","KAFKA-7638: Add support for multiple task creation in one request","[JIRA](https://issues.apache.org/jira/browse/KAFKA-7638) Trogdor Coordinator now supports creation of multiple tasks in one request call. This is exposed under `/coordinator/tasks/mass_create` Sample body:  ```json {""tasks"": [{...}, {...}]} ```  Additional changes: The /tasks/create endpoint (and now mass_create) return 400 with an error message whenever an error is encountered during the task creation. Previously, Trogdor would create the task as ""DONE"" and return a successful status code, making users manually inspect the tasks' status. This, unfortunately, has the nasty downside of returning 400 when an unexpected internal server error occurs.  I think that we can live with this for now and open up another KIP that could address error handling in the Trogdor Coordinator REST API.","open","","stanislavkozlovski","2018-11-19T22:30:05Z","2018-12-07T16:07:18Z"
"","6470","KAFKA-8127 It may need to import scala.io","[https://issues.apache.org/jira/browse/KAFKA-8127](url)  It may get error when compile kafka if no scala.io imported","closed","","hejiefang","2019-03-19T08:32:58Z","2019-03-19T12:02:42Z"
"","6108","MINOR: Streams upgrade guide section for newly merged KIPs","@shawnsnguyen for KIP-393. @shunge for KIP-376 And also the state transition change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-01-09T03:10:29Z","2019-01-15T18:25:34Z"
"","5985","KAFKA-7673: Upgrade rocksdb to 5.15.10","@bbejeck   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-12-01T01:36:33Z","2020-04-25T00:06:02Z"
"","5669","KAFKA-3514: Modify pause logic if we being enforced processing","1. When we are being enforced processing (idleStartTime is not UNKNOWN): 1.a we will only resume a partition if it is drained empty, instead of below the max.partition buffer 1.b we will immediately pause a partition if it has at least one record after being appended to buffer.  2. Related upgrade docs, and unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-09-21T01:31:02Z","2018-09-29T00:19:42Z"
"","6001","KAFKA-7706: Spotbugs task fails with Gradle 5.0","1. When I'm building Kafka with Gradle 5.0, the failure of Spotbugs task occurred.     I'm running ""gradle build --stacktrace"".     An interesting part of the stacktrace is:  ``` Caused by: java.lang.NoClassDefFoundError: org/gradle/api/internal/ClosureBackedAction          at com.github.spotbugs.SpotBugsTask.reports(SpotBugsTask.java:136)          at com.github.spotbugs.SpotBugsTask.reports(SpotBugsTask.java:55)          at org.gradle.api.reporting.Reporting$reports.call(Unknown Source)          at build_9sk7crqolfjf8m0yenkwy63v1$_run_closure1.doCall(/Users/mchalupa/projects/others/spotbugsFailExample/build.gradle:18)          at org.gradle.util.ClosureBackedAction.execute(ClosureBackedAction.java:70)          at org.gradle.util.ConfigureUtil.configureTarget(ConfigureUtil.java:154)          at org.gradle.util.ConfigureUtil.configureSelf(ConfigureUtil.java:130)          at org.gradle.api.internal.AbstractTask.configure(AbstractTask.java:600)          at org.gradle.api.internal.AbstractTask.configure(AbstractTask.java:92)          at org.gradle.util.ConfigureUtil.configure(ConfigureUtil.java:103) at org.gradle.util.ConfigureUtil$WrappedConfigureAction.execute(ConfigureUtil.java:166)          at org.gradle.api.internal.DefaultDomainObjectCollection.all(DefaultDomainObjectCollection.java:161)          at org.gradle.api.internal.DefaultDomainObjectCollection.all(DefaultDomainObjectCollection.java:190)          at org.gradle.api.internal.tasks.DefaultRealizableTaskCollection.all(DefaultRealizableTaskCollection.java:229)          at org.gradle.api.internal.DefaultDomainObjectCollection.withType(DefaultDomainObjectCollection.java:201)          at org.gradle.api.DomainObjectCollection$withType.call(Unknown Source)          at build_9sk7crqolfjf8m0yenkwy63v1.run(/Users/mchalupa/projects/others/spotbugsFailExample/build.gradle:17)          at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:90) ... 102 more  ```  2. Similar to the previous one--- ---When I'm building Kafka with Gradle 5.0, apply plugin[org.scoverage] fails     I'm running ""gradle build --stacktrace"".     An interesting part of the stacktrace is:  ``` Caused by: org.gradle.api.internal.plugins.PluginApplicationException: Failed to apply plugin [id 'org.scoverage']         at org.gradle.api.internal.plugins.DefaultPluginManager.doApply(DefaultPluginManager.java:160)         at org.gradle.api.internal.plugins.DefaultPluginManager.apply(DefaultPluginManager.java:130)         ... ... Caused by: org.gradle.api.reflect.ObjectInstantiationException: Could not create an instance of type org.scoverage.ScoverageExtension_Decorated.         at org.gradle.internal.reflect.DirectInstantiator.newInstance(DirectInstantiator.java:53)         at org.gradle.api.internal.ClassGeneratorBackedInstantiator.newInstance(ClassGeneratorBackedInstantiator.java:36)         at org.gradle.api.internal.plugins.DefaultConvention.instantiate(DefaultConvention.java:242)         at org.gradle.api.internal.plugins.DefaultConvention.create(DefaultConvention.java:142)         at org.scoverage.ScoveragePlugin.apply(ScoveragePlugin.groovy:18)         at org.scoverage.ScoveragePlugin.apply(ScoveragePlugin.groovy)         at org.gradle.api.internal.plugins.ImperativeOnlyPluginTarget.applyImperative(ImperativeOnlyPluginTarget.java:42)         at org.gradle.api.internal.plugins.RuleBasedPluginTarget.applyImperative(RuleBasedPluginTarget.java:50)         at org.gradle.api.internal.plugins.DefaultPluginManager.addPlugin(DefaultPluginManager.java:174)         at org.gradle.api.internal.plugins.DefaultPluginManager.access$300(DefaultPluginManager.java:50)         ... 167 more Caused by: org.gradle.api.InvalidUserDataException: You can't map a property that does not exist: propertyName=testClassesDir         at org.gradle.api.internal.ConventionAwareHelper.map(ConventionAwareHelper.java:56)         at org.gradle.api.internal.ConventionAwareHelper.map(ConventionAwareHelper.java:80)         at org.gradle.api.internal.ConventionMapping$map.call(Unknown Source)         at org.scoverage.ScoverageExtension$_closure6.doCall(ScoverageExtension.groovy:89)         at org.gradle.util.ClosureBackedAction.execute(ClosureBackedAction.java:70)         at org.gradle.util.ConfigureUtil.configureTarget(ConfigureUtil.java:154)         at org.gradle.util.ConfigureUtil.configureSelf(ConfigureUtil.java:130)         ... 186 more  ```","closed","","FuqiaoWang","2018-12-05T09:14:41Z","2018-12-05T09:21:35Z"
"","6079","MINOR:: Fix typos","1. Use singular form instead of plural form 2. Add a missing period","closed","","yaojingguo","2018-12-31T06:04:17Z","2019-01-19T02:55:16Z"
"","6085","KAFKA-6928: Refactor StreamsPartitionAssignor retry logic","1. The retry loop of the InternalTopicManager would just be: a) describe topics, and exclude those which already exist with the right num.partitions, b) for the remaining topics, try to create them. Remove any inner loops.  2. In CreateTopicResponse and MetadataResponse (for describe topic), handle the special error code of TopicExist and UnknownTopicOrPartition in order to retry in the next loop.  3. Do not handle TimeoutException since it should already been handled inside AdminClient.  Add corresponding unit tests for a) topic marked for deletion but not complete yet, in which case metadata response would not contain this topic, but create topic would return error TopicExists; b) request keep getting timed out.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-01-03T18:32:14Z","2020-04-25T00:06:48Z"
"","6117","MINOR: Remove unused imports, exceptions, and values","1. Remove unthrown exceptions from `MemoryRecordsBuilderTest` 2. Remove unused imports from `ReplicaFetcherThread`, `ZooKeeperClient`, `ApiVersionTest`, `PartitionTest` 3. Remove unused value from `PartitionTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-01-10T08:57:47Z","2019-01-20T21:27:41Z"
"","6134","KAFKA-7652: Part I; Fix SessionStore's findSession(single-key)","1. Let `findSessions(final K key)` to call on underlying bytes store directly, using the more restricted range.  2. Fix the conservative upper range for multi-key range in session schema.  3. Minor: removed unnecessary private WrappedSessionStoreBytesIterator  class as it is only used in unit test.  4. Minor: removed unnecessary schema#init function by using the direct bytes-to-binary function.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-01-12T00:37:38Z","2019-01-19T01:10:07Z"
"","6113","KAFKA-7672: Restoring tasks need to be closed upon task suspension","1. In activeTasks.suspend, we should also close all restoring tasks as well. Closing restoring tasks would not require `task.close` as in `closeNonRunningTasks `, since the topology is not initialized yet, instead only state stores are initialized. So we only need to call `task.closeStateManager`.  2. Also add @linyli001 's fix.  3. Unit tests updated accordingly.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-01-10T01:23:12Z","2020-04-24T23:59:29Z"
"","5754","MINOR: fixes on streams upgrade test","1. In `test_upgrade_downgrade_brokers`, allow duplicates to happen. 2. In `test_version_probing_upgrade`, grep the generation numbers from brokers at the end, and check if they can ever be synchronized.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-10-06T01:21:03Z","2018-10-14T05:39:28Z"
"","5488","KAFKA-6998: Disable Caching when max.cache.bytes are zero.","1. As titled, add a `rewriteTopology` that 1) sets application id, 2) maybe disable caching, 3) adjust for source KTable. This optimization can hence be applied for both DSL or PAPI generated Topology.  2. Defer the building of globalStateStores in `rewriteTopology` so that we can also disable caching. But we still need to build the state stores before `InternalTopologyBuilder.build()` since we should only build global stores once for all threads.  3. Added withCachingDisabled to StoreBuilder, it is a public API change.  4. [Optional] Fixed unit test config setting functionalities, and set the necessary config to shorten the unit test latency (now it reduces from 5min to 3.5min on my laptop).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-08-10T22:23:58Z","2018-08-17T16:35:42Z"
"","6107","KAFKA-7798: Expose embedded clientIds","1. Add consumer / restoreConsumer / producer(s) / admin client ids via ThreadMetadata; for producerIds, if EOS is turned on add the list of task-producer-ids, otherwise it is a singleton of thread-producer-id.  2. Consolidate the logic of generating clientIds from thread names and client id into StreamThread.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-01-09T02:29:07Z","2020-04-25T00:05:17Z"
"","5521","KAFKA-7456: Serde Inheritance in DSL","1. Add `keySerde` and `valSerde` in the `AbstractStream`, which are null-able. 2. For all classes extending `AbstractStream`, including `KStream/KTable/KGroupedStream/KGroupedTable/TimeWindowedKStream/SessionWindowedKStream`-impl classes, try to pass the serdes specified from `Materialized / Serialized / Joined` to children nodes. Note here I've only done the forward-inheritance, but not backward-inheritance, which is left for future works (thinking about this, only `Produced / Serialized` serdes are worth passing backwards to parent nodes, so the scope of it may not be too large). 3. Removed a bunch of unchecked annotations to strengthen the typing system. 4. Found a couple of lurking bugs, fixed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-08-16T22:51:37Z","2018-10-01T23:25:42Z"
"","5911","MINOR: Remove deprecated callers","1) Windows#until 2) Windows#of 3) Serialized  @mjsax @ijuma @vvcephei @bbejeck   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-11-14T03:33:07Z","2020-04-24T23:39:00Z"
"","6191","KAFKA-7652: Part III; Put to underlying before Flush","1) In the caching layer's flush listener call, we should always write to the underlying store, before flushing (see https://github.com/apache/kafka/pull/4331 's point 4) for detailed explanation). When fixing 4331, it only touches on KV stores, but it turns out that we should fix for window and session store as well.  2) Also apply the optimization that was in session-store already: when the new value bytes and old value bytes are all null (this is possible e.g. if there is a put(K, V) followed by a remove(K) or put(K, null) and these two operations only hit the cache), upon flushing this mean the underlying store does not have this value at all and also no intermediate value has been sent to downstream as well. We can skip both putting a null to the underlying store as well as calling the flush listener sending `null -> null` in this case.  Modifies corresponding unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-01-24T06:48:25Z","2019-02-13T06:37:43Z"
"","6283","KAFKA-7941: Catch TimeoutException in KafkaBasedLog worker thread","- When calling readLogToEnd(), the KafkaBasedLog worker thread should  catch TimeoutException and log a warning, which can occur if brokers  are unavailable, otherwise the worker thread terminates.  - Includes an enhancement to MockConsumer that allows simulating  exceptions not just when polling but also when querying for offsets,  which is necessary for testing the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","pgwhalen","2019-02-18T01:17:25Z","2020-10-16T06:21:23Z"
"","5900","KAFKA-7612: Fix javac warnings and enable warnings as errors","- Use Xlint:all with 3 exclusions (filed KAFKA-7613 to remove the exclusions) - Use the same javac options when compiling tests (seems accidental that we didn't do this before) - Replaced several deprecated method calls with non-deprecated ones:   - `KafkaConsumer.poll(long)` and `KafkaConsumer.close(long)`   - `Class.newInstance` and `new Integer/Long` (deprecated since Java 9)   - `scala.Console` (deprecated in Scala 2.11)   - `PartitionData` taking a timestamp (one of them seemingly a bug)   - `JsonMappingException` single parameter constructor - Fix unnecessary usage of raw types in several places. - Add @SuppressWarnings for deprecations, unchecked and switch fallthrough in several places. - Scala clean-ups (var -> val, ETA expansion warnings, avoid reflective calls) - Use lambdas to simplify code in a few places - Add @SafeVarargs, fix varargs usage and remove unnecessary `Utils.mkList` method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-11-11T10:40:29Z","2018-11-14T18:05:45Z"
"","5629","MINOR: Extract CoordinatorUtils.readRecords","- Use it from both GroupMetadataManager and TransactionStateManager - Remove redundant buffer.clear() - Fix warning in TransactionStateManager to mention ""transaction metadata""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ijuma","2018-09-09T16:12:35Z","2018-09-14T05:34:49Z"
"","6361","KAFKA-8032: Wait for quota override removal in Quota tests","- Update UserQuotaTest/UserClientIdQuotaTest/ClientIdQuotaTest  tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","omkreddy","2019-03-03T15:42:23Z","2019-09-02T12:04:10Z"
"","6263","MINOR: Add missing Alter Operation to Topic supported operations list in AclCommand","- Update the AclCommandTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-02-13T11:41:29Z","2019-02-14T04:17:25Z"
"","6422","MINOR: Correct /admin/delete_topic to /admin/delete_topics","- Update delete topics zk path  from /admin/delete_topic to /admin/delete_topics in assertion error messages  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-03-11T08:41:24Z","2019-03-15T03:18:47Z"
"","6072","KAFKA-7752","- This commits sets ACL on /kafka-acl-extended  - Extended ZkAuthorizationTest to check ACL on /kafka-acl-extended - Using zookeeper-security-migration.sh tool on a Kerberized test cluster, I verified the changes: secured and unsecured Kafka znodes and examined ACL on /kafka-acl-extended with zookeeper client  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-12-28T15:24:37Z","2019-01-04T10:47:55Z"
"","6474","KAFKA-8098: Fix Flaky Test testConsumerGroups","- The flaky failure is caused by the fact that the main thread sometimes issues DescribeConsumerGroup request before the consumer assignment takes effect. Added a latch to make sure such situation is not going to happen.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-03-20T09:53:22Z","2019-03-20T14:01:32Z"
"","6441","KAFKA-8098: Fix Flaky Test testConsumerGroups","- The flaky failure is caused by the fact that the main thread sometimes issues DescribeConsumerGroup request before the consumer assignment takes effect. Added a latch to make sure such situation is not going to happen.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-03-14T03:16:41Z","2019-03-20T09:54:01Z"
"","5771","MINOR: Fix remaining core, connect and clients tests to pass with Java 11","- SslFactoryTest should use SslFactory to create SSLEngine - Use Mockito instead of EasyMock in `ConsoleConsumerTest` as one of the tests mocks a standard library class and the latest released EasyMock version can't do that when Java 11 is used. - Avoid mocking `ConcurrentMap` in `SourceTaskOffsetCommitterTest` for similar reasons. As it happens, mocking is not actually needed here.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-10T06:37:47Z","2018-10-10T20:31:12Z"
"","6116","MINOR: Update dependencies for Kafka 2.2","- Scala 2.12.7 -> 2.12.8 - Gradle 5.0 -> 5.1 - Jetty 9.4.12 -> 9.4.14 - Rat 0.12 -> 0.13 - Patch bumps for easymock, jackson, powermock - Patch bumps for gradle plugins: shadow, spotbugs, dependency-check, spotless  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-01-10T05:49:15Z","2019-01-10T09:14:31Z"
"","6363","KAFKA-5505: Incremental cooperative rebalancing in Connect (KIP-415)","- Resolve the stop-the-world effect in Connect by allowing connectors and tasks to keep running if possible.  - Maintain connector and task assignment in the presence of quick restarts or rolling upgrades of Connect workers - Upgrade Connect protocol to allow workers to report their active assignments and the leader to revoke assignments.   - Select the Connect protocol during runtime in a fully backwards compatible way.  - Introduce ConnectAssignor and make task assignment scheduling pluggable.  - Implement incremental cooperative rebalancing of connectors and tasks - https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect  Tested via:  * Unit tests for protocol compatibility * Unit tests on task assignment scheduling * Unit tests on incremental cooperative rebalancing * Integration tests on incremental cooperative rebalancing  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-03-04T05:50:40Z","2020-10-16T05:50:54Z"
"","6257","MINOR: Remove deprecated assertThat method usage from KafkaLog4jAppenderTest","- Replace `Assert.assertThat` with `MatcherAssert.assertThat`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-02-12T12:18:53Z","2019-02-12T14:40:02Z"
"","5850","MINOR: Various code cleanups in core and clients","- Removed unused imports - Use string interpolations instead of concatenations  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-10-28T16:08:46Z","2018-10-29T19:17:47Z"
"","5480","KAFKA-7259: Remove deprecated ZKUtils usage from ZkSecurityMigrator","- Remove ZKUtils usage from various tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-08T17:40:54Z","2018-11-30T05:43:31Z"
"","6102","MINOR: Improve ducker-ak script","- Remove -it from docker exec commands. When running integration tests on some CI systems the script fails and complains that there is no TTY. TTY is not needed to run ducktape so it is OK to remove them. - Add -f/--force option to ""ducker-ak down"" to forcefully cleanup existing docker instances.  The contribution is my original work and I license the work to the project under the project's open source license.  Signed-off-by: Kan Li likan@uber.com  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* Runs the script in jenkins, without the commit the script fails, while with the commit the script runs successfully.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","likan999","2019-01-08T02:32:12Z","2019-01-23T21:39:48Z"
"","5546","MINOR: Return correct instance of SessionWindowSerde","- minor javadoc cleanups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-08-21T12:21:27Z","2018-08-24T04:33:44Z"
"","5804","KAFKA-7080 and KAFKA-7222: Cleanup overlapping KIP changes","- KIP-319 and KIP-328 overlap and we can remove non-released deprecates methods - add upgrade docs for KIP-319","closed","kip,","mjsax","2018-10-16T00:55:53Z","2020-06-12T23:57:17Z"
"","6389","KAFKA-8044: Increase stop timeout for VerifiableProducer in ReassignPartitionsTest","- increase stop timeout from 150 to 200 seconds to avoid transient failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-07T09:46:52Z","2019-03-16T08:26:22Z"
"","6250","MINOR: Improve log messages when authentications fail","- Include more detail in the client log message if the disconnection happens during authentication. - Include exception message in the Selector info entry when authentication fails and unwrap `DelayedResponseAuthenticationException`. - Remove duplicate debug log on authentication failure in the Selector.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-10T17:44:50Z","2019-02-13T00:03:49Z"
"","6247","KAFKA-7466: Add IncrementalAlterConfigs API (KIP-339)","- https://cwiki.apache.org/confluence/display/KAFKA/KIP-339%3A+Create+a+new+IncrementalAlterConfigs+API  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-02-09T00:52:31Z","2019-04-17T07:01:31Z"
"","5756","KAFKA-7487: DumpLogSegments misreports offset mismatches","- Compare last offset of first batch (instead of first offset) with index offset - Early exit from loop due to zero entries must happen before checking for mismatch - {TimeIndex,OffsetIndex}.entry should return absolute offset like other methods. These methods are only used by DumpLogSegments. - DumpLogSegments now calls `closeHandlers` on OffsetIndex, TimeIndex and FileRecords. - Add OffsetIndex, TimeIndex and DumpLogSegments tests - Remove unnecessary casts by using covariant returns in OffsetIndex and TimeIndex - Minor clean-ups - Fix `checkArgs` so that it does what it says only.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-07T16:23:06Z","2019-02-18T19:42:44Z"
"","5501","KAFKA-7285: Create new producer on each rebalance if EOS enabled","- close producer on suspend() if EOS enabled - create new producer on resume() if EOS enabled  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-08-13T23:29:25Z","2018-08-16T03:04:01Z"
"","6424","KAFKA-7875: Add KStream.flatTransformValues","- Adds flatTrasformValues methods in KStream - Adds processor supplier and processor for flatTransformValues - Improves API documentation of transformValues   This contribution is my original work and I license the work to the project under the project's open source license.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-03-11T11:21:53Z","2020-06-12T23:45:59Z"
"","6380","KAFKA-8044: Add timeout to VerifiableProducer close to avoid indefinite blocking","- Add support to add delivery.timeout.ms  config in verifiable_producer.py  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-06T15:01:49Z","2019-03-06T20:30:41Z"
"","6258","MINOR: Fix bugs identified by compiler warnings","- Add missing string interpolation - Fix and simplify testElectPreferredLeaders - Remove unused code - Replace deprecated usage of JUnit `assertThat` - Change var to val and fix non-exhaustive pattern match - Fix eta warning - Simplify code - Remove commented out code  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-02-12T15:14:14Z","2019-02-15T01:13:32Z"
"","6421","KAFKA-7904: Add AtMinIsr partition metric and TopicCommand option (KIP-427)","- Add `AtMinIsrPartitionCount` metric to `ReplicaManager` - Add `AtMinIsr` metric to `Partition` - Add `--at-min-isr-partitions` describe `TopicCommand` option  https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=103089398  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2019-03-11T05:24:41Z","2019-04-05T16:16:51Z"
"","6352","KAFKA-7922: Return authorized operations in Metadata request response (KIP-430 Part-2)","-  Use automatic RPC generation in Metadata Request/Response classes -  https://cwiki.apache.org/confluence/display/KAFKA/KIP-430+-+Return+Authorized+Operations+in+Describe+Responses   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-01T11:38:10Z","2019-03-10T12:02:30Z"
"","6322","KAFKA-7922: Return authorized operations in describe consumer group responses (KIP-430 Part-1)","-  Use automatic RPC generation in DescribeGroups Request/Response classes  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-02-25T15:31:56Z","2019-03-01T15:56:01Z"
"","5578","KAFKA-6789: Handle COORDINATOR_LOAD_IN_PROGRESS, COORDINATOR_NOT_AVAILABLE retriable errors in AdminClient API","-  Handle COORDINATOR_LOAD_IN_PROGRESS, COORDINATOR_NOT_AVAILABLE retriable errors in  AdminClient describe groups, delete Groups, list Offsets API  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-27T17:40:19Z","2019-05-09T22:31:11Z"
"","5773","MINOR: Fix broken standalone ReplicationQuotasTestRig test","-  Fix ZkUtils. getReassignmentJson to return valid  json string -  Allow new file creation in ReplicationQuotasTestRig test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-10-10T11:38:44Z","2018-10-11T01:32:42Z"
"","6071","MINOR: support choosing different JVMs when running integration tests.","+ Add a parameter to the ducktap-ak to control the OpenJDK base image. + Fix a few issues of using OpenJDK:11 as the base image.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangxi","2018-12-28T06:40:54Z","2019-01-11T23:13:50Z"
"","6400","KAFKA-8067 - Kafka Connect JsonConverter Missing Optional field handling","*The JsonConverter is failing with NPE on Struct/Map types with missing optional fields while top level primitives are being handled correctly prior to this patch.*  *Unit tests to cover NPE on required Struct/Map cases with optional/null default variants.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","epheatt","2019-03-07T23:45:57Z","2020-11-07T15:57:25Z"
"","6005","KAFKA-7709: Fix ConcurrentModificationException when retrieving expired inflight batches on multiple partitions.","*Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  - Unit test - Integration test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","markcho","2018-12-05T19:24:58Z","2018-12-07T00:56:07Z"
"","6094","KAFKA-7768: Add version to java html urls","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* https://issues.apache.org/jira/browse/KAFKA-7768?filter=-2 https://github.com/apache/kafka-site/pull/176 *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ouertani","2019-01-07T07:12:13Z","2019-01-07T17:22:25Z"
"","6092","KAFKA-7778: Add KTable.suppress to Scala API","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* Add KTable.suppress to Scala API https://issues.apache.org/jira/browse/KAFKA-7778  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [ ] Verify test coverage and CI build status - [X ] Verify documentation (including upgrade notes)","closed","kip,","ouertani","2019-01-06T11:59:26Z","2020-06-12T23:56:36Z"
"","6319","MINOR: Unifies implementations for commitSync in Kafka Consumer","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Updates the internal commitSync implementation to mirror the implementation of `commitAsync` and use the same underlying method call so external classes can override just a single method.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  The point of this fix is to allow more concise and safe extensions of the base Kafka Consumer class.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ilganeli","2019-02-24T23:48:02Z","2020-11-18T16:06:49Z"
"","6454","java 8 code level update","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Java 7 support has been dropped, java 8 enhancement/shortcuts and performance features has been introduced. *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ouertani","2019-03-16T16:41:15Z","2019-03-18T05:43:53Z"
"","6437","deprecated close call removed","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  All internal call to _deprecated close_ has been removed  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ouertani","2019-03-13T10:00:15Z","2019-04-08T00:15:16Z"
"","6285","KAFKA-7492 : Updated javadocs for aggregate and reduce methods returning null behavior.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  - This is update to the existing javadocs for KGroupedStream class. More details are available at  https://stackoverflow.com/questions/52692202/what-happens-if-the-aggregator-of-a-kgroupedstream-returns-null   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","asutosh936","2019-02-18T19:04:44Z","2019-02-23T01:20:57Z"
"","6095","MINOR: Syntax error, require long but got Duration.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","heshengbang","2019-01-07T10:07:19Z","2019-04-25T16:55:31Z"
"","5977","All partitions will be assigned to one consumer, while others consumer are failover.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","JoyJava","2018-11-30T09:49:51Z","2018-11-30T09:53:15Z"
"","5524","Abstract duplicate code and do some code check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","heshengbang","2018-08-17T03:49:11Z","2018-08-22T01:05:10Z"
"","5592","0.10.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chengruyi","2018-08-31T05:53:38Z","2018-09-15T16:43:47Z"
"","5967","FailoverAssignor All partitions will be assigned to one consumer, while others consumer are failover. (冷备模式)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","JoyJava","2018-11-29T10:28:59Z","2018-11-30T09:41:16Z"
"","6268","KAFKA-7928: Deprecate WindowStore.put(key, value)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","kip,","ouertani","2019-02-14T08:00:44Z","2020-06-12T23:51:44Z"
"","6434","MINOR: Update code to not use deprecated methods","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","mjsax","2019-03-12T20:03:24Z","2019-03-16T02:47:44Z"
"","6417","MINOR: fix Scala compiler warning","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-03-09T04:17:36Z","2019-03-09T19:41:25Z"
"","6406","[DO NOT MERGE] MINOR: Some thoughts on RocksDB default configs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","guozhangwang","2019-03-08T17:17:49Z","2020-06-12T23:02:28Z"
"","6393","KAFKA-8065: restore original input record timestamp in forward()","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-03-07T20:00:02Z","2019-03-09T03:57:57Z"
"","6359","MINOR: improve JavaDocs about global state stores","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-03-03T05:14:21Z","2019-03-07T02:31:54Z"
"","6351","KAFKA-8024: Set locale to enUS for tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pkleindl","2019-03-01T10:34:37Z","2019-07-17T09:03:21Z"
"","6347","HOTFIX: change header back to http instead of https to pass license header check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2019-02-28T21:31:36Z","2019-02-28T21:39:12Z"
"","6344","MINOR: Avoid quota check when replica is in sync","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-02-28T11:56:55Z","2019-02-28T21:43:10Z"
"","6315","MINOR: Update docs to say 2.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2019-02-23T00:35:33Z","2019-02-23T17:52:19Z"
"","6294","*DO NOT MERGE* Work In Progress For Bug #1194","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kobihikri","2019-02-20T14:44:49Z","2022-06-16T07:28:53Z"
"","6275","KAFKA-7880:Naming worker thread by task id","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","FreeeeLy","2019-02-15T09:12:33Z","2019-03-05T00:32:33Z"
"","6273","KAFKA-7880:[WIP]Naming worker thread by task id","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","FreeeeLy","2019-02-15T08:18:12Z","2019-02-15T08:56:07Z"
"","6272","KAFKA-7880:Naming task thread name by task id","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","FreeeeLy","2019-02-15T06:27:56Z","2019-02-15T06:48:39Z"
"","6269","MINOR: improve JavaDocs about auto-repartitioning in Streams DSL","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-02-14T08:04:33Z","2019-02-19T01:57:00Z"
"","6264","MINOR: ConsumerNetworkClient does not need to send the remaining requests when the node is not ready","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-02-13T11:51:03Z","2019-03-03T04:07:21Z"
"","6213","MINOR: Fix producer sender topic metrics","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","lambdaliu","2019-01-31T11:22:21Z","2019-05-15T09:30:16Z"
"","6192","Kafka 3522 [WIP DO NOT MERGE]","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-01-24T08:09:34Z","2019-03-09T20:32:46Z"
"","6190","KAFKA-7440 Add leader epoch to fetch and list-offset request","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2019-01-24T02:14:11Z","2019-02-13T17:51:35Z"
"","6159","MINOR: jdk8 ppa updated to use 8u191","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jarekr","2019-01-16T22:59:05Z","2019-01-17T22:26:28Z"
"","6136","mod readme","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vonzhou","2019-01-13T03:33:31Z","2019-01-13T05:55:56Z"
"","6064","2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","herongyun","2018-12-24T07:58:28Z","2018-12-24T17:30:49Z"
"","5996","MINOR: Improve GlobalKTable docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-12-03T23:15:53Z","2019-01-22T03:13:20Z"
"","5970","create topic disk allocate strategy per broker","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","seayoun","2018-11-29T12:00:57Z","2018-11-30T08:11:16Z"
"","5966","fix","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","GZ315200","2018-11-29T08:02:39Z","2018-12-29T00:11:38Z"
"","5952","0.11.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","GZ315200","2018-11-27T15:38:23Z","2018-12-08T17:37:34Z"
"","5936","i think ? is bettern than string in understanding","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","RainPoetry","2018-11-21T09:58:11Z","2018-12-08T17:49:01Z"
"","5919","KAFKA-7636: Allow consumer to update maxPollRecords value","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kcirtap7","2018-11-16T14:17:49Z","2018-11-18T18:01:02Z"
"","5895","MINOR: improve Puncutation JavaDocs and add runtime argument check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-11-08T19:54:15Z","2018-11-17T03:21:54Z"
"","5891","MINOR: Code cleanup in StreamsResetter","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","mjsax","2018-11-08T01:00:23Z","2018-11-17T03:21:12Z"
"","5880","1.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhonghui","2018-11-05T02:24:28Z","2018-11-06T03:57:49Z"
"","5872","1.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chengxuehe","2018-11-02T08:47:51Z","2018-11-03T17:38:20Z"
"","5854","KAFKA-3955: Handle out of order offsets on log recovery","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","dhruvilshah3","2018-10-30T05:13:58Z","2019-05-07T05:29:32Z"
"","5827","MINOR: fix docs typo","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-10-22T22:38:22Z","2018-10-23T23:48:23Z"
"","5825","1.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","serenawuyajing","2018-10-22T16:44:15Z","2018-10-22T16:45:02Z"
"","5818","KAFKA-7520: Possibility to configure versions in Mirror Maker ducktape test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","akatona84","2018-10-19T09:58:19Z","2019-01-17T10:16:15Z"
"","5815","KAFKA-7518: FutureRecordMetadata.get deadline calculation fix","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","akatona84","2018-10-18T10:46:06Z","2018-11-12T19:18:42Z"
"","5796","MINOR: updates docs for KIP-358","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-10-13T20:35:08Z","2018-10-16T00:23:30Z"
"","5769","KAFKA-4932: Update docs for KIP-206","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-10-09T23:01:53Z","2018-10-10T19:10:21Z"
"","5762","Filestream","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","urcaly","2018-10-09T07:11:36Z","2018-10-10T20:20:41Z"
"","5716","MINOR: Fix typo in comment 'if' in ClusterConnectionStates.canConnect()","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hackerwin7","2018-09-29T06:30:37Z","2019-05-11T07:34:08Z"
"","5705","KAFKA-6605 fix NPE in Flatten when optional Struct is null","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mihbor","2018-09-27T20:13:30Z","2019-07-15T23:01:14Z"
"","5701","KAFKA-7434 fix NPE in DeadLetterQueueReporter - backport to 2.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mihbor","2018-09-26T21:20:46Z","2018-09-29T17:26:09Z"
"","5700","KAFKA-7434 fix NPE in DeadLetterQueueReporter","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mihbor","2018-09-26T21:08:28Z","2018-09-29T17:21:10Z"
"","5697","MINOR: Remove redundant `if` condition.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2018-09-26T16:32:54Z","2018-10-10T04:27:17Z"
"","5696","KAFKA-7367: Streams should not create state store directories unless they are needed","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-09-26T16:27:29Z","2018-11-28T05:38:01Z"
"","5652","use netty-ssl","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yuyang08","2018-09-14T06:11:25Z","2018-09-14T06:12:51Z"
"","5574","KAFKA-7345 Added Finally block to execute Closing FileChannel","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asutosh936","2018-08-26T16:12:22Z","2018-10-04T00:23:35Z"
"","6386","MINOR: Print usage when parse fails during console producer","*Handle OptionException while parsing options when using console producer and print usage before die.*","closed","","sumannewton","2019-03-07T06:24:04Z","2019-03-10T06:51:50Z"
"","5721","Fix to add rack unaware partitions using --disable-rack-aware","*Fix for [KAFKA-7465](https://issues.apache.org/jira/browse/KAFKA-7465)*  * Maintain consistency in AdminZkClient createTopic/addPartitions methods. Accept rackAwareMode in both the methods unlike accepting brokers in addPartitions and accepting rackAwareMode in createTopic. * Unit test to verify create/add partitions with rack aware disabled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sumannewton","2018-10-01T20:41:59Z","2018-10-02T15:14:03Z"
"","5637","MINOR : Fixed KAFKA-6764; Update usage for console-consumer whitelist option","*Fix for [KAFKA-6764](https://issues.apache.org/jira/browse/KAFKA-6764).*  - With group-id, always subscribe() and auto-assign. Seek based on partition/offset config(Not sure if seek should be supported by console-consumer when a group-id is specified. Because assign() and subscribe() can't be used together for the same topic-consumer-group combination).  - Without group-id (Auto Commit Offset is always disabled):   - Partition/Offset specified, then assign.   - Partition/Offset not specified, then subscribe.  Correct if I am wrong.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sumannewton","2018-09-11T07:14:49Z","2018-10-16T03:14:18Z"
"","6088","[KAFKA-7780] Kafka client should validate topic-level configs and update only unique values","**User Interface Improvement :** Kafka client should validate topic-level configs, and throw exception in case of invalid (duplicate) config values.  **Example:** ./kafka-topics.sh --create --topic test --partitions 2 --replication-factor 1 --zookeeper localhost:2181 --config cleanup.policy=delete,compact,delete should throw exception like  Invalid value delete,compact,delete for configuration cleanup.policy: Contains duplicate values  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ManoharVanam","2019-01-04T13:14:41Z","2019-01-08T06:46:48Z"
"","5726","KAFKA-7471: Multiple Consumer Group Management Feature","***FEATURE*** [KAFKA-7471 Multiple Consumer Group Management (Describe, Reset, Delete) + --groups-all option](https://issues.apache.org/jira/browse/KAFKA-7471) [KIP-379: Multiple Consumer Group Management](https://cwiki.apache.org/confluence/display/KAFKA/KIP-379%3A+Multiple+Consumer+Group+Management) [[DISCUSS]: KIP-379: Multiple Consumer Group Management](http://mail-archives.apache.org/mod_mbox/kafka-dev/201810.mbox/%3CCAMnN0WY4%2BnM7AMQ5vqoKM3LtVtgNSojrWLrSkrm5esp4-KDDPg%40mail.gmail.com%3E)  **Description:** * Describe/Delete/Reset offsets on multiple consumer groups at a time (including each group by repeating `--group` parameter) * Describe/Delete/Reset offsets on ALL consumer groups at a time (add new `--all-groups` option similar to `--all-topics`) * Reset plan CSV file generation reworked: structure updated to support multiple consumer groups and make sure that CSV file generation is done properly since there are no restrictions on consumer group names and symbols like commas and quotes are allowed.  * Extending data output table format by adding ""GROUP"" column for all --describe queries  All Unit and integration tests implemented.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rootex-","2018-10-02T15:14:35Z","2019-06-03T09:52:18Z"
"","6158","KAFKA-7834: Extend collected logs in system test services to include heap dumps","* Enable heap dumps on OOM with -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath= in the major services in system tests * Collect the heap dump from the predefined location as part of the result logs for each service * Change Connect service to delete the whole root directory instead of individual expected files * Tested by running the full suite of system tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","kkonstantine","2019-01-16T19:40:12Z","2020-10-16T05:59:21Z"
"","6133","KAFKA-7793: Improve the Trogdor command line.","* Allow the Trogdor agent to be started in ""exec mode"", where it simply runs a single task and exits after it is complete.  * For AgentClient and CoordinatorClient, allow the user to pass the path to a file containing JSON, instead of specifying the JSON object in the command-line text itself.  This means that we can get rid of the bash scripts whose only function was to load task specs into a bash string and run a Trogdor command.  * Print dates and times in a human-readable way, rather than as numbers of milliseconds.  * When listing tasks or workers, output human-readable tables of information.  * Allow the user to filter on task ID name, task ID pattern, or task state.  * Support a --json flag to provide raw JSON output if desired.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cmccabe","2019-01-12T00:36:05Z","2019-05-20T19:03:59Z"
"","6286","KAFKA-7895: fix stream-time reckoning for Suppress (2.2)","* Add suppress to system tests * Move stream-time reckoning from Task into Processor  Even within a Task, different Processors have different perceptions of time, due to record caching on stores and in suppression itself, and in general, due to any processor logic that may hold onto records arbitrarily and emit them later. Thanks to this, we can't rely on the whole task existing in the same ""instant"" of stream-time. The solution is for each processor node that cares about stream-time to track it independently.  See also #6278  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-18T19:30:35Z","2019-02-22T01:18:29Z"
"","6278","KAFKA-7895: Fix stream-time reckoning for suppress","* Add suppress to system tests * Move stream-time reckoning from Task into Processor  Even within a Task, different Processors have different perceptions of time, due to record caching on stores and in suppression itself, and in general, due to any processor logic that may hold onto records arbitrarily and emit them later. Thanks to this, we can't rely on the whole task existing in the same ""instant"" of stream-time. The solution is for each processor node that cares about stream-time to track it independently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-02-15T19:56:40Z","2019-02-18T18:20:58Z"
"","6382","KAFKA-7944: Improve Suppress test coverage","* add a normal windowed suppress with short windows and a short grace   period * improve the smoke test so that it actually verifies the intended   conditions  See https://issues.apache.org/jira/browse/KAFKA-7944  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-03-06T16:47:46Z","2019-03-12T19:34:00Z"
"","5806","KAFKA-7080 and KAFKA-7222: Cleanup overlapping KIP changes Part 2","#5804 removed `Windows#segmentInterval`, but did not remove all references to it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-10-16T15:42:24Z","2018-10-18T03:10:58Z"
"","5481","KAFKA-7250: switch scala transform to TransformSupplier","#5468 introduced a breaking API change that was actually avoidable. This PR re-introduces the old API as deprecated and alters the API introduced by #5468 to be consistent with the other methods  * also, fixed misc syntax problems  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-08T17:51:46Z","2018-08-09T17:13:43Z"
"","5820","KAFKA-7519 Clear pending transaction state when expiration fails","#### Description: *Make sure that the transaction state is properly cleared when the `transactionalId-expiration` task fails. Operations on that transactional id would otherwise return a`CONCURRENT_TRANSACTIONS` error and appear ""untouchable"" to transaction state changes, preventing transactional producers from operating until a broker restart or transaction coordinator change.*  #### Testing: *Unit tested by verifying that having the `transactionalId-expration` task won't leave the transaction metadata in a pending state if the replica manager returns an error.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","BirdHowl","2018-10-19T23:31:33Z","2018-10-22T12:16:11Z"
"","5665","Kafka 7382 - Guarantee atleast one replica of partition be alive during create topic. Kafka 7465 - Fix to add rack unaware partitions using --disable-rack-aware.","#### *Fixed [KAFKA-7382](https://issues.apache.org/jira/browse/KAFKA-7382)*  Validate that the replica assignment provided during topic creation doesn't include brokers(broker ids) that are not registered to the kafka cluster  Reuse the method *AdminZkClient.validateReplicaAssignment()* by making it public. (Hope changing the modifier shouldn't be a problem)     - Test for creating a topic with replica assignment with valid brokers.    - Test for creating a topic with replica assignment with invalid brokers.  #### *Fixed [KAFKA-7465](https://issues.apache.org/jira/browse/KAFKA-7465)*  Fix to add rack unaware partitions using --disable-rack-aware in kafka-topics.sh command. Maintain consistency in AdminZkClient createTopic/addPartitions methods. Accept rackAwareMode in both-the methods unlike accepting brokers in addPartitions and accepting rackAwareMode in createTopic.   - Test to verify create/add partitions with rack aware disabled.  @rajinisivaram @cmccabe @mjsax @vvcephei @hachikuji Review and merge.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sumannewton","2018-09-19T20:01:24Z","2018-11-26T15:30:49Z"
"","5643","[KAFKA-7379] [streams] send.buffer.bytes should be allowed to set -1 in KafkaStreams","### What changes were proposed in this pull request? `atLeast(0)` in `StreamsConfig`, `ProducerConfig` and `ConsumerConfig` were replaced by `SEND_BUFFER_LOWER_BOUND` and `RECEIVE_BUFFER_LOWER_BOUND` from `CommonClientConfigs`.  ### How was this patch tested? Three unit tests were added to `KafkaStreamsTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","aai95","2018-09-12T13:58:19Z","2018-09-21T04:28:26Z"
"","6432","Create BaseSerde","### PROPOSAL ## NOT A PULL REQUEST   I had a problem with creating custom serdes for my custom models. I realized that I make lots of boiler plate code, so I came up with `BaseSerde` class, so creating new custom serdes comes to: ``` public class CustomSerde extends BaseSerde {      private CustomSerde() {     super(Custom.class);   }      public static Serde serde() {     return new CustomSerde();   }  } ``` And then you can simply do: `CustomSerde.serde()`  What do you think of this approach? Or maybe there is some way simpler that I don't know about?","closed","","yeralin","2019-03-12T13:51:43Z","2019-03-13T16:35:50Z"
"","6446","[WIP] KAFKA-7157: Connect TimestampConverter SMT doesn't handle null values","### Goal Introduce null-value handling to TimestampConverter SMT.   ### Details The existing org.apache.kafka.connect.transforms.TimestampConverter does not handle null values. When null value is passed to SMT the NPE is thrown. This PR introduces null vallue handling for this SMT.  `schemaless null value` will result in `null record value` `schemaless null complex object` will result in `null record value` `null struct(has schema)` will result in `null record value with optional struct schema` `null struct(has schema) field` will result in `record value with null field value and optional struct schema for that field`  ### Important We consider that original schema with null value will have optional modifier. Maybe we should be smarter and decide on the value of optional modifier based on the field actual nullability.  ### Testing Unit tests are provided  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","Nimfadora","2019-03-14T17:08:03Z","2020-03-21T23:51:15Z"
"","6314","KAFKA-7778: Add KTable.suppress to Scala API","### Detailed description  * Adds `KTable.suppress` to the Scala API. * Fixed `count` in `KGroupedStream`, `SessionWindowedKStream`, and `TimeWindowedKStream` so that the value serde gets passed down to the `KTable` returned by the internal `mapValues` method. * `Suppress` API support for Java 1.8 + Scala 2.11  ### Testing strategy  I added unit tests covering: 1. Windowed `KTable.count.suppress` w/ `Suppressed.untilTimeLimit` 2. Windowed `KTable.count.suppress` w/ `Suppressed.untilWindowCloses` 3. Non-windowed `KTable.count.suppress` w/ `Suppressed.untilTimeLimit` 4. Session-windowed `KTable.count.suppress` w/ `Suppressed.untilWindowCloses`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","casey-green","2019-02-22T23:23:16Z","2020-06-12T23:49:36Z"
"","5556","KAFKA-7196: Remove heartbeat delayed operation for those removed consumers at the end of each rebalance","### Description During the consumer group rebalance, when the joining group phase finishes, the heartbeat delayed operation of the consumer that fails to rejoin the group should be removed from the purgatory. Otherwise, even though the member ID of the consumer has been removed from the group, its heartbeat delayed operation is still registered in the purgatory and the heartbeat delayed operation is going to timeout and then another unnecessary rebalance is triggered because of it.  ### Summary of testing strategy Extend one of the existing unit test ""testRebalanceCompletesBeforeMemberJoins"" to verify that the delayed heart beat operation has actually been removed so that it would not trigger another unnecessary group rebalance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Lincong","2018-08-22T18:45:37Z","2018-10-04T16:24:19Z"
"","5638","KAFKA-7332: Update CORRUPT_MESSAGE exception message description","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-11T17:39:05Z","2018-09-18T08:38:01Z"
"","5563","KAFKA-5066: Add KafkaMetricsConfig (Yammer metrics reporters) props to documentation","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-23T19:27:43Z","2018-09-25T07:48:23Z"
"","6027","KAFKA-3832: Kafka Connect's JSON Converter never outputs a null value","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  ### Context https://issues.apache.org/jira/browse/KAFKA-3832  When using the Connect JsonConverter it's impossible to produce tombstone messages, thus impacting the compaction of the topic.  ### Solution Allow the converter with and without schemas to output a NULL byte value in order to have a proper tombstone message When it's regarding to get this data into a connect record, the approach is the same as when the payload looks like `""{ ""schema"": null, ""payload"": null }""`, this way the sink connectors can maintain their functionality and reduces the BCC  ### Tests The first commit only introduces the fix and shows that **possibly** no BC break is introduced; Please help me confirm that's the case The second commit introduce tests for the fix  ### Notes  The fix could also be done at `fromConnectData`, but I felt like since the last path within the change is `convertToJsonWithEnvelope` this could avoid future internal calls to be also be faulty","closed","connect,","renatomefi","2018-12-12T16:09:41Z","2020-10-16T06:21:21Z"
"","6452","KAFKA-8114: Wait for SCRAM credential propagation in DelegationTokenEdToEndAuthorizationTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-16T06:44:28Z","2019-03-16T11:03:36Z"
"","6451","KAFKA-8111; Set min and max versions for Metadata requests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-15T18:08:04Z","2019-03-16T11:00:04Z"
"","6450","KAFKA-8091; Use commitSync to check connection failure in listener update test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-03-15T09:12:44Z","2019-03-15T15:36:22Z"
"","6449","MINOR: Fix typos in LogValidator","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-03-15T03:18:17Z","2019-03-15T07:50:27Z"
"","6423","KAFKA-8090: Use automatic RPC generation in ControlledShutdown","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-03-11T11:09:47Z","2019-04-04T16:37:29Z"
"","6408","KAFKA-8056: Use automatic RPC generation in FindCoordinator","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-03-08T18:26:28Z","2019-05-07T08:30:51Z"
"","6366","KAFKA-8034: Use automatic RPC generation in DeleteTopics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-03-04T11:04:12Z","2019-03-29T21:32:37Z"
"","6358","[Do not merge]KAFKA-7976: Increase TestUtils DEFAULT_MAX_WAIT_MS to 20 seconds","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-02T14:42:52Z","2019-03-05T13:11:35Z"
"","6349","KAFKA-7652: [WIP] Peel off the segmenting layer on session store caching","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-03-01T01:36:23Z","2019-05-29T04:54:38Z"
"","6324","KAFKA-7997: Use automatic RPC generation in SaslAuthenticate","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-02-25T17:57:27Z","2019-03-02T16:00:34Z"
"","6320","MINOR: Fix line break issue in upgrade notes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-02-25T13:14:02Z","2019-02-26T09:15:40Z"
"","6301","KAFKA-7972: Use automatic RPC generation in SaslHandshake","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-02-21T17:45:18Z","2019-02-25T05:52:27Z"
"","6259","MINOR: Fixed a couple of typos in Config docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-02-12T18:08:02Z","2019-02-13T16:25:07Z"
"","6234","MINOR: Save failed test output to build output directory","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2019-02-06T23:14:21Z","2019-02-15T18:52:02Z"
"","6212","MINOR: fix docs of common per-broker metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-01-31T09:03:27Z","2019-02-15T02:12:51Z"
"","6172","MINOR: Cleanup handling of mixed transactional/idempotent records","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-01-18T22:40:37Z","2019-02-25T17:02:19Z"
"","6160","MINOR: Update Gradle to 5.1.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-01-17T02:55:34Z","2019-01-27T05:11:00Z"
"","6086","MINOR: Bump js template to 2.2","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-01-03T19:26:30Z","2020-04-24T23:50:12Z"
"","6052","KAFKA-7762 KafkaConsumer uses old API in the javadocs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","matzew","2018-12-20T11:04:57Z","2018-12-20T16:57:32Z"
"","6037","KAFKA-7742: Fixed removing hmac entry for a token being removed from DelegationTokenCache","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2018-12-17T09:20:33Z","2018-12-20T10:27:37Z"
"","6035","MINOR: Replace tbd with the actual link for out-of-ordering data","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-12-14T17:01:44Z","2020-04-24T23:54:43Z"
"","6016","KAFKA-6970: All standard state stores guarded with read only wrapper","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","nizhikov","2018-12-08T17:34:09Z","2018-12-11T14:50:57Z"
"","6003","MINOR: Fix the missing ApiUtils tests in streams","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mrsrinivas","2018-12-05T15:42:40Z","2018-12-14T03:47:59Z"
"","5997","KAFKA-7697: Avoid blocking for leaderIsrUpdateLock in DelayedFetch","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-04T00:15:51Z","2021-04-30T01:39:52Z"
"","5994","KAFKA-7702: Fix matching of prefixed ACLs to match single char prefix","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-12-03T22:55:39Z","2018-12-04T09:44:12Z"
"","5971","KAFKA-6635: Producer close awaits for pending transaction","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-11-29T15:00:20Z","2019-04-15T22:56:37Z"
"","5969","KAFKA-7499: Extend ProductionExceptionHandler to cover serialization exceptions","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","kamalcph","2018-11-29T11:57:43Z","2020-06-12T23:56:52Z"
"","5951","MINOR: hygene cleanup in TransactionManagerTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-11-27T13:55:44Z","2018-12-03T05:32:06Z"
"","5941","MINOR: fix bootstrap-server typo in ReassignPartitionsCommand","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-11-22T12:41:55Z","2018-11-22T16:57:58Z"
"","5892","MINOR: Update version to 2.0.2-SNAPSHOT","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-11-08T16:46:26Z","2018-11-08T19:36:12Z"
"","5861","MINOR: Add try/finally block to close adminclient in DelegationTokenEndToEndAuthorizationTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-10-31T20:47:36Z","2018-11-01T05:13:34Z"
"","5823","MINOR: Mention improved fencing from KIP-320 in upgrade notes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-22T07:35:28Z","2018-10-22T13:54:53Z"
"","5817","MINOR: Add a note about Zstandard compression in the upgrade docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-10-18T17:49:26Z","2018-10-18T18:15:47Z"
"","5800","KAFKA-7505: Process incoming bytes on write error to report SSL failures","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-15T14:51:09Z","2018-10-18T14:08:42Z"
"","5791","KAFKA-7485: Wait for truststore update request to complete in test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-10-12T14:51:47Z","2018-10-12T20:36:11Z"
"","5782","KAFKA-7389: Upgrade spotBugs for Java 11 support (wip)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-11T14:32:48Z","2018-11-24T17:23:29Z"
"","5781","MINOR: Simplify handling of KafkaProducer serializer overrides","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-11T05:04:25Z","2018-10-25T23:31:34Z"
"","5763","KAFKA-7198: Enhance KafkaStreams start method javadoc.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-10-09T11:04:38Z","2018-10-10T04:30:44Z"
"","5757","MINOR: AbstractIndex.close should unmap","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-10-08T00:55:32Z","2018-10-11T00:22:31Z"
"","5751","KAFKA-7483: Allow streams to pass headers through Serializer.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-10-05T20:18:25Z","2018-10-10T04:29:19Z"
"","5745","KAFKA-7482: LeaderAndIsrRequest should be sent to the shutting down broker","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","junrao","2018-10-05T01:47:09Z","2018-10-12T17:11:55Z"
"","5734","KAFKA-3097: Update docs to mention PrincipalType ""User"" is case sensitive","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-10-03T14:08:36Z","2018-10-09T18:57:14Z"
"","5728","KAFKA-7366: Make topic configs segment.bytes and segment.ms to take effect immediately","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-10-02T16:50:13Z","2018-10-09T17:40:55Z"
"","5714","KAFKA-3514: Upgrade Documentation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-09-29T00:20:05Z","2020-04-24T23:50:14Z"
"","5713","KAFKA-7454: Use lazy allocation for SslTransportLayer buffers","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-09-28T15:06:59Z","2018-09-29T09:20:32Z"
"","5712","KAFKA-7453: Expire registered channels not selected within idle timeout","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-09-28T14:35:54Z","2018-09-29T09:07:40Z"
"","5698","MINOR: Docs on state store instantiation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-09-26T17:05:52Z","2018-10-03T18:20:50Z"
"","5690","KAFKA-7403: Follow-up fix for KAFKA-4682 (KIP-211) to correct some edge cases","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-09-25T05:27:22Z","2018-09-25T16:22:21Z"
"","5685","MINOR: add upgrade note for KIP-336","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-09-24T15:08:59Z","2018-10-25T16:45:47Z"
"","5684","KAFKA-5462: Add configuration to build custom SSL principal name (KIP-371)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-24T11:42:35Z","2018-10-25T11:47:13Z"
"","5680","KAFKA-7216: Ignore unknown ResourceTypes while loading acl cache","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-22T03:21:24Z","2018-09-26T02:46:52Z"
"","5679","KAFKA-7216: Ignore unknown ResourceTypes while loading acl cache","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-22T03:06:00Z","2018-09-26T02:46:48Z"
"","5673","KAFKA-7216: Ignore unknown ResourceTypes while loading acl cache","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-21T16:53:42Z","2018-09-26T02:53:14Z"
"","5658","MINOR: Increase timeout in log4j system test to avoid transient failures","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-09-17T11:39:01Z","2018-09-17T18:12:47Z"
"","5633","KAFKA-5690: Add support to list ACLs for a given principal (KIP-357)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-10T15:21:53Z","2018-09-17T17:38:07Z"
"","5626","MINOR: Remove deprecated Metric.value() method usage","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-09T07:56:45Z","2018-09-13T04:26:50Z"
"","5624","MINOR: Remove deprecated Resource class constructor usage","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-08T15:24:21Z","2018-09-11T07:01:16Z"
"","5616","MINOR: Enable topic deletion in test configs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-06T12:14:53Z","2018-09-08T14:48:31Z"
"","5610","[WIP] MINOR: Add streams role to kafka-acls command","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2018-09-04T20:58:07Z","2018-09-04T20:58:07Z"
"","5597","MINOR: Fix Transient test failure SslTransportLayerTest.testNetworkThreadTimeRecorded","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-09-01T14:24:09Z","2018-09-03T15:58:49Z"
"","5593","MINOR: Tidy up pattern type comparisons, remove unused producer-id","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-31T12:30:42Z","2018-09-04T20:24:06Z"
"","5569","MINOR: Remove unused compressionType parameter from TestUtils.produceMessages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-25T05:50:55Z","2018-08-25T17:05:59Z"
"","5548","MINOR: eliminate warnings from KafkaProducerTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-08-21T12:53:35Z","2018-08-23T15:02:08Z"
"","5544","MINOR: Fix transient test failure in DynamicConnectionQuotaTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-21T10:04:40Z","2018-08-22T02:44:34Z"
"","5535","Cherry-pick KAFKA-7278; replaceSegments() should not call asyncDeleteSegment() for segments which have been removed from segments list","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-08-20T18:42:32Z","2018-12-07T02:47:17Z"
"","5525","MINOR: use method references instead of anonymus classes in Errors","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-08-17T11:07:29Z","2018-08-23T13:08:24Z"
"","5508","KAFKA-7295; Fix RequestHandlerAvgIdlePercent metric calculation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-08-15T01:42:33Z","2018-12-07T02:47:40Z"
"","5503","KAFKA-7287: Set open ACL permissions for old consumer znode path","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-14T11:32:12Z","2018-08-29T02:29:19Z"
"","5495","KAFKA-7280: Synchronize consumer fetch request/response handling","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-13T12:55:34Z","2018-09-14T18:51:46Z"
"","5493","MINOR: fix side-effecting nullary methods warning in JsonValueTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-08-13T09:08:58Z","2018-08-23T13:08:48Z"
"","5491","KAFKA-7278; replaceSegments() should not call asyncDeleteSegment() for segments which have been removed from segments list","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-08-12T22:33:45Z","2019-09-04T06:25:01Z"
"","5487","KAFKA-7119: Handle transient Kerberos errors as non-fatal exceptions","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-10T09:41:23Z","2018-08-14T17:00:54Z"
"","5484","KAFKA-7261: Fix request total metric to count requests instead of bytes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-09T11:11:08Z","2018-08-12T17:38:22Z"
"","5483","MINOR: Fixed log in Topology Builder.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-08-09T05:11:30Z","2018-08-13T05:03:25Z"
"","5482","MINOR: Fixed log in Topology Builder.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2018-08-09T03:04:32Z","2018-08-09T05:27:05Z"
"","6475","KAFKA-8126: Fix flaky tests in WorkerTest","## What changes were proposed in this pull request?  Fix flaky test cases in `WorkerTest` by mocking the `ExecutorService` in `Worker`.  Previously, when using a real thread pool executor, the task may or may not have been run by the executor until the end of the test.  Related JIRA issues:   * [KAFKA-8126](https://issues.apache.org/jira/browse/KAFKA-8126)  * [KAFKA-8063](https://issues.apache.org/jira/browse/KAFKA-8063)  * [KAFKA-5141](https://issues.apache.org/jira/browse/KAFKA-5141)  ## How was this patch tested?  Ran all tests (`./gradlew test`). Ran unit tests in `connect/runtime` repeatedly.","closed","connect,","adoroszlai","2019-03-20T10:06:11Z","2020-10-16T06:19:06Z"
"","5938","KAFKA-7653 [WIP] Add KeySerde and ValueSerde types","## KAFKA-7653  https://issues.apache.org/jira/browse/KAFKA-7653  Added two new types `KeySerde[T]` and `ValueSerde[T]` and changed signatures of the relevant methods to accept `KeySerde` and `ValueSerde` instead of `Serde` as implicit params.  Also added `KeyValueAgnostic` object to `org.apache.kafka.streams.scala.Serdes`, to provide implicit conversions from `Key/ValueSerdes` to plain old `Serde` to help minimise impact on existing users  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  @guozhangwang @miguno @vvcephei","open","kip,","mtranter","2018-11-21T23:42:58Z","2020-06-12T23:02:53Z"
"","5722","[WIP] KAFKA-4436: Configuration Builders","## KAFKA-4436 https://issues.apache.org/jira/browse/KAFKA-4436  **Producer Configuration** - [x] Add `ProducerConfigBuilder` to `ProducerConfig` to add easier dynamic creation of `ProducerConfig` properties `Map`. - [x] Add unit test `ProducerConfigBuilderTest` to test builder - [x] Update `KafkaProducerTest` to use `ProducerConfigBuilder`  **Consumer Configuration** - [ ] Add `ConsumerConfigBuilder` to `ConsumerConfig` to add easier dynamic creation of `ConsumerConfig` properties `Map`. - [ ] Update `ConsumerConfigTest` to use builder - [ ] Add unit test `ConsumerConfigBuilderTest` to test builder - [ ] Update `KafkaConsumerTest` to use `ConsumerConfigBuilder`  **Streams Configuration** - [ ] Add `StreamsConfigBuilder` to `StreamsConfig` to add easier dynamic creation of `StreamsConfig` properties `Map`. - [ ] Add unit test `StreamsConfigBuilderTest` to test builder - [ ] Update `KafkaStreamsTest` to use `StreamsConfigBuilder`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wykapedia","2018-10-01T22:24:56Z","2020-05-08T23:52:15Z"
"","6465","MINOR: update docs JSON serde links","## Description  **Note:** This is a Documentation only update  Updated Streams Developer Guide as follows - Modified links to JSON serdes to point to example files in the right versions of the repos - Modified the most recent (v2.2) JSON example links to point to appropriate lines in [PageViewTypedDemo.java](https://github.com/apache/kafka/blob/2.2/streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java#L83) because the examples files for JsonPOJOSerializer and JsonPOJODeserializer were removed after `2.0` per this pull request: https://github.com/apache/kafka/pull/5590 .  ## cc, please review/merge  @joel-hamill , @JimGalasyn , @guozhangwang   ## Related  - PR https://github.com/apache/kafka/pull/5590 - Docs PR for same issue on `kafka-site`: https://github.com/apache/kafka-site/pull/190   Signed-off-by: Victoria Bialas","closed","docs,","londoncalling","2019-03-18T20:30:59Z","2019-04-23T00:26:44Z"
"","6466","Docs: updated names for deprecated streams constants","## Description  **Docs update: Streams Developer Guide**  - `KEY_SERDE_CLASS_CONFIG` and `VALUE_KEY_SERDE_CLASS_CONFIG` were deprecated in https://kafka.apache.org/11/javadoc/deprecated-list.html and replaced by `DEFAULT_KEY_SERDE_CLASS_CONFIG` and `DEFAULT_VALUE_KEY_SERDE_CLASS_CONFIG`  - Replaced deprecated constants in example code with up-to-date ones in `11` and all subsequent versions  ## Related  PR on `apache/kafka-site`:  https://github.com/apache/kafka-site/pull/191  ## cc, please review/merge:  - @joel-hamill , @JimGalasyn, @guozhangwang   Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-03-18T21:33:10Z","2019-04-23T00:26:42Z"
"","5688","KAFKA-7223: Part 3 preview","""preview"" of Part 3 of suppression.  This PR is provided to aid understanding of #5687.  To keep the diff legible, I dropped all the test changes (so the tests will actually not pass on this PR).  After #5687 is merged, I will revamp this one to become Part 3  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-09-24T22:48:55Z","2018-09-25T20:58:12Z"
"","5600","KAFKA-7372: Upgrade Jetty for preliminary Java 11 and TLS 1.3 support","""Jetty 9.4.12 includes compatibility for JDK 11. Additionally, TLS 1.3 support has been implemented. While full functionality for new JDK features is not yet supported, this release has been built and tested for compatibility with the latest releases from Oracle."":  http://dev.eclipse.org/mhonarc/lists/jetty-announce/msg00124.html  Also added an upgrade note as some ciphers are now disabled by default.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-09-01T16:27:32Z","2018-09-11T06:46:33Z"
"","6209","KAFKA-7884: Docs for message.format.version and log.message.format.version show invalid (corrupt?) ""valid values""","![apache kafka](https://user-images.githubusercontent.com/2375128/51978989-33f47600-24cf-11e9-89ef-c8116ff85947.png) The reason for the problem is simple: `ApiVersionValidator#toString` is missing. In contrast, all other Validators like `ThrottledReplicaListValidator` or `Range`, have its own `toString` method.  This update solves this problem by adding `ApiVersionValidator#toString`. It also provides a unit test for it. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-01-30T11:40:53Z","2019-02-16T00:50:01Z"
"","6479","MINOR: fix message protocol help text for ElectPreferredLeadersResult","","closed","","cmccabe","2019-03-20T21:58:18Z","2019-03-23T19:14:35Z"
"","6387","KAFKA-8060: The Kafka protocol generator should allow null defaults","","closed","","cmccabe","2019-03-07T08:00:04Z","2019-05-20T18:54:18Z"
"","6303","Documentation on Deployment Process","","closed","","johnnycaol","2019-02-21T19:53:04Z","2019-02-21T19:58:03Z"
"","6230","MINOR: Fix more places where the version should be bumped from 2.1.0 …","","closed","","cmccabe","2019-02-04T18:45:40Z","2019-05-20T19:04:23Z"
"","6226","MINOR: Bump version to 2.3.0-SNAPSHOT","","closed","","mjsax","2019-02-02T05:46:12Z","2019-02-11T22:46:54Z"
"","6214","MINOR: Fix some field definitions for ListOffsetReponse","","closed","","cmccabe","2019-01-31T18:35:45Z","2019-02-01T22:21:36Z"
"","6199","MINOR: Bump jackson version","","closed","","cmccabe","2019-01-24T23:57:34Z","2019-01-25T17:15:24Z"
"","6196","MINOR: update copyright year in the NOTICE file.","","closed","","cmccabe","2019-01-24T22:29:49Z","2019-05-20T19:03:37Z"
"","6178","MINOR: increase timeouts for KafkaStreamsTest","","closed","streams,","mjsax","2019-01-20T07:00:47Z","2019-01-25T07:35:07Z"
"","6168","KAFKA-7838: Log leader and follower end offsets when shrinking ISR","","closed","","dhruvilshah3","2019-01-18T19:17:25Z","2019-01-25T22:13:58Z"
"","6153","MINOR: log when controller begins processing logdir failure event","","closed","","dhruvilshah3","2019-01-16T01:12:32Z","2019-01-18T02:47:11Z"
"","6076","Should 'minimum' out of","","closed","","hejiefang","2018-12-29T02:14:44Z","2019-01-02T14:03:07Z"
"","6044","[WIP: DO NOT MERGE] KAFKA-3522: Allow storing timestamps in RocksDB","","closed","kip,","mjsax","2018-12-17T16:29:50Z","2020-06-12T23:56:44Z"
"","5975","MINOR: improve Streams error message","","closed","streams,","mjsax","2018-11-29T22:31:44Z","2018-12-17T12:57:07Z"
"","5931","MINOR: Improve maven artifactory url in release.py","","closed","","lindong28","2018-11-19T19:25:59Z","2018-12-07T02:50:32Z"
"","5893","KAFKA-7609: Add Protocol Generator for Kafka","","closed","","cmccabe","2018-11-08T18:42:12Z","2019-05-20T19:04:12Z"
"","5874","KAFKA-7584: StreamsConfig throws ClassCastException if max.in.flight.request.per.connect is specified as String","","closed","streams,","mjsax","2018-11-02T20:05:20Z","2018-11-15T23:25:12Z"
"","5824","MINOR: Change logging level for not-found-plugin-class-loader message","","closed","connect,","mdeverdelhan","2018-10-22T13:53:04Z","2020-04-30T07:48:57Z"
"","5786","MINOR: Refactor the Endpoint class in Java","","closed","","cmccabe","2018-10-11T21:53:03Z","2019-01-23T17:57:13Z"
"","5776","MINOR: Remove redundant try block in LogCleaner","","closed","","dhruvilshah3","2018-10-10T20:51:22Z","2018-10-22T17:34:04Z"
"","5774","KAFKA-7496: KafkaAdminClient#describeAcls should handle invalid filters gracefully","","closed","","cmccabe","2018-10-10T16:41:27Z","2019-05-20T19:05:03Z"
"","5753","Consumer Audit Support","","closed","","harelba","2018-10-06T00:02:10Z","2018-10-06T00:03:00Z"
"","5744","MINOR: Bump version to 2.2.0-SNAPSHOT","","closed","","lindong28","2018-10-05T00:15:17Z","2018-10-05T00:31:55Z"
"","5668","KAFKA-7428: ConnectionStressSpec: add ""action"", allow multiple clients","","closed","","cmccabe","2018-09-20T23:09:57Z","2018-09-26T09:08:49Z"
"","5585","KAFKA-7287: Set open ACL for old consumer znode path","","closed","","omkreddy","2018-08-29T08:34:29Z","2018-09-01T09:01:50Z"
"","5572","KAFKA-7131 Update release script to generate announcement email text","","closed","","bibinss","2018-08-25T17:17:04Z","2018-10-21T20:57:57Z"
"","5547","MINOR: Fix some Python warnings","","open","","rhardouin","2018-08-21T12:48:51Z","2018-09-14T13:14:58Z"
"","5523","MINOR: Add topic config to PartitionsSpec","","closed","","cmccabe","2018-08-17T00:12:47Z","2018-10-05T18:45:51Z"
"","5497","KAFKA-7169: Validate SASL extensions through callback on server side","","closed","","stanislavkozlovski","2018-08-13T13:15:07Z","2018-08-14T18:44:59Z"
"","5496","KAFKA-7169: Add SASL extension validation on server","","closed","","stanislavkozlovski","2018-08-13T13:09:24Z","2018-08-13T13:15:17Z"
"","5490","MINOR: Fix typo in TaskManager","","closed","","lucapette","2018-08-12T10:53:50Z","2020-11-03T02:55:46Z"