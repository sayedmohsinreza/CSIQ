"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","10209","add a conf for user to force select record version and then decides p…","…roduce api version  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mvpydq","2021-02-25T10:15:56Z","2021-02-25T10:15:56Z"
"","10208","add a conf for user to force select record version and then decides p…","…roduce api version  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mvpydq","2021-02-25T10:15:39Z","2021-02-25T10:15:46Z"
"","9592","[KAFKA-10718][Kafka Connect]add config settting, skip record when enc…","…ountering null value  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","ben-bear","2020-11-13T08:06:55Z","2020-11-13T08:17:28Z"
"","9789","KAFKA-10891:The control plane needs to force the validation of requests from the controller","…ect the control plane to be safe and stable.  At present,data requests and control requests use different endpoints,these endpoints are all registered with ZooKeeper.This results in the client business still having access to the metadata that controls the endpoint,this returns the control endpoint back to the data endpoint.One option is to turn on the firewall to disable client access to the control port.But does Kafka need to do something on its own to handle data request access control endpoints to deny those requests.I modified part of the code to deny access control endpoints for data requests.This is the first time I've submitted code to the community,I hope to get some advice, if possible, I will continue to work on this.","open","","wenbingshen","2020-12-25T05:47:20Z","2021-01-04T12:32:54Z"
"","9787","Force the validation control request,isolate other data requests.Prot…","…ect the control plane to be safe and stable.  At present,data requests and control requests use different endpoints,these endpoints are all registered with ZooKeeper.This results in the client business still having access to the metadata that controls the endpoint,this returns the control endpoint back to the data endpoint.One option is to turn on the firewall to disable client access to the control port.But does Kafka need to do something on its own to handle data request access control endpoints to deny those requests.I modified part of the code to deny access control endpoints for data requests.This is the first time I've submitted code to the community,I hope to get some advice, if possible, I will continue to work on this.","closed","","wenbingshen","2020-12-25T02:56:24Z","2020-12-25T05:47:29Z"
"","10201","KAFKA-8946: Log on DEBUG instead of WARNING for single byte array hea…","…der parsing in Kafka Connect.  Fixes https://issues.apache.org/jira/browse/KAFKA-8946  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","htreu","2021-02-24T09:06:43Z","2021-03-02T06:54:21Z"
"","9613","Cherry-pick KAFKA-10687 to 2.7","…(#9569)  Ensures INVALID_PRODUCER_EPOCH recognizable from client side, and ensure the ProduceResponse always uses the old error code as INVALID_PRODUCER_EPOCH.  Reviewers: Guozhang Wang   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-18T16:59:37Z","2020-11-18T22:07:12Z"
"","10221","KAFKA-12379: Allow configuring the location of the offset-syncs topic…","… with MirrorMaker2  This commit implements KIP-716. It introduces a new setting `offset-syncs.topic.location` that allows specifying where the offset-syncs topic is created.  https://cwiki.apache.org/confluence/display/KAFKA/KIP-716%3A+Allow+configuring+the+location+of+the+offset-syncs+topic+with+MirrorMaker2  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mimaison","2021-02-26T18:24:06Z","2021-06-29T20:34:02Z"
"","9499","KAFKA-10470: Zstd upgrade and buffering","Zstd-jni 1.4.5-6 allocates large internal buffers inside of ZstdInputStream and ZstdOutputStream. This caused a lot of allocation and GC activity when creating and closing the streams. It also does not buffer the reads or writes. This causes inefficiency when DefaultRecord.writeTo() does a series of small single bytes reads using various ByteUtils methods. The JNI is more efficient if the writes of uncompressed data were flushed in large pieces rather than for each byte. This is due to the the expense of context switching between the Java code and the native code. This is also the case when reading as well. Per https://github.com/luben/zstd-jni/issues/141 the maintainer of zstd-jni and I agreed to not buffer reads and writes in favor of having the caller do that, so here we are updating the caller. Here is part of a flame graph of  CPU time spent in the single byte writes from Kafka:  ![image](https://user-images.githubusercontent.com/1082334/97130235-3113cd00-1717-11eb-835c-cc145d549604.png)  In this patch, I upgraded to the most recent zstd-jni version with the buffer reuse built-in. This was done in https://github.com/luben/zstd-jni/pull/143 and https://github.com/luben/zstd-jni/pull/146 Since we decided not to add additional buffering of input/output with zstd-jni, I added the BufferedInputStream and BufferedOutputStream to CompressionType.ZSTD just like we currently do for CompressionType.GZIP which also is inefficient for single byte reads and writes. I used the same buffer sizes as that existing implementation.  NOTE: if so desired we could pass a wrapped BufferSupplier into the Zstd*Stream classes to have Kafka decide how the buffer recycling occurs. This functionality was added in the latter PR linked above. I am holding off on this since based on jmh benchmarking the performance gains were not clear and personally I don't know if it worth the complexity of trying to hack around the reflection at this point in time. The zstd-jni uses a very similar default recycler as snappy does currently which seems to provide decent efficiency. While this PR fixes the defect, I feel that using BufferSupplier in both zstd-jni and snappy is outside of the scope of this bugfix and should be considered a separate improvement. I would prefer this change get merged in on its own since the performance gains here are very significant relative to the more incremental and minor optimizations which could be achieved by doing that separate work.  There are some noticeable improvements in the JMH benchmarks (excerpt):  BEFORE: ``` Benchmark                                                                                                                    (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score     Error   Units CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed                                                CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   27743.260 ± 673.869   ops/s CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.alloc.rate                                 CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    3399.966 ±  82.608  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.alloc.rate.norm                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  134968.010 ±   0.012    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Eden_Space                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    3850.985 ±  84.476  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Eden_Space.norm                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  152881.128 ± 942.189    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Survivor_Space                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     174.241 ±   3.486  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Survivor_Space.norm               CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    6917.758 ±  82.522    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.count                                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    1689.000            counts CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.time                                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   82621.000                ms JMH benchmarks done  Benchmark                                                                                                    (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score       Error   Units RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage                                                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   24095.711 ±   895.866   ops/s RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.alloc.rate                                     CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2932.289 ±   109.465  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.alloc.rate.norm                                CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  134032.012 ±     0.013    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Eden_Space                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    3282.912 ±   115.042  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Eden_Space.norm                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  150073.914 ±  1342.235    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Survivor_Space                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     149.697 ±     5.786  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Survivor_Space.norm                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    6842.462 ±    64.515    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.count                                          CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    1449.000              counts RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.time                                           CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   82518.000                  ms RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize                                                     CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    1449.060 ±   230.498   ops/s RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.alloc.rate                                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     198.051 ±    31.532  MB/sec RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.alloc.rate.norm                                 CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  150502.519 ±     0.186    B/op RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space                             CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     200.064 ±    31.879  MB/sec RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space.norm                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  152569.341 ± 13826.686    B/op RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.count                                           CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15      91.000              counts RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.time                                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   75869.000                  ms RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize                                                CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2609.660 ±  1145.160   ops/s RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.alloc.rate                                 CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     815.441 ±   357.818  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.alloc.rate.norm                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  344309.097 ±     0.238    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     808.952 ±   354.975  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space.norm                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  345712.061 ± 51434.034    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen                           CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.019 ±     0.042  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen.norm                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15      18.615 ±    42.045    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Survivor_Space                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15      24.132 ±    12.254  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Survivor_Space.norm               CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   13540.960 ± 14649.192    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.count                                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     148.000              counts RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.time                                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   23848.000                  ms JMH benchmarks done ```  AFTER: ``` Benchmark                                                                                                                (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score      Error   Units CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed                                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  147792.454 ± 2721.318   ops/s CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.alloc.rate                             CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2708.481 ±   50.012  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.alloc.rate.norm                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   20184.002 ±    0.002    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Eden_Space                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2732.667 ±   59.258  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Eden_Space.norm               CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   20363.460 ±  120.585    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Old_Gen                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.042 ±    0.033  MB/sec CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.churn.G1_Old_Gen.norm                  CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.316 ±    0.249    B/op CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.count                                  CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     833.000             counts CompressedRecordBatchValidationBenchmark.measureValidateMessagesAndAssignOffsetsCompressed:·gc.time                                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    8390.000                 ms JMH benchmarks done  Benchmark                                                                                                (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score      Error   Units RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage                                                CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  166786.092 ± 3285.702   ops/s RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.alloc.rate                                 CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2926.914 ±   57.464  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.alloc.rate.norm                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   19328.002 ±    0.002    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Eden_Space                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2938.541 ±   66.850  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Eden_Space.norm                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   19404.357 ±  177.485    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Old_Gen                           CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.516 ±    0.100  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Old_Gen.norm                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       3.409 ±    0.657    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Survivor_Space                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.032 ±    0.131  MB/sec RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.churn.G1_Survivor_Space.norm               CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.207 ±    0.858    B/op RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.count                                      CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     834.000             counts RecordBatchIterationBenchmark.measureIteratorForBatchWithSingleMessage:·gc.time                                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    9370.000                 ms RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize                                                 CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   15988.116 ±  137.427   ops/s RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.alloc.rate                                  CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     448.636 ±    3.851  MB/sec RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.alloc.rate.norm                             CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   30907.698 ±    0.020    B/op RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space                         CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     450.905 ±    5.587  MB/sec RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space.norm                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   31064.113 ±  291.190    B/op RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.043 ±    0.007  MB/sec RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen.norm                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       2.931 ±    0.493    B/op RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.count                                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     790.000             counts RecordBatchIterationBenchmark.measureSkipIteratorForVariableBatchSize:·gc.time                                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     999.000                 ms RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize                                            CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15   11345.169 ±  206.528   ops/s RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.alloc.rate                             CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2314.800 ±   42.094  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.alloc.rate.norm                        CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  224714.266 ±    0.028    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space                    CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    2320.213 ±   45.521  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Eden_Space.norm               CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15  225235.965 ±  803.309    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen                       CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       0.026 ±    0.005  MB/sec RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.churn.G1_Old_Gen.norm                  CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15       2.551 ±    0.455    B/op RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.count                                  CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15     994.000             counts RecordBatchIterationBenchmark.measureStreamingIteratorForVariableBatchSize:·gc.time                                   CREATE   RANDOM               ZSTD             200           1000                 2  thrpt   15    1189.000                 ms JMH benchmarks done ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yuzawa-san","2020-10-26T03:19:44Z","2020-11-11T02:12:24Z"
"","10297","MINOR: fix failing ZooKeeper system tests","ZooKeeper-related system tests in `zookeeper_security_upgrade_test.py` and `zookeeper_tls_test.py` broke due to https://github.com/apache/kafka/pull/10199/.  That patch changed the logic of `SecurityConfig.enabled_sasl_mechanisms()` to only add the inter-broker SASL mechanism when the inter-broker protocol was `SASL_{PLAINTEXT,SSL}`.  The inter-broker protocol is left to default to `PLAINTEXT` for the `SecurityConfig` instance associated with Zookeeper since that value doesn't apply to ZooKeeper, so the default inter-broker SASL mechanism of `GSSAPI` was not being added into the set returned by `enabled_sasl_mechanisms()`.  This is actually correct -- `GSSAPI` shouldn't be added since inter-broker communication is a Kafka concept and doesn't apply to ZooKeeper.  `GSSAPI` should be added when ZooKeeper uses it, though -- which is the case in these tests.  So the prior patch referred to above uncovered a bug: we were relying on the default inter-broker SASL mechanism to signal that Kerberos was being used by ZooKeeper even though the inter-broker protocol has nothing to do with that determination in such cases.  This patch explicitly includes `GSSAPI` in the list of enabled SASL mechanisms when SASL is enabled for use by ZooKeeper.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-03-10T22:00:17Z","2021-03-17T17:58:43Z"
"","10432","KAFKA-12506","writes data to the Kafka topic as part of the test setup  Change of behavior: set up Kafka Producer and publish data to a topic as part of the AdjustStreamThreadCountTest integration test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kebab-mai-haddi","2021-03-30T02:53:25Z","2021-04-17T18:32:39Z"
"","10419","WIP: KAFKA-12506","writes data to the Kafka topic as part of the test setup  Change of behavior: set up Kafka Producer and publish data to a topic as part of the AdjustStreamThreadCountTest integration test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kebab-mai-haddi","2021-03-27T22:18:41Z","2021-03-30T02:51:29Z"
"","9672","MINOR: work in progress for Eos test(don't review)","work in progress for Eos test(don't review)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-02T03:22:23Z","2021-06-04T03:48:14Z"
"","9985","KAFKA-12237: Support non-routable quorum voter addresses","With KIP-595, we expect the RaftConfig to specify the quorum voter endpoints upfront on startup. In the general case, this works fine. However, for testing we need a more lazy approach that discovers the other voters in the quorum after startup (i.e. controller port bind). This approach also lends itself well to cases where we might have an observer that discovers the voter endpoints from, say a `DescribeQuorum` event.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","aloknnikhil","2021-01-27T20:58:15Z","2021-01-29T02:28:03Z"
"","10066","KAFKA-12278; Ensure exposed api versions are consistent within listener scopes","With KIP-500, we have more complex requirements on API accessibility. Previously all APIs were accessible on every listener exposed by the broker, but now that is no longer true. For example:  - the controller exposes some APIs which are not accessible on the broker listener (e.g. quorum/registration/heartbeat APIs) - most of the client APIs are not exposed on the controller (e.g. consumer group apis) - there are some APIs which are not implemented by the KIP-500 broker (e.g. `LeaderAndIsr` and `UpdateMetadata`) - there are some APIs which are only implemented by the KIP-500 broker (e.g. `DecommissionBroker` and `DescribeQuorum`)  All of this means that we need more sophistication in how we expose APIs and keep them consistent with the `ApiVersions` API. Up to now, we have been working around this using the `controllerOnly` flag inside `ApiKeys`, but this is not rich enough to support all of the cases listed above.  In this patch, we address this by problem by introducing a new `listeners` field to the request schema definitions. This field is an array of strings which indicate the listener types in which the API should be exposed. We currently support the following listener types:   - `zkBroker`: old broker - `broker`: kip-500 broker - `controller`: kip-500 controller  For example, the `DecommissionBroker` API has the following listeners tag: ```json   ""listeners"": [""broker"", ""controller""] ``` This indicates that the API is only on the KIP-500 broker and controller (both are needed because the request will be sent by clients and forwarded to the controller).  The patch changes the generator so that the listener type definitions are added to `ApiMessageType` and exposed through convenient helpers. At the same time, we have removed the `controllerOnly` flag from `ApiKeys` since now we can identify all controller APIs through the ""controller"" tag.  The rest of the patch is dedicated to ensuring that the API listener type is properly set and that the APIs accordingly tagged are carried through to the `ApiVersionsResponse`. We have created a new `ApiVersionManager` which encapsulates the creation of the `ApiVersionsResponse` based on the listener type. Additionally, `SocketServer` is modified to ensure the listener type of received requests before forwarding them to the request handler.  We have also fixed a bug in the handling of the `ApiVersionsResponse` prior to authentication. Previously a static response was sent, which means that changes to features would not get reflected. This also meant that the logic to ensure that only the intersection of version ranges supported by the controller would get exposed did not work. I think this is important because some clients rely on the initial pre-authenticated `ApiVersions` response rather than doing a second round after authentication as the Java client does.  One final cleanup note: I have removed the expectation that envelope requests are only allowed on ""privileged"" listeners. This made sense initially because we expected to use forwarding before the KIP-500 controller was available. That is not the case anymore and we expect the `Envelope` API to only be exposed on the controller listener. I have nevertheless preserved the existing workarounds to allow verification of the forwarding behavior in integration testing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-02-05T04:31:47Z","2021-02-19T00:28:03Z"
"","10408","KAFKA-12283: disable flaky testMultipleWorkersRejoining to stabilize build","Will address the root cause in this PR: https://github.com/apache/kafka/pull/10367, and added this test back in that PR, too.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-26T02:28:59Z","2021-04-02T21:48:36Z"
"","10404","KAFKA-12557: Fix hanging KafkaAdminClientTest","While running tests for https://github.com/apache/kafka/pull/10397, I got a test timeout under Java 8.  I ran it locally via `./gradlew clean -PscalaVersion=2.12 :clients:unitTest --profile --no-daemon --continue -PtestLoggingEvents=started,passed,skipped,failed -PignoreFailures=true -PmaxTestRetries=1 -PmaxTestRetryFailures=5` (copied from the Jenkins log) and was able to determine that the hanging test is:  org.apache.kafka.clients.admin.KafkaAdminClientTest#testClientSideTimeoutAfterFailureToReceiveResponse  It's odd, but it hangs most times on my branch, and I haven't seen it hang on trunk, despite the fact that my PR doesn't touch the client or core code at all.  Some debugging reveals that when the client is hanging, it's because the listTopics request is still sitting in its pendingRequests queue, and if I understand the test setup correctly, it would never be completed, since we will never advance time or queue up a metadata response for it.  I figure a reasonable blanket response to this is just to make sure that the test harness will close the admin client eagerly instead of lazily.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","vvcephei","2021-03-25T18:43:00Z","2021-03-30T16:47:01Z"
"","10149","AbstractCoordinator should log with its subclass","While debugging some group coordinator issues I noticed the consumer coordinator was logging using `org.apache.kafka.clients.consumer.internals.AbstractCoordinator` as the logger. Since there are two implementations of AbstractCoordinator, we should probably use the subclass as the logger","closed","","mumrah","2021-02-18T16:59:02Z","2021-02-20T03:38:44Z"
"","10403","KAFKA-12556: Add --under-preferred-replica-partitions option to describe topics command","Whether the preferred replica is the partition leader directly affects the external output traffic of the broker. When the preferred replica of all partitions becomes the leader, the external output traffic of the broker will be in a balanced state. When there are a large number of partition leaders that are not preferred replicas, it will be destroyed this state of balance.  Currently, the controller will periodically check the unbalanced ratio of the partition preferred replicas (if enabled) to trigger the preferred replica election, or manually trigger the election through the kafka-leader-election tool. However, if we want to know which partition leader is in the non-preferred replica, we need to look it up in the controller log or judge ourselves from the topic details list.  We can add the --under-preferred-replica-partitions configuration option in TopicCommand describe topics to query the list of partitions in the current cluster that are in non-preferred replicas.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","wenbingshen","2021-03-25T17:43:07Z","2021-06-10T16:46:36Z"
"","9838","KAFKA-12153; Update producer state before updating start/end offsets after truncation","When we truncate the log, the first unstable offset might become valid. On the other hand, the logic in `updateHighWatermarkMetadata` assumes that the first stable offset remains at a valid position. Since this method can be reached through either `updateLogStartOffset` or `updateLogEndOffset` in the truncation paths, we need to ensure that the first unstable offset first reflects the truncated state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-06T23:57:30Z","2021-01-09T01:38:41Z"
"","9963","MINOR: Extract ApiVersions logic from the `SocketServer` to the `KafkaApis`","When we implement KIP-511, we put the logic to extract the `SoftwareName` and the `SoftwareVersion` from the `ApiVersionsRequest` in the `SocketServer` to avoid having to wire the `ChannelMetadataRegistry` up to the `KafkaApis` layer. In retrospect, this was not a good idea because it spreads the logic to handle that request in multiple places. This patch is an attempt to move that logic to `KafkaApis`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-25T09:59:55Z","2022-02-04T20:19:23Z"
"","10388","KAFKA-12520: Ensure log loading does not truncate producer state unless required","When we find a `.swap` file on startup, we typically want to rename and replace it as `.log`, `.index`, `.timeindex`, etc. as a way to complete any ongoing replace operations. These swap files are usually known to have been flushed to disk before the replace operation begins.  One flaw in the current logic is that we recover these swap files on startup and as part of that, end up truncating the producer state and rebuild it from scratch. This is unneeded as the replace operation does not mutate the producer state by itself. It is only meant to replace the `.log` file along with corresponding indices. Because of this unneeded producer state rebuild operation, we have seen multi-hour startup times for clusters that have large compacted topics.  This patch fixes the issue by doing a sanity check of all records in the segment to swap and rebuilds corresponding indices without mutating the producer state. Similarly, we also rebuild indices without truncating the producer state when we find a missing or corrupted index in the middle of the log.  The patch also adds an extra sanity check to detect invalid bytes at the end of swap segments. Before this patch, we would truncate invalid bytes from the swap segment which could leave us with holes in the log. Because this is an unexpected scenario, we now raise an exception in such cases which will fail the broker on startup.","closed","","dhruvilshah3","2021-03-23T16:52:01Z","2021-06-09T23:41:44Z"
"","9494","KAFKA-10641: ACL Command Exit properly always with error code","When using ACL Command with SSL mode, the process is not terminating after successful ACL operation.","open","","senthilm-ms","2020-10-24T19:05:48Z","2020-12-09T09:18:04Z"
"","9716","KAFKA-10826; Ensure raft io thread respects linger timeout","When there are no pending operations, the raft IO thread can block indefinitely waiting for a network event. We rely on asynchronous wakeups in order to break the blocking wait in order to respond to a scheduled append. The current logic already does this, but only for the case when the linger time has been completed during the call to `scheduleAppend`. It is possible instead that after making one call to `scheduleAppend` to start the linger timer, the application does not do any additional appends. In this case, we still need the IO thread to wakeup when the linger timer expires. This patch fixes the problem by ensuring that the IO thread gets woken up after the first append which begins the linger timer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2020-12-09T00:10:50Z","2021-08-11T13:17:09Z"
"","10304","KAFKA-12454: Add ERROR logging on kafka-log-dirs when given brokerIds do not  exist in current kafka cluster","When non-existent brokerIds value are given, the kafka-log-dirs tool will have a timeout error:  Exception in thread ""main"" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) at kafka.admin.LogDirsCommand$.describe(LogDirsCommand.scala:50) at kafka.admin.LogDirsCommand$.main(LogDirsCommand.scala:36) at kafka.admin.LogDirsCommand.main(LogDirsCommand.scala) Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeLogDirs     When the brokerId entered by the user does not exist, an error message indicating that the node is not present should be printed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-03-11T17:38:30Z","2021-03-18T08:15:43Z"
"","9624","KAFKA-10655: wrap and catch exception for appendAsLeader failure","When leader append fails, we should trigger the resign process and do a graceful shutdown afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","abbccdda","2020-11-19T16:59:04Z","2021-08-02T23:50:43Z"
"","9531","KAFKA-10661; Add new resigned state for graceful shutdown/initialization","When initializing the raft state machine after shutting down as a leader, we were previously entering the ""unattached"" state, which means we have no leader and no voted candidate. This was a bug because it allowed a reinitialized leader to cast a vote for a candidate in the same epoch that it was already the leader of. This patch fixes the problem by introducing a new ""resigned"" state which allows us to retain the leader state so that we cannot change our vote and we will not accept additional appends.  This patch also revamps the shutdown logic to make use of the new ""resigned"" state. Previously we had a separate path in `KafkaRaftClient.poll` for the shutdown logic which resulted in some duplication. Instead now we incorporate shutdown behavior into each state's respective logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2020-10-29T19:49:31Z","2021-08-11T13:17:09Z"
"","10387","KAFKA-12537: fix application shutdown corner case with only one thread","When in EOS the run loop terminates on that thread before the shutdown can be called. This is a problem for EOS single thread applications using the application shutdown feature.   I changed it so in all cases with a single thread, the dying thread will spin up a new thread to communicate the shutdown and terminate the dying thread. Also @ableegoldman  redactored the catch blocks in runloop  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","wcarlson5","2021-03-23T16:14:56Z","2021-03-29T15:16:37Z"
"","9595","KAFKA-10724:Command to run single quorum in raft is missing ""--config…","When I run ""bin/test-raft-server-start.sh config/raft.properties"", I get an error： [2020-11-14 23:00:38,742] ERROR Exiting Kafka due to fatal exception (kafka.tools.TestRaftServer$) org.apache.kafka.common.config.ConfigException: Missing required configuration ""zookeeper.connect"" which has no default value. at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:478) at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:468) at org.apache.kafka.common.config.AbstractConfig.(AbstractConfig.java:108) at org.apache.kafka.common.config.AbstractConfig.(AbstractConfig.java:142) at kafka.server.KafkaConfig.(KafkaConfig.scala:1314) at kafka.server.KafkaConfig.(KafkaConfig.scala:1317) at kafka.tools.TestRaftServer$.main(TestRaftServer.scala:607) at kafka.tools.TestRaftServer.main(TestRaftServer.scala)   “ ./bin/test-raft-server-start.sh --config ./config/raft.properties” is work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","huldarchen","2020-11-14T15:40:46Z","2020-11-14T15:41:07Z"
"","9637","MONOR; Wrong command line suggestion","When I launch Kafka in IDE or command line, it will print `USAGE: java [options] KafkaServer server.properties [--override property=value]*`, but the real command is `java [options] kafka.Kafka server.properties`.","closed","","dengziming","2020-11-21T11:41:30Z","2020-11-23T04:02:19Z"
"","9973","KAFKA-12239 Detail log 'Error getting JMX attribute'","When collecting bulk metrics, this warning message in logs is unhelpful, it is impossible to determine which MBean is missing this attribute and fix the metric collector:  ``[2021-01-26T15:43:41,078][WARN ][org.apache.kafka.common.metrics.JmxReporter] Error getting JMX attribute 'records-lag-max' javax.management.AttributeNotFoundException: Could not find attribute records-lag-max ``` This pull request just adds the MBean object name in the log to ease debugging.","open","","gquintana","2021-01-26T19:22:48Z","2021-03-17T14:57:11Z"
"","9729","KAFKA-10839: improve consumer group coordinator unavailable message","When a consumer encounters an issue that triggers marking it to mark coordinator as unknown, the error message it prints does not give much context about the error that triggered it. This change includes the response error that triggered the transition or any other cause if not triggered by an error code in a response.","closed","","lbradstreet","2020-12-10T23:32:43Z","2020-12-15T22:38:19Z"
"","9832","KAFKA-12152; Idempotent Producer does not reset the sequence number of partitions without in-flight batches","When a `OutOfOrderSequenceException` error is received by an idempotent producer for a partition, the producer bumps its epoch, adjusts the sequence number and the epoch of the in-flight batches of the partitions affected by the `OutOfOrderSequenceException` error. This happens in `TransactionManager#bumpIdempotentProducerEpoch`.  The remaining partitions are treated separately. When the last in-flight batch of a given partition is completed, the sequence number is reset. This happens in `TransactionManager#handleCompletedBatch`.  However, when a given partition does not have in-flight batches when the producer epoch is bumped, its sequence number is not reset. It results in having subsequent producer request to use the new producer epoch with the old sequence number and to be rejected by the broker.  This PR adds logic to reset the sequences of those partitions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-06T13:24:18Z","2021-01-21T08:38:24Z"
"","10422","MINOR: Improve reproducability of raft simulation tests","When a `@Property` tests fail, jqwik helpfully reports the initial seed that resulted in the failure. For example, if we are executing a test scenario 100 times and it fails on the 51st run, then we will get the initial seed that generated . But if you specify the seed in the `@Property` annotation as the previous comment suggested, then the test still needs to run 50 times before we get to the 51st case, which makes debugging very difficult given the complex nature of the simulation tests. Jqwik also gives us the specific argument list that failed, but that is not very helpful at the moment since `Random` does not have a useful `toString` which indicates the initial seed.   To address these problems, I've changed the `@Property` methods to take the random seed as an argument directly so that it is displayed clearly in the output of a failure. I've also updated the documentation to clarify how to reproduce failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-28T19:10:13Z","2021-08-11T13:14:03Z"
"","10417","HOTFIX: wrap StreamThread#runLoop in outer catch block","We've been amassing more and more `catch` blocks around the `runOnce` loop in the StreamThread's `runLoop`. Some of these have complex handling which is performed inside the catch block, such as `handleCorrupted` which itself can throw exceptions (see KAFKA-12523).   We also have a final `catch Throwable` block which invokes the exception handler on any unexpected exceptions. But since this is part of the same `try` as the other catch blocks, exceptions thrown from eg `handleCorrupted` will bypass the `catch Throwable` block and miss the exception handler.   Also simplifies the EOS vs ALOS code paths by just returning false from the `runLoop` regardless of processing mode  This should probably be cherrypicked back to the 2.8 branch cc @vvcephei","closed","streams,","ableegoldman","2021-03-26T20:32:53Z","2021-03-27T00:15:22Z"
"","9871","KAFKA-12161; Support raft observers with optional id","We would like to be able to use `KafkaRaftClient` for tooling/debugging use cases. For this, we need the localId to be optional so that the client can be used more like a consumer. This is already supported in the `Fetch` protocol by setting `replicaId=-1`, which the Raft implementation checks for. We just need to alter `QuorumState` so that the `localId` is optional. The main benefit of doing this is that it saves tools the need to generate an arbitrary id (which might cause conflicts given limited Int32 space) and it lets the leader avoid any local state for these observers (such as `ReplicaState` inside `LeaderState`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-12T02:38:56Z","2021-08-11T13:16:06Z"
"","10135","KAFKA-10348: Share client channel between forwarding and auto creation manager","We want to consolidate forwarding and auto creation channel into one channel to reduce the unnecessary connections maintained between brokers and controller.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-02-17T03:06:08Z","2021-03-15T15:45:57Z"
"","9605","KAFKA-10727; Handle Kerberos error during re-login as transient failure in clients","We use a background thread for Kerberos to perform re-login before tickets expire. The thread performs logout() followed by login(), relying on the Java library to clear and then populate credentials in Subject. This leaves a timing window where clients fail to authenticate because credentials are not available. We cannot introduce any form of locking since authentication is performed on the network thread. So this PR treats NO_CRED as a transient failure rather than a fatal authentication exception in clients.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-11-17T15:59:54Z","2020-11-23T09:04:17Z"
"","9999","MINOR: Ensure `InterBrokerSendThread` closes `NetworkClient`","We should ensure `NetworkClient` is closed properly when `InterBrokerSendThread` is shutdown. Also use `initiateShutdown` instead of `wakeup()` to alert polling thread.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-29T06:44:53Z","2021-01-29T19:37:43Z"
"","9709","MINOR: Configure reconnect backoff in `BrokerToControllerChannelManager`","We should configure a reconnect backoff for controller connections to prevent tight reconnect loops when the controller cannot be reached. I have borrowed the same configuration we use in `TransactionMarkerChannelManager`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-07T18:24:56Z","2020-12-08T17:19:31Z"
"","9700","KAFKA-10813: InvalidProducerEpoch should be caught and throw as TaskMigrated","We should catch InvalidProducerEpoch and rethrow as TaskMigrated, similar to ProducerFenced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-12-05T02:20:15Z","2020-12-10T17:14:56Z"
"","10046","MINOR: Extends RocksDB docs","We recommend users to switch to jemalloc for RocksDB  Call for review @ableegoldman @JimGalasyn @rodesai","closed","docs,","mjsax","2021-02-03T23:23:09Z","2021-02-05T02:50:22Z"
"","9604","[MINOR] adjust the log level to error","we recently encounter this log which should be more of a warn, or error message. Even though it checks acks=0, it still should bring people's attention on ``` [2020-11-17 01:05:17,005] INFO [KafkaApi-2] Closing connection due to error during produce request with correlation id 1159730542 from client id -0 with ack=0 Topic and partition to exceptions:some-topic-name-30 -> org.apache.kafka.common.errors.RecordTooLargeException (kafka.server.KafkaApis) ```","open","","lisa2lisa","2020-11-17T15:31:50Z","2020-11-18T04:40:11Z"
"","10172","MINOR: add toString to Subscription classes","We now have the debug log [here](https://github.com/apache/kafka/blob/ea8ae976504e7a3f5c6f4a7efa5069d03316b093/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L587):  ```java log.debug(""Performing assignment using strategy {} with subscriptions {}"", assignor.name(), subscriptions); ``` But we didn't override the toString method for subscription class, so user will get the useless debug info: ``` Performing assignment using strategy cooperative-sticky with subscriptions {consumer-groupId-1-7fd39d50-5dc1-46f0-860e-f0361eb2afc0=org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$Subscription@247d8ae,  consumer-groupId-1-09ffd69e-ce6c-4212-9d2c-218d60ad8d19=org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$Subscription@48974e45} ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-22T09:05:11Z","2021-02-23T09:29:11Z"
"","10223","KAFKA-12394: Return `TOPIC_AUTHORIZATION_FAILED` in delete topic response if no describe permission","We now accept topicIds in the `DeleteTopic` request. If the client principal does not have `Describe` permission, then we return `TOPIC_AUTHORIZATION_FAILED`. This is justified because the topicId is not considered sensitive. However, in this case, we should not return the name of the topic in the response since it is sensitive.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-26T22:52:16Z","2021-03-02T18:20:07Z"
"","10228","KAFKA-10251: increase timeout for consumeing records","We need to wait for the translation state changed to `READY` to start consuming the records. But we didn't have any way to change the transationManager state in client, so we can just wait. I've confirmed that if we try another time, we can pass the tests. My test code is like this:  ```java var isFailed = false     try {       pollRecordsUntilTrue(consumer, pollAction,         waitTimeMs = waitTimeMs,         msg = s""Consumed ${records.size} records before timeout instead of the expected $numRecords records"")     } catch {       case e: AssertionFailedError => {         isFailed = true         System.err.println(s""Consumed ${records.size} records before timeout instead of the expected $numRecords records"")       }     }      if (isFailed) {       pollRecordsUntilTrue(consumer, pollAction,         waitTimeMs = 30000,         msg = s""Consumed ${records.size} records before timeout instead of the expected $numRecords records"")        // if we go to this step, it means it passed in 2nd try       fail(""failed at 1st try"")     } ```  And they failed with `failed at 1st try`, which confirmed that we can pass the tests by increasing the timeout. Currently, we wait 15 seconds, increase to 30 seconds. Thanks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-27T01:46:50Z","2021-03-16T08:44:00Z"
"","10093","MINOR: Support Raft-based metadata quorums in system tests","We need to be able to run system tests with Raft-based metadata quorums -- both co-located brokers and controllers as well as remote controllers -- in addition to the ZooKepeer-based mode we run today.  This PR adds this capability to `KafkaService` in a backwards-compatible manner as follows.  If no changes are made to existing system tests then they function as they always do -- they instantiate ZooKeeper, and Kafka will use ZooKeeper.  A good test of this PR is therefore to run a full system test suite with no actual test changes and make sure everything runs as expected.  If we want to use a Raft-based metadata quorum we can do so by introducing a `metadata_quorum` argument to the test method and using `@matrix` to set it to the quorums we want to use for the various runs of the test.  We then also have to skip creating a `ZooKeeperService` when the quorum is Raft-based.  For example, we would do the following:  ``` from ducktape.mark import matrix from kafkatest.services.kafka import KafkaService, quorum ``` ```     def __init__(self, test_context):         super(TestVerifiableProducer, self).__init__(test_context)         self.zk = ZookeeperService(test_context, num_nodes=1) if quorum.for_test(test_context) == quorum.zk else None         self.kafka = KafkaService(test_context, num_nodes=1, zk=self.zk,                                   topics={""topic"": {""partitions"": 1, ""replication-factor"": 1}}) ``` ```     def setUp(self):         if self.zk:             self.zk.start()         self.kafka.start() ``` ```     @cluster(num_nodes=3)     @matrix(producer_version=[str(DEV_BRANCH)], metadata_quorum=quorum.all)     def test_simple_run(self, producer_version=DEV_BRANCH, metadata_quorum=quorum.zk):         # the rest of the test logic remains unchanged ```  The above will end up running 3 separate tests: one with ZooKeeper, one with a co-located Raft-based controller, and once with a remote Raft-based controller.  If we want to set security protocols we could do this: ```     def setUp(self):         if self.zk:             self.zk.start()         # don't start Kafka here because we haven't configured security at this point ``` ```     @cluster(num_nodes=3)     @matrix(producer_version=[str(DEV_BRANCH)], security_protocol=['PLAINTEXT', 'SSL'], metadata_quorum=quorum.all)     @cluster(num_nodes=4)     @matrix(producer_version=[str(DEV_BRANCH)], security_protocol=['SASL_SSL'], sasl_mechanism=['PLAIN', 'GSSAPI'],             metadata_quorum=quorum.all)     def test_simple_run(self, producer_version, security_protocol = 'PLAINTEXT', sasl_mechanism='PLAIN',                         metadata_quorum=quorum.zk):         self.kafka.security_protocol = security_protocol         self.kafka.client_sasl_mechanism = sasl_mechanism         self.kafka.interbroker_security_protocol = security_protocol         self.kafka.interbroker_sasl_mechanism = sasl_mechanism         if self.kafka.quorum_info.using_raft:             controller_quorum = self.kafka.controller_quorum             controller_quorum.controller_security_protocol = security_protocol             controller_quorum.controller_sasl_mechanism = sasl_mechanism             controller_quorum.intercontroller_security_protocol = security_protocol             controller_quorum.intercontroller_sasl_mechanism = sasl_mechanism         # now we can start Kafka         self.kafka.start()         # the rest of the test logic remains unchanged ```  This PR does not update any tests -- those will come later after all the KIP-500 code is merged.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-09T23:12:50Z","2021-02-11T17:44:18Z"
"","9804","MINOR: Propagate version correctly in `FetchSnapshotRequest` constructor","We missed this in the initial check-in of the `FetchSnapshot` API. We need to pass through the version to the super constructor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-30T23:03:55Z","2020-12-31T18:26:46Z"
"","10264","HOTFIX: Controller topic deletion should be atomic","We merged https://github.com/apache/kafka/pull/10253 and https://github.com/apache/kafka/pull/10184 at about the same time, which caused a build error. Topic deletions should be atomic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-04T20:09:48Z","2021-03-04T20:19:34Z"
"","10240","KAFKA-12381: remove live broker checks for forwarding topic creation","We introduced a regression in https://github.com/apache/kafka/pull/9579 where originally we only return `INVALID_REPLICATION_FACTOR` for internal topic creation when there are not enough brokers. This PR addressed the fundamental problem which is a stale forwarding broker should not make the decision on whether to define a replication factor as invalid or not. This should only be verified on the active controller side.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","abbccdda","2021-03-02T03:13:00Z","2021-03-05T23:57:46Z"
"","10256","MINOR: Raft max batch size needs to propagate to log config","We increased the max batch size to 8MB in https://github.com/apache/kafka/pull/10063, but we forgot to propagate the change to the `Log` instance which does its own validation. I had actually noted it in this comment https://github.com/apache/kafka/pull/10063#pullrequestreview-591715059, but I forgot about it afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-04T04:49:15Z","2021-08-11T13:15:02Z"
"","10197","KAFKA-12211: don't change perm for base/state dir when no persistent store","We improved the state directory folder/file permission setting in #9583 . But we forgot to consider one situation: if user doesn't have Persistent Stores, we won't create base dir and state dir. And if there's no such dir, and when we tried to set permission to them, we'll have NoSuchFileException.  Port of https://github.com/apache/kafka/pull/9904 to the 2.6 branch","closed","","ableegoldman","2021-02-23T19:52:35Z","2021-02-24T01:06:26Z"
"","9904","KAFKA-12211: don't change perm for base/state dir when no persistent store","We improved the state directory folder/file permission setting in #9583 . But we forgot to consider one situation: if user doesn't have Persistent Stores, we won't  create base dir and state dir. And if there's no such dir, and when we tried to set permission to them, we'll have `NoSuchFileException`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-15T09:10:18Z","2021-02-23T19:52:43Z"
"","9573","KAFKA-10693: Close quota managers created in tests","We have various tests that call `QuotaFactory.instantiate()`, but don't close the returned `QuotaManagers`. This results in dangling threads from `ClientQuotaManager` and friends in test runs.  After these changes, I ran the unit tests locally and they passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-11-06T22:43:17Z","2020-11-11T10:47:41Z"
"","9911","MINOR: Fix StreamsOptimizedTest","We have seen recent system test timeouts associated with this test. Analysis revealed an excessive amount of time spent searching for test conditions in the logs.  This change addresses the issue by dropping some unnecessary checks and using a more efficient log search mechanism.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-01-15T20:16:32Z","2021-01-19T22:41:51Z"
"","10123","KAFKA-12327: Remove MethodHandle usage in CompressionType","We don't really need it and it causes problems in older Android versions and GraalVM native image usage (there are workarounds for the latter).  Move the logic to separate classes that are only invoked when the relevant compression library is actually used. Place such classes in their own package and enforce via checkstyle that only these classes refer to compression library packages.  To avoid cyclic dependencies, moved `BufferSupplier` to the `utils` package.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-13T15:54:51Z","2021-02-17T06:09:14Z"
"","10080","MINOR: KafkaBroker.brokerState should be volatile instead of AtomicReference","We don't need or use the additional functionality provided by AtomicReference.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-08T01:38:03Z","2021-02-10T15:12:10Z"
"","10157","MINOR: Raft request thread should discover api versions","We do not plan to rely on the IBP in order to determine API versions for raft requests. Instead, we want to discover them through the ApiVersions API. This patch enables the flag to do so.   In addition, this patch adds unsupported version as well as authentication version checking to all of the derivatives of `InterBrokerSendThread` which rely on dynamic api version discovery. Test cases for these checks have been added.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-02-19T02:21:48Z","2021-02-22T18:16:48Z"
"","9863","MINOR: Restore interrupt status when closing","We do not always own the thread that executes the close()  method, i.e., we do not know the interruption policy of the thread. Thus, we should not swallow the interruption. The least we can do is restoring the interruption status before the current thread exits this method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2021-01-11T15:16:45Z","2021-01-25T19:15:05Z"
"","10171","MINOR: avoid duplicate array copying from BufferValue#serialize","We can write ProcessorRecordContext to target `ByteBuffer` directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-02-22T06:55:38Z","2021-02-24T07:57:28Z"
"","10270","MINOR: Update log level in SaslServerAuthenticator","We are logging CONNECTIONS_MAX_REAUTH_MS at debug level for each mechanism. This is spammy when we enable debug logs for SaslServerAuthenticator. Updating the log level to trace level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-03-05T09:25:42Z","2021-03-05T12:30:50Z"
"","9844","MINOR: Updating files with release 2.6.1","Waiting for artifacts to appear in maven before merging  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-01-07T18:18:10Z","2021-01-14T12:24:22Z"
"","10034","MINOR: Fix windows startup scripts to use connect-log4j.properties instead of tools-log4j.properties","Very small change.  Found that the connect-standalone.bat file uses tools-log4j.properties, but if this is used and no connector properties file is specified, the connect-standalone.bat file will exit with no errors or other output.  Output when connect-standalone.bat uses tools-log4j.properties:  ![image](https://user-images.githubusercontent.com/7929462/106696379-2ff41500-660f-11eb-9ad6-37f794c255cc.png)  If connect-standalone.bat uses connect-log4j.properties, it will return an error message noting that no connector properties file is specified.  This looks to be the correct behaviour.  Output when connect-standalone.bat uses connect-log4j.properties:  ![image](https://user-images.githubusercontent.com/7929462/106696471-5dd95980-660f-11eb-8d4f-52680e29c2ed.png)","closed","connect,","ssugar","2021-02-03T04:03:25Z","2021-02-03T19:47:49Z"
"","9840","KAFKA-10867: Improved task idling","Use the new ConsumerRecords.metadata() API to implement improved task idling as described in KIP-695  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2021-01-07T05:32:50Z","2021-01-28T03:57:24Z"
"","10371","KAFKA-12479: Batch partition offset requests in ConsumerGroupCommand","Use single admin client request to obtain topic or partition information since a request per-partition could take a long time when there are a large number of partitions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-03-22T10:16:46Z","2021-03-23T09:56:56Z"
"","9776","KAFKA-10878 Check failed message in ProtocolSerializationTest","use assert to check failed message   -  testReadWhenOptionalDataMissingAtTheEndIsNotTolerated -  testReadWithMissingNonOptionalExtraDataAtTheEnd","closed","","g1geordie","2020-12-22T06:10:47Z","2021-01-10T07:10:08Z"
"","10409","KAFKA-9295: improve KTableKTableForeignKeyInnerJoinMultiIntegrationTest","Updated: improve the `KTableKTableForeignKeyInnerJoinMultiIntegrationTest` test, and keep monitoring this test.   Old: After investigation, I found the test sometimes failed because the streams keep rebalancing before they start to read/process/write data, due to heartbeat timeout: ``` Preparing to rebalance group KTable-FKJ-Multi in state PreparingRebalance with old generation 2 (__consumer_offsets-3) (reason: removing member KTable-FKJ-Multi-5d3d74ab-1475-4cef-a837-1205ba6c2bfe-StreamThread-1-consumer-4d4c9703-765d-4acb-a7f0-a180f09734b8 on heartbeat expiration) ``` When the system is slow, it might not be able to send heartbeat within default 3 seconds, and cause another rebalance triggering, and then, the consumer re-join again, and make the system even slower... To fix it, I increase the heartbeat interval, and corresponding session timeout, to avoid unnecessary rebalance, to make this test reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-26T02:53:51Z","2021-04-13T18:41:54Z"
"","9944","KAFKA-10580: Add topic ID support to Fetch request","Updated FetchRequest and FetchResponse to use topic IDs rather than topic names.  Some of the complicated code is found in FetchSession and FetchSessionHandler. We need to be able to store topic IDs and maintain a cache on the broker for IDs that may not have been resolved. On incremental fetch requests, we will try to resolve them or remove them if in toForget.  We also need to check the topic IDs of the cachedPartitions. If the topic ID changed from when the partition was cached, the strategy here is to remove it from the session since we are operating on a stale partition.  I have added tests to handle some of the edge cases where brokers have different IBP versions and some may have topic IDs while others do not.  Newest benchmarks can be found below.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-01-21T03:57:32Z","2021-07-18T03:16:33Z"
"","9684","KAFKA-10764: Add support for returning topic IDs on create, supplying topic IDs for delete","Updated CreateTopicResponse, DeleteTopicsRequest/Response and added some new AdminClient methods and classes. Now the newly created topic ID will be returned in CreateTopicsResult and found in TopicAndMetadataConfig, and topics can be deleted by supplying topic IDs through deleteTopicsWithIds which will return DeleteTopicsWithIdsResult.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2020-12-04T00:19:12Z","2021-01-29T19:40:17Z"
"","10328","MINOR: update the version number to 2.6.2","update the version number to 2.6.2  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-16T03:54:22Z","2021-03-17T22:10:26Z"
"","10188","MINOR: Update HttpClient to ""4.5.13""","Update HttpClient to recent bug fix version 4.5.13.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-02-23T07:01:00Z","2021-02-23T09:40:34Z"
"","9960","MINOR: Remove `toStruct` and `fromStruct` methods from generated protocol classes","Update few classes that were still using the removed methods (including tests that are no longer required).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-25T03:44:22Z","2021-01-25T12:41:31Z"
"","9515","KAFKA-10651: read  offsets directly from checkpoint for uninitialized tasks","Uninitialized tasks just return an empty collection in `changelogOffsets()` and are indistinguishable from genuinely stateless (or un-logged) tasks. We should just skip over these tasks and read directly from the checkpoint file when computing offset sums for a JoinGroup subscription","closed","","ableegoldman","2020-10-28T00:44:51Z","2020-11-02T18:52:48Z"
"","9568","KAFKA-10689: fix windowed FKJ topology and put checks in assignor to avoid infinite loops","Two pieces: 1) Fix bug in StreamSinkNode#writeToTopology to make sure it always calls the `addSink` overload with the specific topic name, when it exists, so that this topic gets tracked in the InternalTopologyBuilder's `nodeToSinkTopic` map. The sink topics are used by the StreamsPartitionAssignor to resolve the upstream subtopology of a repartition source topic, for whom that repartition topic will be a sink. Without this information the SPA gets stuck permanently during a rebalance 2) Improve the SPA's `setRepartitionTopicMetadataNumberOfPartitions()` method to break out of the loop if we aren't making any progress, to avoid infinitely looping if we ever have another bug like KAFKA-10689. If the SPA hasn't updated the known partition numbers for any repartition topic in the current outer loop, then we know that it's stuck and should throw a TaskAssignmentException to shut down the application","closed","streams,","ableegoldman","2020-11-06T01:24:58Z","2021-02-02T18:06:13Z"
"","9702","CVE-2020-25649: bumping jackson to patched version 2.10.5.1","Two line change in `dependencies.gradle` to pick latest patched version of jackson-databind 2.10.5 series (now in maintenance mode see https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.10#micro-patches). This should likely be backported to 2.6.x and integrated in the 2.7.0-rc cycle  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sirocchj","2020-12-05T10:51:20Z","2020-12-16T09:50:16Z"
"","10222","MINOR: disable test_produce_bench_transactions for Raft metadata quorum","Transactions are not supported in the KIP-500 early access release.  This patch disables a system test for Raft metadata quorums that uses transactions and that was still enabled after https://github.com/apache/kafka/pull/10194.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-26T21:20:28Z","2021-02-26T21:54:59Z"
"","9814","KAFKA-10869: Gate topic IDs behind IBP 2.8","Topics processed by the controller and topics newly created will only be given topic IDs if the inter-broker protocol version on the controller is greater than 2.8. This PR also adds a kafka config to specify whether the IBP is greater or equal to 2.8. System tests have been modified to include topic ID checks for upgrade/downgrade tests.  This PR also adds a new integration test file for requests/responses that are not gated by IBP (ex: metadata) This file can be expanded to check topic IDs are handled correctly when the request/response version uses topic IDs but the brokers are not able to handle them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-01-02T18:10:39Z","2021-01-20T22:32:07Z"
"","9978","KAFKA-10716: persist UUID in state directory for stable processId across restarts","To stabilize the task assignment across restarts of the JVM we need some way to persist the process-specific UUID. We can just write it to a file in the state directory, and initialize it from there or create a new one if no prior UUID exists.   This also adds a state directory-level lock -- partly to protect the ""process file"" where we write the UUID, and partly to address an issue that has been plaguing users for a long time: with this top-level lock, we can now strictly enforce that only one instance of Streams is running on the same physical state directory. This configuration has never been supported, but it's never been explicitly protected against, leading to often murky symptoms such as tasks being stuck uninitialized. With this change, we will fail fast if another process holds the lock for this state directory","closed","","ableegoldman","2021-01-27T02:55:16Z","2021-02-04T02:03:05Z"
"","9600","KAFKA-10674: Controller API version bond with forwardable APIs","To make sure the forwarded request could be properly handled by the controller, when forwarding is enabled, we should acquire the controller API versions to enforce as joint constraints back to the client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-16T19:50:48Z","2021-01-16T04:36:26Z"
"","9848","KAFKA-10357: Extract setup of repartition topics from Streams partition assignor","To implement the explicit user initialization of Kafka Streams as described in KIP-698, we first need to extract the code for the setup of the repartition topics from the Streams partition assignor so that it can also be called outside of a rebalance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-01-08T18:58:10Z","2021-01-22T18:03:46Z"
"","10163","KAFKA-10357: Extract setup of changelog from Streams partition assignor","To implement the explicit user initialization of Kafka Streams as described in KIP-698, we first need to extract the code for the setup of the changelog topics from the Streams partition assignor so that it can also be called outside of a rebalance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2021-02-19T16:54:03Z","2021-03-02T20:00:00Z"
"","10345","KAFKA-12477: dynamically upgrade rebalancing protocol based on selected assignor","To allow users to avoid the second rolling bounce when upgrading to cooperative rebalancing, we can dynamically upgrade the rebalancing protocol based on the assignor that's selected during a rebalance. This assignor is chosen by the group coordinator as the most-preferred assignor which is commonly supported by all members of the group. The assignor preference is just tied to the order in which they are listed in the `partition.assignment.strategy` config.  With this in place, a user can upgrade to cooperative rebalancing following a single rolling bounce. During the upgrade they must add the ""cooperative-sticky"" assignor to the front of the `partition.assignment.strategy` list. If the application hasn't set this config, then they will need to set it to `[""cooperative-sticky"", ""range""]`. After the rolling bounce is complete, all of the members will upgrade their protocol to COOPERATIVE.  To avoid unsafe downgrades to versions which don't support cooperative rebalancing, we throw IllegalStateException if we discover that the selected assignor's highest supported protocol is lower than the one currently being used by the consumer.","open","consumer,","ableegoldman","2021-03-18T01:03:25Z","2021-05-12T04:03:03Z"
"","10321","HOTFIX: timeout issue in Remove thread","timeout is a duration not a point in time  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-03-15T19:08:18Z","2021-03-16T18:45:15Z"
"","10330","MINOR: Add toString to various Kafka Metrics classes","This was useful while debugging a JDK 16 test failure, I noticed these were missing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-16T14:50:52Z","2021-03-16T20:51:48Z"
"","9919","MINOR: Delete unused `jenkins.sh`","This was replaced by `Jenkinsfile` a while back and it's out of date.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-16T21:51:19Z","2021-01-17T19:29:33Z"
"","10459","KAFKA-12601: Remove deprecated `delegation.token.master.key` in 3.0","This was only deprecated in 2.8, but it would be nice not to have the racially charged term `master` in 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-01T13:32:50Z","2021-04-03T18:38:57Z"
"","9928","MINOR: Exclude junit 3 transitive dependency from jfreechart","This was causing IntelliJ to choose the vintage runner when running `core` tests which would then fail.  Verified that `core` tests work in IntelliJ again after this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-19T09:53:34Z","2021-01-19T16:24:21Z"
"","10151","MINOR: Correct warning","This warning is misleading since increasing the number of threads without increasing the number of Streams clients does not improve the situation when there is not enough available capacity for standby replicas.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-02-18T17:44:37Z","2021-02-19T20:26:17Z"
"","9521","KAFKA-10638: Fix QueryableStateIntegrationTest","This test has been observed to have flaky failures. Apparently, in the failed runs, Streams had entered a rebalance before some of the assertions were made. We recently made IQ a little stricter on whether it would return errors instead of null responses in such cases: KAFKA-10598: Improve IQ name and type checks (#9408)  As a result, we have started seeing failures now instead of silently executing an invalid test (I.e., it was asserting the return to be `null`, but the result was `null` for the wrong reason).  Now, if the test discovers that Streams is no longer running, it will repeat the verification until it actually gets a valid positive or negative result.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-10-28T17:22:10Z","2020-10-29T16:57:37Z"
"","10006","KAFKA-12245; Fix flaky `FetcherTest.testEarlierOffsetResetArrivesLate`","This test has been flaky due to a race condition of some kind. I rewrote it so that it no longer relies on multiple threads.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-29T22:49:35Z","2021-02-01T19:35:05Z"
"","10235","KAFKA-12389: Upgrade of netty-codec due to CVE-2021-21290","This security vulnerability was found in netty-codec-http, but [caused by netty itself](https://github.com/netty/netty/commit/c735357bf29d07856ad171c6611a2e1a0e0000ec) and [fixed in 4.1.59.Final](https://github.com/netty/netty/security/advisories/GHSA-5mcr-gq6c-3hq2). So, upgrade the netty version from 4.1.51.Final to 4.1.59.Final.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-01T14:23:18Z","2021-03-03T04:19:46Z"
"","9523","Revert ""KAFKA-9705 part 1: add KIP-590 request header fields (#9144)""","This reverts commit 21dc5231ce9c7398c7ede4dbefa2f2202e06b2d4. As we no longer use initial principals, it makes sense to revert on trunk and 2.7 as well.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-10-28T18:25:25Z","2020-10-29T05:57:11Z"
"","9980","MINOR: Reduce size of the ProducerStateEntry batchMetadata queue.","This reduces the size at construction of the batchMetadata queue from 16 entries to 5. The backing array will be sized to 8 elements, cutting the array size in half.  We know that we will only allow a max of 5 entries in this queue, so this change will not change the alloc/copy behavior.","closed","","gardnervickers","2021-01-27T04:49:59Z","2021-06-07T14:25:46Z"
"","10346","KAFKA-12493: The controller should handle the consistency between the controllerContext and the partition replicas assignment on zookeeper","This question can be linked to this email: https://lists.apache.org/thread.html/redf5748ec787a9c65fc48597e3d2256ffdd729de14afb873c63e6c5b%40%3Cusers.kafka.apache.org%3E  linked to this jira: https://issues.apache.org/jira/browse/KAFKA-12493  This is a 100% recurring problem. The controller needs to determine whether the existing partition replica assignment is consistent between the controllerContext cache and zookeeper when processing the partition modification event  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-03-18T03:25:56Z","2021-03-24T16:29:58Z"
"","9968","KAFKA-12190: Fix setting of file permissions on non-POSIX filesystems","This pull request back-ports to 2.6 the changes I contributed in https://github.com/apache/kafka/pull/9947.  Previously, `StateDirectory` used `PosixFilePermissions` to configure its directories' permissions which fails on Windows as its file system is not POSIX-compliant. This PR updates `StateDirectory` to fall back to the `File` API on non-POSIX-compliant file systems. The File API doesn't allow as much control over the permissions so they're as close as the API permits.  The unit tests have been updated to also verify the behaviour on non-POSIX-compliant file systems.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wilkinsona","2021-01-25T20:21:35Z","2021-01-26T06:38:04Z"
"","9558","KAFKA-10342: migrate remaining RPCs to forwarding","This PR will follow up https://github.com/apache/kafka/commit/0814e4f645880a3c63102fc197c8912c63846ad5 to migrate the remaining RPCs which need forwarding:  - CreateAcls - DeleteAcls - CreateDelegationToken - RenewDelegationToken - ExpireDelegationToken - AlterPartitionReassignment - CreatePartition - DeleteTopics - UpdateFeatures  - Scram  We also refactored the `KafkaApisTest` to make it easier to add new unit tests for the forwarding path of new RPCs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-05T00:33:01Z","2020-11-10T17:18:41Z"
"","10374","(Cherry-pick) KAFKA-9274: handle TimeoutException on task reset (#10000)","This PR was removed by accident in trunk and 2.8, bringing it back to 2.8.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2021-03-22T20:42:16Z","2021-03-23T19:39:07Z"
"","10372","Reapply patch 10000: handle TimeoutException on task reset","This PR was a cherry pick of https://github.com/apache/kafka/pull/10000, which was removed by accident in trunk and 2.8, bringing it back.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-03-22T17:07:35Z","2021-03-22T20:39:30Z"
"","9905","KAFKA-12210; AdminClient should use DescribeCluster API when available","This PR updates the AdminClient to use the DescribeCluster API when available. The clients fails back to the Metadata API otherwise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-15T09:16:10Z","2021-01-20T16:45:50Z"
"","10089","KAFKA-12315: Clearing the ZkReplicaStateMachine request batch state upon ControllerMovedException","This PR resolves the issue by clearing the state of the request batch upon ControllerMovedException. Also as a safety measure, it also clears the state during shutting down of the ReplicaStateMachine.  Testing Strategy: An integration test is added to validate that the request batch state is cleared upon ControllerMovedException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2021-02-09T17:49:39Z","2022-04-12T17:55:59Z"
"","9737","KAFKA-10828: Replacing endorsing with acknowledging for voters","This PR replaces the terms endorsing with acknowledging for voters which have recognised the current leader.","closed","","vamossagar12","2020-12-12T08:32:27Z","2020-12-22T18:05:08Z"
"","9493","KAFKA-10640: Add recursive support to Connect Cast and ReplaceField transforms, and support for casting complex types to either a native or JSON string.","This PR replaces #9470 as I needed to do some cleanup of my repo and break this into its own branch so it did not block everything else. Also I have created Jira request [KAFKA-10640](https://issues.apache.org/jira/browse/KAFKA-10640) to cover a requirement for this change.  ---  I have added support for the `Cast` and `ReplaceField` transformations to recursively traverse the structure of messages (both with and without a schema) and perform the Cast or Replace operations on matching child primitive fields if they are found at any level nested within the structure.  Nested parents of all currently-supported Connect complex types should be supported (Map, Array, or Struct) but most of the primitive field handling is still similar to before (the lowest level within the nesting would normally be a Struct when using Schema or Map when schemaless).  This behavior can be controlled by a new configuration parameter called `recursive` for both transformations.  The default setting is **false** so any existing connectors would not be impacted -- you must set the parameter to **true** in order for the child complex types to be traversed.  Otherwise, the default behavior should be the same as before.  I have also cleaned up the Config names a bit to match some of the other transformations (namely, using a class interface called `ConfigName` which can be accessed statically), it is a bit nicer to work with and brings a little more consistency across the different built-in transforms.  Since this is more than just a trivial change, I have created a few new unit tests, some of them quite complex, to try and make sure that everything is working ok compared to before.  I have also been running both of these as custom SMTs against data in our production kafka cluster and used some learnings there to iron out a few issues.  I have had tens of millions of events actually using this updated code so I think/hope the changes are pretty well-tested but welcome if there is some kind of feedback or concern with them!  Here is a full list of everything that I have changed:  ### Cast  #### Added new Config Parameters: - `recursive` optional boolean, default = `false` - `complex.string.as.json` optional boolean, default = `false`  #### Changes:  - Created new public static interface `ConfigName` and marked the old public static string `SPEC_CONFIG` as deprecated (so it can still be used for a while until it is removed later). - Added new `ConfigName` for `SPEC`, `RECURSIVE`, and `COMPLEX_STRING_AS_JSON` - Created new interface `ConfigDefault` to provide an easy way to set and consume default values for these two new optional configuration parameters. - And then of course defined and set up the config parameters for usage within the rest of the class. - Added ARRAY, MAP, and STRUCT as valid types to convert FROM (but the only thing they can convert TO is STRING, otherwise if your `SPEC` tries to convert them to something else it will throw a `DataException`).  Before you would receive a `DataException` if you tried to include a complex type in the `SPEC` but now at least you are given the option to Cast them to a string! - Refactored the `apply...` methods so that they will call a child method that recursively builds the structure of the schema or value depending if the user sets the `recursive` config parameter to true (otherwise it should basically work the same as before -- look at everything that is at the top level of the structure and then return).  There are new methods for recursively handling different type of structures but in the end the same basic flow happens at each child level as what was happening before (`for each field in fields...`).     - However, one change in the design is that when building the new Value, instead of looping through each field from the new schema and performing an `oldstruct.get(oldfield)`, it instead will loop again through each field in the old schema.  This is so that nested primitive conversions happen correctly and what happens in the Value should be exactly the same thing that happened in the Schema. - When a child schema is created as part of the recursion, it is also added to the `schemaUpdateCache`, and the recursive methods call `getOrBuildUpdatedSchema` so they should fetch it from the cast in case it has already been converted.  This is the same when child values are converted -- they should fetch their new child schemas from the cache instead of building them again. - Added usage of an instance of `JsonConverter` and `JsonDeserializer` if you wish to have the complex types toString come out in a JSON text format instead of the Java object's toString() implementation.  This is helpful for scenarios for example like using a JdbcSinkConnector where you have an array of structs as one field in your source, you can put these values into a database table as a blob of JSON text, then parse these values as JSON (for example PostgreSQL has loads of really good JSON parsing).  This behavior is controlled by the new configuration parameter `complex.string.as.json` but again by default it is set to false (so you have to set to true in order to use this). - I also changed `castValueToType` from `private static` to `private`.  This was so that we can allow a check in this method on the instance value of our new `complex.string.as.json` configuration parameter and decide to call `castToJsonString` instead of the old `castToString` method.   - I realize this one can maybe be a bit controversial, and there were several options I considered.  First, however, I tried to examine any public static method or property of any kind and see if anything ever makes use of `castValueToType` and could not see any (please mention if you see something I missed).  So a change from private static to private in this case seems very minor and should not impact any functionality -- nothing else can see or use it anyway outside of this class itself.   - Another option is possibly a bit more like the `TimestampConverter` transform, where they have created a private instance of a `Config` subclass and then pass the entire instance to some of the static methods.  But I think that change would have to ""touch""  a lot more places, and given the point above (I did not see where it was actually used anywhere from anything public static ) this seemed like a bit too much of a rebuild.   - Also usage of this `castToJsonString` with using the `JsonConverter` and `JsonDeserializer` could be done a few other ways... but this was the one that I felt tried to use existing functionalities within Connect and was the ""cleanest"" looking that I could initially come up with from a code perspective.  And the reason to use a single private instance of the classes was to try and save a bit on resources -- it is possible that you will have multiple Json string conversions even within one record so I thought it was better to only have one instance we can re-use over and over. - Changed the log level for the Cast log message from Trace to Debug (this seems like something you definitely want to see in Debug and not just Trace!). - Added new test cases for casting Arrays and Maps to string, to JSON Strings, as well as for working with different types of recursive records (both with and without schema).  In these tests, I found that a lot of the weird issues only happen once you go a few levels deep so I have made the tests a little more complicated to ensure that everything was working properly even with multiple nested levels.  #### Example Usage  ``` curl -X PUT -H ""Content-Type: application/json"" --data '{   ""connector.class"": ""FileStreamSink"",   ""topics"": ""test"",   ""file"": ""/tmp/cast.txt"",   ""transforms"": ""cast"",   ""transforms.cast.type"": ""org.apache.kafka.connect.transforms.Cast$Value"",   ""transforms.cast.spec"": ""child_int:string,child_array_of_structs:string"",   ""transforms.cast.recursive"": ""true"",   ""transforms.cast.complex.string.as.json"": ""true"" }' http://localhost:8083/connectors/filestreamsink_cast_test/config ```   ### ReplaceField  #### Added new Config Parameters: - `recursive` optional boolean, default = `false`  #### Changes:  - A lot of the changes here are very similar to what was done in `Cast`.  In fact, even before my changes, the flow of these two transforms was actually almost identical, so it worked quite well to do them both like this at the same time and they are easily used together in transform chains within Connect. - Namely, there is a new `recursive` configuration parameter and the `apply...` methods have been updated to recursively call methods to build the new target schema and values.  A lot of the code is exactly the same so you should even find direct copy-paste from `Cast` to here and using a lot of the same private object names, methods, etc.    - Note also that the pattern follows the same as with `Cast` -- looping through the old schema and converted child values based on the old structure instead of first creating a new schema and then getting the old field.  This new flow also means that the `reverseRenamed` method and its `reverseRenames` map are no longer needed either (but I left them in for now). - Added a logger instance and added Debug-level logging for a few different events, such as when a field is excluded or included, or when it is renamed.  So some of the methods were refactored a bit in order to provide this logging (for example the `filter` and `renamed` methods). - I also added support when using `renames` for a ""contains exactly one of"" kind of scenario.  What I mean by this is that in your schema, you have several fields which you know by design that only one of them will have a value, and when that one has a value, all of the rest within that group will be null.  The change to this transform now allows you to specify a target name more than one time, but when the value transform is occurring, if more than one of them have a value then it will throw a `DataException`.  Before, the transform would throw a `DataException` when building the new schema based on the `renames` config (trying to add a duplicate field to the schema) but instead now this check happens when building the updated value instead. - Also added a few unit tests to handle new scenarios and recursive operation (both with and without schemas).  #### Example Usage  ``` curl -X PUT -H ""Content-Type: application/json"" --data '{   ""connector.class"": ""FileStreamSink"",   ""topics"": ""test"",   ""file"": ""/tmp/replace.txt"",   ""transforms"": ""replace"",   ""transforms.replace.type"": ""org.apache.kafka.connect.transforms.ReplaceField$Value"",   ""transforms.replace.renames"": ""child_value_one:child_value,child_value_two:child_value"",   ""transforms.replace.recursive"": ""true"", }' http://localhost:8083/connectors/filestreamsink_replace_test/config ```  I hope this covers everything but if you have any questions or concerns then please feel free to ask!    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","joshuagrisham","2020-10-24T17:45:21Z","2021-08-18T13:37:05Z"
"","9497","KAFKA-10619: Configure producer with idempotence and acks all by default (KIP-679)","This PR relies on existing tests. A subsequent PR will make additional test adjustments to ensure coverage of the non-default behavior is still good after this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","warrenzhu25","2020-10-25T17:54:52Z","2021-07-01T14:07:45Z"
"","10025","Upstream QuotaConfigs","This PR moves static property definitions for user client quotas into a new class called QuotaConfigs in the clients module under the o.a.k.common.config.internals package. This is needed to support the client quotas work in the quorum based controller.","closed","kip-500,","mumrah","2021-02-02T21:35:25Z","2021-02-03T05:32:32Z"
"","9547","KAFKA-9630; Replace OffsetsForLeaderEpoch request/response with automated protocol","This PR migrates the `OffsetsForLeaderEpoch` request/response to the automated protocol. It also refactors the `OffsetsForLeaderEpochClient` to use directly the internal structs generated by the automated protocol. It relies on the existing tests.  It seems that we could also refactor the broker (api layer, fetcher) to directly use the internal structs of the generated protocol but, as it is more involved, I propose to address it in a separate PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-11-03T08:45:39Z","2020-11-19T10:41:55Z"
"","10454","MINOR: Rename LogUtils class in tests to LogTestUtils","This PR is a precursor to the recovery logic refactor work (KAFKA-12553).  I've renamed the file: `core/src/test/scala/unit/kafka/log/LogUtils.scala` to `core/src/test/scala/unit/kafka/log/LogTestUtils.scala`. Also I've renamed the underlying lass from `LogUtils` to `LogTestUtils`. This is going to help avoid a naming conflict with a new file called `LogUtils.scala` that I plan to introduce in `core/src/main/scala/kafka/log/` as part of the recovery logic refactor. The new file will also contain a bunch of static functions.  **Tests:** Relying on existing tests to catch regressions (if any) since this is a simple change.","closed","","kowshik","2021-03-31T21:18:01Z","2021-03-31T23:48:40Z"
"","10426","KAFKA-12571: Eliminate LeaderEpochFileCache constructor dependency on logEndOffset","This PR is a precursor to the recovery logic refactor work ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)).  **Problems:** For refactoring the recovery logic ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)), we would like to [move the logic to initialize](https://github.com/apache/kafka/blob/d9bb2ef596343da9402bff4903b129cff1f7c22b/core/src/main/scala/kafka/log/Log.scala#L579) `LeaderEpochFileCache` out of the `Log` class and into a separate static function. In the future, once we successfully initialize `LeaderEpochFileCache` outside `Log`, we will be able pass it as a dependency into both the `Log` recovery module and `Log` class constructor. However, currently the `LeaderEpochFileCache` constructor takes a [dependency](https://github.com/apache/kafka/blob/d9bb2ef596343da9402bff4903b129cff1f7c22b/core/src/main/scala/kafka/server/epoch/LeaderEpochFileCache.scala#L42) on `logEndOffset` (via a callback), which poses the following problems: 1. Blocks the instantiation of `LeaderEpochFileCache` outside `Log` class. Because, outside `Log` the `logEndOffset` is unavailable to be passed into `LeaderEpochFileCache` constructor. As a result, this situation blocks the recovery logic ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)) refactor work. 2. It turns out the `logEndOffset` dependency is used only in 1 of the `LeaderEpochFileCache` methods: `LeaderEpochFileCache.endOffsetFor`, and just for 1 particular case. Therefore, it is overkill to pass it in the constructor as a dependency. Also a callback is generally not a neat way to access dependencies and it poses code readability problems too.  **Solution:** This PR modifies the code such that we only pass the `logEndOffset` as a parameter into `LeaderEpochFileCache.endOffsetFor` whenever the method is called, thus eliminating the constructor dependency. This will also unblock the recovery logic refactor work ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)).  **Tests:** I have modified the existing tests to suit the above refactor.","closed","","kowshik","2021-03-29T08:42:46Z","2021-03-31T00:31:27Z"
"","9686","KAFKA-10804 add more subsets and exclude performance tests","This PR is a part of https://issues.apache.org/jira/browse/KAFKA-10804  The following tasks are included by this PR.  1. add more subsets 1. not to run all system tests - exclude performance  **The tracing flaky/failed tests**  1. streams_eos_test.py -> #9706 1. connect_distributed_test.py -> #9673 1. downgrade_test.py and upgrade_test.py  -> #9712  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-04T03:24:56Z","2021-02-11T17:02:08Z"
"","10428","KAFKA-12572: Add import ordering checkstyle rule and configure an automatic formatter","This PR is a part of [KAFKA-10787](https://issues.apache.org/jira/browse/KAFKA-10787) umbrella task, replacing the previous implementation [here](https://github.com/apache/kafka/pull/8404/). **In short, this PR provides automatic import ordering and checking configurations, which will be a foundation of actual code formatting tasks later.** Here are some backgrounds.  When I first started [KAFKA-10787](https://issues.apache.org/jira/browse/KAFKA-10787), I thought it is a trivial one and can be resolved with only one PR. But it is entirely wrong.  1. It modifies hundreds of files; it is not only hard to review but also, every time it has some code conflict with the trunk branch. 2. As @mjsax already pointed out, the previous implementation omits the essential way of solving this problem - **automatic import formatter**. Since it includes the checkstyle rules only, it shows error messages for the import ordering violation and that's it.  For the reasons above, it is almost impossible to resolve this issue with one commit or PR.  So, I restarted this feature from scratch. First of all, I changed the approach like following:  1. Take an incremental approach, i.e., enlarge the import ordering rules applied area gradually. (In other words, narrow down the exception area to the import ordering rules, step by step.) 2. By adopting automatic code formatter, make the build tool to apply the import order automatically.  This approach has the advantages like:  1. We can apply the import order step by step; it is feasible to review and greatly reduces the possibility of code conflict. 2. The dev team doesn't have to care about the formatting anymore; the build tool automatically take cares of it.  With this scheme, we can expand the scope of auto-formatting and checkstyle incrementally.  In turn, I converted [KAFKA-10787](https://issues.apache.org/jira/browse/KAFKA-10787) into an umbrella one. As the first subtask, this PR do the following:  1. Define the import ordering rule in checkstyle: `kafka`, `org.apache.kafka`, `com`, `net`, `org`, `java`, `javax` and static imports. It is identical to [what we discussed in the dev mailing list](https://lists.apache.org/thread.html/rf6f49c845a3d48efe8a91916c8fbaddb76da17742eef06798fc5b24d%40%3Cdev.kafka.apache.org%3E), except each package is grouped independely. (e.g., `kafka` and `org.apache.kafka` are regarded as independent groups.) 2. Add an eclipse formatter file, which includes:     - Minimal formatting rules that follow current checkstyle rules.     - `ImportOrder` rule for the newly introduced import ordering rule. 3. **Add a spotless configuration to reformat the subset of modules automatically.**  Here are some reasons for the decisions I made:  1. Why choose eclipse formatter?      As all of you know, Apache Kafka is using gradle as its build tool. To automatically apply the import ordering rule with gradle, we have to use [spotless](https://github.com/diffplug/spotless), which supports two formatters - google formatter and eclipse formatter.          If we choose google formatter, it changes not only import ordering but also the other aspects of code, like an indentation. To minimize non-import ordering modifications, I choose eclipse formatter, which supports a custom formatter configuration.      As you can see here, the formatter defines a minimal rules for out code only.  2. The original proposal consists of 3 groups: `kafka`/`org.apache.kafka`, `com`/`org`/`net`, `java`/`javax`, and static imports. Why are those packages separated into independent groups?      It's simple - the spotless automatic formatter doesn't allow packages with different prefixes into one group. So I separated them.  **\* Updated (June 10th 2021): The `core` module is excluded from the PR, and it now only focuses on providing auto-formatting configurations.**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-03-29T11:51:13Z","2021-07-08T12:25:32Z"
"","10233","KAFKA-9413: Auditing in Kafka","This PR introduces an auditor functionality in Kafka designed in KIP-567.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","viktorsomogyi","2021-03-01T12:37:47Z","2021-03-01T12:39:08Z"
"","9487","KAFKA-9331: Add a streams specific uncaught exception handler","This PR introduces a streams specific uncaught exception handler that currently has the option to close the client or the application. If the new handler is set as well as the old handler (java thread handler) will be ignored and an error will be logged.  The application shutdown is achieved through the rebalance protocol.    Testing is done through a series of integration tests and unit tests. The Integration test make a processor that throws and StreamsException which triggers the handler. Then it verifies that Streams object is in the correct state and had reacted accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2020-10-23T20:45:51Z","2021-01-21T21:46:01Z"
"","9746","MINOR: Replace ApiVersion by auto-generated protocol","This PR includes following changes.  1. rename (auto-generated) ```ApiVersionsResponseKey``` to ```ApiVersion``` 2. replace clients/src/main/java/org/apache/kafka/clients/ApiVersion.java by auto-generated ```ApiVersion```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-14T12:12:30Z","2021-01-19T16:37:46Z"
"","10248","MINOR: main function of o.a.k.c.p.t.Type does not show all types","This PR includes following changes  1. rename `UNSIGNED_INT32` to `UINT32` (consistent to `UINT16`) 1. make sure Type.toHtml shows `UINT16`, `UINT32` and `COMPACT_RECORDS`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-03-02T17:33:23Z","2021-03-03T04:30:17Z"
"","9903","KAFKA-12204; Implement DescribeCluster API in the broker","This PR implements the DescribeCluster API in the broker.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-15T08:16:59Z","2021-01-22T07:59:16Z"
"","10150","KAFKA-3745: Add access to read-only key in value joiner","This PR implements adding read-only access to the key for `KStream.join` as described in [KIP-149](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner)  This PR as it stands does not affect the Streams Scala API.  Updating the Streams Scala API will be done in a follow-up PR. Additionally, the original KIP did not include the `KTable` API, but I don't see any reason why we wouldn't want the same functionality there as well, this will be done in an additional follow-up PR after updating the existing KIP.   Tests added ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","bbejeck","2021-02-18T17:01:34Z","2021-03-21T02:19:34Z"
"","10244","KAFKA-12399: Deprecate Log4J Appender","This PR implements `log4j2-appender`, a log4j2 equivalent of traditional `log4j-appender`.  All `log4j-appender` configuration properties are supported, with some additions:  1. `brokerList` is deprecated for the consistency of the other CLI tools. `bootstrapServers` is added instead. 2. `requiredNumAcks` is deprecated for the consistency of `Producer`'s `Properties` instance. `acks` is added instead. 3. A new configuration option, `producerClass`, is added. Any `Producer` implementation with the `Properties` argument is supported. The unit test itself uses this feature with `MockProducer`. 4. In `log4j-appender`, the default values of `retries`, `requiredNumAcks`, `deliveryTimeoutMs`, `lingerMs`, and `batchSize` are redundantly defined in `Log4jAppender` implementation. Since their default values are already defined in `ProducerConfig`, `log4j2-appender` does not define their default values.  Implements https://cwiki.apache.org/confluence/display/KAFKA/KIP-719%3A+Deprecate+Log4J+Appender","open","kip,","dongjinleekr","2021-03-02T07:40:27Z","2022-03-21T14:29:43Z"
"","9579","KAFKA-9751: Forward CreateTopicsRequest for FindCoordinator/Metadata when topic creation is needed","This PR forward the entire FindCoordinator request to the active controller when the internal topic being queried is not ready to be served yet.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","abbccdda","2020-11-09T17:52:50Z","2021-02-06T21:04:31Z"
"","10207","Fixing documentation source for issue KAFKA-12360","This PR fixes de documentation issue in https://issues.apache.org/jira/browse/KAFKA-12360","closed","docs,","nicodds","2021-02-25T08:37:04Z","2021-07-26T23:52:10Z"
"","10130","MINOR: Fix typo in MirrorMaker","This PR fixes a typo.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","runom","2021-02-16T11:22:20Z","2021-03-04T05:34:36Z"
"","10257","KAFKA-12407: Document Controller Health Metrics","This PR fills the omitted documentation of:  - `kafka.controller:type=ControllerChannelManager,name=RequestRateAndQueueTimeMs,brokerId=([-.\w]+)` - `kafka.controller:type=ControllerEventManager,name=EventQueueSize` - `kafka.controller:type=ControllerEventManager,name=EventQueueTimeMs`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-04T06:22:00Z","2021-03-04T14:24:50Z"
"","9945","KAFKA-12212; Bump Metadata API version to remove `ClusterAuthorizedOperations` fields","This PR bumps the version of the Metadata API and deprecates the `IncludeClusterAuthorizedOperations` and the `IncludeClusterAuthorizedOperations` fields from version 11 and onward.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-21T10:32:48Z","2021-01-22T08:06:41Z"
"","10101","Add ClientQuotaMetadataManager for processing QuotaRecord","This PR brings in the new broker metadata processor for handling QuotaRecord-s coming from the metadata log. Also included is a new cache class to allow for fast lookups of quotas on the broker for handling DescribeClientQuotaRequest.   See the original PR here https://github.com/confluentinc/kafka/pull/477","closed","kip-500,","mumrah","2021-02-10T18:54:06Z","2021-02-10T22:04:22Z"
"","10285","KAFKA-12442: Upgrade ZSTD JNI from 1.4.8-4 to 1.4.9-1","This PR aims to upgrade ZSTD JNI from 1.4.8-4 to 1.4.9-1.  `ZStandard 1.4.9 and its corresponding JNI brings the following bug fixes and improvements. - https://github.com/facebook/zstd/releases/tag/v1.4.9  One of notable improvement of ZStandard 1.4.9 is 2x faster Long Distance Mode, but we are not using it yet.  Since this is a dependency change, this should pass all the existing CIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjoon-hyun","2021-03-09T18:00:45Z","2021-03-13T09:47:18Z"
"","9717","KAFKA-10766: Unit test cases for RocksDBRangeIterator","This PR aims to add unit test cases for RocksDBRangeIterator which were missing.","closed","","vamossagar12","2020-12-09T01:39:26Z","2021-03-02T17:56:01Z"
"","9756","KAFKA-10652: Adding size based linger semnatics to Raft metadata","This PR aims to add leader fsync occur with size based heuristic. Meaning, if we accumulate a configurable N bytes, then we should not wait for linger expiration and should just fsync immediately.","open","","vamossagar12","2020-12-15T10:00:23Z","2021-05-28T02:39:50Z"
"","10113","MINOR: Add KIP-500 BrokerServer and ControllerServer","This PR adds the KIP-500 `BrokerServer` and `ControllerServer` classes.  It makes the following changes to accommodate them:  - `SocketServer` now accepts optional endpoints in `startup()` so that it can work in a `BrokerToControllerChannelManager` that needs to talk to the Raft-based controllers (whose end points are defined differently in the config). - There is a `RaftControllerNodeProvider` implementation for use with `BrokerToControllerChannelManager` - `AutoTopicCreationManager` now accepts `Option[]`s for `ZkAdminManager` and `KafkaController` since those are not used with Raft-based metadata quorums. - We add the epoch parameter in `RaftClient.handleResign(epoch)` because the epoch is required by `MetaLogListener.handleRenounce(epoch)` - We return a random active Broker ID as the Controller ID in `MetadataResponse` for the Raft-based case as per KIP-590. - We add `object EnvelopeUtils` to hold common code related to KIP-590 envelope processing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-11T22:09:43Z","2021-02-18T21:18:07Z"
"","9685","KAFKA-10748: Add IP connection rate throttling metric","This PR adds the IP throttling metric as described in KIP-612.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-12-04T01:09:29Z","2020-12-10T11:24:46Z"
"","9528","MINOR: Add releaseTarGz to args for building docs","This PR adds the `releaseTarGz` command back into the `release.py::command_stage_docs` method when building docs for staging  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-10-29T15:41:49Z","2020-10-30T21:29:15Z"
"","9628","KAFKA-10747: Implement APIs for altering and describing IP connection rate quotas","This PR adds support for IP entities to the `DescribeClientQuotas` and `AlterClientQuotas` APIs. This change does not require any protocol version bumps.   This PR also adds support for describing/altering IP quotas via `kafka-configs` tooling.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-11-20T07:34:54Z","2020-12-10T08:53:56Z"
"","9799","QuotaRecord support in broker","This PR adds support for handling QuotaRecord metadata in the broker. It also includes a new QuotaCache class for efficient lookups of the quotas.","closed","","mumrah","2020-12-30T18:21:04Z","2020-12-30T18:21:37Z"
"","9512","KAFKA-10394: generate snapshot","This PR adds support for generating snapshot for KIP-630.  1. Adds the interfaces `RawSnapshotWriter` and `RawSnapshotReader` and the implementations `FileRawSnapshotWriter` and `FileRawSnapshotReader` respectively. These interfaces and implementations are low level API for writing and reading snapshots. They are internal to the Raft implementation and are not exposed to the users of `RaftClient`. They operation at the `Record` level. These types are exposed to the `RaftClient` through the `ReplicatedLog` interface.  2. Adds a buffered snapshot writer: `SnapshotWriter`. This type is a higher-level type and it is exposed through the `RaftClient` interface. A future PR will add the related `SnapshotReader`, which will be used by the state machine to load a snapshot.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-10-27T17:12:43Z","2020-12-08T02:13:09Z"
"","10254","KAFKA-12406 Integrate client quotas with raft broker","This PR adds DescribeClientQuota RPC as a supported RPC of the raft broker. It also fixes the case where an empty set of entity filters in non-strict mode should return _all_ quota entities. Previously, we were returning no entities in this case. This bug was exposed when running ClientQuotaRequestTest in Raft mode.  Also included here is an example of running an existing integration test in ZK and self-managed mode. Since [ClientQuotaRequestTest](https://github.com/apache/kafka/pull/10254/files#diff-f02b619b5cd14e83f7e21dbe211b3f88336f825e1ee5c630fee32b8a0fbe3d20) had previously been converted to use ClusterTestExtentions, enabling self-managed mode was simply a matter of adding the type to the annotation.  ```diff - @ClusterTestDefaults(clusterType = Type.ZK) + @ClusterTestDefaults(clusterType = Type.BOTH) @ExtendWith(value = Array(classOf[ClusterTestExtensions])) class ClientQuotasRequestTest(cluster: ClusterInstance) { ```  A new test case was also added to RaftClusterTest for client quotas.","closed","kip-500,","mumrah","2021-03-03T20:14:31Z","2021-04-08T17:56:22Z"
"","9986","JUnit extensions for integration tests","This PR adds a set of annotations that can be used to generate one or more Kafka clusters for integration testing. The primary motivation for this is to reuse existing integration tests for both Zk and Raft based Kafka clusters. A secondary motivation is to refactor the complex test hierarchy that exists and instead provide reusable functionality to tests through helper classes.  # Annotations  A new `@ClusterTest` annotation is introduced which allows for a test to declaratively configure an underlying Kafka cluster.  ```scala @ClusterTest def testSomething(): Unit = { ... } ```  This annotation has fields for cluster type and number of brokers, as well as commonly parameterized configurations. Arbitrary server properties can also be provided in the annotation:  ```java @ClusterTest(clusterType = Type.Zk, securityProtocol = ""PLAINTEXT"", properties = {   @ClusterProperty(key = ""inter.broker.protocol.version"", value = ""2.7-IV2""),   @ClusterProperty(key = ""socket.send.buffer.bytes"", value = ""10240""), }) void testSomething() { ... } ```  Multiple `@ClusterTest` annotations can be given to generate more than one test invocation for the annotated method.  ```scala @ClusterTests(Array(     @ClusterTest(securityProtocol = ""PLAINTEXT""),     @ClusterTest(securityProtocol = ""SASL_PLAINTEXT"") )) def testSomething(): Unit = { ... } ```  A class-level `@ClusterTestDefaults` annotation is added to provide default values for `@ClusterTest` defined within the class. The intention here is to reduce repetitive annotation declarations and also make changing defaults easier for a class with many test cases.  # Dynamic Configuration  In order to allow for more flexible cluster configuration, a `@ClusterTemplate` annotation is also introduced. This annotation takes a single string value which references a static method on the test class. This method is used to produce any number of test configurations using a fluent builder style API.  ```java @ClusterTemplate(""generateConfigs"") void testSomething() { ... }  static void generateConfigs(ClusterGenerator clusterGenerator) {   clusterGenerator.accept(ClusterConfig.defaultClusterBuilder()       .name(""Generated Test 1"")       .serverProperties(props1)       .ibp(""2.7-IV1"")       .build());   clusterGenerator.accept(ClusterConfig.defaultClusterBuilder()       .name(""Generated Test 2"")       .serverProperties(props2)       .ibp(""2.7-IV2"")       .build());   clusterGenerator.accept(ClusterConfig.defaultClusterBuilder()       .name(""Generated Test 3"")       .serverProperties(props3)       .build()); } ```  This ""escape hatch"" from the simple declarative style configuration makes it easy to dynamically configure clusters.   # JUnit Extension  One thing to note is that our ""test*"" methods are no longer _tests_, but rather they are test templates. We have added a JUnit extension called `ClusterForEach` which knows how to process these annotations in order to generate test invocations. Test classes that wish to make use of these annotations need to explicitly register this extension:  ```scala @ExtendWith(value = Array(classOf[ClusterForEach])) class ApiVersionsRequestTest {    ... } ```  # JUnit Lifecycle  The lifecycle of a test class that is extended with `ClusterForEach` follows:  * JUnit discovers test template methods that are annotated with `@ClusterTest`, `@ClusterTests`, or `@ClusterTemplate` * `ClusterForEach` is called for each of these template methods in order to generate some number of test invocations  For each generated invocation: * Static `@BeforeAll` methods are called * Test class is instantiated * Non-static `@BeforeEach` methods are called * Kafka Cluster is started * Test method is invoked * Kafka Cluster is stopped * Non-static `@AfterEach` methods are called * Static `@AfterAll` methods are called  `@BeforeEach` methods give an opportunity to setup additional test dependencies before the cluster is started. For example, in [SaslApiVersionsRequestTest](https://github.com/apache/kafka/pull/9986/files#diff-ba6b8459d0e1cb7e5672a259b8e58fa066ca3d656f71c79d3da809d6da6c4a33), a MiniKDC is started before Kafka comes up (in order to provide a SASL backend). As seen in the example `@BeforeEach` (and other places) can have parameters which are provided through JUnit dependency injection.  # Dependency Injection  A few classes are introduced to provide context to the underlying cluster and to provide reusable functionality that was previously garnered from the test hierarchy.   * ClusterConfig: a mutable cluster configuration, includes cluster type, number of brokers, properties, etc * ClusterInstance: a shim to the underlying class that actually runs the cluster, provides access to things like SocketServers * IntegrationTestHelper: connection related functions taken from IntegrationTestHarness and BaseRequestTest  In order to have one of these objects injected, simply add it as a parameter to your test class, `@BeforeEach` method, or test method.   | Injection | Class | BeforeEach | Test | Notes | --- | --- | --- | --- | --- | | ClusterConfig | yes | yes | yes* | Once in the test, changing config has no effect | | ClusterInstance | yes* | no | yes | Injectable at class level for convenience, can only be accessed inside test | | IntegrationTestHelper | yes | yes | yes | - |  ```scala @ExtendWith(value = Array(classOf[ClusterForEach])) class SomeTestClass(helper: IntegrationTestHelper) {     @BeforeEach   def setup(config: ClusterConfig): Unit = {     config.serverProperties().put(""foo"", ""bar"")   }    @ClusterTest   def testSomething(cluster: ClusterInstance): Unit = {     val topics = cluster.createAdminClient().listTopics()   } } ```  # Gradle  In order to overcome some compilation issues, the core module test compile phase is now using the Scala compile for both Java and Scala sources. This is because the new annotations and JUnit extension are written in Java, but depend on some Scala classes in ""core"".  ```gradle     test {       java {         srcDirs = []       }       scala {         srcDirs = [""src/generated/java"", ""src/test/java"", ""src/test/scala""]       }     } ```  Since most of the core module test sources are Scala already, this should have little impact on build time.   # KIP-500  As mentioned above, one of the main motivations of this work is to allow for easy reuse of test code between Raft and Zk modes. This will be accomplished using the `clusterType` field on the annotations. For example:  ```scala @ClusterTest(clusterType=Type.Both) def testSomething(): Unit = { ... } ```  would generate two invocations: one for Zk and one for Raft.   Once more of the KIP-500 code is merged into trunk, we will add a Raft-based test invocation context. The prototype version can be seen on the [original prototype PR](https://github.com/confluentinc/kafka/pull/492/files#diff-5517148d95f0c6ac63152b332014d8aefd705b55e1ae153adfe2603bb5cc8361)  # Gotchas * Test methods annotated with JUnit's `@Test` will still be run, but no cluster will be started and no dependency injection will happen. This is generally not what you want * Even though ClusterConfig is accessible and mutable inside the test method, changing it will have no affect on the cluster","closed","kip-500,","mumrah","2021-01-27T21:56:38Z","2021-02-09T16:49:34Z"
"","9726","KAFKA-10833: Expose task configurations in Connect REST API (KIP-661)","This PR adds a new REST endpoint to Connect: `GET /{connector}/tasks-config`, that returns the configuration of all tasks for the connector.  This changes is for [KIP-661](https://cwiki.apache.org/confluence/display/KAFKA/KIP-661%3A+Expose+task+configurations+in+Connect+REST+API)  Co-authored-by: Mickael Maison  Co-authored-by: Oliver Dineen     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mimaison","2020-12-10T12:24:14Z","2021-02-05T09:34:57Z"
"","10194","KAFKA-12365; Disable APIs not supported by KIP-500 broker/controller","This patch updates request `listeners` tags to be in line with what the KIP-500 broker/controller support today. We will re-enable these APIs as needed once we have added the support.  I have also updated `ControllerApis` to use `ApiVersionManager` and simplified the envelope handling logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-02-23T18:26:15Z","2021-08-11T13:15:01Z"
"","10376","MINOR: Remove duplicate `createKafkaMetricsContext`","This patch removes the duplicated `createKafkaMetricsContext` from `KafkaBroker`. It is already present in `Server`. Also added some small style cleanups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-23T00:46:48Z","2021-03-23T19:44:43Z"
"","9666","MINOR: Remove broken `.travis.yml` with system test hooks","This patch removes `.travis.yml` which has never worked as far as I know, and is lately cluttering up PR requests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-01T18:06:48Z","2020-12-02T17:11:10Z"
"","10234","MINOR; Clean up LeaderAndIsrResponse construction in `ReplicaManager#becomeLeaderOrFollower`","This patch refactors the code, which constructs the `LeaderAndIsrResponse` in `ReplicaManager#becomeLeaderOrFollower`, to improve the readability and to remove unnecessary operations.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-01T13:56:25Z","2021-03-04T09:31:39Z"
"","9912","MINOR: Move `RequestChannel.Response` creation logic into `RequestChannel`","This patch moves some common response creation logic from `RequestHandlerHelper` and into `RequestChannel`. This refactor has the following benefits:  - It allows us to get rid of some logic that was previously duplicated in both `RequestHandlerHelper` and `TestRaftRequestHandler`.  - It ensures that we do not need to rely on the caller to ensure that `updateErrorMetrics` gets called since this is handled internally in `RequestChannel`. - It provides better encapsulation of the quota workflow which relies on custom `Response` objects. Previously it was quite confusing for `KafkaApis` to handle this directly through the `sendResponse` API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-15T21:06:08Z","2021-02-23T04:34:26Z"
"","10206","KAFKA-12369; Implement `ListTransactions` API","This patch implements the `ListTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This is only the server-side implementation and does not contain the `Admin` API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-25T02:59:33Z","2021-03-02T21:01:35Z"
"","10183","KAFKA-12267; Implement `DescribeTransactions` API","This patch implements the `DescribeTransactions` API as documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-23T00:53:59Z","2021-02-25T03:35:08Z"
"","10252","KAFKA-12403; Ensure local state deleted on `RemoveTopicRecord` received","This patch implements additional handling logic for `RemoveTopic` records:  - Update `MetadataPartitions` to ensure addition of deleted partitions to `localRemoved` set - Ensure topic configs are removed from `ConfigRepository` - Propagate deleted partitions to `GroupCoordinator` so that corresponding offset commits can be removed  This patch also changes the controller topic id generation logic to use `Uuid.randomUuid` rather than `Random`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-03T04:55:46Z","2021-08-11T13:15:02Z"
"","9577","KAFKA-9837: KIP-589 new RPC for notifying controller log dir failure","This patch implements [KIP-589](https://cwiki.apache.org/confluence/display/KAFKA/KIP-589+Add+API+to+update+Replica+state+in+Controller), which introduces an asynchronous API for brokers to notifying the controller of log dir failure.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* 1. Unit test for LogDirEventManagerImpl 2. Integration test for new behavior  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2020-11-09T10:50:50Z","2022-03-09T12:50:52Z"
"","9630","KAFKA-10739; Replace EpochEndOffset with automated protocol","This patch follows up https://github.com/apache/kafka/pull/9547. It refactors KafkaApis, ReplicaManager and Partition to use `OffsetForLeaderPartitionResult` instead of `EpochEndOffset`. In the mean time, it removes `OffsetsForLeaderEpochRequest#epochsByTopicPartition` and `OffsetsForLeaderEpochResponse#responses` and replaces their usages to use the automated protocol directly. Finally, it removes old constructors in `OffsetsForLeaderEpochResponse`. The patch relies on existing tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-11-20T15:38:38Z","2020-12-03T17:50:35Z"
"","9689","KAFKA-10740; Replace OffsetsForLeaderEpochRequest.PartitionData with automated protocol","This patch follows up https://github.com/apache/kafka/pull/9547. It refactors AbstractFetcherThread and its descendants to use `OffsetForLeaderEpochRequestData.OffsetForLeaderPartition` instead of `OffsetsForLeaderEpochRequest.PartitionData`. The patch relies on existing tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-12-04T09:33:34Z","2021-01-22T15:46:55Z"
"","10200","KAFKA-12367; Ensure partition epoch is propagated to `Partition` state","This patch fixes two problem with the `AlterIsr` handling of the quorum controller:  - Ensure that partition epoch is updated correctly after partition change records and is propagated to `Partition` - Ensure that `AlterIsr` response includes partitions that were successfully updated  As part of this patch, I've renamed `BrokersToIsrs.TopicPartition` to `BrokersToIsrs.TopicIdPartition` to avoid confusion with the `TopicPartition` object which is used virtually everywhere. I've attempted to address some of the testing gaps as welll.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-24T01:48:44Z","2021-02-25T22:17:19Z"
"","10168","MINOR: Remove redundant log close in `KafkaRaftClient`","This patch fixes a small shutdown bug. Current logic closes the log twice: once in `KafkaRaftClient`, and once in `RaftManager`. This can lead to errors like the following:  ``` [2021-02-18 18:35:12,643] WARN  (kafka.utils.CoreUtils$) java.nio.channels.ClosedChannelException         at java.base/sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:150)         at java.base/sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:452)         at org.apache.kafka.common.record.FileRecords.flush(FileRecords.java:197)         at org.apache.kafka.common.record.FileRecords.close(FileRecords.java:204)         at kafka.log.LogSegment.$anonfun$close$4(LogSegment.scala:592)         at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:68)         at kafka.log.LogSegment.close(LogSegment.scala:592)         at kafka.log.Log.$anonfun$close$4(Log.scala:1038)         at kafka.log.Log.$anonfun$close$4$adapted(Log.scala:1038)         at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)         at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)         at scala.collection.AbstractIterable.foreach(Iterable.scala:919)         at kafka.log.Log.$anonfun$close$3(Log.scala:1038)         at kafka.log.Log.close(Log.scala:2433)         at kafka.raft.KafkaMetadataLog.close(KafkaMetadataLog.scala:295)         at kafka.raft.KafkaRaftManager.shutdown(RaftManager.scala:150) ```  I have tended to view `RaftManager` as owning the lifecycle of the log, so I removed the extra call to close in `KafkaRaftClient`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-02-20T18:10:00Z","2021-08-11T13:15:01Z"
"","9633","KAFKA-10706; Ensure leader epoch cache is cleaned after truncation to end offset","This patch fixes a liveness bug which prevents follower truncation from completing after a leader election. If there are consecutive leader elections without writing any data entries, then the leader and follower may have conflicting epoch entries at the end of the log. The JIRA explains a specific scenario in more detail: https://issues.apache.org/jira/browse/KAFKA-10706.  The problem is the shortcut return in `Log.truncateTo` when the truncation offset is larger than or equal to the end offset, which prevents the conflicting entries from being resolved. Here we change this case to ensure `LeaderEpochFileCache.truncateFromEnd` is still called.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-11-20T22:17:12Z","2020-11-21T17:25:55Z"
"","10181","MINOR: fix a couple of failing system tests","This patch fixes a couple of failing system tests due to https://github.com/apache/kafka/pull/10105/ and should be merged to both `trunk` and `2.8`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-22T22:52:12Z","2021-02-22T23:10:55Z"
"","9617","MINOR: Factor out common response parsing logic","This patch factors out some common parsing logic from `NetworkClient.parseResponse` and `AbstractResponse.parseResponse`. As a result of this refactor, we are now verifying the correlationId in forwarded requests. This patch also adds a test case to verify handling in this case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-11-18T20:50:08Z","2020-11-20T03:00:53Z"
"","10015","MINOR: Factor out controller node provider `BrokerToControllerChannelManager`","This patch factors out a trait to allow for other ways to provide the controller `Node` object to `BrokerToControllerChannelManager`. In KIP-500, the controller will be provided from the Raft client and not the metadata cache.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-02-01T20:11:18Z","2021-02-01T22:30:54Z"
"","9839","MINOR: Factor `RaftManager` out of `TestRaftServer`","This patch factors out a `RaftManager` class from `TestRaftServer` which will be needed when we integrate this layer into the server. This class encapsulates the logic to build `KafkaRaftClient` as well as its IO thread.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-07T02:12:24Z","2021-08-11T13:16:05Z"
"","9563","KAFKA-10684; Avoid additional envelope copies during network transmission","This patch creates a new `SendBuilder` class which allows us to avoid copying ""bytes"" types when transmitting an api message over the network. This is used in `EnvelopeRequest` and `EnvelopeResponse` to avoid copying the embedded data.  The patch also contains a few minor cleanups such as moving envelope parsing logic into `RequestContext`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-11-05T05:42:02Z","2020-11-14T21:16:31Z"
"","9967","KAFKA-12236; New meta.properties logic for KIP-500","This patch contains the new handling of `meta.properties` required by the KIP-500 server as specified in KIP-631. When using the self-managed quorum, the `meta.properties` file is required in each log directory with the new `version` property set to 1. It must include the `cluster.id` property and it must have a `node` matching that in the configuration.  The behavior of `meta.properties` for the Zookeeper-based `KafkaServer` does not change. We treat `meta.properties` as optional and as if it were `version=0`. We continue to generate the clusterId and/or the brokerId through Zookeeper as needed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-25T19:33:40Z","2021-01-31T01:05:31Z"
"","9732","KAFKA-10842; Use `InterBrokerSendThread` for raft's outbound network channel","This patch contains the following improvements:  - Separate inbound/outbound request flows so that we can open the door for concurrent inbound request handling - Rewrite `KafkaNetworkChannel` to use `InterBrokerSendThread` which fixes a number of bugs/shortcomings - Get rid of a lot of boilerplate conversions in `KafkaNetworkChannel`  - Improve validation of inbound responses in `KafkaRaftClient` by checking correlationId. This fixes a bug which could cause an out of order Fetch to be applied incorrectly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2020-12-11T05:45:43Z","2021-08-11T13:17:10Z"
"","10323","KAFKA-12459; Use property testing library for raft event simulation tests","This patch changes the raft simulation tests to use jqwik, which is a property testing library. This provides two main benefits:  - It simplifies the randomization of test parameters. Currently the tests use a fixed set of `Random` seeds, which means that most builds are doing redundant work. We get a bigger benefit from allowing each build to test different parameterizations. - It makes it easier to reproduce failures. Whenever a test fails, jqwik will report the random seed that failed. A developer can then modify the `@Property` annotation to use that specific seed in order to reproduce the failure.  Note that I have resisted making logical changes to the tests themselves. The only difference is the way the parameters are specified.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-15T23:11:01Z","2021-08-11T13:14:02Z"
"","10109","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-11T07:09:04Z","2021-02-11T09:53:35Z"
"","10108","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-11T07:09:02Z","2021-02-11T15:07:07Z"
"","10067","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-05T05:33:22Z","2021-02-10T09:43:17Z"
"","10065","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-05T02:38:43Z","2021-02-10T09:37:58Z"
"","10064","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-05T02:16:48Z","2021-02-10T09:33:34Z"
"","10061","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  Reviewers: Mickael Maison , Satish Duggana , David Jacot   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-02-04T23:43:16Z","2021-02-05T15:21:02Z"
"","9902","KAFKA-12193: Re-resolve IPs after a client disconnects","This patch changes the NetworkClient behavior to resolve the target node's hostname after disconnecting from an established connection, rather than waiting until the previously-resolved addresses are exhausted. This is to handle the scenario when the node's IP addresses have changed during the lifetime of the connection, and means that the client does not have to try to connect to invalid IP addresses until it has tried each address.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2021-01-15T00:39:52Z","2021-12-08T06:50:49Z"
"","9501","MINOR: move the test cases which don't need brokers from TopicCommand…","This patch can reduce the elapsed time of testing ```TopicCommandWithAdminClientTest``` (5m19s -> 4m18s on my local)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-26T07:37:12Z","2020-11-18T08:31:47Z"
"","9883","MINOR: Generalize server startup to make way for KIP-500","This patch attempts to generalize server initialization for KIP-500. It adds a `Server` trait which `KafkaServer` extends for the legacy Zookeeper server, and a new `KafkaRaftServer` for the new server. I have also added stubs for `KafkaRaftBroker` and `KafkaRaftController` to give a clearer idea how this will be used.  This patch attempts to be non-intrusive, but it's worth pointing out that it does away with `KafkaServerStartable`. I believe this was intended to support custom startup logic, but it is not an official API and we do not plan to support it after KIP-500.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-14T03:23:36Z","2021-01-16T01:30:30Z"
"","10275","KAFKA-12434; Admin support for `DescribeProducers` API","This patch adds the new `Admin` API to describe producer state as described by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.  The three new APIs added by KIP-664 require different lookup and request patterns:  - DescribeProducers: send to partition leaders - DescribeTransactions: send to coordinators - ListTransactions: send to all brokers  Our method of handling complex workflows such as these in `KafkaAdminClient` by chaining together `Call` instances has been clumsy and error-prone at best. I have attempted to introduce a new pattern which separates the lookup stage (e.g. finding partition leaders) from the fulfillment stage (e.g. sending `DescribeProducers`). The lookup stage is implemented by `AdminApiLookupStrategy` and the fulfillment stage is implemented by `AdminApiHandler`. There is a new class `AdminApiDriver` which manages the bookkeeping for these two stages. See the corresponding javadocs for more detail.   This PR provides an example of usage through `DescribeProducersHandler`, which is an implementation of `AdminApiHandler`. It relies on `PartitionLeaderStrategy` which implements `AdminApiLookupStrategy`. One of the benefits of this approach is that it provides a more convenient way for testing since all of the logic is not crammed into `KafkaAdminClient`.  In the following PRs to implement the other two APIs introduced by KIP-664, I will also provide a coordinator lookup strategy and an ""all broker"" lookup strategy. You can get an idea of the usage from the previously closed PR: https://github.com/apache/kafka/pull/9268.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-08T04:22:57Z","2021-04-13T01:53:16Z"
"","9827","MINOR; Add producer id in exceptions thrown by ProducerStateManager","This patch adds producer id in exceptions thrown by the `ProducerStateManager`. It was already there in some cases but was not in others. This is really helpful to have when troubleshooting. Otherwise, it is not possible to correlate broker's logs with producer's logs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-05T17:08:15Z","2021-01-08T08:46:34Z"
"","9828","MINOR: Missing entity type tags","This patch adds a few missing ""entity type"" tags to the schema definitions. These aren't really used for anything at the moment, but it's useful to keep them up to date. We had planned to use them perhaps for generating documentation for example.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-05T19:05:32Z","2021-01-06T00:34:02Z"
"","9998","KAFKA-12250; Add metadata record serde for KIP-631","This patch adds a `RecordSerde` implementation for the metadata record format expected by KIP-631.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-29T05:58:58Z","2021-08-11T13:15:01Z"
"","9588","KAFKA-10699: Add system test coverage for group coordinator migration","This newly added system test is to verify that with the fix in #9270 , the `member.id` update caused by static member rejoin would be persisted correctly. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","feyman2016","2020-11-12T08:53:08Z","2020-11-13T03:36:28Z"
"","9910","KAFKA-10877","This moves the Logging trait into companion objects so the loggers are only instantiated once.  Classes import the methods from their companion objects.  Classes that were using inheritance to gain access to logging methods are now accessing them though importing their super class' companion object.  In this way any log4j(2) configuration that were configuring the super class logger should still work.  Since classes no longer inherit from Logging they can't change the logIdent string through inheritance.  This PR introduces a new class LogIndent which is used to wrap String so it can be passed as an implicit variable to calls to info(), debug(), etc...  So none of the actual log statements need to change.  CoreUtils.swallow is changed so that it accepts the LogIdent objects.  It seems like CoreUtils.swallow never used to logger that was passed to it only it's own logger which seems like bug.  Calls to swallow need pass the companion object and not this.  There are 10 spot bugs problems flagged.  But I'm not able to figure out what spotbugs is complaining about.","open","","smccauliff","2021-01-15T17:51:45Z","2021-01-20T00:52:15Z"
"","9489","MINOR: demote ""Committing task offsets"" log to DEBUG","This message absolutely floods the logs, especially in an eos application where the commit interval is just 100ms. It's definitely a useful message but I don't think there's any justification for it being at the INFO level when it's logged 10 times a second. ","closed","streams,","ableegoldman","2020-10-24T00:02:47Z","2021-01-09T01:08:22Z"
"","10429","KAFKA-10847: Add new RocksDBPrefixRangeIterator class","This iterator is similar to `RocksDBRangeIterator` except that compares key prefixes during each iteration instead of the full record key. This iterator will be used by the time-ordered window store from the PR https://github.com/apache/kafka/pull/10331. This iterator is more efficient when doing range queries.  The `RocksDBPrefixRangeIterator` uses a new bytes lexico comparator that compares two bytes by their prefixes only. i.e. ``` comparator.compare(0001, 0001000F); // smallest key prefix is 4 bytes, so 0001 == 0001 comparator.compare(0001000F, 0001); // smallest key prefix is 4 bytes, so 0001 == 0001 comparator.compare(0002000F, 0001); // smallest key prefix is 4 bytes, so 0002 > 0001 comparator.compare(0001000F, 0002); // smallest key prefix is 4 bytes, so 0001 < 0002 ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","spena","2021-03-29T19:16:09Z","2021-04-09T21:41:35Z"
"","9576","KAFKA-10685: strictly parsing the date/time format","This issue doesn't happen in java 8, but happen in java 11(not tested in java 15). In java 8, it'll throw parse Exception, but in java 11, it'll parse microseconds as milliseconds, ex: 001111 microseconds will be parsed as 1 second and 111 milliseconds.   This fix is to Strictly parse the date/time format by setLenient(false). So it won't allow un-matched date/time format input to avoid the wrong parsing for microseconds/nanoseconds. It'll throw parse Exception for any version s of java and be consistent.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-11-09T10:21:55Z","2020-11-17T19:33:35Z"
"","10307","MINOR: Log project, gradle, java and scala versions at the start of the build","This is useful when debugging build issues. I also removed two printlns that are now redundant, so this makes the build more informative and less noisy at the same time. Example output:  > Starting build with version 3.0.0-SNAPSHOT using Gradle 6.8.3, Java 15 and Scala 2.13.5  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-12T15:45:55Z","2021-03-12T21:03:38Z"
"","9915","[KAKFA-8522] Streamline tombstone and transaction marker removal","This is the rebased PR for #7884.   We aim to remove tombstones that persist indefinitely due to low throughput in this issue.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ConcurrencyPractitioner","2021-01-15T22:03:02Z","2021-06-30T17:45:38Z"
"","9841","MINOR: Add a log to print acl change notification details","This is similar to config change logs for topic/broker. This will be useful for debugging any acl related issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-01-07T07:52:32Z","2021-01-07T11:02:34Z"
"","10308","MINOR: Update year in NOTICE","This is required to run a release as it is checked in release.py  This needs to be applied to 2.8, 2.7 too. Sophie already made the change in 2.6  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-03-12T16:50:33Z","2021-03-15T12:18:35Z"
"","9567","MINOR: Always return partitions with diverging epochs in fetch response","This is required to ensure that followers can truncate based on diverging epochs returned in fetch responses.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-11-05T22:50:52Z","2020-11-06T08:51:15Z"
"","9607","[KAFKA-10722] doc: Described the types of the stores used","This is related to KAFKA-10722  Sometimes it's important to know the correct type of the store used by streams. E.g. when iterating over its items.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","fml2","2020-11-17T21:48:47Z","2020-12-01T10:38:10Z"
"","10441","KAFKA-12568, Remove deprecated ""KStream#groupBy/join"", ""Joined#named""  overloads","This is part of the work tracked in [KAFKA-12419](https://issues.apache.org/jira/browse/KAFKA-12419) as part of pre-3.0 cleanup. Verified by running tests locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","swist","2021-03-30T13:44:19Z","2021-03-30T13:46:13Z"
"","9981","MINOR: Upgrade to Scala 2.12.13","this is follow-up of #9643  ### Scala 2.12.13 release page  https://github.com/scala/scala/releases/tag/v2.12.13  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-27T08:10:04Z","2021-01-28T01:02:40Z"
"","9661","KAFKA-10776: update the doc to add version attribute in RequstsPerSec metrics","This is actually a documentation miss, the version attribute is required to match the Kafka version. Thanks.  See https://cwiki.apache.org/confluence/display/KAFKA/KIP-272%3A+Add+API+version+tag+to+broker%27s+RequestsPerSec+metric   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-11-30T02:43:11Z","2020-12-15T14:39:08Z"
"","10219","Minor: Decode metadata records in DumpLogSegments","This is a upstream port of https://github.com/confluentinc/kafka/pull/440  Adds new `--cluster-metadata-decoder` flag to the dump log segment tool. This will pretty print a JSON object with the metadata record type along with the data.   Also adds `--no-log-metadata` which will omit log metadata when printing the log records  The changes to the MessageGenerator and the new MetadataJsonConvertersGenerator have already been merged.","closed","","mumrah","2021-02-26T15:59:51Z","2021-02-26T16:29:29Z"
"","9602","MINOR: Use string interpolation in FinalizedFeatureCache","This is a small change. In this PR, I'm using string interpolation in `FinalizedFeatureCache` at places where string format was otherwise used. This just ensures uniformity, with this change we ensure that throughout the file we just use string interpolation.  **Test plan:**  Rely on existing tests since this PR is not changing behavior.","closed","","kowshik","2020-11-17T03:55:28Z","2020-11-17T21:32:51Z"
"","10110","KAFKA-12297 - Make MockProducer return RecordMetadata with values as per contract","This is a simple change to MockProducer as per request in KAFKA-12297. MockProducer currently returns a null RecordMetadata on Exception. The fix will make MockProducer return the right value as per specification.  This only impacts clients which use send with a custom callback and try to then use the RecordMetadata inspite of getting an exception. This should mostly impact customer unit and integration tests as the mock end point was never intended for use in a real Kafka cluster.","closed","","aadubey","2021-02-11T18:02:46Z","2021-02-13T17:48:38Z"
"","9561","KAFKA-10624: For FeatureZNodeStatus, use sealed trait instead of Enumeration","This is a follow-up to initial KIP-584 development. In this PR, I've switched the `FeatureZNodeStatus` enum to be a sealed trait. In Scala, we prefer sealed traits over Enumeration since the former gives you exhaustiveness checking. With Scala enumeration, you don't get a warning if you add a new value that is not handled in a given pattern match.  **Test plan:** Rely on existing tests.","closed","","kowshik","2020-11-05T02:41:00Z","2020-11-05T23:55:14Z"
"","9958","MINOR: Clean up group instance id handling in `GroupCoordinator`","This is a continuation of a refactor started in #9952. The logic in `GroupCoordinator` is loose and inconsistent in the handling of the `groupInstanceId`. In some cases, such as in the JoinGroup hander, we verify that the groupInstanceId from the request is mapped to the memberId precisely. In other cases, such as Heartbeat, we check the mapping, but only to validate fencing. The patch consolidates the member validation so that all handlers follow the same logic.   A second problem is that many of the APIs where a `groupInstanceId` is expected use optional arguments. For example: ```scala def hasStaticMember(groupInstanceId: Option[String]): Boolean  def addStaticMember(groupInstanceId: Option[String], newMemberId: String): Unit ``` If `groupInstanceId` is `None`, then `hasStaticMember` is guaranteed to return `false` while `addStaticMember` raises an `IllegalStateException`. So the APIs suggest a generality which is not supported and does not make sense.  Finally,  the patch attempts to introduce stronger internal  invariants inside `GroupMetadata`. Currently it is possible for an inconsistent `groupInstanceId` to `memberId` mapping to exist because we expose separate APIs to modify `members` and `staticMembers`. We rely on the caller to ensure this doesn't happen.  Similarly, it is possible for a member to be in the `pendingMembers` set as well as the stable `members` map. The patch fixes this by consolidating the paths to addition and removal from these collections and adding assertions to ensure that invariants are maintained.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-24T23:47:58Z","2021-02-26T19:26:44Z"
"","10088","Backport Jenkinsfile fix and remove Travis build for 2.4","This is a backport of #9453 which changes the Jenkinsfile over to the new new tool names used by Apache's Jenkins.","closed","","mumrah","2021-02-09T14:25:16Z","2021-02-09T17:56:43Z"
"","10087","Backport Jenkinsfile fix and remove Travis build for 2.5","This is a backport of #9453 which changes the Jenkinsfile over to the new new tool names used by Apache's Jenkins.","closed","","mumrah","2021-02-09T14:23:22Z","2021-02-09T17:57:01Z"
"","10169","MINOR: Update Scala to 2.13.5","This includes a fix from Chia-Ping that removes tuple allocations when `Map.forKeyValue` is used (https://github.com/scala/scala/pull/9425) and support for JDK 16.  Release notes: https://github.com/scala/scala/releases/tag/v2.13.5  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-20T19:22:48Z","2021-02-23T06:28:24Z"
"","9644","KAFKA-10565: Only print console producer prompt with a tty","This fixes https://issues.apache.org/jira/browse/KAFKA-10565 by using `System.console` to determine whether we're connected to a TTY before printing the `kafka-console-producer.sh` prompt. That's not a perfect solution (since stdin might be connected to the tty but not stdout, or vice versa), but it's still better that printing many prompts when reading from a pipe.","closed","","tombentley","2020-11-23T15:50:31Z","2020-11-26T03:57:40Z"
"","9722","MINOR: add missing quickstart.html file","This file was accidentally deleted via #9551 -- I am actually not sure if this PR restores the right version as `quickstart.html` seems to have been replaced with `quickstart-zookeeper.html` and their content is different.  Cf. https://github.com/apache/kafka/pull/9721 and https://github.com/apache/kafka-site/pull/313  Must be cherry-picked to `2.7`.  Call for review @guozhangwang @bbejeck","closed","","mjsax","2020-12-10T01:05:26Z","2020-12-23T00:48:04Z"
"","9658","Linking to config parameters","This demos an idea I had for improving the website docs.   The docs often mention config parameters. With recent changes it's now possible to deep link directly to the the doc for a given config parameter. It would be useful for readers if we made use of this in our own docs, so when talking about `listeners` in prose we link to the doc for it.   This PR achieves that in a relatively easy-to-author way using a CSS class on the `` used to wrap some config parameters and some JS which takes the class and the included text and turns it into one or more links. For example `listeners` gets turned into a link to the doc for [`listeners`](https://kafka.apache.org/27/documentation.html#brokerconfigs_listeners), with similar classes for topic, producer consumer etc. configs.  Here a're some screenshots of what the markup changes in the patch result in, first from the upgrade section:  ![Screenshot from 2020-11-27 12-05-57](https://user-images.githubusercontent.com/879487/100448489-467b6e80-30aa-11eb-8794-5db0a7e4bd9f.png)  The script knows to ignore whatever is after the `=` sign. Here's another example from the security section (please excuse the magenta find-in-page highlighting):  ![Screenshot from 2020-11-27 12-06-58](https://user-images.githubusercontent.com/879487/100448544-5c892f00-30aa-11eb-99c2-17a07734bd97.png)  Because the script checks for the existence of the link before generating the link it doesn't linkify the `username` or `password` (the regex isn't powerful enough to properly understand that lines ending with `\` are continuation lines). This is a bit of a hack, but it works well enough.  There's two potential issues I can see with this:  1. Lots of client configs are shared between both producer and consumer (and admin client...), so which should we link to? Probably a link is better than no link, but equally we could chose not to link such configs. 2. This is not really compatible with https://issues.apache.org/jira/browse/KAFKA-2967 though it might be possible to do something similar in ReStructuredText with custom roles.","open","","tombentley","2020-11-27T12:28:29Z","2020-11-27T12:29:22Z"
"","9626","KAFKA-10545: Create topic IDs and propagate to brokers","This change takes the topic IDs created in https://github.com/apache/kafka/pull/9473 and propagates them to brokers using LeaderAndIsr Request. It also removes the topic name from the LeaderAndIsr Response, reorganizes the response to be sorted by topic, and includes the topic ID.  In addition, the topic ID is persisted to each replica in Log as well as in a file on disk. This file is read on startup and if the topic ID exists, it will be reloaded.   This PR bumps the IBP and is expected to be merged at the same time as https://github.com/apache/kafka/pull/9622 as to not bump the protocol twice ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2020-11-19T23:48:46Z","2020-12-18T22:19:51Z"
"","10361","KAFKA-9189: Use MetadataCache instead of ZK during controlled shutdown to avoid hang","This avoids hanging during shutdown if ZK is unavailable. We could change ZK calls to get the controller id and the broker information to have a timeout, but I think this approach is better.  The downside is that the metadata cache may be slightly out of date, but we will retry as per the controlled shutdown configuration. If this broker is partitioned away from the Controller and is not receiving metadata updates, then we want to shutdown asap anyway.  I added a test that timed out without this change and included a couple of clean-ups in `ServerShutdownTest`: * Removed `testCleanShutdownWithDeleteTopicEnabled`, which is redundant since delete topics is enabled by default. * Removed redundant method arguments  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-19T13:52:48Z","2021-03-19T18:20:00Z"
"","10466","KAFKA-12417: streams `copyDependentLibs` should not copy testRuntime configuration jars","This also fixes a Gradle deprecation that unblocks the upgrade to Gradle 7.0.","closed","","dejan2609","2021-04-02T19:26:22Z","2021-04-28T18:31:37Z"
"","9736","Add configurable workloads and E2E latency tracking to Trogdor.","This allows us to run highly granular and configurable workloads to directly simulate customer scenarios and measure the end to end latency all within Trogdor.  This creates a new `ConfigurableProducer` workload that can be used to tune many parts of the workload better than the `ProduceBench` workload.  This also creates all the helper classes for the `ConfigurableProducer` workload.  This adds a new parameter to the `ConsumeBench` workload to allow for processing of records after polling them.  This also adds a new E2E latency test utilizing the new record processor within the `ConsumeBench` workload.  --- # ConfigurableProducer workload  The ConfigurableProducer workload allows for customized and even variable configurations in terms of messages per second, message size, batch size, key size, and even the ability to target a specific partition out of a topic.  The parameters that differ from ProduceBenchSpec:  * `flushGenerator` - Used to instruct the KafkaProducer when to issue flushes.  This allows us to simulate variable batching since batch flushing is not currently exposed within the KafkaProducer class. * `throughputGenerator` - Used to throttle the ConfigurableProducerWorker based on a calculated number of messages within a window. * `activeTopic` - This class only supports execution against a single topic at a time.  If more than one topic is specified, the ConfigurableProducerWorker will throw an error. * `activePartition` - Specify a specific partition number within the activeTopic to run load against, or specify `-1` to allow use of all partitions.  Here is an example spec:  ``` {     ""startMs"": 1606949497662,     ""durationMs"": 3600000,     ""producerNode"": ""trogdor-agent-0"",     ""bootstrapServers"": ""some.example.kafka.server:9091"",     ""flushGenerator"": {         ""type"": ""gaussian"",         ""messagesPerFlushAverage"": 16,         ""messagesPerFlushDeviation"": 4     },     ""throughputGenerator"": {         ""type"": ""gaussian"",         ""messagesPerSecondAverage"": 500,         ""messagesPerSecondDeviation"": 50,         ""windowsUntilRateChange"": 100,         ""windowSizeMs"": 100     },     ""keyGenerator"": {         ""type"": ""constant"",         ""size"": 8     },     ""valueGenerator"": {         ""type"": ""gaussianTimestampRandom"",         ""messageSizeAverage"": 512,         ""messageSizeDeviation"": 100,         ""timestampBytes"": 8,         ""messagesUntilSizeChange"": 100     },     ""producerConf"": {         ""acks"": ""all""     },     ""commonClientConf"": {},     ""adminClientConf"": {},     ""activeTopic"": {         ""topic0"": {             ""numPartitions"": 100,             ""replicationFactor"": 3,             ""configs"": {                 ""retention.ms"": ""1800000""             }         }     },     ""activePartition"": 5 } ```  This example spec performed the following:  * Ran on `trogdor-agent-0` for 1 hour starting at 2020-12-02 22:51:37.662 GMT * Produced with acks=all to Partition 5 of `topic0` on kafka server `some.example.kafka.server:9091`. * The average batch had 16 messages, with a standard deviation of 4 messages. * The message had a 8-bit constant key, with an average value of 512 bytes and a standard deviation of 100 bytes. * The messages had millisecond timestamps embedded in the first 8-bytes of the value. * The average throughput was 500 messages/second, with a window of 100ms and a deviation of 50 messages/second.  ---  # ConsumeBench workload changes  This commit adds the ability for the ConsumeBench workloads to optionally process the records returned through the consumer poll call.  This is done by specifying a new `recordProcessor` parameter.  The record processor's status is then included in the workload's status.  ## RecordProcessor  This interface provides the ability to optionally process records after the ConsumeBench workload polls them.  The interface provides for the ability to include additional data in the status output.  Currently there are 2 processing methods:  * Disabled, by not specifying this parameter. * `timestamp` will use `TimestampRecordProcessor` to process records containing a timestamp in the first several bytes of the message.  ### TimestampRecordProcessor  This includes a `TimestampRecordProcessor` class to process records containing a timestamp in the first several bytes of the message.   This class will process records containing timestamps and generate a histogram based on the data.  It will then be present in the status from the `ConsumeBenchWorker` class.  This must be used with a timestamped PayloadGenerator implementation.  Here's an example spec:  ``` {    ""type"": ""timestampRandom"",    ""timestampBytes"": 8,    ""histogramMaxMs"": 10000,    ""histogramMinMs"": 0,    ""histogramStepMs"": 1 } ```  This will track total E2E latency up to 10 seconds, using 1ms resolution and a timestamp size of 8 bytes.  ---  # FlushGenerator  A FlushGenerator is used to facilitate flushing the KafkaProducers on a cadence specified by the user.  This is useful to simulate a specific number of messages in a batch regardless of the message size, since batch flushing is not exposed in the KafkaProducer.  Currently there are 3 flushing methods:  * Disabled, by not specifying this parameter. * `constant` will use `ConstantFlushGenerator` to keep the number of messages per batch constant. * `gaussian` will use `GaussianFlushGenerator` to vary the number of messages per batch on a normal distribution.  ## ConstantFlushGenerator  This generator will flush the producer after a specific number of messages.  This does not directly control when KafkaProducer will batch, this only makes best effort.  This also cannot tell when a KafkaProducer batch is closed.  If the KafkaProducer sends a batch before this executes, this will continue to execute on its own cadence.  Here is an example spec:  ``` {    ""type"": ""constant"",    ""messagesPerFlush"": 16 } ```  This example will flush the producer every 16 messages.  ## GaussianFlushGenerator  This generator will flush the producer after a specific number of messages, determined by a gaussian distribution.  This does not directly control when KafkaProducer will batch, this only makes best effort.  This also cannot tell when a KafkaProducer batch is closed.  If the KafkaProducer sends a batch before this executes, this will continue to execute on its own cadence.  Here is an example spec:  ``` {    ""type"": ""gaussian"",    ""messagesPerFlushAverage"": 16,    ""messagesPerFlushDeviation"": 4 } ```  This example will flush the producer on average every 16 messages, assuming `linger.ms` and `batch.size` allow for it.  That average changes based on a normal distribution after each flush:  * An average of the flushes will be at 16 messages. * ~68% of the flushes are at between 12 and 20 messages. * ~95% of the flushes are at between 8 and 24 messages. * ~99% of the flushes are at between 4 and 28 messages.  ---  # ThroughputGenerator  Similar to the throttle class, except a simpler design.  This interface is used to facilitate running a configurable number of messages per second by throttling if the throughput goes above a certain amount.  Currently there are 2 throughput methods:  * `constant` will use `ConstantThroughputGenerator` to keep the number of messages per second constant. * `gaussian` will use `GaussianThroughputGenerator` to vary the number of messages per second on a normal distribution.  ## ConstantThroughputGenerator  This throughput generator configures constant throughput.  The lower the window size, the smoother the traffic will be. Using a 100ms window offers no noticeable spikes in traffic while still being long enough to avoid too much overhead.  Due to binary nature of throughput in terms of messages sent in a window, each window will send at least 1 message, and each window sends the same number of messages, rounded down. For example, 99 messages per second with a 100ms window will only send 90 messages per second, or 9 messages per window. Another example, in order to send only 5 messages per second, a window size of 200ms is required. In cases like these, both the `messagesPerSecond` and `windowSizeMs` parameters should be adjusted together to achieve more accurate throughput.  Here is an example spec:  ``` {    ""type"": ""constant"",    ""messagesPerSecond"": 500,    ""windowSizeMs"": 100 } ```  This will produce a workload that runs 500 messages per second, with a maximum resolution of 50 messages per 100 millisecond.  ## GaussianThroughputGenerator  This throughput generator configures throughput with a gaussian normal distribution on a per-window basis. You can specify how many windows to keep the throughput at the rate before changing. All traffic will follow a gaussian distribution centered around `messagesPerSecondAverage` with a deviation of `messagesPerSecondDeviation`.  The lower the window size, the smoother the traffic will be. Using a 100ms window offers no noticeable spikes in traffic while still being long enough to avoid too much overhead.  Due to binary nature of throughput in terms of messages sent in a window, this does not work well for an average throughput of less than 5 messages per window.  In cases where you want lower throughput, the `windowSizeMs` must be adjusted accordingly.  Here is an example spec:  ``` {    ""type"": ""gaussian"",    ""messagesPerSecondAverage"": 500,    ""messagesPerSecondDeviation"": 50,    ""windowsUntilRateChange"": 100,    ""windowSizeMs"": 100 } ```  This will produce a workload that runs on average 500 messages per second, however that speed will change every 10 seconds due to the `windowSizeMs * windowsUntilRateChange` parameters. The throughput will have the following normal distribution:  * An average of the throughput windows of 500 messages per second. * ~68% of the throughput windows are between 450 and 550 messages per second. * ~95% of the throughput windows are between 400 and 600 messages per second. * ~99% of the throughput windows are between 350 and 650 messages per second.  ---  # Additional Payload Generators  This implementation also offers additional payload generators to facilitate the tests these workloads are designed to run.  These are also compatible with the existing ProduceBench workloads.  ## TimestampRandomPayloadGenerator  This generator generates timestamped pseudo-random payloads that can be reproduced from run to run.  The guarantees are the same as those of java.util.Random.  The timestamp used for this class is in milliseconds since epoch, encoded directly to the first several bytes of the payload. This should be used in conjunction with TimestampRecordProcessor in the Consumer to measure true end-to-end latency of a system.  * `size` - The size in bytes of each message. * `timestampBytes` - The amount of bytes to use for the timestamp.  Usually 8. * `seed` - Used to initialize Random() to remove some non-determinism.  Here is an example spec:  ``` {    ""type"": ""timestampRandom"",    ""size"": 512,    ""timestampBytes"": 8 } ```  This will generate a 512-byte random message with the first 8 bytes encoded with the timestamp.  ## GaussianTimestampRandomPayloadGenerator  This class behaves identically to TimestampRandomPayloadGenerator, except the message size follows a gaussian distribution.  This should be used in conjunction with TimestampRecordProcessor in the Consumer to measure true end-to-end latency of a system.  * `messageSizeAverage` - The average size in bytes of each message. * `messageSizeDeviation` - The standard deviation to use when calculating message size. * `timestampBytes` - The amount of bytes to use for the timestamp.  Usually 8. * `messagesUntilSizeChange` - The number of messages to keep at the same size. * `seed` - Used to initialize Random() to remove some non-determinism.  Here is an example spec:  ``` {    ""type"": ""gaussianTimestampRandom"",    ""messageSizeAverage"": 512,    ""messageSizeDeviation"": 100,    ""timestampBytes"": 8,    ""messagesUntilSizeChange"": 100 } ```  This will generate messages on a gaussian distribution with an average size each 512-bytes and the first 8 bytes encoded with the timestamp.  The message sizes will have a standard deviation of 100 bytes, and the size will only change every 100 messages.  The distribution of messages will be as follows:  * The average size of the messages are 512 bytes. * ~68% of the messages are between 412 and 612 bytes * ~95% of the messages are between 312 and 712 bytes * ~99% of the messages are between 212 and 812 bytes  ---  # Testing  ## New Functionality  The ConfigurableProducer workload was tested by running various scenarios and verifying the metrics within the Kafka cluster matched the scenario as defined.  ## Existing Functionality  The ConsumeBench workload was tested without specifying the `recordProcessor` parameter to verify it still behaves as it did prior to this patch set.  All other code paths are in a new workload.","closed","","scott-hendricks","2020-12-11T19:53:16Z","2020-12-21T16:33:51Z"
"","10069","MINOR: Add RaftReplicaManager","This adds the logic to apply partition metadata when consuming from the Raft-based metadata log.  `RaftReplicaManager` extends `ReplicaManager` for now to minimize changes to existing code for the 2.8 release.  We will likely adjust this hierarchy at a later time (e.g. introducing a trait and adding a helper to refactor common code).  For now, we expose the necessary fields and methods in `ReplicaManager` by changing their scope from `private` to `protected`, and we refactor out a couple of pieces of logic that are shared between the two implementation (stopping replicas and adding log dir fetchers).  Existing tests are sufficient to expose regressions in the current `ReplicaManager`.  We intend to exercise the new `RaftReplicaManager` code via system tests and unit/integration tests (both to come in later PRs).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-05T16:36:13Z","2021-02-10T22:37:17Z"
"","10090","Add BrokerMetadataListener and related classes","This adds BrokerMetadataListener which is responsible for processing metadata records received by the broker when running in Raft mode.  Note that several files in this PR are also included in https://github.com/apache/kafka/pull/10106 which will get merged to trunk first.","closed","kip-500,","mumrah","2021-02-09T20:07:21Z","2021-02-11T20:05:44Z"
"","9983","KAFKA-8930: MirrorMaker v2 documentation (#324)","This adds a new user-facing documentation ""Geo-replication (Cross-Cluster Data Mirroring)"" section to the Kafka Operations documentation that covers MirrorMaker v2.  Was already merged to `kafka-site` via https://github.com/apache/kafka-site/pull/324.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","miguno","2021-01-27T16:43:11Z","2021-02-09T10:54:12Z"
"","10450","KAFKA-12590: Remove deprecated kafka.security.auth.Authorizer, SimpleAclAuthorizer and related classes in 3.0","These were deprecated in Apache Kafka 2.4 (released in December 2019) to be replaced by `org.apache.kafka.server.authorizer.Authorizer` and `AclAuthorizer`.  As part of KIP-500, we will implement a new `Authorizer` implementation that relies on a topic (potentially a KRaft topic) instead of `ZooKeeper`, so we should take the chance to remove related tech debt in 3.0.  Details on the issues affecting the old Authorizer interface can be found in the KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-504+-+Add+new+Java+Authorizer+Interface  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-31T13:56:08Z","2021-04-03T15:23:29Z"
"","10410","KAFKA-12561: don't set timeout for future.get","These tests failed quite often, because of `TimeoutException`. We cannot get the future result in time. I think the flaky tests tell us that we should not expect the results will return as soon as we expected in jenkins. Like other tests, we don't usually set timeout when future.get, so remove the timeout setting to make this test reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-26T03:59:44Z","2021-03-31T06:03:01Z"
"","9549","KIP-145: Add SMTs, HeaderFrom, DropHeaders and InsertHeader","These SMTs were originally specified in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect#KIP145ExposeRecordHeadersinKafkaConnect-Transformations) but never implemented at the time.  `HeaderTo` is not included since its original specification doesn't deal with the fact that there can be >1 header with the same name, but a field can only have a single value (while you could use an array, that doesn't work if the headers for the given name had different schemas).","closed","","tombentley","2020-11-03T16:09:55Z","2021-04-16T14:11:31Z"
"","9599","MINOR: Include connector name in error message","These log messages aren't triggered very frequently, but when they are it can indicate a serious problem with the connector, and it'd be nice to know exactly which connector is having that problem without having to dig through other log messages and try to correlate the stack trace here with existing connector configs.","closed","connect,","C0urante","2020-11-16T13:51:15Z","2020-11-18T15:35:22Z"
"","10153","KAFKA-12340: Fix potential resource leak in Kafka*BackingStore","These Kafka*BackingStore classes used in Connect have a recently-added deprecated constructor, which is **not** used within AK. However, this commit corrects a AdminClient resource leak if those deprecated constructors are used outside of AK. The fix simply ensures that the AdminClient created by the “default” supplier is always closed when the Kafka*BackingStore is stopped.  See #9780   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-02-18T19:05:58Z","2021-02-19T17:49:57Z"
"","9675","KAFKA-10794 Replica leader election is too slow in the case of too many partitions","There is more than 6000 topics and 300 brokers in my kafka cluster, and we frequently run kafka-preferred-replica-election.sh to rebalance our cluster. But the reblance process spendes too more time and cpu resource like the picture blow.  We find that the function:'controllerContext.allPartitions' is invoked too many times. Thr jira link is https://issues.apache.org/jira/browse/KAFKA-10794","closed","","Montyleo","2020-12-02T08:47:53Z","2020-12-18T08:25:26Z"
"","9825","KAFKA-10904: There is a misleading log when the replica fetcher thread handles offsets that are out of range","There is ambiguity in the replica fetcher thread's log. When the fetcher thread is handling with offset out of range, it needs to try to truncate the log. When the end offset of the follower replica is greater than the log start offset of the leader replica and smaller than the end offset of the leader replica, the follower replica will maintain its own fetch offset.However, such cases are processed together with cases where the follower replica's end offset is smaller than the leader replica's start offset, resulting in ambiguities in the log, where the follower replica's fetch offset is reported to reset to the leader replica's start offset.In fact, it still maintains its own fetch offset, so this WARN log is misleading to the user.It is more accurate to print this WARN log only when follower replica really need to truncate the fetch offset to the leader replica's log start offset.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-01-05T15:52:01Z","2021-01-05T15:53:23Z"
"","9699","MINOR: remove duplicate code from resetByDuration","There is a request for putting topicPartition and timestamp into topicPartitionsAndTimes's container from inputTopicPartitions. And then take topicPartition and topicPartition's offset to call client's seek in the resetByDuration.  Beacuse I find some code in the resetToDatetime, so I would like to remove them from resetByDuration. And then I would like to take client, inputTopicPartitions and timestamp to call resetToDatetime from resetByDuration. where client is Consumer, inputTopicPartitions is container and timestamp is Duration's EpochMilli.  Finally, I found some error information in the Checks' Tab. I think these problems aren't relative to my pull request. As below information.  Log tail …398 lines… [2020-12-05T01ː06ː29.710Z] > Task ːstreamsːtest-utils:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːtest-utils:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0100:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0100:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0100:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0101:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0101:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0101:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0102:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0102:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0102:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0110:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0110:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-0110:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-10:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-10:testClasses [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-10:checkstyleTest [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-11:processTestResources NO-SOURCE [2020-12-05T01ː06ː30.837Z] > Task ːstreamsːupgrade-system-tests-11:testClasses [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-11:checkstyleTest [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-20:processTestResources NO-SOURCE [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-20:testClasses [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-20:checkstyleTest [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-21:processTestResources NO-SOURCE [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-21:testClasses [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-21:checkstyleTest [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-22:processTestResources NO-SOURCE [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-22:testClasses [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-22:checkstyleTest [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-23:processTestResources NO-SOURCE [2020-12-05T01ː06ː31.789Z] > Task ːstreamsːupgrade-system-tests-23:testClasses [2020-12-05T01ː06ː31.790Z] > Task ːstreamsːupgrade-system-tests-23:checkstyleTest [2020-12-05T01ː06ː31.790Z] > Task ːstreamsːupgrade-system-tests-24:processTestResources NO-SOURCE [2020-12-05T01ː06ː31.790Z] > Task ːstreamsːupgrade-system-tests-24:testClasses [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-24:checkstyleTest [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-25:processTestResources NO-SOURCE [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-25:testClasses [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-25:checkstyleTest [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-26:processTestResources NO-SOURCE [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-26:testClasses [2020-12-05T01ː06ː32.743Z] > Task ːstreamsːupgrade-system-tests-26:checkstyleTest [2020-12-05T01ː06ː33.697Z] > Task ːconnectːspotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-0100:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-0101:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-0102:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-0110:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-10:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-11:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-20:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-21:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-22:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-23:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-24:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-25:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː34.655Z] > Task ːstreamsːupgrade-system-tests-26:spotbugsMain NO-SOURCE [2020-12-05T01ː06ː40.856Z]  [2020-12-05T01ː06ː40.856Z] > Task :rat [2020-12-05T01ː06ː40.856Z] Rat report: /home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-9699/build/rat/rat-report.html [2020-12-05T01ː07ː03.231Z]  [2020-12-05T01ː07ː03.231Z] > Task ːclientsːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːcoreːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːexamplesːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːgeneratorːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːjmh-benchmarksːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːlog4j-appenderːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːraftːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːstreamsːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːtoolsːspotbugsMain [2020-12-05T01ː07ː03.231Z] > Task ːconnectːapi:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːbasic-auth-extension:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːfile:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːjson:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːmirror:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːmirror-client:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːruntime:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːconnectːtransforms:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːstreamsːexamples:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːstreamsːstreams-scala:spotbugsMain [2020-12-05T01ː07ː04.183Z] > Task ːstreamsːtest-utils:spotbugsMain [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] FAILURE: Build failed with an exception. [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] * What went wrong: [2020-12-05T01ː09ː09.077Z] Execution failed for task 'ːstreamsːcompileTestJava'. [2020-12-05T01ː09ː09.077Z] > Compilation failed; see the compiler error output for details. [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] * Try: [2020-12-05T01ː09ː09.077Z] Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights. [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] * Get more help at https://help.gradle.org [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0. [2020-12-05T01ː09ː09.077Z] Use '--warning-mode all' to show the individual deprecation warnings. [2020-12-05T01ː09ː09.077Z] See https://docs.gradle.org/6.7.1/userguide/command_line_interface.html#sec:command_line_warnings [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] BUILD FAILED in 9m 15s [2020-12-05T01ː09ː09.077Z] 203 actionable tasks: 169 executed, 34 up-to-date [2020-12-05T01ː09ː09.077Z]  [2020-12-05T01ː09ː09.077Z] See the profiling report at: file:///home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-9699/build/reports/profile/profile-2020-12-05-00-59-52.html [2020-12-05T01ː09ː09.077Z] A fine-grained performance profile is available: use the --scan option.","closed","","bertber","2020-12-05T00:59:10Z","2020-12-10T03:08:18Z"
"","9682","KAFKA-10803: Fix improper removal of bad dynamic config","There is [a bug](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala#L470) in how incorrect dynamic config keys are removed from the original Properties list, resulting in persisting the improper configs in the properties list.  This eventually results in exception being thrown while parsing the list by [KafkaConfig ctor](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala#L531), resulting in skipping of the complete dynamic list (including the correct ones).  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)  reviewers: @rajinisivaram @omkreddy","closed","","prat0318","2020-12-03T16:29:20Z","2020-12-04T03:59:40Z"
"","10225","MINOR: fix security_test for ZK case due to error change","The ZooKeeper version of this system test is failing because the producer is no longer seeing `LEADER_NOT_AVAILABLE`. When the broker sees a METADATA request for the test topic after it restarts the auto topic creation manager is determining that the topic needs to be created due to the TLS hostname verification failure on the inter-broker security protocol.  It also thinks there aren't enough brokers available to meet the default topic replication factor (it sees 0 available due to the TLS issue), so it returns`INVALID_REPLICATION_FACTOR` for that topic in the Metadata response. In other words, the flow has changed and the inability to produce is not manifesting as it was before, and the test is failing.  This patch updates the test to check for `INVALID_REPLICATION_FACTOR` instead of `LEADER_NOT_AVAILABLE`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-02-26T23:53:36Z","2021-03-08T20:34:35Z"
"","10120","MINOR: Update zstd and use classes with no finalizers","The updated version includes a few optimizations that benefit us: * Classes with no finalizers (opt-in) that have better GC behavior * `InputStream.skip()` implementation that uses cached buffers * Minor buffer recycler optimizations (used for OutputStream only)  Full diff: https://github.com/luben/zstd-jni/compare/v1.4.8-2...v1.4.8-4  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-12T16:13:31Z","2021-02-14T16:14:13Z"
"","9654","MINOR: Increase unit test coverage of ProcessorTopology#updateSourceTopics()","The unit tests for method ProcessorTopology#updateSourceTopics() did not cover all code paths.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-11-25T12:14:04Z","2020-12-03T14:16:34Z"
"","9572","KAFKA-10500: Thread Cache Resizes","The thread cache can now be resized. This will go towards being able to scale the number of threads  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2020-11-06T18:48:33Z","2021-01-08T02:02:42Z"
"","9888","KAFKA-12194: use stateListener to catch each state change","The tests are flaky because we used the `waitForApplicationState` to wait for a state. `waitForApplicationState` is using poll to check the current stream state, which might miss some state changes.  Ex:  ``` state: created -> running check state: running ``` ``` state: running -> rebalancing state: rebalancing -> running check state: running <-- which is not what we expected (we expected to be rebalancing, but missed) ```  I use `StateListener` to keep the new state of each state change. So when we verify a specific state, we can always find it if existed. Also have small refactor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-14T09:10:55Z","2021-01-19T05:42:15Z"
"","9777","KAFKA-7940: wait until we've got the expected partition size","The test will create 99 partitions in a topic, and expect we can get the partition info after 15 seconds. If we can't get the partition info within 15 secs, we'll get the error: ``` org.scalatest.exceptions.TestFailedException: Partition [group1_largeTopic,69] metadata not propagated after 15000 ms ``` Obviously, 15 secs is not enough to complete the 99 partitions creation under slow system. So, fix it by explicitly wait until we've got the expected partition size before retrieving each partition info.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-22T07:25:38Z","2021-02-01T18:16:29Z"
"","9925","MINOR: Tag `RaftEventSimulationTest` as `integration` and tweak it","The test takes over 1 minute to run, so it should not be considered a unit test.  Also: * Replace `test` prefix with `check` prefix for helper methods. A common mistake is to forget to add the @Test annotation, so it's good to use a different naming convention for methods that should have the annotation versus methods that should not. * Replace `Action` functional interface with built-in `Runnable`. * Remove unnecessary `assumeTrue`. * Remove `@FunctionalInterface` from `Invariant` since it's not used in that way.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-18T16:32:21Z","2021-01-23T23:57:29Z"
"","9779","KAFKA-10767: Adding test cases for all, reverseAll and reverseRange for ThreadCache","The test cases for ThreaCache didn't have the corresponding unit tests for all, reverseAll and reverseRange methods. This PR aims to add the same.","closed","","vamossagar12","2020-12-22T13:15:39Z","2021-05-17T03:55:59Z"
"","9530","MINOR: Fix version verification in system test","The system test StreamsUpgradeTest.test_version_probing_upgrade tries to verify the wrong version for version probing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-10-29T19:05:35Z","2020-12-03T14:16:52Z"
"","10237","MINOR: fix failing system test delegation_token_test","The system test in `delegation_token_test.py` broke due to https://github.com/apache/kafka/pull/10199/.   That patch changed the logic of `SecurityConfig.enabled_sasl_mechanisms()` to only add the inter-broker SASL mechanism when the inter-broker protocol was `SASL_{PLAINTEXT,SSL}`.  The inter-broker protocol is `PLAINTEXT` in `delegation_token_test.py`, so the default inter-broker SASL mechanism of `GSSAPI` was not being added into the set returned by `enabled_sasl_mechanisms()`.  This is actually correct -- it shouldn't be added if it isn't used for inter-broker communication.  It should be added because clients use it, of course -- `SASL_PLAINTEXT` is the security protocol on an advertised listener, and `client_sasl_mechanism` is set to the .csv value `""GSSAPI,SCRAM-SHA-256""`in `delegation_token_test`.  Unfortunately in https://github.com/apache/kafka/pull/10199/ we did not take into account the possibility that `client_sasl_mechanism` could be a .csv value, and we therefore fail to create a `krb5.conf` file, which causes `kafka-delegation_tokens.sh` to fail.  This bug of .csv omission therefore uncovered a different bug -- we were relying on the default inter-broker SASL mechanism to signal that Kerberos was being used even though the inter-broker protocol wasn't SASL.  This patch explicitly includes the elements of the `client_sasl_mechanism` .csv value (which in most cases is just a single value but in `delegation_token_test` it is not).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-03-01T16:34:59Z","2021-03-09T21:55:29Z"
"","9803","MINOR: Don't include stack trace for StateChangeFailedException","The stack trace for `StateChangeFailedException` is useless since the exception is just used to propagate an error message to the state change logger. This patch eliminates the stack trace which should help to clean up logging a little bit.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-30T23:00:45Z","2021-08-20T18:16:21Z"
"","9645","MINOR: Update build and test dependencies","The spotbugs upgrade means we can re-enable RCN_REDUNDANT_NULLCHECK_OF_NONNULL_VALUE and RCN_REDUNDANT_NULLCHECK_WOULD_HAVE_BEEN_A_NPE. These uncovered one bug, one unnecessary null check and one false positive. Addressed them all, including a test for the bug.  * gradle (6.7.0 -> 6.7.1): minor fixes. * gradle versions plugin (0.29.0 -> 0.36.0): minor fixes. * grgit (4.0.2 -> 4.1.0): a few small fixes and dependency bumps. * owasp dependency checker plugin (5.3.2.1 -> 6.0.3): improved db schema, data and several fixes.  * scoverage plugin (4.0.2 -> 5.0.0): support Scala 2.13. * shadow plugin (6.0.0 -> 6.1.0): require Java 8, support for Java 16. * spotbugs plugin (4.4.4 -> 4.6.0): support SARIF reporting standard. * spotbugs (4.0.6 -> 4.1.4): support for Java 16 and various fixes including try with resources false positive. * spotless plugin (5.1.0 -> 5.8.2): minor fixes. * test retry plugin (1.1.6 -> 1.1.9): newer gradle and java version compatibility fixes. * mockito (3.5.7 -> 3.6.0): minor fixes. * powermock (2.0.7 -> 2.0.9): minor fixes.  Release notes links: * https://docs.gradle.org/6.7.1/release-notes.html * https://github.com/spotbugs/spotbugs/blob/4.1.4/CHANGELOG.md * https://github.com/scoverage/gradle-scoverage/releases/tag/5.0.0 * https://github.com/johnrengelman/shadow/releases/tag/6.1.0 * https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0 * https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.0 * https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.5.0 * https://github.com/ben-manes/gradle-versions-plugin/releases * https://github.com/ajoberstar/grgit/releases/tag/4.1.0 * https://github.com/jeremylong/DependencyCheck/blob/main/RELEASE_NOTES.md#version-603-2020-11-03 * https://github.com/powermock/powermock/releases/tag/powermock-2.0.8 * https://github.com/powermock/powermock/releases/tag/powermock-2.0.9 * https://github.com/mockito/mockito/blob/v3.6.0/doc/release-notes/official.md * https://github.com/gradle/test-retry-gradle-plugin/releases * https://github.com/diffplug/spotless/blob/main/plugin-gradle/CHANGES.md  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-11-23T19:32:03Z","2020-11-24T14:20:09Z"
"","9652","MINOR: Align the UID inside/outside container","The root cause of causing permission issue is that the uid outside container is not 1000 so container (has uid 1000) can't modify the file of ```/opt/kafka-dev```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-11-25T09:03:29Z","2020-12-03T02:41:51Z"
"","9535","MINOR: remove redundant return statement","the result of `GroupMetadataManager.storeOffsets` is Unit, so remove the redundant return statement","closed","","dengziming","2020-10-30T04:06:13Z","2020-11-02T05:21:53Z"
"","10185","KAFKA-12284: wait for mm2 auto-created the topic","The reason why the test sometimes failed with: `TopicExistsException: Topic 'primary.test-topic-2' already exists.` is because we tried to create the topic that the MM2 already help us created. That is,  ```java primary.kafka().createTopic(""test-topic-2"", NUM_PARTITIONS); // after the primary cluster created ""test-topic-2"" topic, the backup cluster will auto-created the topic: ""primary.test-topic-2"" for us backup.kafka().createTopic(""primary.test-topic-2"", 1); // this line will have race condition with the MM2 ```  Note: after MM2 replicate the topic, the topic partition number will also get increased to the `NUM_PARTITIONS` in the end. So, we should just let MM2 help us create topic. Also fix 2 issues in the tests: 1. We had resource leak due to no close the adminClient  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-23T02:44:26Z","2021-04-15T07:00:14Z"
"","9517","MINOR: add comments to explain why it needs to add synchronization on…","The reason of adding synchronization is obscure so it looks like a bug. It seems to me we should add comment to make it more readable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2020-10-28T08:32:12Z","2020-12-09T15:49:27Z"
"","10070","KAFKA-12276: Add the quorum controller code","The quorum controller stores metadata in the KIP-500 metadata log, not in Apache ZooKeeper.  Each controller node is a voter in the metadata quorum.  The leader of the quorum is the active controller, which processes write requests.  The followers are standby controllers, which replay the operations written to the log.  If the active controller goes away, a standby controller can take its place.  Like the ZooKeeper-based controller, the quorum controller is based on an event queue backed by a single-threaded executor.  However, unlike the ZK-based controller, the quorum controller can have multiple operations in flight-- it does not need to wait for one operation to be finished before starting another.  Therefore, calls into the QuorumController return CompleteableFuture objects which are completed with either a result or an error when the operation is done.  The QuorumController will also time out operations that have been sitting on the queue too long without being processed.  In this case, the future is completed with a TimeoutException.  The controller uses timeline data structures to store multiple ""versions"" of its in-memory state simultaneously.  ""Read operations"" read only committed state, which is slightly older than the most up-to-date in-memory state.  ""Write operations"" read and write the latest in-memory state.  However, we can not return a successful result for a write operation until its state has been committed to the log.  Therefore, if a client receives an RPC response, it knows that the requested operation has been performed, and can not be undone by a controller failover.","closed","kip-500,","cmccabe","2021-02-05T18:33:34Z","2021-02-22T16:54:22Z"
"","9846","KAFKA-12165: Include org.apache.kafka.common.quota in javadoc","The public API classes in `org.apache.kafka.common.quota` should be included in the javadoc.  (they're not currently, see for example https://kafka.apache.org/27/javadoc/org/apache/kafka/clients/admin/Admin.html#alterClientQuotas-java.util.Collection-)","closed","","tombentley","2021-01-08T10:20:30Z","2021-01-08T19:57:23Z"
"","9648","KAFKA-10758: ProcessorTopology should only consider its own nodes when updating regex source topics","The problem is basically just that we compare two incompatible sets in ProcessorTopology#updateSourceTopics: the local `sourceNodesByName` map only contains nodes that correspond to a particular subtopology whereas the passed-in `nodeToSourceTopics` ultimately comes from the InternalTopologyBuilder's map, which contains nodes for the entire topology. So we would end up hitting the IllegalStateException thrown in #updateSourceTopics` any time we tried to update an application with more than one subtopology.  The fix is simple, we just need to ignore any source nodes that aren't part of the ProcessorTopology's subtopology.","closed","streams,","ableegoldman","2020-11-24T01:20:02Z","2020-11-25T02:50:51Z"
"","9934","MINOR: Drop enable.metadata.quorum config","The primary purpose of this patch is to remove the internal `enable.metadata.quorum` configuration. Instead, we rely on `process.roles` to determine if the self-managed quorum has been enabled. As a part of this, I've done the following:  1. Replace the notion of ""disabled"" APIs with ""controller-only"" APIs. We previously marked some APIs which were intended only for the KIP-500 as ""disabled"" so that they would not be unintentionally exposed. For example, the Raft quorum APIs were disabled. Marking them as ""controller-only"" carries the same effect, but makes the intent that they should be only exposed by the KIP-500 controller clearer. 2. Make `ForwardingManager` optional in `KafkaServer` and `KafkaApis`. Previously we used `null` if forwarding was enabled and relied on the metadata quorum check. 3. Make `zookeeper.connect` an optional configuration if `process.roles` is defined.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-20T00:18:55Z","2021-08-11T13:15:29Z"
"","9725","MINOR: optimize EvictableKey/LastUsedKey compareTo function","The previous implementation create two object wraps.   1. tuple 2. primitive type -> object type  As ```compareTo``` is hot method in fetch path, it should be worth rewriting them by java code to avoid useless wrap.  **BEFORE** ``` FetchKeyBenchmark.compareEvictableKey                               avgt   15    12.156 ±   0.010   ns/op FetchKeyBenchmark.compareEvictableKey:·gc.alloc.rate.norm           avgt   15   112.000 ±   0.001    B/op FetchKeyBenchmark.compareLastUsedKey                                avgt   15    13.929 ±   0.088   ns/op FetchKeyBenchmark.compareLastUsedKey:·gc.alloc.rate.norm            avgt   15   136.000 ±   0.001    B/op ```  **AFTER** ``` FetchKeyBenchmark.compareEvictableKey                      avgt   15   2.351 ±  0.001   ns/op FetchKeyBenchmark.compareEvictableKey:·gc.alloc.rate.norm  avgt   15  ≈ 10⁻⁷             B/op FetchKeyBenchmark.compareLastUsedKey                       avgt   15   2.154 ±  0.001   ns/op FetchKeyBenchmark.compareLastUsedKey:·gc.alloc.rate.norm   avgt   15  ≈ 10⁻⁷             B/op ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2020-12-10T06:20:07Z","2020-12-11T04:03:26Z"
"","9733","KAFKA-10017: fix 2 issues in EosBetaUpgradeIntegrationTest","The PR follows #9688, to make the EosBetaUpgradeIntegrationTest more stable. Fixed some issues: 1. Test failed with the error message: ``` org.apache.kafka.streams.errors.InvalidStateStoreException: Cannot get state store store because the stream thread is PARTITIONS_ASSIGNED, not RUNNING ``` https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/274/testReport/org.apache.kafka.streams.integration/EosBetaUpgradeIntegrationTest/shouldUpgradeFromEosAlphaToEosBeta_true_/ After investigation, I found it's because we actually do rebalancing twice after new stream started: 1 for Adding new member, 1 for leader re-joining group during Stable. So, if we only wait for the state to be `RUNNING`, it might enter `REBALANCING` state later, and cause that we can't get store successfully   2. We setUncaughtExceptionHandler, but didn't handle it well. Before, we expected the uncaught exception only got 1, but actually, we'll get 4 here (2 injected exception, 2 injected commit exception). So, we saw many messages in stderr output, which is not good for debugging. After fix, we only output to stderr when the exception is not what we expected for debugging use.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2020-12-11T09:27:46Z","2021-01-19T18:31:15Z"
"","10359","MINOR: LogManager comments update","The parameter ‘loadconfig’ has been deleted, but it still exists in the comments  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","unlizhao","2021-03-19T12:56:55Z","2021-03-19T16:23:09Z"
"","10055","[2.3] Backport mocked HostResolver from KAFKA-12193 to avoid relying on kafka.apache.org for specific DNS behavior","The original PR was #9902   The reason for back-porting this improved test infrastructure is to address some flakes which we have seen when kafka.apache.org DNS changes, specifically in the `ClusterConnectionStatesTest`. This change uses a mocked resolver for the multi-ip tests.","closed","","gardnervickers","2021-02-04T20:55:54Z","2021-02-04T23:57:39Z"
"","10058","[2.5] Backport mocked HostResolver from KAFKA-12193 to avoid relying on kafka.apache.org for specific DNS behavior","The original PR was #9902  The reason for back-porting this improved test infrastructure is to address some flakes which we have seen when kafka.apache.org DNS changes, specifically in the ClusterConnectionStatesTest. This change uses a mocked resolver for the multi-ip tests.  Related PR: #10055 #10057","closed","","gardnervickers","2021-02-04T21:19:04Z","2021-02-04T23:57:43Z"
"","10057","[2.4] Backport mocked HostResolver from KAFKA-12193 to avoid relying on kafka.apache.org for specific DNS behavior","The original PR was #9902  The reason for back-porting this improved test infrastructure is to address some flakes which we have seen when kafka.apache.org DNS changes, specifically in the ClusterConnectionStatesTest. This change uses a mocked resolver for the multi-ip tests.  Related PR: #10055","closed","","gardnervickers","2021-02-04T21:01:47Z","2021-02-04T23:57:41Z"
"","9753","KAFKA-10855; Fix non-local return in `KafkaApis.handle`","The non-local return when `maybeHandleInvalidEnvelope` returns true causes the default error handler to execute after a response has already been sent. This patch rewrites the check as a local return.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-15T05:22:08Z","2020-12-15T21:07:48Z"
"","10331","KAFKA-10847: Add a RocksDBTimeOrderedWindowStore to hold records using their timestamp as key prefix","The new `RocksDBTimeOrderedWindowStore` stores key/value records with a combined key that starts with the record timestamp, followed by a sequential number and the key in bytes (``). This key combination will keep all records ordered by timestamp in the RocksDB store. The advantage of keeping records this way is to make range queries functions more efficient, like `fetchAll(from, to)`. This range method returns all keys in the specified time range using a more efficient key schema to iterate on RocksDB.  `RocksDBTimeOrderedWindowStore` with key query ranges, like `fetch(key, from, to)` or `fetch(keyFrom, keyTo, from, to)`, do not perform very well. For cases with key query ranges, it is better to use the current `RocksDBWindowStore` which stores records using the record key as the RocksDB key prefix. `RocksDBTimeOrderedWindowStore` is meant to fix the issue with https://issues.apache.org/jira/browse/KAFKA-10847 which requires a temporary store where to hold non-joined records, and later do a query range of all keys in a specified time range.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  - Added unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","spena","2021-03-16T15:45:12Z","2021-04-08T16:51:03Z"
"","10470","KAFKA-12612: Remove `checksum` from ConsumerRecord/RecordMetadata for 3.0","The methods have been deprecated since 0.11 without replacement since message format 2 moved the checksum to the record batch (instead of the record).  Unfortunately, we did not deprecate the constructors that take a checksum (even though we intended to) so we cannot remove them. I have deprecated them for removal in 4.0 and added a single non deprecated constructor to `ConsumerRecord` and `RecordMetadata` that take all remaining parameters. `ConsumerRecord` could do with one additional convenience constructor, but that requires a KIP and hence should be done separately.  Also: * Removed `ChecksumMessageFormatter`, which is technically not public API, but may have been used with the console consumer. * Updated all usages of `ConsumerRecord`/`RecordMetadata` constructors to use the non deprecated ones. * Added tests for deprecated `ConsumerRecord/`RecordMetadata` constructors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-03T16:20:20Z","2021-04-14T21:38:40Z"
"","10284","KAFKA-12441: remove deprecated method StreamsBuilder#addGlobalStore","The method StreamsBuilder#addGlobalStore was simplified via KIP-233 in 1.1.0 release. This PR removes the old and deprecated overload.  Call for review @guozhangwang @ableegoldman","closed","streams,","mjsax","2021-03-09T08:02:15Z","2021-03-11T00:14:47Z"
"","10303","MINOR: Remove redundant inheritance from FilteringJmxReporter #onMetricRemoved","The method of inheritance is redundant. This method that already has the same function. So I remove it.","closed","","UnityLung","2021-03-11T08:29:14Z","2021-03-13T09:51:35Z"
"","9503","KAFKA-10644: Fix VotedToUnattached test error","The method name is VotedToUnattached, but the code is UnattachedToUnattached, just fix it.","closed","","dengziming","2020-10-26T12:13:15Z","2020-10-27T23:41:01Z"
"","9546","MINOR: optimeze PartitionStateMachine handleStateChanges","The logic of state change to OfflinePartition or NonExistentPartition are identical, so just merge them. If fact, the logic of the case NewPartition is also the same, but have different log format, so ignore.","closed","","dengziming","2020-11-03T05:02:39Z","2020-11-04T03:16:21Z"
"","10279","MINOR: Refer user to `kafka-storage.sh` if `meta.properties` is missing","The KIP-500 server requires users to run kafka-storage.sh to format log directories before the server will start. If the directory is not formatted, the error message complains about a missing `meta.properties` file. It is useful for the message to refer the user to `kafka-storage.sh` directly since this is a new thing.  I've also reduced the log level of a very spammy log message in `BrokerLifecycleManager`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-08T19:15:15Z","2021-03-08T21:04:46Z"
"","10249","MINOR: disable round_trip_fault_test system tests for Raft quorums","The KIP-500 early access release will not support creating a partition with a manual partition assignment that includes a broker that is not currently online.  This patch disables system tests for Raft-based metadata quorums where the test depends on this functionality to pass.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-03-02T20:23:02Z","2021-03-09T21:57:16Z"
"","10480","KAFKA-12265: Move the BatchAccumulator in KafkaRaftClient to LeaderState","The KafkaRaftClient has a field for the BatchAccumulator that is only used and set when it is the leader. In other cases, leader specific information was stored in LeaderState. In a recent change EpochState, which LeaderState implements, was changed to be a Closable. QuorumState makes sure to always close the previous state before transitioning to the next state. This redesign was used to move the BatchAccumulator to the LeaderState and simplify some of the handling in KafkaRaftClient.  https://issues.apache.org/jira/browse/KAFKA-12265","closed","","dielhennr","2021-04-05T21:43:16Z","2021-04-30T22:41:46Z"
"","10094","KAFKA-12334: Add the KIP-631 metadata shell","The Kafka Metadata shell is a new command which allows users to interactively examine the metadata stored in a KIP-500 cluster.  It can read the metadata from the controllers directly, by connecting to them, or from a metadata snapshot on disk.  In the former case, the quorum voters must be specified by passing the --controllers flag; in the latter case, the snapshot file should be specified via --snapshot.  The metadata tool works by replaying the log and storing the state into in-memory nodes.  These nodes are presented in a fashion similar to filesystem directories.","closed","kip-500,","cmccabe","2021-02-09T23:27:26Z","2021-02-19T23:49:10Z"
"","9962","MINOR: set initial capacity of ArrayList for all json converters","the instance of ```JsonNode``` in this path must be ```ArrayNode``` so we can set initial capacity for all ```ArrayList```.   ### BEFORE (AddPartitionsToTxnRequestDataJsonConverter.java)  ```java ArrayList _collection = new ArrayList(); _object.partitions = _collection; for (JsonNode _element : _partitionsNode) {     _collection.add(MessageUtil.jsonNodeToInt(_element, ""AddPartitionsToTxnTopic element"")); } ```  ### AFTER (AddPartitionsToTxnRequestDataJsonConverter.java)  ```java ArrayList _collection = new ArrayList(_partitionsNode.size()); _object.partitions = _collection; for (JsonNode _element : _partitionsNode) {     _collection.add(MessageUtil.jsonNodeToInt(_element, ""AddPartitionsToTxnTopic element"")); } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-25T09:34:40Z","2021-01-26T18:01:13Z"
"","10318","KAFKA-12330; FetchSessionCache may cause starvation for partitions when FetchResponse is full","The incremental FetchSessionCache sessions deprioritizes partitions where a response is returned. This may happen if log metadata such as log start offset, hwm, etc is returned, or if data for that partition is returned.  When a fetch response fills to maxBytes, data may not be returned for partitions even if the fetch offset is lower than the fetch upper bound. However, the fetch response will still contain updates to metadata such as hwm if that metadata has changed. This can lead to degenerate behavior where a partition's hwm or log start offset is updated resulting in the next fetch being unnecessarily skipped for that partition. At first this appeared to be worse, as hwm updates occur frequently, but starvation should result in hwm movement becoming blocked, allowing a fetch to go through and then becoming unstuck. However, it'll still require one more fetch request than necessary to do so. Consumers may be affected more than replica fetchers, however they often remove partitions with fetched data from the next fetch request and this may be helping prevent starvation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-15T14:10:56Z","2021-03-17T06:49:58Z"
"","9674","KAFKA-10665: close all kafkaStreams before purgeLocalStreamsState","The flaky tests are because we forgot to close the kafkaStreams before purgeLocalStreamsState, so that sometimes there will be some tmp files be created/deleted during streams running(ex: `checkpoint.tmp`), and caused the `DirectoryNotEmptyException` or `NoSuchFileException` be thrown.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-02T07:45:21Z","2020-12-04T14:21:24Z"
"","9752","KAFKA-10854: Fix flaky testConnectionRatePerIp test","The flaky test is because we expected our last(6th) connection will be the one connection who got throttled, but this is not true. The reason is:  (1)  `selector.select()` will return the channels that are ready, so the last connection might not get ready in the last (2) the `selector.selectedKeys()` returns `Set`, and the data structure `Set` doesn't provide any ordering guarantees  So, to fix it, in the setup phase, I created (maximum allowable number + 1) of connections, and make sure there's 1 connection throttled. And then, we can make sure the next connection will also get throttled( of course the time is not ticking due to we use `MockTime`), so that we can make the tests more reliable.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-15T04:10:53Z","2020-12-15T17:12:20Z"
"","9629","KAFKA-10754: fix flaky tests by waiting kafka streams be in running state before assert","The flaky test is because we didn't wait for the streams become RUNNING before verifying the state becoming ERROR state. This fix explicitly wait for the streams become RUNNING state. Also, I found we didn't put the 2nd stream into try resource block, so it won't close the stream after tests. In addition, I also fix some logic errors in the tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2020-11-20T07:44:52Z","2020-11-24T21:59:27Z"
"","10342","KAFKA-12288: remove task-level filesystem locks","The filesystem locks don't protect access between StreamThreads, only across different instances of the same Streams application. Running multiple processes in the same physical state directory is not supported, and as of [PR #9978](https://github.com/apache/kafka/pull/9978) it's explicitly guarded against), so there's no reason to continue locking the task directories with anything heavier than an in-memory map.  Ripping out out the task-level filesystem locks not only cleans up the code, but removes an occasional source of trouble due to IOExceptions. We now use only an umbrella lock of the state directory itself which is held for the lifetime of the application.  We can probably cherrypick this back to 2.6 since that's how far back we merged [PR #9978](https://github.com/apache/kafka/pull/9978)","closed","streams,","ableegoldman","2021-03-17T23:08:09Z","2021-03-31T00:02:42Z"
"","10269","KAFKA-12410 KafkaAPis ought to group fetch data before generating fet…","The fetch data generated by `KafkaApis` is re-grouped when it is converted to `FetchResponse`. That is unnecessary since `KafkaApis` can keep a grouped collection for fetch data. The other main changes are shown below.  1. remove `FetchResponse#of` 1. remove useless constructor from `FetchResponse` 1. remove `PartitionIterator`  # JMH Tests  - cpu: intel i9-10900 - ram: 64 GB - `IncrementalFetchContextBenchmark`  ## trunk (https://github.com/apache/kafka/pull/10291)  ``` IncrementalFetchContextBenchmark.getResponseSize                                                 avgt   10        18.006 ±       1.749   ms/op IncrementalFetchContextBenchmark.getResponseSize:·gc.alloc.rate                                  avgt   10       564.158 ±      54.754  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.alloc.rate.norm                             avgt   10  11190518.325 ±      48.355    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Eden_Space                         avgt   10       564.717 ±     102.891  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Eden_Space.norm                    avgt   10  11202996.185 ± 1780869.078    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Old_Gen                            avgt   10         0.001 ±       0.004  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Old_Gen.norm                       avgt   10        17.825 ±      85.222    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Survivor_Space                     avgt   10        12.259 ±      19.367  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Survivor_Space.norm                avgt   10    252405.775 ±  410781.330    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.count                                       avgt   10        48.000                counts IncrementalFetchContextBenchmark.getResponseSize:·gc.time                                        avgt   10      8665.000                    ms IncrementalFetchContextBenchmark.updateAndGenerateResponseData                                   avgt   10        16.580 ±       1.551   ms/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.alloc.rate                    avgt   10       581.410 ±      54.691  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.alloc.rate.norm               avgt   10  10649362.111 ±      69.365    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Eden_Space           avgt   10       569.026 ±      86.815  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Eden_Space.norm      avgt   10  10441914.443 ± 1589284.501    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Survivor_Space       avgt   10        11.383 ±      22.870  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Survivor_Space.norm  avgt   10    205891.249 ±  413155.900    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.count                         avgt   10        45.000                counts IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.time                          avgt   10      8692.000                    ms ```  ## patch (https://github.com/apache/kafka/pull/10269/commits/603da4d86b155321aa18cc2cda1b8659ea31bc2d)  ``` IncrementalFetchContextBenchmark.getResponseSize                                                 avgt   10        15.883 ±       1.696   ms/op IncrementalFetchContextBenchmark.getResponseSize:·gc.alloc.rate                                  avgt   10       621.511 ±      66.191  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.alloc.rate.norm                             avgt   10  10823170.429 ±      68.899    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Eden_Space                         avgt   10       615.643 ±     108.319  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Eden_Space.norm                    avgt   10  10747008.602 ± 1840451.381    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Survivor_Space                     avgt   10         4.642 ±      10.201  MB/sec IncrementalFetchContextBenchmark.getResponseSize:·gc.churn.G1_Survivor_Space.norm                avgt   10     86612.407 ±  193293.062    B/op IncrementalFetchContextBenchmark.getResponseSize:·gc.count                                       avgt   10        49.000                counts IncrementalFetchContextBenchmark.getResponseSize:·gc.time                                        avgt   10      9855.000                    ms IncrementalFetchContextBenchmark.updateAndGenerateResponseData                                   avgt   10        16.457 ±       1.462   ms/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.alloc.rate                    avgt   10       601.353 ±      53.868  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.alloc.rate.norm               avgt   10  10865891.101 ±      60.561    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Eden_Space           avgt   10       597.302 ±      82.682  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Eden_Space.norm      avgt   10  10813118.583 ± 1536423.202    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Survivor_Space       avgt   10         2.892 ±       8.490  MB/sec IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.churn.G1_Survivor_Space.norm  avgt   10     54084.799 ±  159225.286    B/op IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.count                         avgt   10        48.000                counts IncrementalFetchContextBenchmark.updateAndGenerateResponseData:·gc.time                          avgt   10      8639.000                    ms ```  # Performance Tests  - cpu: intel i9-10900 - ram: 64 GB - script: `benchmark_test.py::Benchmark.test_consumer_throughput` - loops: 30  ## case 0: +2.760595128 % ```json {   ""compression_type"": ""none"",   ""security_protocol"": ""PLAINTEXT"",   ""interbroker_security_protocol"": ""PLAINTEXT"" }	 ``` - TRUNK: 259.228705 MB/sec - PATCH: 266.38496 MB/sec  ## case 1: -0.6257537776 % ```json {   ""compression_type"": ""snappy"",   ""security_protocol"": ""PLAINTEXT"",   ""interbroker_security_protocol"": ""PLAINTEXT"" }	 ``` - TRUNK: 364.161605 MB/sec - PATCH: 361.88285 MB/sec    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-03-05T05:49:04Z","2021-03-19T16:56:28Z"
"","9780","KAFKA-10021: Changed Kafka backing stores to use shared admin client to get end offsets and create topics","The existing `Kafka*BackingStore` classes used by Connect all use `KafkaBasedLog`, which needs to frequently get the end offsets for the internal topic to know whether they are caught up. `KafkaBasedLog` uses its consumer to get the end offsets and to consume the records from the topic.  However, the Connect internal topics are often written very infrequently. This means that when the `KafkaBasedLog` used in the `Kafka*BackingStore` classes is already caught up and its last consumer poll is waiting for new records to appear, the call to the consumer to fetch end offsets will block until the consumer returns after a new record is written (unlikely) or the consumer’s `fetch.max.wait.ms` setting (defaults to 500ms) ends and the consumer returns no more records. IOW, the call to `KafkaBasedLog.readToEnd()` may block for some period of time even though it’s already caught up to the end.  Instead, we want the `KafkaBasedLog.readToEnd()` to always return quickly when the log is already caught up. The best way to do this is to have the `KafkaBackingStore` use the admin client (rather than the consumer) to fetch end offsets for the internal topic. The consumer and the admin API both use the same `ListOffset` broker API, so the functionality is ultimately the same but we don't have to block for any ongoing consumer activity.  Each Connect distributed runtime includes three instances of the `Kafka*BackingStore` classes, which means we have three instances of `KafkaBasedLog`. We don't want three instances of the admin client, and should have all three instances of the `KafkaBasedLog` share a single admin client instance. In fact, each `Kafka*BackingStore` instance currently creates, uses and closes an admin client instance when it checks and initializes that store's internal topic. If we change `Kafka*BackingStores` to share one admin client instance, we can change that initialization logic to also reuse the supplied admin client instance.  The final challenge is that `KafkaBasedLog` has been used by projects outside of Apache Kafka. While `KafkaBasedLog` is definitely not in the public API for Connect, we can make these changes in ways that are backward compatible: create new constructors and deprecate the old constructors. Connect can be changed to only use the new constructors, and this will give time for any downstream users to make changes.  These changes are implemented as follows: 1. Add a `KafkaBasedLog` constructor to accept in its parameters a supplier from which it can get an admin instance, and deprecate the old constructor. We need a supplier rather than just passing an instance because `KafkaBasedLog` is instantiated before Connect starts up, so we need to create the admin instance only when needed. At the same time, we'll change the existing init function parameter from a no-arg function to accept an admin instance as an argument, allowing that init function to reuse the shared admin instance used by the `KafkaBasedLog`. Note: if no admin supplier is provided (in deprecated constructor that is no longer used in AK), the consumer is still used to get latest offsets. 2. Add to the `Kafka*BackingStore` classes a new constructor with the same parameters but with an admin supplier, and deprecate the old constructor. When the classes instantiate its `KafkaBasedLog` instance, it would pass the admin supplier and pass an init function that takes an admin instance. 3. Create a new `SharedTopicAdmin` that lazily creates the `TopicAdmin` (and underlying Admin client) when required, and closes the admin objects when the `SharedTopicAdmin` is closed. 4. Modify the existing `TopicAdmin` (used only in Connect) to encapsulate the logic of fetching end offsets using the admin client, simplifying the logic in `KafkaBasedLog` mentioned in #1 above. Doing this also makes it easier to test that logic. 5. Change `ConnectDistributed` to create a `SharedTopicAdmin` instance (that is `AutoCloseable`) before creating the `Kafka*BackingStore` instances, passing the `SharedTopicAdmin` (which is an admin supplier) to all three `Kafka*BackingStore objects`, and finally always closing the `SharedTopicAdmin` upon termination. (Shutdown of the worker occurs outside of the `ConnectDistributed` code, so modify `DistributedHerder` to take in its constructor additional `AutoCloseable` objects that should be closed when the herder is closed, and then modify `ConnectDistributed` to pass the `SharedTopicAdmin` as one of those `AutoCloseable` instances.) 6. Change `MirrorMaker` similarly to `ConnectDistributed`. 7. Change existing unit tests to no longer use deprecated constructors. 8. Add unit tests for new functionality.  This change should be backported to fix the bug in recent releases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2020-12-22T19:00:59Z","2021-02-09T17:09:42Z"
"","9778","KAFKA-10874 Fix flaky ClientQuotasRequestTest.testAlterIpQuotasRequest","The error  occcur when dynamic ip config  update after borker's config  get .   solve this by waiting dynamic ip config update.","closed","","g1geordie","2020-12-22T10:14:33Z","2021-01-07T08:46:08Z"
"","10306","MINOR: socket setup max should be 30 seconds","The default socket.connection.setup.timeout.max.ms should be 30 seconds. The current value of 127 seconds is longer than the default API timeout for AdminClient, and longer than the default request timeouts for the producer and consumer.  We should bring these configs into line with each other.","closed","","cmccabe","2021-03-11T18:18:17Z","2021-05-10T23:45:49Z"
"","10475","KAFKA-12610: Implement PluginClassLoader::getResource","The default implementation, which is inheritted from ClassLoader, searches the classloader tree from parent first. This causes issues when the resource is available both on the classpath and the plugin path. Instead of attempting to load the resource from plugin path first, the system classloader is consulted first and loads the resource from classpath.  A testcase is added in PluginsTest to verify this behavior is fixed by this commit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","ncliang","2021-04-05T08:47:41Z","2021-06-15T21:49:29Z"
"","10452","KAFKA-12593: Fix Apache License headers","The current headers in the streams-scala module, the gradle wrapper,  and some of the ducktape tests differ from the required headers for this project.  * add a header style check for scala files * apply the correct header * move the original copyright notices to the NOTICE file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-03-31T18:26:48Z","2021-04-06T13:04:46Z"
"","9590","KAFKA-7556: KafkaConsumer.beginningOffsets does not return actual first offsets","The current behavior for KafkaConsumer.beginningOffsets on compacted topics is to always return offset 0. However, when looking at the record with the earliest timestamp in the log with KafkaConsumer.offsetsForTimes, the offset is nonzero.   This PR seeks to correct this issue by updating the segment baseOffsets and the logStartOffset during compaction.  The segment offset will be the offset of the first non-compacted batch, or the original baseOffset if the segment is empty.  Generally, all updates to logStartOffset must maintain the invariant logStartOffset <= end offset of first batch that can be returned by Fetch.  There are a few key changes/decisions to note that may not be immediately apparent:  1. **Producer Snapshot Behavior** Currently the behavior of producer snapshots is that during compaction, if the segment offset of the producer snapshot file is not in the new segment offsets, the snapshot is deleted. This will lead to behavior like the following:  - So for example we have segments with offsets 0 and 1, we want to delete first segment New segment is base offset 0 so snapshot for segment 1 deleted. One segment, no snapshot files  Under this PR, the new behavior is this  - We have segments with offsets 0 and 1, we want to delete first segment New segment is base offset 1 so we don't delete snapshot. One segment, one snapshot file  I was not sure if this is behavior we wanted, so I'm pointing it out.  2.  **High Watermark Compaction** I wasn't sure if we allowed compaction past the high watermark. It seems like there is not a check to prevent it. I've added such a check for this PR.","open","","jolshan","2020-11-12T20:09:04Z","2021-08-23T01:44:33Z"
"","9781","MINOR: Use top-level error in `UpdateFeaturesRequest.getErrorResponse`","The current `getErrorResponse` sets all of the feature errors, but does not set a top-level error. It seems like the whole point of having the top-level error is so that it could be used in cases like this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-22T22:55:45Z","2020-12-29T21:41:57Z"
"","9527","MINOR: avoid unnecessary conversion and tuple when updating error met…","The conversion (from java collection to scala collection) and tuple is unnecessary for hot method  ```updateErrorMetrics```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-29T09:05:18Z","2021-01-14T14:55:05Z"
"","9649","MINOR: Combine repeated top-level error test in AlterIsrManagerTest","The content of the 3 top-level errors are identical, just merge them.","closed","","dengziming","2020-11-24T11:21:29Z","2021-01-06T16:13:52Z"
"","10458","KAFKA-12600: Remove deprecated config value `default` for client config `client.dns.lookup`","The config has been deprecated since Kafka 2.6 (released ~1 year before 3.0), but it was the default before it got deprecated. As such, it's reasonably unlikely that people would have set it explicitly.  Given the confusing `default` name even though it's _not_ the default, I think we should remove it in 3.0.  Also remove `ClientDnsLookup.DEFAULT` (not public API), which unlocks a number of code simplications.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-01T12:45:11Z","2021-04-01T15:00:03Z"
"","10298","MINOR: Disable client compatibility system test for Raft quorums","The client compatibility system test cannot pass for Raft-based quorums because several APIs have not yet been implemented, and these not-yet-implemented APIs were disabled via https://github.com/apache/kafka/pull/10194/.  The client compatibility system tests now fail since that patch was merged.  This patch disables this system test for Raft-based quorums.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-03-10T22:36:12Z","2021-03-11T18:59:40Z"
"","10095","MINOR: Introduce the KIP-500 Broker lifecycle manager","The broker lifecycle manager handles registering the broker and sending periodic heartbeats, as described in KIP-631.  Based on the responses it receives from the controller, it transitions the broker through the states described in BrokerState.java: STARTING, RECOVERY, RUNNING, PENDING_CONTROLLED_SHUTDOWN, etc.","closed","kip-500,","cmccabe","2021-02-10T01:36:37Z","2021-02-11T16:32:39Z"
"","9596","KAFKA-10723: Fix LogManager shutdown error handling","The asynchronous shutdown in `LogManager` has the shortcoming that if during shutdown any of the internal futures fail, then we do not always ensure that all futures are completed before `LogManager.shutdown` returns. This is because, [this line](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogManager.scala#L502) in the `finally` clause shuts down the thread pools asynchronously. As a result, despite the `shut down completed` message from KafkaServer is seen in the error logs, some futures continue to run from inside `LogManager` attempting to close some logs. This is misleading during debugging. Also sometimes it introduces an avoidable post-shutdown activity where resources (such as file handles) are released or persistent state is checkpointed in the Broker.  In this PR, we fix the above behavior such that we prevent leakage of threads. If any of the futures throw an error, we  skip creating of checkpoint and clean shutdown file only for the affected log directory. We continue to wait for all futures to complete for all the directories.  **Test plan:**  Added a new unit test: `LogManager.testHandlingExceptionsDuringShutdown`.","closed","","kowshik","2020-11-15T01:59:13Z","2020-11-19T18:52:18Z"
"","10334","MINOR: Fix BaseHashTable sizing","The array backing BaseHashTable is intended to be sized as a power of two.  Due to a bug, the initial array size was calculated incorrectly in some cases.  Also make the maximum array size the largest possible 31-bit power of two.  Previously it was a smaller size, but this was due to a typo.","closed","kip-500,","cmccabe","2021-03-16T21:33:02Z","2021-03-18T16:59:44Z"
"","9608","MINOR: Enable testLogCleanerStats","The `testLogCleanerStats` test in `LogCleanerTest.scala` was not enabled but it was implemented. This PR adds the `@Test` annotation, and also gives it a larger map to allow the test to pass as intended.","closed","","mattwong949","2020-11-18T01:27:24Z","2020-11-19T22:06:47Z"
"","9885","MINOR: replace NotLeaderForPartitionException with NotLeaderOrFollowerException","The `NotLeaderForPartitionException` is deprecated in 2.6 in this PR #8979. We should replace NotLeaderForPartitionException with `NotLeaderOrFollowerException`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-14T04:08:37Z","2021-01-14T12:16:17Z"
"","10479","MINOR: Jenkinsfile's `post` needs `agent` to be set","The `node` block achieves that.  Tested that an email was sent to the mailing list for: https://github.com/apache/kafka/pull/10479/commits/592a0c31d5dec5b2d33b6239f6243831b7cca361  Added back the condition not to send emails for PR builds after such test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-05T21:23:03Z","2021-04-06T05:20:24Z"
"","10425","KAFKA-12573; Remove deprecated `Metric#value`","The `Metric#value` method was deprecated in AK 1.0. It makes sense to remove it in AK 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-29T08:07:17Z","2021-03-30T07:28:51Z"
"","9687","MINOR: Move lock method outside try block","The `Lock.lock` in the try block may cause `Lock.unlock`  throw exception when it throw exception . I think it's nice to move  outside although` ReentrantLock.lock` impl doesn't throw exception.  ```  class X {    private final ReentrantLock lock = new ReentrantLock();    // ...     public void m() {      lock.lock();  // block until condition holds      try {        // ... method body      } finally {        lock.unlock()      }    }  } ``` pattern recommended by  https://docs.oracle.com/javase/10/docs/api/java/util/concurrent/locks/ReentrantLock.html","closed","","g1geordie","2020-12-04T06:44:58Z","2020-12-04T15:56:02Z"
"","9638","MINOR; Remove duplicate updateLeaderEndOffsetAndTimestamp","The `KafkaRaftClient.onBecomeLeader` will invoke `appendLeaderChangeMessage`, the call stack are: ``` 1. appendLeaderChangeMessage     1.1 flushLeaderLog         1.1.1 updateLeaderEndOffsetAndTimestamp         1.1.2 log.flush() ``` so the `updateLeaderEndOffsetAndTimestamp ` is already invoked, and `updateLeaderEndOffsetAndTimestamp` should only be invoked after leo change(or time change), since `log.flush()` will not change leo(and time),  it's unnessary to invoke twice.","closed","","dengziming","2020-11-22T09:25:34Z","2021-03-16T07:29:39Z"
"","10138","KAFKA-12331: Use LEO for the base offset of LeaderChangeMessage batch","The `KafkaMetadataLog` implementation of `ReplicatedLog` validates that batches appended using `appendAsLeader` and `appendAsFollower` have an offset that matches the LEO. This is enforced by `KafkaRaftClient` and `BatchAccumulator`. When creating control batches for the `LeaderChangeMessage` the default base offset of `0` was being used instead of using the LEO. This is fixed by:  1. Changing the implementation for `MockLog` to validate against this and throw an `RuntimeException` if this invariant is violated. 2. Always create a batch for `LeaderChangeMessage` with an offset equal to the LEO.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-02-17T05:27:46Z","2021-02-19T00:46:36Z"
"","10443","KAFKA-8405; Remove deprecated `kafka-preferred-replica-election` command","The `kafka-preferred-replica-election` command was deprecated in 2.4. As we removed the related APIs in the Admin client (https://github.com/apache/kafka/pull/10440), we can also remove the command line tool. `kafka-leader-election` can be used instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-30T15:19:23Z","2021-03-31T18:45:22Z"
"","9636","KAFKA-10757; resolve compile problems brought by KAFKA-10755","The #9634 calls new TaskManager with 9 params  ``` final TaskManager taskManager = new TaskManager(             null,             null,             null,             null,             null,             null,             null,             null,             null         ) ``` but `new TaskManager` has 10 params, which brought compile problems","closed","","dengziming","2020-11-21T04:23:59Z","2020-11-21T19:32:08Z"
"","10062","MINOR: Add performAndClose default method in KeyValueIterator","That method intends to increase a chance to have KeyValueIterator closed after usage, by providing a convenient performAndClose() default method which executes a given operation and guarantee to automatically close the iterator right after.  ### Rationale  I decided to create that PR observing in different projects how often an iterator is left open after performing operations such as  `.forEachRemaining(...)` (who reads JavaDoc after all? ;-) ). For people aware of the problem, instead of verbose try-with-resources constructions repeated in every place in code:  ``` try (KeyValueIterator ordersKeyValueIterator =          streamsBuilderFactoryBean              .getKafkaStreams()              .store(StoreQueryParameters.fromNameAndType(ORDERS_STORE,                  QueryableStoreTypes.keyValueStore()))              .all()) {     ordersKeyValueIterator         .forEachRemaining(kv -> {             ...         }); } ``` (or hidden in in-house built utility classes)  a developer using Kafka Streams has a built-in clear way to perform operations on elements contained in an iterator (and have that stream closed after that automatically): ``` streamsBuilderFactoryBean     .getKafkaStreams()     .store(StoreQueryParameters.fromNameAndType(ORDERS_STORE,         QueryableStoreTypes.keyValueStore()))     .all()     .performAndClose(iterator -> iterator         .forEachRemaining(kv -> {             ...         })     ); ```  I believe it can increase a rate of closed iterator in user projects.  ### Testing approach  Being a default method in KeyValueIterator, I decided to test it using KeyValueIteratorFacade which already provides a nice unit testing infrastructure with mocked iterator. I don't know the implementation details, but I expect to have it behaved similarly in other implementations. I might miss some extra cases, so feel free to point them and I will happily cover them with tests.  ### Rejected extensions  For some time past, I was thinking also about covering operations that returns some value. However, at least in projects I've been observing the usage of Kafka Streams, performing side effects is more popular and implementation for that use cases might be more tricky and I decided to start with pure Consumer.  ### Other information  Naming is not my area of expertise. Feel free to propose any better name or any other possible enhancements to my MR :-).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","szpak","2021-02-04T23:46:22Z","2021-03-10T09:29:00Z"
"","9711","MINOR: add ""flush=True"" to all print in system tests","That makes the behavior of ```print``` equal to pyhton2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-08T06:14:22Z","2020-12-09T19:19:25Z"
"","10261","MINOR: using INFO level to log 'no meta.properties' for broker server","That is expected behavior for broker server so it seems to me the ""warn"" level is a bit overkill. BTW, the raft server can throw exception for that case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-03-04T09:55:17Z","2021-03-04T09:58:12Z"
"","10026","MINOR: Add mock implementation of `BrokerToControllerChannelManager`","Tests involving `BrokerToControllerChannelManager` are simplified by being able to leverage `MockClient`. This patch introduces a `MockBrokerToControllerChannelManager` implementation which makes that possible.  The patch updates `ForwardingManagerTest` to use `MockBrokerToControllerChannelManager`. We also add a couple additional timeout cases, which exposed a minor bug. Previously we were using the wrong `TimeoutException`, which meant that expected timeout errors were in fact translated to `UNKNOWN_SERVER_ERROR`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-02T22:01:34Z","2021-02-03T18:30:20Z"
"","9606","[KAFKA-10722] doc: Improve JavaDoc for KGroupedStream.aggregate and other similar methods","Tell that the store used internally is always a timestamped one.  This is related to KAFKA-10722.  No tests are necessary because only JavaDoc was changed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fml2","2020-11-17T21:13:47Z","2020-12-24T00:03:49Z"
"","10463","KAFKA-12670: support unclean.leader.election.enable in KRaft mode","Support node-level and topic-level unclean.leader.election.enable in the KRaft controller.","open","kip-500,","cmccabe","2021-04-02T01:00:02Z","2021-08-31T20:28:42Z"
"","9697","KAFKA-10810: Replace stream threads","StreamThreads can now be replaced in the streams uncaught exception handler  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2020-12-04T22:17:21Z","2021-01-11T16:28:30Z"
"","9899","KAFKA-12191 SslTransportTls12Tls13Test can replace 'assumeTrue' by (j…","SslTransportTls12Tls13Test can replace 'assumeTrue' by (junit 5) conditional test Test in Java8 ``` testCiphersSuiteForTls12FailsForTls13() Disabled on JRE version: 1.8.0_275  1.27 spassedtestCiphersSuiteForTls12()  ignoredtestCiphersSuiteForTls13() Disabled on JRE version: 1.8.0_275  ignoredtestCiphersSuiteFailForServerTls12ClientTls13() Disabled on JRE version: 1.8.0_275 ```","closed","","g1geordie","2021-01-14T18:30:56Z","2021-01-15T06:34:47Z"
"","9560","KAFKA-10345: Add ZK-notification based update for trust/key store paths","SSL trust store and key store paths update could no longer go through the direct per broker update due to forwarding. We need to add a mechanism to trigger the update through ZK notification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-05T01:06:53Z","2021-04-06T00:24:04Z"
"","9665","MINOR: fix reading SSH output in Streams system tests","SSH outputs in system tests originating from paramiko are bytes. However, the logger in the system tests does not accept bytes and instead throws an exception. That means, the bytes returned as SSH output from paramiko need to converted to a type that the logger (or other objects) can process.      ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-12-01T12:41:14Z","2020-12-03T14:16:34Z"
"","10167","KAFKA-12261: Mention about potential delivery loss on increasing partition when auto.offset.reset = latest","Splitting partitions while setting `auto.offset.reset` to `latest` may cause message delivery loss, but users might not be aware about that since currently it isn't documented anywhere.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ocadaruma","2021-02-20T09:35:14Z","2021-02-23T23:00:02Z"
"","10192","MINOR: Add missing unit tests for Mirror Connect","Some test cases were missing regarding Mirror Connect. Added some to increase test coverage.","closed","","bmaidics","2021-02-23T14:12:21Z","2021-03-12T11:32:17Z"
"","9790","Some parameters will be overwritten which was configured in consumer.config.","Some parameters will be overwritten which was configured in consumer.config where running ""ConsumerPerformance.scala"" linked see https://issues.apache.org/jira/browse/KAFKA-10043  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","SweetMojito","2020-12-27T15:11:09Z","2020-12-31T07:18:08Z"
"","9933","MINOR: Upgrade ducktape to version 0.8.1","Sister PR to #9932 Ducktape 0.8.1 was updated to include the following changes/fixes from 0.7.x branch: - Junit reporting support - fix for an issue where unicode characters in exception message would cause test runner to hang on py27.   Please merge this to 2.7 also. 2.6 and below are on ducktape 0.7.x branch and #9932 is about those branches.","closed","tests,","stan-confluent","2021-01-19T22:08:39Z","2021-01-23T06:50:42Z"
"","10179","MINOR: tune KIP-631 configurations","Since we expect KIP-631 controller failovers to be fairly cheap, tune the default raft configuration parameters so that we detect node failures more quickly.  Reduce the broker session timeout as well so that broker failures are detected more quickly.","closed","kip-500,","cmccabe","2021-02-22T22:24:37Z","2021-02-26T01:16:38Z"
"","10198","MINOR: Reduce log level of spammy message in `LeaderEpochFileCache`","Since we do epoch validation as part of the fetch handling these days, the log message  `LeaderEpochFileCache.endOffsetFor` has become very noisy when DEBUG is enabled (which is often the default for system tests). This patch reduces the level to TRACE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-23T20:30:57Z","2021-02-23T23:25:09Z"
"","9930","MINOR: Fix always-passing validation in TestRecordTest#testProducerRecord","Since the types of `ProducerRecord` and `TestRecord` are different, calling `assertNotEquals` for these instances always succeeds - and meaningless.  This commit improves `TestRecordTest` by comparing the instances deeply.  cc/ @chia7712  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-01-19T12:14:33Z","2021-02-12T05:48:15Z"
"","10363","MINOR: Assume unclean shutdown for metadata log","Since log recovery for the metadata log is not implemented at the moment always assume that the replica was shutdown uncleanly. This is okay to because:  1. Snapshots are not fully implemented for the metadata log 2. The replicas (controllers and brokers) need to read the entire metadata log to load it into memory.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-03-19T17:19:02Z","2021-03-22T02:54:28Z"
"","10092","KAFKA-12287: Add WARN logging on consumer-groups when reset-offsets by timestamp or duration can't find an offset and defaults to latest","Similar to #10042, this PR introduces a WARN logging when no offset is found in a topic partition.  Test strategy:  - Adding a test case for a topic without records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","jeqo","2021-02-09T20:49:42Z","2021-03-23T19:03:19Z"
"","10299","KAFKA-10070: parameterize Connect unit tests to remove code duplication","Signed-off-by: Lev Zemlyanov   Parameterize Connect unit tests to remove code duplication. Some PRs added duplicate test classes to test topic creation. This fix parameterizes the topic creation to reduce code duplication.  The topic creation tests were copied over to the original test classes to capture any changes to reduce risk of test coverage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","levzem","2021-03-11T02:30:47Z","2021-03-19T14:03:36Z"
"","9867","KAFKA-10835: Replace Runnable and Callable overrides with lambdas in Connect","Signed-off-by: Lev Zemlyanov   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  change interfaces to lambdas  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  unit as this is purely stylistic change  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","levzem","2021-01-11T23:58:28Z","2021-02-04T17:15:50Z"
"","10053","KAFKA-10834: Remove redundant type casts in Connect","Signed-off-by: Lev Zemlyanov   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","levzem","2021-02-04T17:55:39Z","2021-02-05T01:08:57Z"
"","10291","(DON""T MERGE) add benchmark","show benchmark for trunk (compare to https://github.com/apache/kafka/pull/10269)","closed","","chia7712","2021-03-10T08:07:01Z","2021-03-17T11:06:57Z"
"","10214","MINOR: fix message and reduce log level of PartitionGroup enforced processing","Should be cherrypicked back to 2.8","closed","","ableegoldman","2021-02-26T02:16:40Z","2021-03-01T22:48:47Z"
"","10311","KAFKA-12462: proceed with task revocation in case of thread in PENDING_SHUTDOWN","Should be cherrypicked back to 2.7 & 2.8","closed","","ableegoldman","2021-03-13T00:00:41Z","2022-03-01T21:08:54Z"
"","10134","TRIVIAL: fix JavaDocs formatting","Should be cherry-picked to `2.8` branch.","closed","docs,","mjsax","2021-02-17T00:41:11Z","2021-02-19T00:03:58Z"
"","9898","KAFKA-12189 ShellTest can replace 'assumeTrue' by (junit 5) condition…","ShellTest can replace 'assumeTrue' by (junit 5) conditional test  I also test in windows . ``` ShellTest > testEchoHello() SKIPPED ShellTest > testRunProgramWithErrorReturn() SKIPPED ShellTest > testHeadDevZero() SKIPPED ShellTest > testAttemptToRunNonExistentProgram() SKIPPED  ```","closed","","g1geordie","2021-01-14T18:22:43Z","2021-01-15T06:23:43Z"
"","9747","KAFKA-10852: AlterIsr should not be throttled","Set it as a cluster action and update the handler in KafkaApis. We keep the throttleTimeMs field since we intend to enable throttling in the future (especially relevant when we switch to the built-in quorum mode).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-12-14T15:01:38Z","2020-12-15T06:38:45Z"
"","10246","MINOR: add 'toString' to classes which are used by logging","see following links for `toString` references  BrokerHeartbeatState https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/BrokerHeartbeatManager.java#L188  QuorumState https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java#L629  MemberToRemove https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/RemoveMembersFromConsumerGroupResult.java#L86  InsertionSpec https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/InsertField.java#L122  Field https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/protocol/types/TaggedFields.java#L143  FieldSpec https://github.com/apache/kafka/blob/trunk/generator/src/main/java/org/apache/kafka/message/Target.java#L61  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-03-02T10:11:06Z","2021-05-28T19:00:35Z"
"","9878","KAFKA-6987: Add KafkaFuture.toCompletionStage()","See [KAFKA-6987](https://issues.apache.org/jira/browse/KAFKA-6987) and [KIP-707](https://cwiki.apache.org/confluence/display/KAFKA/KIP-707%3A+The+future+of+KafkaFuture)","closed","","tombentley","2021-01-13T15:39:21Z","2021-07-05T16:33:36Z"
"","9735","KAFKA-10846: Grow buffer in FileSourceStreamTask only when needed","See [KAFKA-10846](https://issues.apache.org/jira/browse/KAFKA-10846)","closed","","tombentley","2020-12-11T15:19:30Z","2020-12-18T08:54:18Z"
"","9643","MINOR: Upgrade to Scala 2.13.4","Scala 2.13.4 restores default global `ExecutionContext` to 2.12 behavior (to fix a perf regression in some use cases) and improves pattern matching (especially exhaustiveness checking). Most of the changes are related to the latter as I have enabled the newly introduced `-Xlint:strict-unsealed-patmat`.  More details on the code changes: * Don't swallow exception in `ReassignPartitionsCommand.topicDescriptionFutureToState`. * `RequestChannel.Response` should be `sealed`. * Introduce sealed ClientQuotaManager.BaseUserEntity to avoid false positive exhaustiveness warning. * Handle a number of cases where pattern matches were not exhaustive: either by marking them with @unchecked or by adding a catch-all clause. * Workaround scalac bug related to exhaustiveness warnings in ZooKeeperClient * Remove warning suppression annotations related to the optimizer that are no longer needed in ConsumerGroupCommand and AclAuthorizer. * Use `forKeyValue` in `AclAuthorizer.acls` as the scala bug preventing us from using it seems to be fixed. * Also update scalaCollectionCompat to 2.3.0, which includes minor improvements.  Full release notes: * https://github.com/scala/scala/releases/tag/v2.13.4 * https://github.com/scala/scala-collection-compat/releases/tag/v2.3.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-11-23T15:46:29Z","2020-11-24T18:29:00Z"
"","10442","MINOR: update getMagic java doc","Saw this java doc error while tracing codes. We forgot to update it while updating the method before: https://github.com/apache/kafka/pull/2657/files#diff-d3539714f9c04947365bf02a4dac4183aaba88e70d83b455bcb80207339ddb96R665  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-30T13:56:02Z","2021-04-06T07:51:01Z"
"","10268","MINOR: output meaningful error message","Saw the test failed messages [here](https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk11/565/testReport/junit/org.apache.kafka.connect.mirror.integration/MirrorConnectorsIntegrationTest/testReplication__/): ``` java.lang.AssertionError: Connector MirrorCheckpointConnector tasks did not start in time on cluster: org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster@11eea3c4 ```  It's not helpful for debugging because we cannot know this is which cluster: `EmbeddedConnectCluster@11eea3c4`. Improve it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-05T03:12:45Z","2021-03-12T12:14:53Z"
"","10187","MINOR: document restriction against running multiple Streams apps on same state.dir","Running more than one Streams instance on the same physical state directory has never been supported, but until now it's also not really been enforced. Since we fixed Streams to fail fast in the case of this misconfiguration instead of offering vague symptoms, we will throw an exception on startup if it is detected. We should document this clearly as some users may have been using this setup for testing (note: there's no reason to do this in a real production app, as you should either scale up by adding more threads and/or scale out by adding new instances with their own storage)","closed","","ableegoldman","2021-02-23T04:05:33Z","2021-02-23T16:40:33Z"
"","9564","KAFKA-10667: add timeout for forwarding requests","Right now the forwarding request will retry indefinitely, which is not the ideal behavior. We should timeout the enqueued request when it hits the request timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-05T07:53:51Z","2020-12-08T19:12:31Z"
"","10119","[DO NOT SQUASH]: revert KIP-695","Reverts both commits in KIP-695, which is being postponed until after 2.8  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","vvcephei","2021-02-12T06:28:22Z","2021-02-12T23:14:19Z"
"","9728","MINOR: make sure all dir jobs are completed","revert the change introduced by #9680 and add a unit test to verify  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-10T17:25:43Z","2021-01-06T06:52:16Z"
"","9751","Revert ""MINOR: fix typo in `AbstractIndex.scala` (#9745)""","revert my rough merge :(  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-15T00:39:10Z","2020-12-15T00:39:59Z"
"","10081","MINOR: revert back the intent","Revert back the accidentally removed intent: https://github.com/apache/kafka/pull/10002/files#diff-3485b37e32662f4925ee0374c4f6eb1d4dfa589bbb1c148b2a70e92b4acbe8bdR370  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-08T09:37:36Z","2021-02-08T12:46:48Z"
"","10369","KAFKA-12514: Fix NPE in SubscriptionState","Return null for partitionLag if there is no current position. This was the desired semantics, the lack of the check was an oversight.      Patches: KIP-695 Patches: a92b986c855592d630fbabf49d1e9a160ad5b230  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-03-21T02:33:27Z","2021-03-22T02:05:00Z"
"","10111","MINOR: Add BrokerMetadataListener","Replaces #10090   This adds BrokerMetadataListener which is responsible for processing metadata records received by the broker when running in Raft mode.  This also moves some classes that were added to the wrong folder in trunk","closed","kip-500,","mumrah","2021-02-11T20:07:57Z","2021-02-11T20:43:22Z"
"","10344","MINOR: Remove use of NoSuchElementException","Replace the use of the method `last` and `first` in `ConcurrentSkipListSet` with the descending and ascending iterator respectively. The methods `last` and `first` throw an exception when the set is empty this causes poor `KafkaRaftClient` performance when there aren't any snapshots.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-03-17T23:24:51Z","2021-03-18T18:09:47Z"
"","9642","MINOR: Remove redundant argument from TaskMetricsGroup#recordCommit","Replace the boolean value success with Throwable error.","closed","","UnityLung","2020-11-23T14:46:39Z","2020-11-24T07:02:57Z"
"","10385","KAFKA-12420: Kafka network Selector class has many constructors; use a Builder pattern instead","Replace the 6 network selector constructors with a builder that can be used to specify the needed parameters instead of using one of the 6 defined constructors.   This pull requests updates all usages of the network selector and does not add any tests. Existing tests already tests the correctness of the network selector and our solution only changes the instance creation of the selector. All tests ran correctly..  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","julian9499","2021-03-23T15:41:57Z","2021-03-29T13:52:39Z"
"","10382","KAFKA-12420: Kafka network Selector class has many constructors; use a Builder pattern instead","Replace the 6 network selector constructors with a builder that can be used to specify the needed parameters instead of using one of the 6 defined constructors.   This pull requests updates all usages of the network selector and does not add any tests. Existing tests already tests the correctness of the network selector and our solution only changes the instance creation of the selector. All tests ran correctly..  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","julian9499","2021-03-23T15:25:32Z","2021-03-23T15:41:05Z"
"","10084","MINOR: Rename DecommissionBrokers to UnregisterBrokers","Rename DecommissionBrokers to UnregisterBrokers.  Fix an incorrect JavaDoc comment for the Admin API.  Make sure that UNREGISTER_BROKER is marked as forwardable and not as a controller-only API (since it can received by brokers).","closed","kip-500,","cmccabe","2021-02-08T21:48:45Z","2021-02-10T20:44:48Z"
"","9491","KAFKA-10623: Refactor code to avoid discovery conflicts for admin.VersionRange","Rename admin.VersionRange into admin.Versions to avoid discovery conflicts  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","warrenzhu25","2020-10-24T02:08:49Z","2020-10-24T02:08:49Z"
"","9720","KAFKA-10555: Improve client state machine","Removes the transition to error for when there are no threads and makes Error terminal  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2020-12-09T20:52:06Z","2021-01-26T19:54:40Z"
"","10411","KAFKA-7606: Remove deprecated options from StreamsResetter","Removes deprecated flags from StreamsResetter:  ""--zookeeper"" flag (deprecated in 1.0 via KIP-198) ""--execute"" flag (deprecated in 1.1 via KIP-171)","closed","streams,","ableegoldman","2021-03-26T05:06:23Z","2021-04-13T01:24:19Z"
"","9853","MINOR: Remove unnecessary semicolon in NetworkClient","remove unnecessary semicolon  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-01-10T11:04:19Z","2021-01-11T05:27:20Z"
"","10358","MINOR: Remove redundant semicolon","Remove redundant semicolon  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-03-19T10:07:11Z","2021-03-21T16:43:03Z"
"","10471","KAFKA-12597: remove deprecated zookeeper option in ReassignPartitionsCommand","remove deprecated `--zookeeper` option in ReassignPartitionsCommand. And also the zookeeper dependent methods, and the tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-04T03:13:53Z","2021-06-04T06:33:26Z"
"","10416","KAFKA-8784: remove default close for RocksDBConfigSetter","remove default  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-03-26T16:53:56Z","2021-03-29T21:27:56Z"
"","10457","KAFKA-12596: remove --zookeeper option from topic command","Remove `ZookeeperTopicService` and the test using zookeeper `TopicCommandWithZKClientTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","showuon","2021-04-01T09:24:11Z","2021-06-02T10:03:49Z"
"","10166","MINOR: update the memberMetadata output","Remove 1 redundant comma at the end.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-20T09:31:37Z","2021-02-23T04:51:06Z"
"","9891","MINOR: Remind user index file is empty when dumping LogSegment index file","Remind user index file is empty when dumping LogSegment xxx.index file instead of throwing an `NoSuchElementException`.  BEFORE ![image](https://user-images.githubusercontent.com/18420212/104580643-79001b80-5698-11eb-92ff-5eac5e4423c8.png)  AFTER ![image](https://user-images.githubusercontent.com/18420212/104580756-a0ef7f00-5698-11eb-98fa-3bdb364e8062.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","17hao","2021-01-14T10:47:17Z","2021-01-21T04:06:44Z"
"","10292","MINOR: fix client_compatibility_features_test.py","related to https://github.com/apache/kafka/pull/10105  kafka nodes are set to **older version** so resolved script path is linked to **older assembly**. the 0.10.x versions do not have `org.apache.kafka.tools.ClientCompatibilityTest` so the test gets failed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-10T15:43:02Z","2021-03-17T17:30:53Z"
"","9691","HOTFIX: fix failed build caused by StreamThreadTest","related to https://github.com/apache/kafka/commit/4cc6d204ec5cb21841a2f8f21e6333bb688cd892  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-04T10:41:38Z","2020-12-04T18:55:56Z"
"","9935","HOTFIX: fix RocksDBMetricsTest","related to add160d522d8ce6ce70595bd76196e7f42a21435  The description is changed so mock can't match the expected value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-20T03:44:02Z","2021-01-20T18:24:12Z"
"","10164","MINOR: remove unused org.apache.kafka.streams.processor.internals.Res…","related to #7997  `org.apache.kafka.streams.processor.internals.RestoringTasks` is a internal class so it should be fine to remove it directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-02-20T08:09:19Z","2021-02-20T08:09:19Z"
"","10146","HOTFIX: fix Scala 2.12 build error caused by ControllerApisTest.scala","related to #10113  ### Error Message  ` 14:01:06 > Task :core:compileTestScala 14:01:06 [Error] /home/jenkins/agent/workspace/LoopTest-Kafka/kafka/core/src/test/scala/unit/kafka/server/ControllerApisTest.scala:70: the result type of an implicit conversion must be more specific than Object `  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","chia7712","2021-02-18T07:45:56Z","2021-02-23T04:10:42Z"
"","10056","KAFKA-12293: remove JCenter and Bintray repositories mentions out of build gradle scripts (sunset is announced for those repositories)","Related jira ticket:  https://issues.apache.org/jira/browse/KAFKA-12293 **_Remove JCenter and Bintray repositories mentions out of Gradle build (sunset is announced for those repositories)_**  Some related external links: - https://jfrog.com/blog/into-the-sunset-bintray-jcenter-gocenter-and-chartcenter - https://www.infoq.com/news/2021/02/jfrog-jcenter-bintray-closure  Note: at the moment I am not sure about Kafka releasing process (i.e. publishing to global public repositories). If Kafka exclusively  publishes to JCenter then this solution should be expanded. `update: could not find release process related gradle parts`  FYI @ijuma","closed","","dejan2609","2021-02-04T20:57:09Z","2022-01-11T07:15:54Z"
"","10010","MINOR: AbstractConfigTest.testClassConfigs should reset the class loa…","reference to https://github.com/apache/kafka/pull/10006#issuecomment-770394615  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-01T03:09:40Z","2021-02-02T07:33:30Z"
"","10158","KAFKA-12343: Handle exceptions better in TopicAdmin, including UnsupportedVersionException","Refactored the KafkaBasedLog logic to read end offsets into a separate method to make it easier to test. Also changed the TopicAdmin.endOffsets method to throw the original UnsupportedVersionException, LeaderNotAvailableException, and TimeoutException rather than wrapping, to better conform with the consumer method and how the KafkaBasedLog retries those exceptions.  Added new tests to verify various scenarios and errors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2021-02-19T02:48:01Z","2021-02-19T20:43:32Z"
"","9906","KAFKA-10885 Refactor MemoryRecordsBuilderTest/MemoryRecordsTest to avoid a lot of…","Refactor MemoryRecordsBuilderTest/MemoryRecordsTest to avoid a lot of  ignored test cases  I add different Parameter Source to prevent assume method.  if method uses the source only itself ,  I use the `MethodSource`  with default name (same as test name) The others I specify the provider as before","closed","","g1geordie","2021-01-15T09:34:23Z","2021-02-18T03:15:56Z"
"","10118","KAFKA-10192: Increase max time to wait for worker to start in some integration tests","Recently, we saw some errors:  `org.opentest4j.AssertionFailedError: Condition not met within timeout 30000. Worker did not complete startup in time ==> expected:  but was: `  And after some tests, I confirmed this is just because the slow system, not other issues. What I did is trying to wait 2 times, to see if we failed at 1st time, and passed at 2nd. And after testing in jenkins build, the errors are showing: **failed 1st, but passed in 2nd try**. So, we can just increase the waiting time to fix these flaky tests.  ```java         boolean fail = false;         try {             waitForCondition(                 () -> connect.requestGet(connect.endpointForResource(""connectors/nonexistent"")).getStatus() == 404,                 CONNECT_WORKER_STARTUP_TIMEOUT,                 ""Worker did not complete startup in time""             );         } catch (final java.lang.AssertionError e) {             fail = true;         }           waitForCondition(             () -> connect.requestGet(connect.endpointForResource(""connectors/nonexistent"")).getStatus() == 404,               CONNECT_WORKER_STARTUP_TIMEOUT               ""Worker did not complete startup in time""         );              if (fail) {             fail(""failed 1st, but passed in 2nd try"");         } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-12T05:58:44Z","2021-03-09T17:03:41Z"
"","10002","MINOR: remove the indent in security doc","Reading the security doc recently, and one thing annoys me: the long indent in front of each command, ex:   ![image](https://user-images.githubusercontent.com/43372967/106253151-155a1e80-6252-11eb-97f7-e8f4f60c6047.png)   ![image](https://user-images.githubusercontent.com/43372967/106253176-1be89600-6252-11eb-845b-b8e478534fd7.png)   ![image](https://user-images.githubusercontent.com/43372967/106253249-3589dd80-6252-11eb-82e1-45fe188b26d6.png)  https://kafka.apache.org/documentation/#security_ssl_key  Removing all the prefix indent in the commands. I'll create another PR to kafka-site if this change accepts. Thanks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-29T08:52:22Z","2021-02-06T14:21:21Z"
"","9749","Only schedule AlterIsr thread when we have an ISR change","Rather than scheduling every 50ms to check for unsent updates, we should schedule the propagation thread only after we receive ISR updates","closed","","mumrah","2020-12-14T19:53:25Z","2021-01-13T18:52:39Z"
"","9496","MINOR: fix error in quota_test.py system tests","quota_test.py tests are failing with below error.  ``` 23:24:42 [INFO:2020-10-24 17:54:42,366]: RunnerClient: kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=False: FAIL: not enough arguments for format string 23:24:42 Traceback (most recent call last): 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/tests/runner_client.py"", line 134, in run 23:24:42     data = self.run_test() 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/tests/runner_client.py"", line 192, in run_test 23:24:42     return self.test_context.function(self.test) 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/lib/python3.6/site-packages/ducktape-0.8.0-py3.6.egg/ducktape/mark/_mark.py"", line 429, in wrapper 23:24:42     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs) 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 141, in test_quota 23:24:42     self.quota_config = QuotaConfig(quota_type, override_quota, self.kafka) 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 60, in __init__ 23:24:42     self.configure_quota(kafka, self.producer_quota, self.consumer_quota, ['users', None]) 23:24:42   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 83, in configure_quota 23:24:42     (kafka.kafka_configs_cmd_with_optional_security_settings(node, force_use_zk_conection), producer_byte_rate, consumer_byte_rate) 23:24:42 TypeError: not enough arguments for format string 23:24:42  ```  ran thee tests locally. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-10-24T20:08:36Z","2020-10-25T09:15:32Z"
"","10300","KAFKA-12452: Remove deprecated overloads of ProcessorContext#forward","ProcessorContext#forward was changed via KIP-251 in 2.0.0 release. This PR removes the old and deprecated overloads.  Call for review @guozhangwang @ableegoldman","closed","streams,","mjsax","2021-03-11T03:06:04Z","2021-03-26T02:49:23Z"
"","9850","KAFKA-12168; Move envelope request parsing out of SocketServer","Prior to this patch, envelope handling was a shared responsibility between `SocketServer` and `KafkaApis`.  The former was responsible for parsing and validation, while the latter was responsible for authorization. This patch consolidates logic in `KafkaApis` so that envelope requests follow the normal request flow.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-01-08T21:57:32Z","2021-08-11T13:16:06Z"
"","9548","Disable inlining of Scala library methods.","Previously, the Scala compiler was configured to inline all usages of symbols in under the `scala` package. This is problematic because it means that published Kafka jars must run with the exact same version of the Scala library that the Kafka jar was compiled with.  If the runtime uses a different version of the Scala library then users risk getting a crash like this:  ``` java.lang.NoClassDefFoundError: scala/math/Ordering$$anon$7 ```  This commit disables inlining from the `scala` package to prevent crashes like this. The downside to this change is it may introduce performance regressions.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","olafurpg","2020-11-03T14:23:33Z","2021-02-22T14:39:42Z"
"","10395","KAFKA-12432: AdminClient should time out nodes that are never ready","Previously, if we assigned one or more calls to a remote node, but it never became available, we would block until the calls hit their the API timeout.  This was particularly unfortunate in the case where the calls could have been sent to a different node in the cluster.  This PR fixes this behavior by timing out pending connections to remote nodes if they take longer than the request timeout.  There are a few other small cleanups in this PR: it removes the unecessary Call#aborted, sets Call#curNode to null after the call has failed to avoid confusion when debugging or logging, and adds a ""closing"" boolean rather than setting newCalls to null when the client is closed.  Also, it increases the log level of the log message that indicates that we timed out some calls because AdminClient closed. Finally, it simplifies the type of callsInFlight.","closed","","cmccabe","2021-03-24T15:46:20Z","2021-03-24T15:59:22Z"
"","10281","KAFKA-12432: AdminClient should time out nodes that are never ready","Previously, if we assigned one or more calls to a remote node, but it never became available, we would block until the calls hit their the API timeout.  This was particularly unfortunate in the case where the calls could have been sent to a different node in the cluster.  This PR fixes this behavior by timing out pending connections to remote nodes if they take longer than the request timeout.  There are a few other small cleanups in this PR: it removes the unecessary Call#aborted, sets Call#curNode to null after the call has failed to avoid confusion when debugging or logging, and adds a ""closing"" boolean rather than setting newCalls to null when the client is closed.  Also, it increases the log level of the log message that indicates that we timed out some calls because AdminClient closed.","closed","","cmccabe","2021-03-08T22:36:11Z","2021-03-24T18:10:25Z"
"","9947","KAFKA-12190: Fix setting of file permissions on non-POSIX filesystems","Previously, `StateDirectory` used `PosixFilePermissions` to configure its directories' permissions which fails on Windows as its file system is not POSIX-compliant. This PR updates `StateDirectory` to fall back to the `File` API on non-POSIX-compliant file systems. The File API doesn't allow as much control over the permissions so they're as close as the API permits.  The unit tests have been updated to also verify the behaviour on non-POSIX-compliant file systems.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","wilkinsona","2021-01-21T19:27:22Z","2022-04-28T12:02:58Z"
"","9802","KAFKA-10894; Ensure PartitionInfo replicas are not null in client quota callback","Previously offline replicas were included as `null` in the array of replicas in `PartitionInfo` when populated by the `MetadataCache` for the purpose of the client quota callback. This patch instead initializes empty non-null nodes, which is consistent with how `PartitionInfo` is constructed by the clients in `MetadataResponse`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-30T22:44:30Z","2021-01-05T21:02:56Z"
"","9948","MINOR: fix record time in shouldWipeOutStandbyStateDirectoryIfCheckpointIsMissing","Prevents cleaner thread from occasionally deleting segments  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","wcarlson5","2021-01-21T22:34:32Z","2021-01-23T00:42:28Z"
"","10365","MINOR: bump version in upgrade guide to 2.6.2","Prepare for 2.6.2 release","closed","","ableegoldman","2021-03-19T23:35:05Z","2021-03-19T23:35:38Z"
"","9690","KAFKA-10017: fix flaky EOS-beta upgrade test","PR for `2.6` branch. ""Main"" PR for `trunk` and `2.7` is #9688  The difference is, that in `2.6` and eos-alpha, we commit tasks individually, while in `2.7` and eos-alpha, if one tasks needs a commit we commit all tasks.  Call for review @abbccdda @ableegoldman @guozhangwang","closed","tests,","mjsax","2020-12-04T09:37:52Z","2021-02-23T03:19:47Z"
"","10060","KAFKA-10716: persist UUID in state directory for stable processId across restarts - 2.7","Port of https://github.com/apache/kafka/pull/9978 to the 2.7 branch","closed","","ableegoldman","2021-02-04T23:31:30Z","2021-02-05T20:20:37Z"
"","9823","KAFKA-10903:Optimize producerBatch order performance","please. see https://issues.apache.org/jira/browse/KAFKA-10903, as we konw,if we need sort  the producerBatch by sequence,now we use the new batch compare with the first batch in deque, and if the first batch in deque is less than the new batch,we will loop the deque and let the new batch insert to the right position.and if the new batch is more than the last producerBatch in the deque,we just need add the new batchProducer to the last position in the deque,most of time the deque not exists the mixed producerBatch(has sequence and not has the sequence),so i think this optimize make sense,thank you review ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huangyiminghappy","2021-01-05T14:37:52Z","2021-01-20T00:00:04Z"
"","9614","KAFKA-10500: Add failed-stream-threads metric for adding + removing stream threads","Per [KIP-663](https://cwiki.apache.org/confluence/display/KAFKA/KIP-663%3A+API+to+Start+and+Shut+Down+Stream+Threads), adding a metric to record the failed streams threads over the life of a client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lct45","2020-11-18T18:12:29Z","2021-01-27T02:57:48Z"
"","10072","KAFKA-9274: Throw TaskCorruptedException instead of TimeoutException when TX commit times out","Part of KIP-572: follow up work to PR #9800. It's not save to retry a TX commit after a timeout, because it's unclear if the commit was successful or not, and thus on retry we might get an IllegalStateException. Instead, we will throw a TaskCorruptedException to retry the TX if the commit failed.  Call for review @ableegoldman @vvcephei @abbccdda @guozhangwang","closed","kip,","mjsax","2021-02-05T23:44:50Z","2021-02-19T21:36:11Z"
"","9877","KAFKA-8460: produce records with current timestamp","Originally, we make sure consumer `awaitAssignment`, and then produce records. We total send 30 records to 3 topics, and each topic has 30 partitions, so it takes some time to process it. If it exceeds 6 seconds, it'll make the consumer left due to the `max.poll.interval.ms` is set to 6 secs. And then it will delete the logs due to the record timestamp we set is 0, which will be greater than retentionMs setting (by currentTime - 0). After log deletion, we'll increment the start offset. So, later when the consumer is back, we'll first listOffset, and got the new and unexpected start offset. That's why we sometimes cannot receive expected amount of records.   There are many ways to fix this issue, ex: increment the `max.poll.interval.ms`, send the records with the current timestamp... I fixed it by making producing records before `awaitAssignment`, and then, I make sure we consume records right after awaitAssignemnt. This will make this test much more reliable.  The numRecords changed to `partitionCount` because the partitionCount is the original numRecords. And I tried to reduce the records number months ago, and still found it failed. So that's why I deep dive this flaky test. :) So, I just revert the previous change.  PS. this flaky test investigation takes me a lot of time, and finally found the root cause! Yeah~  [UPDATED] after discussion with reviewer, we decided to produce records with current timestamp to avoid breaking original test purpose.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-13T10:03:50Z","2021-01-20T07:48:45Z"
"","9941","MINOR: further reduce StreamThread loop INFO logging to 2min summary","One more try to get the logging levels right..the only way to log something within the main StreamThread loop without absolutely flooding the logs at a level that isn't appropriate for INFO is to just set some kind of interval to stick to. I chose to log the summary every 2 min, since this is long enough to prevent log spam but short enough to fit at least 2 summaries within the (default) poll interval of 5 min","closed","","ableegoldman","2021-01-20T23:48:51Z","2021-01-22T00:40:31Z"
"","9585","MINOR: commit method doesn't exists for Consumer, but `commitSync` does.","One liner fix for Javadoc. `commit` method doesn't exist for `KafkaConsumer`, we have `commitSync` instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2020-11-11T08:14:16Z","2020-11-11T12:03:10Z"
"","10260","KAFKA-12409: Leaking gauge in ReplicaManager","One gauge is missing in `ReplicaManager#removeMetrics`.","closed","","dongjinleekr","2021-03-04T07:25:41Z","2021-03-04T22:29:38Z"
"","10322","KAFKA-12455: OffsetValidationTest.test_broker_rolling_bounce fail: Raft","OffsetValidationTest.test_broker_rolling_bounce was failing when used with a Raft-based metadata quorum but succeeding with a ZooKeeper-based quorum.  This patch increases the consumers' session timeouts to 30 seconds, which fixes the Raft case and also eliminates flakiness that has historically existed in the Zookeeper case.  This patch also fixes a minor logging bug in `RaftReplicaManager.endMetadataChangeDeferral()` that was discovered during the debugging of this issue, and it adds an extra logging statement in `RaftReplicaManager.handleMetadataRecords()` when a single metadata batch is applied to mirror the same logging statement that occurs when deferred metadata changes are applied.  In the Raft system test case the consumer was sometimes receiving a METADATA response with just 1 alive broker, and then when that broker rolled the consumer wouldn't know about any alive nodes.  It would have to wait until the broker returned before it could reconnect, and by that time the group coordinator on the second broker would have timed-out the client and initiated a group rebalance.  The test explicitly checks that no rebalances occur, so the test would fail.  It turns out that the reason why the ZooKeeper configuration wasn't seeing rebalances was just plain luck.  The brokers' metadata caches in the ZooKeeper configuration show 1 alive broker even more frequently than the Raft configuration does.  If we tweak the metadata.max.age.ms value on the consumers we can easily get the ZooKeeper test to fail, and in fact this system test has historically been flaky for the ZooKeeper configuration.  We can get the test to pass by setting session.timeout.ms=30000 (which is longer than the roll time of any broker), or we can increase the broker count so that the client never sees a METADATA response with just a single alive broker and therefore never loses contact with the cluster for an extended period of time.  We have plenty of system tests with 3+ brokers, so we choose to keep this test with 2 brokers and increase the session timeout.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-03-15T21:50:52Z","2021-03-16T20:57:30Z"
"","9961","MINOR: remove unused code from MessageTest","noticed this issue when reviewing #9960  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-25T07:07:29Z","2021-01-26T08:27:38Z"
"","9875","MINOR: tighten condition for logging StreamThread summary to avoid spam under low traffic","Noticed these messages flooding the logs on several integration tests. Since it doesn't really provide any useful information, we may as well skip the logging when there are no new records to be processed.  Should be cherrypicked back to the 2.7 branch","closed","","ableegoldman","2021-01-12T20:09:36Z","2021-01-13T22:37:53Z"
"","9510","Add additional MetadataRecordType enums","Note that I had to remove `""mapKey"": true` from `FeatureLevelRecord.json` because with it I was seeing this stack trace during the build:  ``` Exception in thread ""main"" java.lang.RuntimeException: Exception while processing src/main/resources/common/metadata/FeatureLevelRecord.json         at org.apache.kafka.message.MessageGenerator.processDirectories(MessageGenerator.java:233)         at org.apache.kafka.message.MessageGenerator.main(MessageGenerator.java:351) Caused by: java.lang.RuntimeException: Cannot set mapKey on top level fields.         at org.apache.kafka.message.MessageDataGenerator.generateClass(MessageDataGenerator.java:87)         at org.apache.kafka.message.MessageDataGenerator.generate(MessageDataGenerator.java:67)         at org.apache.kafka.message.MessageDataGenerator.generateAndWrite(MessageDataGenerator.java:55)         at org.apache.kafka.message.MessageGenerator.processDirectories(MessageGenerator.java:225) ```","closed","","rondagostino","2020-10-27T14:19:04Z","2020-10-27T14:19:35Z"
"","9801","MINOR: code cleanup for Kafka Streams task interface","Not functional change. Pure code cleanup.  Call for review @vvcephei (cf. #8852 -- will split up the old PR into multiple smaller ones to simplify reviewing)","closed","streams,","mjsax","2020-12-30T22:16:37Z","2021-01-06T22:01:39Z"
"","9698","KAFKA-10811: Correct the MirrorConnectorsIntegrationTest to correctly mask the exit procedures","Normally the `EmbeddedConnectCluster` class masks the `Exit` procedures using within the Connect worker. This normally works great when a single instance of the embedded cluster is used. However, the `MirrorConnectorsIntegrationTest` uses two `EmbeddedConnectCluster` instances, and when the first one is stopped it would reset the (static) exit procedures, and any problems during shutdown of the second embedded Connect cluster would cause the worker to shut down the JVM running the tests.  Instead, the `MirrorConnectorsIntegrationTest` class should mask the `Exit` procedures and instruct the `EmbeddedConnectClusters` instances (via the existing builder method) to not mask the procedures.  Ideally this should also be backported to `2.7`, `2.6`, and `2.5` branches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2020-12-04T23:51:48Z","2020-12-07T15:34:35Z"
"","9485","KAKFA-10619: Idempotent producer will get authorized once it has a WRITE access to at least one topic","New stuff implemented: 1. the interface default      a) handle the prefixed and wildcard resources correctly     b) assume allow.anyone is false. so no ACL entry means no access     c) handle the existing ACL entries with wildcard principle, host, and operation correctly 2. the new interface override in AclAuthorizer and     a) handling the prefixed and wildcard resources correctly     b) maintaining the existing behavior wrt allow.anyone value.     c) handling the existing ACL entries with wildcard principle, host, and operation     d) optimized on performance with new indexing on ACE besides on the existing ResourcePattern. 3. InitProducerIdRequest and ProduceRequest will be authorized once the producer has a WRITE access to at least one topic. 3. lots of tests with different scenarios, both for the interface default and the overridden methods.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2020-10-23T05:23:25Z","2021-01-08T20:40:01Z"
"","9582","KAFKA-6687: rewrite topology to allow reading the same topic multiple times in the DSL","Needed to fix this on the side in order to more easily set up some experiments, so here's the PR.  Allows a user to create multiple KStreams ~and/or KTables~ from the same topic, collection of topics, or pattern. At the moment this isn't possible since we can only consume from a topic once, and each source topic maps to a single source node in the topology. The ""fix"" is just to rewrite the logical plan and merge any duplicate source nodes into a single node before it gets compiled into the physical topology.  The one exception is when the stream/table are subscribed to an overlapping-but-unequal collection of topics, which I left as future work (with a TODO in the comments describing a possible solution). If the offset reset policy doesn't match we just throw a TopologyException.  edit: tables are much more complicated so I opted to restrict things to just multiple KStreams for now on and consider allowing multiple KTables (or KStream+KTable) as followup work","closed","streams,","ableegoldman","2020-11-10T04:29:23Z","2020-11-18T05:19:36Z"
"","9529","Revert initial principal from 2.7","Need to make sure 2.7 doesn't bump the request header unexpectedly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-10-29T16:30:19Z","2020-10-29T19:41:09Z"
"","9522","MINOR: revert initial principal PR","Need to make sure 2.7 doesn't bump the request header unexpectedly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-10-28T17:48:10Z","2020-10-29T16:28:39Z"
"","10355","KAFKA-12500: fix memory leak in thread cache","Need to exclude threads in `PENDING_SHUTDOWN` from the num live threads computation used to compute the new cache size per thread.  Also adds some logging to help follow what's happening when a thread is added/removed/replaced","closed","","ableegoldman","2021-03-18T20:07:13Z","2021-03-20T01:16:17Z"
"","10007","KAFKA-10700 - Support mutual TLS authentication for SASL_SSL listeners (KIP-684)","mTLS is enabled if listener-prefixed `ssl.client.auth` is configured for SASL_SSL listeners. Broker-wide `ssl.client.auth` is not applied to SASL_SSL listeners as before, but we now print a warning.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-01-31T21:22:54Z","2021-02-02T14:56:24Z"
"","10131","KAFKA-5146 / move kafka-streams example to a new module","Moved ""streams-examples"" to its own module outside kafka-streams module. Because of org.apache.kafka.streams.processor.internals.StateDirectory in kafka-streams module, I had to add the jackson binder dependency. Before the change, It was probably being retrieved as a transitive dependency through ""connect-json"" dependency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","MarcoLotz","2021-02-16T14:03:52Z","2021-04-07T00:48:04Z"
"","9575","MINOR: optimize lock operation","move lock backward.","closed","","dengziming","2020-11-07T03:22:06Z","2022-03-17T13:02:01Z"
"","10018","Upstream MetadataImage and related classes","Mostly a wholesale copy of the files from confluentinc/kip-500, with a few fixes for the test classes","closed","kip-500,","mumrah","2021-02-01T22:11:03Z","2021-02-03T22:17:26Z"
"","10435","KAFKA-12578: Remove deprecated security classes/methods for 3.0","More specifically, remove deprecated: - Constants in SslConfigs - Constants in SaslConfigs - AclBinding constructor - AclBindingFilter constructor - PrincipalBuilder and DefaultPrincipalBuilder classes - ResourceFilter  Also simplify tests and code that no longer have to handle the removed `PrincipalBuilder`.  These removals seem non controversial. There is a straightforward alternative. The deprecations happened in 1.0.0 and 2.0.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-30T07:59:36Z","2021-03-31T05:02:20Z"
"","9794","Add a job to build on ARM64 at Amazon Graviton2 nodes","More and more software development is being done on ARM64 CPU architecture. It would be good if Apache Kafka is being regularly tested on ARM64.  Changes:  * apply the changes from https://github.com/apache/kafka/pull/8489 - `ducker` needs multi-arch Docker image, like `openjdk:11`  * upgrade Pip to 20.3.3 (latest available at the moment).  * explicitly install pynacl to workaround https://github.com/apache/kafka/pull/8489#issuecomment-751998685","closed","","martin-g","2020-12-29T11:29:41Z","2022-05-03T21:04:54Z"
"","9694","MINOR: Change the input type of DescribeAclsResponse#aclsResources from Collection to Iterable to avoid creating useless collection","Modify DescribeAclsResponse, remove Collection and change to Iterable, Modify KafkaApis, aclsResources method","open","","chris8099","2020-12-04T19:37:37Z","2020-12-09T15:44:31Z"
"","10418","KAFKA-12509 Tighten up StateDirectory thread locking","Modified LockAndOwner class to have Thread reference instead of just name  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ketulgupta1995","2021-03-27T14:51:11Z","2021-03-30T19:13:59Z"
"","10217","KAFKA-12254: Ensure MM2 creates topics with source topic configs","MM2 creates new topics on the destination cluster with default configurations. It has an async periodic task to refresh topic configurations from the source to destination. However, this opens up a window where the destination cluster has data produced to it with default configurations. In the worst case, this could cause data loss if the destination topic is created without the right `cleanup.policy`.  This patch fixes the above issue by ensuring that the right configurations are supplied to `AdminClient#createTopics` when MM2 creates topics on the destination cluster.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2021-02-26T05:20:48Z","2021-03-01T15:58:09Z"
"","9537","MINOR: ApiKey DESCRIBE_QUORUM missing in parseRequest","Missing the ApiKey `DESCRIBE_QUORUM` in `AbstractRequest.parseRequest`.","closed","","anatasiavela","2020-10-30T07:08:49Z","2020-10-30T10:37:45Z"
"","9757","MINOR: Fix bad logging substitution in `AbstractCoordinator`","Missed this in #9729. Caught by @ijuma .  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-15T23:11:57Z","2020-12-15T23:51:38Z"
"","10242","MINOR: format the revoking active log output","Missed one new line symbol.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-03-02T06:38:23Z","2021-03-02T09:32:21Z"
"","9786","MINOR: remove unnecessary code AND Type clarification","MINOR: remove unnecessary code AND Type clarification  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","highluck","2020-12-24T08:03:31Z","2021-04-28T09:03:26Z"
"","9745","MINOR: fix log package `AbstractIndex.scala` file comment typo","MINOR: fix log package `AbstractIndex.scala` file comment typo","closed","","panguncle","2020-12-14T04:57:29Z","2020-12-15T00:39:30Z"
"","10313","KAFKA-12456: Log ""Listeners are not identical across brokers"" message at WARN/INFO instead of ERROR","Minor modification suggested in the following issue: https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-12456?filter=addedrecently  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","AsWali","2021-03-13T12:00:27Z","2021-03-17T23:46:57Z"
"","10444","HOTFIX: move rebalanceInProgress check to skip commit during handleCorrupted","Minor followup to https://github.com/apache/kafka/pull/10407 -- we need to extract the `rebalanceInProgress` check down into the `commitAndFillInConsumedOffsetsAndMetadataPerTaskMap` method which is invoked during `handleCorrupted`, otherwise we may attempt to commit during a a rebalance which will fail","closed","streams,","ableegoldman","2021-03-30T23:55:49Z","2021-03-31T02:01:43Z"
"","9542","MINOR: Update trunk docs to match site docs","Migration of changes for AK site upgrade done on site but not migrated to AK trunk  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-11-02T23:32:55Z","2020-11-02T23:41:23Z"
"","10381","KAFKA-8410: Migrating stateless operators to new Processor API","Migrating KStream stateless operators to new Processor API, first. Following PRs will complete migration of KStream  stateful operators and KTable.  Testing strategy: Keep the current tests green.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-03-23T12:48:27Z","2021-04-12T18:39:32Z"
"","9785","KAFKA-10887 Migrate log4j-appender module to JUnit 5","Migrate log4j-appender module to JUnit 5","closed","","g1geordie","2020-12-24T07:13:54Z","2021-01-10T16:48:59Z"
"","9926","KAFKA-12175 Migrate generator module to junit5","Migrate generator module to junit5.","closed","","bertber","2021-01-18T17:50:13Z","2021-02-03T00:41:01Z"
"","10028","Replace BrokerStates.scala with BrokerState.java","Making BrokerState a straightforward Java enum makes it easier to use from Java code.  The existing BrokerState class is also awkward to use because it is mutable (it holds a volatile variable inside).  While there is also an immutable case class, there is no way to go from a numeric type to the case class.","closed","kip-500,","cmccabe","2021-02-02T23:07:24Z","2021-02-03T21:41:39Z"
"","10356","KAFKA-12503: cache size set by the thread","Make is so threads do not directly resize other threads caches  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-03-18T21:16:14Z","2021-04-03T01:50:54Z"
"","9775","KAFKA-8460: reduce the record size and increase the delay time","Looking into this flaky test, the error messages are: ``` Timed out before consuming expected 1350 records. The number consumed was 1230. ``` https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/303/testReport/kafka.api/PlaintextConsumerTest/testLowMaxFetchSizeForRequestAndPartition/ ``` Timed out before consuming expected 1350 records. The number consumed was 1200. ``` https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/305/testReport/kafka.api/PlaintextConsumerTest/testLowMaxFetchSizeForRequestAndPartition/ ``` Timed out before consuming expected 1350 records. The number consumed was 1215. ``` https://ci-builds.apache.org/job/Kafka/job/kafka-trunk-jdk8/305/testReport/junit/kafka.api/PlaintextConsumerTest/testLowMaxFetchSizeForRequestAndPartition/  We can see, the number consumes are not fixed number and close to 1350. After checking the test, I found the test is expected to be slow because it tests `we can consume all partitions if fetch.max.bytes and max.partition.fetch.bytes are low`. So I think the test has no bug, just need more time.   What I did are: 1. reduce the record size for each partition (from 15 -> 10), it should speed up the test, but also be able to test the original scenario 2. increase the timeout value (from 60 sec -> 90 sec)  Hope this can makes the test more reliable!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-22T02:12:33Z","2021-01-14T03:16:58Z"
"","9680","MINOR: a small refactor for LogManage#shutdown","LogManage Modify  @chia7712","closed","","APaMio","2020-12-03T13:29:54Z","2020-12-10T20:31:55Z"
"","10216","KAFKA-12177: apply log start offset retention before time and size based retention","Log start offset retention is the cheapest retention to evaluate and does not require access to maxTimestamp fields for segments, nor segment sizes. In addition, it may unblock other types of retention such as time based retention. Without this change retention is not idempotent. It's possible for one deleteOldSegments call to delete segments due to log start offset retention, and a follow up call to delete due to time based retention, even if the time has not changed.","closed","","lbradstreet","2021-02-26T05:20:11Z","2021-03-02T21:11:17Z"
"","10390","KAFKA-12536: Add Instant-based methods to ReadOnlySessionStore","KIP-666 implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-03-23T19:25:28Z","2022-04-06T15:05:06Z"
"","10481","KAFKA-12619; Raft leader should expose hw only after committing LeaderChange","KIP-595 describes an extra condition on commitment here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum#KIP595:ARaftProtocolfortheMetadataQuorum-Fetch. In order to ensure that a newly elected leader's committed entries cannot get lost, it must commit one record from its own epoch. This guarantees that its latest entry is larger (in terms of epoch/offset) than any previously written record which ensures that any future leader must also include it. This is the purpose of the `LeaderChange` record which is written to the log as soon as the leader gets elected.  Although we had this check implemented, it was off by one. We only ensured that replication reached the epoch start offset, which does not reflect the appended `LeaderChange` record. This patch fixes the check and clarifies the point of the check. The rest of the patch is just fixing up test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-04-05T22:13:59Z","2021-04-08T17:42:30Z"
"","10282","KAFKA-12426: Missing logic to create partition.metadata files in RaftReplicaManager","KIP-516 introduced partition.metadata file to persist the topic ID on the broker. It is created through handling the LeaderAndIsrRequest in ReplicaManager. (See https://github.com/apache/kafka/pull/10143 for the code path.) RaftReplicaManager was missing the analogue code path for Kip-500 code. Like in ReplicaManager, RaftReplicaManager will now check the partition.metadata file when handling metadata records.  However, since we know that all raft topics will have topic IDs, we can simply set the ID in the log upon the log's creation. Updated the ReplicaManager path to do the same on newly created topics.  There are some tweaks to the checking logic to better handle the scenario when the log exists but is not yet associated to Partition (for example, upon startup after a shutdown).  Tests added to ensure the file is created and that the correct error is thrown when the id is inconsistent. Added tests for creating the log with the new topic ID parameter.  Also adds a few methods to get topic ID from MetadataImageBuilder as this is the most convenient way to get topic ID from RaftReplicaManager.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jolshan","2021-03-08T23:14:51Z","2021-04-06T23:32:22Z"
"","10362","MINOR: Use self-managed mode instead of KIP-500 and nozk","KIP-500 is not particularly descriptive. I also tweaked the readme text a bit.  Tested that the readme for self-managed still works after these changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","ijuma","2021-03-19T14:42:30Z","2021-03-19T23:43:26Z"
"","10445","KAFKA-12548; Propagate record error messages to application","KIP-467 added a field in the produce response to allow the broker to indicate which specific records failed validation. This patch adds the logic to propagate this message up to the application.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-31T01:43:31Z","2021-04-06T22:09:06Z"
"","9744","KAFKA-10062: Add a method to retrieve the current timestamp as known by the Streams app","KIP 622: https://cwiki.apache.org/confluence/display/KAFKA/KIP-622%3A+Add+currentSystemTimeMs+and+currentStreamTimeMs+to+ProcessorContext  jira: https://issues.apache.org/jira/browse/KAFKA-10062  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","rohitrmd","2020-12-13T22:33:06Z","2021-03-11T00:09:32Z"
"","9526","KAFKA-10525: Emit JSONs with new auto-generated schema","Kafka’s request and response traces currently output in a format that is JSON-like and are not easily parsable. There is a new auto-generated schema for each request type that supports outputting JSON payloads for request and response payloads. These can be adapted to provide structured request tracing.  Includes tests that iterate through all the request types and ensure we handle all of them in RequestConvertToJson.  [KIP-673](https://cwiki.apache.org/confluence/display/KAFKA/KIP-673%3A+Emit+JSONs+with+new+auto-generated+schema)","closed","","anatasiavela","2020-10-29T03:33:59Z","2020-12-15T13:33:37Z"
"","9574","MINOR: KIP-497 Not start Controller2ChannelManager when we are using zookeeper to notify isr change","KafkaServer Start alterIsrChannelManager even when we are using zookeeper to notify isr change.   BrokerToControllerChannelManager will consume some resources when started, so we should optimize them as possible.","closed","","dengziming","2020-11-07T02:26:51Z","2021-01-28T10:01:30Z"
"","9707","KAFKA-10790 Detect/Prevent Deadlock on Producer Network Thread","KafkaProducer.flush method in callback will cause deadlock .  because flush method  wait the future complete ``` java //  KafkaProducer public void flush() {    .....         try {             this.accumulator.awaitFlushCompletion();         } catch (InterruptedException e) {             throw new InterruptException(""Flush interrupted."", e);         } }  //RecordAccumulator     public void awaitFlushCompletion() throws InterruptedException {         try {             for (ProducerBatch batch : this.incomplete.copyAll())                 batch.produceFuture.await();         } finally {             this.flushesInProgress.decrementAndGet();         }     } ```  but  future complete after the callback . ```java //ProducerBatch  private void completeFutureAndFireCallbacks(long baseOffset, long logAppendTime, RuntimeException exception) {      ...         // execute callbacks         for (Thunk thunk : thunks) {             try {                 if (exception == null) {                     RecordMetadata metadata = thunk.future.value();                     if (thunk.callback != null)                         thunk.callback.onCompletion(metadata, null);                 } else {                     if (thunk.callback != null)                         thunk.callback.onCompletion(null, exception);                 }             } catch (Exception e) {                 log.error(""Error executing user-provided callback on message for topic-partition '{}'"", topicPartition, e);             }         }          produceFuture.done();  } ```","open","","g1geordie","2020-12-07T10:22:12Z","2021-01-07T11:54:05Z"
"","10083","MINOR: Remove check for comparision of 3rd IP of kafka.apache.org","kafka.apache.org IP resolution count keeps on varying between 2 and 3, but this should not affect the tests. Removing the test condition which depends on the fact that kafka.apache.org will resolve to 3 different IPs.  As of now, kafka.apache.org is resolving to 2 IPs, resulting in failure of the test condition below: ``` $  dig kafka.apache.org kafka.apache.org.	42	IN	A	95.216.26.30 kafka.apache.org.	42	IN	A	207.244.88.140 ```  ``` org.apache.kafka.clients.ClusterConnectionStatesTest > testMultipleIPsWithUseAll FAILED     java.lang.AssertionError: expected not same         at org.junit.Assert.fail(Assert.java:89)         at org.junit.Assert.failSame(Assert.java:820)         at org.junit.Assert.assertNotSame(Assert.java:799)         at org.junit.Assert.assertNotSame(Assert.java:812)         at org.apache.kafka.clients.ClusterConnectionStatesTest.testMultipleIPsWithUseAll(ClusterConnectionStatesTest.java:285) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","prat0318","2021-02-08T18:02:20Z","2021-02-09T03:20:40Z"
"","10173","KAFKA-9548 Added SPIs and public classes/interfaces introduced in KIP-405 for tiered storage feature in Kafka.","KAFKA-9548 Added SPIs and public classes/interfaces introduced in KIP-405 for tiered storage feature in Kafka.  KIP-405 introduces tiered storage feature in Kafka. With this feature, Kafka cluster is configured with two tiers of storage - local and remote. The local tier is the same as the current Kafka that uses the local disks on the Kafka brokers to store the log segments. The new remote tier uses systems, such as HDFS or S3 or other cloud storages to store the completed log segments. Consumers fetch the records stored in remote storage through the brokers with the existing protocol.  We introduced a few SPIs for plugging in log/index store and remote log metadata store.  This involves two parts 1. Storing the actual data in remote storage like HDFS, S3, or other cloud stroages. 2. Storing the metadata about where the remote segments are stored. The default implementation uses an internal Kafka topic.  You can read KIP for more details at https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-02-22T14:28:39Z","2021-03-30T07:50:31Z"
"","10271","KAFKA-12429: Added serdes for the default implementation of RLMM based on an internal topic as storage.","KAFKA-12429:  Added serdes for the default implementation of RLMM based on an internal topic as storage. This topic will receive events of RemoteLogSegmentMetadata, RemoteLogSegmentUpdate, and RemotePartitionDeleteMetadata. These events are serialized into Kafka protocol message format. Added tests for all the event types for that topic.  This is part of the tiered storaqe implementation [KIP-405](https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage#KIP405:KafkaTieredStorage-MessageFormat).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-03-05T16:52:36Z","2021-06-04T15:24:42Z"
"","10218","KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.","KAFKA-12368: Added inmemory implementations for RemoteStorageManager and RemoteLogMetadataManager.  * Added inmemory implementation for RemoteStorageManager and RemoteLogMetadataManager. A major part of inmemory RLMM will be used in the default RLMM implementation which will be based on topic storage. These will be used in unit tests for tiered storage. * Added tests for both the implementations and their supported classes.  This is part of tiered storage implementation, [KIP-405](https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-02-26T08:47:53Z","2021-04-13T17:14:04Z"
"","9640","KAFKA-10283; Consolidate client-level and consumer-level assignment within ClientState","KAFKA-10283 Consolidate client-level and consumer-level assignment within ClientState https://issues.apache.org/jira/browse/KAFKA-10283  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-11-22T15:25:49Z","2021-04-23T17:07:13Z"
"","9835","MINOR: Add 'task container' class to KafkaStreams TaskManager","Kafka Streams' TaskManager is a central class that grew quite big. This PR breaks out a new 'task container' class to descope what TaskManager does. In follow up PRs, we plan to move more methods from TaskManager to the new 'Tasks.java' class and also improve task-type type safety.  Call for review @ableegoldman @vvcephei @guozhangwang","closed","streams,","mjsax","2021-01-06T18:08:19Z","2021-01-20T22:15:40Z"
"","10431","KAFKA-12543: Change RawSnapshotReader ownership model","Kafka networking layer doesn't close `FileRecords` and assumes that they are already open when sending them over a channel. To support this pattern this commit changes the ownership model for `FileRawSnapshotReader` so that they are owned by `KafkaMetadataLog`. This includes:  1. Changing `KafkaMetadataLog`'s `snapshotIds` form a `Set[OffsetAndEpoch]` to a `TreeMap[OffsetAndEpoch, Option[FileRawSnapshotReader]]`. This map contains all of the known snapshots. The value will be `Some` if a snapshot reader has been opened in the past.  2. Split and change the functionality in `KafkaMetadataLog::removeSnapshotFilesBefore` so that a) `forgetSnapshotsBefore` removes any snapshot less that the given snapshot id from `snapshots`; b) `removeSnapshots` deletes the enumerated snapshots from `forgetSnapshotsBefore`.  3. Change the interface `RawSnapshotReader` to not extend `Closeable` since only `KafkaMetadataLog` is responsible for closing snapshots. `FileRawSnapshotReader` implements `AutoCloseable`.  4. Fixed the implementation of `handleFetchSnapshotRequest` in `KafkaRaftClient` so that `RawSnapshotReader` and the associated `FileRecords` are not close before sending to the network layer.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-03-30T01:30:53Z","2021-05-21T20:36:15Z"
"","10038","MINOR: Disable Travis PR builds","Kafka is consuming over 50% of all Apache Travis executors according to Apache Infra, so let's disable it for now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-03T16:18:21Z","2021-02-03T17:02:03Z"
"","10405","KAFKA-3968: fsync the parent directory of a segment file when the file is created","Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.  The problem is that following sequence of calls doesn't guarantee file durability:  fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log"" write(fd); fsync(fd);  If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.  This PR is to flush the directory when flush() is called for the first time.","closed","","ccding","2021-03-25T23:10:17Z","2021-04-03T03:10:22Z"
"","9587","Fix memory leak in JmxReporter","JmxReporter should remove the KafkaMbean from ""mbeans""  when it becomes empty metrics during metricRemoval,not only unregister the metrics. ""mbeans""  will grow bigger and bigger if not removing the KafkaMbean,finally jvm memory runs out.","closed","","chovy-3012","2020-11-11T14:39:42Z","2020-11-12T09:34:12Z"
"","10367","KAFKA-12495: allow consecutive revoke in incremental cooperative assignor in connector","jira: https://issues.apache.org/jira/browse/KAFKA-12495  Allow consecutive revoke in incremental cooperative assignor in connector, to fix the issue that when new members joined right after revocation round, it causes uneven distribution. (please check the jira for better understanding) What I did are:  1. remove `canRevoke` variable, since we allow consecutive revoking (as lone as `delay == 0`) now 2. compute `currentWorkerAssignmentWithoutDuplication`, to remove duplicated connectors/tasks from `currentWorkerAssignment`, so that we can use it in `performTaskRevocation` method, to compute if we need to revoke more connectors/tasks in this round by computing if the remaining assignments in each worker is higher than totalTasks/totalWorkers 3. we also passed the configured assignment into `performTaskRevocation` instead of `activeAssignment`, so that we can compute the correct expected max assignment number (**totalSize** / workerSize), instead of the **activeTotalSize** / workerSize, because the **activeTotalSize** doesn't include the `newAssignments`, which will cause the wrong computation, and cause uneven rebalance, or need more round of revoking rebalance. 4. Improve readability for tests by adding final assignment visualization for each phase of rebalance  With the change of (2) and (3), we can still make sure the revocation is always correct no matter if this is a consecutive revoking, or we have duplicated assignment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","showuon","2021-03-20T03:58:00Z","2022-04-14T13:26:29Z"
"","9627","KAFKA-10746: Change to Warn logs when necessary to notify users","JIRA: https://issues.apache.org/jira/browse/KAFKA-10746  This improvement is the side effect of this PR: https://github.com/apache/kafka/pull/6972, where we tried to fix flooding number of warning messages but never left group case. So we move the warn logs into the `maybeLeaveGroup` method, and log as INFO level. (https://github.com/apache/kafka/pull/6972/files#diff-15efe9b844f78b686393b6c2e2ad61306c3473225742caed05c7edab9a138832L1092)  The INFO log is good when the leaving group is expected, ex: unsubscribe from all topics, consumer close... However, there are some cases that are unexpected, ex: polling timeout, taking too long to read the log, and these cases should be logged as WARN because some Kafka users ignore INFO messages or have the log level set to WARN.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-11-20T06:22:01Z","2021-04-25T12:55:31Z"
"","10276","KAFKA-12253: Add tests that cover all of the cases for ReplicatedLog's validateOffsetAndEpoch","jira:  https://issues.apache.org/jira/browse/KAFKA-12253   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rohitrmd","2021-03-08T04:58:49Z","2021-03-19T17:09:39Z"
"","9556","MINOR: Update jetty to 9.4.33","Jetty 9.4.32 and before are affected by CVE-2020-27216. This vulnerability is fixed in Jetty 9.4.33, please see the jetty project security advisory for details: https://github.com/eclipse/jetty.project/security/advisories/GHSA-g3wg-6mcf-8jj6#advisory-comment-63053  Unit tests and integration tests pass locally after the upgrade.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","niteshmor","2020-11-04T18:24:35Z","2020-11-09T17:12:51Z"
"","10399","MINOR: Use Java 11 for generating aggregated javadoc in release.py","Java 11 generates html5 pages with search support, which provides a better user experience.  Fixed `get_jdk` bugs found during testing and updated `release.py` blurb to indicate that both JDK 8 and JDK 11 are required to perform a release.  Tested by running `python2 release.py stage-docs`, which triggers the `aggregateJavadoc` path without some of the undesired (for testing) release steps.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-25T03:39:43Z","2021-03-26T03:09:34Z"
"","9603","MINOR: Initialize ConnectorConfig constructor with emptyMap and avoid instantiating a new Map","It's a better way to return an empty Map. Use Collections.emptyMap() instead HashMap","closed","connect,","UnityLung","2020-11-17T15:00:54Z","2020-11-20T18:26:25Z"
"","10349","MINOR: Move `configurations.all` to be a child of `allprojects`","It was incorrectly set within `dependencyUpdates` and it still worked. That is, this is a no-op in terms of behavior, but makes it easier to read and understand.  I tested that `releaseTarGz` produced a tar with two versions of `javassist` _without this block_ and a single version with it (in either position).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-18T14:08:36Z","2021-03-18T18:50:16Z"
"","10086","MINOR: expose number of forwarding requests to metrics","it was an exposed metrics value (#9617 removed it). The number of requests in queue is an important metrics item (for example: queue in `SocketServer`) so we should expose it.   ### screenshot  [  ](url)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-09T06:34:52Z","2021-03-02T16:29:11Z"
"","10386","MINOR: fix aggregatedJavadoc dep on compileJava","It seems like gradle is inconsistently failing to build the project with the message that ""compileJava"" isn't defined on the root project. Applying the Java plugin to the root project (as opposed to only the subprojects) seems to fix it.","closed","","vvcephei","2021-03-23T15:45:25Z","2021-03-24T19:18:08Z"
"","10455","MINOR: Support ExponentialBackoff without jitter","It is useful to allow ExponentialBackoff to be configured to work without jitter, in order to make unit tests more repeatable.","closed","kip-500,","cmccabe","2021-03-31T21:47:50Z","2021-04-06T16:49:14Z"
"","10357","MINOR: The new KIP-500 code should treat cluster ID as a string","It is possible that an existing Cluster ID in Zookeeper is not convertible to a UUID.  KIP-500 cannot constrain Cluster IDs to be convertible to a UUID and still expect such a cluster to be able to upgrade to KIP-500.  This patch removes this UUID constraint and makes KIP-500 consistent with existing RPCs, which treat Cluster ID as a String.  It is unlikely that an existing Cluster ID in Zookeeper would not be convertible to a UUID because the Cluster ID is auto-generated in the form of a UUID and stored in ZooKeeper as soon as the first broker connects.  However, there is nothing preventing someone from generating a Cluster ID that does not conform to the UUID format and manually storing/bootstrapping that in ZooKeeper before the first broker connects.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-03-18T22:17:07Z","2021-03-19T16:56:04Z"
"","9631","KAFKA-9672: Leader with ISR as a superset of replicas","It is possible for the the controller to send LeaderAndIsr requests with an ISR that contains ids not in the replica set. This is used during reassignment so that the partition leader doesn't add replicas back to the ISR. This is needed because the controller updates ZK and the replicas through two rounds:  1. The first round of ZK updates and LeaderAndIsr requests shrinks the ISR.  2. The second round of ZK updates and LeaderAndIsr requests shrinks the replica set.  This could be avoided by doing 1. and 2. in one round. Unfortunately the current controller implementation makes that non-trivial.  This commit changes the leader to allow the state where the ISR contains ids that are not in the replica set and to remove such ids from the ISR if required.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-11-20T20:26:39Z","2021-02-20T01:12:36Z"
"","9646","MINOR: Update snappy-java to 1.1.8.1","It includes small performance improvements: * https://github.com/google/snappy/releases/tag/1.1.8 * https://github.com/xerial/snappy-java/blob/1.1.8.1/Milestone.md  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-11-23T22:46:12Z","2020-11-24T03:31:47Z"
"","10447","KAFKA-12587 Remove KafkaPrincipal#fromString for 3.0","issue: https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-12587  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-31T05:45:26Z","2021-04-02T01:04:21Z"
"","9855","KAFKA-7341 Migrate core module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-7341  This PR includes following changes.  1. replace ```org.junit.Assert``` by ```org.junit.jupiter.api.Assertions```  1. replace ```org.junit``` by ```org.junit.jupiter.api```  1. remove ```ScalaTest``` from core dependencies 1. replace ```org.junit.runners.Parameterized``` by ```org.junit.jupiter.params.ParameterizedTest``` 1. replace ```org.junit.runners.Parameterized.Parameters``` by ```org.junit.jupiter.params.provider.{Arguments, MethodSource}``` 1. replace ```Before``` by ```BeforeEach``` 1. replace ```After``` by ```AfterEach```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-10T21:37:11Z","2021-01-18T16:19:25Z"
"","10446","KAFKA-12661 ConfigEntry#equal does not compare other fields when value is NOT null","issue: https://issues.apache.org/jira/browse/KAFKA-12661 I noticed this issue when reviewing #10436.   ### Changes  1. add `type` and `documentation` to `toString` 1. add `type` and `documentation` to `hashCode` 1. add `type` and `documentation` to `equal` 1. fix incorrect comparison of ConfigEntry#equal  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-31T04:27:02Z","2021-05-02T05:07:08Z"
"","10389","KAFKA-12384: stabilize ListOffsetsRequestTest#testResponseIncludesLeaderEpoch","issue: https://issues.apache.org/jira/browse/KAFKA-12384 The root cause is that we don't wait new leader to sync hw with follower so sending request to get offset could encounter `OFFSET_NOT_AVAILABLE` error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-23T17:20:37Z","2021-04-07T03:29:14Z"
"","10152","KAFKA-12339: Add retry to admin client's listOffsets","issue: https://issues.apache.org/jira/browse/KAFKA-12339  After upgrading our connector env to 2.9.0-SNAPSHOT, sometimes the connect cluster encounters following error. ``` Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:324) org.apache.kafka.connect.errors.ConnectException: Error while getting end offsets for topic 'connect-storage-topic-connect-cluster-1' at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:689) at org.apache.kafka.connect.util.KafkaBasedLog.readToLogEnd(KafkaBasedLog.java:338) at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:195) at org.apache.kafka.connect.storage.KafkaStatusBackingStore.start(KafkaStatusBackingStore.java:216) at org.apache.kafka.connect.runtime.AbstractHerder.startServices(AbstractHerder.java:129) at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:310) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:834) Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. at org.apache.kafka.common.internals.KafkaFutureImpl.wrapAndThrow(KafkaFutureImpl.java:45) at org.apache.kafka.common.internals.KafkaFutureImpl.access$000(KafkaFutureImpl.java:32) at org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:89) at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:260) at org.apache.kafka.connect.util.TopicAdmin.endOffsets(TopicAdmin.java:668) ... 10 more ``` #9780 added shared admin to get end offsets. KafkaAdmin#listOffsets does not handle topic-level error, hence the UnknownTopicOrPartitionException on topic-level can obstruct worker from running when the new internal topic is NOT synced to all brokers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-18T18:54:51Z","2021-02-20T13:09:02Z"
"","10145","KAFKA-12335 Upgrade junit from 5.7.0 to 5.7.1","issue: https://issues.apache.org/jira/browse/KAFKA-12335  junit 5.7.1 release notes: https://junit.org/junit5/docs/5.7.1/release-notes/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-18T03:57:52Z","2021-02-20T03:27:46Z"
"","10098","KAFKA-12321 the comparison function for uuid type should be 'equals' …","issue: https://issues.apache.org/jira/browse/KAFKA-12321  ### current comparison ```java if (this.taggedUuid != Uuid.fromString(""H3KKO4NTRPaCWtEmm3vW7A"")) ```  ### fixed comparison ```java if (!this.taggedUuid.equals(Uuid.fromString(""H3KKO4NTRPaCWtEmm3vW7A""))) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-10T15:56:29Z","2021-02-12T17:15:51Z"
"","10077","KAFKA-12309 The revocation algorithm produces uneven distributions","issue: https://issues.apache.org/jira/browse/KAFKA-12309  # issue description When adding a new worker, the revocation algorithm tries to revoke some connectors/tasks from existent workers so as to move them to new worker to balance load. However, the algorithm can revoke incorrect number of connectors/tasks for following cases.   ### Assignments: ""worker_0"" -> 8 connectors/tasks ""worker_1"" -> 8 connectors/tasks (New) ""worker_2"" -> 0 connectors/tasks  ### Result ""worker_0"" -> 6 connectors/tasks ""worker_1"" -> 6 connectors/tasks ""worker_2"" -> 4 connectors/tasks  ### (expected) Result ""worker_0"" -> 6 connectors/tasks ""worker_1"" -> 5 connectors/tasks ""worker_2"" -> 5 connectors/tasks or  ""worker_0"" -> 5 connectors/tasks ""worker_1"" -> 6 connectors/tasks ""worker_2"" -> 5 connectors/tasks  # root cause the algorithm makes all existent workers keep `ceil` (`numToRevoke = existing.tasksSize() - ceilTasks;`) connectors/tasks. That results in that there is no enough revoked connectors/tasks to be assigned to new worker.  # extra changes included by this PR  1. move revocation algorithm to a helper method in order to test it   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","chia7712","2021-02-07T16:27:09Z","2021-07-02T00:01:28Z"
"","10054","KAFKA-12283 Flaky Test RebalanceSourceConnectorsIntegrationTest#testM…","issue: https://issues.apache.org/jira/browse/KAFKA-12283  Increment protocol has three issues:  ## case 0: the revocation algorithm produces uneven distributions  **Assignments:**   - ""W0"" -> 8 connectors/tasks - ""W1"" -> 8 connectors/tasks - (New) ""W2"" -> 0 connectors/tasks  **Revoked (trunk)**  - ""W0"" -> 2 connectors/tasks - ""W1"" -> 2 connectors/tasks  **Revoked (patch)**  - ""W0"" -> 2 connectors/tasks - ""W1"" -> 3 connectors/tasks  ## case 1: the revocation algorithm is NOT based on completed assignments (with new submissions)  **Assignments:**   - (Unassigned-> 4 connectors/tasks) - ""W0"" -> 4 connectors/tasks  **Revoked (trunk)**  - ""W0"" -> 2 connectors/tasks  ## case 2: the revocation algorithm is based on out-of-date workers  **Assignments**  - ""W0"" -> 16 connectors/tasks  **Revoked** (there are 2 workers so 8 connectors/tasks are revoked) - ""W0"" -> 8 connectors/tasks - (new) ""W1""  **Reassign** (there is another new worker so the reassignment is based on ""three"" workers rather than ""two"" workers) - ""W0"" -> 8 connectors/tasks - (new) ""W1"" -> 4 connectors/tasks - (new) ""W2"" -> 4 connectors/tasks     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-04T20:25:00Z","2021-02-07T17:18:43Z"
"","10024","KAFKA-12273 InterBrokerSendThread#pollOnce throws FatalExitError even…","issue: https://issues.apache.org/jira/browse/KAFKA-12273  kafka tests sometimes shutdown gradle process with non-zero code. The (one of) root cause is that ```InterBrokerSendThread#pollOnce``` encounters ```DisconnectException``` when ```NetworkClient``` is closing. ```DisconnectException``` should be viewed as ""expected"" error as we do close thread. In other words, ```InterBrokerSendThread#pollOnce``` should swallow such exception.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-02T18:57:10Z","2021-02-23T03:00:40Z"
"","9989","KAFKA-12246: Remove redundant suppression in KafkaAdminClient","issue: https://issues.apache.org/jira/browse/KAFKA-12246 Remove redundant comments in KafkaAdminClient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wycccccc","2021-01-28T10:00:22Z","2021-02-02T08:07:12Z"
"","9924","KAFKA-12221 remove PowerMock from connect-json module and…","issue: https://issues.apache.org/jira/browse/KAFKA-12221  ### changes 1. remove PowerMock dependency from connect-json and connect-transformer 1. change some modifier from private to package-private for testing   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-18T09:42:52Z","2021-01-18T16:05:45Z"
"","9889","KAFKA-12203 Migrate connect:mirror-client module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-12203  Only changes the imports and the dependencies.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-14T09:43:16Z","2021-01-15T05:42:08Z"
"","9894","KAFKA-12202 Migrate connect:mirror module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-12202  This PR consists of following changes.  1. replace junit 4 APIs by junit 5 2. remove the dependencies of junit 4 from ```EmbeddedKafkaCluster```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-14T16:33:07Z","2021-01-16T17:30:04Z"
"","9887","KAFKA-12195 Fix synchronization issue happening in KafkaStreams","issue: https://issues.apache.org/jira/browse/KAFKA-12195 related to aedb53a4e6313398ed9626fbd2fbd7106e2ce94e  ## Root Cause It seems to me there are two issues on ```AdjustStreamThreadCountTest```.  ### 1) synchronization issue (the number of threads is inconsistent) ``` threads = Collections.synchronizedList(new LinkedList<>()); ```  The synchronization list requires us to manually synchronize the iterator. non-synchronizing the list results in inconsistent results and consequently unstabilize the AdjustStreamThreadCountTest.  ### 2) duplicate thread index when adding new stream thread  ```createAndAddStreamThread``` is not called with holding ```changeThreadCount``` so it is possible that we create two threads with same thread index. It makes different threads have same metadata and then ```localThreadsMetadata``` returns incorrect number of metadata.  ### 3) assert intermediate state (it gets failed if it run too fast)  This PR already reverts the fix and it is traced by #9888  ## Test Result from PR  looped the test 100 times on my local. all pass  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","chia7712","2021-01-14T08:55:11Z","2021-01-21T00:10:15Z"
"","9858","KAFKA-12173 Migrate streams:streams-scala module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-12173  1. replace ```org.junit.Assert``` by ```org.junit.jupiter.api.Assertions```  1. replace ```org.junit``` by ```org.junit.jupiter.api``` 1. replace ```Before``` by ```BeforeEach``` 1. replace ```After``` by ```AfterEach```  1. remove  ```ExternalResource``` from all scala modules 1. add explicit ```AfterClass```/```BeforeClass``` to stop/start ```EmbeddedKafkaCluster```  Noted that this PR does not migrate stream module to junit 5 so it does not introduce callback of junit 5 to deal with beforeAll/afterAll. The next PR of migrating stream module can replace explicit beforeAll/afterAll by junit 5 extension. Or we can keep the beforeAll/afterAll if it make code more readable. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2021-01-11T10:59:17Z","2021-08-26T09:15:28Z"
"","9857","KAFKA-12172 Migrate streams:examples module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-12172  1. replace ```org.junit.Assert``` by ```org.junit.jupiter.api.Assertions```  1. replace ```org.junit``` by ```org.junit.jupiter.api``` 1. replace ```Before``` by ```BeforeEach``` 1. replace ```After``` by ```AfterEach```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-11T08:13:52Z","2021-01-13T13:02:14Z"
"","9856","KAFKA-12171 Migrate streams:test-utils module to JUnit 5","issue: https://issues.apache.org/jira/browse/KAFKA-12171  1. replace ```org.junit.Assert``` by ```org.junit.jupiter.api.Assertions```  1. replace ```org.junit``` by ```org.junit.jupiter.api```  1. replace ```org.junit.runners.Parameterized``` by ```org.junit.jupiter.params.ParameterizedTest``` 1. replace ```org.junit.runners.Parameterized.Parameters``` by ```org.junit.jupiter.params.provider.{Arguments, MethodSource}``` 1. replace ```Before``` by ```BeforeEach``` 1. replace ```After``` by ```AfterEach```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-11T07:05:08Z","2021-01-14T05:23:49Z"
"","9797","KAFKA-10893 Increase target_messages_per_sec of ReplicaScaleTest to r…","issue: https://issues.apache.org/jira/browse/KAFKA-10893  The expected size is 3400000 and the throttle is 10000 msg/sec so the expected minimal run time of one consumer/producer is about 6 mins. Hence, the test case is always timeout on travis (6 + 6 > 10 mins)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-30T09:00:43Z","2021-01-04T16:23:35Z"
"","9712","KAFKA-10822 Force some stdout from system tests for Travis","issue: https://issues.apache.org/jira/browse/KAFKA-10822  downgrade_test.py/upgrade_test.py does upgrade/downgrade for each tests. the upgrade/downgrade tasks take 10+ mins in Travis env so we ought to print something in order to avoid timeout caused by Travis.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-08T06:21:37Z","2021-02-12T19:43:24Z"
"","9706","KAFKA-10815 EosTestDriver#verifyAllTransactionFinished should break loop if all partitions are verified","issue: https://issues.apache.org/jira/browse/KAFKA-10815 If we don't break it when all partitions are verified, the loop will take 10 mins ...  see https://travis-ci.com/github/apache/kafka/jobs/455103274 for related timeout  ```  2994[INFO:2020-12-06 06:06:35,755]: Triggering test 10 of 32... 2995[INFO:2020-12-06 06:06:35,765]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/streams', 'file_name': 'streams_eos_test.py', 'cls_name': 'StreamsEosTest', 'method_name': 'test_rebalance_complex', 'injected_args': {'processing_guarantee': 'exactly_once'}} 2996[INFO:2020-12-06 06:06:35,771]: RunnerClient: kafkatest.tests.streams.streams_eos_test.StreamsEosTest.test_rebalance_complex.processing_guarantee=exactly_once: Setting up... 2997[INFO:2020-12-06 06:07:12,200]: RunnerClient: kafkatest.tests.streams.streams_eos_test.StreamsEosTest.test_rebalance_complex.processing_guarantee=exactly_once: Running... 2998 2999 3000No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself. 3001Check the details on how to adjust your build configuration on: https://docs.travis-ci.com/user/common-build-problems/#build-times-out-because-no-output-was-received ```  this issue is related to https://github.com/apache/kafka/commit/cb88be45ebc400069c03ad8b9a2332da0aecc7b8  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2020-12-07T08:44:05Z","2020-12-23T19:13:34Z"
"","9611","KAFKA-10736 Convert transaction coordinator metadata schemas to use g…","issue: https://issues.apache.org/jira/browse/KAFKA-10736   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-11-18T10:03:55Z","2020-11-30T09:43:42Z"
"","9525","KAFKA-10658 ErrantRecordReporter.report always return completed futur…","issue: https://issues.apache.org/jira/browse/KAFKA-10658  This issue happens when both DLQ and error log are enabled. There is a incorrect filter in handling multiple reports and it results in the uncompleted future is filtered out. Hence, users always receive a completed future even though the record is still in producer buffer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2020-10-29T03:32:48Z","2021-01-28T06:21:43Z"
"","9796","KAFKA-10498 Consumer should do offset/epoch validation through  when …","issue: https://issues.apache.org/jira/browse/KAFKA-10498  Currently the consumer has logic to detect truncations (as a result of unclean leader election for example) using the OffsetsForLeaderEpoch API. It is a rather cumbersome and expensive process since we have to check for the need to send this request on every poll(). We should be able to do better now that KIP-595 has built support for truncation detection directly into the `Fetch` protocol. This should allow us to skip validation when we know that the `Fetch` version is high enough.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2020-12-29T15:26:32Z","2020-12-30T09:13:13Z"
"","9673","KAFKA-10289 fix failed connect_distributed_test.py (ConnectDistribute…","issue: https://issues.apache.org/jira/browse/KAFKA-10289  In Python 3, ```filter``` functions return iterators rather than ```list``` so it can traverse only once. Hence, the following loop only see ""empty"" and then validation fails.  ```python         src_messages = self.source.committed_messages() # return iterator         sink_messages = self.sink.flushed_messages()) # return iterator         for task in range(num_tasks):             # only first task can ""see"" the result. following tasks see empty result             src_seqnos = [msg['seqno'] for msg in src_messages if msg['task'] == task] ```  reference: https://portingguide.readthedocs.io/en/latest/iterators.html#new-behavior-of-map-and-filter  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-02T06:41:27Z","2020-12-09T21:38:17Z"
"","9703","KAFKA-10697 Remove ProduceResponse.responses","issue : https://issues.apache.org/jira/browse/KAFKA-10697  Remove the old method Map  responses()  Use the new method public ProduceResponseData data() instead.","open","","UnityLung","2020-12-05T17:30:28Z","2020-12-16T11:45:15Z"
"","10332","KAFKA-10697: Remove ProduceResponse.responses","issue : https://issues.apache.org/jira/browse/KAFKA-10697  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-03-16T18:45:06Z","2021-03-19T02:04:56Z"
"","9519","KAFKA-10650: Use Murmur3 instead of MD5 in SkimpyOffsetMap","Introduces a new offset map in the log cleaner which uses Murmur hashing algorithm instead of the current MD5. This commit: - Moves Murmur3 from Streams into Clients - Changes SkimpyOffsetMap to use Murmur3 hashing - Removes the option to change hashing algorithm in SkimpyOffsetMap - Adds benchmark tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2020-10-28T13:07:31Z","2022-05-10T12:33:38Z"
"","9882","MINOR: Make JUnit 5 the default for new projects","Instead of listing the projects that should use JUnit 5, list the projects that are still using JUnit 4.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-14T02:32:55Z","2021-01-14T13:16:47Z"
"","9818","MINOR: refactor FetchResponse#toMessage to avoid creating a lot of un…","Instead of creating intermediate collections to batch data, we directly check the last topic in auto-generated data to collect partitions having same topic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-03T08:09:44Z","2021-01-05T02:32:05Z"
"","10314","MINOR: remove redundant null check when testing specified type","inspired by https://github.com/apache/kafka/pull/10141#discussion_r589425927  the `instanceof` includes null check already so we can get rid of redundant null check.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-14T11:50:57Z","2021-03-18T15:39:27Z"
"","10078","MINOR: make sure all generated data tests cover all versions","inspired by #10068  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-07T18:13:33Z","2021-03-04T16:22:57Z"
"","10310","KAFKA-12460; Do not allow raft truncation below high watermark","Initially we want to be strict about the loss of committed data for the `@metadata` topic. This patch ensures that truncation below the high watermark is not allowed. Note that `MockLog` already had the logic to do so, so the patch adds a similar check to `KafkaMetadataLog`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-12T22:38:20Z","2021-08-11T13:15:03Z"
"","10076","KAFKA-9689 [WIP] Automatic broker version detection to initialize stream client","Initial commit of KAFKA-9689, support  1). client feature version publish  2). feature version detection.  This PR is at early stage and need to be polished after remaining part is done~ Test coverage and dynamic feature version switch need to be added  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","feyman2016","2021-02-07T07:29:18Z","2021-03-09T15:34:55Z"
"","9764","MINOR: Eliminate KafkaScheduler#scheduleOnce in favor of KafkaScheduler#schedule","In this PR, I've eliminated `KafkaScheduler#scheduleOnce` API. It had only 1 call site, and instead we can just use `KafkaScheduler#schedule`.  **Test plan:** Rely on existing tests.","closed","","kowshik","2020-12-17T10:09:16Z","2021-05-25T08:25:21Z"
"","10478","KAFKA-12553: Refactor recovery logic to introduce LogLoader","In this PR, I have refactored the recovery logic code introducing a new class `kafka.log.LogLoader` responsible for all activities related with recovery of log segments from disk. With this change, the recovery logic has been moved out of the `Log` class and into the new `LogLoader` class.   **Advantages:** This refactor has the following advantages over the existing code:   * As such, the recovery logic is invoked once only during `Log` instantiation. Some parts of the recovery logic are fairly independent from the rest of the `Log` class. By moving the independent private logic to a separate `LogLoader` class, the existing  `Log` class has become more modular, and the constructor behavior is a lot simpler now. Therefore, this makes the code more maintainable.  * This PR takes us a step closer towards the Log layer reactor work (KAFKA-12554). The Log recovery logic reads and writes to `LeaderEpochFileCache` and `ProducerStateManager` instances, so as such the logic does not fit very well into the definition of a ""local log"". By extracting it out of the `Log` class, in the future this will make it much easier to clearly define the separation of concerns between `LocalLog` and `UnifiedLog`.  **Note to reviewers:** Most of this PR is about moving code around (including tests) and sharing static functions wherever possible. In order to start reviewing this PR, it is best to start looking at the sequence of operations when a `Log` instance is created (see `Log.apply()`): 1. First `LogLoader` loads/recovers the segments from disk and using the metadata it also suitably sets up the related components viz. log start offset, recovery point, next offset, leader epoch cache and producer state manager. The entry point for the `LogLoader` is the `LogLoader.load()` API. The implementation is pretty much identical to the previously existing `Log` recovery logic. 2. Next, the log components setup by the `LogLoader` are passed to the `Log` instance during construction. Some of these components are further mutable by the `Log` instance during its lifecycle. The rest of the `Log` class pretty much remains intact, except that some private methods have been converted to static functions to be shared with the recovery logic.  **Tests:** I'm relying on the newly added `LogLoaderTest.scala` for testing. I have moved 35 recovery related tests out of `LogTest.scala` and into the new `LogLoaderTest.scala` test suite.","closed","","kowshik","2021-04-05T15:57:02Z","2021-04-20T16:51:28Z"
"","9536","MINOR: KIP-584: Remove admin client facility to read features from controller","In this PR, I have eliminated the facility in `Admin#describeFeatures` API and it's implementation to be able to optionally send a `describeFeatures` request to the controller. This feature was not seen to be particularly useful, and besides it also poses some hindrance to post KIP-500 world where no client would be able to access the controller directly.  **Test plan:**  Rely on existing unit & integration tests.","closed","","kowshik","2020-10-30T06:17:14Z","2020-11-03T01:16:53Z"
"","10406","MINOR: Update testUpdate method with correct Crc32","In this PR I have updated the `testUpdate` method to use the correct `Crc32` class  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","alok123t","2021-03-26T00:47:17Z","2021-04-08T19:01:49Z"
"","9594","Improve JavaDoc in KeyValueBytesStoreSupplier","In the JavaDoc, the implemented interface was described inaccurately. Also, the ordered list was formatted as plain text, not as html ""ol"".  No testing necessary since only a comment was changed.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","fml2","2020-11-13T22:42:34Z","2020-11-15T12:24:22Z"
"","9715","Upstream ApisUtils from kip-500","In the [KIP-500 development branch](https://github.com/confluentinc/kafka/tree/kip-500), we have a separate ControllerApis that shares a lot of functionality with KafkaApis. We introduced a utility class ApisUtils to pull out the common code. Some things were moved to RequestChannel as well.  We'd like to upstream this work now so we don't continue to diverge (since KafkaApis is a frequently modified class). There should be no logical changes in this PR, only shuffling code around.","closed","kip-500,","mumrah","2020-12-08T21:01:41Z","2021-01-15T20:28:44Z"
"","10469","KAFKA-12611: Fix using random payload in ProducerPerformance incorrectly","In ProducerPerformance, random payload always same. It has a great impact when use the compression.type option.  https://issues.apache.org/jira/browse/KAFKA-12611  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lamberken","2021-04-03T13:40:07Z","2021-04-13T06:28:47Z"
"","9569","KAFKA-10687: make ProduceRespone only returns INVALID_PRODUCER_EPOCH","In KIP-588, we added the new `PRODUCER_FENCED` error code 90 to represent existing ProducerFencedException, which would be unknown error for producer client who sends ProduceRequest and doesn't know how to handle it. The fix is to revise the error code back to the known `INVALID_PRODUCER_EPOCH` in the ProduceResponse.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-06T03:51:11Z","2020-11-18T03:56:39Z"
"","9518","KAFKA-10645: Add null check to the array/Iterable values in RecordHeaders constructor","In https://github.com/apache/kafka/pull/6484, we add null check while adding headers with `add()` method, but we forgot to protect it while user passing the headers with null value in constructor. Add it and add tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-10-28T09:01:13Z","2020-10-29T23:40:50Z"
"","9693","KAFKA-10825 Refactor ZK ISR updates to use AlterIsrManager","In an effort to consolidate the ISR write paths, this PR adapts the ZK ISR update into the new AlterIsrManager trait. This will allow us minimize divergence in the ISR update code in Partition.scala.   TODO: * [x] Remove expandIsr and shrinkIsr from PartitionStateStore * [x] Move ISR propagation thread to ZkIsrManager","closed","","mumrah","2020-12-04T18:48:45Z","2020-12-08T19:53:42Z"
"","9713","KAFKA-10825 ZK ISR manager","In an effort to consolidate the ISR write paths, this PR adapts the ZK ISR update into the new AlterIsrManager trait. This will allow us minimize divergence in the ISR update code in Partition.scala.  This also removes PartitionStateStore and moves the ISR propagation code to the new ZK ISR manager class","closed","","mumrah","2020-12-08T19:23:07Z","2020-12-21T19:44:03Z"
"","9586","MINOR: Fix documentation for Kafka Quick Start Demo Code","In 1.3 Quick Start, STEP 7: PROCESS YOUR EVENTS WITH KAFKA STREAMS. There is a demo to implement the popular WordCount algorithm using Kafka Streams.  But these code can't run directly, because there is an extra `)` after `.to(""output-topic""` . This PR removed the extra brackets in the demo code.","closed","","zhangyue19921010","2020-11-11T09:00:54Z","2021-03-05T10:11:20Z"
"","9482","KAFKA-10632; Raft client should push all committed data to state machines","In #9418, we add a listener to the `RaftClient` interface. In that patch, we used it only to send commit notifications for writes from the leader. In this PR, we extend the `handleCommit` API to accept all committed data and we remove the pull-based `read` API. Additionally, we add two new callbacks to the listener interface in order to notify the state machine when the raft client has claimed or resigned leadership.  Finally, this patch allows the `RaftClient` to support multiple listeners. This is necessary for KIP-500 because we will have one listener for the controller role and one for the broker role.  Note this patch builds on top of #9418. Once merged, I will rebase this patch and remove draft status.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2020-10-22T18:32:33Z","2021-08-11T13:17:09Z"
"","9619","MINOR: Reduce sends created by `SendBuilder`","In #9401, I observed some strange latency behavior when testing locally (on macos). The behavior seemed to be caused by slightly different write behavior as a result of the new `SendBuilder` class. After the change, we were making several calls down to both `GatheringByteChannel.write(ByteBuffer[])` as well as `WritableByteChannel.write(ByteBuffer)` whereas previously there would be just one path through the former. The difference is clearly seen in the flame graphs posted here: https://github.com/apache/kafka/pull/9401#issuecomment-729258346.   This patch changes the grouping of `Send` objects created by `SendBuilder` to try and restore the previous write behavior. Rather than creating a single `ByteBufferSend` for each `ByteBuffer`, we attempt to group consecutive buffers into a single `ByteBufferSend`. I confirmed that did indeed address the latency issue I observed using the same test setup with a single broker that was discussed in #9401.  ``` bin/kafka-topics.sh --create --topic foo --replication-factor 1 --partitions 10 --bootstrap-server $BROKER bin/kafka-producer-perf-test.sh --topic foo --num-records 250000000 --throughput -1  --record-size 256 --producer-props bootstrap.servers=$BROKER ```  Here are the results:  ``` Before KAFKA-9628: 250000000 records sent, 1398546.630342 records/sec (341.44 MB/sec), 39.71 ms avg latency, 424.00 ms max latency, 16 ms 50th, 137 ms 95th, 194 ms 99th, 270 ms 99.9th.  Trunk: 250000000 records sent, 1413619.374502 records/sec (345.12 MB/sec), 87.72 ms avg latency, 2058.00 ms max latency, 64 ms 50th, 247 ms 95th, 429 ms 99th, 790 ms 99.9th.                                               This patch: 250000000 records sent, 1469723.691946 records/sec (358.82 MB/sec), 12.72 ms avg latency, 320.00 ms max latency, 2 ms 50th, 83 ms 95th, 162 ms 99th, 256 ms 99.9th.                                                 ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-11-18T22:33:47Z","2020-11-19T20:45:24Z"
"","9677","KAFKA-10799 AlterIsr utilizes ReplicaManager ISR metrics","In #9100, we missed the inclusion of the high level ISR shrink/expand metrics that are managed by ReplicaManager. This PR adds a small abstraction that allows us to mark these metrics without bringing ReplicaManager in as a dependency of Partition.","closed","","mumrah","2020-12-02T19:40:28Z","2020-12-03T21:11:08Z"
"","10324","MINOR: Add a few more benchmark for the timeline map","Improve the benchmark tests for TimelineHashMap by adding tests for adding entries, removing entries and Scala's immutable hash map.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jsancio","2021-03-16T00:06:00Z","2021-03-29T19:24:10Z"
"","9553","KAFKA-10427:  Fetch snapshot","Implements the code necessary for the leader to response to fetch snapshot requests and for the follower to fetch snapshots. It depends on PR https://github.com/apache/kafka/pull/9512 and that PR should be reviewed first. Included are the following changes.  Leader Changes: 1. Raft leader response to FetchSnapshot request by reading the local snapshot and sending the requested bytes in the response. This implementation currently copies the bytes to memory. This will be fixed in a future PR.  Follower Changes: 1. Raft followers will start fetching snapshot if the leader sends a Fetch response that includes a SnapshotId.  2. Raft followers send FetchSnapshot requests if there is a pending download. The same timer is used for both Fetch and FetchSnapshot requests.  3. Raft follower handle FetchSnapshot responses by comping the bytes to the pending SnapshotWriter. This implementation doesn't fix the replicated log after the snapshot has been downloaded. This will be implemented in a future PR.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-11-03T18:59:48Z","2021-01-06T18:00:35Z"
"","9979","KAFKA-12238; Implement `DescribeProducers` API from KIP-664","Implements the `DescribeProducers` API specified by KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.  Note that this patch contains only the server implementation. The `Admin` changes will come in a follow-up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-27T03:31:33Z","2021-01-30T01:00:14Z"
"","10366","KAFKA-12467: Add controller-side snapshot generation","Implement writing snapshots in the controller, and reading them back.","closed","kip-500,","cmccabe","2021-03-20T00:08:44Z","2021-04-06T17:18:24Z"
"","10343","KAFKA-12471: Implement createPartitions in KIP-500 mode","Implement the createPartitions RPC which adds more partitions to a topic in the KIP-500 controller.  Factor out some of the logic for validating manual partition assignments, so that it can be shared between createTopics and createPartitions.  Add a startPartition argument to the replica placer.","closed","kip-500,","cmccabe","2021-03-17T23:09:09Z","2021-04-13T18:00:26Z"
"","10085","KAFKA-12154: Snapshot Loading API","Implement Raft Snapshot loading API.  1. Adds a new method `handleSnapshot` to `raft.Listener` which is called whenever the `RaftClient` determines that the `Listener` needs to load a new snapshot before reading the log. This happens when the `Listener`'s next offset is less than the log start offset also known as the earliest snapshot.  2.  Adds a new type `SnapshotReader` which provides a `Iterator>` interface and de-serializes records in the `RawSnapshotReader` into `T`s  3.  Adds a new type `RecordsIterator` that implements an `Iterator>` by scanning a `Records` object and deserializes the batches and records into `Batch`. This type is used by both `SnapshotReader` and `RecordsBatchReader` internally to implement the `Iterator` interface that they expose.   4. Changes the `MockLog` implementation to read one batch at a time. The previous implementation always read from the given offset to the high-watermark. This made it impossible to test interesting snapshot loading scenarios.  5. Removed `throws IOException` from some methods. Some of types were inconsistently throwing `IOException` in some cases and throwing `RuntimeException(..., new IOException(...))` in others. This PR improves the consistent by wrapping `IOException` in `RuntimeException` in a few more places and replacing `Closeable` with `AutoCloseable`.  6. Updated the Kafka Raft simulation test to take into account snapshot. `ReplicatedCounter` was updated to generate snapshot after 10 records get committed. This means that the `ConsistentCommittedData` validation was extended to take snapshots into account.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-02-09T00:06:38Z","2021-05-01T20:47:20Z"
"","10294","KAFKA-12450: Remove deprecated methods from ReadOnlyWindowStore","Implement first part of https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore. Preparing it for v3.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-03-10T17:46:49Z","2021-05-04T16:23:41Z"
"","10456","MINOR: support ImplicitLinkedHashCollection#sort","Implement an O(N log N) merge sort in ImplicitLinkedHashCollection. This is useful sometimes in unit tests for comparing collections.","closed","kip-500,","cmccabe","2021-03-31T22:17:23Z","2021-04-02T01:31:00Z"
"","9791","KAFKA-10873: ignore warning messages if connector/task start failed","Ignore warning message if connector/task start failed because we already log error messages to user when start connectors/tasks failed.  I was trying to add a `null` value to connectors/tasks map, and use `containsKey()` method to distinguish start failed ones from the non-present/unowned ones. But the concurrent map doesn't  allow null value. So, use a hashSet to store the startFailed connectors/tasks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","showuon","2020-12-28T07:56:22Z","2021-01-29T07:29:23Z"
"","9798","MINOR: fix the soft link created by ducktape when running system test…","If we run system tests by container, the soft link created by ```ducktape``` references to path inside container. This PR adds a tiny function to correct soft link.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2020-12-30T14:22:22Z","2021-02-12T19:42:52Z"
"","9668","MINOR: add test for repartition/source-topic/changelog optimization","If topology optimization is enabled, KafkaStreams does not create store changelog topics but re-uses source input topics if possible. However, this optimization should not be applied to internal repartition topics, because those are actively purged.  Call for review @ableegoldman","closed","tests,","mjsax","2020-12-01T19:38:51Z","2020-12-23T19:57:00Z"
"","9495","KAFKA-10642: Expose the real stack trace if any exception occurred during SSL Client Trust Verification in extension","If there is any exception occurred in the custom implementation of client trust verification (i.e. using security.provider), the inner exception is suppressed or hidden and not logged to the log file...  @junrao @mjsax @guozhangwang","open","","senthilm-ms","2020-10-24T19:35:49Z","2020-12-17T15:21:15Z"
"","10107","MINOR: Improve confusing admin client shutdown logging","If the admin client is shutdown with some unfinished calls, we see messages such as the following in the log: ``` 2021-02-09 11:08:05.964 DEBUG [AdminClient clientId=adminclient-1] Call(callName=fetchMetadata, deadlineMs=1612843805378) timed out at 9223372036854775807 after 1 attempt(s) ``` The problem is that we are using passing `Long.MaxValue` as the current time in `Call.fail` in order to ensure the call is timed out and we are discarding the original cause. The patch fixes the problem by setting `aborted=true` instead and preserving the original exception message.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-02-11T02:24:43Z","2021-02-11T23:12:26Z"
"","9800","KAFKA-9274: Fix commit-TimeoutException handling for EOS","If EOS is enabled and the TX commit fails with a timeout, we should not process more messages (what is ok for non-EOS) because we don't really know the status of the TX. If the commit was indeed successful, we won't have an open TX and calling send() would fail with a fatal error.  Instead, we should retry the (idempotent) commit of the TX, and start a new TX afterwards.  Call for review @vvcephei @abbccdda @guozhangwang @hachikuji @bob-barrett","closed","kip,","mjsax","2020-12-30T21:38:19Z","2021-01-08T02:01:29Z"
"","9813","MINOR: gradle wrapper should handle directories with spaces","If attempting to build the project from a directory with spaces in its name and gradle-wrapper.jar is missing, the script will fail to download a new one because the ""if"" condition will break because $APP_HOME will resolve to multiple strings. Protecting $APP_HOME with quotes fixes the issue. Tested by deleting gradle.wrapper and trying to build.","closed","","gwenshap","2021-01-02T07:32:16Z","2021-01-17T20:01:44Z"
"","9784","MINOR: Fix connector startup error logging","If a connector fails on startup, the original cause of the error gets discarded by the framework and the only message that gets logged looks like this:  ``` [2020-12-04 16:46:30,464] ERROR [Worker clientId=connect-1, groupId=connect-cluster] Failed to start connector 'conn-1' (org.apache.kafka.connect.runtime.distributed.DistributedHerder) org.apache.kafka.connect.errors.ConnectException: Failed to start connector: conn-1         at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$startConnector$5(DistributedHerder.java:1297)         at org.apache.kafka.connect.runtime.Worker.startConnector(Worker.java:258)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startConnector(DistributedHerder.java:1321)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$1400(DistributedHerder.java:127)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:1329)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:1325)         at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)         at java.base/java.lang.Thread.run(Thread.java:834) ```  The changes here should cause the original cause of the connector startup failure to be logged as well.","closed","","C0urante","2020-12-23T21:25:03Z","2020-12-30T16:06:25Z"
"","10224","MINOR: Disable transactional/idempotent system tests for Raft quorums","Idempotent producers and transactions are not supported in the KIP-500 early access release. This patch disables system tests for Raft metadata quorums that use these features and that were still enabled after #10194.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-26T23:39:22Z","2021-03-02T18:58:26Z"
"","9810","MINOR: Tweak IBM i platform support in ""stop"" scripts","IBM i support to the ""stop"" scripts was added in a previous PR (https://github.com/apache/kafka/pull/9023), but it misses cases where certain service managers dispatch the zookeeper/kafka jobs with ""nohup"" or system batch job capabilities. We need a tweak to the `ps` invocation for this to work properly in those scenarios.   Due to the minor nature of the problem and the trivial nature of the fix, I did not create a JIRA issue, but I will happily do so if that's the recommended best route.    ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] ~~Verify test coverage and CI build status~~ (self tested) - [x] ~~Verify documentation (including upgrade notes)~~ n/a","closed","","ThePrez","2021-01-01T19:04:58Z","2021-01-21T18:18:49Z"
"","9971","MINOR: Call logSegments.toBuffer only when required","I've improved `Log.fetchOffsetByTimestamp` implementation slightly, such that we only copy the `segments` map values when `targetTimestamp` is neither `EARLIEST_TIMESTAMP` nor `LATEST_TIMESTAMP`. Previously, it was cached outside the `if` block, and that seemed a little bit of an overkill to me.  **Test plan:** Rely on existing tests.","closed","","kowshik","2021-01-26T02:47:32Z","2021-01-27T04:43:11Z"
"","9966","MINOR: Fix visibility of Log.{unflushedMessages, addSegment} methods","I've changed the visibility of the following methods, since these need not be `public` methods:  * The method `Log.unflushedMessages` from public to private.  * The method `Log.addSegment` from public to package private.  **Test plan:** Rely on existing tests.","closed","","kowshik","2021-01-25T19:02:46Z","2021-01-26T05:59:33Z"
"","10100","MINOR: Prevent creating partition.metadata until ID can be written","I'm porting https://github.com/apache/kafka/pull/10041 to the 2.8 branch.  Currently the partition.metadata file is created when the log is created. However, clusters with older inter-broker protocols will never use this file. This PR moves the creation of the file to when we write to the file.  This PR also deletes the partition.metadata file on startup if the IBP version is lower than 2.8.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-10T18:00:38Z","2021-02-11T01:18:24Z"
"","9864","MINOR: system tests can't parse version of JDK 15","I'm deploying a new system tests suite for latest JDK (i.e JDK 15) on my machine. the version of JDK is ```openjdk version ""15"" 2020-09-15```. Unfortunately,  it can't be parsed by kafka system tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-11T17:53:33Z","2021-02-11T17:01:14Z"
"","9762","KAFKA-10861; Fix race condition in flaky test `testFencingOnSendOffsets`","I wasn't able to reproduce the failure locally, but it looks like there is a race condition with the sending of the records in the first producer. The test case assumes that these records have been completed before the call to `sendOffsetsToTransaction`, but they very well might not be. It is even possible for the writes from the second producer to arrive first which would then result in the test failure that we are seeing. The solution is to force the send with `flush()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-12-16T23:51:51Z","2020-12-17T02:31:16Z"
"","9938","MINOR: Refactor DescribeAuthorizedOperationsTest","I was looking at `DescribeAuthorizedOperationsTest` in the context of KIP-700. I have made few changes to improve the readability of the code/tests. The PR only moves code around.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-01-20T17:11:43Z","2021-01-21T16:34:13Z"
"","10071","KAFKA-12298: Create LeaderAndIsrRequestBenchmark","I thought it would be useful to check how the changes to LeaderAndIsrRequests (for KIP-516) affect the handling of the request. This benchmark builds a LeaderAndIsrRequest and calls  `kafkaApis.handleLeaderAndIsrRequest`. Other benchmarks for this type of request could be added here as well.  I also slightly changed the MetadataRequestBenchmark since I noticed that the MetadataCache used in that benchmark did not add topic IDs. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jolshan","2021-02-05T20:55:46Z","2021-02-18T16:51:23Z"
"","9918","MINOR: fix typo in SnapshotWriter","I think the author wants to use ‘for a' here.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-01-16T17:55:58Z","2021-01-19T14:20:28Z"
"","9743","KAFKA-10841 LogReadResult should be able to converted to FetchPartiti…","I put the conversion logic into LogReadResult named toFetchPartitionData.","closed","","g1geordie","2020-12-13T17:53:12Z","2021-01-19T00:55:22Z"
"","10398","MINOR: Fix newly added flaky test `testClientSideTimeoutAfterFailureTest`","I noticed this test hanging frequently locally. There is a race condition between the background thread calling `ready` and the call to `MockTime.sleep` in the test. If the call to `sleep` happens first, then the test hangs. I fixed it by giving `MockClient` a way to listen to `ready` calls. This combined with a latch fixes the race.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-03-25T03:09:58Z","2021-03-30T22:45:08Z"
"","10347","MINOR: the request thread in clientToControllerChannelManager is NOT…","I noticed this issue when running system tests. Most raft-related tests fails.  related to #10135  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-18T06:26:00Z","2021-03-18T10:58:50Z"
"","10001","MINOR: AbstractCoordinatorTest should close coordinator explicitly","I noticed this issue when digging into some flaky by JVM profiler. ```AbstractCoordinatorTest``` does not close coordinator so it can cause a lot of idle heartbeat threads in the following tests.  ``` ""kafka-coordinator-heartbeat-thread | dummy-group"" #239 daemon prio=5 os_prio=0 cpu=4.40ms elapsed=29.26s tid=0x00007f4798c34000 nid=0x11b6 in Object.wait()  [0x00007f471dbf5000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x000000008152b250> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #240 daemon prio=5 os_prio=0 cpu=4.15ms elapsed=29.16s tid=0x00007f4798c36800 nid=0x11b7 in Object.wait()  [0x00007f471d7f4000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x000000008152e9c0> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #242 daemon prio=5 os_prio=0 cpu=0.23ms elapsed=29.04s tid=0x00007f4798c39000 nid=0x11b9 in Object.wait()  [0x00007f471d3f3000]    java.lang.Thread.State: WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at java.lang.Object.wait(java.base@11.0.9.1/Object.java:328) 	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1355) 	- waiting to re-lock in wait() <0x00000000815107f8> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #244 daemon prio=5 os_prio=0 cpu=3.62ms elapsed=29.03s tid=0x00007f4798c3b000 nid=0x11bb in Object.wait()  [0x00007f471cff2000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x00000000815330b0> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #245 daemon prio=5 os_prio=0 cpu=4.09ms elapsed=28.93s tid=0x00007f4798c3d800 nid=0x11bc in Object.wait()  [0x00007f471cbf1000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x00000000815387e8> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #246 daemon prio=5 os_prio=0 cpu=4.14ms elapsed=28.83s tid=0x00007f4798c3f000 nid=0x11bd in Object.wait()  [0x00007f471c7f0000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x00000000815389d8> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator)  ""kafka-coordinator-heartbeat-thread | dummy-group"" #247 daemon prio=5 os_prio=0 cpu=4.08ms elapsed=28.72s tid=0x00007f4798c41800 nid=0x11be in Object.wait()  [0x00007f46e3ffe000]    java.lang.Thread.State: TIMED_WAITING (on object monitor) 	at java.lang.Object.wait(java.base@11.0.9.1/Native Method) 	- waiting on  	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1398) 	- waiting to re-lock in wait() <0x0000000081538bc8> (a org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator) ```  I don't observe the relationship between this issue and flaky. However, it seems to me explicitly releasing idle resource is always a good pattern.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-29T06:49:51Z","2021-01-30T12:03:16Z"
"","10251","MINOR: add missing docs for record-e2e-latency metrics","I missed updating the documentation for these metrics since I didn't notice we had Streams metrics docs outside of the usual Streams docs.","closed","","ableegoldman","2021-03-03T04:17:06Z","2021-03-04T22:45:09Z"
"","9761","KAFKA-10768 Make ByteBufferInputStream.read(byte[], int, int) to follow the contract","I made a test for ByteBufferInputStream in the ByteBufferLogInputStreamTest. First, I add a ByteBuffer that it's not empty to the ByteBufferInputStream, in order to verify it. After that, I try to use ByteBufferInputStream's read function and check return value whether it's correct.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bertber","2020-12-16T16:49:59Z","2021-01-03T07:09:32Z"
"","9742","KAFKA-10768 Add a test for ByteBufferInputStream to ByteBufferLogInputStreamTest","I made a test for ByteBufferInputStream in the ByteBufferLogInputStreamTest. First, I add a ByteBuffer that it's not empty to the ByteBufferInputStream, in order to verify it. After that, I try to use ByteBufferInputStream's read function and check return value whether it's correct.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bertber","2020-12-13T16:10:20Z","2020-12-15T17:04:50Z"
"","9492","KAFKA-10627: Added support for Connect TimestampConverter to convert multiple fields using a comma-separated list, and multiple input formats when parsing from a string.","I have made an update to **TimestampConverter** Connect transform to address the main issues that I logged in [KAFKA-10627](https://issues.apache.org/jira/browse/KAFKA-10627).  Namely, that it now ...  - supports multiple fields via a new configuration parameter `fields` as a comma-separated list of field names. The old parameter `field` is still supported for compatibility but the value is moved to the new parameter. - supports a DateTimeFormatter-compatible pattern string that can support multiple timestamp formats for parsing input of string values to whatever target you configure (e.g. parsing strings to Timestamp type).    - `format` config is now split into two: `format.input` and `format.output` but you can still just send `format` by itself if you do not need to use a more complicated input pattern. When providing only `format`, the string pattern which you provide will be used for both `format.input` and `format.output`.  I realized that kafka is using `java.util.Date` everywhere and as part of its core types (including in Schemas, values, etc).  In theory it would be good over time to upgrade to `java.time` classes but on first reflection it seems like quite a big overhaul to do this.  So instead I focused on the specific problem at hand: parsing strings into `Date` where the strings can come in different formats.  So for this part alone I changed to use `DateTimeFormatter` so we can use multiple patterns to match input strings and convert them to a `java.util.Date` after.  I also updated some of the way the Config parameters and values work, to bring in line with the other classes and similar to what I did with #9470.  #### String Input and Output Timestamp Format updates  Because now for input formats we allow multiple different possibilities using pattern matching, this does not work for the output format of a Timestamp to a String (which was another possibility of this transform).  So I have changed the configuration a bit... now there are three parameters:  - `format` which is the original one. You can still use this one, and it will set both input (parsing) and output (Date/Timestamp to string format) based on this format. - `format.input` is a new parameter, where you can specify a DateTimeFormatter-compatible pattern string that supports multiple different formats in case you have a mix in your data.  For just one example, now you can use something like this as `format.input` and it will catch a lot of different variations which you might see in one timestamp field: `""[yyyy-MM-dd[['T'][ ]HH:mm:ss[.SSSSSSSz][.SSS[XXX][X]]]]""` - `format.output` is a new parameter which only controls the output of a Date/Timestamp to target type of `string`. This is the same as before and still uses `SimpleDateFormat` to create the output string, it is just controlled in a separate parameter now.  I also added some code which checks the value of each of these three.  Basically it forces you to use either `format`, or one or both of the new parameters -- you cannot mix the old and new together.  In the end, `format.input` and `format.output` are the ones used in the rest of the logic, but the code first compares `format` against these values and sets the value for both of the new parameters depending on what was sent in the config.  #### Support for multiple fields instead of one single field  I changed the `field` parameter to now be called `fields` and supports multiple values as a comma-separated list.  I used this new `ConfigUtils.translateDeprecatedConfigs` method to provide automatic translation of of the old parameter to the new one as well.  With this change I also updated the `apply` methods so that they loop through each field and check against the list of `fields`.  Now you can specify a comma-separated list of multiple fields to have the same input format/output type applied.   Unit tests have been added for both new updates (string formatting and multiple field support).  As I looked at this one then I realized that maybe it would be good to add `recursive` support similar to what I have done in #9470 but I guess that can come at another day!   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","joshuagrisham","2020-10-24T17:31:21Z","2021-11-22T16:59:50Z"
"","9718","KAFKA-10832: Fix Log to use the correct ProducerStateManager instance when updating producers","I have fixed what looked like a potential bug to me.  **Bug:** The bug is that from within `Log.updateProducers(…)`, the code operates on the `producerStateManager` attribute of the `Log` instance instead of operating on an input parameter. Please see [this](https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L1464) LOC where it calls `producerStateManager.prepareUpdate` thus accessing the attribute from the `Log` object (see [this](https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L251)). This looks unusual particularly for `Log.loadProducersFromLog(...)` [path](https://github.com/apache/kafka/blob/1d84f543678c4c08800bc3ea18c04a9db8adf7e4/core/src/main/scala/kafka/log/Log.scala#L956). Here I believe we should be using the instance passed to the method, rather than the attribute from the `Log` instance. I have fixed the same in this PR.  I'm not sure (yet) if this bug has any drastic consequences (probably not), but it is still worthwhile making things consistent.  **Fix:** The fix is to explicitly pass the `ProducerStateManager` into `Log.updateProducers` and move the following methods: `Log.loadProducersFromLog` and `Log.updateProducers` into the `Log` companion object.  **Tests:** Rely on existing tests.","closed","","kowshik","2020-12-09T06:32:56Z","2020-12-12T00:34:47Z"
"","10339","MINOR: Remove redundant allows in import-control.xml","I found this problem while working on [KIP-719: Add Log4J2 Appender](https://cwiki.apache.org/confluence/display/KAFKA/KIP-719%3A+Add+Log4J2+Appender).  1. `org.apache.log4j` don't need to be allowed in shell, trogdor subpackage; they uses `slf4j`, not `log4j`. 2. `org.slf4j` don't need to be allowed in clients, server subpackage: `org.slf4j` is allowed globally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-17T05:33:47Z","2021-03-17T11:03:29Z"
"","10258","KAFKA-12408: Document omitted ReplicaManager metrics","I found it while configuring a JMX monitoring.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-04T07:16:06Z","2021-04-12T14:35:16Z"
"","9509","MINOR: rename metrics ""iotime-total"" to ""io-time-total"" and ""io-waitt…","I believe that is a kind of typo since it is not consistent to other related metrics name.  1. io-wait-time-ns-avg 1. io-time-ns-avg 1. io-wait-ratio 1. io-ratio  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-27T09:40:30Z","2020-10-27T12:23:33Z"
"","9845","MINOR: Bump Bouncy Castle Dependency","https://nvd.nist.gov/vuln/detail/CVE-2020-28052  It affects 1.65 and 1.66, which Kafka uses in 2.7+ test code","closed","","cyrusv","2021-01-07T22:14:08Z","2021-01-08T22:58:38Z"
"","9917","KAFKA-12200 Migrate connect:file module to JUnit 5","https://issues.apache.org/jira/browse/KAFKA-12200  replace junit4 methods with junit5 methods  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-01-16T02:49:22Z","2021-01-18T01:01:52Z"
"","9851","KAFKA-10769 Remove JoinGroupRequest#containsValidPattern as it is dup…","https://issues.apache.org/jira/browse/KAFKA-10769 .cc @chia7712   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2021-01-09T05:07:24Z","2021-04-07T03:39:48Z"
"","9541","KAFKA-10675: Add schema name to ConnectSchema.validateValue() error message","https://issues.apache.org/jira/browse/KAFKA-10675  The following error message ```java org.apache.kafka.connect.errors.DataException: Invalid Java object for schema type INT64: class java.lang.Long for field: ""moderate_time"" ``` can be confusing because `java.lang.Long` is acceptable type for schema `INT64`.   In fact, in this case `org.apache.kafka.connect.data.Timestamp` is used but this info is not logged.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","Iskuskov","2020-11-02T22:34:14Z","2021-07-10T05:35:02Z"
"","9559","HOTFIX: RequestContext constructor change","Hit an unfortunate merge conflict at the same time with https://github.com/apache/kafka/commit/5df8457e05f6808145e90f5637d7f8a4aed548d9  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-05T00:53:15Z","2020-11-05T03:43:18Z"
"","10004","MINOR : add memberId to JoinGroup Success message","Hi team,  Minor improvement to log the received memberId upon joining a group at INFO level on the memberside (consumer, worker) so that we can easily track broker logs with the member logs without the need to increase log verbosity  Thank you","closed","","nicolasguyomar","2021-01-29T16:36:23Z","2021-02-15T08:20:25Z"
"","10245","KAFKA-12400: Upgrade jetty to fix CVE-2020-27223","Here is the fix. The reason of [CVE-2020-27223](https://nvd.nist.gov/vuln/detail/CVE-2020-27223) was DOS vulnerability for Quoted Quality CSV headers and [patched in 9.4.37.v20210219](https://github.com/eclipse/jetty.project/security/advisories/GHSA-m394-8rww-3jr7).  This PR updates Jetty dependency into the following version, 9.4.38.v20210224.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-02T08:18:19Z","2021-03-03T04:43:59Z"
"","10177","KAFKA-12324: Upgrade jetty to fix CVE-2020-27218","Here is the fix. The reason of [CVE-2020-27218](https://nvd.nist.gov/vuln/detail/CVE-2020-27218) was [Incorrect recycling of `HttpInput`](https://bugs.eclipse.org/bugs/show_bug.cgi?id=568892) and [patched in 9.4.35.v20201120](https://github.com/eclipse/jetty.project/security/advisories/GHSA-86wm-rrjm-8wh8).  This PR updates Jetty dependency into the following version, 9.4.36.v20210114.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-02-22T15:36:27Z","2021-02-22T17:49:29Z"
"","10472","KAFKA-12613: Inconsistencies between Kafka Config and Log Config","Here is how I fixed the inconsistencies:  - `log.index.size.max.bytes` (`segment.index.bytes`): `KafkaConfig` seems right. (`atLeast(4)`) - `log.flush.interval.messages` (`flush.messages`): `KafkaConfig` seems right. (`atLeast(0)`) - `log.cleaner.delete.retention.ms`, `log.cleaner.min.compaction.lag.ms`, `log.cleaner.max.compaction.lag.ms`, `log.cleaner.min.cleanable.ratio`, `log.message.timestamp.difference.max.ms`:  `LogConfig` seems right. (i.e., Validation is omitted in `KafkaConfig`)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-04-04T10:22:47Z","2022-04-14T09:58:57Z"
"","9513","MINOR: improve `null` checks for headers","Headers are not allowed to be `null`, and thus the code does not check for `null` headers. However, we don't have proper guards in place and had NPE in the past if a header was `null`. This PR adds additional `null` checks to avoid that users create corrupted headers.  Cf https://issues.apache.org/jira/browse/KAFKA-8142 and https://issues.apache.org/jira/browse/KAFKA-10645  Call for review @mimaison","closed","","mjsax","2020-10-27T20:46:23Z","2020-10-29T23:45:50Z"
"","9792","KAFKA-10870: handle REBALANCE_IN_PROGRESS error in JoinGroup","handle REBALANCE_IN_PROGRESS error in JoinGroup to log correct info and request a rejoin.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-28T09:24:57Z","2021-01-04T22:20:25Z"
"","10203","KAFKA-12415: Prepare for Gradle 7.0 and restrict transitive scope for non api dependencies","Gradle 7.0 is required for Java 16 compatibility and it removes a number of deprecated APIs. Fix most issues preventing the upgrade to Gradle 7.0. The remaining ones are more complicated and should be handled in a separate PR. Details of the changes:  * Release tarball no longer includes includes test, sources, javadoc and test sources jars (these are still published to the Maven Central repository). * Replace `compile` with `api` or `implementation` - note that `implementation` dependencies appear with `runtime` scope in the pom file so this is a (positive) change in behavior * Add missing dependencies that were uncovered by the usage of `implementation` * Replace `testCompile` with `testImplementation` * Replace `runtime` with `runtimeOnly` and `testRuntime` with `testRuntimeOnly` * Replace `configurations.runtime` with `configurations.runtimeClasspath` * Replace `configurations.testRuntime` with `configurations.testRuntimeClasspath` (except for the usage in the `streams` project as that causes a cyclic dependency error) * Use `java-library` plugin instead of `java` * Use `maven-publish` plugin instead of deprecated `maven` plugin - this changes the commands used to publish and to install locally, but task aliases for `install` and `uploadArchives` were added for backwards compatibility * Removed `-x signArchives` line from the readme since it was wrong (it was a no-op before and it fails now, however) * Replaces `artifacts` block with an approach that works with the `maven-publish` plugin * Don't publish `jmh-benchmark` module - the shadow jar is pretty large and not particularly useful (before this PR, we would publish the non shadow jars) * Replace `version` with `archiveVersion`, `baseName` with `archiveBaseName` and `classifier` with `archiveClassifier` * Update Gradle and plugins to the latest stable version (7.0 is not stable yet) * Use `plugin` DSL to configure plugins * Updated notable changes for 3.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-24T16:37:31Z","2021-03-12T15:47:25Z"
"","9730","MINOR: fix error message","Got the accumulator lambda wrong in #9688   Call for review @ableegoldman","closed","streams,","mjsax","2020-12-11T01:55:58Z","2020-12-18T01:17:01Z"
"","10301","KAFKA-9831: increase max.poll.interval.ms to avoid unexpected rebalance","Found the root cause about why the the test `shouldNotViolateEosIfOneTaskFailsWithState` failed sometimes with unexpected committed/uncommitted result. The reason is the **unexpected rebalance** during committing messages, and it causes the fail over mechanism. And the reason why the rebalance is triggered is because we reduce the `max.poll.interval.ms` value to **5 seconds** (default is 5 mins) for the `shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances` test, which is trying to stall a thread, and wait for exceeding the `max.poll.interval.ms`, and trigger the rebalance. As we know, under `withState` situation, we have more things to handle with the state and additional topics..., so it explains why only the `shouldNotViolateEosIfOneTaskFailsWithState` is flaky, not other tests. In this rockDB tuning in kafka stream article also mentioned: >  If writes to RocksDB stall, the time interval between the invocations of poll() may exceed max.poll.interval.ms. To avoid this, you can increase max.poll.interval.ms in your Kafka Streams application.  ref: https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/?utm_source=twitter&utm_medium=organicsocial&utm_campaign=tm.devx_ch.bp-tune-rocksdb-for-kafka-streams-application_content.stream-processing#increase-max.poll.interval.ms  I increased the `max.poll.interval.ms` for the `withState` test to fix the flaky test. Also, did some enhancement: 1. add failed reason. Currently, the failed message is like: `Expected: <[...]>, but: was: <[...]>`, and it didn't tell us the result is committed or uncommitted result, before injected error or after. We have to map the stack trace to know it. Improve it 2. The fail() in `uncaughtException` will only fail the stream thread, not the test. fix it/ 3. add the capacity for the ArrayList to avoid memory reallocation. 4. Improve the comments, and add the state view for each phase  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","showuon","2021-03-11T04:46:45Z","2021-04-09T19:46:19Z"
"","9976","MINOR: Remove redundant apostrophe in doc","Found a redundant apostrophe appeared in doc. Also, if we wrap commands in","closed","","showuon","2021-01-27T02:05:22Z","2021-01-28T08:18:36Z"
"","10142","KAFKA-12294: forward auto topic request within envelope on behalf of clients","For the metadata auto topic creation case, it is favorable to use `Envelope` to wrap the CreateTopicsRequest alongside the original client principal for auditing purpose.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-02-17T20:46:57Z","2021-04-05T22:54:57Z"
"","9532","MINOR: Move upgraded docs from site to Kafka docs","For the 2.7 release, we need to migrate some docs changes that went to `kafka-site` but didn't go into `kafka/docs`  This PR covers the `documentation.html` and `upgrade.html` changes.  Once these are merged to trunk, I'll cherry-pick them to the 2.7 branch   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-10-29T19:55:35Z","2020-10-31T11:52:28Z"
"","9667","MINOR: Do not print log4j for memberId required","For MemberIdRequiredException, we would not print the exception at INFO with a full exception message since it may introduce more confusion that clearance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-12-01T18:49:26Z","2020-12-10T21:08:07Z"
"","10266","KAFKA-10357: Add validation method for internal topics","For KIP-698, we need a way to validate internal topics before we create them. This PR adds a validation method to the InternalTopicManager for that purpose.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-03-04T20:52:11Z","2021-03-11T17:55:30Z"
"","10317","KAFKA-10357: Add setup method to internal topics","For KIP-698, we need a way to setup internal topics without validating them. This PR adds a setup method to the InternalTopicManager for that purpose.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-03-15T12:19:54Z","2021-03-18T16:52:09Z"
"","10460","KAFKA-10357: Use validate and setup during internal topics creation","For KIP-698, we introduce new setup and validation methods for internal topics. This PR uses the newly added methods and removes the old methods.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","cadonna","2021-04-01T15:41:38Z","2021-07-31T01:26:08Z"
"","10049","Refactor MetadataCache for Raft metadata","For 2.8 we will keep the existing MetadataCache implementation intact. Instead of replacing it with the changes needed for Raft, we will extract an interface for both implementations to use. KafkaServer (ZK-backed clusters) will only pick the ZK metadata cache impl.  The only real change to production code here will be the use of MetadataBroker instead of Broker for MetadataCache#getAliveBroker and MetadataCache#getAliveBrokers. This was done to eliminate unnecessary divergence between the two implementations.","closed","kip-500,","mumrah","2021-02-04T02:08:59Z","2021-02-06T00:25:26Z"
"","9609","KAFKA-6687: restrict DSL to allow only Streams from the same source topics","Followup to https://github.com/apache/kafka/pull/9582  Will leave the ability to create multiple KTables from the same source topic as followup work. Similarly, creating a KStream and a KTable from the same topic can be tackled later if need be","closed","","ableegoldman","2020-11-18T04:57:40Z","2020-12-02T19:26:09Z"
"","9987","KAFKA-10895: Gracefully handle invalid JAAS configs (follow up fix)","Follow-up to https://github.com/apache/kafka/pull/9806  If an invalid JAAS config is present on the worker, invoking `Configuration::getConfiguration` throws an exception. The changes from #9806 cause that exception to be thrown during plugin scanning, which causes the worker to fail even if it is not configured to use the basic auth extension at all.  This follow-up handles invalid JAAS configurations more gracefully, and only throws them if the worker is actually configured to use the basic auth extension, at the time that the extension is instantiated and configured.  Two unit tests are added to test the green-path and red-path behavior of the extension when it encounters well-formed and ill-formed JAAS configurations, respectively.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-01-27T22:16:13Z","2021-02-03T21:51:04Z"
"","10236","MINOR; Small refactor in `GroupMetadata`","Follow-up of https://github.com/apache/kafka/pull/9958#discussion_r582662669.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-01T14:44:29Z","2021-03-01T17:25:27Z"
"","9696","MINOR: Clean up streams metric sensors","Follow-up from https://github.com/apache/kafka/pull/9614, updates streams metrics sensor logic","closed","","lct45","2020-12-04T20:50:08Z","2020-12-09T09:51:49Z"
"","9821","KAFKA-5876: Apply UnknownStateStoreException for Interactive Queries","follow-up #8200  KAFKA-5876's PR break into multiple parts, this PR is part 2 - apply UnknownStateStoreException   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vitojeng","2021-01-04T06:53:50Z","2021-05-09T22:37:19Z"
"","9974","KAFKA-10500: Add docs for failed stream thread metric","Follow up to https://github.com/apache/kafka/pull/9614, adds documentation for failed-stream-threads metric  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lct45","2021-01-26T23:38:21Z","2021-02-02T18:12:37Z"
"","10420","MINOR: update Streams upgrade notes with removed APIs","Follow up to #10284 and #10300  Call for review @guozhangwang @ableegoldman","closed","docs,","mjsax","2021-03-27T22:38:30Z","2021-03-28T18:36:45Z"
"","10122","KAFKA-12326: Corrected regresion in MirrorMaker 2 executable introduced with KAFKA-10021","Fixes the recent change (#9780) to the `MirrorMaker` class (used only in the MirrorMaker 2 executable) that uses a `SharedTopicAdmin` client as part of Connect, so that the class passes the distributed worker properties rather than the MM2 properties into the `SharedTopicAdmin`.  The error running the MirrorMaker 2 executable was reproduced manually without this fix, and then this one-line change was manually verified to correct the problem. Unfortunately, the `MirrorMaker` class as currently written is not easily tested, so this change only includes the fix to unblock the 2.6.2 release and relies upon the aforementioned manual testing for verification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-02-12T18:16:10Z","2021-02-14T03:33:40Z"
"","10474","KAFKA-12602: Fix LICENSE file","Fixes the LICENSE files that we ship with our releases: * the source-distribution license included wrong and unnecessary dependencies * the binary-distribution license was missing most of our actual dependencies  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2021-04-05T03:50:21Z","2021-04-06T20:08:27Z"
"","10462","KAFKA-10847: Fix spurious results on left/outer stream-stream joins","Fixes the issue with https://issues.apache.org/jira/browse/KAFKA-10847.  To fix the above problem, the left/outer stream-stream join processor uses a buffer to hold non-joined records for some time until the window closes, so they are not processed if a join is found during the join window time. If the window of a record closes and a join was not found, then this should be emitted and processed by the consequent topology processor.  A new time-ordered window store is used to temporary hold records that do not have a join and keep the records keys ordered by time. The `KStreamStreamJoin` has a reference to this new store . For every non-joined record seen, the processor writes it to this new state store without processing it. When a joined record is seen, the processor deletes the joined record from the new state store to prevent further processing.   Records that were never joined at the end of the window + grace period are emitted to the next topology processor. I use the stream time to check for the expiry time for determinism results . The `KStreamStreamJoin` checks for expired records and emit them every time a new record is processed in the join processor.  The new state store is shared with the left and right join nodes. The new store needs to serialize the record keys using a combined key of ``. This key combination helps to delete the records from the other join if a joined record is found. Two new serdes are created for this, `KeyAndJoinSideSerde` which serializes a boolean value that specifies the side where the key is found, and `ValueOrOtherValueSerde` that serializes either V1 or V2 based on where the key was found.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","spena","2021-04-01T18:43:28Z","2021-04-29T00:57:28Z"
"","9591","DOCS-5813: Update tip about multi-cluster support","Fixes internal ticket DOCS-5813.","open","","JimGalasyn","2020-11-12T22:24:35Z","2020-11-12T22:24:35Z"
"","10392","KAFKA-12435: Fix javadoc errors","Fixes errors while generating javadoc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vvcephei","2021-03-24T02:58:17Z","2021-03-24T18:55:31Z"
"","10453","DOCS-7723: Fix left/outer join descriptions in Streams DSL reference topic","Fixes descriptions of tombstone semantics in KTable-KTable joins, per Slack [discussion](https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1617037244337200?thread_ts=1616955220.317900&cid=C48AHTCUQ). @mjsax This PR ports the fix from PR 594.","closed","docs,","JimGalasyn","2021-03-31T18:57:50Z","2021-04-09T18:56:15Z"
"","10210","MINOR: fix syntax error in upgrade_test.py","Fixes a syntax error introduced in https://github.com/apache/kafka/pull/10105 that is causing 39 system tests to fail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-25T18:57:41Z","2021-02-25T20:14:44Z"
"","9655","MINOR: fix listeners doc to close  properly","Fixes a problem with an incorrectly closed `` in the doc for `listeners`.","closed","","tombentley","2020-11-25T16:13:12Z","2020-11-30T20:42:01Z"
"","9822","KAFKA-10860: Fix NPE in JmxTool, add tests","Fixes [KAFKA-10860](https://issues.apache.org/jira/browse/KAFKA-10860) and adds a unit test.  The existing code was not easily testable, so refactored to allow the main logic to be executed without invoking `System.exit` or equivalent. The tests added here cover most of the non-connection options, but have to use `--one-time`. I just use the current JVM's MBean server for these tests. Testing the connection code would require spinning up a JVM to connect to, or using the Attach API (which is prevented by https://bugs.openjdk.java.net/browse/JDK-8180425).  Although `--wait` was described as not supported in the presence of a pattern but this wasn't validated previously. Supporting that actually makes the code simpler by avoiding the need to special-case the presence of a pattern. The semantics are that we wait until each of the given patterns selects at least one `ObjectName`, which is consistent with the behaviour for a non-patterned `--wait`.","open","","tombentley","2021-01-05T11:37:32Z","2021-02-03T13:19:23Z"
"","9562","MINOR: Fix param doc in FinalizedFeatureChangeListener.initOrThrow","Fixed the param doc in `FinalizedFeatureChangeListener.initOrThrow` method. The parameter `waitOnceForCacheUpdateMs` is expected to be > 0, but the doc was incorrect.","closed","","kowshik","2020-11-05T03:07:38Z","2020-11-05T07:01:31Z"
"","9834","MINOR: fix typo in TimeIndex","Fixed a syntax error.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-01-06T17:14:02Z","2021-01-14T21:25:48Z"
"","9811","MINOR: rename produce to produces","Fixed a syntax error where the predicate needed to use the singular.","closed","","wenbingshen","2021-01-01T19:17:53Z","2021-01-04T05:20:49Z"
"","9939","MINOR: fix @link tag in javadoc","fix warning: @link: reference not found  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-01-20T18:16:08Z","2021-02-01T04:40:03Z"
"","10144","MINOR: Fix Typo in MirrorMaker README file","Fix Typo in metric name of MirrorMaker README file from 'replication-latecny-ms' to 'replication-latency-ms'  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","iczellion","2021-02-18T02:15:46Z","2021-02-18T09:54:02Z"
"","10277","KAFKA-9914: Fix replication cycle detection","Fix replication cycle detection algorithm to detect recursive topic cycles also if kafka cluster naming is not consistent.  Unit tests updated to reflect fixed cycle detection algorithm  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","tvainika","2021-03-08T06:55:37Z","2021-08-13T12:13:17Z"
"","9812","MINOR: fix log message","Fix log message during shutdown  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2021-01-01T23:52:00Z","2021-01-03T13:11:32Z"
"","9524","MINOR: Fix documentation for KIP-585","Fix invalid path for `TopicNameMatches` predicate in KIP-585 docs","closed","","Iskuskov","2020-10-28T19:11:41Z","2021-01-04T02:56:40Z"
"","10413","MINOR: fix the wrong/missing anchor caused link error","Finding this link issue when reading the streaming doc: https://kafka.apache.org/27/documentation/streams/developer-guide/config-streams.html#optional-configuration-parameters  1. `processing.guarantee`: the anchor link name has typo: guarantedd -> guarantee 2. `upgrade.from`: we didn't set the span element id to allow the anchor link.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","showuon","2021-03-26T08:17:43Z","2021-03-28T18:40:01Z"
"","9865","Fill in the 2.8 release note for Authorizer","Fill in the 2.8 release note for Authorizer","closed","","ctan888","2021-01-11T21:11:40Z","2021-02-10T11:24:13Z"
"","9621","KAFKA-9892: Producer state snapshot needs to be forced to disk","FileChannel.close() does not guarantee modified buffer would be written on the file system. We are changing  it with force() semantics to enforce file buffer and metadata written to filesystem ( FileChannel.force(true) updates buffer and metadata).  *Summary of testing strategy (including rationale) I have run unittests after making the changes.","closed","","bristy","2020-11-19T11:16:49Z","2020-12-09T22:13:37Z"
"","9710","MINOR: Remove redundant default parameter values in call to LogSegment.open","Few call sites in `Log.scala` were passing redundant default values in their calls to `LogSegment.open(...)`. In this PR, I've cleaned these up. For reference, here is a link to the definition of `LogSegment.open`: https://github.com/apache/kafka/blob/d2521855b8eb8a0dbb6f94cd9bb5093276fb7db2/core/src/main/scala/kafka/log/LogSegment.scala#L660-L661   **Test plan:** Relying on existing unit and integration tests.","closed","","kowshik","2020-12-07T21:16:24Z","2020-12-08T00:42:19Z"
"","9836","KAFKA-10866: Add metadata to ConsumerRecords","Expose fetched metadata via the ConsumerRecords object as described in KIP-695.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2021-01-06T20:10:21Z","2021-02-03T01:39:16Z"
"","9616","KAFKA-10091: KIP-695: Deterministic semantics for task idling","Expose fetched metadata per partition so that Streams can enqueue records for each input and select among them for processing in timestamp order.  Implements KIP-695  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2020-11-18T19:31:03Z","2021-07-20T21:58:31Z"
"","9620","MINOR: Get rid of generic from FetchResponse","Except for LazyDownConversionRecords, we don't use the generic. We should handle the specify case directly instead of making generic in whole project.  In short, this patch is a kind of cleanup.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-11-19T05:57:26Z","2021-02-12T19:44:13Z"
"","10283","MINOR: KRPC supports to get true type from entity type","Except for array type, the usage of `EntityType` is almost same to `type`. It makes our KRPC json file verbose. We can add a bit sugar to our KRPC that `EntityType` can be converted to true type in generated code.   ### TODO (if this idea get approved) 1. remove all redundant declaration from all json files. 2. add `ThrottleTime` entity type  3. add `TopicId` entity type  4. add `ErrorCode` entity type  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-03-09T06:29:21Z","2021-03-09T12:10:40Z"
"","9552","KAFKA-10656: Log the feature flags received by the client","Example log line:  [2020-11-03 17:47:17,076] DEBUG Node 0 has finalizedFeaturesEpoch: 42, finalizedFeatures: [FinalizedFeatureKey(name='feature_1', maxVersionLevel=2, minVersionLevel=1), FinalizedFeatureKey(name='feature_2', maxVersionLevel=4, minVersionLevel=3)], supportedFeatures: [SupportedFeatureKey(name='feature_1', minVersion=1, maxVersion=2), SupportedFeatureKey(name='feature_2', minVersion=3, maxVersion=4)] (org.apache.kafka.clients.NetworkClient:926)","closed","","tombentley","2020-11-03T17:50:08Z","2020-12-16T14:10:23Z"
"","9921","MINOR: Ensure compile and runtime classpaths have consistent versions","Ensure that runtime-only dependencies don't cause a different version to be used. More specifically:  > The relationship is directed, which means that if the runtimeClasspath configuration has to be resolved, Gradle will first resolve the compileClasspath and then ""inject"" the result of resolution as strict constraints into the runtimeClasspath.  For more details, see: https://docs.gradle.org/6.8/userguide/resolution_strategy_tuning.html#sec:configuration_consistency  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-17T19:38:49Z","2021-01-19T16:12:27Z"
"","10148","Update KafkaConsumerMetrics.java","Enhance the documentation with the metric unit  which is milliseconds","closed","","nicolasguyomar","2021-02-18T16:19:49Z","2021-02-20T03:33:20Z"
"","10213","KAFKA-12375: fix concurrency issue in application shutdown","enforce rebalance is now used in a thread safe way  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-02-26T01:43:20Z","2021-02-26T20:36:39Z"
"","10184","MINOR: enable topic deletion in the KIP-500 controller","Enable the new KIP-500 controller to delete topics.  Fix a bug where feature level records were not correctly replayed.  Fix a bug in TimelineHashMap#remove where the wrong type was being returned.","closed","kip-500,","cmccabe","2021-02-23T01:45:13Z","2021-03-04T19:44:57Z"
"","9972","KAFKA-8779: Reintroduce flaky tests","Enable some previously-flaky PlaintextAdminIntegrationTests.  The cause of the flakiness appears to have been lack of test isolation between those tests and other tests which manipulate log levels (which are effectively global to the JVM). In general there are quite a few tests which manipulate log4j and/or make assertions on log4j global state, so introduce some of helpers to try to get test isolation across the whole of the broker code.  1. `logLock` in `TestUtils` provides the mutual exclusion. We can't use `synchronized` on an `Object` because some tests change the logging in their `@Before(Class)` and restore in ``@After(Class)`. 2. `TestUtils.withLogLock` encapsulates the isolation logic, and it used by most of the log-changing tests, rather than them using `logLock` directly. 3. `LogCaptureAppender` now uses `withLogLock` too, and there's no reason not to apply the same state management pattern via `captureLogging`.","open","","tombentley","2021-01-26T10:02:38Z","2022-02-09T13:14:41Z"
"","10360","KAFKA-12508: Emit records with same value and same timestamp","Emit on change introduced in Streams with KIP-557 might lead to data loss if a record is put into a source KTable and emitted downstream and then a failure happens before the offset could be committed. After Streams rereads the record, it would find a record with the same key, value and timestamp in the KTable (i.e. the same record that was put into the KTable before the failure) and not forward it downstreams. Hence, the record would never be processed downstream of the KTable which breaks the at-least-once processing guarantee.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-03-19T13:47:39Z","2021-03-22T19:37:13Z"
"","10019","MINOR: Introduce KafkaBroker trait for use in dynamic reconfiguration","Dynamic broker reconfiguration needs to occur for both ZooKeeper-based brokers and brokers that use a Raft-based metadata quorum.  `DynamicBrokerConfig` currently operates on `KafkaServer`, but it needs to operate on `BrokerServer` (the broker implementation that will use the Raft metadata log) as well. This PR introduces a `KafkaBroker` trait to allow dynamic reconfiguration to work with either implementation.  Existing tests are sufficient to detect bugs and regressions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-01T23:07:42Z","2021-02-04T19:28:45Z"
"","9533","KAFKA-10751: Generate logs to help estimate the amount of data loss during ULE","During Unclean Leader Election, there could be data loss due to truncation at the resigned leader. This PR tries to add more logs to understand the scale of message loss during an unclean leader election.  Suppose there are 3 brokers that has replicas for a given partition: Broker A (leader) with largest offset 9 (log end offset 10) Broker B (follower) with largest offset 4 (log end offset 5) Broker C (follower) with largest offset 1 (log end offset 2)  Only the leader A is in the ISR with B and C lagging behind. Now an unclean leader election causes the leadership to be transferred to C. Broker A would need to truncate 8 messages, and Broker B 3 messages.  Case 1: if these messages have been produced with acks=0 or 1, then clients would experience 8 lost messages. Case 2: if the client is using acks=all and the partition's minISR setting is 2, and further let's assume broker B dropped out of the ISR after receiving the message with offset 4, then only the messages with offset<=4 have been acked to the client. The truncation effectively causes the client to lose 3 messages.  Knowing the exact amount of data loss involves knowing the client's acks setting when the messages are produced, and also whether the messages have been sufficiently replicated according to the MinISR setting. Without getting too involved, this PR reduces the requirement from getting the exact data loss numbers to getting an ESTIMATE of the data loss. Specifically this PR adds logs during truncation to show the log end offset, number of messages truncated, and number of bytes truncated.  Testing: I manually tested a ULE case where the resigned leader contains two more messages than the newly elected leader, and saw the following log on the resigned leader [2020-10-29 15:43:50,154] INFO [Log partition=topic0-0, dir=/tmp/kafka-logs-0] Truncated to offset 0 from the log end offset 2 with 2 messages and 85 bytes truncated (kafka.log.Log)  The bytes truncated match the result shown by the kafka-dump-log.sh script before the truncation Starting offset: 0 baseOffset: 0 lastOffset: 1 count: 2 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 12 isTransactional: false isControl: false position: 0 CreateTime: 1604010918227 size: 85 magic: 2 compresscodec: NONE crc: 324998737 isvalid: true   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2020-10-29T21:47:18Z","2022-04-12T17:56:49Z"
"","9554","KAFKA-10679: [Streams] migrate kafka-site updated docs to kafka/docs","During the AK website upgrade, changes made to kafka-site weren't migrated back to kafka-docs.  This PR is an attempt at porting the streams changes to kafka/docs  For the most part, the bulk of the changes in the PR are cosmetic.  For testing:  1. I reviewed the PR diffs 2. Rendered the changes locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-11-03T21:44:13Z","2020-11-04T13:34:08Z"
"","9551","KAFKA-10679: Migrate upgrade changes from site to kafka/docs","During the AK website upgrade, changes made to `kafka-site` weren't migrated back to `kafka-docs.`  This PR is an initial attempt at porting the changes to `kafka/docs,` but it does not include the streams changes. Those will come in a separate PR.  For the most part, the bulk of the changes in the PR are cosmetic.  Only the `introduction.html` has substantial changes, but it's a direct port from the live documentation.  For testing:  1. I reviewed the PR diffs  2. Rendered the changes locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-11-03T17:19:53Z","2020-11-03T18:46:20Z"
"","10241","MINOR: time and log producer state recovery phases","During a slow log recovery it's easy to think that loading `.snapshot` files is a multi-second process. Often it isn't the snapshot loading that takes most of the time, rather it's the time taken to further rebuild the producer state from segment files. This PR times both snapshot load and segment recovery phases to better indicate what is taking time.  Example test output: ``` [2021-03-01 22:35:28,129] INFO [Log partition=foo-0, dir=/var/folders/cb/5my51vjd1js380qcr_v245bh0000gp/T/kafka-16876782135717603479] Reloading from producer snapshot and rebuilding producer state from offset 0  [2021-03-01 22:35:28,129] INFO [Log partition=foo-0, dir=/var/folders/cb/5my51vjd1js380qcr_v245bh0000gp/T/kafka-16876782135717603479] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0  [2021-03-01 22:35:28,131] INFO Completed load of Log(dir=/var/folders/cb/5my51vjd1js380qcr_v245bh0000gp/T/kafka- ```","closed","","lbradstreet","2021-03-02T05:49:10Z","2021-03-02T10:14:43Z"
"","9490","Pin ducktape to version 0.7.10","Ducktape version 0.7.10 pinned paramiko to version 2.3.2 to deal with random SSHExceptions confluent had been seeing since ducktape was updated to a later version of paramiko.  The idea is that we can backport ducktape 0.7.10 change as far back as possible, while 2.7 and trunk can update to 0.8.0 and python3 separately.  Tested: In progress, but unlikely to affect anything, since the only difference between ducktape 0.7.9 and 0.7.10 is paramiko version downgrade.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2020-10-24T02:08:26Z","2020-10-24T16:32:48Z"
"","9488","Pin ducktape to version 0.7.10","Ducktape version 0.7.10 pinned paramiko to version 2.3.2 to deal with random `SSHException`s confluent had been seeing since ducktape was updated to a later version of paramiko. This PR pins both the version in `setup.py` and in `ducker-ak`'s `Dockerfile` to the same version. Previously ducker version was pinned to 0.8.0.  I'll send a separate PR that pins both versions to 0.8.0 (unless someone is already working on that separately) - the idea is that we can backport ducktape 0.7.10 change as far back as possible, while keep the ducktape 0.8.0 change in trunk only (unless we plan to backport python3 changes).  Tested: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-10-23--001.1603440055--stan-confluent--ducktape-710--3c51234d0/report.html  There are 10 failing tests, 9 of which have been failing before this change, and 1 (streams_upgrade_test) I've seen failing on different branches recently as well, though it was passing on trunk as of couple of days ago. Since ducktape version only changed the paramiko dependencies, I don't think it could've affected it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2020-10-23T23:54:19Z","2020-10-24T02:09:30Z"
"","9932","MINOR: Upgrade ducktape to version 0.7.11","Ducktape 0.7.11 fixes a bug where a unicode exception message would cause test runner to hang up and never finish. This change should be backported to all the branches using ducktape 0.7.10.  An update to ducktape 0.8.0 will be released separately (and if used with python 3 it probably won't be an issue anyway)","closed","tests,","stan-confluent","2021-01-19T20:27:49Z","2021-01-23T06:50:35Z"
"","10265","[KAFKA-7718] KIP-634 implementation","Draft implementation of [KIP-634](https://cwiki.apache.org/confluence/display/KAFKA/KIP-634%3A+Complementary+support+for+headers+and+record+metadata+in+Kafka+Streams+DSL)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jeqo","2021-03-04T20:27:26Z","2022-02-10T17:21:02Z"
"","9826","KAFKA-10816: Initialize REST endpoints only after the herder has started","Do this by making DistributedHerder#start() block until the herder has really started, thus preventing the initialization of the REST resources until that point.","open","connect,","tombentley","2021-01-05T15:54:13Z","2022-07-04T06:36:59Z"
"","9550","[DO NOT MERGE] Temporary PR to track test failure in Jenkins build","DO NOT MERGE  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-11-03T16:58:08Z","2020-11-04T19:55:06Z"
"","10290","KAFKA-12445: Improve the display of ConsumerPerformance indicators","Display the indicator variable name and indicator test value in the form of a table, so that the meaning of each measurement value can be clearly expressed.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-03-10T08:04:23Z","2021-04-16T08:00:24Z"
"","9995","Add log about broker info when getting UNKNOWN_TOPIC_OR_PARTITION error","Description: This patch adds additional logs in the `NetworkClient` to print metadata info such as leader id and epoch for partitions that fail metadata fetch with UNKNOWN_TOPIC_OR_PARTITION error. This will help with a Venice investigation: https://jira01.corp.linkedin.com:8443/browse/LIKAFKA-33540 Testing: N/A, since it's for logs.","closed","","sudoa","2021-01-29T00:44:24Z","2021-01-29T00:48:26Z"
"","10059","KAFKA-8562: SaslChannelBuilder - avoid (reverse) DNS lookup while building underlying SslTransportLayer","Description: As suggested by @omkreddy in this [comment](https://issues.apache.org/jira/browse/KAFKA-8562?focusedCommentId=16912437&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16912437), implemented avoiding (reverse) DNS lookup while building underlying SslTransportLayer.  How the problem was manifested: When clients or other brokers are connecting to a broker using SASL_SSL, a broker was doing (reverse) DNS lookup and if there is no PTR Record, the lookup could last several seconds, which in the end caused big latencies on several parts of the system... replication, consume requests and produce requests. Here you can see a recorded sample:   Also, here is a Wireshark packet capture for DNS requests, and in this case you can see that it lasted more then 11 seconds: ![KAFKA-8562 wireshark dns packet capture](https://user-images.githubusercontent.com/1514332/106960332-37650c80-673c-11eb-91ab-9cab8dd4873d.png) When using PLAINTEXT or SSL, this problem doesn't manifest.  Solution: In #2835 , @rajinisivaram already added a helper method `SslChannelBuilder.peerHost`, so I just moved it to a new class called `ChannelBuilderUtils` and used it in `SaslChannelBuilder.buildTransportLayer` method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dpoldrugo","2021-02-04T22:00:42Z","2021-03-05T09:28:02Z"
"","9504","MINOR; DescribeUserScramCredentialsRequest API should handle request with users equals to `null`","DescribeUserScramCredentialsRequest states that all users are described when `Users` is empty or `null`. `null` is not handled at the moment and throws an NPE.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-10-26T13:49:42Z","2020-10-26T16:28:10Z"
"","10451","KAFKA-12952: Remove deprecated LogConfig.Compact in 3.0","Deprecated since 1.0.0 for misleading name.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-31T14:01:45Z","2021-04-01T12:14:20Z"
"","10195","MINOR: Remove use of deprecated Gradle syntax","Deprecated in:  https://github.com/gradle/gradle/commit/5c458c522af58612f2bfa85568e07aa29e585931  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soarez","2021-02-23T19:02:56Z","2021-05-18T16:53:23Z"
"","9847","KAFKA-10703: Better handling and doc for config defaults of topics","Default configs are not supported for topic resources, so document this and prevent the creation of such ConfigResources.","open","","tombentley","2021-01-08T11:51:04Z","2021-02-25T09:46:15Z"
"","10190","KAFKA-12336 Custom stream naming does not work while calling stream[K…","Custom stream naming does not work while calling stream[K, V](topicPattern: Pattern)","closed","streams,","g1geordie","2021-02-23T10:04:15Z","2021-06-24T16:55:30Z"
"","10044","MINOR: Word count should account for extra whitespaces between words","Currently, the word count demo only accounts for single whitespaces between words. This fix uses a more generic regex to skip multiple spaces or tabs that may occur between words. Corresponding unit tests were updated.   Also, the quickstart docs already use the correct regex.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","wicknicks","2021-02-03T23:17:29Z","2021-02-05T02:54:09Z"
"","10309","KAFKA-12181; Loosen raft fetch offset validation of remote replicas","Currently the Raft leader raises an exception if there is a non-monotonic update to the fetch offset of a replica. In a situation where the replica had lost it disk state, this would prevent the replica from being able to recover. In this patch, we relax the validation to address this problem. It is worth pointing out that this validation could not be relied on to protect from data loss after a voter has lost committed state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-03-12T22:14:01Z","2021-08-11T13:15:02Z"
"","10041","MINOR: Prevent creating partition.metadata until ID can be written","Currently the partition.metadata file is created when the log is created. However, clusters with older inter-broker protocols will never use this file. This PR moves the creation of the file to when we write to the file.   This PR also deletes the partition.metadata file on startup if the IBP version is lower than 2.8.   I will be looking at benchmarks to see how these changes affect LISR request processing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-03T18:15:19Z","2021-02-10T23:49:27Z"
"","9545","[mm2] Allow Checkpoints for consumers using static partition assignments","Currently mm2 is not generating checkpoints for upstream consumers using static partition assignments. The reason is that these consumers are being explicitly filtered out. I couldn't find why this filter is being applied. This PR removes this limitation.  I've tested this change (but only in one direction: source->target) and it does not break any functionality. Even KAFKA-9076 continues working fine and allows consumer offset sync for consumers using static assignments.","closed","","scanterog","2020-11-03T01:35:34Z","2020-11-19T15:24:17Z"
"","10039","MINOR: Defer log recovery until LogManager startup","Currently log recovery begins as soon as we instantiate `LogManager`, but when using a Raft-based metadata quorum we won't have configs until after we catch up on the metadata log.  We therefore defer log recovery until we actually invoke `startup()` on the `LogManager` instance.  This timing difference has no effect when using ZooKeeper because we immediately invoke `startup()` on the instantiated instance, but it gives us the necessary flexibility for accurate log recovery with updated configs when using a Raft-based metadata quorum.  The `LogCleaner` is currently instantiated during construction just after log recovery completes, and then it is started in `startup()`.  As an extra precaution, since we are no longer performing recovery during construction, we both instantiate and start the log cleaner in `startup()` after log recovery completes.  We also convert `LogManager` to use a `ConfigRepository` to load topic configs (which can override the default log configs) instead of having a hard-coded dependency on ZooKeeper.  We retrieve the topic configs when we invoke `startup()` -- which again is effectively no different from a timing perspective than what we do today for the ZooKeeper case.  One subtlety is that currently we create the log configs for every topic at this point -- if a topic has no config overrides then we associate a copy of the default configuration with the topic inside a map, and we retrieve the log configs for that topic's partitions from from that map during recovery.  This PR makes a change to this series of events as follows.  We do not associate a copy of the the default configuration with a topic in the map if the topic has no configs set when we query for them.  This saves some memory -- we don't unnecessarily copy the default config many times -- but it also means we have use the default log configs for that topic later on when recovery for each of its partitions begins.  The difference is that the default configs are dynamically reconfigurable, and they could potentially change between the time when we invoke `startup()` and when log recovery begins (log recovery can begin quite some time after `startup()` is invoked if shutdown was unclean).  Prior to this patch such a change would not be used; with this patch they could be if they happen before recovery begins.  This actually is better -- we are performing log recovery with the most recent known defaults when a topic had no overrides at all. Also, `Partition.createLog` has logic to handle missed config updates, so the behavior is eventually the same.  The transition of the broker state from `STARTING` to `RECOVERY` currently happens within the `LogManager`, and it only occurs if the shutdown was unclean.  We move this transition into the broker as it avoids passing a reference to the broker state into the `LogManager`.  We also now always transition the broker into the `RECOVERY` state as dictated by [the KIP-631 broker state machine](https://cwiki.apache.org/confluence/display/KAFKA/KIP-631%3A+The+Quorum-based+Kafka+Controller#KIP631:TheQuorumbasedKafkaController-TheBrokerStateMachine).  Finally, a few clean-ups were included. One worth highlighting is that `Partition` no longer requires a `ConfigRepository`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-03T16:53:43Z","2021-02-08T14:53:03Z"
"","9824","MINOR: Update log statements in alterBrokerConfigs/alterTopicConfigs methods","Current below log statements are not useful. This PR logs readable/masked configs during alterBrokerConfigs/alterTopicConfigs method call.  `[Admin Manager on Broker 1]: Updating topic test with new configuration kafka.server.KafkaConfig@c9ba35e3`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-01-05T15:41:55Z","2021-01-06T14:29:37Z"
"","10326","Avoid newly replicating brokers in RackAwareReplicaSelector","cross rack boundaries if my rack does not contain a replica which has _ever_ appeared in ISR. otherwise, if we have just added a new broker to the replica set and are now behind on replication, it may take a while to catch up and we may become bottlenecked/blocked until that replica catches up.  testing strategy to be determined after discussion. #6832 updated ReplicaSelectorTest so I've done so too; the Scala server stuff I'm not sure how to test. the one existing test on that function isn't informative.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lizthegrey","2021-03-16T02:34:36Z","2021-05-20T15:15:24Z"
"","9731","MINOR: Make Log.recordVersion private and other small cleanups","Couple small cleanups in this PR: - I've converted `Log.recordVersion` to be private, since it is used only inside `Log` class. I've also changed one of the call sites to reuse the same method. - In a few classes I've removed a redundant default parameter `fileAlreadyExists = false` in a call to `LogSegment.open`.  **Tests:** Relying on existing unit & integration tests.","closed","","kowshik","2020-12-11T03:22:18Z","2020-12-14T07:20:21Z"
"","9540","KAFKA-10669: Make CurrentLeaderEpoch field ignorable and set MaxNumOffsets field default to 1","Couple of failures observed after KAFKA-9627: Replace ListOffset request/response with automated protocol (https://github.com/apache/kafka/pull/8295)  1. Latest consumer fails to consume from 0.10.0.1 brokers. Below system tests are failing kafkatest.tests.client.client_compatibility_features_test.ClientCompatibilityFeaturesTest kafkatest.tests.client.client_compatibility_produce_consume_test.ClientCompatibilityProduceConsumeTest  Solution: Current default value for MaxNumOffsets is 0. because to this brokers are not returning offsets for v0 request. Set default value for MaxNumOffsets field to 1.  This is similar to previous [approach] (https://github.com/apache/kafka/blob/2.6/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java#L204)   2. In some scenarios, latest consumer fails with below error when connecting to a Kafka cluster which consists of newer and older (<=2.0) Kafka brokers  `org.apache.kafka.common.errors.UnsupportedVersionException: Attempted to write a non-default currentLeaderEpoch at version 3`  Solution: After #8295, consumer can set non-default CurrentLeaderEpoch value for v3 and below requests. One solution is to make CurrentLeaderEpoch ignorable.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-10-31T20:48:41Z","2020-11-02T18:10:12Z"
"","9598","KAFKA-10701 : First line of detailed stats from consumer-perf-test.sh incorrect","Corrected the initialization of joinStart variable to fix the first line of the consumer performance stats. Since the joinStart was initialized with 0 , so when the first time onPartitionAssigned method is called it makes the joinTime as System.currentTimeMillis which inturn made the fetchTimeMs incorrect (-ve)","closed","","lijubjohn","2020-11-16T11:32:57Z","2020-11-19T14:18:05Z"
"","10165","KAFKA-12350: Correct the default value in doc","Correct the wrong default value in document.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-20T09:16:17Z","2021-02-24T17:52:35Z"
"","9992","KAFKA-10759 Add ARM build stage","Copy of https://github.com/apache/kafka/pull/9872, but opened by a committer so Jenkins will actually apply the Jenkinsfile changes","closed","","mumrah","2021-01-28T20:04:28Z","2021-03-03T20:57:28Z"
"","9653","MINOR: Remove unnecessary statement from WorkerConnector#doRun","Continue is unnecessary as the last statement in a loop. So I remove it from WorkerConnector.","closed","","UnityLung","2020-11-25T11:15:55Z","2020-11-25T14:48:02Z"
"","9714","MINOR: Remove connection id from Send and consolidate request/message utils","Connection id is now only present in `NetworkSend`, which is now the class used by `Selector`/`NetworkClient`/`KafkaChannel` (which works well since `NetworkReceive` is the class used for received data).  The previous `NetworkSend` was also responsible for adding a size prefix. This logic is already present in `SendBuilder`, but for the minority of cases where `SendBuilder` is not used (including a number of tests), we now have `ByteBufferSend.sizePrefixed()`.  With regards to the request/message utilities: * Renamed `toByteBuffer`/`toBytes` in `MessageUtil` to `toVersionPrefixedByteBuffer`/`toVersionPrefixedBytes` for clarity. * Introduced new `MessageUtil.toByteBuffer` that does not include the version as the prefix. * Renamed `serializeBody` in `AbstractRequest/Response` to `serialize` for symmetry with `parse`. * Introduced `RequestTestUtils` and moved relevant methods from `TestUtils`. * Moved `serializeWithHeader` methods that were only used in tests to `RequestTestUtils`. * Deleted `MessageTestUtil`.  Finally, a couple of changes to simplify coding patterns: * Added `flip()` and `buffer()` to `ByteBufferAccessor`. * Added `MessageSizeAccumulator.sizeExcludingZeroCopy`. * Used lambdas instead of `TestCondition`. * Used `Arrays.copyOf` instead of `System.arraycopy` in `MessageUtil`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-12-08T20:30:17Z","2020-12-09T19:16:03Z"
"","9805","MINOR: Remove outdated comment in Connect's WorkerCoordinator","Connect supports multiple assignment strategies now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-12-31T17:30:51Z","2021-09-28T16:57:49Z"
"","10293","KAFKA-12449: Remove deprecated WindowStore#put","Completes KIP-474 https://cwiki.apache.org/confluence/x/kcviBg by removing deprecated WindowStore#put(k, v).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-03-10T17:28:02Z","2021-04-14T12:20:29Z"
"","10296","KAFKA-12451: Remove deprecation annotation on long-based read operations in WindowStore","Complete https://cwiki.apache.org/confluence/display/KAFKA/KIP-667%3A+Remove+deprecated+methods+from+ReadOnlyWindowStore by removing deprecation annotation on long-based read operations in WindowStore.  Depends on #10294   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-03-10T19:24:51Z","2021-05-07T09:07:33Z"
"","9632","KAFKA-10702; Skip bookkeeping of empty transactions","Compacted topics can accumulate a large number of empty transaction markers as the data from the transactions gets cleaned. For each transaction, there is some bookkeeping that leaders and followers must do to keep the transaction index up to date. The cost of this overhead can degrade performance when a replica needs to catch up if the log has mostly empty or small transactions. This patch improves the cost by skipping over empty transactions since these will have no effect on the last stable offset and do not need to be reflected in the transaction index.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-11-20T21:11:05Z","2020-11-30T22:48:29Z"
"","10180","KAFKA- 12347: expose offsets to streams client (WIP)","collect the offsets after they are committed  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-02-22T22:38:45Z","2021-03-09T21:19:24Z"
"","10407","KAFKA-12523: handle TaskCorruption/TimeoutException during handleCorruption  and handleRevocation","Clean up handling of TaskCorruptedException in  `handleRevocation`: if we try to commit and get a TaskCorrupted, we should just immediately clean up the affected tasks instead of bubbling the TaskCorruptedException up through poll and trying to deal with any corrupted tasks which have since been revoked  `handleCorrupted`: if we get a TaskCorrupted when trying to commit the clean tasks before closing and reviving the corrupted ones, we should just include these tasks in the subsequent `closeAndRevive`  Left some things as followup work to keep the changes minimal and low-risk for the 2.8 release. If it looks good I'll file tickets for any TODOs and add the ticket # in the TODO before merging  Should be cherrypicked to 2.8 @vvcephei","closed","","ableegoldman","2021-03-26T02:10:44Z","2021-03-30T17:08:17Z"
"","9670","MINOR: Clarify config names for EOS versions 1 and 2","Clarify that EOS ""Alpha"" is version 1 and EOS ""Beta"" is version 2. These names don't denote the readiness of the feature; both are good for production.","closed","docs,","JimGalasyn","2020-12-01T21:11:21Z","2021-02-17T03:03:37Z"
"","9782","KAFKA-10815 EosTestDriver#verifyAllTransactionFinished should break l…","cherry-pick from #9706 to branch 2.7  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-23T02:07:13Z","2020-12-23T16:12:49Z"
"","9783","KAFKA-10815 EosTestDriver#verifyAllTransactionFinished should break l…","cherry-pick from #9706 to branch 2.6  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-23T02:09:58Z","2020-12-23T16:13:07Z"
"","9815","MINOR: Fixed two properties in EOS example plus some thread cleanup","Changes: * Passing the latches into the consumer/producer wrappers seemed a bit unclean. I removed the latches and used futures to wait on producer/consumer execution. * Consumer that is part of the transaction should not auto-commit * Transactional producers don't need to set idempotency explicitly.","open","","gwenshap","2021-01-02T18:35:50Z","2021-01-11T19:33:51Z"
"","9772","MINOR: Updating files with latest release 2.7.0","Changes to trunk for the 2.7.0 release.  Updating dependencies.gradle, Dockerfile, and vagrant/bash.sh  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-12-21T15:37:50Z","2020-12-21T16:53:15Z"
"","9543","KAFKA-10500: Makes the Stream thread list resizable","Changes the StreamThreads to be held in a ArrayList so that we can dynamically change the number of threads easily.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2020-11-02T23:39:54Z","2020-12-11T17:08:05Z"
"","9727","[KAFKA-10417] Update Cogrouped processor to work with suppress() and joins","Changes the cogrouped processor from `PassThrough` to `KTablePassThrough` to allow for sending old values. `KTablePassThrough` extends `KTableProcessorSupplier` instead of `ProcessorSupplier` to implement sending old values and the `view()` method.","closed","","lct45","2020-12-10T16:20:38Z","2020-12-15T20:53:57Z"
"","10286","KAFKA-12318: system tests need to fetch Topic IDs via Admin Client instead of via ZooKeeper","Changes system tests to support both ZK and raft topic IDs. Clarifies the IBP check is for zk code.  Ran `upgrade_test.py` and `downgrade_test.py` as these both use the `all_nodes_support_topic_ids` and/or `topic_id` methods. Currently does not include tests using raft, but verified that the output for describe topics in raft code returns same  output format and contains a valid topic ID.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-03-09T18:44:18Z","2021-03-19T18:41:50Z"
"","10143","KAFKA-12332: Error partitions from topics with invalid IDs in LISR requests","Changes how invalid IDs are handled in LeaderAndIsr requests. The ID check now occurs before leader epoch. If the ID exists and is invalid, the partition is ignored and a new `INCONSISTENT_TOPIC_ID` error is returned in the response.  This error should be rare, but if it occurs, it signals the need for manual intervention.  Added tests for this behavior. I also plan to rerun the benchmark from https://github.com/apache/kafka/pull/10071 to ensure there are no regressions with this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-17T20:55:09Z","2021-02-19T19:08:00Z"
"","10022","KAFKA-12268; Return from KafkaConsumer.poll only when records available or timeout","Changes from https://issues.apache.org/jira/browse/KAFKA-10866 cause early return from KafkaConsumer.poll() even when records are not available. We should respect timeout specified in poll() and return only when records are available.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-02-02T13:53:48Z","2021-02-03T19:08:34Z"
"","9750","MINOR: Change toArray usage for Increase efficiency","Change the toArray usage from toArray(new 'SomeType'[Sometype.size()]) to toArray(new 'Sometype'[0])  @chia7712","closed","","APaMio","2020-12-14T22:41:33Z","2020-12-21T09:38:50Z"
"","9705","MINOR: Using primitive data types for loop index","Change the loop index from Object into primitive data type  @chia7712","closed","","APaMio","2020-12-06T17:42:27Z","2020-12-09T02:44:56Z"
"","9704","Minor:Using primitive data types for loop index","Change the loop index from Object into primitive data type  @chia7712","closed","","APaMio","2020-12-06T10:32:51Z","2020-12-06T14:59:38Z"
"","9583","[KAFKA-10705]: Make state stores not readable by others","Change permissions on the folders for the state store so they're no readable or writable by ""others"", but still accessible by owner and group members.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","lct45","2020-11-10T16:26:57Z","2021-01-09T01:08:19Z"
"","9721","MINOR: remove dangling quickstart-*.html","Cf https://github.com/apache/kafka-site/pull/313 and https://github.com/apache/kafka/pull/9722  Call for review @guozhangwang (\cc @mimaison @bbejeck)  Must be cherry-picked to `2.7` and `2.6` branches.","closed","","mjsax","2020-12-10T00:44:55Z","2020-12-23T00:38:23Z"
"","9950","KAFKA-12170: Fix for Connect Cast SMT to correctly transform a Byte array into a string","Cast SMT transformation for bytes -> string. Without this fix, the conversion becomes  ByteBuffer.toString(), which always gives one of these useless results:     ""java.nio.HeapByteBuffer[pos=0 lim=4 cap=4]""      ""[B@5ec0a365"" (for byte array)       With this change, the byte array is converted into a hex string of the byte buffer content, for example      ""FEDCBA9876543210""  Completed with test case and successfully tried out in a real database conversion.   ### Committer Checklist (excluded from commit message) - [X ] Verify design and implementation  - [ X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","sknop","2021-01-22T13:02:16Z","2021-03-03T22:17:49Z"
"","9615","KAFKA-10500: Add thread option","Can add stream threads now  replace https://github.com/apache/kafka/pull/9581 after rebase  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2020-11-18T18:54:55Z","2021-01-08T02:02:30Z"
"","9581","KAFKA-10500: Add thread","Can add stream threads now  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2020-11-09T23:37:03Z","2020-12-11T17:10:03Z"
"","9498","MINOR: call super.close() when closing RocksDB options","Call super.close when closing rocksdb options. If we don't, then closing a state store will leak the underlying rocksdb options object. This is actually somewhat costly, since the default options object allocates it's own cache instance.","closed","streams,","rodesai","2020-10-25T23:55:33Z","2020-10-28T18:29:13Z"
"","9634","KAFKA-10755: Should consider commit latency when computing next commit timestamp","Call for review @wcarlson5 @guozhangwang","closed","streams,","mjsax","2020-11-20T22:59:17Z","2020-11-24T19:12:56Z"
"","9940","KAFKA-12185: fix ConcurrentModificationException in newly added Tasks container class","Call for review @vvcephei @ableegoldman","closed","streams,","mjsax","2021-01-20T19:50:54Z","2021-01-21T16:52:58Z"
"","9837","KAFKA-9566: Improve DeserializationExceptionHandler JavaDocs","Call for review @vvcephei","closed","docs,","mjsax","2021-01-06T20:37:16Z","2021-01-07T23:59:37Z"
"","10048","MINOR: add docs for KIP-680","Call for review @bbejeck.  Follow up PR to #9660 for 2.8.0 release.","closed","kip,","mjsax","2021-02-04T01:42:21Z","2021-02-05T04:18:24Z"
"","10102","KAFKA-12272: Fix commit-interval metrics","Call for review @ableegoldman  Fixes a regression bug. Should be cherry-picked back to 2.6 branch.","closed","streams,","mjsax","2021-02-10T20:42:05Z","2021-02-12T01:00:32Z"
"","10391","MINOR: disable flaky system test","Call for review @abbccdda @guozhangwang @vvcephei","closed","tests,","mjsax","2021-03-23T23:17:31Z","2022-04-05T23:56:08Z"
"","9688","KAFKA-10017: fix flaky EOS-beta upgrade test","Call for review @abbccdda @ableegoldman @guozhangwang   This PR is for `trunk` and `2.7`. PR for `2.6` is slightly different: #9690","closed","tests,","mjsax","2020-12-04T09:10:05Z","2020-12-11T01:49:29Z"
"","9618","MINOR: change default TX timeout only if EOS is enabled","Call for review @abbccdda","closed","streams,","mjsax","2020-11-18T21:43:41Z","2020-11-22T22:00:40Z"
"","9589","KAFKA-10710 - Mirror Maker 2 - Create herders only if source->target.enabled=true","By default Mirror Maker 2 creates herders for all the possible combinations even if the ""links"" are not enabled.  This is because the beats are emitted from the ""opposite"" herder. If there is a replication flow from A to B and heartbeats are required, 2 herders are needed :  - A->B for the MirrorSourceConnector - B->A for the MirrorHeartbeatConnector  The MirrorHeartbeatConnector on B->A emits beats into topic heartbeats on cluster A. The MirrorSourceConnector on A->B then replicates whichever topic is configured as well as heartbeats.  In cases with multiple clusters (10 and more), this leads to an incredible amount of connections, file descriptors and configuration topics created in every target clusters that are not necessary.  With this code change, we add a new top level property ""emit.heartbeats.enabled"" which defaults to ""true"". We skip creating the A->B herder whenever A->B.emit.heartbeats.enabled=false (defaults to true) and A->B.enabled=false (defaults to false).   Existing users will not see any change and if they depend on these ""opposites"" herders for their monitoring, it will still work. New users with more complex use case can change this property and fine tune their heartbeat generation.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","twobeeb","2020-11-12T15:33:35Z","2021-01-29T02:35:07Z"
"","10354","MINOR: Exclude KIP-500.md from rat check","Builds are failing since this file does not have a license. Similar .md files do not seem to have licenses, so I've added this file to the exclude list.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-03-18T18:45:20Z","2021-03-18T20:32:29Z"
"","9993","Schema Exception Interface fix","Bug fix. I'm passing in a SchemaBuilder for schema, and this will always throw an exception since it's doing an equals comparison. The reason that I need to pass in a builder is because my schema contains possibilities for a graph, and the only way to represent this is with SchemaBuilder since ConnectSchema is not mutable.  Please let me know if this is a valid approach or if the original code is intended to function that way.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wangwalton","2021-01-28T21:33:06Z","2021-01-28T21:34:23Z"
"","10003","MINOR: Add HostedPartition.Deferred state in ReplicaManager","Brokers receive metadata from the Raft metadata quorum very differently than they do from ZooKeeper today, and this has implications for ReplicaManager.  In particular, when a broker reads the metadata log it may not arrive at the ultimate state for a partition until it reads multiple messages.  In normal operation the multiple messages associated with a state change will all appear in a single batch, so they can and will be coalesced and applied together.  There are circumstances where messages associated with partition state changes will appear across multiple batches and we will be forced to coalesce these multiple batches together.  The circumstances when this occurs are as follows:  - When the broker restarts it must ""catch up"" on the metadata log, and it is likely that the broker will see multiple partition state changes for a single partition across different batches while it is catching up.  For example, it will see the `TopicRecord` and the `PartitionRecords` for the topic creation, and then it will see any `IsrChangeRecords` that may have been recorded since the creation.  The broker does not know the state of the topic partitions until it reads and coalesces all the messages. - The broker will have to ""catch up"" on the metadata log if it becomes fenced and then regains its lease and resumes communication with the metadata quorum. - A fenced broker may ultimately have to perform a ""soft restart"" if it was fenced for so long that the point at which it needs to resume fetching the metadata log has been subsumed into a metadata snapshot and is no longer independently fetchable.  A soft restart will entail some kind of metadata reset based on the latest available snapshot plus a catchup phase to fetch after the snapshot end point.  The first case -- during startup -- occurs before clients are able to connect to the broker.  Clients are able to connect to the broker in the second case.  It is unclear if clients will be able to to connect to the broker during a soft restart (the third case).  We need a way to defer the application of topic partition metadata in all of the above cases, and while we are deferring the application of the metadata the broker will not service clients for the affected partitions.  As a side note, it is arguable if the broker should be able to service clients while catching up or not.  The decision to not service clients has no impact in the startup case -- clients can't connect yet at that point anyway.  In the third case it is not yet clear what we are going to do, but being unable to service clients while performing a soft reset seems reasonable.  In the second case it is most likely true that we will catch up quickly; it would be unusual to reestablish communication with the metadata quorum such that we gain a new lease and begin to catch up only to lose our lease again.  So we need a way to defer the application of partition metadata and make those partitions unavailable while deferring state changes.  This PR adds a new internal partition state to ReplicaManager to accomplish this.  Currently the available partition states are simple `Online`, `Offline` (meaning a log dir failure) and `None` (meaning we don't know about it).  We add a new `Deferred` state.  We also rename a couple of methods that refer to ""nonOffline"" partitions to instead refer to ""online"" partitions.  **The new `Deferred` state never happens when using ZooKeeper for metadata storage.** Partitions can only enter the `Deferred` state when using a KIP-500 Raft metadata quorum and one of the above 3 cases occurs.  The testing strategy is therefore to leverage existing tests to confirm that there is no functionality change in the ZooKeeper case.  We will add the logic for deferring/applying/reacting to deferred partition state in separate PRs since that code will never be invoked in the ZooKeeper world.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-01-29T15:12:43Z","2021-02-04T19:27:30Z"
"","9650","MINOR: Update vagrant/tests readme","Before: ![image](https://user-images.githubusercontent.com/43372967/100190205-b5c95500-2f28-11eb-9566-d8561f02e7e9.png)  After: ![image](https://user-images.githubusercontent.com/43372967/100190226-c1b51700-2f28-11eb-8359-04c6ae4beecc.png)  Before: ![image](https://user-images.githubusercontent.com/43372967/100190627-a72f6d80-2f29-11eb-99dd-f3e79738eb9f.png)  After: ![image](https://user-images.githubusercontent.com/43372967/100190654-b4e4f300-2f29-11eb-8d9c-442a7546082d.png)   And some other small fixes  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-11-25T06:16:17Z","2020-11-28T05:06:49Z"
"","9862","MINOR: Fix error message in SnapshotWriter.java","before ```             String message = String.format(                 ""Append not supported. Snapshot is already frozen: id = {}."",                 snapshot.snapshotId()             ); ```  after ```             String message = String.format(                 ""Append not supported. Snapshot is already frozen: id = '%s'."",                 snapshot.snapshotId()             ); ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-01-11T14:35:10Z","2021-01-13T13:49:12Z"
"","9920","MINOR: Fix error message in KafkaRaftClient.java","before  `logger.trace(                 ""Leader doesn't know about snapshot id {}, returned error {} and snapshot id {}"",                 state.fetchingSnapshot(),                 partitionSnapshot.errorCode(),                 partitionSnapshot.snapshotId()             );`  after  `logger.trace(                 ""Leader doesn't know about snapshot {}, returned error {} and snapshot id {}"",                 state.fetchingSnapshot(),                 partitionSnapshot.errorCode(),                 partitionSnapshot.snapshotId()             );`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-01-17T08:06:35Z","2021-01-20T02:00:15Z"
"","9763","MINOR: Use ApiUtils' methods static imported consistently.","Because there have been using the static import methods of ApiUtils, it is not necessary to import the whole ApiUtils which also keeps the consistency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongxuwang","2020-12-17T06:11:56Z","2020-12-23T16:22:31Z"
"","10215","KAFKA-12375: don't reuse thread.id until a thread has fully shut down","Basically any id that isn't actively being used by a non-DEAD thread in the `threads` list is fair game. This is relevant in two scenarios:  `REPLACE_THREAD`: when choosing to replace a thread after a recoverable error, we should just grab a new (and free) thread.id rather than reusing the id of the dying thread, to avoid a race condition between the old thread shutting down and the new thread starting up.   `#removeStreamThread()`: in this case, if we haven't explicitly waited for the thread to complete the shutdown, then we should not remove it from the `threads` list since this will allow reusing that thread.id. This can happen if a thread is removing itself, or if we timed out waiting for it to reach the `DEAD` state.  In all of these scenarios, the `threads` list is considered the source of truth about available thread.ids. When trying to start up a new thread and computing the next available id, we'll trim any `DEAD` threads from this list  Should be cherrypicked to the 2.8 branch cc @vvcephei","closed","","ableegoldman","2021-02-26T03:33:05Z","2021-03-03T00:33:48Z"
"","9880","KAFKA-10792 (2.5 backport): Prevent source task shutdown from blocking herder thread (#9669)","Backports the changes from #9669.  Changes the `WorkerSourceTask` class to only call `SourceTask::stop` from the task thread when the task is actually stopped (via `Source:task::close` just before `WorkerTask::run` completes), and only if an attempt has been made to start the task (which will not be the case if it was created in the paused state and then shut down before being started). This prevents `SourceTask::stop` from being indirectly invoked on the herder's thread, which can have adverse effects if the task is unable to shut down promptly.  Unit tests are tweaked where necessary to account for this new logic, which covers some edge cases mentioned in PR #5020 that were unaddressed up until now.  The existing integration tests for blocking connectors added later in 2.6 are backported here to provide cases for blocking source and sink tasks. Full coverage of every source/sink task method is intentionally omitted from these expanded tests in order to avoid inflating test runtime (each one adds an extra 5 seconds at minimum) and because the tests that are added here were sufficient to reproduce the bug with source task shutdown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-01-13T17:29:36Z","2021-01-14T20:11:36Z"
"","10103","KAFKA-12272: Fix commit-interval metrics","Backporting #10102   \cc @ableegoldman","closed","streams,","mjsax","2021-02-10T21:33:01Z","2021-02-11T00:44:51Z"
"","10104","KAFKA-12272: Fix commit-interval metrics","Backporting #10102  \cc @ableegoldman","closed","streams,","mjsax","2021-02-10T21:33:34Z","2021-02-11T00:44:48Z"
"","10020","KAFKA-10689: fix windowed FKJ topology and put checks in assignor to avoid infinite loops","Backport of #9568 to `2.6`  Fix infinite loop in assignor when trying to resolve the number of partitions in a topology with a windowed FKJ. Also adds a check to this loop to break out and fail the application if we detect that we are/will be stuck in an infinite loop.","closed","streams,","mjsax","2021-02-01T23:49:17Z","2021-02-04T00:11:27Z"
"","9578","MINOR: Log resource pattern of ACL updates at INFO level","At the moment, we have one log entry for ACL updates that says: ``` Processing notification(s) to /kafka-acl-changes ``` For other updates like broker configuration updates, we have an additional entry at INFO level that shows what was updated when the change notification was processed. Since every resource pattern may have 100s or 1000s of access control entries associated with it, we don't want to log the entire contents on ACL update at INFO level. But it would be useful to log the resource pattern. This shows that we processed the notification in AclAuthorizer and gives the resource and version that was refreshed from ZK. This PR adds an additional INFO-level log entry for ACL updates in AclAuthorizer and retains the existing DEBUG level entry with full details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-11-09T13:36:08Z","2020-11-10T10:50:26Z"
"","10082","MINOR: use 'mapKey' to avoid unnecessary grouped data","as title. Thanks to great KRPC :)  # changes in this PR  1. add 'mapKey=true' to `DescribeLogDirsRequest` 1. rename `PartitionIndex` to `Partitions` for `DescribeLogDirsRequest` 1. add 'mapKey=true' to `ElectLeadersRequest` 1. rename `PartitionId` to `Partitions` for `ElectLeadersRequest` 1. add 'mapKey=true' to `ConsumerProtocolAssignment`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-08T14:12:03Z","2021-02-17T04:14:09Z"
"","9641","MINOR: Convert connect assignment schemas to use generated protocol","as title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","chia7712","2020-11-23T08:41:50Z","2021-02-02T08:45:57Z"
"","9723","Cherry-pick stream producer fix 10813","As title suggested, this is merging the blocker https://github.com/apache/kafka/pull/9700  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-12-10T04:20:40Z","2020-12-10T17:13:10Z"
"","9768","HOTFIX: fix failed ControllerChannelManagerTest#testUpdateMetadataReq…","as title  related to #9626  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-19T07:02:20Z","2020-12-21T04:29:11Z"
"","10274","MINOR: add missing space to errro message when setting uint16","as title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-03-06T12:07:46Z","2021-03-08T06:21:56Z"
"","10193","MINOR: correct the error message of validating uint32","as title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-23T15:41:45Z","2021-03-03T03:34:59Z"
"","9964","MINOR: remove duplicate code of serializing auto-generated data","as title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-25T11:01:42Z","2021-01-28T09:14:29Z"
"","9949","MINOR: replace FlattenedIterator by java stream","as title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-22T09:31:48Z","2021-01-23T17:46:41Z"
"","9959","MINOR: Remove redundant casting and if condition from ConnectSchema","as title","closed","connect,","g1geordie","2021-01-25T03:38:11Z","2021-01-27T06:52:33Z"
"","10327","MINOR: No fetcher for partitions don't need to print remove fetcher log","As the title.In order to properly reflect the cluster state, partitions that are not in the fetcher should not print the remove fecher log.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-03-16T03:20:54Z","2021-04-19T16:18:50Z"
"","10329","MINOR: remove some specifying types in tool command","As the title.I checked some of the command-line tools and removed some of the specified types.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-03-16T05:40:44Z","2021-03-17T10:56:44Z"
"","10370","MINOR: Remove duplicate definition about 'the' from kafka project","As the title.Here is a duplicate definition about 'the'?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-03-21T15:08:47Z","2021-03-23T02:44:56Z"
"","10383","MINOR: Query topic describe and sort output by topic name when using adminClient","As the title. When using zk client, the query topicDescribe are output in order of topic names. Similarly, when using adminClient, they are also output in order of topic names.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-03-23T15:31:24Z","2021-06-11T16:27:42Z"
"","9833","MINOR: rename support to supports","As the title.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-01-06T17:09:05Z","2021-01-06T17:16:08Z"
"","9831","MINOR: rename iff to if","As the title  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-01-06T05:09:12Z","2021-01-06T07:22:44Z"
"","9886","MINOR: Rename examples to example","As the title","closed","","wenbingshen","2021-01-14T06:28:09Z","2021-01-14T09:52:57Z"
"","9808","MINOR: rename @returns to @return","As the title","closed","","dengziming","2021-01-01T10:48:18Z","2021-01-03T13:04:29Z"
"","10017","KAFKA-12260: Avoid hitting NPE for partitionsFor","As mentioned in the ticket, returning `null` for `consumer#partitionsFor` is not a good client side agreement. Addressing this problem by returning the empty list instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-02-01T21:13:51Z","2021-05-26T06:59:30Z"
"","10174","KAFKA-12357: Do not inline methods from the scala package by default","As mentioned in #9548, users currently use the kafka jar (`core` module) for integration testing and the current inlining behavior causes problems when the user's classpath contains a different Scala version than the one that was used for compilation (e.g. 2.13.4 versus 2.13.3).  An example error:  `java.lang.NoClassDefFoundError: scala/math/Ordering$$anon$7`  We now disable inlining of the `scala` package by default, but make it easy to enable it for those who so desire (a good option if you can ensure the scala library version matches the one used for compilation). While at it, we make it possible to disable scala compiler optimizations (`none`) or to use only method local optimizations (`method`). This can be useful if optimizing for compilation time during development.  Verified behavior by running gradlew with `--debug` and checking the output after `[zinc] The Scala compiler is invoked with:`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-22T14:33:04Z","2021-02-23T05:13:08Z"
"","10121","MINOR: Revert AdminClient changes for DeleteTopics","As discussed in the mailing list thread, this PR removes the AdminClient changes pertaining to `deleteTopicsWithIds` and `DeleteTopicsWithIdsResult`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-12T18:02:49Z","2021-02-12T20:26:02Z"
"","10023","KAFKA-12263: Change MockClient.RequestMatcher to RequestAssertion","As described in [KAFKA-12263](https://issues.apache.org/jira/browse/KAFKA-12263), this PR refactors `MockClient.RequestMatcher` to become `MockClient.RequestAssertion` using assertions in the implementations.","open","","tombentley","2021-02-02T17:31:21Z","2021-04-28T11:25:14Z"
"","9842","KAFKA-12156: Document single threaded response handling in Admin client","As described in [KAFKA-12156](https://issues.apache.org/jira/browse/KAFKA-12156), if users block the response handling thread in one call waiting for the result of a second ""nested"" call then the client effectively hangs because the 2nd call's response will never be processed.","closed","","tombentley","2021-01-07T10:05:37Z","2021-01-11T05:43:57Z"
"","9566","KAFKA-10618: Update to Uuid class","As decided in KIP-516, the UUID class has been named Uuid. This PR changes all instances of org.apache.kafka.common.UUID to org.apache.kafka.common.Uuid.  It also modifies the Uuid class so that it no longer wraps a java.util.UUID object. Now it simply stores two longs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2020-11-05T20:56:44Z","2020-11-18T08:58:21Z"
"","10351","MINOR: use new method to get number of topics in DeleteTopicsRequest","As a result of https://github.com/apache/kafka/pull/9684, a new field for topic names was created. For versions 6+ `DeleteTopicsRequestData.topicNames` will return an empty list in KafkaApis. This PR uses a new method to efficiently initialize the size of the collection.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-03-18T16:28:49Z","2021-03-23T07:29:17Z"
"","9601","KAFKA-10729: Bump remaining RPC's to use tagged fields.","As a follow-up from [KIP-482](https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields), this PR bumps the version for several RPC's to enable tagged fields via the flexible versioning mechanism.  Additionally, a new IBP version `KAFKA_2_8_IV0` is introduced to allow replication to take advantage of these new RPC versions for OffsetForLeaderEpoch and ListOffset.","closed","","gardnervickers","2020-11-16T19:52:21Z","2020-12-01T23:55:08Z"
"","10394","MINOR: Always apply the java-library gradle plugin","As @chia7712 found, we currently apply the `java` plugin indirectly via `rat.gradle` if the `.git` folder exists (https://github.com/apache/kafka/blob/7071ded2a61448da21c149fa1d85b3999b0d2f73/gradle/rat.gradle#L101).  This led to inconsistent behavior if the `.git` directory was not present. With this change, the `java-library` plugin (which has slightly more features than the `java` plugin) is always applied.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-24T13:03:37Z","2021-03-25T03:19:31Z"
"","9679","MINOR: Make Histogram.clear more readable","Arrays.fill is more readable than that for loop  Can I replace it ?","closed","","g1geordie","2020-12-03T06:30:11Z","2020-12-04T03:54:45Z"
"","10124","MINOR: apply Utils.isBlank to code base","apply Utils.isBlank to code base  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-02-14T09:46:02Z","2021-02-20T03:44:30Z"
"","10253","KAFKA-12376: Apply atomic append to the log","Append to the log in one batch when handling:  1. Client quota changes 2. Configuration changes 3. Feature changes 4. Topic creation   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-03-03T17:38:10Z","2021-03-09T18:37:35Z"
"","10191","KAFKA-12341: Ensure consistent versions for javassist","And update to 3.27.0-GA.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-02-23T13:34:44Z","2021-02-23T18:48:07Z"
"","10154","MINOR: Added missing import to kafka.py","An import got removed from kafka.py and caused `downgrade_test` to fail. This PR adds the import back in. Should also be applied to trunk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-18T19:27:01Z","2021-02-19T04:09:29Z"
"","10320","MINOR: revert stream logging level back to ERROR","An accidental change of logging level for streams from https://github.com/apache/kafka/pull/9579, correcting it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-03-15T16:12:43Z","2021-03-15T23:00:58Z"
"","10231","MINOR: Remove stack trace of the lock exception in a debug log4j","Although the lock exception log is at the DEBUG level only, many people were confused with stack traces that something serious happened; plus, in the source code there is only one call path that can lead to the capture of LockException at task manager/stream thread, so even for debugging purposes there’s no extra information we can get from anyways.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-02-28T04:36:11Z","2021-03-01T19:32:06Z"
"","9571","KAFKA-10691: AlterIsr Respond with wrong Error Id","AlterIsr send by an unknown broker will respond with a STALE_BROKER_EPOCH, which should be UNKNOWN_MEMBER_ID.","closed","","dengziming","2020-11-06T06:20:21Z","2020-11-06T06:55:42Z"
"","9892","KAFKA-12201: Migrate connect:basic-auth-extensio module to JUnit 5","Also: * Remove unused powermock dependency * Remove ""examples"" from the JUnit 4 list since one module was already converted and the other has no tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-14T13:28:35Z","2021-01-14T23:43:51Z"
"","10340","MINOR: Start the broker-to-controller channel for request forwarding","Also use different log prefixes for the different channels. Log prefixes now look like:  ``` [BrokerToControllerChannelManager broker=1 name=alterIsr] ... [BrokerToControllerChannelManager broker=1 name=forwarding] ... [BrokerToControllerChannelManager broker=1 name=heartbeat] ... ```","closed","","mumrah","2021-03-17T20:34:40Z","2021-03-18T18:03:07Z"
"","9913","MINOR: Move a few more methods to AuthHelper","Also move some tests to `AuthHelperTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-15T21:39:28Z","2021-01-16T14:44:07Z"
"","9849","MINOR: Upgrade gradle to 6.8 and test retry plugin to 1.2.0","Also fix generation of `gradlew` when `APP_HOME` contains a directory with spaces in its name.  Release notes: * https://docs.gradle.org/6.8/release-notes.html * https://github.com/gradle/test-retry-gradle-plugin/releases/tag/v1.2.0  In addition to existing tests, verified that `./gradlewAll jar` succeeds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-08T19:09:15Z","2021-01-18T17:04:24Z"
"","10439","MINOR: Extend Predicate interface to allow combining and negation","After this commit Predicate interface will be aligned as much as possible with the java.util.function.Predicate.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","wituss","2021-03-30T09:21:28Z","2021-04-16T09:12:13Z"
"","10338","KAFKA-10251: wait for consumer rebalance completed before consuming records","After investigation, I found this test is unstable because we bouncing broker during the test to verify the transnational data is still as expected. However, the broker down and on, will fail the rebalance due to `error when storing group assignment during SyncGroup`. To fix it, we can make the rebalance happened earlier before broker bouncing, so that we can make sure when test starts, we can start the records consuming, instead of rebalancing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-03-17T05:33:05Z","2021-07-08T12:23:31Z"
"","9884","MINOR: remove unused flag 'hasIdempotentRecords'","ae3a6ed990f91708686d27c6023bac050c422248 removed the usage of ```hasIdempotentRecords``` so we don't need to find out it from produce request. This PR includes following changes.  1. remove ```hasIdempotentRecords``` 2. avoid creating unused Map.Entry 3. move testing-only code (```hasIdempotentRecords```) from ```RequestUtils``` to ```RequestTestUtils```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-14T03:58:20Z","2021-01-15T16:53:15Z"
"","9505","KAFKA-10393: messages for fetch snapshot and fetch","Adds the changes to Fetch and FetchSnapshot as describe in KIP-630.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-10-26T16:29:20Z","2020-12-08T02:43:17Z"
"","9816","KAFKA-10761: Kafka Raft update log start offset","Adds support for nonzero log start offsets.  Changes to `Log`: 1. Add a new ""reason"" for increasing the log start offset. This is used by `KafkaMetadataLog` when a snapshot is generated. 2. `LogAppendInfo` should return if it was rolled because of an records append. A log is rolled when a new segment is created. This is used by `KafkaMetadataLog` to in some cases delete the created segment based on the log start offset.  Changes to `KafkaMetadataLog`: 1. Update both append functions to delete old segments based on the log start offset whenever the log is rolled. 2. Update `lastFetchedEpoch` to return the epoch of the latest snapshot whenever the log is empty. 3. Add a function that empties the log whenever the latest snapshot is greater than the replicated log. This is used when first loading the `KafkaMetadataLog` and whenever the `KafkaRaftClient` downloads a snapshot from the leader.  Changes to `KafkaRaftClient`: 1. Improve `validateFetchOffsetAndEpoch` so that it can handle fetch offset and last fetched epoch that are smaller than the log start offset. This is in addition to the existing code that check for a diverging log. This is used by the raft client to determine if the Fetch response should include a diverging epoch or a snapshot id.  2. When a follower finishes fetching a snapshot from the leader fully truncate the local log. 3. When polling the current state the raft client should check if the state machine has generated a new snapshot and update the log start offset accordingly.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-01-03T01:28:13Z","2021-02-01T14:43:23Z"
"","10220","KAFKA-12383: Get RaftClusterTest.java and other KIP-500 junit tests working","Adds KafkaClusterTestKit, an integration test framework for starting up Kafka brokers and controllers in Raft mode. This was authored by @cmccabe on the kip-500 development branch. See [RaftClusterTest](https://github.com/apache/kafka/blob/58fcdd85885a5ea4d8e8af3fafbd6b65ae4ad7ad/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala) for example usage   Also included here is the support for Raft mode in the JUnit integration test extensions. See [ClusterTestExtensionsTest](https://github.com/apache/kafka/blob/58fcdd85885a5ea4d8e8af3fafbd6b65ae4ad7ad/core/src/test/java/kafka/test/ClusterTestExtensionsTest.java) for example usage","closed","kip-500,","mumrah","2021-02-26T16:11:54Z","2021-03-22T15:45:57Z"
"","9937","KAFKA-4759 Add support for IPv4 and IPv6 ranges in AclAuthorizer","Adds IPv4 and IPv6 subnet support to ACL authorizer.  Notation supported:        - CIDR blocks(192.168.1.0/24)       - Ranges (192.168.1.1-192.168.1.254)  For the test strategy, I defined ranges and set ACLs with IPs included in that range and other that it is not included inside the range.  BTW suggestions are welcome (I don't know Scala and I haven't been coding in a long time) :)  JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-4759   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","rgo","2021-01-20T16:30:54Z","2021-04-04T19:08:10Z"
"","9597","KAFKA-10720: Document prohibition on header mutation by SMTs","Adds a sentence to the Javadoc for `Transformation` about not mutating headers. See [KAFKA-10720](https://issues.apache.org/jira/browse/KAFKA-10720).","closed","","tombentley","2020-11-16T09:35:09Z","2020-11-26T22:39:42Z"
"","10175","MINOR: V2.8 system tests for Raft-based metadata quorums","Adds a new `sanity_checks/bounce_test.py` system test for a simple produce/bounce/produce series of events.  Augments this and other `sanity_checks` tests so they run all metadata quorum types: ZooKeeper, remote Raft, and co-located Raft.  Augments several `tests/client` and `tests/core` system tests to run for remote Raft-based metadata quorums in addition to ZooKeeper.  Co-located Raft controllers are not tested here as this configuration is presumed to work assuming `sanity_checks` as well as any unit/integration tests pass.  Augments a simple set of tests in `tests/connect`, `tests/streams`, and `tests/tools` to run for remote Raft-based metadata quorums in addition to ZooKeeper.  Adds missing `@cluster` annotations for dozens of system tests that were missing them -- this was causing these tests to grab the entire cluster of nodes and negatively impact the parallelism of the system test run.  This PR adds 40% more system tests, but thanks to the addition of the missing `@#cluster` annotations we are able to complete the new 140% test workload in 8% less time than before.  This is equivalent to https://github.com/apache/kafka/pull/10105/ targeted at `trunk` except for `get_offset_shell_test.py` which has diverged on `trunk` compared to `2.8` -- the PRs differ in this single file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-02-22T15:14:43Z","2021-02-22T22:07:25Z"
"","10105","MINOR: System tests for Raft-based metadata quorums","Adds a new `sanity_checks/bounce_test.py` system test for a simple produce/bounce/produce series of events.  Augments this and other `sanity_checks` tests so they run all metadata quorum types: ZooKeeper, remote Raft, and co-located Raft.  Augments several `tests/client` and `tests/core` system tests to run for remote Raft-based metadata quorums in addition to ZooKeeper.  Co-located Raft controllers are not tested here as this configuration is presumed to work assuming `sanity_checks` as well as any unit/integration tests pass.  Augments a simple set of tests in `tests/connect`, `tests/streams`, and `tests/tools` to run for remote Raft-based metadata quorums in addition to ZooKeeper.  Adds missing `@cluster` annotations for dozens of system tests that were missing them -- this was causing these tests to grab the entire cluster of nodes and negatively impact the parallelism of the system test run.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-10T22:58:04Z","2021-02-22T21:57:18Z"
"","9708","KAFKA-9126: KIP-689: StreamJoined changelog configuration","Adds `withLoggingEnabled` and `withLoggingDisabled` for `StreamJoined` to give `StreamJoined` the same flexibility as `Materialized`","closed","kip,","lct45","2020-12-07T18:03:11Z","2021-01-28T01:14:17Z"
"","10040","KAFKA-12259: Use raw config to infer connector type when returning connect status response","Addressed [KAFKA-12259](https://issues.apache.org/jira/browse/KAFKA-12259).  Problem: when calling the connect status endpoint, a 500 error is returned, e.g. ``` {   ""error_code"": 500,   ""message"": ""Could not read properties from file /tmp/somefile.properties"" } ``` when any of the connector's has an exception from the config provider. This is because the `AbstractHerder` is trying to use resolved config. However, only the `connector.class` is needed from the config to infer if this connector is of source or sink type. The endpoint should still return the status of the connector instead of a 500 error.  This change uses raw config from the config backing store to retrieve the connector class to avoid any variable resolution.  Unit tests have been updated to reflect this change.  when any of the connector's has an exception from the config provider.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","Cyril-Engels","2021-02-03T18:00:04Z","2021-02-03T23:52:26Z"
"","10014","KAFKA-12252 and KAFKA-12262: Fix session key rotation when leadership changes","Addressed [KAFKA-12252](https://issues.apache.org/jira/browse/KAFKA-12252) and [KAFKA-12262](https://issues.apache.org/jira/browse/KAFKA-12262).  - To address `KAFKA-12262`, all workers now track key expiration instead of just the leader. - To address `KAFKA-12252`, a conditional check is added in the herder tick thread to only cut rebalance polling short for key rotation if the worker is the leader.  Unit tests for the distributed herder are added for both bugs. The first ensures that if a worker becomes the leader after reading a session key from the config topic, it wakes up from polling for rebalance in time for key rotation. The second ensures that if a worker loses leadership, it stops taking key expiration into account when calculating the time to poll for rebalance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-02-01T15:38:00Z","2021-05-05T21:24:16Z"
"","10211","KAFKA-12347: updating TaskMetadata","adding fields to taskMetadata  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-02-25T20:54:51Z","2021-03-09T21:19:45Z"
"","9969","MINOR: updated upgrade and architecture for KIP-663, KIP-696, and KIP-671","adding docs for KIPs 663, 696 and 671  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2021-01-25T22:12:37Z","2021-01-29T17:06:44Z"
"","9951","KAFKA-9126: Add docs for stream joined logging configs","Adding docs for KIP-689 / KAFKA-9126 which added `withLoggingEnabled()` and `withLoggingDisabled()` to `StreamJoined`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lct45","2021-01-22T15:46:45Z","2021-01-26T19:38:23Z"
"","10461","KAFKA-12603: Add benchmarks for handleFetchRequest and FetchContext","Added benchmarks to help test https://github.com/apache/kafka/pull/9944. These benchmarks can be used to compare the handleFetchRequest path, FullFetchContext and IncrementalFetchContext. Results below and flame graphs attached: [FetchApiBench.zip](https://github.com/apache/kafka/files/6250489/FetchApiBench.zip) [full fetch context.zip](https://github.com/apache/kafka/files/6245147/full.fetch.context.zip) [incremental.zip](https://github.com/apache/kafka/files/6255229/incremental.zip)    KafkaApisFetchBenchmark: ``` KafkaApisFetchBenchmark.testHandleFetchRequest                       10            10  avgt   15    106580.752 ±   8069.373  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       10            20  avgt   15    202546.131 ±   7748.566  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       10           100  avgt   15   1287241.258 ±  57127.925  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       20            10  avgt   15    210566.535 ±   8284.169  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       20            20  avgt   15    485464.773 ±  22446.261  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       20           100  avgt   15   3311306.969 ± 294758.904  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       50            10  avgt   15    560291.055 ±  28779.937  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       50            20  avgt   15   1373390.613 ± 208724.875  ns/op KafkaApisFetchBenchmark.testHandleFetchRequest                       50           100  avgt   15  12345590.790 ± 443485.022  ns/op ```  FullFetchContextBenchmark: ``` Benchmark                                                                     (partitionCount)  (topicCount)  Mode  Cnt            Score           Error  Units FullFetchContextBenchmark.newFullContext                                                    10            10  avgt   15            9.400 ±         0.642  ns/op FullFetchContextBenchmark.newFullContext                                                    10            20  avgt   15            9.163 ±         0.177  ns/op FullFetchContextBenchmark.newFullContext                                                    10           100  avgt   15            9.014 ±         0.281  ns/op FullFetchContextBenchmark.newFullContext                                                    20            10  avgt   15            8.935 ±         0.334  ns/op FullFetchContextBenchmark.newFullContext                                                    20            20  avgt   15            8.769 ±         0.102  ns/op FullFetchContextBenchmark.newFullContext                                                    20           100  avgt   15            8.729 ±         0.291  ns/op FullFetchContextBenchmark.newFullContext                                                    50            10  avgt   15            9.032 ±         0.447  ns/op FullFetchContextBenchmark.newFullContext                                                    50            20  avgt   15            9.074 ±         0.422  ns/op FullFetchContextBenchmark.newFullContext                                                    50           100  avgt   15            9.122 ±         0.283  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       10            10  avgt   15          920.954 ±        12.131  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       10            20  avgt   15         1825.846 ±        26.501  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       10           100  avgt   15    271718890.632 ±  12225638.258  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       20            10  avgt   15         2282.090 ±        98.191  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       20            20  avgt   15         4424.841 ±       159.225  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       20           100  avgt   15   2496876236.183 ±  87972780.491  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       50            10  avgt   15         5646.134 ±       457.860  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       50            20  avgt   15        10576.038 ±       173.532  ns/op FullFetchContextBenchmark.updateAndGenerateResponseDataForFullContext                       50           100  avgt   15  52923741093.333 ± 302811682.429  ns/op ```  IncrementalFetchContextBenchmark ``` Benchmark                                                                                   (partitionCount)  (toForgetPercentage)  (topicCount)  Mode  Cnt        Score       Error  Units IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                     5            10  avgt   10       24.198 ±     0.862  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                     5            20  avgt   10       19.128 ±     0.137  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                     5            50  avgt   10       19.138 ±     0.115  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    10            10  avgt   10       23.959 ±     0.565  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    10            20  avgt   10       19.161 ±     0.266  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    10            50  avgt   10       19.357 ±     1.049  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    50            10  avgt   10       23.626 ±     0.207  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    50            20  avgt   10       19.045 ±     0.121  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       10                    50            50  avgt   10       18.986 ±     0.130  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                     5            10  avgt   10       18.991 ±     0.126  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                     5            20  avgt   10       18.969 ±     0.141  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                     5            50  avgt   10       19.446 ±     1.025  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    10            10  avgt   10       20.460 ±     2.441  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    10            20  avgt   10       19.000 ±     0.092  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    10            50  avgt   10       19.009 ±     0.161  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    50            10  avgt   10       19.555 ±     0.393  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    50            20  avgt   10       19.112 ±     0.093  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       20                    50            50  avgt   10       18.868 ±     0.159  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                     5            10  avgt   10       19.047 ±     0.130  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                     5            20  avgt   10       19.021 ±     0.150  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                     5            50  avgt   10       22.758 ±     0.440  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    10            10  avgt   10       19.026 ±     0.147  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    10            20  avgt   10       19.001 ±     0.120  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    10            50  avgt   10       23.185 ±     0.133  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    50            10  avgt   10       19.040 ±     0.141  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    50            20  avgt   10       19.059 ±     0.156  ns/op IncrementalFetchContextBenchmark.updateAndGenerateResponseDataForIncrementalContext                       50                    50            50  avgt   10       23.380 ±     0.686  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                     5            10  avgt   10      739.484 ±     9.827  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                     5            20  avgt   10      754.875 ±    11.633  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                     5            50  avgt   10   147673.425 ±   620.465  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    10            10  avgt   10      804.706 ±     5.787  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    10            20  avgt   10      677.133 ±    22.897  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    10            50  avgt   10   137332.596 ±  1397.904  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    50            10  avgt   10      946.258 ±     5.860  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    50            20  avgt   10     2742.098 ±    16.485  ns/op IncrementalFetchContextBenchmark.updateSession                                                            10                    50            50  avgt   10    45436.163 ±  1060.107  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                     5            10  avgt   10      682.163 ±    28.272  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                     5            20  avgt   10     3092.904 ±    23.698  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                     5            50  avgt   10    10471.081 ±   113.056  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    10            10  avgt   10      747.679 ±    10.476  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    10            20  avgt   10     3365.438 ±    50.438  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    10            50  avgt   10   117471.422 ±   885.095  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    50            10  avgt   10     1223.101 ±    10.043  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    50            20  avgt   10     3352.377 ±    45.685  ns/op IncrementalFetchContextBenchmark.updateSession                                                            20                    50            50  avgt   10    46741.668 ±  3626.344  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                     5            10  avgt   10     1367.241 ±    30.799  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                     5            20  avgt   10     3994.033 ±    65.925  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                     5            50  avgt   10  1236542.020 ± 10982.921  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    10            10  avgt   10     1480.727 ±    16.649  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    10            20  avgt   10     4440.147 ±    39.263  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    10            50  avgt   10  1714720.865 ± 14837.272  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    50            10  avgt   10     2847.598 ±    27.665  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    50            20  avgt   10     6220.523 ±    74.483  ns/op IncrementalFetchContextBenchmark.updateSession                                                            50                    50            50  avgt   10   146124.265 ±  1667.414  ns/op ```","open","","jolshan","2021-04-01T16:58:09Z","2022-03-12T00:04:24Z"
"","10125","MINOR: Add note about topic IDs to upgrade doc","Added a note about how topic IDs will be added in version 2.8 and the implications of downgrading + reupgrading.  Also fixed small typo and made all notable changes part of a single list.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-14T22:36:44Z","2021-02-17T17:50:01Z"
"","9773","MINOR: Kafka Streams updates for 2.7.0 release","Added `upgrade-system-tests-27` module, updated `settings.gradle` and `build.gradle` for the new module, added the latest release to the `streams_upgrade_test.py`  For testing, I ran `streams:testAll` locally and everything passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2020-12-21T18:41:10Z","2021-01-04T16:02:14Z"
"","9984","KAFKA-12247: add timeout and static group rebalance to remove thread","add timeout and static group rebalance to remove thread  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2021-01-27T20:12:28Z","2021-01-29T21:18:46Z"
"","9872","KAFKA-10759: ARM support for Kafka","Add the stage of arm ci in pipeline of jenkins.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xiao-penglei","2021-01-12T12:13:30Z","2021-03-03T20:58:20Z"
"","10114","MINOR: KIP-631 KafkaConfig fixes and improvements","Add the new KIP-631 configs to KafkaConfigTest.  Rename InitialBrokerRegistrationTimeoutMs to InitialBrokerRegistrationTimeoutMsProp for consistency with the other properties.  Add ControllerListenerNamesProp as specified in KIP-631.  Give nodeId and brokerId the same value in KafkaConfig.","closed","kip-500,","cmccabe","2021-02-12T00:00:25Z","2021-02-12T05:35:24Z"
"","9876","KAFKA-12183: Add the KIP-631 metadata record definitions","Add the metadata gradle module, which will contain the metadata record definitions, and other metadata-related broker-side code.  Add MetadataParser, MetadataParseException, etc.","closed","kip-500,","cmccabe","2021-01-12T22:45:30Z","2021-01-14T18:17:13Z"
"","9647","KAFKA-10500: Remove thread","Add the ability to remove running threads  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2020-11-24T00:36:16Z","2020-12-11T17:10:16Z"
"","9695","KAFKA-10500: Remove thread","Add the ability to remove running threads  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including uprade notes)","closed","","wcarlson5","2020-12-04T20:08:09Z","2021-01-11T20:42:35Z"
"","10212","MINOR: Add cluster-metadata-decoder to DumpLogSegments","Add the --cluster-metadata-decoder and --skip-batch-metadata options to the DumpLogSegments command-line tool, as described in KIP-631.  Co-authored-by: David Arthur","closed","kip-500,","cmccabe","2021-02-26T01:14:29Z","2021-02-26T19:28:11Z"
"","9584","[KAFKA-10708]: Add ""group-id"" Tag to Kafka Consumer Metrics","Add the ""group-id"" to the metrics created during instantiation of the `KafkaConsumer`  https://issues.apache.org/jira/browse/KAFKA-10708  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","brianwyka","2020-11-10T22:47:29Z","2022-01-10T16:08:53Z"
"","9879","Restoration time tracking","Add Stream restoration time tracking log  Reviewers: John Roesler   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-01-13T17:28:10Z","2021-01-13T20:35:50Z"
"","10043","MINOR: Add StorageTool as specified in KIP-631","Add StorageTool as specified in KIP-631.  It can format and describe storage directories.","closed","kip-500,","cmccabe","2021-02-03T20:07:08Z","2021-02-08T20:42:41Z"
"","9580","KAFKA-10350: add forwarding manager implementation with metrics","Add metric for forwarding request tracking. Note that the implementation is slightly diverged from the KIP, where we decide to get rid of the client.id tag since most admin clients would only have one inflight forwarding request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-11-09T20:52:08Z","2020-11-12T07:21:11Z"
"","10106","MINOR: add the MetaLogListener, LocalLogManager, and Controller interface.","Add MetaLogListener, LocalLogManager, and related classes.  These classes are used by the KIP-500 controller and broker to interface with the Raft log.  Also add the Controller interface.  The implementation will be added in a separate PR.","closed","kip-500,","cmccabe","2021-02-10T23:11:17Z","2021-02-11T16:43:00Z"
"","9830","MINOR: Add restoration time tracking","Add logging for restoration time in milliseconds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-01-06T05:04:01Z","2021-01-13T17:05:24Z"
"","10030","MINOR: Add KafkaEventQueue","Add KafkaEventQueue, which is used by the KIP-500 controller to manage its event queue. Compared to using an Executor, KafkaEventQueue has the following advantages:  * Events can be given ""deadlines.""  If an event lingers in the queue beyond the deadline, it will be completed with a timeout exception.  This is useful for implementing timeouts for controller RPCs.  * Events can be prepended to the queue as well as appended.  * Events can be given tags to make them easier to manage.  This is especially useful for rescheduling or cancelling events which were previously scheduled to execute in the future.","closed","kip-500,","cmccabe","2021-02-03T00:00:50Z","2021-02-04T22:46:58Z"
"","10156","KAFKA-10345: File watch store reloading","Add file-based store reloading mechanism, which does both file watch triggering and time based reloading in a separate thread.  In addition, we also rebuilt the hierarchy of the DynamicBrokerReconfigurationTest into one base class, one with forwarding and one without forwarding.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","abbccdda","2021-02-18T21:56:06Z","2021-06-01T10:08:42Z"
"","10029","MINOR: Add DuplicateBrokerRegistrationException","Add DuplicateBrokerRegistrationException, as specified in KIP-631.","closed","kip-500,","cmccabe","2021-02-02T23:28:40Z","2021-02-03T05:44:06Z"
"","9623","KAFKA-10692: Add delegation.token.secret.key, deprecate ...master.key","Add delegation.token.secret.key broker config and deprecate delegation.token.master.key as described in [KIP-681](https://cwiki.apache.org/confluence/display/KAFKA/KIP-681%3A+Rename+master+key+in+delegation+token+feature)","closed","","tombentley","2020-11-19T12:16:23Z","2020-11-19T15:26:26Z"
"","10047","MINOR: Add ClusterTool as specified in KIP-631","Add ClusterTool as specified in KIP-631.  It can report the current cluster ID, and also send the new RPC for removing broker registrations.","closed","kip-500,","cmccabe","2021-02-04T01:25:56Z","2021-02-08T20:09:35Z"
"","9922","MINOR: Use Gradle's JUnitPlatform for upgrade-system-tests modules","Add a runtime dependency on the jupiter engine to avoid failures during `./gradlew unitTest/integrationTest/test` for `upgrade-system-tests-*`.  Also remove `connect` and `examples` from the JUnit 5 blocklist since they contains no tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-18T02:54:50Z","2021-01-18T16:59:13Z"
"","9692","MINOR: add a description for calling resetToDatetime's function to resetByDuration","Add a request that it will call resetToDatetime of description to resetByDuration of function.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bertber","2020-12-04T15:51:20Z","2020-12-04T17:25:47Z"
"","9788","KAFKA-10671: improve the partition.assignment.strategy docs","Add `StickyAssignor` and `CooperativeStickyAssignor`, and also briefly introduce the `RangeAssignor` and `RoundRobinAssignor`.    **Before:** ![image](https://user-images.githubusercontent.com/43372967/103117054-b7ab4300-46a3-11eb-83b4-69b60f8fb18c.png)   **After:** ![image](https://user-images.githubusercontent.com/43372967/103117141-18d31680-46a4-11eb-9737-704d92fb1560.png)    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-25T03:27:07Z","2021-01-07T02:04:15Z"
"","9817","KAFKA-10899: Producer's BufferPool closing check","According to the idea of #7967, it made BufferPool support closing. Now, there are two places in the BufferPool#allocate() method to judge the 'closed' flag. One is when 'lock' is acquired, and the other is when 'condition' is awakened.  However, if the memory is allocated outside the 'lock' code block after 'freeup', Therefore, it is possible for another thread to modify the 'closed' flag, causing this part of memory to be allocated in vain.  So two modifications have been made, One is to add the 'volatile' modifier before 'closed', One is to determine whether to 'closed' before allocating memory.","open","","unlizhao","2021-01-03T06:53:59Z","2021-01-10T01:14:14Z"
"","10477","KAFKA-12615: Correct comments for the method Selector.clear","According to my understanding, the second clearCompletedSends which is highlighted as followed should be clearCompletedReceives  /**  Clears all the results from the previous poll. This is invoked by Selector at the start of a poll() when all the results from the previous poll are expected to have been handled.  SocketServer uses clearCompletedSends() and **clearCompletedSends**() to clear `completedSends` and `completedReceives` as soon as they are processed to avoid holding onto large request/response buffers from multiple connections longer than necessary. Clients rely on Selector invoking {@link #clear()} at the start of each poll() since memory usage  is less critical and clearing once-per-poll provides the flexibility to process these results in any order before the next poll. */","closed","","zhaohaidao","2021-04-05T13:48:09Z","2021-04-06T01:02:00Z"
"","10476","KAFKA-12615: Correct comments for the method Selector.clear","According to my understanding, the second clearCompletedSends which is highlighted as followed should be clearCompletedReceives  /**  Clears all the results from the previous poll. This is invoked by Selector at the start of a poll() when all the results from the previous poll are expected to have been handled.  SocketServer uses clearCompletedSends() and **clearCompletedSends**() to clear `completedSends` and `completedReceives` as soon as they are processed to avoid holding onto large request/response buffers from multiple connections longer than necessary. Clients rely on Selector invoking {@link #clear()} at the start of each poll() since memory usage  is less critical and clearing once-per-poll provides the flexibility to process these results in any order before the next poll. */","closed","","zhaohaidao","2021-04-05T13:16:22Z","2021-04-05T15:08:52Z"
"","10161","MINOR: update the old anchor #intro_topic into the new one","Accidentally found the anchor # intro_topic doesn't exist anymore, and cause the quick start page has a wrong link. Correct it as the new # intro_concepts_and_terms anchor  ![image](https://user-images.githubusercontent.com/43372967/108479894-3e118900-72d1-11eb-84a6-5603f98f6b6a.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-02-19T08:41:03Z","2021-02-19T11:36:12Z"
"","9635","KAFKA-10756; Add missing unit test for `UnattachedState`","Accidentally find that there is no unit test for UnattachedState, We should add missing unit tests, code is similar to ResignedStateTest and VotedStateTest.","closed","","dengziming","2020-11-21T03:40:09Z","2020-12-08T18:27:12Z"
"","10364","HOTFIX: fix build in EmitOnChangeIntegrationTest","Accidentally broke the build when cherrypicking https://github.com/apache/kafka/pull/10360","closed","","ableegoldman","2021-03-19T23:14:59Z","2021-03-20T02:09:22Z"
"","10384","Update ops.html","A ZooKeeper ensemble of 7 servers tolerates 4 servers down, because 3 is minimum. At least, that is what I understood.","closed","","swiedenfeld","2021-03-23T15:32:28Z","2021-03-23T18:41:31Z"
"","9923","KAFKA-12219: Potential race condition in InMemoryKeyValueStore","A simple but serious problem, what I found while working on #8117.  cc/ @ableegoldman @bbejeck @guozhangwang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2021-01-18T08:58:39Z","2021-01-21T00:09:02Z"
"","9506","KAFKA-10647; Only serialize owned partitions when consumer protocol version >= 1","A regression got introduced by https://github.com/apache/kafka/pull/8897. The owned partition field must be ignored for version < 1 otherwise the serialization fails with an unsupported version exception.  This must be merged in 2.7.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-10-26T20:42:04Z","2020-10-27T10:11:29Z"
"","10155","Fix Raft broker restart issue when offset partitions are deferred","A Raft-based broker is unable to restart if the broker defers partition metadata changes for a `__consumer_offsets` topic-partition.  The issue is that `GroupMetadataManager` is asked to `removeGroupsForPartition()` upon the broker becoming a follower, but in order for that code to function it requires that the manager's scheduler be started.  There are multiple possible solutions here since `removeGroupsForPartition()` is a no-op at this point in the broker startup cycle (nothing has been loaded, so there is nothing to unload).  We could just not invoke the callback.  But it seems more reasonable to not special-case this and instead start `ReplicaManager` and the coordinators just before applying the deferred partitions states.  We also mark deferred partitions for which we are a follower as being online a bit earlier to avoid `NotLeaderOrFollowerException` that was being thrown upon restart.  Fixing this issue exposed the above issue regarding the scheduler not being started.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-18T20:03:52Z","2021-02-19T19:07:10Z"
"","9671","KAFKA-10793: move handling of FindCoordinatorFuture to fix race condition","A race condition between the consumer and hb thread can lead to a failed but non-null `findCoordinatorFuture`, causing the AbstractCoordinator to wait endlessly on the request which it thinks is still in flight. We should move the handling of this future out of the listener callbacks and into the `ensureCoordinatorReady()` method where we can check the exception and clear the future all in one place.    See ticket for full analysis.  Also starts logging a warning if the consumer is unable to connect to the coordinator for longer than the max poll interval.","closed","","ableegoldman","2020-12-02T02:43:04Z","2022-01-21T06:12:39Z"
"","9555","KAFKA-10673: Cache inter broker listener name used in connection quotas","A profile from a moderately busy cluster shows that calls to `protectedListener` can make up more than 1% of the allocations on a broker.  `config.interBrokerListenerName` is a relatively expensive call that both makes a copy of the `KafkaConfig`'s backing map and performs string/regex parsing. Given that we call `protectedListener()` multiple times per call to `ConnectionQuotas.inc()`, we end up performing a lot of unnecessary allocations, particularly given that the inter broker listener is not reconfigurable. We can instead store the inter broker listener name and check against it rather than recompute from the config.  An additional, smaller optimization included is to check the size of listener counts instead of `config.listeners`, as `config.listeners()` does some additional string csv parsing to recompute the listeners each time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-11-04T04:45:55Z","2020-11-06T08:28:32Z"
"","10204","MINOR: Fix the generation extraction util","A previous PR #10149 changed the log context from AbstractCoordinator to ConsumerCoordinator.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","guozhangwang","2021-02-24T17:59:14Z","2021-02-26T00:20:28Z"
"","9953","MINOR: Update to Gradle 6.8.1","A number of regressions were fixed (see ""Fixed issues"" section):  https://docs.gradle.org/6.8.1/release-notes.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-23T17:19:11Z","2021-01-23T23:54:12Z"
"","9565","MINOR: Move upgraded docs from site to Kafka docs","A number of doc updates have been applied to the website. For the 2.6.1 release, resync Kafka's docs folder with the website.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-11-05T12:07:46Z","2020-11-06T14:38:32Z"
"","10160","MINOR: Introduce KafkaChannel.newRequestContext","A minor clean-up that I noticed while looking at the reauthentication PR.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","garimagupta3","2021-02-19T08:10:21Z","2021-02-19T08:10:21Z"
"","10397","KAFKA-12508: Disable KIP-557","A major issue has been raised that this implementation of emit-on-change is vulnerable to a number of data-loss bugs in the presence of recovery with dirty state under at-least-once semantics. This should be fixed in the future when we implement a way to avoid or clean up the dirty state under at-least-once, at which point it will be safe to re-introduce KIP-557 and complete it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-03-24T18:46:43Z","2021-03-25T19:42:29Z"
"","10333","KAFKA-12481: Add socket.nagle.disable config property","A large number of topic-partitions on one broker causes burst of host's packets/sec metric. This pull request allow disable TCP_NODELAY socket option via Kafka Config.  Big amount topic-partitions per broker raise enormous packets count. For example 30k topic-partitions under load per 4 broker spawn ~150k packets/sec. With disabled TCP_NODELAY this value reduced to ~3k packets/sec. More about how to reproduce problem and result after solving in JIRA: https://issues.apache.org/jira/browse/KAFKA-12481  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","iatsuk","2021-03-16T19:18:01Z","2021-05-08T02:27:34Z"
"","9663","MINOR: Small cleanups in `AlterIsr` handling logic","A few small cleanups in `Partition` handling of `AlterIsr`:  - Factor state update and log message into `sendAlterIsrRequest` - Ensure illegal state error gets raised if a retry fails to be enqueued - Always check the proposed state against the current state in `handleAlterIsrResponse` - Add `toString` implementations to `IsrState` case classes  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2020-11-30T23:09:22Z","2021-08-11T13:16:05Z"
"","9952","MINOR: A few small group coordinator cleanups","A few small cleanups in `GroupCoordinator` and related classes.  - Remove redundant `groupId` field from `MemberMetadata` - Remove redundant `isStaticMember` field from `MemberMetadata` - Fix broken log message in `GroupMetadata.loadGroup` and apply it to all loaded members - Remove ancient TODOs and no-op methods from `GroupCoordinator` - Move removal of static members into `GroupMetadata.remove`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-01-23T02:26:14Z","2021-01-23T21:07:32Z"
"","9977","KAFKA-12310: Update zookeeper to 3.5.9","A few important fixes: * ZOOKEEPER-3829: Zookeeper refuses request after node expansion * ZOOKEEPER-3842: Rolling scale up of zookeeper cluster does not work with reconfigEnabled=false * ZOOKEEPER-3830: After add a new node, zookeeper cluster won't commit any proposal if this new node is leader  Full release notes: https://zookeeper.apache.org/doc/r3.5.9/releasenotes.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-27T02:34:30Z","2021-02-08T01:33:24Z"
"","10013","MINOR: Remove ZK dependency for __transaction_state partition count","`TransactionStateManager` currently uses ZooKeeper to obtain any non-default value for the `__transaction_state` partition count.  ZooKeeper will not be available when the broker uses a Raft-based metadata quorum.  This PR removes the hard dependency on ZooKeeper by allowing a function returning an `Int` to be provided instead.  Providing a ZooKeeper client is still supported in `apply()` methods, but those methods transparently wrap the client with a function and pass that function instead.  Existing tests are sufficient to detect bugs and regressions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-02-01T15:03:37Z","2021-02-01T18:29:28Z"
"","10133","MINOR: Update raft README and add a more specific error message.","`test-raft-server-start.sh` requires the config to be specified with `--config`. I've included this in the README and added an error message for this specific case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-02-16T21:12:10Z","2021-02-16T23:12:29Z"
"","9946","KAFKA-12230: do not interrupt execution on failure to `setPosixFilePermissions` on Windows machines","`StateDirectory` constructor should catch not only `IOException`, but also `UnsupportedOperationException`  in order to work correctly on Windows machines (see [KAFKA-12230](https://issues.apache.org/jira/browse/KAFKA-12230))   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","inponomarev","2021-01-21T16:51:08Z","2021-01-22T06:33:57Z"
"","10414","MINOR: Self-managed -> KRaft (Kafka Raft)","`Self-managed` is also used in the context of Cloud vs on-prem and it can be confusing.  `KRaft` is a cute combination of `Kafka Raft` and it's pronounced like `craft` (as in `craftsmanship`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-26T12:14:51Z","2021-03-29T22:39:13Z"
"","10267","KAFKA-12427: Don't update connection idle time for muted connections","`Selector.poll()` will always call `pollSelectionKeys()` for channels with buffered data. `pollSelectionKeys()` will always update connection last idle time, even if the channel is muted and we don't actually read from the channel.  There is an existing unit test `idleExpiryWithBufferedReceives` that fails to catch this behavior because the `MockTime` object used in the test is updated in a large enough increment to expire a connection between calls to `poll()`. After updating the test to advance time in smaller increments, the test fails without the Selector change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2021-03-05T00:08:59Z","2021-03-16T19:18:29Z"
"","10427","KAFKA-12591; Remove deprecated `quota.producer.default` and `quota.consumer.default` configurations","`quota.producer.default` and `quota.consumer.default` were deprecated in AK 0.11.0.0. I propose to remove them in AK 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-29T09:23:31Z","2021-04-09T16:11:37Z"
"","10009","MINOR: Introduce ProducerIdGenerator trait","`ProducerIdManager` is an existing class that talks to ZooKeeper directly.  We won't have ZooKeeper when using a Raft-based metadata quorum, so we need an abstraction for the functionality of generating producer IDs.  This PR introduces `ProducerIdGenerator` for this purpose, and we pass an implementation when instantiating `TransactionCoordinator` rather than letting `TransactionCoordinator.apply()` itself always create a ZooKeeper-based instance.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-01T01:56:43Z","2021-02-04T19:28:06Z"
"","10005","MINOR: Add ConfigRepository, use in Partition and KafkaApis","`Partition` objects are able to retrieve topic configs when creating their log, and currently they do so with an implementation of `trait TopicConfigFetcher` that uses ZooKeeper.  ZooKeeper is not available when using a Raft-based metadata log, so we need to abstract the retrieval of configs so it can work either with or without ZooKeeper.  This PR introduces `trait ConfigRepository` with `ZkConfigRepository` and `CachedConfigRepository` implementations.  `Partition` objects now use a provided `ConfigRepository` to retrieve topic configs, and we eliminate `TopicConfigFetcher` as it is no longer needed.  `ReplicaManager` now contains an instance of `ConfigRepository` so it can provide it when creating `Partition` instances.  `KafkaApis` needs to be able to handle describe-config requests; it currently delegates that to `ZkAdminManager`, which of course queries ZooKeeper.  To make this work with or without ZooKeeper we move the logic from `ZkAdminManager` into a new `ConfigHelper` class that goes through a `ConfigRepository` instance.  We provide `KafkaApis` with such an instance, and it creates an instance of the helper so it can use that instead of going through `ZkAdminManager`.  Existing tests are sufficient to identify bugs and regressions in `Partition`, `ReplicaManager`, `KafkaApis`, and `ConfigHelper`.  The `ConfigRepository` implementations have their own unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-01-29T19:10:13Z","2021-02-04T20:58:27Z"
"","10341","KAFKA-12491: Make rocksdb an `api` dependency for `streams`","`org.rocksdb.Options` is part of Kafka Streams public api via `RocksDBConfigSetter`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-17T22:27:02Z","2021-03-18T14:00:26Z"
"","10464","MINOR: remove KTable.to from the docs","`KTable#to` was already removed and should not be mentioned in the docs any longer.  Call for review @bbejeck","closed","docs,","mjsax","2021-04-02T07:55:57Z","2021-04-02T16:38:18Z"
"","10045","MINOR: Allow KafkaApis to be configured for Raft controller quorums","`KafkaApis` is configured differently when it is used in a broker with a Raft-based controller quorum vs. a ZooKeeper quorum.  For example, when using Raft, `ForwardingManager` is required rather than optional, and there is no `AdminManager`, `KafkaController`, or `KafkaZkClient`.  This PR introduces `MetadataSupport` to abstract the two possibilities: `ZkSupport` and `RaftSupport`.  This provides a fluent way to decide what to do based on the type of support that `KafkaApis` has been configured with.  Certain types of requests are not supported when using raft (`AlterIsrRequest`, `UpdateMetadataRequest`, etc.), and `MetadataSupport` gives us an intuitive way to identify the constraints and requirements associated with the different configurations and react accordingly.  Existing tests are sufficient to detect bugs and regressions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-03T23:19:17Z","2021-02-05T20:57:45Z"
"","10141","KAFKA-12329; kafka-reassign-partitions command should give a better error message when a topic does not exist","`kafka-reassign-partitions` command gives a generic error message when one tries to reassign a topic which does not exist:  ``` $ ./bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --execute --reassignment-json-file reassignment.json Error: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. ```  When the reassignment contains multiple topics, it is hard to find out the correct one. This PR improves this to give the name of the topic in the error:  ``` $ ./bin/kafka-reassign-partitions.sh --bootstrap-server localhost:9092 --execute --reassignment-json-file reassignment.json Error: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: Topic test-test not found. ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-02-17T09:48:24Z","2021-03-08T14:38:07Z"
"","10437","MINOR: Remove deprecated `host.name`, `advertised.host.name`, `port` and `advertised.port` for 3.0","`host.name`, `port`, `advertised.host.name` and `advertised.port` were deprecated a long time ago in favour of using `listeners`.  I think that it is time to remove them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-30T08:45:36Z","2021-06-14T15:56:29Z"
"","10008","MINOR: Remove ZK dependency for coordinator topics' partition counts","`GroupMetadataManager` currently uses ZooKeeper to obtain any non-default value for the `__consumer_offsets` partition count. ZooKeeper will not be available when the broker uses a Raft-based metadata quorum. This PR removes the hard dependency on ZooKeeper by allowing a function returning an `Int` to be provided instead. Providing a ZooKeeper client is still supported in `apply()` methods, but those methods transparently wrap the client with a function and pass that function instead.  `TransactionStateManager` currently uses ZooKeeper to obtain any non-default value for the `__transaction_state` partition count.  ZooKeeper will not be available when the broker uses a Raft-based metadata quorum.  This PR removes the hard dependency on ZooKeeper by allowing a function returning an `Int` to be provided instead.  Providing a ZooKeeper client is still supported in `apply()` methods, but those methods transparently wrap the client with a function and pass that function instead.  Existing tests are sufficient to prevent bugs and regressions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-01-31T22:21:07Z","2021-02-05T16:51:01Z"
"","10436","KAFKA-12577; Remove deprecated `ConfigEntry` constructor for 3.0","`ConfigEntry`'s constructor was deprecated in 1.1.0. It seems that we can safely remove it now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-30T08:41:17Z","2021-09-06T15:06:36Z"
"","9520","MINOR: replace test ""expected"" parameter by assertThrows","`assertThrows` can make checks in testing more flexible and accurate so I think it is worth substituting `assertThrows` for ""expected"" parameter.  This PR includes following changes.  1. ```@Test(expected = Exception.class)``` is replaced by ```assertThrows``` 1. remove reference to ```org.scalatest.Assertions```  1. change the magic code from 1 to 2 for ```testAppendAtInvalidOffset``` to test ZSTD 1. rename ```testMaybeAddPartitionToTransactionXXXX``` to ```testNotReadyForSendXXX``` 1. increase maxBlockMs from 1s to 3s to avoid unexpected timeout from ```TransactionsTest#testTimeout```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-28T15:06:37Z","2021-01-10T12:36:46Z"
"","9854","MINOR: Remove assertDoesNotThrow at last line","`assertDoesNotThrow(.....)` in the last line   Above is unnecessary  noise in my commit . Please let me revert it .  Discuss from  https://github.com/apache/kafka/pull/9785/files#r554586805","closed","","g1geordie","2021-01-10T17:13:24Z","2021-01-13T16:04:33Z"
"","10440","KAFKA-12581: Remove deprecated `Admin.electPreferredLeaders`","`Admin.electLeaders` is the replacement since the deprecation in Apache Kafka 2.4.0. The methods were originally introduced in Apache Kafka 2.2.0, so they were only non deprecated for two releases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-30T09:29:17Z","2021-03-31T05:04:16Z"
"","9502","MINOR: mark StandbyTaskEOSIntegrationTest as an IntegrationTest","```StandbyTaskEOSIntegrationTest ```instantiates a ```EmbeddedKafkaCluster``` so it should be an ```IntegrationTest```.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-26T08:48:55Z","2021-02-02T08:45:54Z"
"","9755","MINOR: refactor SelectingIterator by scala iterator","```SelectingIterator``` can be replaced by scala iterator with filter. Also, this PR adds a unit test for it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-15T09:40:36Z","2020-12-21T08:11:52Z"
"","10036","MINOR: change modifier of ConsumerRecords(FetchedRecords) from public…","```FetchedRecords``` is internal class so it is unnecessary to open ```ConsumerRecords(FetchedRecords)```.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-03T07:27:19Z","2021-02-03T15:46:12Z"
"","10050","MINOR: Don't assume number of dns results (stabilize ClusterConnectio…","```ClusterConnectionStatesTest#testMultipleIPsWithUseAll``` assume the number of addresses is 3. That assumption is unstable since it can be changed in the future. For example, the addresses of ```kafka.apache.org``` is changed to 2 recently.  It seems to me the test case is used to make sure all addresses (from resolving) are used when building connections. Hence, we can rewrite it by for-loop to make sure all addresses are used across connections.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-02-04T03:39:23Z","2021-02-05T16:46:26Z"
"","10011","MINOR: remove org.apache.kafka.streams.errors.BrokerNotFoundException","```BrokerNotFoundException``` is useless after 234ec8a8af76bfb7874dd99714a65089d6048953 was merged.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","chia7712","2021-02-01T06:42:24Z","2021-02-03T12:02:55Z"
"","9516","MINOR: make Send and Receive work with TransferableChannel rather than Gat…","```BlockingChannel``` was removed by cc4dce94af8b19a796eeb7a9be78640739cb1a48 so it is time to refactor ```Send``` and ```NetworkReceive``` to remove ScatteringByteChannel and GatheringByteChannel.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-10-28T06:19:54Z","2021-01-10T07:55:26Z"
"","9612","MINOR: add sudo to travis.yml to run ducktape","``` Traceback (most recent call last): 3157  File ""/usr/local/bin/ducktape"", line 8, in  3158    sys.exit(main()) 3159  File ""/usr/local/lib/python3.7/dist-packages/ducktape/command_line/main.py"", line 118, in main 3160    os.makedirs(ConsoleDefaults.METADATA_DIR) 3161  File ""/usr/lib/python3.7/os.py"", line 211, in makedirs 3162    makedirs(head, exist_ok=exist_ok) 3163  File ""/usr/lib/python3.7/os.py"", line 221, in makedirs 3164    mkdir(name, mode) 3165PermissionError: [Errno 13] Permission denied: '.ducktape' 3166ducker-ak test failed ``` Let see what will happen :)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-11-18T16:51:08Z","2020-11-24T01:40:52Z"
"","9942","KAFKA-12229: reset to original class loader after connector stop","``` java.lang.NullPointerException at  org.apache.kafka.connect.mirror.MirrorSourceConnector.listTopics(MirrorSourceConnector.java:348) at  org.apache.kafka.connect.mirror.MirrorSourceConnector.findSourceTopicPartitions(MirrorSourceConnector.java:192) at  org.apache.kafka.connect.mirror.MirrorSourceConnectorTest.testRefreshTopicPartitionsTopicOnTargetFirst(MirrorSourceConnectorTest.java:222) ``` After days of investigation, I finally found the root cause of the test failure reason: **class loader**.  The issue is quite weird, we mocked the method, but still call the real method, and cause the NPE. Digging into the Mockito, found it's not about JUnit 5, it's because of the class loader. In Mockito, we relies on the class loader to generate the proxy instance ([source](https://github.com/mockito/mockito/blob/release/3.x/src/main/java/org/mockito/internal/creation/bytebuddy/SubclassBytecodeGenerator.java#L91)) to intercept the method call, and if the class loader is not expected, we'll generate the wrong proxy instance (with wrong class path). We set the class loader during connector start to resolve conflicting dependencies ([KIP-146](https://cwiki.apache.org/confluence/display/KAFKA/KIP-146+-+Classloading+Isolation+in+Connect)), so we should set it back to the original class loader after connector stop in tests (`EmbeddedConnectCluster` is only used in tests) for the following Mockito works as expected.  So, there's an interference of integration tests with unit tests when Connect integration tests run before the MM2 unit tests, and that will cause the Mockito used in unit tests not work as expected.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","showuon","2021-01-21T02:17:59Z","2021-01-26T09:42:33Z"
"","9610","MINOR: remove ""gradle wrapper"" from travis.yml","``` > Failed to apply plugin [class 'com.github.spotbugs.snom.SpotBugsBasePlugin']    > Gradle version Gradle 5.1.1 is unsupported. Please use Gradle 5.6 or later. ```  ```com.github.spotbugs.snom.SpotBugsBasePlugin``` requires gradle 5.6+ but the gradle supported by travis is 4.0 or 5.1.1 (https://docs.travis-ci.com/user/reference/trusty/#gradle-version and https://docs.travis-ci.com/user/reference/xenial/)  However, we don't need to call ```gradle wrapper``` since ```gradlew``` already exists in our project.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-11-18T07:20:01Z","2020-11-18T16:05:02Z"
"","10032","KAFKA-12270: Handle race condition when Connect tasks attempt to create topics","_**This is option #2 for the fix, which is a bit more ideal since no additional admin client calls are made. See #10027 for option 1 that is simpler but results in additional admin client calls.**_  When a source connector is configured to create missing topics has multiple tasks that generate records for the same topic, it is possible that multiple tasks may simultaneously describe the topic, find it does not exist, and attempt to create the task. One of those create topic requests will succeed, and the other concurrent tasks will receive the response from the topic admin as having not created the task and will fail unnecessarily.  This change corrects the logic by moving the `TopicAdmin` logic to create a topic to a new `createOrFindTopics(…)` method that returns the set of created topic names and the set of existing topic names. This allows the existing `createTopics(…)` and `createTopic(…)` methods to retain the same behavior, but also allows the `WorkerSourceTask` to know from its single call to this new method whether the topic was created or was found to exist.  This adds one unit test and modifies several unit tests in `WorkerSourceTaskWithTopicCreationTest` that use mocks to verify the behavior, and modifies several existing unit tests for `TopicAdminTest` to ensure the logic of the new method is as expected.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-02-03T02:02:43Z","2021-02-04T00:29:56Z"
"","10448","KAFKA-12583: Upgrade netty to 4.1.62.Final","[This security vulnerability](https://nvd.nist.gov/vuln/detail/CVE-2021-21295) was found in netty-codec-http2, but caused by netty itself and [fixed in 4.1.60.Final](https://github.com/netty/netty/security/advisories/GHSA-wm47-8v5p-wjpj). So, upgrade the netty version from 4.1.59.Final to 4.1.62.Final.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-31T08:05:00Z","2021-04-01T13:19:03Z"
"","10182","KAFKA-12160: Remove max.poll.interval.ms from config docs (and KSTREAMS-4881)","[KAFKA-12160](https://issues.apache.org/jira/browse/KAFKA-12160): Remove `max.poll.interval.ms` from the Kafka Streams docs.","closed","docs,","JimGalasyn","2021-02-23T00:11:52Z","2021-02-23T01:08:00Z"
"","10396","KAFKA-12474: Handle failure to write new session keys gracefully","[Jira](https://issues.apache.org/jira/browse/KAFKA-12474)  If a distributed worker fails to write (or read back) a new session key to/from the config topic, it dies.  This fix softens the blow a bit by instead restarting the herder tick loop anew and forcing a read to the end of the config topic until the worker is able to successfully read to the end.  At this point, if the worker was able to successfully write a new session key in its first attempt, it will have read that key back from the config topic and will not write a new key during the next tick iteration. If it was not able to write that key at all, it will try again to write a new key (if it is still the leader).  Verified with new unit tests for both cases (failure to write, failure to read back after write).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-03-24T18:24:13Z","2021-04-01T17:40:23Z"
"","10315","KAFKA-12463: Update default sink task partition assignor to cooperative sticky assignor","[Jira](https://issues.apache.org/jira/browse/KAFKA-12463)  Changes sink tasks to use the `CooperativeStickyAssignor` by default, in a fashion that permits rolling upgrades of Connect workers and retains existing support for worker-level and connector-level overrides for the consumer assignor property.  Update 3/17/21: Converted to DRAFT due to the discovery of [KAFKA-12487](https://issues.apache.org/jira/browse/KAFKA-12487); discussion is ongoing on the ticket for [KAFKA-12463](https://issues.apache.org/jira/browse/KAFKA-12463).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-03-14T18:15:47Z","2021-04-11T19:42:22Z"
"","10239","KAFKA-12372: Enhance TimestampCoverter Connect transformation to handle multiple timestamp or date fields","[JIRA](https://issues.apache.org/jira/browse/KAFKA-12372)  Our team is having an issue of handling multiple timestamp fields in a kafka message, so for now if we use the converter then we have to add more fields in the config.  We can implement it in a generic way to check if the field.schema().name() is timestamp or date then we can convert it.","open","connect,","vanhoale","2021-03-01T21:15:53Z","2021-06-25T12:47:27Z"
"","10178","KAFKA-12361: Use default request.timeout.ms value for Connect producers","[Jira](https://issues.apache.org/jira/browse/KAFKA-12361)  The super-high request timeout makes it harder for the producer to gracefully handle unclean connection terminations, which might happen in the case of sudden broker death.  Reducing that value to the default of 30 seconds should address that issue, without compromising the existing delivery guarantees of the Connect framework. Since the delivery timeout is still set to a very-high value, this change shouldn't make it more likely for `Producer::send` to throw an exception and fail the task.  This may make duplicate record delivery more likely in cases with extremely-slow broker response time, but that can be addressed by enabling idempotence in the underlying producers for the connector's tasks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-02-22T20:55:21Z","2022-02-27T03:53:19Z"
"","10074","KAFKA-12305: Fix Flatten SMT for array types","[Jira](https://issues.apache.org/jira/browse/KAFKA-12305)  (Copied from Jira):  The `Flatten` SMT fails for array types. A sophisticated approach that tries to flatten arrays might be desirable in some cases, and may have been punted during the early design phase of the transform, but in the interim, it's probably not worth it to make array data and the SMT mutually exclusive.  A naive approach that preserves arrays as-are and doesn't attempt to flatten them seems fair for now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-02-06T03:22:41Z","2021-09-28T16:07:10Z"
"","10073","KAFKA-12303: Fix handling of null values by Flatten SMT","[Jira](https://issues.apache.org/jira/browse/KAFKA-12303)  Using `return` instead of `continue` when encountering null fields causes the remainder of the fields to be skipped. This PR addresses that, and adds a lightweight unit test that is able to reproduce the error on current versions of Connect, and verify the accuracy of the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-02-06T02:45:26Z","2021-09-28T16:07:07Z"
"","10112","KAFKA-12226: Prevent source task offset failure when producer is overwhelmed","[Jira](https://issues.apache.org/jira/browse/KAFKA-12226)  When a task fails to commit offsets because all outstanding records haven't been ack'd by the broker yet, it's better to retry that same batch. Otherwise, the set of outstanding records can grow indefinitely and all subsequent offset commit attempts can fail. By retrying the same batch, it becomes possible to eventually commit offsets, even when the producer is unable to keep up with the throughput of the records provided to it by the task.  Two unit tests are added to verify this behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-02-11T20:52:58Z","2021-09-13T18:53:55Z"
"","9806","KAFKA-10895: Attempt to prevent JAAS config from being overwritten for basic auth filter in Connect","[Jira](https://issues.apache.org/jira/browse/KAFKA-10895)  # Problem  If a connector, converter, etc. invokes [Configuration::setConfiguration](https://docs.oracle.com/javase/8/docs/api/javax/security/auth/login/Configuration.html#setConfiguration-javax.security.auth.login.Configuration-), it will cause the Connect basic auth filter to use that JAAS config instead of the one configured at startup with the `-Djava.security.auth.login.config` JVM property. This can cause requests to the worker to succeed initially but start to be rejected after the JVM's global JAAS config is altered.  # Solution  Cache the JVM's global JAAS configuration as soon as possible (in this case, as soon as the `BasicAuthSecurityRestExtension` class is loaded), and use that for all future authentication. It's still possible that a different JAAS config than the one that the JVM was configured to use on startup will be captured at this point, but the chances of that should be very slim since it would require a plugin class to install a new global JAAS config as soon as it was loaded. Even if that does happen, this should at the very least guarantee consistent behavior on the worker--any request that succeeds once will always succeed, as opposed to the current situation where requests can succeed for a little bit but then start to get inexplicably rejected after a little while when a plugin class chooses to install a new global JAAS config.  # Testing  Existing tests for the JAAS basic auth filter are modified to work with the new internal logic. The `testEmptyCredentialsFile` test is corrected to actually operate on an empty credentials file (instead of a non-existent credentials file, which it currently operates on). A new test is added to ensure that, even if the global JAAS config is overwritten, the basic auth filter will use the first one it could find.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-12-31T22:02:13Z","2021-01-12T15:13:39Z"
"","9669","KAFKA-10792: Prevent source task shutdown from blocking herder thread","[Jira](https://issues.apache.org/jira/browse/KAFKA-10792)  The functional changes are simple: change the `WorkerSourceTask` class to only call `SourceTask::stop` from one location, during task shutdown, and only if an attempt has been made to start the task (which will not be the case if it was created in the paused state and then shut down before being started). This is important in order to prevent `SourceTask::stop` from being indirectly invoked on the herder's thread, which can have adverse effects if the task is unable to shut down promptly.  Unit tests are tweaked where necessary to account for this new logic, which covers some edge cases mentioned in https://github.com/apache/kafka/pull/5020 that were unaddressed up until now.  The existing integration tests for blocking connectors are expanded to also include cases for blocking source and sink tasks. Full coverage of every source/sink task method is intentionally omitted from these expanded tests in order to avoid inflating test runtime (each one adds an extra 5 seconds at minimum) and because the tests that are added here were sufficient to reproduce the bug with source task shutdown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2020-12-01T19:59:55Z","2020-12-04T17:50:26Z"
"","9765","KAFKA-10763: Fix incomplete cooperative rebalances preventing connector/task revocations","[Jira](https://issues.apache.org/jira/browse/KAFKA-10763) When two cooperative rebalances take place soon after one another, a prior rebalance may not complete before the next rebalance is started. Under Eager rebalancing, no tasks would have been started, so the subsequent onRevoked call is intentionally skipped whenever rebalanceResolved was false. Under Cooperative rebalancing, the same logic causes the DistributedHerder to skip stopping all of the connector/task revocations which occur in the second rebalance. The DistributedHerder still removes the revoked connectors/tasks from its assignment, so that the DistributedHerder and Worker have different knowledge of running connectors/tasks. This causes the connector/task instances that would have been stopped to disappear from the rebalance protocol, and left running until their workers are halted, or they fail. Connectors/Tasks which were then reassigned to other workers by the rebalance protocol would be duplicated, and run concurrently with zombie connectors/tasks. Connectors/Tasks which were reassigned back to the same worker would encounter exceptions in Worker, indicating that the connector/task existed and was already running.  * Add a test for revoking and then reassigning a connector under normal circumstances * Add a test for revoking and then reassigning a connector following an incomplete cooperative rebalance * Change expectRebalance to make assignment fields mutable before passing them into the DistributedHerder * Only skip revocation for the Eager protocol, and never skip revocation for cooperative/sessioned protocols  This should be backported to all branches with Incremental Cooperative Rebalancing, 2.3+ if possible. This is a serious consistency bug, and the fix should probably be applied widely.  Signed-off-by: Greg Harris   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-12-18T04:34:09Z","2021-01-26T18:17:06Z"
"","10016","KAFKA-10340: Proactively close producer when cancelling source tasks","[Jira](https://issues.apache.org/jira/browse/KAFKA-10340)  When a source task produces records for a topic that doesn't exist on the Kafka cluster and automatic topic creation is disabled on the broker and not configured à la [KIP-158](https://cwiki.apache.org/confluence/display/KAFKA/KIP-158%3A+Kafka+Connect+should+allow+source+connectors+to+set+topic-specific+settings+for+new+topics) on the connector, the task hangs indefinitely until and unless the topic is created. Even if the task is scheduled for shutdown by the worker, it continues to hang; the `SourceTask` instance isn't stopped and the producer isn't closed.  One possible approach to handle this situation is to proactively close the producer for the task when it is abandoned after exceeding the graceful shutdown timeout period. This can increase the likelihood of duplicate records for tasks that are blocked on shutdown for other reasons (high throughput, for example), as offsets will not be committed for any outstanding batches. However, given that the Connect framework's overall delivery guarantees of Connect source connectors still remain intact with this approach (either at-least-once or at-most-once, but not exactly-once), the tradeoff seems acceptable in order to prevent resource leaks that, if stacked over a long enough period, will require worker restarts to deal with.  An existing unit test for the `WorkerSourceTask` class is expanded to ensure that the producer is closed when the task is abandoned, and a new integration test is added that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-02-01T20:59:08Z","2021-03-01T16:12:07Z"
"","9767","KAFKA-10047: Remove unnecessary widening of (int to long) scope in FloatSerializer.","@halorgium @astubbs @alexism @glasser @rhardouin","closed","","tguruprasad","2020-12-18T21:22:47Z","2020-12-19T17:34:43Z"
"","10115","MINOR: set copyright year to 2021","2020 is over (thank god) so we need to bump the year for the 2.6.2 release","closed","","ableegoldman","2021-02-12T00:44:57Z","2021-02-12T00:51:53Z"
"","10348","MINOR: KafkaAdminClient check null for group.groupState()","1. when group.groupState() == null , the original writing method will throw a java.lang.NullPointerException, so the following is judged group.groupState   2. the caller of the equal method is flipped.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","unlizhao","2021-03-18T09:30:56Z","2021-03-18T14:28:49Z"
"","10063","KAFKA-12258: Add support for splitting appending records","1. Type `BatchAccumulator`. Add support for appending records into one or more batches. 2. Type `RaftClient`. Rename `scheduleAppend` to `scheduleAtomicAppend`. 3. Type `RaftClient`. Add a new method `scheduleAppend` which appends records to the log using as many batches as necessary. 4. Increase the batch size from 1MB to 8MB.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-02-05T01:59:19Z","2021-02-19T04:46:41Z"
"","9507","KAFKA-10628: remove all the unnecessary parameters from the tests which are using TopologyTestDriver","1. remove unneeded javadoc content. 2. Replace containsKey/setProperty with `putIfAbsent` (good refactor) 3. refactor the constructor of `TopologyTestDriverTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-10-27T05:10:27Z","2020-11-19T03:37:36Z"
"","10402","MINOR: Remove unthrown exceptions, fix typo, etc.","1. Fix typo: clustes → clusters 2. Remove unused class: `RemoteClusterUtils` 3. Remove unthrown Exceptions: `MirrorClientTest`, `MirrorHeartbeatTask`, and `MirrorSourceConnector`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-03-25T13:43:54Z","2021-04-21T06:57:46Z"
"","10232","KAFKA-12352: Make sure all rejoin group and reset state has a reason","1. Create a reason string to be used for INFO log entry whenever we request re-join or reset generation state. 2. Some minor cleanups.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-03-01T06:46:59Z","2021-03-15T16:24:42Z"
"","10176","KAFKA-12359: Update Jetty to 11","1. Add new dependency for Jetty 11.    - javax.ws.rs:jsr311-api:1.1.1   - jakarta.servlet:jakarta.servlet-api:5.0.0  2. Update the Java EE related dependency names. (see: [#1](https://webtide.com/renaming-from-javax-to-jakarta/) [#2](https://wiki.eclipse.org/Jakarta_EE_Maven_Coordinates))    - javax.activation:activation:1.1.1 -> com.sun.activation:javax.activation:1.2.0   - javax.ws.rs:javax.ws.rs-api 2.1.1 -> jakarta.ws.rs:jakarta.ws.rs-api 3.0.0.   - javax.xml.bind:jaxb-api:2.3.0 -> jakarta.xml.bind:jakarta.xml.bind-api:2.3.0  3. Change all `javax.ws.rs.*` imports into `jakarta.ws.rs.*`.  4. Make Jackson to be compatible with new Jakarta API.    - Add jakarta classifier to jackson-jaxrs-json-provider for compatibility with jakarta.ws.rs-api. (see: [here](https://stackoverflow.com/questions/26207252/messagebodywriter-not-found-for-media-type-application-json))   - Add org.glassfish.jersey.media:jersey-media-json-jackson:3.0.1 dependency for registering jackson as a `MessageBodyWriter`.  5. Upgrade jersey from 2.31 to 3.0.1. 6. Upgrade Jetty to 11.0.2 and apply the API changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-02-22T15:18:50Z","2021-06-01T05:25:21Z"
"","10378","KAFKA-7106: remove deprecated Windows APIs","1) Remove all deprecated APIs in KIP-328. 2) Remove deprecated APIs in Windows in KIP-358.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-03-23T04:50:48Z","2021-03-28T19:33:43Z"
"","10350","MINOR: Demo use of *-server-stop commands to stop","...rather than CTRL+C.","open","","bsolomon1124","2021-03-18T16:28:33Z","2021-03-19T18:45:36Z"
"","9534","KAFKA-10664: Delete existing checkpoint when writing empty offsets","...otherwise we can get stuck in an endless loop of initializing corrupted offsets, hitting OffsetOutOfRangeException and closing the task, then reviving the task with those same corrupted offsets.","closed","","ableegoldman","2020-10-30T02:20:55Z","2020-11-02T18:53:00Z"
"","10272","MINOR: Various javadoc fixes","- Use consistent options for `javadoc` and `aggregatedJavadoc` - `aggregatedJavadoc` depends on `compileJava` - `connect-api` inherits `options.links` - `streams` and `streams-test-utils` javadoc exclusions should be more specific to avoid unexpected behavior in `aggregatedJavadoc` when the javadoc for multiple modules is generated together  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-05T22:19:46Z","2021-03-09T14:01:41Z"
"","9916","MINOR: Import RaftConfig parameters into KafkaConfig","- Typically, Raft Quorum Configuration is provided through the same properties file as the Broker/Controller a.k.a KafkaConfig - During the lifecycle of the Broker/Controller, we do reference the Quorum configuration outside of the RaftConfig (Eg: RaftManager.scala)  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","aloknnikhil","2021-01-15T22:59:36Z","2021-01-21T18:27:32Z"
"","9881","MINOR: Initialize QuorumState lazily in RaftClient.initialize()","- There are cases where the quorum voters connect string is not ready   up until after the controller starts.   Eg: If the controller is configured to choose a free port, we need to       wait until after the controller starts to configure the right       host:port connect string for the RaftClient to connect to  ### Tested with the Raft unit-tests  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","aloknnikhil","2021-01-13T21:29:05Z","2021-01-19T17:38:39Z"
"","10424","MINOR Replaced File with Path in LogSegmentData.","- Replaced File with Path in LogSegment Data. - Addressed a few minor renames and comments.  This is a followup of https://github.com/apache/kafka/pull/10173  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-03-29T07:31:37Z","2021-03-31T17:43:11Z"
"","9991","MINOR: Reorder the modifiers and Replace Map.get with Map.computeIfAbsent","- Reorder the modifiers to comply with the Java Language Specification. - Replace Map.get() and condition with a call to Map.computeIfAbsent().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-01-28T16:27:26Z","2021-02-09T07:41:09Z"
"","9570","KAFKA-9274: Handle TimeoutException on commit","- part of KIP-572  - when KafkaStreams commit a task, a TimeoutException should not kill    the thread but `task.timeout.ms` should be triggered and the commit    should be retried in the next loop  Call for review @vvcephei","closed","kip,","mjsax","2020-11-06T05:57:41Z","2021-02-23T03:21:24Z"
"","9997","KAFKA-9274: Add timeout handling for `StreamPartitioner`","- part of KIP-572  When a custom `StreamPartitioner` is used, we need to get the number of partitions of output topics from the producer. This `partitionFor(topic)` call may through a `TimeoutException` that we now handle gracefully.","closed","kip,","mjsax","2021-01-29T04:59:50Z","2021-02-23T03:21:20Z"
"","10000","KAFKA-9274: handle TimeoutException on task reset","- part of KIP-572  This changes move the offset reset for the internal ""main consumer"" when we revive a corrupted task, from the ""task cleanup"" code path, to the ""task init"" code path. For this case, we have already logic in place to handle `TimeoutException` that might be thrown by `consumer#committed()` method call.","closed","kip,","mjsax","2021-01-29T06:46:49Z","2021-02-23T03:21:20Z"
"","9660","Kafka 10629 - TopologyTestDriver should not require a Properties argument","- KIP-680: TopologyTestDriver should not require a Properties argument. https://cwiki.apache.org/confluence/display/KAFKA/KIP-680%3A+TopologyTestDriver+should+not+require+a+Properties+argument - Jira for the KIP:  https://issues.apache.org/jira/browse/KAFKA-10629  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","rohitrmd","2020-11-28T01:50:31Z","2021-02-04T01:43:07Z"
"","10012","MINOR: Refactor return statement and log info","- Invoke method only conditionally. - No need to call ""toString()"" method as formatting and string conversion is done by the Formatter.    before ```java log.info(""Successfully validated token with principal {}: {}"", unsecuredJwt.principalName(), unsecuredJwt.claims().toString()); ```  after ```java if (log.isInfoEnabled()) {     log.info(""Successfully validated token with principal {}: {}"", unsecuredJwt.principalName(), unsecuredJwt.claims()); } ```  - Immediately return the expression instead of assigning it to the temporary variable.  before ```java String principalClaimName = principalClaimNameValue != null && !principalClaimNameValue.trim().isEmpty()         ? principalClaimNameValue.trim()         : ""sub""; return principalClaimName; ```  after ```java return principalClaimNameValue != null && !principalClaimNameValue.trim().isEmpty()         ? principalClaimNameValue.trim()         : ""sub""; ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-02-01T08:10:42Z","2021-02-09T07:40:44Z"
"","10199","KAFKA-12374: Add missing config sasl.mechanism.controller.protocol","- Implements the KIP-631 config `sasl.mechanism.controller.protocol` - Updates `KafkaRaftManager` to use the first entry in `controller.listener.names` to determine the listener name; that listener name's mapped value in the `listener.security.protocol.map` (if such a mapping exists, otherwise the listener name itself) for the security protocol; and the value of `sasl.mechanism.controller.protocol` for the SASL mechanism.  Prior to this patch it was incorrectly using inter-broker security information when it connects to the Raft controller quorum. - Updates `RaftControllerNodeProvider` to use the value of `sasl.mechanism.controller.protocol` instead of the inter-broker sasl mechanism (it was already determining the listener name and security protocol correctly) - Updates system tests in `tests/kafkatest/tests/core/security_test.py` to correctly test various TLS hostname verification scenarios for Raft controller quorums, and includes new tests to confirm that hostname verification failures to both a Raft metadata quorum and a ZooKeeper quorum prevent Kafka from starting.  The system tests now exercise the changes to `KafkaRaftManager` and `RaftControllerNodeProvider` described above. - Adds system tests in `tests/kafkatest/sanity_checks/test_verifiable_producer.py` to check for various Raft-related security protocol/sasl mechanism combinations to make sure they work.  -  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-02-23T22:33:29Z","2021-02-27T00:57:17Z"
"","10433","MINOR: Fix  - MirrorMaker v2 documentation","- Fix typo in code example - s/enable/enabled  Related to this PR: https://github.com/apache/kafka-site/pull/341#issuecomment-808951896  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","AdilHoumadi","2021-03-30T04:55:39Z","2022-05-01T11:58:10Z"
"","10316","KAFKA-10034: Make Producer's max.request.size configuration description clearer","- Clarify the description of max.request.size configuration to explain the two ways places it is being used - Clarify its relationship to batch.size configuration - Add warning if batch.size is set to be greater than max.request.size    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","badaiaqrandista","2021-03-15T06:54:24Z","2021-03-15T06:54:24Z"
"","9909","KAFKA-12196 Migrate connect:api module to JUnit 5","- change import  - assertThrows replaces try-catch","closed","","g1geordie","2021-01-15T16:32:17Z","2021-01-18T03:17:48Z"
"","10139","MINOR: Print the warning log before truncation.","- After truncation, hw can be moved to the truncation offset.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-02-17T05:46:08Z","2021-08-04T07:06:05Z"
"","9975","MINOR: Build modules in parallel","- According to https://docs.gradle.org/current/userguide/performance.html#parallel_execution gradle executes builds serially by default. - With this change, the build performance is significantly better (~2x) on multi-core machines  ### With this change **Here's a terminal recording of a build with this change:** https://ascii.purplpkg.com/a/kuBqEKEeLgvKXTUJEkq8MNk5v  **Here's a build scan with this change:** https://gradle.com/s/kqmmxwwu6r4lk  ### Current default **Here's a terminal recording of a build with the default:** https://ascii.purplpkg.com/a/P7W6Z4FmKtaKkPazFWGKwNmTC  **Here's a build scan with the default:** https://gradle.com/s/e4mkzj53gbu2k  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [  ] Verify documentation (including upgrade notes)","closed","","aloknnikhil","2021-01-27T01:22:10Z","2021-01-28T15:25:23Z"
"","10259","MINOR: Provide valid examples in README page.","- `testMetadataUpdateWaitTime` method is removed from MetadataTest class. - travis-ci.org will be shut down in some time. All the interactions are now via travis-ci.com. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-03-04T07:17:31Z","2022-02-21T06:49:19Z"
"","9659","KAFKA-10770: Remove duplicate defination of Metrics#getTags","*Removal of duplicate code.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ArunParthiban-ST","2020-11-27T22:41:26Z","2020-12-01T05:10:23Z"
"","10337","KAFKA-12380: Executor in Connect's Worker is not shut down when the worker is","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  When the worker is stopped, it does not shutdown this executor.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  The following tests are run: * `./gradlew connect:test` * `./gradlew connect:unitTest` * `./gradlew connect:integrationTest`   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","sridhav","2021-03-17T02:32:35Z","2022-04-29T05:36:36Z"
"","10287","TranslateSurrogates SMT","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Kafka Connect does not have an out of the box way to process UTF16 surrogate pairs. This SMT adds that capability.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Unit tests included. Change has no impact outside of Connect transforms, so System Tests are not considered.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","sivakunapuli","2021-03-09T19:46:50Z","2021-03-12T12:42:25Z"
"","9908","KAFKA-12213: Kafka Streams aggregation Initializer to accept record key","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  As per title and jira ticket.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Ran gradlew tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","jCalamari","2021-01-15T15:19:23Z","2021-01-15T20:31:30Z"
"","9760","KAFKA-10850: Use 'Int.box' to replace deprecated 'new Integer' from BrokerToControllerRequestThreadTest","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","govi20","2020-12-16T14:34:08Z","2020-12-17T16:43:22Z"
"","10468","Kafka 12373:Improve KafkaRaftClient handling of graceful shutdown","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vamossagar12","2021-04-03T11:01:12Z","2021-05-28T02:40:38Z"
"","10421","KAFKA-12568: Remove deprecated APIs in KStream, KTable and Joined","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-03-28T02:02:40Z","2021-04-08T00:38:47Z"
"","10368","[DO NOT MERGE] investigate flaky test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-20T08:01:15Z","2021-03-31T02:37:53Z"
"","10302","KAFKA-7785: move internal DefaultPartitionGrouper","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","highluck","2021-03-11T05:41:15Z","2021-04-07T20:36:49Z"
"","10295","Merge master 3 10 21","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-03-10T18:26:02Z","2021-03-10T18:26:34Z"
"","10288","[KAFKA-10434] KIP-667 implementation","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-03-09T22:39:55Z","2021-03-10T17:28:17Z"
"","10278","KAFKA-10526: leader fsync deferral on write","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vamossagar12","2021-03-08T16:20:51Z","2021-05-28T02:38:09Z"
"","10230","Pa.first commit","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ahuja0007","2021-02-27T22:09:20Z","2021-02-27T22:09:58Z"
"","10186","MINOR: bump release version to 3.0.0-SNAPSHOT","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2021-02-23T02:53:06Z","2021-02-25T01:49:22Z"
"","10132","Fix ssl close","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","purplefox","2021-02-16T16:09:29Z","2021-02-17T09:27:08Z"
"","10052","KAFKA-12289: Adding test cases for prefix scan in InMemoryKeyValueStore","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2021-02-04T11:33:23Z","2021-03-02T17:54:37Z"
"","10051","Adding documentation for KIP-614","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2021-02-04T09:54:04Z","2021-02-24T19:36:36Z"
"","9936","[WIP] reset to default class loader","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-01-20T09:33:19Z","2021-01-21T01:56:36Z"
"","9927","[WIP] KAFKA-8460","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-19T09:50:14Z","2021-01-20T05:25:58Z"
"","9870","KAFKA-12174: A program for dynamically alter log4j levels at runtime.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lordcheng10","2021-01-12T02:24:41Z","2022-02-27T16:12:07Z"
"","9869","Test2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2021-01-12T00:40:34Z","2021-01-12T08:11:38Z"
"","9868","Test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2021-01-12T00:36:10Z","2021-01-12T02:17:37Z"
"","9860","KAFKA-12174: A program for dynamically alter log4j levels at runtime.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2021-01-11T12:22:38Z","2021-01-12T02:14:36Z"
"","9859","A program for dynamically alter log4j levels at runtime.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2021-01-11T12:07:24Z","2021-01-11T12:08:59Z"
"","9795","2.5","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","keashem","2020-12-29T11:40:59Z","2020-12-29T11:41:42Z"
"","9793","Trunk","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cshuig","2020-12-29T10:21:38Z","2020-12-29T10:22:33Z"
"","9774","MINOR: Add 2.7.0 release to broker and client compat tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-12-21T19:40:45Z","2021-01-05T14:45:34Z"
"","9724","MINOR: work in progress for 2.6","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-12-10T05:19:31Z","2021-05-20T14:28:44Z"
"","9719","[MINOR] enrich the kafka log message","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lisa2lisa","2020-12-09T17:32:05Z","2020-12-09T17:32:05Z"
"","9651","[WIP] fix Travis CI error","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2020-11-25T08:58:07Z","2020-11-25T09:31:16Z"
"","9625","MINOR: remove semicolon","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Hamza-Slama","2020-11-19T22:35:29Z","2020-11-20T07:34:28Z"
"","9544","MINOR: Add back 2.6 notable update section taken out by mistake","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-11-02T23:53:55Z","2020-11-04T19:44:58Z"
"","9539","KAFKA-10634: Adding LeaderId to Voters list in LeaderChangeMessage","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2020-10-31T06:00:57Z","2020-12-09T01:37:49Z"
"","9514","MINOR: Add KIP-431 to upgrade.html file","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-10-27T23:29:02Z","2020-10-27T23:35:39Z"
"","9511","MINOR: Add KIP-584 to upgrade.html file","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-10-27T17:02:16Z","2020-10-27T18:39:00Z"
"","9508","KAFKA-10648: Add Prefix Scan support to State Stores","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2020-10-27T05:18:06Z","2021-02-04T11:41:31Z"
"","9500","Investigate Flaky tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2020-10-26T06:05:58Z","2022-06-30T09:56:59Z"
"","9483","MINOR: Update docs to point to next release add notable features for 2.7","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-10-22T21:33:01Z","2020-10-22T21:57:50Z"
"","9820","MINOR: Make data in FetchSnapshotReq and FetchSnapshotResp private","*More detailed description of your change* We keep all data in request and response private, use public method `AbstractRequestResponse.data()` to access them.  Following work: maybe we could try to parameterize `AbstractRequestResponse` to `AbstractRequestResponse` to simplify the data field.  *Summary of testing strategy (including rationale)* just trivial changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-04T05:06:01Z","2021-01-18T12:42:47Z"
"","10325","MINOR: Remove redudant LocalLogManager","*More detailed description of your change* We have 2 identical LocalLogManager in `src/main/org.apache.kafka.metalog` and `src/test/org.apache.kafka.metalog`, the only difference is that the test class have more docs, I just move the docs from test to src and remove test class.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","dengziming","2021-03-16T01:42:33Z","2021-08-28T14:11:31Z"
"","10289","KAFKA-12440: ClusterId validation for Vote, BeginQuorum, EndQuorum and FetchSnapshot","*More detailed description of your change* This pr follows up #10129 which add clusterId validation to FetchRequest.  *Summary of testing strategy (including rationale)* Unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-10T07:16:13Z","2021-03-20T01:34:36Z"
"","9852","MINOR: substitute assertEquals(null) with assertNull","*More detailed description of your change* This is a suggestion of IDEA  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-10T07:37:58Z","2021-01-10T19:06:38Z"
"","10255","KAFKA-12388: Share broker channel between alterIsrManager and lifecycleManager","*More detailed description of your change* There are some `BorkerToControllerChannerManager` in `BrokerServer` and `KafkaServer`, We are planning to consolidate into two channels eventually: 1. broker to controller channel 2. client to controller channel  Auto topic creation and forwarding fall into the 2nd category, while AlterIsr, lifecycleManager, and logDirEventManager(see KAFKA-9837) would be the 1st category. #10135 is consolidating the 2nd category, this pr is trying to consolidate the 1st category.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-03-04T02:44:56Z","2021-05-19T21:47:43Z"
"","10243","KAFKA-12398: Fix flaky test `ConsumerBounceTest.testClose`","*More detailed description of your change* The test fails some times as follow: ![Pasted Graphic](https://user-images.githubusercontent.com/26023240/109610770-286d5080-7b68-11eb-92a5-ab6b45b79c2f.png)  We'd better use `TestUtils.waitUntilTrue` instead of waiting for 1 second because sometimes 1 second is too long and sometimes is too short.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-02T07:04:26Z","2022-03-17T13:04:10Z"
"","10159","KAFKA-12338: Consolidate MetadataRecordSerde and MetadataParser serial/deserial code","*More detailed description of your change* The logics are duplicated except that `MetadataRecordSerde` has an extra `DEFAULT_FRAME_VERSION`, if we want to change the serial/deserial format of metadata, we should modify 2 classes, this is unreasonable.  *Summary of testing strategy (including rationale)* unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-19T03:06:49Z","2021-06-08T01:49:43Z"
"","9988","MINOR: remove useless repeated method call in KafkaApiTest","*More detailed description of your change* SetReplicas was invoked twice with same params, I think the author means setIsrs  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-28T09:58:10Z","2021-02-03T04:12:12Z"
"","9970","KAFKA-12233: Align the length passed to FileChannel by `FileRecords.writeTo`","*More detailed description of your change* See https://issues.apache.org/jira/browse/KAFKA-12233  *Summary of testing strategy (including rationale)* unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-26T02:23:42Z","2021-01-27T04:51:28Z"
"","10467","KAFKA-12609: Rewrite ListOffsets using AdminApiDriver","*More detailed description of your change* See here: https://github.com/apache/kafka/pull/10275#issuecomment-806331897  *Summary of testing strategy (including rationale)* 1. add a unit test for `ListOffsetsHandler` 2. existing test in `KafkaAdminClientTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-04-03T00:48:40Z","2021-08-29T13:28:23Z"
"","9754","KAFKA-10856: Convert sticky assignor userData schemas to use generated protocol","*More detailed description of your change* Replace sticky assignor userData schemas to use generated protocol  *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2020-12-15T05:25:21Z","2022-03-08T10:07:56Z"
"","10031","KAFKA-12269: Replace partition metada with auto-generated protocal","*More detailed description of your change* MetadataResponse.PartitionMetadata is almost the same as MetadataResponsePartition so we can replace it, below is the difference that we need to notice: 1. `MetadataResponse.PartitionMetadata` contains a `TopicPartition` but `MetadataResponsePartition` only contains partitionIndex, so we need to change some method to add topic if necessary. 2. leaderId and leaderEpoch in `MetadataResponse.PartitionMetadata` are `Optional`, we use `NO_LEADER_ID` and `NO_PARTITION_LEADER_EPOCH` to replace null `Optional`.  Following work: The `MetadataResponse.TopicMetadata` can also be replaced in a next pr, this pr only replace `MetadataResponse.PartitionMetadata` to make pr simple for review.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-02-03T00:25:48Z","2021-03-03T06:00:02Z"
"","10128","MINOR: remove duplicate code of serializing auto-generated data","*More detailed description of your change* Inspired by #9964  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-16T05:52:58Z","2021-02-16T15:55:20Z"
"","9956","MINOR: Revert assertion in MockProducerTest","*More detailed description of your change* In #9955 I made a mistake,  !A && !B = ! (A || B), thank @g1geordie  for pointing out my mistake.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-24T08:11:54Z","2021-01-24T13:09:58Z"
"","10037","MINOR: remove unused import in TopicIdWithOldInterBrokerProtocolTest","*More detailed description of your change* I found a warning message in every QA Jenkins build: ``` [2021-02-03T01ː12ː51.812Z] [Warn] /home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-10031/core/src/test/scala/unit/kafka/server/TopicIdWithOldInterBrokerProtocolTest.scalaː24ː imported ‘BaseRequestTest‘ is permanently hidden by definition of class BaseRequestTest in package server ``` just remove the unsed import.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-03T15:25:32Z","2021-02-04T03:07:46Z"
"","9982","MINOR: remove some explicit type argument in generator","*More detailed description of your change* From `ArrayList newCollection = new ArrayList(arrayLength)` to `ArrayList newCollection = new ArrayList<>(arrayLength)`  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-27T13:42:14Z","2021-02-08T18:22:41Z"
"","10250","MINOR: Fix null exception in coordinator log","*More detailed description of your change* Found that the `fatalException` is always null when calling `log.info(""xxx"", fatalException)`, maybe we should first assign a value to it.    *Summary of testing strategy (including rationale)* Test locally.  from ``` [2021-03-03 10:18:06,203] INFO FindCoordinator request hit fatal exception (org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator:260) ``` to  ``` [2021-03-03 10:17:37,123] INFO FindCoordinator request hit fatal exception (org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator:260) org.apache.kafka.common.errors.AuthenticationException: Authentication failed ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-03T02:18:39Z","2021-03-03T04:38:06Z"
"","10136","MONOR: Import classes that is used in docs to fix warnings.","*More detailed description of your change* Fix this: ![image](https://user-images.githubusercontent.com/26023240/108154064-43c36f00-7117-11eb-80dc-e62db0bac081.png)   *Summary of testing strategy (including rationale)* None   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-17T03:57:49Z","2021-02-18T03:08:29Z"
"","10097","MINOR: Add FetchSnapshot API doc in KafkaRaftClient","*More detailed description of your change* Currently, we have added FetchSnapshot API in raft, so improve the docs in `KafkaRaftClient`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-10T12:12:11Z","2021-02-12T15:38:10Z"
"","10400","MINOR: fix package name in core test","*More detailed description of your change* Change package name of `IntegrationTestUtils` ,`TransactionsWithMaxInFlightOneTest`, `ControllerContextTest` and `DefaultMessageFormatterTest` to `kafka.server` since we set the package name to `kafka.xxx` in all other classes.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-25T06:43:11Z","2021-04-16T06:38:43Z"
"","10116","MINOR: Add entityType for metadata record definitions","*More detailed description of your change* As title  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-12T03:06:26Z","2021-03-10T05:27:06Z"
"","9907","KAFKA-12197: Migrate connect:transforms module to JUnit 5","*More detailed description of your change* As title  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-15T13:43:36Z","2021-01-16T15:17:55Z"
"","10229","MINOR: Account for extra whitespaces in WordCountDemo","*More detailed description of your change* As the title.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dengziming","2021-02-27T03:07:04Z","2021-08-28T13:58:36Z"
"","10434","MINOR: Remove wrong suppress","*More detailed description of your change* As the title.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-30T04:59:12Z","2021-03-31T04:02:46Z"
"","10162","DOCS: Update protocol doc for missing data type","*More detailed description of your change* Add missing records(KAFKA-9629) uint16(KAFKA-12180) docs.  *Summary of testing strategy (including rationale)* No  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-19T14:09:56Z","2021-08-28T13:58:41Z"
"","9770","MINOR: Add ByteBufferAccessorTest","*More detailed description of your change* Add ByteBufferAccessorTest  similar to SendBuilderTest 1. test basic read and write 2. test none zero-copy when writing ByteBuffer and Records 3. Write ByteBuffer respects position  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-19T11:38:59Z","2022-04-01T06:17:47Z"
"","10068","MINOR: StopReplicaResp and StopReplicaReq Test should cover all available version","*More detailed description of your change* Accidently found that the test case in `StopReplicaRespTest` and `StopReplicaReqTest` didn't cover all versions.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-05T10:54:27Z","2021-08-28T13:58:37Z"
"","10247","MINOR: Fix log format in AbstractCoordinator","*More detailed description of your change* Accidentally saw a log: ``` [2021-03-02 20:39:56,418] DEBUG FindCoordinator request failed due to {} (org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator:863) org.apache.kafka.common.errors.AuthenticationException: Authentication failed ```  I fixed it to: ``` [2021-03-02 20:51:39,050] DEBUG FindCoordinator request failed due to Authentication failed (org.apache.kafka.clients.consumer.internals.AbstractCoordinatorTest$DummyCoordinator:863) ```  I think the author wants to call `Logger.debug(String format, Object arg)`, but called `Logger.debug(String msg, Throwable t)`. In fact, there are 5 overloadings `Logger.debug()` so we need to be careful.  *Summary of testing strategy (including rationale)* test locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-02T13:10:33Z","2022-03-25T08:08:33Z"
"","10033","MINOR: Fix always pass unit test in MetadataRequestTest","*More detailed description of your change* `Seq[Int].contains(KafkaServer)` will always be false, we should use `Seq[Int].contains(KafkaServer.config.brokerId)`  *Summary of testing strategy (including rationale)*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-03T02:27:31Z","2021-02-04T05:51:54Z"
"","10021","KAFKA-12205: Delete snapshots less than the snapshot at the log start","*More detailed description of your change* 3 times to delete a snapshot: 1. When a follower completing fetch snapshot and truncate the log to the latest snapshot 2. When a controller pollCurrentState and delete old segment 3. When restarting the RaftClient.  *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","dengziming","2021-02-02T05:43:06Z","2021-05-06T08:40:56Z"
"","10312","MINOR: Fix log statement whose placeholders are inconsistent with arguments","*More detailed description of your change* 1. When the 2nd argument is an exception we don't need a placeholder 2. Placeholders should equal to arguments.   *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-13T01:48:33Z","2021-03-22T05:39:05Z"
"","9771","MINOR: Add paramerter for local CompletedBatch variable","*More detailed description of your change* 1. When I was reviewing KAFKA-10394, I accidentally find a generic type is losing, maybe the author forget it. 2. Remove `public` in  java Interface","closed","","dengziming","2020-12-21T14:28:35Z","2022-04-23T11:44:29Z"
"","9890","KAFKA-12198: Migrate connect:json module to JUnit 5","*More detailed description of your change* 1. Use junit5 methods to replace junit4 ones 2. change assertEquals(true) to assertTrue 3. optimize import order   *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-14T10:29:12Z","2021-01-15T19:14:09Z"
"","9807","KAFKA-10779; Reassignment tool sets throttles incorrectly when overriding a reassignment","*More detailed description of your change* 1. The addingReplicas is included in the AR during reassignment, so remove addingReplicas from AR  in `calculateCurrentMoveMap` 2.  The addingReplicas is also included in MetadataResponse, so only use MetadataResponse data when there is no existing reassignment of this partition 3. Alter test code data 4. Improve some code style  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-01T10:06:01Z","2021-01-07T23:11:22Z"
"","9664","KAFKA-10780; Rewrite ControllerZNode struct with auto-generated protocol","*More detailed description of your change* 1. The #9662 rewrite FeatureZNode struct with auto-generated protocol, but it's a non-trivial change, so we can just review this simple pr first. 2. Copy some code of `org.apache.kafka.raft.FileBasedStateStore` to generate json data from and to FeatureZNodeData  *Summary of testing strategy (including rationale)* unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-01T02:38:16Z","2020-12-05T03:14:04Z"
"","10079","MINOR: replace hard-coding utf-8 with StandardCharsets.UTF_8","*More detailed description of your change* 1. Replace hard-code charset 2. remove UnsupportCharsetException  *Summary of testing strategy (including rationale)* run test locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-08T01:24:39Z","2021-02-09T02:06:02Z"
"","9766","KAFKA-10864: Convert end txn marker schema to use auto-generated protocol","*More detailed description of your change* 1. Convert end txn marker schema to use auto-generated protocol `EndTxnMarker` 2. substitute `CURRENT_END_TXN_MARKER_VALUE_SIZE` with an `endTnxMarkerValueSize` method since the size is accumulated from `EndTxnMarker`. 3. add buffer to `EndTransactionMarker` to avoid twice compute from `serializeValue` and `endTnxMarkerValueSize`.  *Summary of testing strategy (including rationale)* A unit test to verify serialize and deserialize  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2020-12-18T06:36:04Z","2021-05-19T10:49:23Z"
"","9955","MINOR: Optimize assert in unit test","*More detailed description of your change* 1. assertTrue(!) -> assertFalse() 2. assertNotEquals(null, x) -> assertNotNul(x) 3. assertEquals(null, x) -> assertNull(x)  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-24T04:43:48Z","2021-01-24T07:55:08Z"
"","9769","KAFKA-10774; Support Describe topic using topic IDs","*More detailed description of your change* 1. Add topicNames in `MetadataCache` and alter `KafkaApis.handleTopicMetadataRequest` according to the KIP 2. Add method `describeTopics(TopicCollection)` in KafkaAdminClient, similar to `describeTopics` 3. Change `DescribeTopicsResult` to support TopicId  *Summary of testing strategy (including rationale)* Test locally, here is some result: New server + new Client  ``` kafka-topics.sh --describe --zookeeper localhost:2181 --topic-id a-Paxi-LSka9sIVy9UFp_Q Exception in thread ""main"" java.lang.IllegalArgumentException: --topic-id can used only with --bootstrap-server  kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic-id a-Paxi-LSka9sIVy9UFp_Q Topic: old-version-topic	TopicId: a-Paxi-LSka9sIVy9UFp_Q	PartitionCount: 2	ReplicationFactor: 1	Configs: segment.bytes=1073741824 	Topic: old-version-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0 	Topic: old-version-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0 ```  Old Server + new Client ``` kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic-id a-Paxi-LSka9sIVy9UFp_Q Error while executing topic command : TopicId a-Paxi-LSka9sIVy9UFp_Q not found. ```  New server + old client ``` kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic-id a-Paxi-LSka9sIVy9UFp_Q Exception in thread ""main"" joptsimple.UnrecognizedOptionException: topic-id is not a recognized option ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-19T10:42:50Z","2021-08-28T08:00:37Z"
"","10075","KAFKA-12251: Add topicId and remove topic name in stopReplicaRep and stopReplicaResp","*More detailed description of your change* 1. Add topic id and remove topicName in stopReplicaRep/stopReplicaResp 2. bump IBPversion, old-version broker will reply `UNSUPPORTED_VERSION` on new-version stopReplicaRequest 3. new-version controller will retry with topicName when receiving `UNSUPPORTED_VERSION` stopReplicaRepResponse  *Summary of testing strategy (including rationale)* 1. Unit test 2. ITCase to test retry on `UNSUPPORTED_VERSION`, this is still in progress.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-07T04:57:25Z","2022-02-11T12:46:52Z"
"","10147","MINOR: Add raft resigned state metric name","*More detailed description of your change* 1. Add resigned state metric 2. Update doc for raft state metrics  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-02-18T12:41:36Z","2021-06-18T01:43:55Z"
"","10393","KAFKA-12539: Refactor KafkaRaftCllient handleVoteRequest to reduce cyclomatic complexity","*More detailed description of your change* 1. Add grantVote to `EpochState` 2. Move the if-else in `KafkaRaftCllient.handleVoteRequest` to `EpochState` 3. Add unit-test for `grantVote`  *Summary of testing strategy (including rationale)* Unit Test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-03-24T09:10:00Z","2021-04-07T02:02:33Z"
"","9809","KAFKA-10898: Support snakeCaseName in JsonConverterGenerator","*More detailed description of your change* 1. Add a field `jsonFieldNameStrategy` in `MessageSpec` to represent the naming strategy of JsonConverter 2. `jsonFieldNameStrategy` can be ""camel"" or ""snack"", ""camel"" is default value  *Summary of testing strategy (including rationale)* Tried to parse ReassignPartitionsCommand args and it works normally: 1. define a PartitionReassignmentArgs to represent ReassignPartitionsCommand args 2.  Use `PartitionReassignmentArgsJsonConverter` to read and write ReassignPartitionsCommand json file  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-01T15:21:34Z","2021-06-16T02:50:21Z"
"","9819","KAFKA-10694: Implement zero copy for FetchSnapshot","*More detailed description of your change* 1. `RawSnapshotWriter` and `RawSnapshotReader` interact with `BaseRecords` instead of `ByteBuffer` 2. Use `fileRecords.slice` when read a snapshot 3. No big change of writing a snapshot  *Summary of testing strategy (including rationale)* 1. Change code in unit test of `RawSnapshotWriter` and `RawSnapshotReader`  2. Add unit test in `SendBuilderTest` and `FileRawSnapshotTest` 3. Add unit test for `FileRegion` and `MemoryRegion`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-03T09:31:29Z","2021-01-27T02:37:58Z"
"","9741","KAFKA-10849: Remove useless ApiKeys#parseResponse and ApiKeys#parseRequest","*More detailed description of your change* #7409 has removed the conversion to Struct when parsing resp, `ApiVersionsResponse.parse` will be used instead of `API_VERSIONS.parseResponse`, so the overwrite in `ApiKeys.API_VERSIONS` is useless.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-13T09:51:46Z","2020-12-15T01:29:30Z"
"","9893","MINOR: Fix typo in `shouldUseJUnit5` in build.gradle","*More detailed description of your change* ""mirorr-client"" => ""mirror-client""  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-14T13:43:26Z","2021-08-28T14:14:54Z"
"","9740","MINOR: Skip `Struct` conversion in `FetchRequest.parse`","*More detailed description of your change*  when I was review #7409 , I accidentally found that FetchRequest is missed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-13T09:11:27Z","2020-12-13T16:36:38Z"
"","9734","KAFKA-10845; Add VisibleForTesting annotation","*More detailed description of your change*  There are a lot of comments like ""visible for testing"" ""public for testing"", for example:  https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/controller/KafkaController.scala#L95 so introduce a VisibleForTesting annotation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-12-11T14:37:30Z","2021-09-11T23:27:07Z"
"","9639","KAFKA-10677; Complete fetches in purgatory immediately after resigning","*More detailed description of your change*  If the condition of fetch is satisfied, `BROKER_NOT_AVAILABLE` or `NOT_LEADER_OR_FOLLOWER` is returned when the leader is shutting down. so we just return `BROKER_NOT_AVAILABLE` with a message.  *Summary of testing strategy* A simple unit test to verify fetches in purgatory is completed after resigning.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-11-22T14:36:20Z","2020-12-10T17:25:05Z"
"","9965","MINOR: Fix meaningless message in assertNull validation","*More detailed description of your change*  I think the author wants to say assertEquals(null, xxx)  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-01-25T14:33:27Z","2021-01-27T03:14:32Z"
"","9662","KAFKA-10130; Rewrite FeatureZNode struct with auto-generated protocol","*More detailed description of your change*  1. remove FeatureZNode and replace it with FeatureZNode.json 2. Change code where FeatureZNode is used 3. copy some code of `org.apache.kafka.raft.FileBasedStateStore` to generate json data from and to FeatureZNodeData 4. I will also replace the rest ZNode with auto-generated protocol if this was approved.   *Summary of testing strategy (including rationale)* Replace FeatureZNode  in unit test and integration test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-11-30T10:32:40Z","2021-07-06T07:40:11Z"
"","9622","KAFKA-10547; add topicId in MetadataResp","*More detailed description of your change*  1. Bump the version of MetadataReq and MetadataResp, add topicId in MetadataResp 2. Alter describeTopic in AdminClientTopicService and ZookeeperTopicService 3. TopicMetadata is cached in MetadataCache, so we need to add topicId to MetadataCache 4. MetadataCache is updated by UpdateMetadataRequest, bump the version of UpdateMetadataReq and UpdateMetadataResp, add topicId in UpdateMetadataReq.  *Summary of testing strategy (including rationale)*  Tested locally, here is some result:  New server + new Client :   kafka-topics.sh --describe --zookeeper localhost:2181 --topic old-version-topic  Topic: old-version-topic	TopicId: wRPl6VAlQeyE77bDxEESzg	PartitionCount: 2	ReplicationFactor: 1	Configs:  	Topic: old-version-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0 	Topic: old-version-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0  kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic old-version-topic  Topic: old-version-topic	TopicId: wRPl6VAlQeyE77bDxEESzg	PartitionCount: 2	ReplicationFactor: 1	Configs: segment.bytes=1073741824 	Topic: old-version-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0 	Topic: old-version-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0  Old Server + new Client  kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic old-version-topic  Topic: old-version-topic	PartitionCount: 2	ReplicationFactor: 1	Configs: segment.bytes=1073741824 	Topic: old-version-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0 	Topic: old-version-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0  New server + old client  kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic old-version-topic  Topic: old-version-topic	PartitionCount: 2	ReplicationFactor: 1	Configs: segment.bytes=1073741824 	Topic: old-version-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0 	Topic: old-version-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2020-11-19T11:59:14Z","2021-01-02T18:08:48Z"
"","10189","MINOR: Update copyright year in NOTICE","*More detailed description of your change*  *Summary of testing strategy (including rationale)*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-02-23T09:11:46Z","2022-04-23T11:44:35Z"
"","10280","KAFKA-12554: Refactor Log layer","**TL;DR:**  This PR implements the details of the Log layer refactor, as outlined in this document: https://docs.google.com/document/d/1dQJL4MCwqQJSPmZkVmVzshFZKuFy_bCPtubav4wBfHQ/edit. Few details maybe different from the doc, but it is more or less the same.  **STRATEGY:**  In this PR, I've extracted a new class called `LocalLog` out of `Log`.  Currently `LocalLog` is purely an implementation detail thats not exposed outside `Log` class (except for tests). The object encapsulation is that each `Log` instance  wraps around a `LocalLog` instance.  This new `LocalLog` class attempts to encompass **most** of the responsibilities of local log surrounding the `segments` map, which otherwise were present in `Log` previously. Note that **not** all local log responsibilities have been moved over to this new class (yet). The criteria I used was to preserve (for now) in existing `Log` class, any logic that is mingled  in a complex manner with the `logStartOffset` or the `LeaderEpochCache` or the `ProducerStateManager`.  **WINS:**  The main win is that the new `LocalLog` class is now agnostic of the `logStartOffset`, which continues to be managed mainly by `Log` class. Below is the local log functionality that has successfully moved over from `Log` to `LocalLog`:  1. Access of `LogSegments` instance containing the local `LogSegment` objects. 2. Read path logic to read records from the log. 2. Segment file deletion logic. 3. Segment truncation logic. 4. Segment roll logic. 5. Segment split logic. 6. Segment replacement logic.  Below is the main local log functionality that continues to remain in `Log`:  1. Segment append logic. The reason is that the below logic is mingled with one or more of the following: `logStartOffset` or `LeaderEpochCache` or `ProducerStateManager`. This makes it hard to separate just the local logic out of it. 2. Last stable offset and logic surrounding it. 3. High watermark and logic surrounding it. 4. Logic to `fetchOffsetByTimestamp` and logic to `legacyFetchOffsetsBefore`. 5. Some of the retention logic thats related with the global view of the log. 6. All other logic related with handling  `LeaderEpochCache` and `ProducerStateManager`.  **PAINPOINTS:**  1. Log locking semantics had to be changed in handful of areas, with lock taken at a coarse level. 2. Few API implementations needed re-ordering of logic in `Log` class to make migration feasible. 3. Certain APIs added to `LocalLog` are crude in nature or signature, examples: `def checkIfMemoryMappedBufferClosed`, `def markFlushed`, `def updateRecoveryPoint`, `def replaceSegments` etc. 4. Certain important APIs (such as `def append` logic) were hard to migrate because logic was mingled with Leader epoch cache, Producer state manager and log start offset.  **TESTS:**  * New unit suite: `LocalLogTest.scala` has been provided containing tests specific to `LocalLog` class. All other existing tests are expected to pass.  * 6/10/2021: System test run on top of 28bf22af168ca0db76796b5d3cd67a38ed8ed1c2: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4550/  *  6/12/2021:     * System test runs [4560](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4560/) and [4562](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4562/) on top of 008b701386ce5a4d892d6ac5b90798b981c4fba0.     * System test runs [4561](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4561/) and [4563](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4563/) on top of `trunk/6de37e536ac76ef13530d49dc7320110332cd1ee`.     * `kafkatest.tests.client.consumer_test` rerun:         * [4564](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4564/) against  `trunk/6de37e536ac76ef13530d49dc7320110332cd1ee`.         * [4566](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4566/) on top of 008b701386ce5a4d892d6ac5b90798b981c4fba0.","closed","","kowshik","2021-03-08T21:59:16Z","2021-07-14T16:40:29Z"
"","10401","KAFKA-12552: Introduce LogSegments class abstracting the segments map","**This PR is a precursor to the recovery logic refactor work ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)).**  In this PR, I've extracted the behavior surrounding segments map access within `kafka.log.Log` class into a new class: `kafka.log.LogSegments`. This class encapsulates a thread-safe navigable map of `kafka.log.LogSegment` instances and provides the required read and write behavior on the map. The `Log` class now encapsulates an instance of the `LogSegments` class.  Couple advantages of this PR:  * Makes the `Log` class a bit more modular as it moves out certain private behavior thats otherwise within the `Log` class.  * This is a precursor to refactoring the recovery logic (KAFKA-12553). In the future, the logic for recovery and loading of segments from disk (during `Log`) init will reside outside the `Log` class. Such logic would need to instantiate and access an instance of the newly added `LogSegments` class.  **Tests:** Added a new test suite: `kafka.log.LogSegmentsTest` covering the APIs of the newly introduced class.","closed","","kowshik","2021-03-25T07:52:24Z","2021-03-30T16:51:50Z"
"","10430","KAFKA-12575: Eliminate Log.isLogDirOffline boolean attribute","**This PR is a precursor to the recovery logic refactor work ([KAFKA-12553](https://issues.apache.org/jira/browse/KAFKA-12553)).**  I have made a change to eliminate `Log.isLogDirOffline` attribute. This boolean also comes in the way of refactoring the recovery logic. This attribute was added in https://github.com/apache/kafka/pull/9676. But it is redundant and can be eliminated in favor of looking up `LogDirFailureChannel` to check if the `logDir` is offline. The performance/latency implication of such a `ConcurrentHashMap` lookup inside `LogDirFailureChannel` should be very low given that `ConcurrentHashMap` reads are usually lock free.  **Tests:** Relying on existing unit/integration tests.","closed","","kowshik","2021-03-29T22:42:04Z","2021-05-13T23:49:33Z"
"","10091","KAFKA-9524: increase retention time for window and grace periods longer than one day","**Reproducing:** The bug can be easily reproduced for any TimeWindow where window + grace period > 1 day. Changing any test in TimeWindowedKStreamImplTest.java for this condition will reproduce the bug.  **Cause:** The root cause is that .grace(...) never updates the default maintainDurationMs field value. The value is thus always 1 day - throwing the IllegalArgumentException when validating it at a later stage.  **Implementation comments:** I believe that a minimum retention period of 1 day is desired because of log compaction - thus I used the Math.Max(window+grace, default maintainDurationMs-1 day-) to calculate the minimum retention period.  I am a bit unsure about the test - currently it indirectly tests the bug. The bug would throw a IllegalArgumentException, preventing the test scenario from working with any aggregation (count, reduce, etc). I've implemented the test with count operation to ensure consistency of the window behaviour. If this is not required, maybe just calling processData() and asserting that the original reported exception is not thrown should be enough.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","MarcoLotz","2021-02-09T20:26:41Z","2021-02-19T02:51:15Z"
"","10375","KAFKA-12522: Cast SMT should allow null value records to pass through","**Problem** The [current Cast SMT](https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java) fails on a null record value (or a null record key), which is problematic for tombstone records. When a tombstone record reaches the transformation the error below is thrown:  With schema: ``` Cannot list fields on non-struct type org.apache.kafka.connect.errors.DataException: Cannot list fields on non-struct type 	at org.apache.kafka.connect.data.ConnectSchema.fields(ConnectSchema.java:179) 	at org.apache.kafka.connect.transforms.Cast.getOrBuildSchema(Cast.java:190) ```  For schemaless: ``` Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [cast types], found: null at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38) ```  **Solution** Null value records should instead be allowed to pass through as there is no cast transformation to be done, with the benefit of allowing the connector to handle the tombstone records as intended.   **Testing** Added SMT unit tests to verify the records pass through when the keys or values are null.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","dosvath","2021-03-23T00:06:49Z","2021-05-21T10:34:45Z"
"","10377","KAFKA-12515 ApiVersionManager should create response based on request version","**Goal** Make supported and finalized features ignorable to achieve backward compatibility for ApiVersionsRequest(version < 3).  **Backgroud** Please check the Jira description: https://issues.apache.org/jira/browse/KAFKA-12515  **Test proof** Currently, the feature versioning system is not used, so the backward compatibility issue is not coming up. When for example the feature versioning system is being used, for example, backward compatibility issue will come up and this patch can fix that.   ``` // kafka.server.BrokerFeatures.scala  def createDefault(): BrokerFeatures = {     val supportedFeatures =     Map[String, SupportedVersionRange](""feature_1"" -> new SupportedVersionRange(1, 4))     new BrokerFeatures(Features.supportedFeatures(supportedFeatures.asJava))   } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","feyman2016","2021-03-23T04:01:52Z","2021-06-21T18:19:05Z"
"","10042","KAFKA-9527: fix NPE when using time-based argument for Stream Resetter","**Description:** A NPE is happening when using the StreamResetter tool with the arguments ""to-timedate"" and ""by-duration"" on empty partitions.  **Cause:** This happens because [fetcher](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L516) returns a null value on the map for empty partitions.  **Solution:** The null values needs to be correctly handled with Optional. In the case of empty partition, consumer.seek() should not be called - letting it fallback to the ""auto.offset.reset"" behaviour. The user should be notified by it.  **Implementation comments:**  - System.out.println was used instead of log.warn - since the whole class uses syso for interacting with the user. - There was no test for empty partition on reset to offset [streamsResetter.resetOffsetsTo()]. Test was added. - Added test for empty partition when resetToDatetime() is called - that was the cause of the bug. - MockConsumer doesn't have an implementation of offsetsForTimes() method - it throws an exception when called, breaching Liskov Substitution Principle. This method is used by resetToDatetime(). Since I couldn't find Mockito in the project dependencies to make a Spy, I decided to extends the MockConsumer class as a nested class and especialize the method. - Looking at Gradle, I saw that the min. java version is 1.8 - thus is decided to use 1.8 implementation of Optional instead of 1.9 - which would be a lot cleaner specially for the Optional.ProcessOrElse().    ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","tools,","MarcoLotz","2021-02-03T18:52:48Z","2021-04-12T16:29:12Z"
"","9843","MINOR: revise error message from TransactionalRequestResult#await","**before** ``` Timeout expired after 1000milliseconds while awaiting InitProducerId ```  **after** ``` Timeout expired after 1000 milliseconds while awaiting InitProducerId ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-01-07T15:32:27Z","2021-01-08T04:07:50Z"
"","10027","KAFKA-12270: Handle race condition when Connect tasks attempt to create topic","**_This is option 1 for the fix, which is simpler but less ideal because it results in an extra admin client call. See #10032 for option 2 that is a bit more complicated but results in no extra admin client calls._**  When a source connector is configured to create missing topics has multiple tasks that generate records for the same topic, it is possible that multiple tasks may simultaneously describe the topic, find it does not exist, and attempt to create the task. One of those create topic requests will succeed, and the other concurrent tasks will receive the response from the topic admin as having not created the task and will fail unnecessarily.  This change corrects the logic after the create topic method returns false (signifying that it did not create the task) to check again whether the topic exists (i.e., was created since the worker task last described the topic) and to continue if it does exist and fail only if the topic did not exist after the second check. This is not as efficient as would be ideal, but this ensures that tasks do not fail in this race condition and that tasks will continue to fail if the topic could not be created and did not exist after the create topic request did not create the topic.  Modified one unit test that mocks the behavior, and added a new test with the new behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-02-02T23:02:36Z","2021-02-03T17:53:26Z"
"","10238","KAFKA-10340: Backport proactively close producer when cancelling source tasks","**_NOTE: This is targeted to the `2.8` branch (currently frozen), but includes the same commit cherry-picked from `trunk` and originating in #10016 for [KAFKA-10340](https://issues.apache.org/jira/browse/KAFKA-10340)._**  **_This should be merged only after this issue is approved as a blocker for 2.8.0, or after 2.8.0 is released._** When it is merged, add ""2.8.0"" or ""2.8.1"" to the fix version for [KAFKA-10340](https://issues.apache.org/jira/browse/KAFKA-10340).  Close the producer in `WorkerSourceTask` when the latter is cancelled. If the broker do not autocreate the topic, and the connector is not configured to create topics written by the source connector, then the `WorkerSourceTask` main thread will block forever until the topic is created, and will not stop if cancelled or scheduled for shutdown by the worker.  Expanded an existing unit test for the WorkerSourceTask class to ensure that the producer is closed when the task is abandoned, and added a new integration test that guarantees that tasks are still shut down even when their producers are trying to write to topics that do not exist.","closed","connect,","rhauch","2021-03-01T16:37:54Z","2021-05-05T18:07:08Z"
"","10336","KAFKA-12483: Enable client overrides in connector configs by default (KIP-722)","**[KIP-722](https://cwiki.apache.org/confluence/display/KAFKA/KIP-722%3A+Enable+connector+client+overrides+by+default) has been approved.**  Changes the default value for the `connector.client.config.override.policy` worker configuration property from `None` to `All`. Modified unit tests to verify all policies still work, and that by default connectors can override all client policies.  See https://cwiki.apache.org/confluence/display/KAFKA/KIP-722%3A+Enable+connector+client+overrides+by+default  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","rhauch","2021-03-16T22:03:58Z","2021-06-22T14:12:11Z"
"","10335","KAFKA-12484: Enable Connect's connector log contexts by default (KIP-721)","**[KIP-721](https://cwiki.apache.org/confluence/display/KAFKA/KIP-721%3A+Enable+connector+log+contexts+in+Connect+Log4j+configuration) has been approved.**  Change the `connect-log4j.properties` file to use the connector log context by default. This feature was previously added in KIP-449, but was not enabled by default.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","rhauch","2021-03-16T21:47:01Z","2021-06-22T14:11:06Z"
"","9874","KAFKA-7340: Migrate clients module to JUnit 5","* Use the packages/classes from the new version * Move description in assert methods to last parameter * Convert parameterized tests so that they work with JUnit 5 * Remove hamcrest, it didn't seem to add much value * Fix Utils.mkEntry to have correct `equals` implementation * Add a missing @Test annotation in `SslSelectorTest` override * Adjust regex in `SaslAuthenticatorTest` due to small change in the assert failure string in JUnit 5  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-12T19:56:07Z","2022-05-05T23:51:01Z"
"","9748","MINOR: Simplify ApiKeys by relying on ApiMessageType","* The naming for `ListOffsets` was inconsistent, in some places it was `ListOffset` and in others it was `ListOffsets`. Picked the latter since it was used in metrics and the protocol documentation and made it consistent. * Removed unused methods in ApiKeys. * Deleted `CommonFields`. * Added `lowestSupportedVersion` and `highestSupportedVersion` to `ApiMessageType` * Removed tests in `MessageTest` that are no longer relevant.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-12-14T15:42:17Z","2020-12-16T14:33:14Z"
"","9957","MINOR: Update zstd-jni to 1.4.8-2","* The latest version zstd-jni doesn't use `RecyclingBufferPool` by default, so we pass it via the relevant constructors to maintain the behavior before this change. * zstd-jni fixes an issue when using Alpine, see https://github.com/luben/zstd-jni/issues/157. * zstd 1.4.7 includes several months of improvements across many axis, from performance to various fixes. Details: https://github.com/facebook/zstd/releases/tag/v1.4.7 * zstd 1.4.8 is a hotfix release, details: https://github.com/facebook/zstd/releases/tag/v1.4.8  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-24T20:48:07Z","2021-01-25T04:21:00Z"
"","10473","KAFKA-12614: Use Jenkinsfile for trunk and release branch builds","* Run all JDK/Scala version combinations for trunk/release branch builds. * Only retry failures in PR builds for now (we can remove this distinction if/when we report flaky failures as described in KAFKA-12216). * Disable concurrent builds * Send email to dev list on build failure * Use triple double quotes in `doValidation` since we use string interpolation for `SCALA_VERSION`. * Update release.py to output new `Unit/integration tests` Jenkins link  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-04T16:54:35Z","2021-04-05T15:46:20Z"
"","10096","KAFKA-12268: Make early poll return opt-in","* Revert the default Consumer#poll behavior back to early return on records only * Add config to enable early return on metadata or records * Set the return-on-metadata config in Streams to support KIP-695 * Revert the undesired addition of Hamcrest to the Client module * Revert the now unnecessary poll-until-records logic in PlaintextConsumerTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","vvcephei","2021-02-10T05:05:14Z","2021-03-02T14:21:17Z"
"","9484","MINOR: Update raft/README.md and minor RaftConfig tweaks","* Replace quorum.bootstrap.servers and quorum.bootstrap.voters with quorum.voters. * Remove seemingly unused `verbose` config. * Use constant to avoid unnecessary repeated concatenation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-10-22T21:52:40Z","2020-10-23T00:51:06Z"
"","10196","KAFKA-9203: Check for buggy LZ4 libraries and remove corresponding workarounds","* Remove the workarounds that were added back in https://github.com/apache/kafka/pull/7769 * Add a check to detect buggy LZ4 library versions  This check allows us to safely remove the workarounds for buggy LZ4 versions without users encountering cryptic errors if they accidentally have an older LZ4 library on the classpath, as described in KAFKA-9203.  With this change the use will get a clear error message indicating what the problem might be if they encounter this situation.  Note: This now instantiates a compressor in the decompression code. This should be safe with respect to JNI libraries, since we always use `LZ4Factory.fastestInstance()` which takes care of falling back to a pure Java implementation if JNI libraries are not present.  This was tested with lz4 1.3.0 to make sure it triggers the exception when running `KafkaLZ4Test`","closed","","xvrl","2021-02-23T19:03:06Z","2021-07-23T21:32:41Z"
"","10438","KAFKA-12579: Remove various deprecated clients classes/methods for 3.0","* Remove `ExtendedSerializer` and `ExtendedDeserializer`, deprecated since 2.1. The extra functionality was also made available in `Serializer` and `Deserializer`. * Remove `close(long, TimeUnit)` from the producer, consumer and admin client, deprecated since 2.0 for the consumer and 2.2 for the rest. The replacement is `close(Duration)`. * Remove `ConsumerConfig.addDeserializerToConfig` and `ProducerConfig.addSerializerToConfig`, deprecated since 2.7 with no replacement. These methods were not intended to be public API and are likely not used much (if at all). * Remove `NoOffsetForPartitionException.partition()`, deprecated since 0.11. `partitions()` should be used instead. * Remove `MessageFormatter.init(Properties)`, deprecated since 2.7. The `configure(Map)` method should be used instead. * Remove `kafka.common.MessageFormatter`, deprecated since 2.7. `org.apache.kafka.common.MessageFormatter` should be used instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-30T09:20:13Z","2021-04-06T15:54:57Z"
"","10263","KAFKA-12393: Document multi-tenancy considerations","* KAFKA-12393: Document multi-tenancy considerations * Addressed review feedback by @dajac and @rajinisivaram  Ported from https://github.com/apache/kafka-site/pull/334","closed","","miguno","2021-03-04T15:24:28Z","2021-03-04T15:51:24Z"
"","9866","KAFKA-12180: Implement the KIP-631 message generator changes","* Implement the uint16 type * Implement MetadataRecordType and MetadataJsonConverters","closed","kip-500,","cmccabe","2021-01-11T22:41:26Z","2021-01-12T20:46:08Z"
"","10226","MINOR: fix kafka-metadata-shell.sh","* Fix CLASSPATH issues in the startup script  * Fix overly verbose log messages during loading  * Update to use the new MetadataRecordSerde (this is needed now that we   have a frame version)  * Fix initialization","closed","","cmccabe","2021-02-27T00:44:35Z","2021-02-27T01:13:21Z"
"","9738","KAFKA-8744: Update Scala API to give names to processors","* As it's only API extension to match the java API with Named object with lots of duplication, I only tested the logic once.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mdespriee","2020-12-12T20:35:02Z","2021-01-28T17:11:59Z"
"","10137","KAFKA-12268: Implement task idling semantics via currentLag API","* Adds `Consumer#currentLag()`. * Updates the Streams task idling feature to use the new API. * Implements KIP-695 * Reverts a previous behavior change to Consumer.poll and replaces it with a new Consumer.currentLag API, which returns the client's currently cached lag. The reverted PR is #9836 . The reverted Jira is KAFKA-10866 . * Ports 2.8's #10119 to trunk. The 2.8 PR had to revert both KAFKA-10866 and KAFKA-10867, but this PR needed to revert KAFKA-10866 and adapt KAFKA-10867.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","vvcephei","2021-02-17T05:07:47Z","2021-03-02T14:20:53Z"
"","9538","MINOR: Fix group_mode_transactions_test","### What #9099 changed the format of console consumer output to  `Partition:$PARTITION\t$VALUE` whereas previously the output format was `$VALUE\t$PARTITION` This PR updates the message verifier to accommodate the updated console consumer output format.  After this change, the group transaction tests pass locally.  this should be cherry-picked to 2.7 as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-10-31T01:14:11Z","2020-10-31T12:51:04Z"
"","10140","MINOR: remove unused helper methods from utils classes","### removed methods  1. `CollectionUtils#subtractMap` 1. `CollectionUtils#groupPartitionDataByTopic` 1. `Utils#mkObjectProperties` 1. `Utils#safe` 1. `Utils#concatListsUnmodifiable` 1. `Utils#transformMap` 1. `Utils#UncheckedCloseable` 1. `OAuthBearerExtensionsValidatorCallback#ignoredExtensions` (this method is used by testing only and it can be replaced by other method. `CollectionUtils#subtractMap` is used by only this method)  ### removed classes 1. `CollectionUtilsTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-02-17T08:34:59Z","2021-03-04T10:23:08Z"
"","10117","MINOR: Fix Javadoc strings in Serdes.java","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)  ping @guozhangwang","closed","","hongshaoyang","2021-02-12T04:30:32Z","2021-02-14T18:19:33Z"
"","10423","MINOR website quickstart, fix typo","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","docs,","Alee4738","2021-03-29T01:05:58Z","2021-04-09T19:15:10Z"
"","9996","KAFKA-12249: KIP-500 - Add client-side Decommission Broker RPC","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","kip-500,","aloknnikhil","2021-01-29T02:23:46Z","2021-02-03T16:41:10Z"
"","9994","KAFKA-12248: Add BrokerHeartbeat/BrokerRegistration RPCs for KIP-500","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","kip-500,","aloknnikhil","2021-01-28T23:56:53Z","2021-02-02T06:35:18Z"
"","9943","MINOR: Fix typo in Utils#toPositive","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","hongshaoyang","2021-01-21T03:21:44Z","2021-01-25T07:54:25Z"
"","10449","MINOR: Fix docs for end-to-end record latency metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-03-31T12:39:31Z","2021-04-01T11:55:29Z"
"","10415","KAFKA-12790: Fix SslTransportLayerTest.testUnsupportedTlsVersion with JDK 16","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-26T13:39:25Z","2022-02-08T23:46:12Z"
"","10412","KAFKA-12562: Remove deprecated APIs in KafkaStreams and returned state classes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2021-03-26T05:36:02Z","2021-03-28T19:20:31Z"
"","10380","KAFKA-12527: Remove deprecated PartitionGrouper annotation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-03-23T07:38:59Z","2021-04-07T20:57:15Z"
"","10379","KAFKA-12524: Remove deprecated segments()","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-03-23T06:47:36Z","2021-03-24T04:05:43Z"
"","10319","MINOR; Various code cleanups","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-03-15T14:57:59Z","2021-03-16T16:00:07Z"
"","10305","KAFKA-10357: Add missing repartition topic validation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-03-11T18:07:16Z","2021-03-12T16:59:41Z"
"","10273","MINOR: Include number of members in group coordinator messages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-03-06T00:34:31Z","2021-03-09T13:49:33Z"
"","10205","KAFKA-12323 Follow-up: Refactor the unit test a bit","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","guozhangwang","2021-02-24T20:52:11Z","2021-03-01T20:12:16Z"
"","10202","KAFKA-12392: Deprecate the batch-size option in console producer","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-02-24T12:55:19Z","2022-03-06T09:13:28Z"
"","10170","KAFKA-12323: Set timestamp in record context when punctuate","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2021-02-21T07:17:27Z","2021-02-24T18:33:18Z"
"","10129","KAFKA-10817; Add clusterId validation to Fetch handling","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-02-16T10:22:45Z","2021-03-04T02:51:54Z"
"","10127","MINOR: Remove unused LeaderAndIsrResponse.partitions() since it has been replaced with partitionErrors()","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-02-15T09:31:22Z","2021-02-19T17:51:50Z"
"","10126","MINOR: Fix wrong Import control declaration","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-02-15T05:06:58Z","2021-08-04T07:08:08Z"
"","10099","MINOR: Updated the package name to correct one.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-02-10T16:24:44Z","2021-04-30T11:43:53Z"
"","10035","MINOR: Remove unused parameters in functions.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-02-03T04:27:56Z","2021-02-10T04:20:30Z"
"","9990","KAFKA-12235: Fix ZkAdminManager.describeConfigs on 2+ config keys","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ivanyu","2021-01-28T14:35:15Z","2021-02-27T05:34:58Z"
"","9897","MINOR: Remove incorrect comment in metadata module build","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-01-14T18:16:26Z","2021-01-15T13:40:52Z"
"","9895","KAFKA-9924: Add docs for RocksDB properties-based metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-01-14T17:14:18Z","2021-01-20T18:36:50Z"
"","9873","MINOR: Fix flaky test shouldQuerySpecificActivePartitionStores","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-01-12T12:58:04Z","2021-01-15T17:35:41Z"
"","9861","MINOR: Modify unnecessary access specifiers","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2021-01-11T12:52:25Z","2021-04-20T05:58:15Z"
"","9739","KAFKA-10636 Bypass log validation for writes to raft log","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","feyman2016","2020-12-13T07:15:12Z","2021-02-02T06:58:23Z"
"","9681","MINOR: Fix flaky test shouldQueryOnlyActivePartitionStoresByDefault","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-12-03T15:03:09Z","2021-01-15T17:34:10Z"
"","9678","KAFKA-10798; Ensure response is delayed for failed SASL authentication with connection close delay","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-12-02T20:05:06Z","2020-12-07T16:12:19Z"
"","9656","repro demonstrating non-owner commit","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2020-11-25T17:24:32Z","2020-11-25T18:08:36Z"
"","9486","KAFKA-9381: Fix publishing valid scaladoc for streams-scala","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-10-23T17:28:01Z","2020-10-27T00:46:45Z"
"","9954","MINOR: apply FilterByKeyIterator and FlattenedIterator to code base","### Changes  1. add public ```FilterByKeyIterator``` to replace private ```FilterByKeyIterator``` in ```RecordHeaders``` and ```ConnectHeaders``` 1. apply ```FlattenedIterator``` to code base 1. add benchmarks for ```FlattenedIterator``` and ```FilterByKeyIterator``` to prove those implementations are faster than java stream  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-01-23T19:56:42Z","2021-01-25T05:17:13Z"
"","9758","MINOR: remove FetchResponse.AbortedTransaction and redundant construc…","## Changes 1. rename ```INVALID_HIGHWATERMARK```  to ```INVALID_HIGH_WATERMARK``` 1. replace ```FetchResponse.AbortedTransaction``` by ```FetchResponseData.AbortedTransaction``` 1. remove redundant constructors from ```FetchResponse.PartitionData``` 1. rename ```recordSet``` to ```records```   ## Performance Tests  loop **10** times and get average.  ### case 1:  @parametrize(acks=1, topic=TOPIC_REP_ONE)  diff: **-0.3994016685 %** - trunk: 64.847 MB/sec - patch: 64.588 MB/sec      ### case 2:  @parametrize(acks=-1, topic=TOPIC_REP_THREE)      diff: **+0.4917916785 %** - trunk: 28.264 MB/sec - patch: 28.403 MB/sec  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-12-16T08:52:35Z","2021-03-09T19:21:57Z"
"","10465","MINOR: add unit test for ControllerApis#createTopics","","closed","kip-500,","cmccabe","2021-04-02T17:37:00Z","2021-04-07T17:34:54Z"
"","10373","MINOR: Typo fixes","","open","","andygarfield","2021-03-22T18:49:32Z","2021-09-11T20:13:13Z"
"","10353","DO NOT MERGE: Add a failing unit test to check Gradle and Github behavior","","closed","","mumrah","2021-03-18T18:28:25Z","2021-03-19T14:39:45Z"
"","10352","MINOR: Properly use language-java to highlight Java code","","open","","bsolomon1124","2021-03-18T16:30:32Z","2021-03-19T18:45:49Z"
"","10262","MINOR: Add missing log argument","","closed","","tombentley","2021-03-04T14:41:50Z","2021-03-09T13:56:42Z"
"","10227","KAFKA-12382: add a README for KIP-500","","closed","kip-500,","cmccabe","2021-02-27T00:46:25Z","2021-03-18T19:08:21Z"
"","9931","MINOR: MessageUtil: remove some deadcode","","closed","","cmccabe","2021-01-19T17:49:08Z","2021-01-25T07:42:46Z"
"","9929","MINOR: Pre-KIP-697, log warning about protocol in address configs","","open","","tombentley","2021-01-19T12:08:42Z","2021-01-19T12:08:42Z"
"","9914","KAFKA-12214: Generated code does not include UUID or struct fields in toString()","","closed","kip-500,","cmccabe","2021-01-15T21:46:33Z","2021-01-16T00:32:56Z"
"","9901","KAFKA-12209: Add the timeline data structures for the KIP-631 controller","","closed","kip-500,","cmccabe","2021-01-14T22:37:02Z","2021-02-03T08:28:03Z"
"","9900","KAFKA-12208: Rename AdminManager to ZkAdminManager","","closed","kip-500,","cmccabe","2021-01-14T22:26:20Z","2021-01-15T20:56:09Z"
"","9896","KAFKA-12206: o.a.k.common.Uuid should implement Comparable","","closed","","cmccabe","2021-01-14T18:07:36Z","2021-01-15T13:41:27Z"
"","9829","MINOR: improve KafkaStreams replication factor documentation","","closed","docs,","mjsax","2021-01-05T21:38:38Z","2021-01-06T19:31:09Z"
"","9759","HOTFIX: Access apiversions data via method not field","","closed","","tombentley","2020-12-16T14:08:28Z","2020-12-16T14:22:09Z"
"","9701","KAFKA-10713: (redux) Also allow underscore in protocol name","","closed","","tombentley","2020-12-05T07:31:50Z","2020-12-09T18:12:08Z"
"","9683","MINOR: Fix KTable-KTable foreign-key join example","","closed","docs,","JimGalasyn","2020-12-03T19:51:20Z","2020-12-03T20:19:30Z"
"","9676","KAFKA-10778: Fence appends after write failure","","closed","","tombentley","2020-12-02T15:48:24Z","2021-01-06T18:06:57Z"
"","9657","MINOR: Remove erroneous extra  in design doc","","closed","docs,","tombentley","2020-11-26T14:08:22Z","2020-11-30T20:45:53Z"
"","9593","KAFKA-10713: Stricter protocol parsing in hostnames","","closed","","tombentley","2020-11-13T09:43:17Z","2020-12-05T07:33:42Z"
"","9557","Kip 500 move legacy","","closed","","cmccabe","2020-11-04T18:35:44Z","2020-11-04T18:36:27Z"