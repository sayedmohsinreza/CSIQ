"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","5468","KAFKA-7250: fix transform function in scala DSL to accept TranformerS…","…upplier  Restructuring scala DSL transform function to accept TransformerSupplier instead of a single instance of Transformer that was shared across tasks.  Updated scaladoc.  Added a unit test to ensure created topology corresponds to equivalent java definition.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mdziemianko","2018-08-06T23:00:32Z","2018-08-07T15:02:47Z"
"","5213","Update InternalTopologyBuilder to throw TopologyException if a proces…","…sor or sink is added with no upstream node attached  https://issues.apache.org/jira/browse/KAFKA-7055: This change checks to ensure that processors and sinks have at least one parent node when attached to a Streams topology, as records cannot be forwarded to unconnected nodes.","closed","","nixsticks","2018-06-13T16:55:56Z","2018-06-13T16:57:45Z"
"","5415","KAFKA-7134: KafkaLog4jAppender - Appender exceptions are propagated t…","…o caller  Handling ignoreExceptions property in KafkaLog4jAppender  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","akatona84","2018-07-23T14:45:29Z","2018-08-30T09:13:03Z"
"","5356","KAFKA-7141: ConsumerGroupCommand should describe group assignment eve…","…n with no offsets  committed.  https://issues.apache.org/jira/browse/KAFKA-7141  Currently, if a consumer group never commits offsets, ConsumerGroupCommand cannot describe it at all even if the member assignment is valid. Instead, the tool should be able to describe the group information showing empty current_offset and LAG.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-07-11T06:18:26Z","2018-07-20T15:52:21Z"
"","5446","WIP: (DO NOT MERGE) Extract functionality from ReplicaManager into separate classes to re…","…move circular dependencies  Product code builds and ReplicaManager is no longer part of any circular dependencies. Test code has not yet been updated to reflect these changes.","open","","bob-barrett","2018-08-02T05:41:12Z","2018-08-02T05:41:12Z"
"","5074","KAFKA-6937: In-sync replica delayed during fetch if replica throttle …","…is exceeded  * Added a unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","junrao","2018-05-23T21:39:47Z","2018-05-25T23:30:18Z"
"","4496","KAFKA-6312: Update website documentation for --reset-offsets option, …","…for Kafka consumer groups, introduced in KIP-122  KIP-122 added the ability for kafka-consumer-groups.sh to reset/change consumer offsets, at a fine grained level.  There is documentation on it in the kafka-consumer-groups.sh usage text.  There was no such documentation on the kafka.apache.org website. This change updates the documentation on the website, so that users can read about the functionality without having the tools installed.  @omkreddy , @guozhangwang , @ijuma kindly review this PR. This is my first first contribution to any project ever. Please let me know if I have missed something.","closed","","tankhiwale","2018-01-31T15:25:22Z","2018-02-05T20:15:15Z"
"","4589","KAFKA-6332: Kafka system tests should use nc instead of log grep to d…","…etect start-up  - Extracted a new function (listening) in system test utils module to test whether a specified port on a specified node is listening for connections. - Refactored system tests to use the new function to test whether a particular server is started / listening on a port (instead of grepping for lines in server logs).  - Specified kdc.port for the MiniKdc server in minikdc.properties so that the server does not listen for connections on a ""random"" port. - Fixed a typo in the documentation of the node_is_reachable function in utils/util.py  Testing done: - Executed the following system tests: sanity_checks tests,  simple_consumer_shell_test.py, consumer_group_command_test.py, trogdor_test.py, zookeeper_security_upgrade_test.py  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asasvari","2018-02-19T10:30:45Z","2020-03-01T08:23:48Z"
"","4580","KAFKA-6568; The log cleaner should check the partition state before r…","…emoving it from the inProgress map.  The log cleaner should not naively remove the partition from in progress map without checking the partition state. This may cause the other thread calling `LogCleanerManager.abortAndPauseCleaning()` to hang in definitely.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","becketqin","2018-02-16T03:24:53Z","2018-02-17T03:27:05Z"
"","5247","KAFKA-5098: KafkaProducer.send() dose not block if topic name has ill…","…egal char and generates InvalidTopicException  If config parameter max.block.ms config parameter is set to a non-zero value, KafkaProducer.send() blocks for the max.block.ms time if topic name has illegal char or is invalid.  Wrote a unit test that verifies the appropriate exception is returned when performing a get on the returned future by KafkaProducer.send().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ahmedha","2018-06-18T19:04:40Z","2018-07-18T05:32:12Z"
"","5209","KAFKA-5098: KafkaProducer.send() dose not block if topic name has illegal char an…","…d generates InvalidTopicException  *KafkaProducer.send() dose not block if topic name has illegal char or invalid.  The producer caches the invalid topic name to avoid getting metadata from broker in the future.  The call generates an InvalidTopicException when invalid topic name is encountered.*  *Wrote a unit test that verifies the appropriate exception is passed to the Callback function passed to KafkaProducer.send(),  and the returned future has the correct exception message.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ahmedha","2018-06-13T07:18:21Z","2018-06-18T17:20:31Z"
"","4664","MINOR: Error messages and comments had a typo in a config name. The correct …","…config name is transaction.max.timeout.ms  I built the site docs and the javadocs, and verified that the docs have the right config name.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wushujames","2018-03-08T07:22:36Z","2018-03-09T19:50:07Z"
"","4777","Minor: Remove unnecessary calls to getInetAddress() and getPort() in …","…BlockingChannel  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Ran unit tests in SyncProducerTest, and they passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","attila-s","2018-03-26T22:30:30Z","2020-10-19T06:36:01Z"
"","5395","KAFKA-6960: Remove the methods from the internal Scala AdminClient th…","…at are provided by the new AdminClient  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  This is a follow-up task of KAFKA-6884.   Remove all the methods from the internal Scala ``AdminClient`` that are provided by the new AdminClient.  - deleted: ``deleteConsumerGroups, describeConsumerGroup, listGroups, listAllGroups,  listAllGroupsFlattened``,  - updated ``LegacyAdminClientTest``.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Deleted tests in ``LegacyAdminClientTest`` that validated the behaviour of the deleted methods in ``AdminClient.scala``.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-07-19T16:49:00Z","2020-10-21T07:50:11Z"
"","4703","MINOR: Remove kafka-consumer-offset-checker.bat as follow up to patch KAFKA-…","…3356  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hmcl","2018-03-13T19:13:29Z","2018-03-13T23:41:55Z"
"","4527","KAFKA-6312 Add documentation about kafka-consumer-groups.sh's ability…","… to set/change offsets  KIP-122 added the ability for kafka-consumer-groups.sh to reset/change consumer offsets, at a fine grained level.  There is documentation on it in the kafka-consumer-groups.sh usage text.  There is no such documentation on the kafka.apache.org website. We should add some documentation to the website, so that users can read about the functionality without having the tools installed.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","tools,","tankhiwale","2018-02-05T15:49:28Z","2018-08-20T20:27:45Z"
"","4564","KAFKA-6106; Postpone normal processing of tasks within a thread until…","… restoration of all tasks have completed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-02-13T09:31:31Z","2018-03-16T06:51:57Z"
"","5214","KAFKA-7055: Update InternalTopologyBuilder to throw TopologyException…","… if a processor or sink is added with no upstream node attached  InternalTopologyBuilder throws an exception if a sink or a processor is added without at least one upstream node, as records cannot be forwarded downstream to an unconnected node. This does not prevent users from attempting to forward to unconnected nodes, but it does prevent them from attaching effectively useless downstream nodes, and the error message for forwarding to an unconnected node has been updated to be slightly more specific.","closed","","nixsticks","2018-06-13T17:04:32Z","2018-06-13T17:49:59Z"
"","5318","MINOR: Close timing window in SimpleAclAuthorizer startup","ZooKeeper listener for change notifications should be created before loading the ACL cache to avoid timing window if acls are modified when broker is starting up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-02T11:37:33Z","2018-07-02T21:40:57Z"
"","5414","KAFKA-7193: Use ZooKeeper IP address in streams tests to avoid timeouts","ZooKeeper client from version 3.4.13 doesn't handle connections to `localhost` very well. If ZooKeeper is started on 127.0.0.1 on a machine that has both ipv4 and ipv6 and a client is created using `localhost` rather than the IP address in the connection string, ZooKeeper client attempts to connect to ipv4 or ipv6 randomly with a fixed one second backoff if connection fails. Use `127.0.0.1` instead of `localhost` in streams tests to avoid intermittent test failures due to ZK client connection timeouts if ipv6 is chosen in consecutive address selections. Also add note to upgrade docs for 2.0.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-23T12:53:02Z","2018-07-23T18:10:33Z"
"","4940","Upgrade ZooKeeper to 3.4.12 and Scala to 2.12.6","ZK 3.4.12 fixes the regression that forced us to go back to 3.4.10. Release notes:  https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310801&version=12342040  Scala 2.12.6 fixes the issue that prevented us from upgrading to 2.12.5.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-27T18:12:10Z","2018-05-03T16:25:37Z"
"","4876","Kafka-6792","Wrong pointer in the link for stream dsl.  actual is : http://kafka.apache.org/11/documentation/streams/developer-guide#streams_dsl correct is : http://kafka.apache.org/11/documentation/streams/developer-guide/dsl-api.html#streams-dsl","closed","","ro7m","2018-04-16T04:26:56Z","2018-04-16T17:50:39Z"
"","4715","MINOR: Allow users to specify 'if-not-exists' when creating topics while testing","Would like to be able to add the `--if-not-exists` flag when creating topics during setup in some of our tests; this small change adds that option to `KafkaService` in `services/kafka/kafka.py`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2018-03-14T23:00:19Z","2018-03-16T22:32:26Z"
"","4555","KAFKA-4651: improve test coverage of stores","Working on increasing the coverage of stores in unit tests.   Started with `InMemoryKeyValueLoggedStore`  ![screen shot 2018-02-09 at 1 15 11 pm](https://user-images.githubusercontent.com/199238/36044796-c7ddf31e-0da1-11e8-87a5-1d6727a5fe4d.png) ![screen shot 2018-02-09 at 1 24 38 pm](https://user-images.githubusercontent.com/199238/36044805-cd1bc23e-0da1-11e8-9b57-8f8bfa0a21ea.png)      ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-09T19:03:21Z","2018-02-20T20:47:02Z"
"","4677","Fix WorkerUtils#abort, add Utils#fullStackTrace","WorkerUtils#abort should complete the future with the full exception information, not just Throwable#getMessage, which could be null or empty.  Add the Utils#stringifyException utility function.","closed","","cmccabe","2018-03-09T23:47:51Z","2019-05-20T19:05:32Z"
"","4535","MINOR: Add missing imports to 'Hello Kafka Streams' examples","Without those imports examples does not compile.","closed","docs,","wojda","2018-02-06T22:21:48Z","2018-02-08T17:55:59Z"
"","5233","MINOR: Simplify replica fetcher (step one)","With the removal of `ConsumerFetchetThread`, we can start simpifying the fetcher thread code. This is a possible first step.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-14T22:53:38Z","2018-09-11T06:46:50Z"
"","5409","MINOR: increase dev version from 1.1.1-SNAPSHOT to 1.1.2-SNAPSHOT","With the release of `1.1.1` the `StreamsUpgradeTest.test_metadata_upgrade` and `StreamsUpgradeTest.test_metadata_downgrade` system tests were failing unable to find Kafka Streams version `1.1.1-SNAPSHOT`.  This PR updates `DEV_VERSION` in `version.py` to `1.1.2-SNAPSHOT`  Testing was completed by running the streams system tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-07-20T22:37:47Z","2018-07-22T02:39:19Z"
"","4550","MINOR: typos in KStream javadoc","with -> which","closed","docs,","MeneDev","2018-02-08T15:27:38Z","2018-02-13T02:34:00Z"
"","4945","MINOR: Use Scala's `-release` flag if possible (WIP)","WIP  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-28T04:07:15Z","2018-05-01T22:25:59Z"
"","5474","KAFKA-7080: pass segmentInterval to CachingWindowStore","WindowStoreBuilder incorrectly passes the number of segments instead of the segment interval.  In trunk, we made a larger change to fix this, but for backporting we simply compute the segment interval to pass.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-08-07T20:14:36Z","2018-08-08T21:00:42Z"
"","4908","MINOR: Removed deprecated schedule function","While working on this, I also refactored the MockProcessor out of the MockProcessorSupplier to cleanup the unit test paths.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-21T00:50:51Z","2020-04-24T23:55:05Z"
"","5453","MINOR: Follow up for KAFKA-6761 graph should add stores for consistency","While working on 4th PR, I noticed that I had missed adding stores via the graph vs. directly via the `InternalStreamsBuilder`.  Probably ok to do so, but we should be consistent.  For testing, I ran the existing tests in streams.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-08-02T21:24:36Z","2018-09-07T01:22:31Z"
"","4801","KAFKA-6704: InvalidStateStoreException from IQ when StreamThread closes store","While using an `iterator` from IQ, it's possible to get an `InvalidStateStoreException` if the `StreamThread` closes the store during a range query.  Added a unit test to `SegmentIteratorTest` for this condition.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-03-30T22:10:11Z","2018-06-05T20:28:34Z"
"","5040","KAFKA-6919: Fix for the location of the trogdor.sh executable file in the documentation","While trying to use trogdor, I ran into this.","closed","","KoenDG","2018-05-19T19:22:19Z","2018-06-06T08:30:04Z"
"","4792","MINOR: Remove unnecessary registerGlobalStateStores","While reviewing another PR I realize that registerGlobalStateStores is no longer needed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-28T21:38:02Z","2020-04-24T23:38:56Z"
"","5476","MINOR : kafka-examples, replaced deprecated KafkaConsumer.poll(long) method with KafkaConsumer.poll(Duration)","While looking at the kafka-examples code, found one of the deprecated method is used in kafka.examples.Consumer.java   The commit here replaces deprecated         **KafkaConsumer.poll(long)**  method with         **KafkaConsumer.poll(Duration)**  Author: Piyush Sagar","closed","","pas725","2018-08-08T04:03:26Z","2018-11-26T17:14:26Z"
"","5466","KAFKA-7158: Add unit test for window store range queries","While debugging the reported issue, I found that our current unit test lacks coverage to actually expose the underlying root cause.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-08-06T21:01:31Z","2018-08-08T21:02:50Z"
"","4717","KAFKA-6662: Consumer use offsetsForTimes() get offset return None.","When we use Consumer's method  offsetsForTimes()  to get the topic-partition offset, sometimes it will return null. Print the client log  ``` 2018-03-15 11:54:05,239] DEBUG Collector TraceCollector dispatcher loop interval 256 upload 0 retry 0 fail 0 (com.meituan.mtrace.collector.sg.AbstractCollector) [2018-03-15 11:54:05,241] DEBUG Set SASL client state to INITIAL (org.apache.kafka.common.security.authenticator.SaslClientAuthenticator) [2018-03-15 11:54:05,241] DEBUG Set SASL client state to INTERMEDIATE (org.apache.kafka.common.security.authenticator.SaslClientAuthenticator) [2018-03-15 11:54:05,247] DEBUG Set SASL client state to COMPLETE (org.apache.kafka.common.security.authenticator.SaslClientAuthenticator) [2018-03-15 11:54:05,247] DEBUG Initiating API versions fetch from node 53. (org.apache.kafka.clients.NetworkClient) [2018-03-15 11:54:05,253] DEBUG Recorded API versions for node 53: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0]) (org.apache.kafka.clients.NetworkClient) [2018-03-15 11:54:05,315] DEBUG Handling ListOffsetResponse response for org.matt_test2-0.  ``` Fetched offset -1, timestamp -1 (org.apache.kafka.clients.consumer.internals.Fetcher) From the log, we find broker return the offset, but it's value is -1, this value will be removed in Fetcher.handleListOffsetResponse()  ```java // Handle v1 and later response log.debug(""Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}"",         topicPartition, partitionData.offset, partitionData.timestamp); if (partitionData.offset != ListOffsetResponse.UNKNOWN_OFFSET) {     OffsetData offsetData = new OffsetData(partitionData.offset, partitionData.timestamp);     timestampOffsetMap.put(topicPartition, offsetData); } ```  We test several situations, and we found that in the following two cases it will return none.  1. The topic-partition msg number is 0, when we use offsetsForTimes() to get the offset, the offset will retuan -1; 2. The targetTime we use to find offset is larger than the partition active_segment's largestTimestamp, the offset will return -1;  If the offset is set -1, it will not be return to consumer client. I think in these situation, it should be return the latest offset, and it's also defined in kafka/core annotation.  ```scala /**  * Search the message offset based on timestamp.  * This method returns an option of TimestampOffset. The offset is the offset of the first message whose timestamp is  * greater than or equals to the target timestamp.  *  * If all the message in the segment have smaller timestamps, the returned offset will be last offset + 1 and the  * timestamp will be max timestamp in the segment.  *  * If all the messages in the segment have larger timestamps, or no message in the segment has a timestamp,  * the returned the offset will be the base offset of the segment and the timestamp will be Message.NoTimestamp.  *  * This methods only returns None when the log is not empty but we did not see any messages when scanning the log  * from the indexed position. This could happen if the log is truncated after we get the indexed position but  * before we scan the log from there. In this case we simply return None and the caller will need to check on  * the truncated log and maybe retry or even do the search on another log segment.  *  * @param timestamp The timestamp to search for.  * @return the timestamp and offset of the first message whose timestamp is larger than or equals to the  *         target timestamp. None will be returned if there is no such message.  */ def findOffsetByTimestamp(timestamp: Long): Option[TimestampOffset] = {   // Get the index entry with a timestamp less than or equal to the target timestamp   val timestampOffset = timeIndex.lookup(timestamp)   val position = index.lookup(timestampOffset.offset).position   // Search the timestamp   log.searchForTimestamp(timestamp, position) } ```","closed","","wangzzu","2018-03-15T06:36:01Z","2018-03-21T15:06:05Z"
"","4907","MINOR: Ensure exception messages include partition/segment info when possible","When we hit an exceptional case with an operation on a specific partition, it is always useful to know which partition the failed operation was being applied to. I did a quick pass through `Log` and several related components and attempted to improve the messages to ensure they include appropriate context.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-20T22:42:56Z","2018-04-30T15:59:05Z"
"","5116","MINOR: Make KStreamSessionWindowAggregate public.","When using the processor API it is useful to leverage existing Kafka streams processors. KStreamSessionWindowAggregate cannot be used with the processor API because it is package private. KStreamWindowAggregate is public however.  No tests were written as this is only a visibility change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cwildman","2018-06-01T18:45:27Z","2018-06-04T17:37:57Z"
"","5412","KAFKA-7152: Avoid moving a replica out of isr if its LEO equals leader's LEO","When there are many inactive partitions in the cluster, we observed constant churn of URP in the cluster even if follower can catch up with leader's byte-in-rate because leader broker frequently moves replicas of inactive partitions out of ISR. This PR mitigates this issue by not moving replica out of ISR if follower's LEO == leader's LEO.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-07-21T20:15:52Z","2018-07-25T06:25:55Z"
"","4607","added quotation around $JAVA nohup and exec call","When the `$JAVA` environment value contains spaces, the scripting context fails to interpret the command correctly.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lamchakchan","2018-02-21T16:22:44Z","2020-10-19T06:39:05Z"
"","5282","Minor: add exception to debug log for Sender#maybeSendTransactionalRequest","When looking at code related to RequestHandler (for KAFKA-7088), I found that the exception was not included in DEBUG log for Sender#maybeSendTransactionalRequest  This PR includes the exception in DEBUG log.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-06-22T22:50:48Z","2018-06-25T15:49:01Z"
"","4843","[MINOR] ConsumerGroupCommand does not print cause of failure","When kafka-consumer-groups.sh fails to execute a command (for example, because of a configuration problem), it does not print the cause of failure which could make it hard to debug the root cause. This patch includes a minor change to print the actual cause of the exception.  Example error output with the patch: ``` Error: Executing consumer group command failed (Failed to construct kafka consumer) org.apache.kafka.common.config.ConfigException: request.timeout.ms should be greater than session.timeout.ms and fetch.max.wait.ms ```","closed","","dhruvilshah3","2018-04-09T23:20:53Z","2018-05-24T22:45:37Z"
"","5286","MINOR: Print exception stack traces in ConsumerGroupCommand.","When an exception is caught in ConsumerGroupCommand command line utility it does not print the stack trace, this could lead to crucial information regarding the error to not be displayed to the user.  More specifically, when misconfiguring consumer configuration the user will only see the message: ``` Error: Executing consumer group command failed due to Failed to construct kafka consumer ``` This happens since [the code which instantiates this exception](https://github.com/apache/kafka/blob/2.0.0-rc0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L795) does not add all relevant information in the message but rather in the ""cause"" member.  In order to find out the issue I had to compile Kafka and debug this code to find out that the error was due to a misconfiguration of the consumer: ``` Caused by: org.apache.kafka.common.config.ConfigException: request.timeout.ms should be greater than session.timeout.ms and fetch.max.wait.ms ```  Suggested change is to simply print the stack traces since this is a command line utility (same as the error is printed in the line preceding this one). This will provide the user with the most information for cases like these when the exception message is not sufficient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aviemzur","2018-06-24T14:27:45Z","2018-07-12T06:00:56Z"
"","5448","KAFKA-7190: Retain producerIds when truncate log head to avoid UNKNOWN_PRODUCER_ID","When a streams application has little traffic, then it is possible that consumer purging would delete even the last message sent by a producer (i.e., all the messages sent by this producer have been consumed and committed), and as a result, the broker would delete that producer's ID. The next time when this producer tries to send, it will get this UNKNOWN_PRODUCER_ID error code.  This PR fix the above problem by delay the deletion of producer ID until it expires. more details and discussions can be found [KAFKA-7190](https://issues.apache.org/jira/browse/KAFKA-7190)  - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","lambdaliu","2018-08-02T11:53:30Z","2020-05-08T23:53:38Z"
"","5221","KAFKA-7019; Make reading metadata lock-free by maintaining an atomically-updated read snapshot","we've seen cases where under a constant metadata update load (automated partition rebalance) request handling threads can block for a significant amount of time on the metadata lock.  the reason is that for large enough clusters (~200,000 topic partitions) a read operation can actually take a long while to compose a response. under a constant stream of reads + writes we see situations where a reader is currently in, a writer is pending (blocked) and then a big pile-up of more readers that are blocked behind the pending writer.  this patch makes the read path lock-free. the metadata is now stored in a logically-immutable snapshot. all read operations grab a snapshot and serve data out of it. write paths create an entirely new snapshot and atomically assign it. writers are still under a lock, for mutual exclusion.  here's the benchmark code i used to measure the effects of this patch: ```java public class MetadataCacheBenchmark {     private volatile boolean running = true;      int numBrokers = 150;     int numTopics = 3500;     int maxPartitionsPerTopic = 100;     int replicationFactor = 2;     int numUpdaters = 1;     double updateRateLimit = 10.0; //qps     int numReaders = 5;     boolean partialUpdate = true;      private final ListenerName listener = new ListenerName(""listener"");      private final AtomicLong updateCounter = new AtomicLong();     private final AtomicLong readCounter = new AtomicLong();      @Test     public void benchmarkAllTheThings() throws Exception {         //long seed = System.currentTimeMillis();         long seed = 666;         System.err.println(""seed is "" + seed);         Random r = new Random(seed);          MetadataCache cache = new MetadataCache(666);         UpdateMetadataRequest fullRequest = buildRequest(r, -1);         UpdateMetadataRequest partialRequest = buildRequest(r, 1);          cache.updateCache(0, fullRequest); //initial data (useful in case there are no writers)          Set allTopics = new HashSet<>();         for (int i = 0; i < numTopics; i++) {             allTopics.add(""topic-"" + i);         }         scala.collection.mutable.Set topicsScalaSet = JavaConverters.asScalaSetConverter(allTopics).asScala();          Thread.UncaughtExceptionHandler exceptionHandler = new Thread.UncaughtExceptionHandler() {             @Override             public void uncaughtException(Thread t, Throwable e) {                 running = false;                 System.err.println(""thread "" + t + "" died"");                 e.printStackTrace(System.err);                 System.exit(1);             }         };         List threads = new ArrayList<>();          for (int i = 0; i < numUpdaters; i++) {             UpdateMetadataRequest req = partialUpdate ? partialRequest : fullRequest;              Runnable updaterRunnable;             if (updateRateLimit > 0) {                 updaterRunnable = new RateLimitedUpdateRunnable(updateRateLimit, cache, req);             } else {                 updaterRunnable = new UpdateRunnable(cache, req);             }             Thread updaterThread = new Thread(updaterRunnable, ""updater-"" + i);             updaterThread.setDaemon(true);             updaterThread.setUncaughtExceptionHandler(exceptionHandler);             threads.add(updaterThread);         }          for (int i = 0; i < numReaders; i++) {             ReadRunnable readRunnable = new ReadRunnable(cache, topicsScalaSet);             Thread readerThread = new Thread(readRunnable, ""reader-"" + i);             readerThread.setDaemon(true);             readerThread.setUncaughtExceptionHandler(exceptionHandler);             threads.add(readerThread);         }          for (Thread t : threads) {             t.start();         }          long prevTime = System.currentTimeMillis();         long prevUpdates = 0;         long prevReads = 0;          long now;         long updates;         long reads;          long timeDiff;         long updateDiff;         long readDiff;          double updateQps;         double readQps;          while (running) {             Thread.sleep(TimeUnit.SECONDS.toMillis(30));             now = System.currentTimeMillis();             updates = updateCounter.longValue();             reads = readCounter.longValue();              timeDiff = now - prevTime;             updateDiff = updates - prevUpdates;             readDiff = reads - prevReads;              updateQps = ((double) updateDiff * 1000) / timeDiff;             readQps = ((double) readDiff * 1000) / timeDiff;              prevTime = now;             prevUpdates = updates;             prevReads = reads;              System.err.println(""updates: "" + updateQps + "" / sec"");             System.err.println(""reads: "" + readQps + "" / sec"");         }     }      private UpdateMetadataRequest buildRequest(Random random, int numTopicsOverride) {         int controllerEpoch = 0;         int totalPartitions = 0;         Set liveBrokers = new HashSet<>();          for (int i = 0; i < numBrokers; i++) {             UpdateMetadataRequest.EndPoint endPoint =                 new UpdateMetadataRequest.EndPoint(""host-"" + i, 6666, SecurityProtocol.PLAINTEXT, listener);             UpdateMetadataRequest.Broker broker =                 new UpdateMetadataRequest.Broker(i, Collections.singletonList(endPoint), ""rack-"" + i);             liveBrokers.add(broker);         }          Map partitions = new HashMap<>();          int topicCount = numTopicsOverride > 0 ? numTopicsOverride : numTopics;          for (int i = 0; i < topicCount; i++) {             String topicName = ""topic-"" + i;             int numPartitions = 1 + random.nextInt(maxPartitionsPerTopic);             for (int j = 0; j < numPartitions; j++) {                 TopicPartition tp = new TopicPartition(topicName, j);                 List replicas = pick(replicationFactor, numBrokers, random);                 UpdateMetadataRequest.PartitionState state =                     new UpdateMetadataRequest.PartitionState(controllerEpoch, replicas.get(0), 0, replicas, 0, replicas,                         Collections.emptyList());                 partitions.put(tp, state);             }             totalPartitions += numPartitions;         }          UpdateMetadataRequest.Builder builder =             new UpdateMetadataRequest.Builder((short) 4, 0, controllerEpoch, partitions, liveBrokers);          UpdateMetadataRequest request = builder.build((short) 4);         System.err.println(""request has "" + totalPartitions + "" TPs total"");         return request;     }      private List pick(int howMany, int from, Random random) {         List result = new ArrayList<>(howMany);         while (result.size() < howMany) {             int chosen = random.nextInt(from); //exclusive             if (!result.contains(chosen)) {                 result.add(chosen);             }         }         return result;     }      private class UpdateRunnable implements Runnable {         private final MetadataCache cache;         private final UpdateMetadataRequest request;         private int counter = 0;          public UpdateRunnable(MetadataCache cache, UpdateMetadataRequest request) {             this.cache = cache;             this.request = request;         }          @Override         public void run() {             while (running) {                 cache.updateCache(counter++, request);                 updateCounter.incrementAndGet();             }         }     }      private class RateLimitedUpdateRunnable implements Runnable {         private final MetadataCache cache;         private final UpdateMetadataRequest request;         private int counter = 0;          private final double targetQps;         private final int intervalMillis;          public RateLimitedUpdateRunnable(double targetQps, MetadataCache cache, UpdateMetadataRequest request) {             this.cache = cache;             this.request = request;             this.targetQps = targetQps;             this.intervalMillis = (int) (1000.0 / targetQps);         }          @Override         public void run() {             while (running) {                 long start = System.currentTimeMillis();                 cache.updateCache(counter++, request);                 long end = System.currentTimeMillis();                 updateCounter.incrementAndGet();                 long took = end - start;                 long remaining = intervalMillis - took;                 if (remaining > 0) {                     try {                         Thread.sleep(remaining);                     } catch (Exception e) {                         e.printStackTrace(System.err);                     }                 }             }         }     }      private class ReadRunnable implements Runnable {         private final MetadataCache cache;         private final scala.collection.mutable.Set topics;          public ReadRunnable(MetadataCache cache, scala.collection.mutable.Set topics) {             this.cache = cache;             this.topics = topics;         }          @Override         public void run() {             while (running) {                 cache.getTopicMetadata(topics, listener, false);                 readCounter.incrementAndGet();             }         }     } } ```  the interesting/problematic scenario is a combination of writers and readers,","closed","performance,","radai-rosenblatt","2018-06-14T02:01:01Z","2019-03-07T22:07:09Z"
"","4540","KAFKA-6469 Batch ISR change notifications","We've failures in one of our test clusters as the partition count started to climb north of 60k per broker. We had brokers writing child nodes under /isr_change_notification that were larger than the jute.maxbuffer size in ZooKeeper (1MB), causing the ZooKeeper server to drop the broker's session, effectively bricking the cluster.      This can be mitigated by chunking ISR notifications to increase the maximum number of partitions a broker can host, which is the purpose of this patch.      ReplicaManager#maybePropagateIsrChanges() now batches the set of TopicPartitions into sets of at most 3200 entries. This ensures that the JSON payload is never more than around 912k at the worst case.      An integration test was added which propagates a set of 5000 TopicPartitions in two batches.      The dependency on System.getCurrentTimeMillis() was broken for testing purposes.","closed","performance,","ambroff","2018-02-07T18:02:18Z","2021-11-02T04:15:33Z"
"","4998","KAFKA-6896: Add producer metrics exporting in KafkaStreams.java","We would like to also export the producer metrics from `StreamThread` just like consumer metrics, so that we could gain more visibility of stream application. The approach is to pass in the `threadProducer` into the StreamThread so that we could export its metrics in dynamic.   Note that this is a pure internal change that doesn't require a KIP, and in the future we also want to export admin client metrics. A followup KIP for admin client will be created once this is merged.","closed","","abbccdda","2018-05-10T16:49:57Z","2018-05-18T21:20:51Z"
"","4755","KAFKA-6683; Ensure producer state not mutated prior to append","We were unintentionally mutating the cached queue of batches prior to appending to the log. This could have several bad consequences if the append ultimately failed or was truncated. In the reporter's case, it caused the snapshot to be invalid after a segment roll. The snapshot contained producer state at offsets higher than the snapshot offset. If we ever had to load from that snapshot, the state was left inconsistent, which led to an error that ultimately crashed the replica fetcher.  The fix required some refactoring to avoid sharing the same underlying queue inside `ProducerAppendInfo`. I have added test cases which reproduce the invalid snapshot state. I have also made an effort to clean up logging since it was not easy to track this problem down.  One final note: I have removed the duplicate check inside `ProducerStateManager` since it was both redundant and incorrect. The redundancy was in the checking of the cached batches: we already check these in `Log.analyzeAndValidateProducerState`. The incorrectness was the handling of sequence number overflow: we were only handling one very specific case of overflow, but others would have resulted in an invalid assertion. Instead, we now throw `OutOfOrderSequenceException`.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-22T07:53:36Z","2018-03-23T05:10:59Z"
"","5362","MINOR: resolve warnings in KStreamImpl","We shouldn't be ignoring compiler warnings, since they tell us when our code is likely incorrect.  Sadly, our build produces hundreds of warnings in Streams alone. I plan to just go down the list and resolve them every now and then.  This is the second installment: KStreamImpl.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-12T18:15:50Z","2018-07-30T20:32:45Z"
"","5363","MINOR: resolve warnings in GroupedStreamAggregateBuilder","We shouldn't be ignoring compiler warnings, since they tell us when our code is likely incorrect.  Sadly, our build produces hundreds of warnings in Streams alone. I plan to just go down the list and resolve them every now and then.  This is the second installment: GroupedStreamAggregateBuilder.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-12T18:18:12Z","2018-07-30T20:32:51Z"
"","5364","MINOR: resolve warnings in topology graph","We shouldn't be ignoring compiler warnings, since they tell us when our code is likely incorrect.  Sadly, our build produces hundreds of warnings in Streams alone. I plan to just go down the list and resolve them every now and then.  This is the fourth installment: OptimizableRepartitionNode, StatelessProcessorNode, and GroupedTableOperationRepartitionNode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-12T18:22:41Z","2018-07-30T20:32:57Z"
"","5360","MINOR: resolve warnings in Streams 1","We shouldn't be ignoring compiler warnings, since they tell us when our code is likely incorrect.  Sadly, our build produces hundreds of warnings in Streams alone. I plan to just go down the list and resolve them every now and then.  This is the first installment: RocksDBStore and KTableImpl.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-12T16:18:30Z","2018-07-30T20:32:40Z"
"","4722","MINOR KAFKA-3978 followup","We should use logStartOffset as HW offset if the current HW offset is out of range.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-16T00:26:12Z","2018-03-20T22:47:16Z"
"","4711","KAFKA-6656; Config tool should return non-zero status code on failure","We should propagate exceptions when altering or describing configs in order to ensure that the command fails with a non-zero status.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-14T18:39:41Z","2018-03-15T17:33:30Z"
"","4553","MINOR: UpdateMetadataRequest should be lazily created","We should defer to construct UpdateMetadataRequest Builder to the right time when we really need them.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","core,","huxihx","2018-02-09T11:51:37Z","2018-03-02T19:31:12Z"
"","5049","MINOR: Remove o.a.kafka.common.utils.Base64 and IS_JAVA8_COMPATIBLE","We no longer need them since we now require Java 8.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-21T04:42:48Z","2018-09-11T06:47:01Z"
"","5341","KAFKA-7136: Avoid deadlocks in synchronized metrics reporters","We need to use the same lock for metric update and read to avoid NPE and concurrent modification exceptions.  Sensor add/remove/update are synchronized on `Sensor` since they access lists and maps that are not thread-safe. Reporters are notified of metrics add/remove while holding (`Sensor`, `Metrics`) locks and reporters may synchronize on the reporter lock. Metric read may be invoked by metrics reporters while holding a reporter lock. So read/update cannot be synchronized using `Sensor` since that could lead to deadlock. This PR introduces a new lock in Sensor for update/read.  Locking order: ``` - Sensor#add: Sensor -> Metrics -> MetricsReporter - Metrics#removeSensor: Sensor -> Metrics -> MetricsReporter - KafkaMetric#metricValue: MetricsReporter -> Sensor#metricLock - Sensor#record: Sensor -> Sensor#metricLock ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-06T10:34:43Z","2018-07-06T18:10:39Z"
"","4641","KAFKA-6606; Ensure consumer awaits auto-commit interval after sending…","We need to reset the auto-commit deadline after sending the offset commit request so that we do not resend it while the request is still inflight.   Added unit tests ensuring this behavior and proper backoff in the case of a failure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-03T01:22:09Z","2018-03-03T21:27:30Z"
"","4895","MINOR: Fix kafka-run-class for Java 10","We need to match to the end of the line to make it work with Java 10 as explained in the expanded comment.  Tested manually for all supported versions:  ```shell echo $(.../jdk1.8.0_152.jdk/Contents/Home/bin/java -version 2>&1 | sed -E -n 's/.* version ""([0-9]*).*$/\1/p') 1  echo $(.../jdk-9.0.4.jdk/Contents/Home/bin/java -version 2>&1 | sed -E -n 's/.* version ""([0-9]*).*$/\1/p') 9  echo $(.../jdk-10.jdk/Contents/Home/bin/java -version 2>&1 | sed -E -n 's/.* version ""([0-9]*).*$/\1/p') 10  echo $(.../jdk-10.0.1.jdk/Contents/Home/bin/java -version 2>&1 | sed -E -n 's/.* version ""([0-9]*).*$/\1/p') 10 ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-18T23:13:41Z","2018-09-11T06:47:06Z"
"","5004","Update kafka-run-class.sh to work with paths containing spaces","we need to escape the JAVA binary variable to avoid issues with paths containing specials characters (eg: SPACE)  #### the contribution is my original work and I license the work to the project under the project's open source license  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tsopokis","2018-05-11T10:46:44Z","2018-05-11T13:46:50Z"
"","5308","MINOR: Ensure heartbeat last poll time always updated","We need to ensure that the last poll time is always updated when the user call `poll(Duration)`. This patch fixes a bug in the new KIP-266 timeout behavior which would cause this to be skipped if the coordinator could not be found while the consumer was in an active group.  Note that I've also fixed some type inconsistencies for various timeouts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-06-28T23:16:51Z","2018-06-29T03:46:20Z"
"","5173","[MINOR] Fix chunked down-conversion behavior when no valid batch exists after conversion","We might decide to drop certain message batches during down-conversion because older clients might not be able to interpret them. One such example is control batches which are typically removed by the broker if down-conversion to V0 or V1 is required. This patch makes sure the chunked down-conversion implementation is able to handle such cases.  Credit to @omkreddy for reporting this issue, providing logs and a reproducible test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-06-09T07:45:50Z","2018-06-15T06:00:34Z"
"","4586","KAFKA-6569: Move OffsetIndex/TimeIndex logger to companion object","We identified that we spend a lot of time in reflection when creating OffsetIndex, TimeIndex, or other implementations of AbstractIndex[K, V], because of the Logging mixin. When the broker is bootstrapping it's just doing this in a tight loop, so this time adds up.  This patch moves the logging to a companion objects, statically initializing the logger.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","performance,","ambroff","2018-02-18T22:42:19Z","2019-02-18T08:05:42Z"
"","5336","MINOR: Avoid conflicting versions of org.apache.mina for MiniKdc deps","We have updated `apacheda` to a newer version that uses `mina-core-2.0.18` while `apacheds` uses `mina-core-2.0.16`. Since these are used only for testing using `MiniKdc`, it would be better to use the versions from `apacheds`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-05T10:58:27Z","2018-07-05T16:07:54Z"
"","5015","[KAFKA-6899] Fix potential NPE when retrieving JAAS configuration","We have developed a static analysis tool [NPEDetector ](https://github.com/lujiefsi/NPEDetector) find some potential NPE. Our analysis shows that NPE reason can be simple:some callees may return null directly in corner case(e.g. node crash , IO exception), some of their callers have  !=null check but some do not have.   ### Bug:  callee JaasConfig#getAppConfigurationEntry  can return null, it has 13 callers, 11 of the callers have the null check while using the return value, one of them have no checker :  //caller:KerberosLogin#login() AppConfigurationEntry[] entries = configuration().getAppConfigurationEntry(contextName()); if (entries.length == 0)/may NPE  I am not sure whether it is an bug or not, please correct me without any without any hesitation.","closed","","lujiefsi","2018-05-13T11:06:30Z","2021-10-23T03:14:11Z"
"","4851","KAFKA-6773; Allow offset commit/fetch/describe with empty groupId","We had a regression in #4788 which caused the offset commit/fetch/describe APIs to fail if the groupId was empty. This should be allowed for backwards compatibility. I've added a test case to ensure that we do not miss this again in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-10T22:59:17Z","2018-04-11T23:47:12Z"
"","5044","KAFKA-6840: Add windowed-KTable API","We are proposing adding a new API called `windowedTable` to `StreamsBuilder.java` to bring in the ability of materializing a windowed topic to local. Links to related sources: KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-300%3A+Add+Windowed+KTable+API+in+StreamsBuilder Jira: https://issues.apache.org/jira/browse/KAFKA-6840","open","kip,","abbccdda","2018-05-20T02:03:35Z","2020-06-12T23:03:56Z"
"","4736","KAFKA-6473: Add MockProcessorContext to public test-utils","We are adding a public testing utility to make it easier to unit test Processor implementations.  See KIP-267 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-267%3A+Add+Processor+Unit+Test+Support+to+Kafka+Streams+Test+Utils).  The testing for this change is in this commit. There are behavioral and unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-03-20T00:28:57Z","2018-03-27T23:08:34Z"
"","5112","MINOR: Fix bug in AdminClient node reassignment following connection failure","We added logic to reassign nodes in `callToSend` after a connection failure, but we do not handle the case when there is no node currently available to reassign the request to. This can happen when using `MetadataUpdateNodeIdProvider` if all of the known nodes are blacked out awaiting the retry backoff. To fix this, we need to ensure that the call is added to `pendingCalls` if a new node cannot be found.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-31T23:46:06Z","2018-06-04T09:23:06Z"
"","4933","HOTFIX: ListConsumerGroupsResult should use KafkaFuture","We accidentally changed this #4884. The public API should use `KafkaFuture` and not `KafkaFutureImpl`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-26T06:43:48Z","2018-04-26T13:58:37Z"
"","4930","KAFKA-5697: issue Consumer#wakeup during Streams shutdown","Wakeup consumers during shutdown to break them out of any internally blocking calls.  Semantically, it should be fine to treat a WakeupException as ""no work to do"", which will then continue the threads' polling loops, leading them to discover that they are supposed to shut down, which they will do gracefully.  The existing tests should be sufficient to verify no regressions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-04-25T19:21:25Z","2018-05-07T23:05:29Z"
"","5256","MINOR: Fix timing issue in advertised listener update test","Wait for produce to fail before updating listener to avoid send succeeding after the listener update. Also use different topics in tests with connection failures where one is expected to fail and the other is expected to succeed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-20T15:35:47Z","2018-06-25T10:38:59Z"
"","4786","Minor: Enable VerifiableConsumer to decode bytes by different encoding","VerifiableConsumer doesn't config the StringDeserializer so it is impossible to change the encoding by specific configuration.  VerifiableConsumerMainTest expect the encoding in StringDeserializer is same with passed encoding.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-03-28T12:12:12Z","2018-11-06T06:25:17Z"
"","4487","Updated kafka-server start/stop scripts to use a pidfile","Using ps is sometimes not a good idea. Especially if there are multiple kafkas running.  This change implements using a pidfile (that can be changed through environment variables).  This will make things more robust and follow best practices.","closed","","AlexanderThaller","2018-01-30T11:05:32Z","2021-10-14T12:05:42Z"
"","4507","KAFKA-6367: StateRestoreListener use actual last restored offset for restored batch","Use the last actual restored offset for the `StoreRestoreListener.onBatchRestored` method.  Probably should update the docs to inform users this could include gaps in sequence due to commit markers.  Updated existing tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-01T20:29:03Z","2018-02-07T19:24:10Z"
"","4660","KAFKA-6530: Use actual first offset of message set when rolling log segment","Use the exact first offset of message set when rolling log segment. This is possible to do for message format V2 and beyond without any performance penalty, because we have the first offset stored in the header. This augments the fix made in KAFKA-4451 to avoid using the heuristic for V2 and beyond messages.  Added unit tests to simulate cases where segment needs to roll because of overflow in index offsets. Verified that the new segment created in these cases uses the first offset, instead of the heuristic in use previously.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-03-07T23:55:35Z","2018-05-11T07:29:36Z"
"","5016","MINOR: Use private modifier for doc vars","Use private modifier instead of public for static *_DOC variables in ProducerConfig & ConsumerConfig  Some of such variables are already private, but some of them not. I think, that clients shouldn't to be able to put `*_DOC` var in properties instead of `*_CONFIG`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nryanov","2018-05-13T13:42:35Z","2019-01-17T18:09:58Z"
"","4503","KAFKA-6494; ConfigCommand update to use AdminClient for broker configs","Use new AdminClient for describing and altering broker configs using ConfigCommand. Broker quota configs as well as other configs will continue to be processed directly using ZooKeeper until KIP-248 is implemented.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-01T01:21:41Z","2018-02-03T00:31:59Z"
"","4989","MINOR: A few small cleanups from KAFKA-6299","Use local fields in `AdminClientRunnable` to avoid the need to pass the collections as method parameters and fix warnings about invalid javadoc tags. Also use the `internals` package name for `AdminMetadataManager` for consistency with `consumer.internals` and `producer.internals`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-09T21:50:30Z","2018-05-11T00:31:13Z"
"","5131","KAFKA-6987 Reimplement KafkaFuture with CompletableFuture","Use CompletableFuture in KafkaFutureImpl to implement KafkaFuture.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrasbeni","2018-06-04T18:13:43Z","2018-11-30T11:31:43Z"
"","5208","MINOR: Upgrade Jackson to 2.9.6","Upgrade strongly recommended due to security fixes for `jackson-databind` (same as ones in 2.7.9.4 and 2.8.11.2).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-13T06:47:44Z","2018-06-13T21:51:54Z"
"","5272","MINOR: KIP-211 Follow-up","Updates the description of `offsets.retention.minutes` config, and fixes an upgrade note.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-06-22T02:10:50Z","2018-06-26T14:57:29Z"
"","4911","MINOR: Remove deprecated parameter in ProcessorContext#register","Updated the upgrade doc as well since we do not have an overloaded function without the deprecated parameter before. Also renamed the 1.2 release version to 2.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-21T20:17:47Z","2020-04-24T23:55:08Z"
"","5384","KAFKA-7177: Update 2.0 documentation to reflect changed quota behaviors by KIP-219","Updated the 2.0 document for changed quota behaviors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-07-18T05:47:39Z","2018-07-22T16:46:34Z"
"","4836","KAFKA-6628: RocksDBSegmentedBytesStoreTest does not cover time window serdes","Updated RocksDBSegmentedBytesStoreTest class to include time window serdes","closed","streams,","lijubjohn","2018-04-08T10:59:47Z","2018-05-07T17:57:36Z"
"","4602","MINOR: Fix error message for compatibility tests","Updated error message for failing system test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-02-20T16:59:55Z","2018-02-21T19:30:28Z"
"","4506","KAFKA-6514: Add API version as a tag for the RequestsPerSec metric","Updated `RequestChannel` to include `version` as a tag for all RequestsPerSec metrics. Updated tests to verify that the extra tag exists.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","allenxwang","2018-02-01T18:01:00Z","2018-04-16T17:16:27Z"
"","5279","MINOR: Add exception.toString for error log formatting","Update to use `exception.toString()` as a parameter for `log.error` as in log4j impl, if the passed in parameter is an Exception and it is the last parameter, it is not considered as the parameter of the forming string.  (thanks to @guozhangwang for that)  There will also be a corresponding PR coming for 1.1  Testing strategy - ran current tests. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-06-22T19:07:23Z","2018-06-23T01:16:23Z"
"","4732","KAFKA-6659: Improve error message if state store is not found","Update the exception message thrown to state about the state store not found and suggest checking store has been connected to the processor.","closed","streams,","perkss","2018-03-19T08:03:06Z","2018-03-23T14:37:34Z"
"","4603","KAFKA-6573: Update brokerInfo in KafkaController on listener update","Update KafkaController.brokerInfo when listeners are updated since this value is used to register the broker in ZooKeeper if there ZK session expires. Also added test to verify values in ZK after session expiry.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-20T20:11:44Z","2018-02-21T16:43:26Z"
"","5300","Merge pull request #1 from apache/trunk","update from origin trunk  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","noarter","2018-06-27T02:02:39Z","2018-06-27T06:48:41Z"
"","4831","KAFKA-6753: Update controller metrics periodically instead of after processing every event","Update controller metrics periodically, e.g. once per second, instead of after processing every event  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-04-06T00:19:38Z","2018-04-06T02:31:36Z"
"","4802","HOTFIX: Enforce a rebalance upon task migration","Unsubscribe / resubscribe a rebalance upon task migration (either false positive or not) to enforce a rebalance, also to refresh on log end offset  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-30T23:19:55Z","2020-04-24T23:48:01Z"
"","4552","MINOR: Fixed typos in KGroupedTable Javadoc","Two more fixes for you all to consider. Thank you.","closed","docs,","dminkovsky","2018-02-08T16:58:46Z","2018-02-13T00:56:47Z"
"","4605","KAFKA-5660: Don't throw TopologyBuilderException during runtime","TopologyBuilderException is a pre-runtime exception that should only be thrown before KafkaStreams#start() is called.","closed","streams,","nafshartous","2018-02-20T22:49:49Z","2018-03-03T22:39:59Z"
"","4682","MINOR: Fix wrong message in `bin/kafka-run-class.sh`.","To build jar you need to specify `scalaVersion` instead of `scala_version`. https://github.com/apache/kafka/blob/8df96a4119a5d46372eecdceb916f80dd073338a/gradle/dependencies.gradle#L34  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiminhsieh","2018-03-11T14:13:39Z","2018-04-04T07:55:58Z"
"","5157","Minor: fix javadocs of StreamsConfig and ValueTransformerWithKey","Those doc errors are explicit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","chia7712","2018-06-07T06:47:19Z","2018-06-30T21:27:31Z"
"","4657","MINOR: Tag AWS instances with Jenkins build url","This will allow us to trace leaked instances back to the job, so that we can figure out what happened and fix the leak.  Testing: Verified Jenkins build URL is set on the AWS instances when running system tests.  Backporting: Can this be backported to 0.10.2? The oldest branch we can backport the better.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","maxzheng","2018-03-06T23:49:25Z","2018-03-09T23:02:17Z"
"","5204","KAFKA-7047: Added SimpleHeaderConverter to plugin isolation whitelist","This was originally missed when headers were added as part of KIP-145 in AK 1.1. An additional unit test was added in line with the StringConverter.  This should be backported to the AK `1.1` branch so that it is included in the next bugfix release. The `SimpleHeaderConverter` class that we're referencing was first added in the 1.1.0 release, so there's no reason to backport earlier.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-06-12T20:58:04Z","2020-10-16T06:32:36Z"
"","4983","KAFKA-6761: Construct logical Streams Graph in DSL Parsing","This version is a WIP and intentionally leaves out some additional required changes to keep the reviewing effort more manageable. This version of the process includes  1. Cleaning up the graph objects to reduce the number of parameters and make the naming conventions more clear. 2. Intercepting all calls to the `InternalToplogyBuilder` and capturing all details required for possible optimizations and building the final topology.   This PR does not include writing out the current physical plan, so no tests included.  The next PR will include additional changes to building the graph and writing the topology out without optimizations, using the current streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-05-09T02:41:45Z","2018-06-18T17:58:37Z"
"","5175","KAFKA-6971: Passing in help flag to kafka-console-producer should pri…","this small fix introduces to print all available option for Kafka console producer and consumer. I did few tests while producing and consuming messages after this one-liner change which looks positive and dont have any side effects.","closed","","rajkrrsingh","2018-06-09T19:07:14Z","2018-11-26T17:10:48Z"
"","5042","MINOR: Use statically compiled regular expressions instead of inline","This should improve performance, as in-place means that the regex has to be compiled again each time the code path is traversed. Which is not performant.  A nice writeup on this topic can be found here: https://softwareengineering.stackexchange.com/questions/216320/java-regex-patterns-compile-time-constants-or-instance-members  And it's also recommended in Joshua Bloch's ""Effective Java 3rd Edition"" (Item 6: Avoid creating unnecessary objects)  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","KoenDG","2018-05-19T21:19:52Z","2018-06-08T20:21:33Z"
"","4685","KAFKA-6560: Add docs for KIP-261","This should be part of the original PR but missing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-11T20:21:35Z","2020-04-24T23:48:55Z"
"","5358","MINOR: Added description for zookeeper.connect","This setting allows specifying a chroot so we documented it.  Co-authored-by: Mickael Maison  Co-authored-by: Katherine Farmer   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-07-12T15:10:30Z","2018-07-12T18:44:13Z"
"","4541","Dynamically reload truststore.","This RP allows kafka brokers to dynamically reload truststore based on last modify time of the truststore file. No need to run AdminClient/kafka-configs.sh.","open","","allenxiang","2018-02-07T20:46:21Z","2018-03-02T19:31:11Z"
"","4705","KAFKA-6052: Fix the request retry issue (on Windows) in InterBrokerSendThread","This resolves the issue detected on Windows. It's a follow-up to the investigation done by @hachikuji (details on the JIRA).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-03-13T22:47:30Z","2018-03-24T21:04:09Z"
"","4964","Remove Scala procedure syntax","This removes the Scala procedure syntax from the codebase as this is deprecated since Scala 2.11: https://github.com/scala/scala/pull/3076  Let me know your thoughts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","joan38","2018-05-03T21:28:38Z","2018-05-09T18:54:24Z"
"","4600","MINOR: Fix logger name override","This regressed during the log4j -> scalalogging change.  Added unit tests, one of which failed before the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-02-20T15:49:57Z","2018-02-20T20:42:05Z"
"","4805","KAFKA-6657: Add StreamsConfig prefix for different consumers","This pull request is for jira [6657](https://issues.apache.org/jira/browse/KAFKA-6657). The KIP proposal is [here](https://cwiki.apache.org/confluence/display/KAFKA/KIP-276+Add+StreamsConfig+prefix+for+different+consumers)  Added unit tests for new `getGlobalConsumerConfigs` API and make sure existing restore consumer tests are passing.","closed","streams,","abbccdda","2018-04-01T01:41:14Z","2018-05-02T20:24:15Z"
"","5276","KAFKA-7103: Use bulkloading for RocksDBSegmentedBytesStore during init","This PR uses bulk loading for recovering RocksDBWindowStore, same as RocksDBStore. Will perform unit test and integration test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Ishiihara","2018-06-22T16:09:35Z","2018-07-17T21:04:52Z"
"","4923","KAFKA-6761: Part 1 of 3; Graph nodes","This PR supersedes PR #4654 as it was growing too large. All comments in that PR should be addressed here.  I will attempt to break the PRs for the topology optimization effort into 3 PRs total and will follow this general plan:  1. This PR only adds the graph nodes and graph. The graph nodes will hold the information used to make calls to the `InternalTopologyBuilder` when using the DSL. Graph nodes are stored in the `StreamsTopologyGraph` until the final topology needs building then the graph is traversed and optimizations are made at that point.  There are no tests in this PR relying on the follow-up PR to use all current streams tests, which should suffice. 2. PR 2 will intercept all DSL calls and build the graph.  The `InternalStreamsBuilder` uses the graph to provide the required info to the `InternalTopologyBuilder` and build a topology.  The condition of satisfaction for this PR is that all current unit, integration and system tests pass using the graph. 3. PR 3 adds some optimizations mainly automatically repartitioning for operations that may modify a key and have child operations that would normally create a separate repartition topic, saving possible unnecessary repartition topics.  For example the following topology: ``` KStream mappedStreamOther = inputStream.map(new KeyValueMapper>() {             @Override             public KeyValue apply(String key, String value) {                  return KeyValue.pair(key.substring(0, 3), value);             }         });           mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(5000)).count().toStream().to(""count-one-out"");         mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(10000)).count().toStream().to(""count-two-out"");         mappedStreamOther.groupByKey().windowedBy(TimeWindows.of(15000)).count().toStream().to(""count-three-out""); ``` would create 3 repartion topics, but after applying an optimization strategy, only one is created.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-04-24T21:36:06Z","2018-05-04T15:39:57Z"
"","4960","KAFKA-6846: Throw InterruptedException after poll in awaitReady() and sendAndReceive() if the interrupt flag is set","This PR resolves the issue that controller can spend a long time (more than 60s) in processing BrokerChange event when there are dead brokers, by throwing InterruptedException in the right place if the RequestSendThread sees the interrupt flag is set. In this case, RequestSendThread can break the poll loop before timeout to finish the shutdown and unblock the controller event thread, who is waiting for RequestSendThread to shutdown when removing the broker.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","hzxa21","2018-05-02T21:41:53Z","2018-07-20T22:54:59Z"
"","4931","KAFKA-6776 : ConnectRestExtension Interfaces & Rest integration","This PR provides the implementation for KIP-285 and also a reference implementation for authenticating BasicAuth credentials using JAAS LoginModule  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-04-25T19:58:35Z","2020-10-16T06:02:56Z"
"","4489","KAFKA-6166: Update javadoc for streams config","This PR piggy-back a few different javadoc changes:  1. KIP-220: add admin client prefix. 2. Clarify on user customized configs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-30T18:55:47Z","2020-04-24T23:49:28Z"
"","5344","MINOR: Add StreamsConfig to TopologyTestDriver constructor to enable ease of testing","This PR offers a simple addition to enable a possibility to pass a **StreamsConfig** instance for the **TopologyTestDriver**.  Why this was offered? In our project we have found very useful to create a **StreamsConfig** instance with all injected dependencies (by Spring) and then still be able to create **TopologyTestDriver**, which is now possible with this PR.  P.S. Sorry for so many commits -- I should have merged it first and then create a branch.","open","kip,","wlsc","2018-07-06T21:27:11Z","2020-06-12T23:03:49Z"
"","5390","KAFKA-7144: Fix task assignment to be even","This PR now justs removes the check in `TaskPairs.hasNewPair` that was causing the task assignment issue.  This was done as we need to further refine task assignment strategy and this approach needs to include the statefulness of tasks and is best done in one pass vs taking a ""patchy"" approach.  Updated current tests and ran locally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-07-18T20:03:25Z","2018-08-01T16:48:52Z"
"","5350","KAFKA-7098: Improve accuracy of throttling by avoiding under-estimating actual rate in Throttler","This PR modifies Throttler.scala by setting the `periodStartNs` to the current time instead of the time before the potential `sleep` call when throttling is needed. The reason behind is that if we reset `periodStartNs` to the time before `sleep`, we will increase the time window in the next actual rate calculation, which will underestimate the actual rate and may miss the throttling opportunity or sleep for less time. A unit test is also added to test the fix.  For example, if we use Throttler to throttle the pre sec rate to 10 with checkInterval 1s, in the original implementation: 1. 15 events happen during [t0, t0+1s] 2. Throttler will sleep the thread until t0+1.5s, then reset period start time to t0+1s 3. 10 events happen during [t0+1.5s, t0+2s], Throttler will not throttle this time because the estimated rate is `10 / [(t0+2s) - (t0+1s)] = 10`  But the actual rate during [t0, t0+2s] is `(10+15) / 2 = 12.5 > 10`  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-07-09T21:59:28Z","2018-07-20T06:50:55Z"
"","4536","[WIP] Migrate HTML to RST","This PR migrates the HTML documentation content to restructuredText.   CC: @ewencp @guozhangwang","closed","docs,","joel-hamill","2018-02-06T23:56:41Z","2021-08-04T23:52:06Z"
"","4654","KAFKA-6761: Reduce Kafka Streams Footprint","This PR is the first pass on the topology optimization effort. Here's a guide to the changes and the order of the list is meant to imply a possible review order.    1. `StreamsTopolgyGraphImpl` - This class is used to build the graph, record any metadata to be used later during the topology building process. Some stubbed out code to that end is already present 2. `TopologyOptimizerImpl` - This class traverses the graph and uses information contained in a `StreamGraphNode` to make calls to the `InternalTopologyBuilder` to construct the topology.  It's at this point any optimizations will be applied. 3. `StreamsBuilder` -  The `StreamsBuilder#build` method has been modified to call `InternalStreamdBuilder#buildAndOptimize` to build topology. 4. `InternalStreamsBuilder` - Contains a reference to the `StreamsTopologyGraph` added a methods `InternalStreamsBuilder#addNode`  and `InternalStreamsBuilder#buildAndOptimize` `InternalStreamsBuilder` is the only intermediary for building the graph from the DSL. 5. `KStreamImpl`, `KTableImpl`, `KGroupedStream` and `KGroupedTable` and others have been refactored to create `StreamGraphNode` instances and place topology information in the graph vs. calling `InternalTopologyBuilder`. 6. `StreamsGraphNode` and sub-classes are used to contain information from the DSL calls.  IMHO this is the part of the PR so far that needs the most refactoring work from me, but these are merely container objects so the refactoring should be straightforward.  I plan on updating the PR with updates to these classes soon.  Some other notes: 1. I have not added units test for the new code (will be added soon).  But IMHO the fact that the framework is used to handle all DSL operations, I feel that all of our current tests passing suffice for the moment. 2. Due to depreciation, I have not included the `KStreamsBuilder` in this initial PR.  I can do so, but I think this takes us a step backward regarding separation of DSL and PAPI.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-03-06T18:05:08Z","2018-04-24T21:37:03Z"
"","5177","MINOR: KIP-297 related - Fix regex to exclude ConfigProvider interface from class loading isolation","This PR is fixing a bug in the regex that was recently added by https://github.com/apache/kafka/pull/5141 when implementations of the `ConfigProvider` interface were added to the set of Connect plugins benefit from class loading isolation.  To prevent breaking initialization of class loading isolation during class scanning and allow for future extension of plugin interfaces, such interfaces should not be loaded in isolation. They are considered part of the framework. A previous example is Transformation interface in Connect.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2018-06-10T04:26:06Z","2018-06-13T00:27:57Z"
"","4782","KAFKA-6711: GlobalStateManagerImpl should not write offsets of in-mem…","This PR is addressing issues when persisting non persistent stores into checkpoint file.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cemo","2018-03-27T20:08:39Z","2018-06-14T23:24:14Z"
"","5052","KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [part 3]","This PR is a further step towards the complete replacement of `KStreamTestDriver` with `TopologyTestDriver`. These straightforward changes were split from another [PR](https://github.com/apache/kafka/pull/4986) to simplify the review process.  * Refactor:   - KStreamWindowReduceTest   - KTableMapKeysTest   - SessionWindowedKStreamImplTest   - TimeWindowedKStreamImplTest","closed","streams,","h314to","2018-05-21T17:28:40Z","2018-05-22T15:54:38Z"
"","4986","KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [part 2]","This PR is a further step towards the complete replacement of `KStreamTestDriver` with `TopologyTestDriver`.  * Add task, processorTopology, and globalTopology access to TopologyTestDriverAccessor * Add condition to prevent NPE in ProcessorContextImpl * Refactor:   - KTableFilterTest   - KTableSourceTest   - KTableMapValuesTest   - KTableImplTest.   edit: To simplify the review process some straightforward changes were moved to another [PR](https://github.com/apache/kafka/pull/5052).","closed","streams,","h314to","2018-05-09T10:41:43Z","2018-06-15T13:41:52Z"
"","5064","KAFKA-6028: Improve the quota throttle communication (KIP-219)","This PR is a continuation from https://github.com/apache/kafka/pull/4830. I create a new PR because the original PR got big and it looks like some reviewers are having problems with loading it. I also used a new branch.   This implements KIP-219, where a broker return a response with throttle time on quota violation immediately after processing the corresponding request. After the response is sent out, the broker will keep the channel muted until the throttle time is over. Also, on receiving a response with throttle time, client will also block outgoing communication to the broker for the specified throttle time.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-05-22T18:26:50Z","2018-06-06T16:31:47Z"
"","4499","MINOR: KIP-229 Follow-up","This PR includes * Minor fix to command line output message * Relevant documentation update  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-01-31T21:52:40Z","2018-02-02T06:04:15Z"
"","5065","KAFKA-6738: Implement error handling for source and sink tasks","This PR implements the features described in this KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect  This PR changes the Connect framework to allow it to automatically deal with errors encountered while processing records in a Connector. The following behavior changes are introduced here:  **Retry on Failure**: Retry the failed operation a configurable number of times, with backoff between each retry. **Task Tolerance Limits**: Tolerate a configurable number of failures in a task.  We also add the following ways to report errors, along with sufficient context to simplify the debugging process:  **Log Error Context**: The error information along with processing context is logged along with standard application logs. **Dead Letter Queue**: Produce the original message into a Kafka topic (applicable only to sink connectors).  New **metrics** which will monitor the number of failures, and the behavior of the response handler are added.  The changes proposed here **are backward compatible**. The current behavior in Connect is to kill the task on the first error in any stage. This will remain the default behavior if the connector does not override any of the new configurations which are provided as part of this feature.  Testing: added multiple unit tests to test the retry and tolerance logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-05-22T20:20:55Z","2020-10-16T06:02:57Z"
"","5002","KAFKA-6725: Addition of isClosing to SinkTaskContext","This PR implements KAFKA-6725 and KIP-275 to provide for the addition of an `isClosing` method on the `SinkTaskContext`. This permits `SinkTask`s to know if their `preCommit` hook is being invoked because the task is about to be shut down. This allows tasks to optionally apply different heuristics in a ""I am about to close"" situation.  For example, a sink configured to archive data to S3 might choose to checkpoint and upload sooner than it would have otherwise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","farmdawgnation","2018-05-11T03:47:30Z","2020-06-14T02:15:28Z"
"","4756","KAFKA-6670: Implement a Scala wrapper library for Kafka Streams","This PR implements a Scala wrapper library for Kafka Streams. The library is implemented as a project under streams, namely `:streams:streams-scala`. The PR contains the following:  * the library implementation of the wrapper abstractions * the test suite  * the changes in `build.gradle` to build the library jar  The library has been tested running the tests as follows:  ``` $ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdes streams:streams-scala:test $ ./gradlew -Dtest.single=StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro streams:streams-scala:test $ ./gradlew -Dtest.single=WordCountTest streams:streams-scala:test ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","debasishg","2018-03-22T13:19:09Z","2018-05-07T10:44:03Z"
"","4935","MINOR: Fixes for streams system tests","This PR fixes some regressions introduced into streams system tests and sets the upgrade tests to ignore until PR #4636 is merged as it has the fixes for the upgrade tests.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-04-26T14:18:40Z","2018-04-26T17:05:08Z"
"","4841","MINOR: update VerifiableProducer to send keys if configured and removed StreamsRepeatingKeyProducerService","This PR does the following: - Remove the `StreamsRepeatingIntegerKeyProducerService` and  the associated Java class  - Add a parameter to `VerifiableProducer.java` to enable sending keys when specified - Update the corresponding Python file `verifiable_producer.py` to support the new parameter.   Tested by kicking off all system tests https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1677/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-04-09T13:49:45Z","2018-04-28T05:13:02Z"
"","5433","KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [part 4]","This PR continues the work towards the removal of KStreamTest driver. As suggested in a [previous PR](https://github.com/apache/kafka/pull/4986), `OutputVerifier` is used instead of `MockProcessor`.  Refactor:  * KTableKTableOuterJoinTest  * KTableKTableLeftJoinTest  * KTableKTableOuterJoinTest","closed","streams,","h314to","2018-07-29T22:27:56Z","2019-02-15T20:50:46Z"
"","5101","KAFKA-6082: Fence zookeeper updates with controller epoch zkVersion","This PR aims to enforce that the controller can only update zookeeper states after checking the controller epoch zkVersion. The check and zookeeper state updates are wrapped in the zookeeper multi() operations to ensure that they are done atomically. This PR is necessary to resolve issues related to multiple controllers (i.e. old controller updates zookeeper states before resignation, which is possible during controller failover based on the single threaded event queue model we have)  This PR includes the following changes: - Add MultiOp request and response in ZookeeperClient - Ensure all zookeeper updates done by controller are protected by checking the current controller epoch zkVersion - Modify test cases in KafkaZkClientTest to test mismatch controller epoch zkVersion  Tests Done: - Unit tests (with updated tests to test mismatch controller epoch zkVersion) - Existing integration tests + newly added test cases in ControllerIntegrationTest - Perf testing: https://github.com/apache/kafka/pull/5101#issuecomment-418243421. Test results show that there is no performance overhead before and after this patch for common controller operations including controller failover, preferred replica leader election, and broker shutdown/startup.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2018-05-30T23:31:22Z","2018-09-13T14:01:19Z"
"","5451","KAFKA-6761: Reduce streams footprint part IV add optimization","This PR adds the optimization of eliminating multiple repartition topics when the `KStream` resulting from a key-changing operation executes other methods using the new key and reduces the repartition topics to one.  Note that this PR leaves in place the optimization for re-using a source topic as a changelog topic for source `KTable` instances.  I'll have another follow-up PR to move the source topic optimization to a method within `InternalStreamsBuilder` so it can be performed in the same area of the code.  Additionally, the current value of `StreamsConfig.OPTIMIZE` is `all` and we'll need to have another KIP to change the value to `2.1`.     An integration test `RepartitionOptimizingIntegrationTest` which asserts the same results for an optimized topology with one repartition topic as the un-optimized version with four repartition topics. More tests will be added, but I wanted to get reviews on the approach now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-08-02T20:14:01Z","2018-08-16T22:52:01Z"
"","5163","KAFKA-7021: Reuse source based on config","This PR actually contains two changes:  1) leverage on the TOPOLOGY_OPTIMIZATION config to ""adjust"" the topology internally to reuse the source topic.  2) fixed a long dangling bug that whenever source topic is reused as changelog topic, write the checkpoint file for the consumed offset, this is done by union the `ackedOffset` from the producer, plus the `consumed` offset from the consumer, note we will priori ackedOffset since the same topic may show up in both (think about repartition topic), by doing this the consumed offset from source topics can be treated as checkpointed offset when reuse happens.  3) added a few unit and integration tests with / wo  the reusing, and make sure the restoration, standby task, and internal topic creation behaviors are all correct.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-07T23:41:30Z","2018-06-12T21:39:38Z"
"","4668","KAFKA-6630: Speed up the processing of TopicDeletionStopReplicaResponseReceived events on the controller","This patch tries to speed up the inefficient functions identified in Kafka-6630 by grouping partitions in the ControllerContext.partitionReplicaAssignment variable by topics. Hence trying to find all replicas for a topic won't need to go through all the replicas in the cluster.  Passed all tests using ""gradle testAll""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-03-08T22:58:35Z","2018-03-30T06:56:49Z"
"","4611","MINOR: Revert incompatible behavior change to consumer reset tool","This patch reverts the removal of the --execute option in the offset reset tool and the change to the default behavior when no options were present. These changes were introduced in KIP-171, but they are not compatible with previous releases. We should at least give users a deprecation window for incompatible changes.  Test cases were not actually validating that offsets were committed when the --execute option was present, so I have fixed that and added basic assertions for the dry-run behavior. I also removed some duplicated test boilerplate.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-22T08:22:15Z","2018-02-23T23:31:51Z"
"","5254","KAFKA-7076: Prevent reading through log data for constructing producer state when using old message format","This patch removes the need to build up producer state when the log is using V0 / V1 message format where we did not have idempotent and transactional producers yet.  Also fixes a small issue where we incorrectly reported the offset index corrupt if the last offset in the index is equal to the base offset of the segment.","closed","performance,","dhruvilshah3","2018-06-19T23:36:27Z","2018-09-18T06:30:10Z"
"","4974","KAFKA-6877; Remove completedFetch upon a failed parse if it contains no records.","This patch removed a completedFetch from the completedFetches queue upon a failed parse if it contains no records. The following scenario explains why this is needed for an instance of this case – i.e. in TopicAuthorizationException.  0. Let's assume a scenario, in which the consumer is attempting to read from a topic without the necessary read permission. 1. In Fetcher#fetchedRecords(), after peeking the completedFetches, the Fetcher#parseCompletedFetch(CompletedFetch) throws a TopicAuthorizationException (as expected). 2. Fetcher#fetchedRecords() passes the TopicAuthorizationException up without having a chance to poll completedFetches. So, the same completedFetch remains at the completedFetches queue. 3. Upon following calls to Fetcher#fetchedRecords(), peeking the completedFetches will always return the same completedFetch independent of any updates to the ACL that the topic is trying to read from. 4. Hence, despite the creation of an ACL with correct permissions, once the consumer sees the TopicAuthorizationException, it will be unable to recover without a bounce.","closed","","efeg","2018-05-07T23:38:56Z","2020-01-06T19:31:29Z"
"","4616","KAFKA-6588: Add metric for alive log cleaner thread count","This patch introduces a Log Cleaner metric - `live-cleaner-thread-count` to monitor the log cleaner thread. It additionally fixes a minor issue to ensure that the correct offsets are logged in `LogCleaner#recordStats`.","open","core,","navina","2018-02-23T20:18:38Z","2018-03-07T23:31:13Z"
"","5408","KAFKA-7126: Reduce number of rebalance for large consumer group after a topic is created","This patch forces metadata update for consumers with pattern subscription at the beginning of rebalance (retry.backoff.ms is respected). This is to prevent such consumers from detecting subscription changes (e.g., new topic creation) independently and triggering multiple unnecessary rebalances. KAFKA-7126 contains detailed scenarios and rationale.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-07-20T22:01:10Z","2018-07-26T17:36:03Z"
"","5169","MINOR: Handle segment splitting edge cases and fix recovery bug","This patch fixes the following issues in the log splitting logic added to address KAFKA-6264:  1. We were not handling the case when all messages in the segment overflowed the index. In this case, there is only one resulting segment following the split. 2. There was an off-by-one error in the recovery logic when completing a swap operation which caused an unintended segment deletion.  Additionally, this patch factors out of `splitOverflowedSegment` a method to write to a segment using from with an instance of `FileRecords`. This allows for future reuse and isolated testing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-06-08T21:18:13Z","2018-06-19T20:26:30Z"
"","4634","KAFKA-6604; ReplicaManager should not remove partitions on the log directory from high watermark checkpoint file","This patch fixes an issue to prevent broker from unnecessarily truncating many partitions to log start offset.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","core,","lindong28","2018-03-01T21:48:37Z","2018-12-07T02:48:33Z"
"","4842","KAFKA-6768; Transactional producer may hang in close with pending requests","This patch fixes an edge case in producer shutdown which prevents `close()` from completing due to a pending request which will never be sent due to shutdown initiation. I have added a test case which reproduces the scenario.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-09T20:31:17Z","2018-04-09T22:39:08Z"
"","4583","KAFKA-6238; Fix inter-broker protocol message format compatibility check","This patch fixes a bug in the validation of the inter-broker protocol and the message format version. We should allow the configured message format api version to be greater than the inter-broker protocol api version as long as the actual message format versions are equal. For example, if the message format version is set to 1.0, it is fine for the inter-broker protocol version to be 0.11.0 because they both use message format v2.  I have added a unit test which checks compatibility for all combinations of the message format version and the inter-broker protocol version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-17T08:05:27Z","2018-02-21T17:48:24Z"
"","4498","KAFKA-6492: Fix log truncation to empty segment","This patch ensures that truncation to an empty segment forces resizing of the index file in order to prevent premature rolling.  I have added unit tests which verify that appends are permitted following truncation to an empty segment. Without the fix, this test case reproduces the failure in which the rolled segment matches the current active segment.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-01-31T21:06:53Z","2018-02-02T23:55:57Z"
"","5203","KAFKA-7050: Decrease default consumer request timeout to 30s","This patch changes the default `request.timeout.ms` of the consumer to 30 seconds. Additionally, it adds logic to `NetworkClient` to set timeouts at the request level. We use this to handle the special case of the JoinGroup request, which may block for as long as the value configured by `max.poll.interval.ms`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-06-12T18:43:53Z","2018-06-13T23:21:31Z"
"","4484","KAFKA-6352: Down-convert fetched records lazily for more efficent memory usage","This patch adds support for lazy down-conversion of records for older clients. Effectively, it moves down-conversion out of the handler threads and into the network threads. This should improve memory usage since responses that have only been queued for sending will not occupy the memory needed to hold the converted records.  This is largely a refactor which is covered through existing unit and integration tests. I have also added unit tests for the new `LazyDownConvertingRecords` implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-01-28T22:18:45Z","2018-02-15T00:42:46Z"
"","4666","KAFKA-6612: Added logic to prevent increasing partition counts during topic deletion","This patch adds logic in handling the PartitionModifications event, so that if the partition count is increased when a topic deletion is still in progress, the controller will restore the data of the path /brokers/topics/""topic"" to remove the added partitions.  Testing done: Added a new test method to cover the bug  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","gitlw","2018-03-08T21:09:21Z","2018-03-24T01:20:41Z"
"","4649","KAFKA-6612: Added logic to prevent increasing partition counts during topic deletion","This patch adds logic in handling the PartitionModifications event, so that if the partition count is increased when a topic deletion is still in progress, the controller will restore the data of the path /brokers/topics/""topic"" to remove the added partitions.  Testing done: Added a new test method to cover the bug  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","gitlw","2018-03-05T23:58:58Z","2018-03-08T21:05:36Z"
"","5194","KIP-297: MINOR: move FileConfigProvider to provider subpackage","This moves FileConfigProvider to the org.apache.common.config.provider package to more easily isolate provider implementations going forward.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-06-11T23:59:01Z","2020-10-16T06:02:59Z"
"","5345","HOTFIX: Fix checkstyle errors in MetricsTest","This may have been broken when picking #5341 into older branches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-07-06T22:56:58Z","2018-07-06T23:10:14Z"
"","4994","KIP-255: OAuth Authentication via SASL/OAUTHBEARER","This KIP proposes to add the following functionality related to SASL/OAUTHBEARER:  1) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to flexibly retrieve an access token from an OAuth 2 authorization server based on the declaration of a custom login CallbackHandler implementation and have that access token transparently and automatically transmitted to a broker for authentication.  2) Allow brokers to flexibly validate provided access tokens when a client establishes a connection based on the declaration of a custom SASL Server CallbackHandler implementation.  3) Provide implementations of the above retrieval and validation features based on an unsecured JSON Web Token that function out-of-the-box with minimal configuration required (i.e. implementations of the two types of callback handlers mentioned above will be used by default with no need to explicitly declare them).  4) Allow clients (both brokers when SASL/OAUTHBEARER is the inter-broker protocol as well as non-broker clients) to transparently retrieve a new access token in the background before the existing access token expires in case the client has to open new connections.  Signed-off-by: Ron Dagostino","closed","","rondagostino","2018-05-10T04:34:40Z","2018-05-26T07:18:42Z"
"","4521","MINOR:  exchange redundant Collections.addAll with parameterized constructor","This issue is a rework to the outdated ticket created earlier (https://github.com/apache/kafka/pull/2707). This issue was created because original branch was already removed.  Basic static analysis. What was accomplished: - exchanged redundant Collections.addAll with parameterized constructor - exchange manual copy to collection with Collections.addAll call","closed","","wlsc","2018-02-03T21:10:43Z","2018-02-06T19:35:24Z"
"","4963","Make Serdes less confusing in Scala","This is the corresponding PR of https://github.com/lightbend/kafka-streams-scala/pull/70  Serdes are confusing in the Scala wrapper: 1) We have wrappers around `Serializer`, `Deserializer` and `Serde` which are not very useful. 2) We have Serdes in 2 places `org.apache.kafka.common.serialization.Serde` and in `com.lightbend.kafka.scala.streams.DefaultSerdes`, instead we should be having only one place where to find all the Serdes.  I wanted to do this PR before the release as this is a breaking change.  This shouldn't add more so the current tests should be enough.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-03T18:45:08Z","2018-05-08T16:46:24Z"
"","4720","MINOR: Use statically compiled patterns instead of having regular expressions be compiled repeatedly.","This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  The purpose of this PR is introduce statically compiled regular expressions, as opposed to the current way the code uses regular expressions, which is to compile them every time the code path is traversed. This is done inside the String class source code.  Using statically compiled patterns like this helps avoid creating unnecessary objects, requiring less memory and cpu time to execute the same things.  All refactors were done based on the Java Oracle JDK8 source code.  You might notice some missing `Matcher.quoteReplacement()` calls. This is because the JDK8 source code for that method is this:  ```     public static String quoteReplacement(String s) {         if ((s.indexOf('\\') == -1) && (s.indexOf('$') == -1))             return s;         StringBuilder sb = new StringBuilder();         for (int i=0; i","closed","","KoenDG","2018-03-15T23:55:07Z","2018-05-19T21:09:02Z"
"","5138","KAFKA-6722: SensorAccess.getOrCreate should be more efficient","This is related to this PR: https://github.com/apache/kafka/pull/5109, which is closed.   There's a JIRA related to this: https://issues.apache.org/jira/browse/KAFKA-6722","open","","wuqingjun","2018-06-05T17:00:20Z","2018-06-05T17:00:20Z"
"","5473","MINOR: add spotlessScalaCheck to jenkins job","This is related to https://github.com/apache/kafka/pull/5472  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-08-07T19:55:41Z","2018-08-08T04:08:11Z"
"","4679","KAFKA-6648 - Fetcher.getTopicMetadata() should return all partitions for each requested topic","this is on the path of `KafkaConsumer.listTopics()`, but more importantly its also on the path of `KafkaConsumer.partitionsFor()`, where it means that under some circumstances the consumer will return only the ""healthy"" partitions (those with a controller) as opposed to all of the partitions.  this leads to issues downstream where various systems think the partition count on a topic has changed where in reality some broker just went down.","closed","","radai-rosenblatt","2018-03-10T01:02:31Z","2018-08-03T17:46:19Z"
"","4996","Fix typo in ConsumerRebalanceListener JavaDoc","This is just a minor typo in the javadoc.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Gunju-Ko","2018-05-10T10:42:27Z","2018-05-10T16:53:50Z"
"","4627","MINOR: Fix javadoc typo","This is just a minor typo in the javadoc. This contribution is my original work and that I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thomasleplus","2018-02-27T23:58:44Z","2018-02-28T03:04:30Z"
"","4622","KAFKA-4920: Stamped implements equals","This is contributed by @backender.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-02-26T19:35:56Z","2020-04-24T23:49:13Z"
"","4988","KAFKA-6878 Switch the order of underlying.init and initInternal","This is continuation of #4978. From Guozhang: ``` I think to fix this issue, in init we could consider switching the steps of 1 and 2:  initInternal(context); underlying.init(context, root); since  volatile boolean open = false; it should be sufficient. In this case the check on step 3) will fail if underlying.init is not completed and we will throw InvalidStateStoreException. ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-05-09T20:22:15Z","2018-05-10T00:07:49Z"
"","4686","KAFKA-4831: add documentation for KIP-265","This is based on top of https://github.com/apache/kafka/pull/4685 so should only be merged after that one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-03-11T20:22:35Z","2020-04-24T23:48:56Z"
"","5284","KAFKA-6966: Extend TopologyDescription.Sink to return TopicNameExtractor","This is an update `TopologyDescription.Sink` and `TopologyDescription.Source`.  The original intent for `TopologyDescription` was to provide users with an interface to perform runtime checking of Topologies. However, `TopologyDescription` has relied too heavily on returning strings as representations of various inner components of Topologies such as topics, topic pattern, etc. This PR instead changes the return types to that of the underlying object and relies on the `toString()` method of these objects to maintain their human readable representations.  More details are outlined in this PR's corresponding [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-321%3A+Update+TopologyDescription+to+better+represent+Source+and+Sink+Nodes).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","nprad","2018-06-23T03:27:26Z","2020-06-13T00:03:38Z"
"","5259","KAFKA-7082: Concurrent create topics may throw NodeExistsException","This is an unexpected exception so `UnknownServerException` is thrown back to the client.  This is a minimal change to make the behaviour match `ZkUtils`. This is better, but one could argue that it's not perfect. A more sophisticated approach can be tackled separately.  Added a concurrent test that fails without this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-20T19:10:40Z","2018-06-23T08:08:41Z"
"","5087","MINOR: Add Timer to simplify timeout bookkeeping","This is an attempt to find a better pattern for blocking methods with a timeout. We currently do a lot of bookkeeping for timeouts which is both error-prone and distracting.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-27T18:59:22Z","2018-08-10T21:39:29Z"
"","5248","hack upload remote checkpoint","This is a testing branch. Just doing our internal reviews.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2018-06-18T23:55:35Z","2018-06-21T18:45:43Z"
"","5353","KAFKA-7146: Grouping consumer requests per consumer coordinator in admin client in describeConsumerGroups","This is a subtask from KAFKA-6788 (https://issues.apache.org/jira/browse/KAFKA-6788)  Basically the idea is to use thread-safe collections like ConcurrentLinkedQueue and ConcurrentHashMap to save to result of each ""findCoordinator"" response. In the map, we will have the coordinator node Id as the key, and the value will be the collection of all the belonging groupIds.   When all the groupId processed (either successful or completed exceptionally), then we will iterate through the map and group the requests by coordinators.  This PR is mainly for discussion, there are a couple concerns:  1. Code logic: Is there any thing wrong with the logic of this solution? 2. Concurrency issues: can these collections guarantee the thread-safety? 3. Memory issue: using too many collections to store information, should cut down the memory usage. 4. Efficiency: is this more efficient than the implementation before (i.e. latency)? 5. Naming issues. 6. More unit tests.  I am open to all kind of suggestions and feedbacks, and will make several other commits after I receive opinion.   P.S. Is a KIP needed?  @guozhangwang @cmccabe Thank you!","open","","shunge","2018-07-10T19:23:26Z","2018-08-10T22:26:12Z"
"","4621","MINOR: Rename stream partition assignor to streams partition assignor","This is a straight-forward change that make the name of the partition assignor to be aligned with Streams.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-26T18:57:24Z","2020-04-24T23:55:53Z"
"","5141","MINOR : KIP-297 : use service loader for ConfigProvider impls","This is a small change to use the Java ServiceLoader to load ConfigProvider plugins.  It uses code added by @mageshn for Connect Rest Extensions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-06-05T21:25:56Z","2018-06-06T16:26:29Z"
"","4561","KAFKA-6503: Parallelize plugin scanning","This is a small change to parallelize plugin scanning.  This may help in some environments where otherwise plugin scanning is slow.","closed","connect,","rayokota","2018-02-13T00:02:47Z","2020-10-16T06:05:16Z"
"","4629","MINOR: Make PushHttpMetricsReporter use daemon threads.","This is a safe guard against users that do not properly close() the reporter, which causes the process to hang even if its main() method returns. This was the case with Kafka Streams apps in some cases in Kafka < 1.1.0 (KAFKA-6383). Without this fix, this behavior can make it difficult to use this class in some types of system tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-02-28T22:44:34Z","2018-03-02T21:24:55Z"
"","4714","MINOR: Pass a streams config to replace the single state dir","This is a general change and is re-requisite to allow streams benchmark test with different streams tests. For the streams benchmark itself I will have a separate PR for switching configs. Details:  1. Create a ""streams.properties"" file under PERSISTENT_ROOT before all the streams test. For now it will only contain a single config of state.dir pointing to PERSISTENT_ROOT. 2. For all the system test related code, replace the main function parameter of state.dir with propsFilename, then inside the function load the props from the file and apply overrides if necessary. 3. Minor fixes.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-03-14T22:30:09Z","2020-04-24T23:49:08Z"
"","5026","KAFKA-6905: Document that Transformers may be re-used by Streams","This is a follow-up to #5022 which added documentation to the Processor interface. This commit adds similar documentation to Transformer and ValueTransformer.  Also, s/processor/transformer/ in the close() docs.","closed","streams,","glasser","2018-05-16T21:24:44Z","2018-05-20T01:44:20Z"
"","5143","MINOR: remove duplicate map in StoreChangelogReader","This is a follow up to #5013. The end offset for each partition is already maintained correctly in `endOffsets` within `initialize()`; thus, the introduced `updatedEndOffsets` is redundant -- we missed this during review. Thanks to @guozhangwang for pointing out.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-06-06T01:07:39Z","2018-06-08T01:48:55Z"
"","4828","KAFKA-6746 Update documentation for zookeeper.connect property","This is a documentation update for ``zookeeper.connect`` property which will help ``Kafka`` users who want to use a single ``ZooKeeper`` quorum for multiple ``Kafka`` clusters.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bijugs","2018-04-05T15:40:19Z","2018-10-18T15:24:19Z"
"","5207","KAFKA-7021: checkpoint offsets from committed","This is a cherry-pick PR of KAFKA-7051 for 1.1 and older. In this PR:  1. With old StateStoreSupplier APIs the source topics are reused, and this PR will make sure the offsets are written for source topics.  2. With new Materialized API we do not enforce reusing source topic, and will use the changelog topic if it is specified.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-13T00:49:30Z","2020-04-24T23:57:30Z"
"","5232","KAFKA-7021: checkpoint offsets from committed","This is a cherry-pick PR from https://github.com/apache/kafka/pull/5207  1) add the committed offsets to checkpointable offset map.  2) add the restoration integration test for the source KTable case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-06-14T21:25:28Z","2018-06-15T15:27:09Z"
"","5455","MINOR: Fix task and cache level metrics caller","This is a cherry-pick of the metrics fixes included in #5398  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-08-03T02:29:58Z","2018-08-03T18:22:17Z"
"","5394","MINOR: fix upgrade docs for Streams","This is a cherry-pick from #5392 plus some more cleanup. Should be merged before 2.0 gets release.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-19T16:12:02Z","2018-07-20T18:02:07Z"
"","5020","KAFKA-6566: Improve Connect Resource Cleanup","This is a change to improve resource cleanup for sink tasks and source tasks.  Now `Task.stop()` is called from both `WorkerSinkTask.close()` and `WorkerSourceTask.close()`.  It is called from `WorkerXXXTask.close()` since this method is called in the `finally` block of `WorkerTask.run()`, and Connect developers use `stop()` to clean up resources.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-05-15T17:51:21Z","2020-10-16T06:24:17Z"
"","4690","MINOR: Rolling bounce upgrade fixed broker system test","This introduces a system test where we do a rolling bounce upgrade of streams application instances against a fixed broker version. Each broker version starting with `0.10.2` up to `trunk` is tested.  Streams applications start at `0.10.2` and do rolling upgrades up to `trunk` then do rolling downgrades back to `0.10.2`.  This PR also includes a new utility class for Streams systems tests `StreamsTest` which extends `KafkaTest` and provides some convenience methods for working with `VerifiableProducer` and `VerfiiableConsumer` and validating results in log files.  There are other system tests `streams_standby_replica_test.py` and `streams_broker_down_resilience_test.py` using common functionality that can be pulled out and extend the `StreamsTest` class instead.  I'll do that in a follow-on PR.  EDIT: Since the base class `StreamsTest` only exists in this PR so far, I've refactored the other tests as mentioned above.  I tested this by running branch builder https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1430/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-03-12T18:34:21Z","2018-03-22T23:02:40Z"
"","5377","MINOR: Update Jetty to 9.4.11","This includes important fixes:  http://dev.eclipse.org/mhonarc/lists/jetty-announce/msg00122.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-07-17T08:08:32Z","2018-07-18T05:31:42Z"
"","5376","KAFKA-4041: Update ZooKeeper to 3.4.13","This includes a fix for ZOOKEEPER-2184 (Zookeeper Client should re-resolve hosts when connection attempts fail), which fixes KAFKA-4041.  Updated a couple of tests as unresolvable addresses are now retried until the connection timeout. Cleaned up tests a little.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-07-17T03:55:18Z","2018-07-17T20:53:10Z"
"","4939","KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [cleanup]","This implements the suggestions made after the previous [PR](https://github.com/apache/kafka/pull/4832) for KAFKA-6474 was merged.  The majority of changes deals with using try-with-resources and a new method in `StreamsTestUtils` to set the test properties and instantiate the `TopologyTestDriver`, thus allowing the removal of the cumbersome `@Before` and `@After` methods.  I also replaced `stringSerde` and `intSerde` variables with (almost equally succinct) inline calls to `Serdes.String()` and `Serdes.Integer()`.  * Add method to create test properties to StreamsTestUtils * Make TopologyTestDriver protected constructor package-private * Add comment suggesting the use of TopologyTestDriver to KStreamTestDriver * Cleanup:     - GlobalKTableJoinsTest     - KGroupedStreamImplTest     - KGroupedTableImplTest     - KStreamBranchTest     - KStreamFilterTest     - KStreamFlatMapTest     - KStreamFlatMapValuesTest     - KStreamForeachTest     - KStreamGlobalKTableJoinTest     - KStreamGlobalKTableLeftJoinTest     - KStreamImplTest     - KStreamKStreamJoinTest     - KStreamKStreamLeftJoinTest     - KStreamGlobalKTableLeftJoinTest     - KStreamKTableJoinTest     - KStreamKTableLeftJoinTest     - KStreamMapTest     - KStreamMapValuesTest     - KStreamPeekTest     - StreamsBuilderTest     - KStreamSelectKeyTest     - KStreamTransformTest     - KStreamTransformValuesTest     - KStreamWindowAggregateTest     - KTableForeachTest","closed","","h314to","2018-04-27T17:07:03Z","2018-05-07T16:28:53Z"
"","4830","KAFKA-6028: Improve the quota throttle communication (KIP-219)","This implements KIP-219, where a broker return a response with throttle time on quota violation immediately after processing the corresponding request. After the response is sent out, the broker will keep the channel muted until the throttle time is over. Also, on receiving a response with throttle time, client will also block outgoing communication to the broker for the specified throttle time.  In the original KIP, it was stated that ""In order to let the clients know whether the received response is already throttled or not, we will need to bump up all the related request version."" However, after further discussion with @lindong28 and @becketqin, it was decided to bump up the protocol version of ApiVersionsRequest/Response only, which should be sufficient to detect the throttling behavior of the brokers that each client communicate with.   Initial review was done by @lindong28.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-04-05T23:18:15Z","2018-05-30T17:17:09Z"
"","5411","MINOR: Close ZooKeeperClient if waitUntilConnected fails during construction","This has always been an issue, but the recent upgrade to ZooKeeper 3.4.13 means that it is also an issue when an unresolvable ZK address is used, causing some tests to leak threads.  The change in behaviour in ZK 3.4.13 is that no exception is thrown from the ZooKeeper constructor in case of an unresolvable address. Instead, ZooKeeper tries to re-resolve the address hoping it becomes resolvable again. We eventually throw a `ZooKeeperClientTimeoutException`, which is similar to the case where the the address is resolvable, but ZooKeeper is not reachable.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-21T12:33:11Z","2018-07-22T16:49:33Z"
"","4965","Proposal to use Scalafmt with the Scala files","This eliminates the different people's opinion on coding style and makes the codebase consistant. I put arbitrary rules, let me know your thoughts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-03T21:36:38Z","2018-08-08T14:55:46Z"
"","5220","MINOR: Use KafkaConsumer in GetOffsetShell","This does the minimal amount of work so that the tool relies on public non-deprecated APIs (i.e. it no longer relies on Scala clients code).  Additional improvements (not included here) have been proposed via KIP-308.  There are a few other PRs that touch this class with overlapping goals:  - https://github.com/apache/kafka/pull/2891 - https://github.com/apache/kafka/pull/3051 - https://github.com/apache/kafka/pull/3320  One of them remains relevant in the context of KIP-308, but the others have been superseded. I included the authors of the 3 PRs as co-authors.  Co-authored-by: Arseniy Tashoyan  Co-authored-by: Vahid Hashemian  Co-authored-by: Mohammed Amine GARMES Co-authored-by: Ismael Juma   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-14T00:47:50Z","2018-06-14T17:37:48Z"
"","5151","KAFKA-7009: Suppress the Reflections log warning messages in system tests","This could be backported to older branches to reduce the extra log warning messages there, too.  Running Connect system tests in this branch builder job: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1773/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-06-06T14:24:55Z","2020-10-16T06:02:58Z"
"","5068","KAFKA-6886 Externalize secrets from Connect configs","This commit allows secrets in Connect configs to be externalized and replaced with variable references of the form `${provider:[path:]key}`, where the ""path"" is optional.  There are 2 main additions to `org.apache.kafka.common.config`: a `ConfigProvider` and a `ConfigTransformer`.  The `ConfigProvider` is an interface that allows key-value pairs to be provided by an external source for a given ""path"".  An a TTL can be associated with the key-value pairs returned from the path.  The `ConfigTransformer` will use instances of `ConfigProvider` to replace variable references in a set of configuration values.  In the Connect framework, `ConfigProvider` classes can be specified in the worker config, and then variable references can be used in the connector config.  In addition, the herder can be configured to restart connectors (or not) based on the TTL returned from a `ConfigProvider`.  The main class that performs restarts and transformations is `WorkerConfigTransformer`.    Finally, a `configs()` method has been added to both `SourceTaskContext` and `SinkTaskContext`.  This allows connectors to get configs with variables replaced by the latest values from instances of `ConfigProvider`.  Most of the other changes in the Connect framework are threading various objects through classes to enable the above functionality.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-05-23T00:06:50Z","2020-10-16T06:33:07Z"
"","5246","MINOR: Do not require request timeout be larger than session timeout","This check was left over from the old consumer logic in which the join group was bound by the session timeout. Since we use a custom timeout for JoinGroup, this restriction no longer makes sense.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-06-18T15:50:22Z","2018-06-18T20:43:02Z"
"","4853","MINOR: make Sensor#add idempotent","This change makes adding a metric to a sensor idempotent. That is, if the metric is already added to the sensor, the method  returns with success.  The current behavior is that any attempt to register a second metric with the same name is an error.  Testing strategy: There is a new unit test covering this behavior  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-04-11T15:40:25Z","2018-05-07T23:04:30Z"
"","5270","KAFKA-5886: Introduce delivery.timeout.ms producer config (KIP-91)","This change is based on @sutambe 's change https://github.com/apache/kafka/pull/3849 earlier.   primary changes in this pr:  1. In RecordAccumulator.java, use `inFlightBatches` to track the in-flight batches,  instead of using `soonToExpireInFlightsBatches` to only track the soon-to-expire batches.  With this change, in RecordAccumulator.expiredBatches, we check both `inFlightBatches` and `batches` to find the expired batches.   2. Fixed the test failures in SenderTest.java and RecordAccumulatorTest.java.","closed","producer,","yuyang08","2018-06-21T23:09:21Z","2019-01-12T01:46:44Z"
"","4514","KAFKA-6504: Fix creation of a sensor to be specific to a metric group so it is not shared","This change ensures that when sensors are created, they are specific to a metric group.  Previously the sensors were being shared between metric groups, causing incorrect metrics.","closed","connect,","rayokota","2018-02-02T00:02:38Z","2020-10-16T06:24:14Z"
"","4565","MINOR: Simplify Node.hashCode and equals","This can be a bottleneck in RecordAccumulator.ready if the number of brokers and partitions is large enough. We just use the `id` since it should be unique.  Originally reported in https://github.com/apache/kafka/pull/4350:  ""Faced with the producer performance degradation in case of high load and large number of brokers (100), topics (150) and partitions (350). Made several diagnostic records with the java flight recorder and found that the method HashSet::contains in RecordAccumulator::ready takes about 40% of the whole time of the application. It is caused by re-calculating a hash code of a leader (Node instance) for every batch entry.""  That PR cached the hashCode, which still requires computing the complex `hashCode` once and does not make `equals` cheap.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","ijuma","2018-02-13T10:09:03Z","2018-03-01T19:35:11Z"
"","5281","MINOR: Avoid FileInputStream/FileOutputStream","They rely on finalizers (before Java 11), which create unnecessary GC load. The alternatives are as easy to use and don't have this issue.  Also use `FileChannel` directly instead of retrieving it from `RandomAccessFile` whenever possible since the indirection is unnecessary.  Finally, add a few `try/finally` blocks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-22T21:51:47Z","2018-06-27T08:00:10Z"
"","5063","MINOR: AdminClient consumer group domain objects should have public constructors","These constructors should be public to allow users to write test cases using them. We follow a similar pattern for the other domain objects that we expose in `AdminClient` (e.g. `TopicDescription`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-22T18:17:17Z","2018-05-22T18:23:49Z"
"","5312","KAFKA-7111: Log error connecting to node at a higher log level","There were cases where the broker would return an unresolve-able address (e.g broker inside a docker network while client is outside) and the client would not log any information as to why it is timing out, since the default log level does not print `DEBUG` messages.  Changing this log level will enable easier troubleshooting in such circumstances. This change does not change the logs shown on transient failures like a broker failure, judging from my local tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2018-06-29T13:18:21Z","2018-07-04T10:00:15Z"
"","4791","MINOR: Downgrade to Gradle 4.5.1","There is a regression in Gradle 4.6 that causes `testAll` (but not `test`) to fail:  https://github.com/gradle/gradle/pull/4680  `./gradlew testAll` works after this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-28T20:46:50Z","2018-03-28T23:10:30Z"
"","5309","MINOR: Upgrade RocksDB to 5.13.4","There are a few JNI improvements added recently in RocksDB, plus some bug fixes that may cause `assertion error` that we have seen transiently in Streams unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-29T01:04:48Z","2018-06-29T18:10:25Z"
"","4894","MINOR: add window store range query in simple benchmark","There are a couple minor additions in this PR:  1) add a new test for window store, to range query upon receiving each record. 2) in the non-windowed state store case, add a `get` call before the `put` call. 3) Enable caching by default to be consistent with other Join / Aggregate cases, where caching is enabled by default.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-18T21:51:39Z","2020-04-24T23:55:02Z"
"","5035","KAFKA-5697: Revert streams wakeup","The wakeup-based strategy caused more problems than it solved, so we'll instead focus on KIP-266.  Revert commit 2d8049b.  Keep the metrics addition and the new test util.  Also keep the tests for shutdown, although they must be ignored until `poll(Duration)` is done in the scope of KIP-266.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-05-18T00:30:34Z","2018-05-18T16:08:02Z"
"","5019","Fix type inference on joins and aggregates on Scala API","The type inference doesn't currently work for the join functions in Scala as it doesn't know yet the types of the given `KStream[K, V]` or `KTable[K, V]`.  The fix here is to curry the joiner function. I personally prefer this notation but this also means it differs more from the Java API. I believe the diff with the Java API is worth in this case as it's not only solving the type inference but also fits better the Scala way of coding (ex: `fold`).  Moreover any Scala dev will bug and spend little time on these functions trying to understand why the type inference is not working and then get frustrated to be obliged to be explicit here where it's not harmful to be inferred.  The change is fairly straight forward but is also breaking, the good news is that we didn't release the Scala API yet, so this is perfect time to do this change.  This would also need some documentation update that I'm happy to do if there is positive feedback on this.  Thanks   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","joan38","2018-05-15T15:33:05Z","2018-08-21T16:43:29Z"
"","4845","Make [Config]Resource.toString() consistent with existing code","The toString() for ConfigResource was using { } instead of ( ) which is inconsistent with the existing toStrings in the code, while toString for Resource was using a mix of ( and }.","closed","","edenhill","2018-04-10T08:11:10Z","2018-04-10T11:40:43Z"
"","4587","MINOR:Fix typo in the impl source","The static method `KStreamImpl.createReparitionedSource()` is missing a `t`.  This PR globally fixes the typo and keeps the code indentation consistent.  I have not yet run the unit tests, because `gradle` looks like it is going to take upwards of an hour to download :sleeping:   I did confirm that the misspelling `Reparitioned` does not occur anywhere else in the sources or tests. It seems that it would only break a dependent library if that library is using the implementation classes directly.","closed","streams,","blak3mill3r","2018-02-19T08:17:56Z","2018-02-26T20:26:16Z"
"","4767","MINOR: Fix flaky standby task test","The standby-task test failed due to standby task distribution not be exactly equal.  I think this will be the case from time to time, so I've updated test to make sure the standby task assignment count is not zero  I tested this by running the system test on branch builder 25 times all tests passed - http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-03-23--001.1521845338--bbejeck--MINOR_fix_flaky_standy_task_test--c3054e9/report.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-03-23T23:06:01Z","2018-03-27T21:02:31Z"
"","5047","MINOR: Use reflection for signal handler and do not enable it for IBM JDK","The Signal classes are not available in the compile classpath if `--release` is used so we use reflection as a workaround. As part of that moved the code to Java and added a simple unit test.  Also disabled the signal handler if the IBM JDK is being used due to KAFKA-6918.  Manually tested shutdown via ctrl+c and verified that the message is printed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-20T05:34:55Z","2018-05-30T09:59:29Z"
"","4623","MINOR: Fix incorrect print in KafkaProducer","The print of timeout ms is not correctly formatted.","closed","producer,","wangshao","2018-02-27T08:05:50Z","2018-07-21T10:13:59Z"
"","4874","KAFKA-6790","The page here https://kafka.apache.org/11/documentation/streams/developer-guide/memory-mgmt.html talks about processor nodes and refers to non existing links.  Broken link (appears twice in the same document):  https://kafka.apache.org/11/documentation/streams/concepts.html#streams-concepts-processor  To find this search for the word ""processor node"" on the page memory-management , the ones which are links are broken.","closed","","ro7m","2018-04-15T06:37:19Z","2018-04-15T17:06:23Z"
"","5471","MINOR: Fix minikdc cleanup in system tests","The original way of stopping the minikdc process sometimes misfires because the process arg string is very long, and `ps` is not able to find the correct process. Using the `kill_java_processes` method is more reliable for finding and killing java processes.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-08-07T17:20:58Z","2020-10-16T06:03:01Z"
"","5217","MINOR: use new API timeout config in test","The old timeout configs no longer take effect, as of 53ca52f855e903907378188d29224b3f9cefa6cb. They are replaced by the new one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-06-13T21:56:23Z","2018-06-13T22:05:27Z"
"","5242","KAFKA-7067:Include connector configs added in KIP-297 and KIP-298 in the system test assertion","The new connector configs added in  KIP-297 and KIP-298 would need to be updated in the connect_rest_test.py so that the expected results match the actual.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mageshn","2018-06-17T07:18:29Z","2018-06-18T22:37:27Z"
"","5316","MINOR: Change ""no such session ID"" log to debug","The metric will give us good information about the number of evictions per second, so we don't need the INFO log.","closed","","cmccabe","2018-07-01T20:46:54Z","2019-10-10T00:41:06Z"
"","4806","KAFKA-6732","The link to processor topology on write-streams.html is redirecting to 404 error. Links which result in 404 : referred from https://kafka.apache.org/11/documentation/streams/developer-guide/write-streams.html  Broken : https://kafka.apache.org/11/documentation/streams/concepts.html#streams-concepts   https://kafka.apache.org/documentation/streams/concepts.html#streams-concepts","closed","docs,","ro7m","2018-04-01T13:01:42Z","2018-04-02T16:54:22Z"
"","4967","KAFKA-6857: Leader must always reply with undefined offset if undefined leader epoch requested","The leader must explicitly check if requested leader epoch is undefined, and return undefined offset so that the follower can fall back to truncating to high watermark. Otherwise, if the leader also is not tracking leader epochs, it may return its LEO, which will the follower to truncate to the incorrect offset.   Added unit test to verify the behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-05-04T00:10:36Z","2018-05-04T06:06:34Z"
"","5003","Minor: Remove the unused declaration","The issue is reported by the code analysis (intellij). Just do some garden works.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-05-11T07:26:59Z","2018-05-26T08:18:06Z"
"","4763","Minor: Don’t send the DeleteTopicsRequest for the invalid topic names","The invalid topic name is already handled locally so it is unnecessary to send the DeleteTopicsRequest. This PR adds a count to MockClient for testing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-03-23T09:36:31Z","2018-04-05T07:06:15Z"
"","4896","KAFKA-4682: Revise expiration semantics of consumer group offsets (KIP-211 - Part 1)","The implementation of [expiration semantics for unsubscribed topics](https://cwiki.apache.org/confluence/display/KAFKA/KIP-211%3A+Revise+Expiration+Semantics+of+Consumer+Group+Offsets#KIP-211:ReviseExpirationSemanticsofConsumerGroupOffsets-UnsubscribingfromaTopic) will be done in a separate PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-04-19T07:43:01Z","2018-06-29T07:23:55Z"
"","5295","MINOR: removed kafka-streams subproject .gitignore","the generated eclipse project gitignore would not be created if an existing gitignore was there  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-06-26T13:43:43Z","2018-09-24T09:49:45Z"
"","5391","KAFKA-7182: SASL/OAUTHBEARER client response missing %x01 seps","The format of the SASL/OAUTHBEARER client response is defined in RFC 7628 Section 3.1 as follows:       kvsep          = %x01      key            = 1*(ALPHA)      value          = *(VCHAR / SP / HTAB / CR / LF )      kvpair         = key ""="" value kvsep      client-resp    = (gs2-header kvsep *kvpair kvsep) / kvsep  ;;gs2-header = See RFC 5801 (Section 4)  The SASL/OAUTHBEARER client response as currently implemented in OAuthBearerSaslClient sends the valid gs2-header ""n,,"" but then sends the ""auth"" key and value immediately after it, like this:  String.format(""n,,auth=Bearer %s"", callback.token().value())  This does not conform to the specification because there is no %x01 after the gs2-header, no %x01 after the auth value, and no terminating %x01. The code should instead be as follows:  String.format(""n,,\u0001auth=Bearer %s\u0001\u0001"", callback.token().value())  Similarly, the parsing of the client response in OAuthBearerSaslServer, which currently allows the malformed text, must also change.  This should be fixed prior to the initial release of the SASL/OAUTHBEARER code in 2.0.0 to prevent compatibility problems.  Signed-off-by: Ron Dagostino   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2018-07-18T21:03:07Z","2018-07-19T16:53:48Z"
"","4544","KAFKA-6534: Enforce a rebalance in the next poll call when encounter task migration","The fix is in two folds:  1) For tasks that's closed in `closeZombieTask`, their corresponding partitions are still in `runningByPartition` so those closed tasks may still be returned in `activeTasks` and `standbyTasks`. Adding guards on the returned tasks and if they are closed notify the thread to trigger rebalance immediately.  2) When triggering a rebalance, un-subscribe and re-subscribe immediately to make sure we are not dependent on the background heartbeat thread timing.  3) Some minor changes on log4j. More specifically, I moved the log entry of `closeZombieTask` to its callers with more context information and the action going to take.  I can re-produce the issue with EosIntegrationTest may hand-code the heartbeat thread to GC, and confirmed this patch fixed the issue. Unfortunately this test cannot be added to AK since currently we do not have ways to manipulate the heartbeat thread in unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-08T00:53:42Z","2020-04-24T23:56:44Z"
"","4827","KAFKA-6748: double check before scheduling a new task after the punctuate call","The first commit is a new test, illustrating the problem. The second commit fixes the problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","fredfp","2018-04-05T10:13:37Z","2018-04-05T21:06:43Z"
"","5359","MINOR: clean up window store interface to avoid confusion","The existing interface and javadoc on window store are confusing and slightly misleading about the roles of the `windowSize` parameter to the store and the `timestamp` parameter of `WindowStore#put`.  * clarify the parameters * deprecate `WindowStore#put()`. This variant is confusing. It's almost certainly not the right thing to use for anyone, since it still inserts the record at a timestamp, and it's (currently) undefined what the timestamp will be. In practice, this is always the record timestamp, which is again, unlikely to be correct for time window stores. If that is the intent, callers can easily supply the record timestamp themselves.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-12T15:33:17Z","2018-08-06T17:32:21Z"
"","4873","Minor: Log the exception thrown by Selector.poll","The error merits careful logging.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-04-14T08:17:33Z","2018-04-16T20:43:54Z"
"","5296","MINOR: avoid ambiguous scala import error in Eclipse","the Eclipse scala compiler complained about FetchRequest being ambiguosly imported  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-06-26T13:55:05Z","2018-06-26T17:44:11Z"
"","5361","MINOR: Updated Quickstart to mention log.dirs","The default server.properties file now contains the log.dirs setting and not log.dir anymore.  Co-authored-by: Mickael Maison  Co-authored-by: Katherine Farmer   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-07-12T18:05:24Z","2018-07-18T11:29:39Z"
"","5460","KAFKA-7247: Update a link to Apache BookKeeper project","The current URL is old and invalid.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","okumin","2018-08-04T14:31:18Z","2018-08-19T04:23:14Z"
"","4946","KAFKA-6836: Upgrade metrics library","The current metrics library which Kafka is using is pretty old (version 2.2.0 from Yammer and now we have 4.X from Dropwizard).  In the latest versions of the Dropwizard library (which comes from Yammer and this is deprecated), there are a lot of bugfixes and new features included which could be interesting for the metrics (ie: Reservoris, support JDK9, etc).  This patch includes the upgrade to this new version of the library so that we could add new features in Kafka metrics.  All tests metrics-related in the project have been updated.  Notice that: * ``yammer-metrics-count`` metric in ``KafkaServer`` and ``ZookeeperConsumerConnector`` has been renamed to ``dropwizard-metrics-count``. * All meters have SECONDS as a rate-unit. Previously, all meters in Kafka had the rate unit in seconds except the meter ``RequestHandlerAvgIdlePercent`` (in ``KafkaRequestHandlerPool``) which now is in seconds as well (before it was in nanosecs). * The property ``eventType`` in JMXReporter MBean doesn't exist anymore. So, all meters don't have this mbean property. * I've done a refactor in the metrics trying to encapsulate a little bit the metrics inside the ``kafka.metrics`` package. * The current metrics library version used is 3.2.6. When dependency with JDK7 is removed, we'll be able to upgrade to version 4.X.  Maybe this needs a KIP?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mmolimar","2018-04-29T16:56:42Z","2019-07-26T23:33:54Z"
"","4884","MINOR: refactor AdminClient ListConsumerGroups API","The current Iterator-based ListConsumerGroups API is synchronous.  The API should be asynchronous to fit in with the other AdminClient APIs.  Also fix some error handling corner cases.","closed","","cmccabe","2018-04-17T00:34:49Z","2019-05-20T18:55:56Z"
"","4512","KAFKA-6513: Corrected how Converters and HeaderConverters are instantiated and configured","The commits for KIP-145 (KAFKA-5142) changed how the Connect workers instantiate and configure the Converters, and also added the ability to do the same for the new HeaderConverters. However, the last few commits removed the default value for the `converter.type` property for Converters and HeaderConverters, and this broke how the internal converters were being created.  This change corrects the behavior so that the `converter.type` property is always set by the worker (or by the Plugins class), which means the existing Converter implementations will not have to do this. The built-in JsonConverter, ByteArrayConverter, and StringConverter also implement HeaderConverter which implements Configurable, but the Worker and Plugins methods do not yet use the `Configurable.configure(Map)` method and instead still use the `Converter.configure(Map,boolean)`.  Several tests were modified, and a new PluginsTest was added to verify the new behavior in Plugins for instantiating and configuring the Converter and HeaderConverter instances.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-02-01T23:46:29Z","2020-10-16T06:05:16Z"
"","4932","Fix small issue in comment","The comment seems not correct when I try to figure out how TimingWheel works  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","QuantumBear","2018-04-26T06:01:01Z","2018-04-26T06:01:01Z"
"","5383","KAFKA-6123: Give client MetricsReporter auto-generated client.id","The client's `MetricsReporter` class currently receives the `client.id` field only if the user has configured it. The auto-generated `client.id` is not passed to the `MetricsReporter`.  - Give auto generated `client.id` as a config override to `MetricsReporter` (same way [KAFKA-4756](https://issues.apache.org/jira/browse/KAFKA-4756) handles it)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2018-07-18T01:37:16Z","2018-10-04T02:47:21Z"
"","4482","HOTFIX: ambiguity issue calling putAll() in scala compilation against JAVA 9","The cause for compilation error in JDK 9.0 was an ambiguity issue in scalac: ``` both method putAll in class Properties of type (x$1: java.util.Map[_, _])Unit and  method putAll in class Hashtable of type (x$1: java.util.Map[_ <: Object, _ <: Object])Unit match argument types (java.util.Properties)       newProps.putAll(props) ``` This pull request fixes this error by avoiding the call to``` putAll ```and instead uses the ```put ```method directly.","closed","","ConcurrencyPractitioner","2018-01-28T03:23:32Z","2018-01-28T13:31:34Z"
"","4613","MINOR: Fix javadoc for consumer offsets lookup APIs which do not block indefinitely","The blocking time for these APIs is bounded by the request timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-22T16:37:44Z","2018-02-22T17:47:13Z"
"","5299","MINOR: Reduce time windows for flaky test","The `StreamStreamJoinIntegrationTest` is a bit flaky.  The failures seem to occur with caching turned off.  With join windows of 10 seconds, it's possible that if one stream experiences some latency there may be values present from the other stream for a join when not expected by the test.  Tested by running streams tests and running the `StreamStreamJoinIntegrationTest` test 25 times.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-06-26T22:43:40Z","2018-07-28T14:59:26Z"
"","4495","KAFKA-6487: ChangeLoggingKeyValueBytesStore does not propagate delete","The `ChangeLoggingKeyValueBytesStore` used to write null to its underlying store instead of propagating the delete, which has two drawbacks: - an iterator will see null values - unbounded memory growth of the underlying in-memory keyvalue store  The fix will just propagate the delete instead of performing put(key, null).   The changes to the tests: - extra test whether the key is really gone after delete by calling the `approximateEntries` on the underlying store. This number is exact because we know the underlying store in the test is of type `InMemoryKeyValueStore` - extra test to check a delete is logged as   (the existing test would also succeed if the key is just absent)  While also updating the corresponding tests of the `ChangeLoggingKeyValueStore` I noticed the class is nowhere used anymore so I removed it from the source code for clarity.","closed","","bartdevylder","2018-01-31T13:29:19Z","2018-02-02T08:19:40Z"
"","4743","MINOR: fix flaky TestUtils functions","TestUtils#produceMessages should always close the KafkaProducer, even when there is an exception.  Otherwise, the test will leak threads when there is an error.  TestUtils#createNewProducer should create a producer with a requestTimeoutMs of 30 seconds by default, not around 10 seconds. This should avoid tests that flake when the load on Jenkins climbs.  Fix two cases where a very short timeout of 2 seconds was getting set.","closed","","cmccabe","2018-03-20T22:07:56Z","2019-05-20T19:06:51Z"
"","5055","MINOR: Ignore test_broker_type_bounce_at_start system test","test_broker_type_bounce_at_start tries to validate that when the controller is down, the streams client will always fail trying to create the topic; with the current behavior of admin client it is actually not always true: the actual behavior depends on the admin client internals as well as when the controller becomes unavailable during the leader assign partitions phase. I'd suggest at least ignore this test for now until the admin client has more stable (personally I'd even suggest removing this test as its coverage benefits is smaller than its introduced issues to me).  Also adding a few more log4j entries as a result of investigating this issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-21T21:38:00Z","2020-04-24T23:54:48Z"
"","4924","MINOR: Disabled flaky DynamicBrokerReconfigurationTest.testAddRemoveSslListener","test until fixed.  https://issues.apache.org/jira/browse/KAFKA-6824 for the build failures due to this test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-04-24T22:15:34Z","2018-04-24T22:24:08Z"
"","4863","MINOR: Disable failing testDescribeConsumerGroupOffsets test case","Temporarily ignore this failing test until the fix is merged in #4856.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-12T18:01:55Z","2018-04-12T18:37:00Z"
"","4513","MINOR: adding system tests for how streams functions with broker faiures","System test for two cases   1. Starting a multi-node streams application with the broker down initially, broker starts and confirm rebalance completes and streams application still able to process records.  2. Multi-node streams app running, broker goes down, stop stream instance(s) confirm after broker comes back remaining streams instance(s) still function  [System Test Results Here ](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-02-01--001.1517522571--bbejeck--MINOR_streams_system_tests_with_broker_failure--e571620/report.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-01T23:51:45Z","2018-02-08T01:21:44Z"
"","4554","MINOR: Add System test for standby task-rebalancing","System test for standby replicas and rebalancing occurs within a reasonable period (20 seconds) the timeout period for verification was increased due to the nature of nature of checking the rebalance.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-09T12:45:59Z","2018-03-05T19:06:33Z"
"","4898","KAFKA-6805: Enable broker configs to be stored in ZK before broker start","Support configuration of dynamic broker configs in ZooKeeper before starting brokers using ConfigCommand. This will allow password configs to be encrypted and stored in ZooKeeper, without requiring clear passwords in server.properties to bootstrap the broker first.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-19T15:47:19Z","2018-06-18T17:28:09Z"
"","4680","Feature/refactor subscription info","SubscriptionInfo implements two versions of encoding/decoding in one class. I have split implementations into classes SubscriptionInfoV1 and SubscriptionInfoV2","open","","pavelpg","2018-03-10T10:38:23Z","2018-03-10T10:38:23Z"
"","4481","KAFKA-6288: Broken symlink interrupts scanning of the plugin path","Submitting a fail safe fix for rare IOExceptions on symbolic links.   The fix is submitted without a test case since it does seem easy to reproduce such type of failures (just having a broken symbolic link does not reproduce the issue) and it's considered pretty low risk.    If accepted, needs to be ported at least to 1.0, if not 0.11  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-01-27T01:21:00Z","2020-10-16T06:05:16Z"
"","4597","MINOR: ignore streams eos tests","Streams EOS tests are always failing so I'm disabling them on 1.1  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2018-02-20T10:53:44Z","2018-02-20T17:26:44Z"
"","5452","MINOR: Require final variables in Streams","Streams code style requires the `final` modifier when applicable. Rather than leaning on the reviewers to notice and nitpitck this issue, we can use checkstyle to enforce it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-02T20:21:57Z","2018-08-03T22:07:39Z"
"","5178","MINOR: fix Vagrant VM setup","Starting up the VMs with `vagrant/vagrant-up.sh` fails with: ``` ==> worker1: + git clone -q https://github.com/confluentinc/kibosh.git ==> worker1: fatal: destination path 'kibosh' already exists and is not an empty directory. ```","closed","","mjsax","2018-06-10T18:51:39Z","2018-06-17T01:36:55Z"
"","4867","KAFKA-6772: Load credentials from ZK before accepting connections","Start processing client connections only after completing KafkaServer initialization to ensure that credentials are loaded from ZK into cache before authentications are processed. Acceptors are started earlier so that bound port is known for registering in ZK.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-13T10:58:36Z","2018-04-18T09:20:28Z"
"","5397","KAFKA-3702: Change log level of SSL close_notify failure","SslTransportLayer currently closes the SSL engine and logs a warning if `close_notify` message canot be sent because the remote end closed its connection. This tends to fill up broker logs, especially when using clients which close connections immediately. Since this log entry is not very useful anyway, it would be better to log at debug level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-19T20:39:51Z","2018-07-20T08:23:02Z"
"","5371","KAFKA-7168: Treat connection close during SSL handshake as retriable","SSL `close_notify` from broker connection close is processed as an `SSLException` while unwrapping the final message when the I/O exception due to remote close is processed. This should be handled as a retriable `IOException` rather than a non-retriable `SslAuthenticationException`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-16T16:23:46Z","2018-07-18T13:48:30Z"
"","4975","KAFKA-6264: Log cleaner thread may die on legacy segment containing messages whose offsets are too large","Split log segments on index offset overflow.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-05-08T00:09:29Z","2018-06-10T20:20:49Z"
"","5435","MINOR: fix typo ""intervall"" to ""interval""","Spelling bee maestro","closed","","mowczare","2018-07-30T16:56:43Z","2020-12-09T12:15:34Z"
"","4702","KAFKA-6647 KafkaStreams.cleanUp creates .lock file in directory it tries to clean","Specify StandardOpenOption#DELETE_ON_CLOSE when creating the FileChannel.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-03-13T18:27:36Z","2018-10-03T23:49:42Z"
"","5275","MINOR: report streams benchmarks separately","Specify each benchmark as a separate test so that we can see the results reported independently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-22T15:52:27Z","2018-06-23T00:10:18Z"
"","4624","KAFKA-6039: Improve task assignor load balance","Sorts TaskIds on first assignment evenly distributing tasks by `topicGroupId` should help with evening the load of work across topologies.  This PR is an initial ""strawman"" approach which will be followed up  (at a later date YTBD) by scoring or assigning weight to processing nodes to ensure even processing distribution.  Added a new test to existing unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-27T14:58:52Z","2018-06-28T23:42:11Z"
"","5201","KAFKA-6761: Construct Physical Plan using Graph, Reduce streams footprint part III","Sorry for the massive PR, but at this point, it's very difficult to break up into smaller parts now that we are building the logical and physical plan. It's worth noting at the moment this PR does not include optimizations to help with the review burden, the 4th PR will include an optimization for repartition topics and re-using source topics as changelogs for KTables    The specific changes in this PR from the second PR include 2. Changed the types of graph nodes to names conveying more context 2. Build the entire physical plan from the graph, after `StreamsBuilder.build()` is called.  Other changes are addressed directly as review comments on the PR.  Testing consists of using all existing streams tests to validate building the physical plan with graph  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-06-12T15:23:09Z","2018-08-01T22:01:33Z"
"","4833","KAFKA-6757 Log runtime exception in case of server startup errors","Sometimes while running Kafka server inside tests of an upstream application it can happen that the server cannot start due to a bad runtime error, like a missing jar on the classpath, see KAFKA-6757 for examples .  I would like KafkaServerStartable to log any 'Throwable' in order to catch these unpredictable error  Testing strategy: no test case is needed or it is worth to add  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","eolivelli","2018-04-06T08:03:50Z","2018-07-30T15:10:49Z"
"","5404","MINOR: Implicit conversion from Long to SessionWindows","Some syntactic sugar on the Scala Streams API with an implicit conversion from Long to Session Windows to use the ```windowedBy(SessionWindows)``` function this way: ```scala stream   .groupBy((_, _) => something)   .windowedBy(5l) ```","closed","","teivah","2018-07-20T10:24:36Z","2018-07-28T14:54:44Z"
"","4689","MINOR: Streams system test fixes/updates","Some changes required to get the Streams system tests working via Docker  To test:  ```bash TC_PATHS=""tests/kafkatest/tests/streams"" bash tests/docker/run_tests.sh ``` That command will take about 3.5 hours, and should pass. Note there are a couple of ignored tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-03-12T18:22:50Z","2018-03-15T21:58:37Z"
"","4859","KAFKA-6775: Fix the issue of without init super class's","Some anonymous classes of `AbstractProcessor` didn't initialize their superclass. This will not set up `ProcessorContext context` at `AbstractProcessor`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jiminhsieh","2018-04-12T11:58:27Z","2018-04-24T13:57:42Z"
"","4542","MINOR: Move processor response queue into Processor","Small refactor which moves the processor response queue into the Processor object itself. This simplifies the logic for dequeuing a response for sending and also eliminates the response listeners collection which was only used to wakeup the Processor after a new response had been enqueued.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-07T21:43:50Z","2018-02-08T20:02:28Z"
"","4630","KAFKA-6054: Code cleanup to prepare the actual fix for an upgrade path","Small change in decoding version 1 metadata: don't upgrade to version 2 automatically","closed","streams,","mjsax","2018-03-01T06:08:35Z","2018-03-05T18:56:49Z"
"","5067","MINOR: Fix transiently failing consumer group admin integration test","Since the producer is using retries=0, we need to await topic creation before sending any records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-22T22:46:29Z","2018-05-22T23:24:55Z"
"","5094","KAFKA-6028: Improve the quota throttle communication (KIP-219)","Since Jon is on vacation and we would like to have KIP-219 in Kafka 2.0 release before he gets back, I am opening this PR to rebase the patch in https://github.com/apache/kafka/pull/5064 and run unit/integration tests. If everthing looks good, we can merge https://github.com/apache/kafka/pull/5064 with the change in this PR.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-05-29T17:06:53Z","2018-05-30T19:21:42Z"
"","4942","MINOR: Column style fix to streams page","Similar changes to this PR for Apache Kafka site - https://github.com/apache/kafka-site/pull/136    *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","manjuapu","2018-04-27T21:13:27Z","2018-04-27T21:20:30Z"
"","5253","KAFKA-7072: clean up segments only after they expire","Significant refactor of Segments to use stream-time as the basis of segment expiration. Previously Segments assumed that the current record time was representative of stream time.  In the event of a ""future"" event (one whose record time is greater than the stream time), this would inappropriately drop live segments. Now, Segments will provision the new segment to house the future event and drop old segments only after they expire.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-06-19T22:01:12Z","2018-06-21T17:33:34Z"
"","5228","KAFKA-7059 Offer new constructor on ProducerRecord","Signed-off-by: Matthias Wessendorf   Done for https://issues.apache.org/jira/browse/KAFKA-7059   When developers are creating a ProducerRecord, with custom headers, it currently requires the usage of a constructor with a slightly longer arguments list.  This is OK, but it would be handy or more convenient if there was a ctor, like:  ```java public ProducerRecord(String topic, K key, V value, Iterable headers) ```","open","","matzew","2018-06-14T14:52:14Z","2018-07-13T09:38:14Z"
"","4885","MINOR: Elaborate meaning of last offset in javadocs","Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-04-17T05:56:16Z","2018-04-20T20:00:11Z"
"","5146","KAFKA-7001: Rename errors.allowed.max property in Connect to errors.tolerance (KIP-298)","Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-06-06T04:58:00Z","2020-10-16T06:32:36Z"
"","5121","MINOR: Setup DLQ only if there are properties with the right prefix","Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-06-01T21:20:13Z","2018-06-02T07:38:06Z"
"","4672","MINOR: Fix deadlock in ZooKeeperClient.close() on session expiry","Shutdown session expiry scheduler without holding lock to avoid deadlock if scheduler thread needs to acquire write lock to process session expiry.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-09T14:45:43Z","2018-03-09T23:20:02Z"
"","5338","KAFKA-6431: Shard purgatory to mitigate lock contention","Shard purgatory and use ReentrantLock instead of ReentrantReadWriteLock  This fix has been deployed to Uber's production environment for several months","closed","","ying-zheng","2018-07-06T00:21:34Z","2019-01-04T00:08:50Z"
"","5198","KAFKA-7043: Modified plugin isolation whitelist with recently added converters (KIP-305)","Several recently-added converters are included in the plugin isolation whitelist, similarly to the `StringConverter`. This is a change in the implementation, and does not affect the approved KIP. Several unit tests were added to verify they are being loaded in isolation, again similarly to `StringConverter`.  These changes should be applied only to `trunk` and `2.0`, since these converters were added as part of KIP-305 for AK 2.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-06-12T02:13:24Z","2020-10-16T06:02:59Z"
"","4949","MINOR: Build and code sample updates for Kafka Streams DSL for Scala","Several build and documentation updates were required after the merge of [KAFKA-6670: Implement a Scala wrapper library for Kafka Streams](https://github.com/apache/kafka/pull/4756).  ## Encode Scala major version into `streams-scala` artifacts.  To differentiate versions of the `kafka-streams-scala` artifact across Scala major versions it's required to encode the version into the artifact name before its published to a maven repository.  This is accomplished by following a similar release process as kafka core, which encodes the Scala major version and then runs the build for each major version of Scala supported.  **This is considered standard practice when releasing Scala libraries**, but is not handled for us automatically with the basic Scala for Gradle support.  After this change you can generate and install the `kafka-streams-scala` artifact into the local maven repository:  ``` $ ./gradlew -PscalaVersion=2.11 install $ ./gradlew -PscalaVersion=2.12 install ```  Which results in the following files generated:  ``` /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12 /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/maven-metadata-local.xml /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT-sources.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/maven-metadata-local.xml /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT-test-sources.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT.pom /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT-test.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT-javadoc.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT-scaladoc.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.12/2.0.0-SNAPSHOT/kafka-streams-scala_2.12-2.0.0-SNAPSHOT.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11 /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/maven-metadata-local.xml /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT-test.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/maven-metadata-local.xml /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT-sources.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT-scaladoc.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT-test-sources.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT-javadoc.jar /home/seglo/.m2/repository/org/apache/kafka/kafka-streams-scala_2.11/2.0.0-SNAPSHOT/kafka-streams-scala_2.11-2.0.0-SNAPSHOT.pom ```  ## Review Code Samples  I reviewed all the code samples introduced by the main PR.  [I created a sample project which validates the main WordCount example](https://github.com/seglo/kafka-streams-scala-example).  The other examples are code snippets instead of whole programs, but are equivalent to existing tests found in the `apache/kafka` `streams-scala` project, or the original `lightbend/kafka-streams-scala` project.  Code sample / documentation references  #### WordCountApplication example  Docs location:  * `/documentation/streams/` (Scala Example) * `/documentation/streams/developer-guide/dsl-api.html#scala-dsl-sample-usage`  See [`WordCountApplication`](https://github.com/seglo/kafka-streams-scala-example/blob/master/src/main/scala/seglo/WordCountApplication.scala#L3..L38) in sample project.  > **NOTE**: The `WordCountApplication` usage of `count` reveals how using the `Materialized` does not take advantage of implicit SerDes as other Kafka Streams operators do (operators that accept a `Serialized`, `Consumed`, `Produced` etc).  This is not an issue when you do not want to provide a user-defined and named state store, but to make this example consistent with the Java examples we use the overload of the `count` API which takes the `Materialized` parameter.  It's required to pass the key SerDes in this example because a global serializer is not defined in the Kafka Streams config.  @debasishg is going to follow up with a proposal to amend this API in a separate thread.  #### Implicit SerDes Example  Docs location:  * `/documentation/streams/developer-guide/dsl-api.html#scala-dsl-implicit-serdes`  See `StreamToTableJoinScalaIntegrationTestImplicitSerdes` test in [`apache/kafka`](https://github.com/apache/kafka/)  https://github.com/apache/kafka/blob/trunk/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala#L77..L102   #### User-defined SerDes Example  Docs location:  * `/documentation/streams/developer-guide/dsl-api.html#scala-dsl-user-defined-serdes`  See `StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro` integration test in [`lightbend/kafka-streams-scala`](https://github.com/lightbend/kafka-streams-scala/).  This test doesn't exist in `apache/kafka` because we didn't want to add the Avro dep.  https://github.com/lightbend/kafka-streams-scala/blob/v0.2.1/src/test/scala/com/lightbend/kafka/scala/streams/StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro.scala#L61..L142  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","seglo","2018-04-30T18:13:07Z","2018-05-07T03:55:25Z"
"","4591","MINOR: Redirect response code in Connect's RestClient to logs instead of stdout","Sending the response code of an http request issued via `RestClient` in Connect to stdout seems like a unconventional choice.   This PR redirects the responds code with a message in the logs at DEBUG level (usually the same level as the one that the caller of `RestClient.httpRequest` uses.   This fix will also fix system tests that broke by outputting this response code to stdout.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-02-19T20:52:00Z","2020-10-16T06:24:16Z"
"","4837","MINOR: Use Scala's `-release` flag if possible (WIP)","Seems like there are some issues with this flag, so this should not be merged yet.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-08T15:18:40Z","2018-04-19T05:51:22Z"
"","4959","KAFKA-6849: add transformValues methods to KTable.","See the KIP:  https://cwiki.apache.org/confluence/display/KAFKA/KIP-292%3A+Add+transformValues%28%29+method+to+KTable  This PR adds the `transformValues` method to the `KTable` interface. The semantics of the call are the same as the methods of the same name on the `KStream` interface.  Fixes KAFKA-6849","closed","streams,","big-andy-coates","2018-05-02T19:42:51Z","2018-05-19T00:57:52Z"
"","4971","KAFKA-6871: KStreams Scala API: incorrect Javadocs and misleading parameter name","See PR title. This is a minor fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","miguno","2018-05-07T10:02:53Z","2018-05-07T16:41:17Z"
"","5257","KAFKA-7080: replace numSegments with segmentInterval","See also KIP-319.  Replace number-of-segments parameters with segment-interval-ms parameters in various places. The latter was always the parameter that several components needed, and we accidentally supplied the former because it was the one available.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2018-06-20T17:24:55Z","2020-06-13T00:03:22Z"
"","5119","KAFKA-6925: fix parentSensors memory leak","See also #5108 / 0a7462e3b6c5b73e836f53e6b4dc7fc1ff23e1b3 .  Previously, we failed to remove sensors from the parentSensors map, effectively a memory leak.  Add a test to verify that removed sensors get removed from the underlying registry as well as the parentSensors map.  Reviewers: Bill Bejeck , Guozhang Wang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-01T20:01:51Z","2018-06-01T20:59:22Z"
"","5120","KAFKA-6925: fix parentSensors memory leak (#5108)","See also #5108 / 0a7462e  and #5119 .  Previously, we failed to remove sensors from the parentSensors map, effectively a memory leak.  Add a test to verify that removed sensors get removed from the underlying registry as well as the parentSensors map.  Reviewers: Bill Bejeck , Guozhang Wang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-01T20:06:23Z","2018-06-03T17:58:35Z"
"","4985","KAFKA-6870 Concurrency conflicts in SampledStat","see [KAFKA-6870](https://issues.apache.org/jira/browse/KAFKA-6870) for more details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-05-09T09:23:06Z","2018-05-10T11:27:46Z"
"","4560","MINOR: Add a new system test for resilience","Rolling hard-kill and restart streams with broker temporarily unavailable and make sure the restarting can still be successful eventually.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-12T22:04:07Z","2020-04-24T23:49:21Z"
"","5317","KAFKA-7120: Add information to indicate which connector resource request cannot be completed","Right now, we throw: throw new ConnectRestException(Response.Status.CONFLICT.getStatusCode(), ""Cannot complete request because of a conflicting operation (e.g. worker rebalance)""); There's no information about WHICH request can't be completed. It will help to know. This PR add the path, method and body of the request to the message field of ConnectRestException to indicate which request can't be completed.","open","connect,","lambdaliu","2018-07-02T09:42:47Z","2020-03-21T23:44:14Z"
"","4724","MINOR: Updated SASL Authentication Sequence Docs","Reworded the SASL Authentication sequence to update it to >= 1.0.0  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-03-16T11:22:44Z","2018-04-18T13:28:08Z"
"","4635","Revert ""KAFKA-6111: Improve test coverage of KafkaZkClient, fix bugs found by new tests""","Reverts apache/kafka#4596","closed","","junrao","2018-03-02T02:02:55Z","2018-07-18T05:33:15Z"
"","4681","MINOR: Use large batches in metrics test so that conversion time >= 1ms","Retry conversion time test with larger batches to make  `MetricsTest.testMetrics` more stable since conversion time may be < 1ms (reported as zero) with small batches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-10T18:16:10Z","2018-03-17T06:54:13Z"
"","4961","KAFKA-6853: ZooKeeperRequestLatencyMs is incorrect","ResponseMetadata.responseTimeMs is always 0 or negative.","closed","","Fuud","2018-05-03T11:24:47Z","2018-05-03T16:46:31Z"
"","4875","KAFKA-6788: Grouping consumer requests per consumer coordinator in admin client","Resolves KAFKA-6788 for the deleteGroup case  For the ""describeGroup"" case, we build a hash-set and `continue` when we have already described the group. The describe API lets us submit all groups so this is the only work we must do.  For the ""deleteGroup"" case, we ask the Coordinator which groups it knows about, and delete those. Also use a hashset to `continue` over groups we have already deleted.  Passes tests, no new testing or documentation necessary.","closed","","cyrusv","2018-04-16T03:37:26Z","2018-04-16T06:22:52Z"
"","4877","Document fix","resolved merge conflicts and link fix for stream dsl","closed","","ro7m","2018-04-16T05:07:15Z","2018-04-16T14:29:18Z"
"","5056","KSTREAMS-1400: Fix link in num.standby.replicas section","Replace broken link with link to [State restoration during workload rebalance](https://docs.confluent.io/current/streams/developer-guide/running-app.html#state-restoration-during-workload-rebalance).","closed","","JimGalasyn","2018-05-21T23:15:40Z","2018-05-22T00:05:58Z"
"","4683","MINOR: Remove code duplication + excessive space","Removing code duplication and excessive space  /cc @mjsax @guozhangwang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jaceklaskowski","2018-03-11T15:37:56Z","2018-03-11T23:19:22Z"
"","4968","KAFKA-5965: Remove Deprecated AdminClient from Streams Resetter Tool","Removed usage of deprecated AdminClient from StreamsResetter No additional tests are required.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","fedosov-alexander","2018-05-04T09:03:48Z","2018-05-14T06:56:27Z"
"","4938","MINOR: Removed unused imports in a few tests","Removed unused imports in: - core/src/test/scala/unit/kafka/network/SocketServerTest.scala - core/src/test/scala/unit/kafka/server/DelegationTokenRequestsOnPlainTextTest.scala - core/src/test/scala/unit/kafka/server/DelegationTokenRequestsWithDisableTokenFeatureTest.scala  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-04-27T16:16:32Z","2018-05-01T22:27:21Z"
"","4906","MINOR: Remove deprecated streams config","Removed the following: ""zookeeper.connect"",  ""key.serde"", ""value.serde"", ""timestamp.extractor""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-20T21:59:08Z","2020-04-24T23:55:04Z"
"","5097","KAFKA-5588: Remove deprecated new-consumer option for tools","Removed the deprecated ""--new-consumer"" option for all consumer based tools. This option was already deprecated in the previous version so it can be removed in the coming new 2.0.0 major version.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ppatierno","2018-05-30T10:40:55Z","2018-06-06T06:45:53Z"
"","4669","MINOR: Remove unused local variable","Remove unused local variable.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jiminhsieh","2018-03-09T05:44:07Z","2018-03-10T04:53:16Z"
"","4708","MINOR: remove unnecessary null checks","Remove unnecessary null check in StringDeserializer, MockProducerInterceptor and KStreamImpl.","closed","","smurakozi","2018-03-14T12:23:15Z","2018-03-17T06:34:17Z"
"","4909","MINOR: Remove KafkaStreams#toString","Remove the deprecated KafkaStreams#toString function. Also override toString() for internal classes for debugging purposes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-21T03:50:51Z","2020-04-24T23:55:06Z"
"","4810","[MINOR] refactor return value","remove if clause to return boolean value","closed","streams,","khaireddine120","2018-04-02T07:27:59Z","2018-04-03T18:46:25Z"
"","5385","KAFKA-4994: Remove findbugs suppression for OffsetStorageWriter","Remove findbugs suppression for OffsetStorageWriter because it is no longer needed.","closed","","cmccabe","2018-07-18T16:38:38Z","2018-07-19T06:05:52Z"
"","5152","KAFKA-7006 - remove duplicate Scala ResourceNameType in preference to…","remove duplicate Scala ResourceNameType in preference to in preference to Java ResourceNameType.  See [KAFKA-7006](https://issues.apache.org/jira/browse/KAFKA-7006).  This is follow on work for KIP-290 and PR #5117, which saw the Scala ResourceNameType class introduced.  I've added tests to ensure AclBindings can't be created with ResourceNameType.ANY or UNKNOWN.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  @junrao, @cmccabe - this is a follow on PR for KIP-290.","closed","","big-andy-coates","2018-06-06T15:28:18Z","2018-06-08T20:48:21Z"
"","4497","MINOR: Fix brokerId passed to metrics reporters","Remove caching of brokerId in DynamicBrokerConfig constructor since it is called before KafkaConfig is fully initialized.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-31T20:26:05Z","2018-02-01T01:59:49Z"
"","5050","KAFKA-6916: Refresh metadata in admin client if broker connection fails","Refresh metadata if broker connection fails so that new calls are sent only to nodes that are alive and requests to controller are sent to the new controller if controller changes due to broker failure. Also reassign calls that could not be sent.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-21T08:03:07Z","2018-05-29T15:37:18Z"
"","4504","MINOR: Refactor cleanupGroupMetadata","Refactoring avoids the need to call this method with a infinity as current time to remove all group offsets (when manually deleting the group).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-02-01T06:28:32Z","2018-02-22T16:19:14Z"
"","4856","KAFKA-6058: Refactor consumer API result return types","Refactored the return types in consumer group APIs the following way:  ``` Map> DeleteConsumerGroupsResult#deletedGroups()  Map> DescribeConsumerGroupsResult#describedGroups()  KafkaFuture> ListConsumerGroupsResult#listings()  KafkaFuture> ListConsumerGroupOffsetsResult#partitionsToOffsetAndMetadata() ```  1. For DeleteConsumerGroupsResult and DescribeConsumerGroupsResult, for each group id we have two round-trips to get the coordinator, and then send the delete / describe request; I leave the potential optimization of batching requests for future work.  2. For ListConsumerGroupOffsetsResult, it is a simple single round-trip and hence the whole map is wrapped as a Future.  3. ListConsumerGroupsResult, it is the most tricky one: we would only know how many futures we should wait for after the first listNode returns, and hence I constructed the flattened future in the middle wrapped with the underlying map of futures.  3.a Another big change I made is, we do not return the exception in the flattened future if only a few of the nodes returns ERROR code, and instead just return the rest of the listings; to use that I added a new `anyOf` function in `KafkaFuture`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-12T01:11:09Z","2020-04-24T23:51:28Z"
"","5095","MINOR: Reduce commit time on test","Reduce on commit time in the test.  The test fails as it can't read all sent messages within 30 seconds, I've reduced commit interval to help mitigate timing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-05-29T19:34:29Z","2018-05-29T23:33:01Z"
"","5354","KAFKA-7142: fix joinGroup performance issues","Reduce joinGroup time complexity from O(N) to O(1)  Trace the number of ""awaiting join members"", and the number of members that support each protocol. So that, we don't need to count the members in GroupMetadata .notYetRejoinedMembers(), and don't need to calculate set intersection in GroupMetadata .supportsProtocols(), making the 2 operations O(1) instead of O(N).","closed","","ying-zheng","2018-07-10T19:45:29Z","2018-08-06T20:21:10Z"
"","4601","COSMETIC : Reading configuration fields from ProducerConfig class","Reading the configuration field names from ProducerConfig class and taking the key and value serializer names from class name directly instead of hardcoding  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","VenkataNU","2018-02-20T16:12:37Z","2018-02-21T16:58:35Z"
"","4970","KAFKA-6867: corrected the typos in upgrade.html","Ran tests, documentation change, so no expected behavioral changes.","closed","","surabhidixit","2018-05-04T21:45:12Z","2018-05-04T22:42:44Z"
"","4869","KAFKA-6765: Handle exception while reading throttle metric value in test","Quota tests wait for throttle metric to be updated without waiting for requests to complete to avoid waiting for potentially large throttle times. This requires the test to read metric values while a broker may be updating the value, resulting in exception in the test. Since this issue can also occur with JMX metrics reporter, change synchronization on metrics with sensors to use the sensor as lock.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-13T13:14:27Z","2018-05-10T11:46:04Z"
"","4569","KAFKA-6536: Adding versions for japicmp-maven-plugin and maven-shade-plugin in","quickstart  *Added versions to japicmp-maven-plugin and maven-shade-plugin  in quickstart pom.xml. Previously these versions were missing and the latest versions of the plugins were automatically being used.  I have explicitly set the version of each plugin to their latest.*","closed","streams,","sawyna","2018-02-14T08:34:24Z","2018-02-17T08:18:07Z"
"","4966","Count fix and Type alias refactor in Streams Scala API","Provide a tiny helper for creating Materialized with implicit Serdes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-03T21:42:15Z","2018-05-11T17:26:03Z"
"","4955","KAFKA-6850: Add Record Header support to Kafka Streams Processor API (KIP-244)","Proposal to add support for Headers on Streams Processor API, discussed on KIP-244.  KIP documentation: https://cwiki.apache.org/confluence/display/KAFKA/KIP-244%3A+Add+Record+Header+support+to+Kafka+Streams","closed","","jeqo","2018-05-02T12:45:48Z","2020-03-20T16:45:51Z"
"","4557","KAFKA-6397: Consumer should not block setting positions of unavailable partitions","Prior to this patch, the consumer always blocks in poll() if there are any partitions which are awaiting their initial positions. This behavior was inconsistent with normal fetch behavior since we allow fetching on available partitions even if one or more of the assigned partitions becomes unavailable _after_ initial offset lookup. With this patch, the consumer will do offset resets asynchronously, which allows other partitions to make progress even if the initial positions for some partitions cannot be found.  I have added several new unit tests in `FetcherTest` and `KafkaConsumerTest` to verify the new behavior. One minor compatibility implication worth mentioning is apparent from the change I made in `DynamicBrokerReconfigurationTest`. Previously it was possible to assume that all partitions had a fetch position after `poll()` completed with a non-empty assignment. This assumption is no longer generally true, but you can force the positions to be updated using the `position()` API which still blocks indefinitely until a position is available.  Note that this this patch also removes the logic to cache committed offsets in `SubscriptionState` since it was no longer needed (the consumer's `committed()` API always does an offset lookup anyway). In addition to avoiding the complexity of maintaining the cache, this avoids wasteful offset lookups to refresh the cache when `commitAsync()` is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-11T22:40:35Z","2018-02-14T18:52:47Z"
"","5108","KAFKA-6925: fix parentSensors memory leak","Previously, we failed to remove sensors from the `parentSensors` map, effectively a memory leak.  * Add a test to verify that removed sensors get removed from the underlying registry as well as the `parentSensors` map.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-05-31T16:51:45Z","2018-06-01T17:11:13Z"
"","5277","MINOR: bugfix streams total metrics","Previously, we erroneously summed the invocation metrics instead of counting them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-22T17:13:20Z","2018-06-23T00:15:23Z"
"","5407","KAFKA-7109: Close cached fetch sessions in the broker on consumer close","Previously, the consumer's incremental fetch sessions would time out once the consumer was gone.","closed","","stanislavkozlovski","2018-07-20T17:44:56Z","2022-02-08T12:58:40Z"
"","5301","KAFKA-6809: Count inbound connections in the connection-creation metric","Previously, the connection-creation metric only accounted for opened connections **from** the broker. This change extends it to account for received connections.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2018-06-27T13:09:17Z","2018-06-28T13:15:16Z"
"","5311","KAFKA-7028: Properly authorize custom principal objects","Previously, it would compare two different classes `KafkaPrincipal` and the custom class, which would always return false because of the implementation of `KafkaPrincipal#equals`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2018-06-29T09:48:12Z","2018-06-29T13:13:46Z"
"","5443","MINOR: Use explicit construction of clients in IntegrationTestHarness","Pre-initialization of clients in IntegrationTestHarness is a cause of significant confusion and additionally has resulted in a bunch of inconsistent client creation patterns. This patch requires test cases to create needed clients explicitly and makes the creation logic more consistent.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-01T23:44:00Z","2018-08-14T21:34:43Z"
"","4515","MINOR: Removed explicit class checks for PolicyViolationException","PolicyViolationException is a subclass of ApiException,  and the handling was identical  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-02-02T16:48:07Z","2018-02-05T23:30:31Z"
"","4646","[KAFKA-6049] Kafka Streams: Add Cogroup in the DSL","Picking up Kyle's PR.","closed","streams,","ConcurrencyPractitioner","2018-03-05T03:39:55Z","2018-05-12T18:23:33Z"
"","4695","KAFKA-3978; highwatermark should always be positive","Partition highwatermark may become -1 during partition reassignment. The bug was fixed and validated with unit test in this patch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-13T00:28:52Z","2018-03-16T16:52:49Z"
"","5369","KAFKA-7222: Add Windows grace period","Part I of KIP-238: * add grace period to Windows * deprecate retention/maintainMs and segmentInterval from Windows * record expired records in the store with a new metric * record late record drops as a new metric instead of as a ""skipped record""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-16T02:04:46Z","2018-08-20T23:15:50Z"
"","4501","KAFKA-6519: Reduce log level for normal replica fetch errors","Out of range errors are common in replica fetchers and not an indication of a problem. Therefore we should reduce the log level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-01-31T22:41:12Z","2018-02-08T00:20:18Z"
"","4543","HOTFIX: broken javadoc links in streams web docs","Our javadoc links in developer guide are broken, inspired by https://github.com/apache/kafka-site/pull/127.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-02-07T21:45:17Z","2020-04-24T23:48:00Z"
"","4888","HOTFIX: use the new prop object in SimpleBenchmark","Otherwise we would get NPE as the new prop does not have the bootstrap server values.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-17T21:29:59Z","2020-04-24T23:48:03Z"
"","4794","Minor: auto-commit-offset doesn't execute after the deadline","one line fix. The callback may take some time to execute so updating the current time is necessary.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-03-29T12:42:59Z","2019-02-24T07:51:25Z"
"","4709","MINOR: Correct ""exeption"" to ""exception""","one line change","closed","","chia7712","2018-03-14T13:26:20Z","2018-11-06T06:25:16Z"
"","4651","KAFKA-6106: Postpone normal processing of tasks until restoration of all tasks completed","Once all the state stores are restored, then the processing of tasks takes place. This approach will reduce the time taken to restore the state stores as single thread is used to restore the state and process the task.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kamalcph","2018-03-06T05:02:48Z","2018-03-16T06:51:55Z"
"","4632","MINOR: Fix incorrect JavaDoc (type mismatch)","Omitting the type would lead to a compilation error, because if the method is not parametrized, the type parameter of Serde would be Object, instead of Long and String.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Detharon","2018-03-01T18:39:53Z","2018-03-12T23:32:20Z"
"","4700","KAFKA-6632: Very slow hashCode methods in Kafka Connect types","Objects.hash function is very slow method to calculate hashCode. This PR change it to more efficient implementation.  Tests. 1) Simple String Schema * Original hashCode: 2995ms * My implementation: 517ms 2) Struct Schema * Original hashCode: 18880 * My implementation: 6026  In real world application this PR increase throughput of Kafka Connect by 20%.   ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","","maver1ck","2018-03-13T10:43:01Z","2018-10-03T13:05:33Z"
"","5158","MINOR: Remove APIs deprecated in 0.11.0 for core and clients","Not included: old consumers and checksum methods  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-07T06:52:26Z","2018-06-09T23:12:13Z"
"","5293","KAFKA-6949; alterReplicaLogDirs() should grab partition lock when accessing log of the future replica","NoSuchElementException will be thrown if ReplicaAlterDirThread replaces the current replica with future replica right before the request handler thread executes `futureReplica.log.get.dir.getParent` in the ReplicaManager.alterReplicaLogDirs(). The solution is to grab the partition lock when request handler thread attempts to check the destination log directory of the future replica.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-06-26T06:43:30Z","2018-06-26T16:50:22Z"
"","5081","KAFKA-6949; alterReplicaLogDirs() should grab partition lock when accessing log of the future replica","NoSuchElementException will be thrown if ReplicaAlterDirThread replaces the current replica with future replica right before the request handler thread executes `futureReplica.log.get.dir.getParent` in the ReplicaManager.alterReplicaLogDirs(). The solution is to grab the partition lock when request handler thread attempts to check the destination log directory of the future replica.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-05-26T00:52:40Z","2018-06-26T16:50:42Z"
"","5083","MINOR: Remove MaxPermSize from gradle.properties","No longer needed since we dropped support for Java 7.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-26T08:02:13Z","2018-09-11T06:46:59Z"
"","4596","KAFKA-6111: Improve test coverage of KafkaZkClient, fix bugs found by new tests","New test cases and checks were added to cover most of the functionality in  KafkaZkClient.  The new tests found two issues: - deleteLogDirEventNotifications used wrong paths when it attempted to delete notifications - updateBrokerInfoInZk did not throw an exception if the update was not successful.  These issues are also fixed in this PR. New tests were added, they discovered the issues mentioned above.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","smurakozi","2018-02-20T09:45:35Z","2018-03-05T08:09:51Z"
"","4807","KAFKA-6733: Support of printing additional ConsumerRecord fields in DefaultMessageFormatter","New supported fields: offset, partition and headers. Code is backward compatible, so output will remain the same if user won't specify new configuration properties. Added unit tests to DefaultMessageFormatter which cover previous and new properties.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","Matzz","2018-04-01T22:14:34Z","2020-10-27T05:32:05Z"
"","4642","MINOR: Complete inflight requests in correct order on disconnect","NetworkClient should use FIFO order when completing inflight requests following a disconnect.  I've added new unit tests for `InFlightRequests` and `NetworkClient` which verify completion order.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-03T07:07:30Z","2018-03-06T00:48:06Z"
"","5025","MINOR: Fix broken links in streams doc","Need to be cherry-picked to older versions as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-05-16T19:37:48Z","2020-04-24T23:50:37Z"
"","4950","KAFKA-6844: Call shutdown on GlobalStreamThread after all StreamThreads have stopped","Moved the shutdown of `GlobalStreamThread` to after all `StreamThread` instances have stopped.   There can be a race condition where shut down is called on a `StreamThread` then shut down is called on a GlobalStreamThread, but if StreamThread is delayed in shutting down, the GlobalStreamThread can shutdown first. If the StreamThread tries to access a GlobalStateStore before closing the user can get an exception stating  ""..Store xxx is currently closed ""    Tested by running all current streams tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-05-01T22:56:07Z","2018-05-04T22:18:45Z"
"","5125","KAFKA-6981: Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes","Move the error handling configuration properties into the ConnectorConfig and SinkConnectorConfig classes, and refactor the tests and classes to use these new properties.  Testing: Unit tests and running the connect-standalone script with a file sink connector.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-06-02T03:07:30Z","2020-10-16T06:02:57Z"
"","4848","MINOR: Move creation of quota callback to ensure single instance","Move creation of quota callback instance out of KafkaConfig constructor to `QuotaFactory.instantiate` to avoid creating a callback instance for every KafkaConfig since we create temporary KafkaConfigs during dynamic config updates.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-10T13:50:57Z","2018-04-10T19:42:08Z"
"","5051","KAFKA-6926: Simplified some logic to eliminate some suppressions of NPath complexity checks","Modified several classes' `equals` methods and simplified a complex method to reduce the NPath complexity so they could be removed from the checkstyle suppressions that were required with the [recent move to Java 8 and upgrade of Checkstyle](https://github.com/apache/kafka/pull/5046).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-05-21T16:38:47Z","2018-09-13T04:21:47Z"
"","5425","MINOR: Producers should set delivery timeout instead of retries","MirrorMaker should set `delivery.timeout.ms` instead of `retries` now that we have KIP-91.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-07-26T16:20:22Z","2018-08-01T18:04:17Z"
"","4559","MINOR: Use enum for close mode in Selector instead of two booleans","Minor follow-up update for KAFKA-6529  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-12T16:51:11Z","2018-02-13T18:19:34Z"
"","4926","Minor: Fix MetadataTest.testMetadata","MetadataTest.testMetadata fails frequently on our jenkins. The error message is shown below.  `java.lang.AssertionError: Exception in background thread : org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 500 ms. expected null, but was:`  The failure is caused by taking too much time to create the Cluster. Given the test always uses the identical cluster object, moving the creation out of loop shouldn't break the previous behavior. This PR also makes some collection have precise initial size.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-04-25T09:03:37Z","2018-06-20T03:41:33Z"
"","5264","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T14:25:31Z","2018-06-21T17:02:06Z"
"","5268","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T14:37:56Z","2018-06-21T16:51:57Z"
"","5267","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T14:36:58Z","2018-06-21T18:03:35Z"
"","5266","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T14:27:31Z","2018-06-21T16:50:30Z"
"","5265","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T14:26:11Z","2018-06-21T17:25:23Z"
"","5263","MINOR: Upgrade to Gradle 4.8.1","Maven Central dropped support for all versions but TLS 1.2, so dependency resolution fails if Gradle builds run with JDK 7. 2.0 and trunk require JDK 8, but every other version is affected. Gradle 4.8.1 fixes the issue by enabling TLS 1.2 by default even when JDK 7 is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-21T13:59:47Z","2018-06-21T15:27:14Z"
"","5107","KAFKA-5697: Use nonblocking poll in Streams","Make use of the new `Consumer#poll(Duration)` to avoid getting stuck in `poll` when the broker is unavailable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-05-31T15:57:12Z","2018-06-10T00:20:53Z"
"","5310","MINOR: Make the constructor of InMemoryKeyValueLoggedStore public","Make the constructor of InMemoryKeyValueLoggedStore public so that it can be re-used by custom (in-memory) stores  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","hashangayasri","2018-06-29T06:44:27Z","2018-07-17T18:42:44Z"
"","4710","KAFKA-6658: Fix RoundTripWorkload and make k/v generation configurable","Make PayloadGenerator an interface which can have multiple implementations: constant, uniform random, sequential.  Allow different payload generators to be used for keys and values.  This change fixes RoundTripWorkload.  Previously RoundTripWorkload was unable to get the sequence number of the keys that it produced.","closed","","cmccabe","2018-03-14T18:25:10Z","2019-05-20T18:57:51Z"
"","4956","KAFKA-3665: Enable TLS hostname verification by default (KIP-294)","Make HTTPS the default ssl.endpoint.identification.algorithm  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-02T13:42:14Z","2018-06-05T11:08:14Z"
"","5129","MINOR: Correct versioning scheme reference in API doc","Major version example in documentation of InterfaceStability.Stable should follow current versioning scheme (major is first digit instead of 0.m)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrasbeni","2018-06-04T07:12:44Z","2018-11-30T11:31:34Z"
"","4785","KAFKA-6718: Rack Aware Replica Task Assignment","Machines in data centre are sometimes grouped in racks. Racks provide isolation as each rack may be in a different physical location and has its own power source. When tasks are properly replicated across racks, it provides fault tolerance in that if a rack goes down, the remaining racks can continue to serve traffic.   This feature is already implemented at Kafka KIP-36 but we needed similar for task assignments at Kafka Streams Application layer.    This features enables replica tasks to be assigned on different racks for fault-tolerance. NUM_STANDBY_REPLICAS = x totalTasks = x+1 (replica + active)  1. If there are no rackID provided: Cluster will behave rack-unaware 2. If same rackId is given to all the nodes: Cluster will behave rack-unaware 3. If (totalTasks >= number of racks), then Cluster will be rack aware i.e. each replica task is each assigned to a different rack. Among the available racks, it'll further preserve stickiness as well as select least loaded if not stiky.  4. If (totalTasks < number of racks), then it will first assign tasks on different racks, further tasks will be assigned to least loaded node, cluster wide.","closed","streams,","deegoy2","2018-03-28T11:02:21Z","2019-11-10T07:05:45Z"
"","4840","MINOR: FileRecords.readInto should return Unit","Looking at the usages, it also seemed to add ambiguity.  No behaviour change, so no additional tests needed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-09T07:06:36Z","2018-09-11T06:47:14Z"
"","4644","MINOR: Skip sending fetches/offset lookups when awaiting the reconnect backoff","Logging can get spammy during the reconnect blackout period because any requests we send to `ConsumerNetworkClient` will immediately be failed when `poll()` returns. This patch checks for connection failures prior to sending fetches and offset lookups and skips sending to any failed nodes. Test cases added for both.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","hachikuji","2018-03-04T07:07:21Z","2018-03-08T11:00:33Z"
"","4692","KAFKA-6264: Log cleaner thread may die on legacy segment containing messages whose offsets are too large","Log segments created before the patch for KAFKA-5413 could have messages with offsets greater than baseOffset + Integer.MAX_VALUE. This could cause the offset and time index to overflow, and could also cause various other side effects (for example, compaction could fail for such segments).  This patch includes a workaround for this issue by loosening some checks when trying to append to the index, or when the log cleaner is trying to clean such a segment. We now allow the cleaner to continue processing such segments, but make sure we do not append an index entry corresponding to messages that could cause index overflow. Because we only skip append for the last bit of the segment that causes overflow, from a correctness point-of-view, skipping append to offset and time index should be okay.  However, we do have an implicit assumption that time index contains (maximum_timestamp_in_log, offset_of_maximum_timestamp) as its last entry on clean shutdown / log close -- this assumption would not always remain true with this patch. This means, on log open, in addition to reading the last entry in the time index, we also need to scan all messages after the offset in the last entry to make sure we know the exact (maximum_timestamp_in_log, offset_of_maximum_timestamp).  Note that we continue maintaining the time and offset index accurately for segments not affected by KAFKA-5413.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-03-12T21:31:55Z","2018-07-23T21:45:23Z"
"","4962","KAFKA-6854: Handle batches deleted during log cleaning of logs with txns","Log cleaner grows buffers when `result.messagesRead` is zero. This contains the number of filtered messages read from source which can be zero when transactions are used because batches may be discarded. Log cleaner incorrectly assumes that messages were not read because the buffer was too small and attempts to double the buffer size unnecessarily, failing with an exception if the buffer is already `max.message.bytes`. Additional check for discarded batches has been added to avoid growing buffers when batches are discarded.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-03T16:17:14Z","2018-05-03T20:05:36Z"
"","4760","MINOR: Depend on streams:test-utils for streams and examples tests","Link :streams:test-utils as a test dependency for :streams and :streams:examples, avoiding a circular dependency.  Testing strategy: the project should build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-03-23T00:09:53Z","2018-03-28T18:51:25Z"
"","4879","MINOR: Add line break so example command is readable without scrolling","Line explaining how to remove topic level config is too long, thus the containing div becomes scrollable. Depending on browser this is either clumsy or invisible.  Change-Id: If79515da36df6fcb24b6429ab325aaede0e589e3  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrasbeni","2018-04-16T08:14:02Z","2018-04-18T14:24:07Z"
"","5269","MINOR: Cleanup threads in integration tests","Leftover threads doing network I/O can interfere with subsequent tests. Add missing shutdown in tests and include admin client in the check for leftover threads.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-21T21:48:45Z","2018-06-22T14:41:53Z"
"","5302","KAFKA-7104: Handle leader's log start offset beyond last fetched offset","Leader replica may return log start offset in the fetch response that is higher then last fetched offset. This may happen if the log start offset on the leader moves while the fetch response is being build (eg., due to rolling and deleting old segments, or deleting records). This PR limits setting log start offset on the follower to its LEO.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-27T15:21:31Z","2018-06-28T00:17:14Z"
"","5037","MINOR: Remove unused class","KTableKTableLeftJoinValueGetter.java has been moved into `KTableKTableLeftJoin` and the class itself can be removed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-05-18T16:21:01Z","2020-04-24T23:38:52Z"
"","5439","KAFKA-7215: Improve LogCleaner Error Handling","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-346+-+Improve+LogCleaner+behavior+on+error JIRA: https://issues.apache.org/jira/browse/KAFKA-7215","closed","","stanislavkozlovski","2018-08-01T15:32:08Z","2018-11-24T21:29:29Z"
"","5334","KAFKA-6751: Support dynamic configuration of max.connections.per.ip/max.connections.per.ip.overrides configs (KIP-308)","KIP-308: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=85474993 ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-04T10:17:39Z","2018-08-10T02:39:52Z"
"","5222","KAFKA-7056: Moved Connect’s new numeric converters to runtime (KIP-305)","KIP-305 added numeric converters to Connect, but these were added in Connect’s API module in the same package as the `StringConverter`. This commit moves them into the Runtime module and into the `converters` package where the `ByteArrayConverter` already lives. These numeric converters have not yet been included in a release, and so they can be moved without concern.  All of Connect’s converters must be referenced in worker / connector configurations and are therefore part of the API, but otherwise do not need to be in the “api” module as they do not need to be instantiated or directly used by extensions. This change makes them more similar to and aligned with the `ByteArrayConverter`.  It also gives us the opportunity to move them into the “api” module in the future (keeping the same package name), should we ever want or need to do so. However, if we were to start out with them in the “api” module, we would never be able to move them out into the “runtime” module, even if we kept the same package name. Therefore, moving them to “runtime” now gives us a bit more flexibility.  This PR moves the unit tests for the numeric converters accordingly, and updates the `PluginsUtil` and `PluginUtilsTest` as well.","closed","connect,","rhauch","2018-06-14T03:37:20Z","2020-10-16T06:03:00Z"
"","5297","MINOR: Use kill_java_processes when killing ConsoleConsumer process","kill_java_processes uses jcmd instead of grep for find pids, which is more reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-26T17:29:13Z","2018-06-29T16:19:23Z"
"","4524","KAFKA-6532: Reduce impact of delegation tokens on public interfaces","Keep delegation token implementation internal without exposing implementation details to pluggable classes:   1. KafkaPrincipal#tokenAuthenticated must always be set by SaslServerAuthenticator so that custom PrincipalBuilders cannot override.   2. Replace o.a.k.c.security.scram.DelegationTokenAuthenticationCallback with a more generic ScramExtensionsCallback that can be used to add more extensions in future.   3. Separate out ScramCredentialCallback (KIP-86 makes this a public interface) from delegation token credential callback (which is internal).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-05T11:51:14Z","2018-02-07T11:29:09Z"
"","4734","MINOR: KafkaFutureImpl#addWaiter should be protected","KafkaFutureImpl#addWaiter should be protected, just like KafkaFuture#addWaiter.  As described in KIP-218, whenComplete is the public API, not addWaiter.","closed","","cmccabe","2018-03-19T17:38:40Z","2019-05-20T19:06:31Z"
"","4659","KAFKA-6617; Improve controller performance by batching reassignment znode write operation","KafkaController currently writes reassignment znode once for every partition that has been successfully reassigned. This is unnecessary and controller should be able to update reassignment znode once to remove all partitions that have been reassigned from the reassignment znode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-07T06:28:34Z","2018-05-30T19:21:15Z"
"","5210","KAFKA-6986: Export Admin Client metrics through Stream Threads","KAFKA-6986:Export Admin Client metrics through Stream Threads  We already exported producer and consumer metrics through KafkaStreams class:  https://github.com/apache/kafka/pull/4998  It makes sense to also export the Admin client metrics.  I didn't add a separate unittest case for this. Let me know if it's needed.   This is my first contribution, feel free to point out any mistakes that I did.  @abbccdda","closed","streams,","shunge","2018-06-13T07:56:02Z","2018-06-29T20:48:38Z"
"","5106","KAFKA-6973: TopicCommand should verify topic-level config","KAFKA-6973: https://issues.apache.org/jira/browse/KAFKA-6973  Specifying an invalid config other than `CreateTime` or `LogAppendTime` will cause broker restarting to fail.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-05-31T08:08:22Z","2018-06-01T14:12:42Z"
"","5090","MINOR: Remove findbugs exclusion matching removed old producer","KAFKA-6921 removed deprecated scala producer. This pull request aims to remove now unnecessary findbugs exclusion that matched one of the affected classes.  Tested by running `gradle :core:findbugsMain`. No problems reported.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrasbeni","2018-05-28T10:05:17Z","2018-05-28T19:11:51Z"
"","4902","KAFKA-6791: Add CoordinatorNodeProvider in KafkaAdminClient","KAFKA-6791: Add CoordinatorNodeProvider in KafkaAdminClient https://issues.apache.org/jira/browse/KAFKA-6791  Add CoordinatorNodeProvider interface to support batch retrieval for group coordinators.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-04-20T03:41:11Z","2018-05-22T01:03:07Z"
"","4860","KAFKA-6783/6784: consumer poll(timeout) blocked infinitely when no available bootstrap server, FindCoordinatorResponse cannot be cast to FetchResponse","KAFKA-6783: consumer poll(timeout) blocked infinitely when no available bootstrap server KAFKA-6784: FindCoordinatorResponse cannot be cast to FetchResponse","closed","","koqizhao","2018-04-12T15:02:08Z","2018-04-12T15:10:18Z"
"","4861","KAFKA-6783: consumer poll(timeout) blocked infinitely","KAFKA-6783: consumer poll(timeout) blocked infinitely when no available bootstrap server","closed","","koqizhao","2018-04-12T15:23:52Z","2018-05-30T02:24:24Z"
"","4783","KAFKA-6716: Should close the `discardChannel` in completeSend","KAFKA-6716: Should close the `discardChannel` in completeSend https://issues.apache.org/jira/browse/KAFKA-6716  Should close the `discardChannel` in MockSelector#completeSend  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-03-27T23:04:00Z","2018-03-28T22:36:29Z"
"","4759","KAFKA-6696 Trogdor should support destroying tasks","KAFKA-6696 Trogdor should support destroying tasks  Implement destroying tasks and workers.  This means erasing all record of them on the Coordinator and the Agent. Workers should be identified by unique 64-bit worker IDs, rather than by the names of the tasks they are implementing.  This ensures that when a task is destroyed and re-created with the same task ID, the old workers will be not be treated as part of the new task instance.      Fix some return results from RPCs.  In some cases RPCs were returning values that were never used.  Attempting to re-create the same task ID with different arguments should fail.  Add RequestConflictException to represent HTTP error code 409 (CONFLICT) for this scenario.  If only one worker in a task stops, don't stop all the other workers for that task, unless the worker that stopped had an error.","closed","","cmccabe","2018-03-22T21:55:20Z","2018-04-16T07:51:34Z"
"","4645","KAFKA-5660 Don't throw TopologyBuilderException during runtime","KAFKA-5660 Don't throw TopologyBuilderException during runtime  Created this PR and after closing https://github.com/apache/kafka/pull/4605 due to conflicts.","closed","streams,","nafshartous","2018-03-04T21:34:32Z","2018-03-07T01:49:34Z"
"","5462","MINOR: increase AdminClient retries for integration tests","Kafka Streams join integration tests are flaky recently and timeout during topic deletion. Maybe the delete request timeout in Jenkins. Hope increasing retries if admin client will fix the issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-08-05T20:03:12Z","2018-08-15T18:56:18Z"
"","5011","improve kafka client sensor registration performance by lazily calculating JMX attributes","kafka re-registers its sensor MBean on any sensor change (addition/removal of sensors). kafka also has per-topic-partition sensors, and the mbean attribute array size is a multiple of those. on large assignment sets (~35K topic partitions assigned to a single consumer), we've seen sensor registration code take 5 entire consecutive minutes (!!) of CPU time.  the offending code path is in (re)registering the MBean, which triggers this code (called by `DefaultMBeanServerInterceptor.registerMBean()`) ```java     private static String getNewMBeanClassName(Object mbeanToRegister)             throws NotCompliantMBeanException {         if (mbeanToRegister instanceof DynamicMBean) {             DynamicMBean mbean = (DynamicMBean) mbeanToRegister;             final String name;             try {                 name = mbean.getMBeanInfo().getClassName(); <----- THIS             } catch (Exception e) {                 // Includes case where getMBeanInfo() returns null                 NotCompliantMBeanException ncmbe =                     new NotCompliantMBeanException(""Bad getMBeanInfo()"");                 ncmbe.initCause(e);                 throw ncmbe;             }             if (name == null) {                 final String msg = ""MBeanInfo has null class name"";                 throw new NotCompliantMBeanException(msg);             }             return name;         } else             return mbeanToRegister.getClass().getName();     } ``` this triggers the creation of the (big) attribute array, while the caller really only wants the mbean name.  this patch delays the array creation to the time when the mbean attributes are actually queried - which may be never (in case no one is even looking at the jmx sensors).  in local testing this removes a ~5 minute delay in rebalancing/assigning large groups of topic partitions.","closed","","radai-rosenblatt","2018-05-12T15:50:32Z","2018-10-08T17:00:17Z"
"","5072","KIP-228 Negative record timestamp support","Kafka does not support negative record timestamps, and this prevents the storage of historical data in Kafka. In general, negative timestamps are supported by UNIX system timestamps.  In general, there should be is no reason to fail on broker/client side working with negative timestamps.  ### Committer Checklist - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chuhlomin","2018-05-23T18:17:44Z","2018-05-23T18:17:55Z"
"","4626","MINOR: remove unnecessary semicolon","Just removing an unnecessary semicolon.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-02-27T18:31:30Z","2018-08-23T13:10:28Z"
"","4776","Update Jackson to 2.9.5","Just another bug fix release in the 2.9.x series.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-26T20:57:59Z","2018-09-11T06:47:30Z"
"","4881","MINOR: Mention that -1 disables retention by time","Just a doc clarification so no tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-16T16:45:22Z","2018-09-11T06:47:17Z"
"","4573","Propose fix some typos","Just a doc change  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","docs,","jeis2497052","2018-02-15T15:51:07Z","2018-10-21T02:42:52Z"
"","4656","Create a new tool for bringing up soak clusters","Json serialization changes for Trogdor and SoakTool: * Skip serializing empty or null fields * When deserializing, enforce that all fields are non-null  Trogdor AgentClient, CoordinatorClient, JsonRestServer#httpRequest changes: * The Trogdor Agent and Client should have constants for the default port. * Support configuring the log object which the client uses * Add a builder  Trogdor WorkerManager, NodeManager, TaskManager changes: * Worker status should be a JsonNode, not just a string * TaskManager should track the status of all workers, and combine that into a complete task status. * Be more verbose when handling exceptions in TaskWorker#start  Trogdor WorkerUtils#createTopics changes: * Retry after getting InvalidReplicationFactorException, to avoid racing with broker startup. * Add a sleep in the topic creation retry loop. * Increase topic creation timeout  Trogdor ProduceBenchWorker test changes: * The background Executor should have more than 1 thread, so that the periodic StatusUpdater task can run while the other test tasks are running.  SoakTool cloud directory: * Interfaces for bringing up cloud nodes and running remote commands there  SoakTool cluster directory: * Objects which represent the cluster as a whole  soak/common/JsonTransformer: performs string search and replace JSON objects.  soak/common/NullOutputStream: an output stream which drops all its output.  soak/common/SoakConfig: soak tool configuration keys  soak/common/SoakLog: a log object which implements slf4j interfaces, and can redirect output to a file, stdout, or another slf4j object.  soak/common/SoakUtil: utility functions  SoakTool tool directory: * Soak tool entry point. * Soak tool subcommand implementations.  SoakTool role directory: * ActionScheduler: schedules a directed acyclic graphs of dependencies on multiple nodes. * Role: roles which a node in the cluster can have.  Roles imply actions.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cmccabe","2018-03-06T23:18:17Z","2019-05-20T18:55:07Z"
"","5410","KAFKA-5682: Include partitions in exceptions raised during consumer record deserialization/validation","JIRA: [KAFKA-5682](https://issues.apache.org/jira/browse/KAFKA-5682) KIP: [KIP-334](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=87297793) Mailing Thread: [thread](http://mail-archives.apache.org/mod_mbox/kafka-dev/201807.mbox/%3CCANZZNGx2jq9Xi9nxYQ6tW%2B_orkF3enOmGN13Jqgh9k5DLJLgwA%40mail.gmail.com%3E)  * Introduce new Exception - `RecordDeserializationException` - it extends `SerializationException` for backwards compatibility. Throw that exception where appropriate with attached partition/offset. * Introduce new Exception - `InoperativeRecordException` - it extends `KafkaException` for backwards compatibility. Throw that exception where appropriate with attached partition/offset. * Introduce new Interface - `UnconsumableRecordException`. `InoperativeRecordException` and `RecordDeserializationException` implement it * Make `InvalidRecordException` extend `ApiException`. It previously extended `CorruptRecordException` and thrown only for corrupt record scenarios. (as it's not in a public package). * Add some tests for `KafkaConsumer#poll()`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2018-07-21T01:09:12Z","2019-10-11T17:12:53Z"
"","4951","KAFKA-6845: Shrink size of docker image","Jira issue: https://issues.apache.org/jira/browse/KAFKA-6845  Proposing a very small change to slightly reduce the size of the Docker image:  - Adding `--no-cache-dir` flag to all `pip install` commands to prevent caching of build artifacts  When building, I'm seeing an ~20 MB reduction in image size. (1.76 GB -> 1.74 GB)","open","","jayqi","2018-05-01T23:47:05Z","2018-05-01T23:47:05Z"
"","4925","MINOR: use jdk8 to build/run system tests","java 7 is way past expired and the debian installer packages are no longer available.  Also upgrade ami to latest ubuntu/trusty 14 amd64 as old one is not available.  Tested by running ./vagrant/vagrant-up.sh successfully.","closed","","jarekr","2018-04-25T06:59:17Z","2018-04-28T00:14:50Z"
"","4678","MINOR: Revert to ZooKeeper 3.4.10 due to ZOOKEEPER-2960","It's a critical bug that only affects the server, but we don't have an easy way to use 3.4.11 for the zookeeper client only.  For reference the upgrade to ZooKeeper 3.4.11 was done via 2652565d429138c58.  Testing strategy: relying on existing tests and reverted a change to a test to pass with 3.4.10.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-10T00:38:43Z","2018-03-12T11:41:04Z"
"","5058","MINOR: Remove unnecessary conditional in KafkaAdminClient to fix checkstyle","It was introduced in 70a506b98 shortly before e70a191d303 was merged. The latter updates checkstyle and the new version is stricter.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-22T06:52:33Z","2018-05-22T14:38:50Z"
"","5172","MINOR: Remove deprecated per-partition lag metrics","It takes O(n^2) time to instantiate a mbean with n attributes which can be very slow if the number of attributes of this mbean is large. This PR removes metrics whose number of attributes can grow with the number of partitions in the cluster to fix the performance issue. These metrics have already been marked for removal in 2.0 by KIP-225.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","lindong28","2018-06-09T00:39:58Z","2018-06-21T19:14:30Z"
"","4588","MINOR: use bash regex matching instead of egrep inside kafka-run-class.sh","It seems there is no reason to call egrep for each including jar since bash support regex matching. The reason of why this change might be useful is that we use kafka-run-class.sh for healthcheck and have audit enabled for exec so every launch of healthcheck produces a lot of audit logs. Besides egrep is deprecated.","open","","kurnevsky","2018-02-19T09:08:45Z","2018-03-02T19:31:14Z"
"","5073","Kafka 6936 - Use implicit serializer for KGroupedStream aggregate","It seems as if the aggregate method in KGroupedStream was missed in the implementation of the Scala wrapper API. It is the only method which falls back onto the configured default serializer. This is unexpected behavior for a user of the Scala API.  The method signature of the aggregate method was changed for this fix, you have to decide if it matches your coding style.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Danny02","2018-05-23T21:17:47Z","2018-05-24T13:32:52Z"
"","5133","KAFKA-6975: Fix fetching from non-batch-aligned log start offset","It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW.   Added two integration tests: 1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset; 2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-04T20:13:12Z","2018-06-14T15:26:47Z"
"","5235","KAFKA-6975; Fix replica fetching from non-batch-aligned log start offset","It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW.  Reviewers: Ismael Juma , Jun Rao , Jason Gustafson   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-15T04:23:49Z","2018-06-15T20:26:33Z"
"","5229","KAFKA-6975: Fix replica fetching from non-batch-aligned log start offset","It is possible that log start offset may fall in the middle of the batch after AdminClient#deleteRecords(). This will cause a follower starting from log start offset to fail fetching (all records). Use-cases when a follower will start fetching from log start offset includes: 1) new replica due to partition re-assignment; 2) new local replica created as a result of AdminClient#AlterReplicaLogDirs(); 3) broker that was down for some time while AdminClient#deleteRecords() move log start offset beyond its HW.  Added two integration tests: 1) Produce and then AdminClient#deleteRecords() while one of the followers is down, and then restart of the follower requires fetching from log start offset; 2)  AdminClient#AlterReplicaLogDirs() after AdminClient#deleteRecords()  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-14T17:39:44Z","2018-06-14T21:40:53Z"
"","5304","MINOR: Make replica verification test deterministic","It is possible for replica_verificaiton_test to fail with a `RemoteCommandError` when `self.stop_producer()` is called. This looks like a timing issue, as the implementation for `BackgroundThreadService#stop` is the following:  ```     def stop(self):         alive_workers = [worker for worker in self.worker_threads.itervalues() if worker.is_alive()]         if len(alive_workers) > 0:             self.logger.debug(                 ""Called stop with at least one worker thread is still running: "" + str(alive_workers))              self.logger.debug(""%s"" % str(self.worker_threads))          super(BackgroundThreadService, self).stop() ```  It is possible that the worker exits after the point `alive_workers` is constructed.  `VerifiableProducer#stop` expects the process to exist, and sets `allow_fail` to `false`.  ```     def stop_node(self, node):         self.kill_node(node, clean_shutdown=True, allow_fail=False)          stopped = self.wait_node(node, timeout_sec=self.stop_timeout_sec)         assert stopped, ""Node %s: did not stop within the specified timeout of %s seconds"" % \                         (str(node.account), str(self.stop_timeout_sec)) ```  This patch adds a wait to make sure the producer has finished before we try to stop it.","closed","","dhruvilshah3","2018-06-27T19:04:05Z","2019-05-07T05:29:47Z"
"","4997","KAFKA-6890: Add connector-level configurability for client configs","Introduced capability for individual connectors to override client config defaults.  Connector properties that are prefixed with ""producer."" and ""consumer."" are now used to feed into the producer and consumer clients embedded within source and sink connectors respectively.  Author: Allen Tang   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","natengall","2018-05-10T15:42:44Z","2020-10-16T06:02:57Z"
"","4934","KIP-81: KAFKA-4133: Bound memory usage of the Consumer","Introduce new Consumer config 'buffer.memory' Allocate MemoryPool based on 'buffer.memory' Allocate NetworkReceive through the MemoryPool Communication between client and Coordinator node skips MemoryPool Expose MemoryPool metrics in consumer-metrics group  https://cwiki.apache.org/confluence/display/KAFKA/KIP-81%3A+Bound+Fetch+memory+usage+in+the+consumer  Co-authored-by: Mickael Maison  Co-authored-by: Edoardo Comar   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-04-26T12:35:33Z","2022-02-09T13:46:46Z"
"","5196","MINOR: code cleanup follow up for KAFKA-6906","Intellij warned that the condition of the `if` will always be `false` -- thinking about this, it makes sense. After refactoring the code in KAFKA-6906 and moving the EOS-part out of the `commitOffsetNeeded` block L433-440 cover this case already and we can remove redundant code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-06-12T00:24:29Z","2018-06-13T18:06:35Z"
"","5388","KAFKA-6753: Updating the OfflinePartitions count only when necessary","Instead of updating the offline partitions count blindly after processing any event in the controller,  this patch maintains the offline partitions count in a variable inside the PartitionStateMachine, and only updates it when necessary. This will be the first step in optimizing the KafkaController#updateMetrics function.  Testing done: 1. Added new test methods for the new way of updating offline partitions count. 2. The new logic has been used inside LinkedIn in production for a few months without any problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-07-18T17:41:41Z","2018-08-21T18:37:36Z"
"","4676","MINOR: Improve WorkerUtils#CreateTopics","Increase the batch size from 10 to 500.  Lengthen the request timeout. Retry failed requests in a background thread even when there are other pending requests.  Do not treat ""topic already exists"" as an error. Do not fail before the timeout expires.","closed","","cmccabe","2018-03-09T22:47:47Z","2019-05-20T18:57:58Z"
"","5227","MINOR: Use ListOffsets request instead of SimpleConsumer in LogOffsetTest","Included a few clean-ups related to unused variables in tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-14T14:01:00Z","2018-06-14T18:38:05Z"
"","4803","MINOR: refactor error message of task migration","In the stream thread capture of the TaskMigration exception, print the task full information in WARN. In other places only log as INFO, plus additional context information.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-03-30T23:42:50Z","2020-04-24T23:51:38Z"
"","5370","MINOR: Fix broken Javadoc of [AbstractIndex|OffsetIndex]","In the javadoc of `AbstractIndex` and `OffsetIndex`, thrown `Exception`s are not imported.","closed","","dongjinleekr","2018-07-16T13:44:32Z","2018-07-18T00:09:46Z"
"","4788","MINOR: Fix partition loading checks in GroupCoordinator","In the group coordinator, we currently we check whether the partition is owned before checking whether it is loading. Since loading is a prerequisite for partition ownership, it means that it is not actually possible to see the COORDINATOR_LOAD_IN_PROGRESS error. The impact is mostly harmless: while loading the group, the client may send unnecessary FindCoordinator requests to rediscover the coordinator. I've fixed the bug and restructured the code to enable testing.  In the process of fixing this bug, I made the following minor changes:  1. We now verify valid groupId in all request handlers. 2. Currently if the coordinator is loading when a SyncGroup is received, we'll return NOT_COORDINATOR. I've changed this to return REBALANCE_IN_PROGRESS since the rebalance state will have been lost on coordinator failover. This effectively forces the consumer to rejoin the group, which seems preferable over unnecessarily rediscovering the coordinator.  3. I added a check for the COORDINATOR_LOAD_IN_PROGRESS handler in SyncGroup. Although we do not currently return this error, it seems reasonable that we might want to some day, so it seems better to get the check in now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-28T16:57:07Z","2018-04-02T15:35:18Z"
"","5291","KAFKA-7026: Sticky Assignor Partition Assignment Improvement (KIP-341)","In the current implementation of sticky assignor the leader does not cache the most recent calculated assignment. It relies on the fact that each consumer in the group sends its subscribed topics and also its current assignment when a rebalance occurs. This could lead to the issue described in [KAFKA-7026](https://issues.apache.org/jira/browse/KAFKA-7026), in which current assignment of a consumer is no longer valid and should be ignored. The solution implemented in this PR involves the leader caching the most recent assignment of each consumer, so the assignment reported by a consumer can be properly validated (and ignored if necessary) by the leader during a rebalance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-06-26T01:52:46Z","2019-04-18T15:35:24Z"
"","4893","MINOR: Retry setting aligned time until set","In the `AbstractResetIntegrationTest` we can have a transient error when setting the time for the test where the new time is less than the original time, for those cases we should catch the exception and re-try setting the time once versus letting the test fail.  For testing, ran the entire streams test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-04-18T21:48:45Z","2018-04-19T18:51:25Z"
"","5373","MINOR: Add thread dumps if broker node cannot be stopped","In system tests, it is useful to have the thread dumps if a broker cannot be stopped using SIGTERM.  Signed-off-by: Arjun Satish","closed","","wicknicks","2018-07-16T22:23:36Z","2018-07-20T18:10:44Z"
"","4617","KAFKA-6024 Move validation in KafkaConsumer ahead of acquireAndEnsure…","In several methods, parameter validation is done after calling acquireAndEnsureOpen() in Kafka Consumer :  public void seek(TopicPartition partition, long offset) {     acquireAndEnsureOpen();     try {         if (offset < 0)             throw new IllegalArgumentException(""seek offset must not be a negative number"");  Since the value of parameter would not change per invocation, it seems performing validation ahead of acquireAndEnsureOpen() call would be better.  Lost access to branch in the following PR. Basically incorporating the feedback from this. https://github.com/apache/kafka/pull/4109","closed","consumer,","shivsantham","2018-02-25T04:36:27Z","2018-03-13T16:05:55Z"
"","4947","KAFKA-1194 Forces unmapping of buffer if on Windows","In response to KAFKA-1194 issue, it now forces buffer to unmap if running on Windows OS. Invdividual Unit Test 'testChangeFileSuffixes' on LogSegmentTest.scala passes. but other file IO related unit tests still fail on Windows (Which is irrelevant to this code change).","open","","mmanna-sapfgl","2018-04-30T13:47:31Z","2018-05-08T10:45:46Z"
"","5287","KAFKA-7094: Variate should unify code style in one method, and use  camel name","In one method, there are two variates, partitionsTobeLeader and partitionsToBeFollower, which should use unify code style, that will be helpful to code maintenance.","closed","","wangzzu","2018-06-25T10:12:41Z","2018-08-03T16:11:16Z"
"","4790","MINOR: change exception.getMessage to toString","In many places of Streams we use exception.getMessage(), but for some exceptions the message is null, making it harder to trouble shoot. Propose to change them to toString() to maintain the exception name at least; in some places we choose to even print out the stack trace.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-28T19:09:41Z","2020-04-24T23:55:01Z"
"","5406","MINOR: Append or create file in FileStreamSinkTask","In FileStreamSinkTask we need to create the file it doesn't exist.   A recent change from `new FileOutputStream` to `Files.newOutputStream` missed the `CREATE` flag additionally to `APPEND`","closed","","kkonstantine","2018-07-20T16:55:10Z","2018-07-21T04:01:11Z"
"","4718","MINOR: Fix incorrect expression for KTable in stream doc.","In dsi-api.html, when reading from Kafka to a KTable, the doc says:  > In the case of a KStream, the local KStream instance of every application instance will be populated with data from only a subset of the partitions of the input topic. Collectively, across all application instances, all input topic partitions are read and processed.  where `Kstream` here is not correct. They should be `KTable`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-03-15T09:26:31Z","2018-03-15T22:49:00Z"
"","5170","MINOR: Relax Unsupported Version check on BrokerCompatibilityTest","In BrokerCompatibilityTest.java, when older versioned broker is used (0.10.1, 0.10.2), LIST_OFFSET is not supported as well. Hence in the verification phase, there is a possibility that consumer hit the UnsupportedVersionException earlier than Streams actually hits it:  ``` rg.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support LIST_OFFSETS with version in range [2,3]. The supported range is [0,1]. ```  While the test is waiting for  ``` ATAL: An unexpected exception org.apache.kafka.common.errors.UnsupportedVersionException: Cannot create a v0 FindCoordinator request because we require features supported only in 1 or later. ```  Call seek with specific offset would avoid sending LIST_OFFSET.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-06-08T23:45:01Z","2020-04-24T23:54:49Z"
"","5469","MINOR: Add connector configs to site-docs","In AK's documentation, the config props for connectors are not listed (https://kafka.apache.org/documentation/#connectconfigs). This PR adds these sink and source connector configs to the html site-docs.   Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-08-06T23:30:09Z","2018-08-07T22:09:24Z"
"","5148","MINOR: Upgrade Gradle to 4.8 and bug fix updates for other deps","In addition to Gradle, updated snappy, owasp-dependency-check, apache directory service api.  Gradle 4.8 fixes a fatal issue when building with Java 11, but full support is coming in 4.9 or later.  Manually tested that `jarAll` and importing into IntelliJ works, relying on PR build for the rest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-06T11:37:36Z","2018-06-07T07:28:12Z"
"","5066","KAFKA-6936: Implicit Materialized for aggregates","In #4919 we propagate the SerDes for each of these aggregation operators.  As @guozhangwang mentioned in that PR: > reduce: inherit the key and value serdes from the parent XXImpl class. > count: inherit the key serdes, enforce setting the Serdes.Long() for value serdes. > aggregate: inherit the key serdes, **do not set for value serdes internally**.  Although it's all good for `reduce` and `count`, it is quiet unsafe to have `aggregate` without `Materialized` given. In fact I don't see why we would not give a `Materialized` for the `aggregate` since the result type will always be different (otherwise use `reduce`) and also the value Serde is simply not propagated.  This has been discussed previously in a broader PR before but I believe for `aggregate` we could pass implicitly a `Materialized` the same way we pass a `Joined`, just to avoid the stupid case. Then if the user wants to specialize, he can give his own `Materialized`.  @guozhangwang @debasishg @seglo Let me know your thoughts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-22T20:48:04Z","2018-08-13T23:38:54Z"
"","5206","KAFKA-7051: Improve the efficiency of ReplicaManager","Improve the efficiency of the ReplicaManager when there are many partitions.  The 'fetchInfo' argument to ReplicaManager#fetchMessages can be an iterator rather than a sequence.  This avoids the need to materialize a new collection in some cases.  Reduce the number of times we loop over the read results obtained from 'readFromLog'.  Fix a case where we were performing an O(N^2) operation to join the set of partitions we wanted info about, and the set of partitions we obtained information about.","closed","performance,","cmccabe","2018-06-12T22:59:56Z","2019-05-20T18:54:46Z"
"","4519","KAFKA-6454: Allow timestamp manipulation in Processor API","implements KIP-251","closed","streams,","mjsax","2018-02-03T00:34:19Z","2018-03-16T23:02:18Z"
"","4882","KAFKA-6361: Fix log divergence between leader and follower after fast leader fail over","Implementation of KIP-279 as described here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-279%3A+Fix+log+divergence+between+leader+and+follower+after+fast+leader+fail+over  In summary: - Added leader_epoch to OFFSET_FOR_LEADER_EPOCH_RESPONSE - Leader replies with the pair( largest epoch less than or equal to the requested epoch, the end offset of this epoch) - If Follower does not know about the leader epoch that leader replies with, it truncates to the end offset of largest leader epoch less than leader epoch that leader replied with, and sends another OffsetForLeaderEpoch request. That request contains the largest leader epoch less than leader epoch that leader replied with.  Added integration test EpochDrivenReplicationProtocolAcceptanceTest.logsShouldNotDivergeOnUncleanLeaderElections that does 3 fast leader changes where unclean leader election is enabled and min isr is 1. The test failed before the fix was implemented.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-04-16T20:06:59Z","2018-05-11T02:58:43Z"
"","5437","KAFKA-7027: Add overloaded build method to StreamsBuilder","Implementation of [KIP-312](https://cwiki.apache.org/confluence/display/KAFKA/KIP-312%3A+Add+Overloaded+StreamsBuilder+Build+Method+to+Accept+java.util.Properties) required for enabling the use of the optimization framework.    This PR is required to allow for a 4th PR following on from https://github.com/apache/kafka/pull/5201 to enable optimizations.  The existing streams tests were used for testing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-07-31T15:17:51Z","2018-07-31T16:24:14Z"
"","4693","KAFKA-5540: Deprecate internal converter configs","Implementation of [KIP-174](https://cwiki.apache.org/confluence/display/KAFKA/KIP-174+-+Deprecate+and+remove+internal+converter+configs+in+WorkerConfig)  Configuration properties 'internal.key.converter' and 'internal.value.converter' are deprecated, and default to org.apache.kafka.connect.json.JsonConverter.  Warnings are logged if values are specified for either, or if properties that appear to configure instances of internal converters (i.e., ones prefixed with either 'internal.key.converter.' or 'internal.value.converter.') are given.  The property 'schemas.enable' is also defaulted to false for internal JsonConverter instances (both for keys and values) if it isn't specified.  Documentation and code have also been updated with deprecation notices and annotations, respectively.  Unit tests have been updated in `PluginsTest` to account for the new defaults for `schemas.enable` for internal key/value converters, and to ensure that (for the time being), internal key/value converters are still configurable despite being deprecated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2018-03-12T22:07:23Z","2020-10-16T06:24:16Z"
"","4871","KAFKA-6927: Message down-conversion causes Out Of Memory on broker","Implementation for lazy down-conversion in a chunked manner for efficient memory usage during down-conversion. This pull request is mainly to get initial feedback on the direction of the patch. The patch includes all the main components from KIP-283.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-04-13T20:38:21Z","2018-06-07T17:09:53Z"
"","4766","MINOR: Ignoring tests using old versions of Streams until KIP-268 is merged","Ignoring tests in `streams_upgrade_test.py` and `streams_multiple_rolling_upgrade_test.py` until KIP-268 is merged.  Kicked off system test build of all streams tests - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1581/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-03-23T19:18:38Z","2018-03-29T00:41:08Z"
"","4891","KAFKA-6802: Improved logging for missing topics during task assignment","If users don't create all topics before starting a streams application, they could get unexpected results.  For example, sharing a state store between sub-topologies where one input topic is not created ahead time results in log message that that ""Partition X is not assigned to any tasks"" does not give any clues as to how this could have occurred.  Also, this PR changes the log level from `INFO` to `WARN` when metadata does not have partitions for a given topic.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-04-18T18:42:17Z","2018-04-18T22:35:11Z"
"","4704","KAFKA-6653: Complete delayed operations even when there is lock contention","If there is lock contention while multiple threads check if a delayed operation may be completed (e.g. a produce request with acks=-1), the threads perform completion only if the lock is free, to avoid deadlocks. This leaves a timing window when an operation becomes ready to complete after another thread has acquired the lock and performed the check for completion, but not yet released the lock. The PR adds an additional flag to ensure that the operation is completed in this case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-13T22:32:02Z","2018-03-15T08:17:56Z"
"","4665","KAFKA-6560: [FOLLOW-UP] don't deserialize null byte array in window store fetch","If the result of a fetch from a Window Store results in a null byte array we should return null rather than passing it to the serde to deserialize.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2018-03-08T10:51:18Z","2018-03-08T21:24:23Z"
"","5159","KAFKA-7003: Set error context in message headers (KIP-298)","If the property `errors.deadletterqueue.context.headers.enable` is set to true, add a set of headers to the message describing the context under which the error took place.  A unit test is added to check the correctness of header creation.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-06-07T07:23:42Z","2020-10-16T06:02:58Z"
"","5057","MINOR: AdminClient metadata manager should reset state on failure","If the internal metadata request fails, we must reset the state inside `AdminClientMetadataManager` or we will be stuck indefinitely in the `UPDATE_PENDING` state and have no way to fetch new metadata.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-21T23:39:09Z","2018-05-22T17:14:13Z"
"","5339","Minor: Fix the NPE happening in closing KafkaServer","If server fails to connect to zk, the kafkaScheduler will be null when closing the kafka server. The NPE won't hurt kafka since the exception is swallowed.  `kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING 	at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:225) 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) 	at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251) 	at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:221) 	at kafka.zookeeper.ZooKeeperClient.(ZooKeeperClient.scala:95) 	at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1581) 	at kafka.server.KafkaServer.createZkClient$1(KafkaServer.scala:348) 	at kafka.server.KafkaServer.initZkClient(KafkaServer.scala:372) 	at kafka.server.KafkaServer.startup(KafkaServer.scala:202) 	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) 	at kafka.Kafka$.main(Kafka.scala:75) 	at kafka.Kafka.main(Kafka.scala) [2018-07-06 05:51:27,036] INFO shutting down (kafka.server.KafkaServer) [2018-07-06 05:51:27,042] WARN  (kafka.utils.CoreUtils$) java.lang.NullPointerException 	at kafka.server.KafkaServer.$anonfun$shutdown$6(KafkaServer.scala:579) 	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86) 	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:579) 	at kafka.server.KafkaServer.startup(KafkaServer.scala:329) 	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38) 	at kafka.Kafka$.main(Kafka.scala:75) 	at kafka.Kafka.main(Kafka.scala) `  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-07-06T06:03:54Z","2018-08-29T23:01:17Z"
"","4937","MINOR: Pin pip to 9.0.3 as 10 is not compatible with system pip","If not pinned, the following error will happen: Traceback (most recent call last):   File ""/usr/bin/pip"", line 9, in      from pip import main ImportError: cannot import name main  Testing: Ran tests/docker/run_tests.sh and it works instead of getting above error.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","maxzheng","2018-04-27T02:04:53Z","2018-04-27T04:01:27Z"
"","5342","MINOR: Use FetchRequest v8 and ListOffsetRequest v3 in ReplicaFetcherThread","If inter.broker.protocol.version is 2.0-IV1 or newer. Also fixed ListOffsetRequest so that v2 is used, if applicable.  Added a unit test that verifies that we use the latest version of the various requests by default. Included a few minor tweaks to make testing easier.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-07-06T13:30:31Z","2018-07-06T21:35:31Z"
"","4517","KAFKA-6529: Stop file descriptor leak when client disconnects with staged receives","If an exception is encountered while sending data to a client connection, that connection is disconnected. If there are staged receives for that connection, it is tracked to process those records. However, if the exception was encountered during processing a `RequestChannel.Request`, the `KafkaChannel` for that connection is muted and won't be processed.  Add the channel to failed sends so the connection is cleaned up on those exceptions. This stops the leak of the memory for pending requests and the file descriptor of the TCP socket.  Only flag channel as failed send when an exception is encountered while actually attempting to send something. Other socket interactions don't count.  Test that a channel is closed when an exception is raised while writing to a socket that has been closed by the client. Since sending a response  requires acks != 0, allow specifying the required acks for test requests in SocketServerTest.scala.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","parafiend","2018-02-02T23:03:43Z","2020-04-05T01:07:02Z"
"","5445","KAFKA-7225 - Pretransform validated props","If a property requires validation, it should be pretransformed if it is a variable reference, in order to have a value that will properly pass the validation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-08-02T03:17:00Z","2018-08-07T20:20:03Z"
"","4567","KAFKA-6559: Iterate record sets before calling Log.append","If a Produce request contains multiple record sets for a single topic-partition, it is better to iterate these before calling Log.append. This is because append will process all the sets together, and therefore will need to reassign offsets even if the offsets for an individual record set are properly formed. By iterating the record sets before calling append, each set can be considered on its own and potentially be appended without reassigning offsets.  3 tests added to cover this: - Append a single MemoryRecords that contains multiple batches - Append a single MemoryRecords that contains no batches - Append a single MemoryRecords that has an empty batch in the middle of valid batches  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","core,","toddpalino","2018-02-13T19:15:29Z","2018-03-02T19:31:13Z"
"","5320","KAFKA-6859: Do not send LeaderEpochRequest for undefined leader epochs","If a broker or topic has a message format < 0.11, he does not track leader epochs. LeaderEpochRequests for such will always return undefined, making the follower truncate to the highest watermark. Since there is no use to use the network for such cases, don't send a request.  ### Notes * I am not sure whether we should even check `brokerConfig.interBrokerProtocolVersion >= KAFKA_0_11_0_IV2`, since log message format is the only thing we care about and as far as I understand it is always defined * I am not fond of the subclassing in the tests for mocking the method (and also making it protected) but this was the cleanest solution I could come up with. Other ideas are welcome  The ticket also mentions  > Another example is a bootstrapping follower that does not have any leader epochs recorded,   but that is handled here: https://github.com/apache/kafka/blob/57320981bb98086a0b9f836a29df248b1c0378c3/core/src/main/scala/kafka/server/AbstractFetcherThread.scala#L127 as far as I can tell  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2018-07-02T13:19:18Z","2018-08-31T08:08:07Z"
"","5144","HOTFIX: Use alternative state store changelog topic","I've tried multiple ways to add a upgrade path patch for users from StreamsBuilder users in 1.0 / 1.1 to 2.0 with source KTable topics reusing. The following approach has the smallest changes on code and hopefully have the least changes.  Note that I do not intentionally write a checkpoint file after the restoration is done, since it will cause the logic to become too complex with / without EOS. If we do not have this checkpoint file containing the offset of the source topic, then if there is a crash between starting the restoration and writing the first checkpoint file in task.commit during normal processing, upon resuming we will restore from the old changelog again and may have duplicates if EOS is not turned on. But both are existed today so we are not introducing any regressions.  Another note is that the changelog topic map now have three different code paths (it is not introduced by this PR, but just want to bring this up to attention):  1. From `InternalTopologyBuilder.topicGroups`, used by the partition assignor to create those changelog topics; for this case if a state store is not logging enabled it should not be in this map.  2. From `InternalTopologBuilder.build` that generates the `ProcessorTopology` and then later used in `ProcessorStateManager`. Here even if a store is not logging enabled, if it still need to be restored either in a standby task or as part of the restoration of an active task, it should still be in the storesToChangelogs map, but note that the changelog topic names may not always be `stateStore-changelog`.  3. In `StoreBuilder`, decide whether we should really wrap it with the logging layer, and if yes, which topic to write to. Here if logging is disabled we should never wrap the logging layer, otherwise we wrap logging with the normal `stateStore-changelog` topic name.  Right now these logic modules are handled by completely different code paths; it is a bit complex for future optimizations on the topology.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-06-06T04:28:18Z","2018-06-07T16:12:48Z"
"","5000","KAFKA-6894: Improve err msg when explicitly connecting processor to global store","I've improved the error message when explicitly trying to connect a processor to a global store as in this example: ```       builder.globalTable(""supersteps"", Consumed.with(Serdes.Integer(), Serdes.Long()),                         Materialized.>as(""superstepStore"")                                 .withKeySerde(Serdes.Integer()).withValueSerde(Serdes.Long()));        stream.transform((TransformerSupplier>, KeyValue>)                         InitialTransformer::new, ""superstepStore""); ```","closed","streams,","rayokota","2018-05-10T19:18:25Z","2018-05-11T00:17:59Z"
"","4919","KAFKA-6813: Remove deprecated APIs in KIP-182, Part I","I'm breaking KAFKA-6813 into a couple of ""smaller"" PRs and this is the first one. It focused on:  1. Remove deprecated APIs in KStream, KTable, KGroupedStream, KGroupedTable, SessionWindowedKStream, TimeWindowedKStream.  2. Also found a couple of overlooked bugs while working on them:  2.a) In KTable.filter / mapValues without the additional parameter indicating the materialized stores, originally we will not materialize the store. After KIP-182 we mistakenly diverge the semantics: for KTable.mapValues it is still the case, for KTable.filter we will always materialize.  2.b) In XXStream/Table.reduce/count, we used to try to reuse the serdes since their types are pre-known (for `reduce` it is the same types for both key / value, for `count` it is the same types for key, and `Long` for value). This was somehow lost in the past refactoring.  2.c) We are enforcing to cast a `Serde` to `Serde` for XXStream / Table.aggregate, for which the returned value type is NOT known, such the enforced casting should not be applied and we should require users to provide us the value serde if they believe the default ones are not applicable.  2.d) Whenever we are creating a new `MaterializedInternal` we are effectively incrementing the suffix index for the store / processor-node names. However in some places this `MaterializedInternal` is only used for validation, so the resulted processor-node / store suffix is not monotonic.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-04-24T06:23:42Z","2020-04-24T23:57:12Z"
"","4533","MINOR: Bump version to 2.0.0-SNAPSHOT","I proposed last year that the June/July release should be 2.0.0 to allow us to: - Bump the minimum Java version to 8 - Remove the deprecated Scala clients  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-02-06T20:48:09Z","2018-02-06T20:51:17Z"
"","4804","MINOR: Bump version to 2.0.0-SNAPSHOT","I proposed last year that the June/July release should be 2.0.0 to allow us to:  * Bump the minimum Java version to 8 * Remove the deprecated Scala clients  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-31T02:58:28Z","2018-09-11T06:47:26Z"
"","4958","Minor: Empty list returned by poll() destroys the source task when assert is enabled","I noticed this issue when running the embedded connectors. There is a assert in SourceRecordWriteCounter. ```         public SourceRecordWriteCounter(int batchSize, SourceTaskMetricsGroup metricsGroup) {             assert batchSize > 0;   // this one             assert metricsGroup != null;             this.batchSize = batchSize;             counter = batchSize;             this.metricsGroup = metricsGroup;         } ``` Hence, the empty list will cease the source task. By contrast, empty list is not a issue in production since the assert is usually disabled in production. It seems to me making the behavior consistent is necessary. A simple fix is to remove the assert. Or empty list should be handled as null list. I prefer later since both of them mean ""no data can be processed"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2018-05-02T16:12:35Z","2020-04-30T08:05:35Z"
"","4835","MINOR: Mention leader in a few follower/controller log messages","I noticed these while debugging an issue recently. I have a bigger refactoring of the Controller and state change logs in progress, but these don't have to wait for that.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-07T19:05:52Z","2018-04-08T03:21:43Z"
"","4774","1.0","I found that all versions of the Kafka “kafka-server-stop.sh” script would have a problem, that is, the $PIDS variable is calculated to be empty for example:  `[mysql@cd-dc-kafka-node-1 ~]$ [mysql@cd-dc-kafka-node-1 ~]$ PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}') [mysql@cd-dc-kafka-node-1 ~]$ echo $PIDS  [mysql@cd-dc-kafka-node-1 ~]$ [mysql@cd-dc-kafka-node-1 ~]$ `  But if I use JPS to get it, it's all right ` [mysql@cd-dc-kafka-node-1 ~]$ [mysql@cd-dc-kafka-node-1 ~]$ PIDS=$(/opt/kafkaenv/java/jdk1.8.0_65/bin/jps | grep -v Jps | grep -v grep | grep -i Kafka | awk -F ' ' '{print $1}') [mysql@cd-dc-kafka-node-1 ~]$ echo $PIDS 21130 [mysql@cd-dc-kafka-node-1 ~]$  `  So I recommend modifying the script in the JPS way","closed","","linwaterbin","2018-03-26T16:00:13Z","2018-03-29T05:03:58Z"
"","5447","KAFKA-7237: Add explicit FATAL marker to log messages","https://issues.apache.org/jira/browse/KAFKA-7237","open","","stanislavkozlovski","2018-08-02T10:14:25Z","2019-09-13T20:18:11Z"
"","5429","[MINOR] KAFKA-7207: Make -rate & -total metrics documentation consistent","https://issues.apache.org/jira/browse/KAFKA-7207  Some sections of the `Monitoring` metrics documentation list out the `-total` metrics, and some sections do not list them out. We should make them consistent and list out the missing `-total` metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","KevinLiLu","2018-07-27T01:07:27Z","2018-08-07T07:49:57Z"
"","5225","KAFKA-7058 [Connect] Comparing schema default values using Objects#deepEquals()","https://issues.apache.org/jira/browse/KAFKA-7058 * Summary of testing strategy: Added new unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gunnarmorling","2018-06-14T10:18:08Z","2020-10-16T06:32:37Z"
"","5224","KAFKA-7015: Fixed RecordCollectorImpl exception messages with more human readable context info.","https://issues.apache.org/jira/browse/KAFKA-7015 Fixed `RecordCollectorImpl` class to enhance exceptions messages  to human readable format for Key/Value pair.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-06-14T10:14:34Z","2020-05-08T23:50:54Z"
"","5069","KAFKA-6931 Make worker id configurable","https://issues.apache.org/jira/browse/KAFKA-6931 The broker id and consumer/producer id (client id) are configurable so why we don't make the worker id configurable? This pr enable user to configure the worker id by the property file. If unset, we use the advertised host and port instead. Also, the worker id is used in metrics hence a configurable id make the metrics more readable.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-05-23T06:14:57Z","2020-03-17T09:00:14Z"
"","5086","Kafka 6906: Fixed Kafka Streams to commit transactions if data is produced via wall-clock punctuation.","https://issues.apache.org/jira/browse/KAFKA-6906  Fixed Class : `StreamTask`  to commit transactions if data is produced via wall-clock punctuation.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jadireddi","2018-05-27T18:14:36Z","2018-06-11T22:01:03Z"
"","5105","KAFKA-6906: Fixed to commit transactions if data is produced via wall clock punctuation","https://issues.apache.org/jira/browse/KAFKA-6906  Fixed `StreamTask` to commit transactions if the data is produced via wall-clock punctuation too.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jadireddi","2018-05-31T06:07:45Z","2018-06-13T02:14:07Z"
"","4914","KAFKA-6815 ""default.production.exception.handler"" default value is not specified in doc","https://issues.apache.org/jira/browse/KAFKA-6815  in KafkaStreams config,  default.deserialization.exception.handler is LogAndFailExceptionHandler  but, document has 30000ms","closed","","bistros","2018-04-23T06:03:15Z","2018-04-23T16:49:54Z"
"","4913","KAFKA-6814: Refine shown message when failing to delete groups","https://issues.apache.org/jira/browse/KAFKA-6814  When deleting a nonexistent group, exception message is as below: `The group id The group id does not exist was not found` which is very unfriendly. This patch will fix this misleading message in this case, and also cover deleting-not-empty-group as well.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-04-23T02:42:03Z","2018-05-22T01:02:37Z"
"","4912","KAFKA-6749: Fixed TopologyTestDriver to process stream processing guarantee as exactly once","https://issues.apache.org/jira/browse/KAFKA-6749   Fixed Stream processing topologies which are configured to use EXACTLY_ONCE processing guarantee can be tested with the `TopologyTestDriver`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-04-22T11:12:34Z","2018-06-14T05:33:37Z"
"","4808","KAFKA-6731: waitOnState should check the state to be the target start.","https://issues.apache.org/jira/browse/KAFKA-6731  KafkaStreams.waitOnState() should check the state to be the given one instead of the hard-coded `NOT_RUNNING`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","huxihx","2018-04-02T01:23:13Z","2018-04-02T17:43:54Z"
"","4765","KAFKA-6685: Added Exception to distinguish message Key from Value during deserializing.","https://issues.apache.org/jira/browse/KAFKA-6685  Added Exception message in `WorkerSinkTask.convertMessages` to distinguish message Key from Value during deserialization to Kafka connect format.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","jadireddi","2018-03-23T10:45:59Z","2020-10-16T06:24:16Z"
"","4868","KAFKA-6677: Fixed streamconfig producer's maxinflight allowed when EOS Enabled.","https://issues.apache.org/jira/browse/KAFKA-6677 Modified `StreamsConfig` Producer's default MaxInFlight Request allowed per connection.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-04-13T12:55:41Z","2018-04-24T18:27:59Z"
"","4723","KAFKA-6663: Doc for `GlobalKTable` should be corrected.","https://issues.apache.org/jira/browse/KAFKA-6663  Doc should be refined to express the fact that GlobalKTable should be able to consume all the partitions of the input topic.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","huxihx","2018-03-16T03:00:07Z","2018-03-17T01:42:46Z"
"","4707","KAFKA-6649: Should catch OutOfRangeException for ReplicaFetcherThread","https://issues.apache.org/jira/browse/KAFKA-6649  `AbstractFetcherThread.processFetchRequest` should catch OffsetOutOfRangeException lest the thread was forcibly stopped.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2018-03-14T04:16:14Z","2018-03-15T02:12:12Z"
"","4698","KAFKA-6637: Avoid divide /zero error with segment.ms set to zero","https://issues.apache.org/jira/browse/KAFKA-6637  This patch enforces a minimum value of 1 for the user-custom `segment.ms`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-03-13T09:03:26Z","2018-03-24T19:14:54Z"
"","4797","KAFKA-6592: ConsoleConsumer should support WindowedSerdes","https://issues.apache.org/jira/browse/KAFKA-6592   Have Console consumer support TimeWindowedDeserializer/SessionWindowedDeserializer by adding new parameters `default.windowed.key.serde.inner` and `default.windowed.value.serde.inner`.  An example would be like this: `... --property value.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property key.deserializer=org.apache.kafka.streams.kstream.TimeWindowedDeserializer --default.windowed.key.serde.inner org.apache.kafka.common.serialization.Serdes\$StringSerde `  When key/value.deserializer is not TimeWindowedDeserializer or SessionWindowedDeserializer, specifying `default.windowed.key.serde.inner` or `default.windowed.value.serde.inner` takes no effect.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-03-30T07:26:43Z","2018-04-12T16:35:44Z"
"","4846","KAFKA-6538: Fixed RocksDBStore to enhance processor state  exceptions with more context","https://issues.apache.org/jira/browse/KAFKA-6538 Fixed against `Base:1.1` version. Fixed Class:`RocksDBStore` to enhance Processor State Exception containing Key/Value values as Bytes/byte[] to Bytes/Bytes form in the error message for displaying more useful context .  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-04-10T12:04:15Z","2018-06-05T18:27:50Z"
"","5103","KAFKA-6538: Changes to enhance  ByteStore exceptions thrown from RocksDBStore with more human readable info","https://issues.apache.org/jira/browse/KAFKA-6538  Enhanced exceptions thrown from `RocksDBStore` with corresponding information for which key/value the operation failed in the wrapping stores (KeyValueStore, WindowedStored, and SessionStore).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-05-31T03:33:35Z","2018-06-07T05:01:12Z"
"","4509","KAFKA-6515 Adding toString() method to o.a.k.connect.data.Field","https://issues.apache.org/jira/browse/KAFKA-6515  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","gunnarmorling","2018-02-01T22:24:14Z","2018-02-03T21:27:02Z"
"","4538","KAFKA-6472 - Fix WordCount example code error","https://issues.apache.org/jira/browse/KAFKA-6472  - related https://github.com/apache/kafka-site/pull/126  CC: @guozhangwang","closed","docs,","joel-hamill","2018-02-07T01:07:01Z","2018-02-13T00:56:19Z"
"","4563","KAFKA-6446: KafkaProducer should use timed version of `await` to avoid endless waiting","https://issues.apache.org/jira/browse/KAFKA-6446  Replaced await() with timed version to avoid endless waiting and refined the code to have Sender thread able to exit from infinitely connecting the `bad` broker.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","huxihx","2018-02-13T08:43:54Z","2018-03-27T16:21:19Z"
"","4793","KAFKA-5253: Fixed KStreamTestDriver to handle streams created with patterns","https://issues.apache.org/jira/browse/KAFKA-5253  Fixed `KStreamTestDriver#sourceNodeByTopicName` to handle streams created with patterns.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jadireddi","2018-03-29T06:38:13Z","2018-04-24T18:55:36Z"
"","4816","MINOR: Make kafka-streams-test-utils dependencies work with releases tarballs","https://github.com/apache/kafka/pull/4760 unintentionally included extra raw class files in the release tarballs by making the .class file output (instead of the jar) for a streams a dependency of the streams-test-utils. This fixes that issue by instead breaking the circular dependency by using a `compileOnly`/`provided` dependency on those sources and also including the dependency as a test dependency.  I verified by using `gradlew clean installAll releaseTarGzAll`, then checking that the release tarball doesn't have the extraneous files and the installed pom file has the expected dependencies. The dependency on kafka-streams is now in the `test` scope, but that should be fine since a streams application would only use this dependency if it already had a dependency on streams in `compile` (or in weird edge cases the user could handle specifying the right dependencies). This actually seems to even be an improvement over the previous situation where the actual dependency was not expressed in the pom at all (since the dependency was on the sourceSet output rather than the actual project).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-04-03T03:50:55Z","2018-04-03T16:10:06Z"
"","4579","MINOR: Fix bug introduced by adding batch.size without default in FileStreamSourceConnector","https://github.com/apache/kafka/pull/4356 added `batch.size` config property to `FileStreamSourceConnector` but the property was added as required without a default in config definition (`ConfigDef`). This results in validation error during connector startup.   Unit tests were added for both `FileStreamSourceConnector` and `FileStreamSinkConnector` to avoid such issues in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-02-16T00:45:34Z","2020-10-16T06:05:17Z"
"","5176","KAKFA-7018: remember consumer member id during service restart","Hey there, I have come up with an implementation for the consumer generation materialization. Jira here: https://issues.apache.org/jira/browse/KAFKA-7018  The background is that to reduce number of rebalance during consumer restart and blocking rebalance based on session timeout (especially for stream applications), we are proposing to remember the consumer client-id to generation mapping, so that on the broker side when the same instance connects with existing generation data, it would be treated as an existing member so that no rebalance will be triggered.  This implementation doesn't have unit test yet. We hope to have an initial review to finalize details. Looking forward to @guozhangwang @mjsax @hachikuji @Ishiihara to review!","closed","","abbccdda","2018-06-09T23:45:47Z","2019-01-01T05:08:47Z"
"","4508","KAFKA-4750: Bypass null value and treat it as deletes","Here is the new rule for handling nulls: * in the interface store, put(key, null) are handled normally and value serde applied to null. * in the inner most store, null bytes after serialization will always be treated as deletes. * in the interface store, if null bytes get returned in get(key), serde will be avoided and null object will be returned.  More changes: * Update javadocs, add unit tests accordingly; augment MockContext to set serdes for the newly added tests. * Fixed a discovered bug which is exposed by the newly added tests. * Use the new API to remove all old APIs in the existing state store tests. * Remove SerializedKeyValueIterator since it is not used any more.  This is originally contributed by @evis.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-01T20:44:51Z","2020-04-25T00:08:39Z"
"","4800","KAFKA-6728 Kafka Connect Header Null Pointer Exception","Guard against potential null recordHeader in WorkerSinkTask#convertHeadersFor  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","tedyu","2018-03-30T18:00:22Z","2020-10-16T06:24:17Z"
"","4637","deleted","Give metric reporters configured for clients the auto-generated client id. Without this fix, they do not receive the client id in the `configure` method at all if the user does not specify the client id. Brokers had this fixed in KAFKA-4756 but this still exists for clients.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2018-03-02T09:27:02Z","2018-12-02T23:52:05Z"
"","4872","KAFKA-6376: preliminary cleanup","General cleanup of Streams code, mostly resolving compiler warnings and re-formatting.  The regular testing suite should be sufficient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-04-13T20:43:12Z","2018-04-17T20:19:54Z"
"","4620","KAFKA-6592: ConsoleConsumer should support specifying inner deserializers","For those deserializers with inner deserializer such as WindowedDeserializer, user cannot directly specify the underlying inner class and NullPointerException would be thrown.  This patch adds two new properties named `key.deserializer.inner.class` and `value.deserializer.inner.class`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","huxihx","2018-02-26T07:13:09Z","2018-05-16T19:41:53Z"
"","4948","KAFKA-6829: Retry commits on unknown topic or partition","For the UNKNOWN_TOPIC_OR_PARTITION error, we could change the consumer's behavior to retry after this error.  While this is a rare case since the user would not commit offsets for topics unless they had been able to fetch from them, but this doesn't really handle the situation where the broker hasn't received any metadata updates.  Updated unit tests   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-04-30T14:26:49Z","2018-05-03T00:01:28Z"
"","5216","MINOR: Replace test usages of ClientUtils.fetchTopicMetadata with BaseRequestTest","For tests that are not testing the old consumers functionality. As part of this, consolidate `TopicMetadataTest` into `MetadataRequestTest`. Finally, remove `ProducerBounceTest` which has no tests left in it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-13T19:59:19Z","2018-06-14T03:33:44Z"
"","5189","KAFKA-6546: Use LISTENER_NOT_FOUND_ON_LEADER error for missing listener","For metadata request version 6 and above, use a different error code to indicate missing listener on leader broker to enable diagnosis of listener configuration issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-11T18:16:44Z","2018-06-20T16:08:59Z"
"","4494","KAFKA-6378 KStream-GlobalKTable null KeyValueMapper handling","For KStream-GlobalKTable joins let `null` `KeyValueMapper` results indicate no match  For KStream-GlobalKTable joins, a `KeyValueMapper` is used to derive a key from the stream records into the `GlobalKTable`. For some stream values there may be no valid reference to the table stream. This patch allows developers to use `null` return values to indicate there is no possible match. This is possible in this case since `null` is never a valid key value for a `GlobalKTable`. Without this patch, providing a `null` value caused the stream to crash on Kafka 1.0.  I added unit tests for KStream-GlobalKTable left and inner joins, since they were missing. I also covered this additional scenario where `KeyValueMapper` returns `null` to insure it is handled correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andybryant","2018-01-31T11:15:03Z","2018-02-01T18:28:51Z"
"","5239","KAFKA-7066 added better logging in case of Serialisation issue","Following the error message of: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java#L93  Would be good if included in 2.0","closed","streams,","simplesteph","2018-06-16T09:02:32Z","2018-06-19T21:11:31Z"
"","4821","MINOR: fix streams test-utils dependencies","Follow-up to @ewencp 's PR #4816 (a fix of my #4760).  Break the cycle on the other side so that `:streams:test-utils` can declare a regular dependency on `:streams`.  To test that we don't re-introduce the bug that #4816 fixed, do the following: ``` ./gradlew clean installAll releaseTarGzAll tar --list -f ./core/build/distributions/kafka_2.11-1.2.0-SNAPSHOT.tgz | grep 'class' ```  That latter command should return only the following (instead of 580 class files): ``` kafka_2.11-1.2.0-SNAPSHOT/bin/kafka-run-class.sh kafka_2.11-1.2.0-SNAPSHOT/bin/windows/kafka-run-class.bat ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-04-03T21:01:25Z","2018-05-07T23:03:59Z"
"","5367","KAFKA-7162: Fixing flaky tests","Fixing the flaky tests by either 1. Use the same timestamp in the records as the validation timestamp 2. Disable the timestamp validation by using LOG_APPEND_TIME as the timestamp type because the time difference check will only apply when the timestamp type is CREATE_TIME, as shown in the LogValidator.validateTimestamp method.  Testing done: Verified all the tests in the LogValidatorTest class passeh that after making the code changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-07-14T01:40:57Z","2018-11-21T22:49:18Z"
"","4977","KAFKA-6879; Invoke session init callbacks outside lock to avoid deadlock","Fixes a deadlock between the controller's `beforeInitializingSession` callback which holds the zookeeper client initialization lock while awaiting completion of an asynchronous event which itself depends on the same lock.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-08T05:39:02Z","2018-05-08T22:59:56Z"
"","5444","KAFKA-7231; Ensure NetworkClient uses overridden request timeout","Fixed incorrect use of default timeout instead of the argument explicitly passed to `newClientRequest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-08-02T00:05:26Z","2018-08-02T18:02:39Z"
"","5029","KAFKA-6911: Fix dynamic keystore/truststore update check","Fix the check, add unit test to verify the change, update DynamicBrokerReconfigurationTest to avoid dynamic keystore update in tests which are not expected to update keystores.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-17T09:58:42Z","2018-05-24T23:24:38Z"
"","5135","KAFKA-6991 : KIP-285 : Fix ServiceLoader issue with PluginClassLoader","Fix ServiceLoader issue with PluginClassLoader and add basic-auth-extension packaging & classpath  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-06-05T05:24:58Z","2020-10-16T06:02:58Z"
"","5241","KAFKA-7068 : KIP-297 : Handle null config values during transform","Fix NPE when processing null config values during transform.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2018-06-17T07:00:41Z","2020-10-16T06:32:37Z"
"","4568","MINOR: Support dynamic JAAS config for broker's LoginManager cache","Fix LoginManager caching when `sasl.jaas.config` is defined for broker and add unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-13T19:17:03Z","2018-02-15T09:26:35Z"
"","4530","MINOR: Improve log4j messaging","Fix log4j formatting for a couple of log entries, plus include key information in the exception message  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-02-06T00:00:52Z","2020-04-24T23:49:25Z"
"","5160","KAFKA-7011 - Remove ResourceNameType field from Java Resource class.","Fix for [KAFKA-7011](https://issues.apache.org/jira/browse/KAFKA-7011).  The initial PR for KIP-290 #5117 added a `ResourceNameType` field to the Java and Scala `Resource` classes to introduce the concept of Prefixed ACLS.  This does not make a lot of sense as these classes are meant to represent cluster resources, which would not have a concept of 'name type'. This work has not been released yet, so we have time to change it.  This PR looks to refactor the code to remove the name type field from the Java `Resource` class. (The Scala one will age out once KIP-290 is done, and removing it would involve changes to the `Authorizer` interface, so this class was not touched).  This is achieved by replacing the use of `Resource` with `ResourcePattern` and `ResourceFilter` with `ResourceFilterPattern`.  A `ResourcePattern` is a combination of resource type, name and name type, where each field needs to be defined. A `ResourcePatternFilter` is used to select patterns during describe and delete operations.  The adminClient uses `AclBinding` and `AclBindingFilter`. These types have been switched over to use the new pattern types.  The AclCommands class, used by Kafka-acls.sh, has been converted to use the new pattern types.  The result is that the original `Resource` and `ResourceFilter` classes are not really used anywhere, except deprecated methods. However, the `Resource` class will be used if/when KIP-50 is done.  cc @cmccabe, @junrao   This PR will need cherry picking onto the 2.0 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-07T14:31:45Z","2018-06-08T10:46:15Z"
"","5205","KAFKA-7010: Rename ResourceNameType to PatternType","Fix for [KAFKA-7010](https://issues.apache.org/jira/browse/KAFKA-7010).  The initial PR for KIP-290 #5117 added a new `ResourceNameType`, which was initially a field on `Resource` and `ResourceFilter`. However, follow on PRs have now moved the name type fields to new `ResourcePattern` and `ResourcePatternFilter` classes. This means the old name is no longer valid and may be confusing. The PR looks to rename the class to a more intuitive `resource.PatternType`.  @cmccabe also requested that the current `ANY` value for this class be renamed to avoid confusion. `PatternType.ANY` currently causes `ResourcePatternFilter` to bring back all ACLs that would affect the supplied resource, i.e. it brings back literal, wildcard ACLs, and also does pattern matching to work out which prefix acls would affect the resource.  This is very different from the behaviour of `ResourceType.ANY`, which just means the filter ignores the type of resources.    `ANY` is to be renamed to `MATCH` to disambiguate it from other `ANY` filter types. A new `ANY` will be added that works in the same way as others, i.e. it will cause the filter to ignore the pattern type, (but won't do any pattern matching).  cc @junrao   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-12T22:41:39Z","2018-06-14T21:09:43Z"
"","5161","KAFKA-7007:  All ACL changes should use single /kafka-acl-changes path","Fix for [KAFKA-7007](https://issues.apache.org/jira/browse/KAFKA-7007).  The initial PR for KIP-290 #5117 added a new path in ZK for ACL change events to come through for the new Prefixed resource pattern.  @Jun requested that this be reverted, and in its place we should use JSON values for the change events.  This PR looks to make these changes. Where possible, I've put code back to being the same as pre PR 5117.  cc @cmccabe, @junrao   *UPDATE*: After implementing the switch to JSON it was decided that this causes too many potential problems. Instead, the code now looks to encode a 3 part ACL change event, (`::`), where the old brokers used a 2 part event, (`:`).  Older brokers will effectively ignore the new style events, without any errors or warnings.  *UPDATE*: After more discussions a new hybrid approach is being adopted. The PR _now_ looks to:  - Keep Literal ACLs on the old paths, using the old formats, to maintain backwards compatibility.  - Have Prefixed, and any latter types, go on new paths, using JSON, (old brokers are not aware of them)  - Add checks to reject any adminClient requests to add prefixed acls before the cluster is fully upgraded. Note:  - Using new AckCommands / kafka-acls during upgrade for literal resource patterns will work  - Using new AckCommands / kafka-acls during upgrade for prefixed resource patterns will lead to indeterminate behaviour, (which is highlighted in upgrades.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-07T20:44:09Z","2018-06-12T22:22:42Z"
"","5184","KAFKA-7005: Remove duplicate resource class.","Fix for [KAFKA-7005](https://issues.apache.org/jira/browse/KAFKA-7005).  This is a follow-on change requested as part of the initial PR for KIP-290 #5117.  @cmccabe requested that the `resource.Resource` class be factored out in favour of `ConfigResource` to avoid confusion between all the `Resource` implementations.  cc @cmccabe, @junrao   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-11T12:18:01Z","2018-06-11T20:32:12Z"
"","4796","KAFKA-6727 fix broken Config hashCode() and equals()","Fix for [KAFKA-6727](https://issues.apache.org/jira/browse/KAFKA-6727).  Current implementation stores reference to `entries` wrapped in `Collections.unmodifiableCollection`, which breaks `hashCode` and `equals`.  From [Java docs](https://docs.oracle.com/javase/7/docs/api/java/util/Collections.html#unmodifiableCollection\(java.util.Collection\))  > The returned collection does not pass the hashCode and equals operations through to the backing collection, but relies on Object's equals and hashCode methods.  Contribution is my own original work and I license the work to the project under the project's open source license.","closed","","big-andy-coates","2018-03-29T14:04:19Z","2018-04-17T10:06:36Z"
"","5245","Kafka_7064 - bug introduced when switching config commands to ConfigResource","Fix for  [KAFKA-7064](https://issues.apache.org/jira/browse/KAFKA-7064)  I some how managed to get the Ids wrong when copying them from request.Resource to ConfigResource.Type, even though I double checked. :-/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-18T13:42:07Z","2018-06-20T11:14:25Z"
"","4742","MINOR: Remove acceptor creation in network thread update code","Fix dynamic addition of network threads to only create new Processor threads and not the Acceptor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-20T21:47:53Z","2018-03-21T09:30:16Z"
"","4865","KAFKA-6784: FindCoordinatorResponse cannot be cast to FetchResponse","fix bug: FindCoordinatorResponse cannot be cast to FetchResponse","closed","","koqizhao","2018-04-13T01:42:18Z","2018-07-17T01:47:54Z"
"","4608","MINOR: Fix ConcurrentModificationException in TransactionManager","Fix a ConcurrentModificationException in TransactionManager#shouldResetProducerStateAfterResolvingSequences.  The fix is to use an Iterator rather than a Java foreach loop. When using a Java foreach loop, the collection cannot be modified, because the for-each loop hides the iterator.","closed","","cmccabe","2018-02-21T18:41:46Z","2019-05-20T18:57:12Z"
"","4982","KAFKA-6786: Remove additional configs in StreamsBrokerDownResilienceTest","First iteration of KAFKA-6786. Currently a WIP as it is my first time contributing; want to make sure that we implemented this correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","sh-abhi","2018-05-09T02:40:42Z","2019-02-03T15:54:25Z"
"","5379","KAFKA-7169: Custom SASL extensions for OAuthBearer authentication mechanism","Excuse the number of commits - AFAIA they're all squashed together in the end so it should be okay","closed","","stanislavkozlovski","2018-07-17T21:29:46Z","2018-08-06T16:22:05Z"
"","5139","KAFKA-6997 : Exclude test-sources.jar when $INCLUDE_TEST_JARS is FALSE","Exclude test-sources.jar when $INCLUDE_TEST_JARS is FALSE   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mageshn","2018-06-05T19:58:23Z","2018-06-06T00:00:15Z"
"","4670","MINOR: Reduce ZK reads and ensure ZK watch is set for listener update","Ensures that ZK watch is set for each live broker for listener update notifications in the controller by invoking `pathExists` since a watch is registered only if `exists` or `getData` is invoked after `registerZNodeChangeHandler`. Also avoids reading all brokers from ZooKeeper when a broker metadata is modified by passing in brokerId to `BrokerModifications` and reading only the updated broker.  The existing listener update test verifies both these changes. Earlier, the test did not detect missing watch for the last broker since metadata of all brokers were read from ZK (adding a watch for all) when any broker was updated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-09T07:33:31Z","2018-03-10T15:57:06Z"
"","4729","KAFKA-6676: Ensure Kafka chroot exists in system tests and use chroot on one test with security parameterizations","Ensures Kafka chroot exists in ZK when starting KafkaService so commands that use ZK and are executed before the first Kafka broker starts do not fail due to the missing chroot.  Also uses chroot with one test that also has security parameterizations so Kafka's test suite exercises these combinations. Previously no tests were exercising chroots.  To validate, I kicked off a [test run](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1483/) of the sanity_checks (which include the chroot-ed test as well as some non-chroot-ed tests).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-03-17T21:33:27Z","2018-03-19T15:22:39Z"
"","5149","KAFKA-7061: Enhanced log compaction","Enhance log compaction to support more than just offset comparison, so the insertion order isn't dictating which records to keep.  Default behavior is kept as it was, with the enhanced approached having to be purposely activated. The enhanced compaction is done either via the record timestamp, by settings the new configuration as ""timestamp"" or via the record headers by setting this configuration to anything other than the default ""offset"" or the reserved ""timestamp"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","blaghed","2018-06-06T13:46:24Z","2018-08-30T13:22:47Z"
"","4822","KIP-280: Enhanced log compaction","Enhance log compaction to support more than just offset comparison, so the insertion order isn't dictating which records to keep.  Default behavior is kept as it was, with the enhanced approached having to be purposely activated. The enhanced compaction is done either via the record timestamp, by settings the new configuration as ""timestamp"" or via the record headers by setting this configuration to anything other than the default ""offset"" or the reserved ""timestamp"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","blaghed","2018-04-04T09:00:29Z","2019-08-01T20:45:09Z"
"","5218","KAFKA-6978: make window retention time strict","Enforce window retention times strictly: * records for windows that are expired get dropped * queries for timestamps old enough to be expired immediately answered with `null`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-06-13T23:04:47Z","2018-06-23T00:08:01Z"
"","4584","MINOR: Restore scanning for super types of Connect plugins","Enabling scans for super types in reflections is required in order to discover Connect plugins.  Smoke tests, system tests, and manual testing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-02-18T08:54:24Z","2020-10-16T06:24:15Z"
"","4699","KAFKA-6576: Configurable Quota Management (KIP-257)","Enable quota calculation to be customized using a configurable callback. See KIP-257 for details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-13T09:40:29Z","2018-04-06T21:49:35Z"
"","4904","KAFKA-6810: Enable dynamic update of SSL truststores","Enable broker's SSL truststores to be dynamically updated using ConfigCommand in the same way as keystores are updated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-20T19:38:37Z","2018-05-24T16:55:25Z"
"","4631","MINOR: fixing streams test-util compilation errors in Eclipse","Eclipse (4.7.2 using jdk 1.8.0_151 ) was complaining about 3 compilation errors in  streams/test-utils/src/test/java/org/apache/kafka/streams/test/ConsumerRecordFactoryTest.java  all three were about generics Long/Integer mismatch  The method create(Integer) in the type ConsumerRecordFactory is not applicable for the arguments (List>)	ConsumerRecordFactoryTest.java	line 181	/test-utils/src/test/java/org/apache/kafka/streams/test	Java Problem The method create(Integer) in the type ConsumerRecordFactory is not applicable for the arguments (List>)	ConsumerRecordFactoryTest.java	line 210	/test-utils/src/test/java/org/apache/kafka/streams/test	Java Problem The method create(List>, long, long) in the type ConsumerRecordFactory is not applicable for the arguments (List>, long, long)	ConsumerRecordFactoryTest.java	line 239	/test-utils/src/test/java/org/apache/kafka/streams/test	Java Problem  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","edoardocomar","2018-03-01T12:46:26Z","2018-03-19T10:29:50Z"
"","4905","KAFKA-5034: Enable plugins to be added to the plugin path at runtime","Each directory in the plugin.path is monitored for file system changes via a daemon thread, and when a change is detected that appears to correspond to a plugin creation (e.g., a directory or archive file is created), the DelegatingClassLoader is alerted and scans the location for new plugins to register.  Plugin removals currently only result in a warning, since once they are loaded by the DelegatingClassLoader, deletion of the corresponding .class, .zip, or .jar files does not remove them from the class loader.  Testing involves creating a directory, populating it with several fake plugins, and watching it with a `PluginPathDirectoryListener`. At that point, several more fake plugins are created, and it is verified that their creation has been detected by the listener. Afterward, some fake plugins are deleted, and it is verified that their deletion has been detected by the listener. A ""red herring"" file is also created and deleted during the test, and it is verified that that file is not acknowledged by the listener. All current plugin formats are included in the test (directory, JAR file, and ZIP file).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2018-04-20T21:38:38Z","2022-02-27T03:56:28Z"
"","4764","MINOR: Fix encoder config to make DynamicBrokerReconfigurationTest stable","DynamicBrokerReconfigurationTest currently assumes that passwords encoded with one secret will fail with an exception if decoded with another secret and configures an old.secret in `setUp`. This could potentially cause test failures if a password was incorrectly decoded with the wrong secret, since the test writes passwords encoded with the new secret directly to ZooKeeper. Since old.secret is only used in one test for verifying secret rotation, this config can be moved to that test to avoid transient failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-23T09:50:42Z","2018-03-23T17:01:33Z"
"","4488","KAFKA-6246: Dynamic update of listeners and security configs","Dynamic update of listeners as described in KIP-226. This includes:   - Addition of new listeners with listener-prefixed security configs   - Removal of existing listeners   - Password encryption   - sasl.jaas.config property for broker's JAAS config prefixed with listener and mechanism name  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-30T15:45:21Z","2018-03-09T07:34:43Z"
"","4870","MINOR: Use distinct consumer groups in dynamic listener tests","Dynamic listener tests should use distinct consumer groups to ensure there are no committed offsets.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-13T16:16:44Z","2018-04-20T16:46:58Z"
"","4696","KAFKA-6638; Leader should remove unassigned replica from ISR","During partition reassignment, controller may remove a replica from the replica set while still keeping the replica in the isr set. One solution to fix this issue is to let the controller remove replica from isr set as well. This requires more change in the code because controller needs to update the zookeeper node to change the isr properly. Another solution, which keeps controller simple, is to let leader node remove unassigned replica from the isr set.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-13T00:53:32Z","2018-07-22T22:41:29Z"
"","5475","KAFKA-7242: Reverse xform configs before saving","During actions such as a reconfiguration, the task configs are obtained via `Worker.connectorTaskConfigs` and then subsequently saved into an instance of `ClusterConfigState`.  The values of the properties that are saved are post-transformation (of variable references) when they should be pre-transformation.  This is to avoid secrets appearing in plaintext in the `connect-configs` topic, for example.  The fix is to change the 2 clients of `Worker.connectorTaskConfigs` to perform a reverse transformation (values converted back into variable references) before saving them into an instance of `ClusterConfigState`. The 2 places where the save is performed are `DistributedHerder.reconfigureConnector` and `StandaloneHerder.updateConnectorTasks`.  The way that the reverse transformation works is by using the ""raw"" connector config (with variable references still intact) from `ClusterConfigState` to convert config values back into variable references for those keys that are common between the task config and the connector config.  There are 2 additional small changes that only affect `StandaloneHerder`:  1) `ClusterConfigState.allTasksConfigs` has been changed to perform a transformation (resolution) on all variable references.  This is necessary because the result of this method is compared directly to `Worker.connectorTaskConfigs`, which also has variable references resolved.  2) `StandaloneHerder.startConnector` has been changed to match `DistributedHerder.startConnector`.  This is to fix an issue where during `StandaloneHerder.restartConnector`, the post-transformed connector config would be saved back into `ClusterConfigState`.  I also performed an analysis of all other code paths where configs are saved back into `ClusterConfigState` and did not find any other issues.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-08-07T21:22:41Z","2018-08-28T20:38:13Z"
"","5048","MINOR: Reduce required disconnect message frequency","Due to https://github.com/apache/kafka/pull/4644 the consumer connector logs will be much more clean with fewer ""broker may not be available"" entries. We need to reduce the required frequency from 100 to a smaller number.  I've thought about reducing to just 1, but it may still be transient (i.e. even if broker is starting up you may see a few entries) so I reduced it to 10.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-05-20T23:58:43Z","2020-04-24T23:55:13Z"
"","4922","KAFKA-6376: Document skipped records metrics changes","Document the metrics changes in https://github.com/apache/kafka/commit/ed51b2cdf5bdac210a6904bead1a2ca6e8411406 .  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vvcephei","2018-04-24T19:22:35Z","2018-05-07T23:06:01Z"
"","4558","KAFKA-6476: Documentation for dynamic broker configuration","Docs for dynamic broker configuration (KIP-226).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-12T15:40:51Z","2018-02-14T22:09:46Z"
"","5305","KAFKA-7104: Consistent leader's state in fetch response","Do not update LogReadResult after it is initially populated when returning fetches immediately (i.e. without hitting the purgatory). This was done in https://github.com/apache/kafka/pull/3954 as optimization so that followers get most recent high watermark and log start offset. However, since many things can happen (like deleting old segments and advancing log start offset) between initial creation of LogReadResult and the update, we can hit issues like log start offset in fetch response being higher than the last offset in fetched records.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-06-28T00:16:32Z","2018-06-28T15:32:21Z"
"","4639","MINOR: do not start processor for bounce-at-start","Do not start the processor for test_broker_type_bounce_at_start; only start it after the broker has been shutdown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-02T19:09:10Z","2020-04-24T23:49:05Z"
"","4864","KAFKA-6592: Follow-up","Do not require ConsoleConsumer to specify inner serde as s special property, but just a normal property of the message formatter.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-12T18:46:32Z","2020-04-24T23:57:02Z"
"","5236","WIP: introduce type alias for WindowedKTable","Do not merge.  Just sketching out what a type alias for `KTable, V>` would look like.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-15T14:48:01Z","2018-06-15T16:25:50Z"
"","5337","DRAFT: KIP-328: suppression operator","DO NOT MERGE  This is a scratchpad for discussing ideas related to the kip  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-05T22:35:18Z","2018-10-16T19:29:59Z"
"","5346","Kafka 6432: make index lookup more cache friendly","Do a binary lookup at the end of index first, as the end section of the index is more likely to be in the page cache than the median part of the index","closed","","ying-zheng","2018-07-08T00:21:08Z","2020-11-02T18:33:42Z"
"","5440","KAFKA-7228: Set errorHandlingMetrics for dead letter queue","DLQ reporter does not get a `errorHandlingMetrics` object when created by the worker. This results in an NPE.   Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-08-01T16:01:20Z","2020-10-16T06:03:00Z"
"","5079","KAFKA-6841: Add support for Prefixed ACLs","Details are in KIP-290: https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+wildcard+suffixed+ACLs  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","piyushvijay","2018-05-25T09:43:11Z","2018-06-05T19:30:59Z"
"","4944","MINOR: use jdk8 to build/run system tests (#4925)","Debian installer packages are no longer available for Java 7.  Also upgrade AMI to latest ubuntu/trusty 14 amd64 as the older one is no longer available.  Note that this only changes the JDK used to build and run the system tests. We still have Jenkins jobs that compile and run the JUnit tests with Java 7 so that we don't use features that are only available in newer Java versions.","closed","","jarekr","2018-04-27T23:27:56Z","2018-04-28T00:17:23Z"
"","4757","Trogdor: Added commonClientConf and adminClientConf to workload specs","Currently, WorkerUtils will be able to create topics when there is no security. To be able to work with secure kafka, WorkerUtils.createTopic() needs to be able to take security configs. This PR adds commonClientConf field to both producer bench and roundtrip workload specs so that users can specify security and other common configs once for producer/consumer and adminClient. Also added adminClientConf field to workload specs so that users can specify adminClient specific configs if they want to. For completeness, added consumerConf and producerConf to roundtrip workload spec.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-03-22T17:02:26Z","2018-04-07T01:28:25Z"
"","5340","[KAFKA-7132] [WIP] Consider adding a faster form of rebalance","Currently, when a consumer falls out of a consumer group, it will restart processing from the last checkpointed offset. However, this design could result in a lag which some users could not afford to let happen. For example, lets say a consumer crashed at offset 100, with the last checkpointed offset being at 70. When it recovers at a later offset (say, 120), it will be behind by an offset range of 50 (120 - 70). This is because the consumer restarted at 70, forcing it to reprocess old data. To avoid this from happening, one option would be to allow the current consumer to start processing not from the last checkpointed offset (which is 70 in the example), but from 120 where it recovers. Meanwhile, a new KafkaConsumer will be instantiated and start reading from offset 70 in concurrency with the old process, and will be terminated once it reaches 120. In this manner, a considerable amount of lag can be avoided, particularly since the old consumer could proceed as if nothing had happened.   Here is the design doc for the pull request:  https://cwiki.apache.org/confluence/display/KAFKA/KIP-333%3A+Add+faster+mode+of+rebalancing  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ConcurrencyPractitioner","2018-07-06T06:04:09Z","2019-01-02T03:13:48Z"
"","5436","KAFKA-7164: Follower should truncate after every leader epoch change","Currently, we skip the steps to make a replica a follower if the leader does not change, inlcuding truncating the follower log if necessary. This can cause problems if the follower has missed one or more leader updates. Change the logic to only skip the steps if the new epoch is the same or one greater than the old epoch. Tested with unit tests that verify the behavior of Partition.scala and that show log truncation when the follower's log is ahead of the leader's, the follower has missed an epoch update, and the follower receives a LeaderAndIsrRequest making it a follower  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2018-07-31T05:52:43Z","2018-08-13T16:14:33Z"
"","5014","[KAFKA-6608] Add timeout parameter to methods which retrieves offsets","Currently, this PR is based off of what was agreed upon in KIP-266. For further information, please look in this link: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75974886","closed","","ConcurrencyPractitioner","2018-05-12T18:43:37Z","2018-05-30T15:29:55Z"
"","5145","KAFKA-7002: Add a config property for DLQ topic's replication factor (KIP-298)","Currently, the replication factor is hardcoded to a value of 3. This means that we cannot use a DLQ in any cluster setup with less than three brokers. It is better to have the user specify this value if the default value does meet the requirements.  Testing: A unit test is added.  Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-06-06T04:57:32Z","2020-10-16T06:32:36Z"
"","5085","KAFKA-6957 make InternalTopologyBuilder accessible from AbstractStrea…","Currently, the AbstractStream class defines a copy-constructor that allow to extend KStream and KTable APIs with new methods without impacting the public interface.  However adding new processor or/and store to the topology is made throught the internalTopologyBuilder that is not accessible from AbstractStream subclasses defined outside of the package (package visibility).","closed","streams,","fhussonnois","2018-05-26T19:33:46Z","2018-05-30T18:13:25Z"
"","5164","KAFKA-6946: Keep the session id for incremental fetch when fetch responses are throttled","Currently, a throttled fetch response is returned with INVALID_SESSION_ID, which causes dropping the current fetch session if incremental fetch is in progress. This patch fixes this by returning the correct session id.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-06-08T12:36:27Z","2018-06-11T17:37:55Z"
"","4574","MINOR: Free sends in MultiSend as they complete","Currently we hold onto all Records references in a multi-partition fetch response until the full response has completed. This can be a problem when the records have been down-converted since they will be occupying a potentially large chunk of memory. This patch changes the behavior in MultiSend so that once a Send is completed, we no longer keep a reference to it, which will allow the Records objects to be freed sooner.  I have added a simple unit test to verify that sends are removed as the MultiSend progresses.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-15T19:24:58Z","2018-02-16T15:58:50Z"
"","4571","KAFKA-6561: Change visibility of aclMatch in SimpleAclAuthorizer to allow access from subclasses","Currently the visibility of the aclMatch function in the SimpleAclAuthorizer class is set to private, thus prohibiting subclasses from overriding this method. I think this was originally done as this function is not supposed to be part of the public Api of this class, which makes sense. However when creating a custom authorizer this would be a very useful method to override, as it allows to reuse a large amount of boilerplate code around loading and applying ACLs and simply changing the way that ACLs are matched.  By changing the visibility to _protected_ we allow subclasses to override this.","closed","","soenkeliebau","2018-02-14T17:31:57Z","2018-02-27T15:39:43Z"
"","4618","KAFKA-6591: Move super user check before ACL matching","Currently the check whether a user as a super user in SimpleAclAuthorizer is performed only after all other ACLs have been evaluated. Since all requests from a super user are granted we don't really need to apply the ACLs.  This commit returns true if the user is a super user before checking ACLs, thus bypassing the needless evaluation effort.","closed","core,","soenkeliebau","2018-02-25T22:12:49Z","2018-06-25T22:46:10Z"
"","4604","MINOR: Cancel port forwarding for HttpMetricsCollector during cleanup","Currently port forwarding is setup for HttpMetricsCollector when the Service's start_node method is called, but not canceled during stop. This hasn't presented a problem so far because we don't have tests that use this *and* restart the service. However, if a test/service does that, it will throw an exception since the port is already bound.  This just does the cleanup when stopping so a subsequent attempt to start again will succeed.  https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1320 is a test run for a Test that uses ProducerPerformanceService, which in turn uses HttpMetricsCollector to validate the change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-02-20T22:02:17Z","2018-03-02T18:27:51Z"
"","4969","KAFKA-6864: Pass null key and value to deserializer","currently null values are not passed to the keyDeserializer and valueDeserializer in the Fetcher class. This prevents custom deserialization of null values.","open","","makubi","2018-05-04T14:37:43Z","2018-05-07T09:13:17Z"
"","4633","KAFKA-5891: Proper handling of LogicalTypes in Cast","Currently logical types are dropped during Cast Transformation. This patch fixes this behaviour.  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","connect,","maver1ck","2018-03-01T20:55:52Z","2018-08-20T23:43:12Z"
"","4752","KAFKA-6697; JBOD configured broker should not die if log directory is invalid","Currently JBOD configured broker will still die on startup if dir.getCanonicalPath() throws IOException. We should mark such log directory as offline and broker should still run if there is good disk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-21T21:28:17Z","2018-06-21T19:14:01Z"
"","4694","KAFKA-6640; Improve efficiency of KafkaAdminClient.describeTopics()","Currently in KafkaAdminClient.describeTopics(), for each topic in the request, a complete map of cluster and errors will be constructed for every topic and partition. This unnecessarily increases the complexity of describeTopics() to O(n^2). This patch improves the complexity to O(n).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-12T23:28:34Z","2018-03-14T22:01:09Z"
"","4883","KAFKA-6796; Fix surprising UNKNOWN_TOPIC error from requests to non-replicas","Currently if the client sends a produce request or a fetch request to a broker which isn't a replica, we return UNKNOWN_TOPIC_OR_PARTITION. This is a bit surprising to see when the topic actually exists. It would be better to return NOT_LEADER to avoid confusion. Clients typically handle both errors by refreshing metadata and retrying, so changing this should not cause any change in behavior on the client. This case can be hit following a partition reassignment after the leader is moved and the local replica is deleted.  To validate the current behavior and the fix, I've added  integration tests for the fetch and produce APIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-04-16T22:49:07Z","2018-04-25T04:49:45Z"
"","5289","KAFKA-7096 : Clear buffered data for unassigned topicPartitions","currently if a client has assigned topics : T1, T2, T3 and calls poll(), the poll might fetch data for partitions for all 3 topics T1, T2, T3. Now if the client unassigns some topics (for example T3) and calls poll() we still hold the data (for T3) in the completedFetches queue until we actually reach the buffered data for the unassigned Topics (T3 in our example) on subsequent poll() calls, at which point we drop that data. This process of holding the data is unnecessary.  When a client creates a topic, it takes time for the broker to fetch ACLs for the topic. But during this time, the client will issue fetchRequest for the topic, it will get response for the partitions of this topic. The response consist of TopicAuthorizationException for each of the partitions. This response for each partition is wrapped with a completedFetch and added to the completedFetches queue. Now when the client calls the next poll() it sees the TopicAuthorizationException from the first buffered CompletedFetch. At this point the client chooses to sleep for 1.5 min as a backoff (as per the design), hoping that the Broker fetches the ACL from ACL store in the meantime. Actually the Broker has already fetched the ACL by this time. When the client calls poll() after the sleep, it again sees the TopicAuthorizationException from the second completedFetch and it sleeps again. So it takes (1.5 * 60 * partitions) seconds before the client can see any data. With this patch, the client when it sees the first TopicAuthorizationException, it can all assign(EmptySet), which will get rid of the buffered completedFetches (those with TopicAuthorizationException) and it can again call assign(TopicPartitions) before calling poll(). With this patch we found that client was able to get the records as soon as the Broker fetched the ACLs from ACL store.","closed","","MayureshGharat","2018-06-25T20:21:45Z","2018-09-11T04:44:07Z"
"","5292","KAFKA-7097 VerifiableProducer does not work properly with --message-create-time argument","Currently create time is interpreted as integer.  This PR makes the tool accept long values.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-06-26T02:08:38Z","2018-06-29T20:37:35Z"
"","5191","KAFKA-7039 : Create an instance of the plugin only it's a Versioned Plugin","Create an instance of the plugin only it's a Versioned Plugin. Prior to KIP-285, this was done for only for Connector and this PR will continue to have the same behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-06-11T19:04:07Z","2020-10-16T06:02:59Z"
"","4887","ConnectorConfig could override Producer's config","Could we improve Worker to be able to override producer configuration properties from ConnectorConfig. It would be able to override for example sasl.jaas.config (credentials)  Is there another way to handle this case?  Thank you  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cyriljoui","2018-04-17T11:03:01Z","2018-05-16T07:07:08Z"
"","4516","KAFKA-6511: Corrected list parsing logic","Corrected the parsing of invalid list values. A list can only be parsed if it contains elements that have a common type, and a map can only be parsed if it contains keys with a common type and values with a common type.  This should only be merged to `trunk`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-02-02T18:21:20Z","2020-10-16T06:05:16Z"
"","4625","KAFKA-6593; Fix livelock with consumer heartbeat thread in commitSync","Contention for the lock in ConsumerNetworkClient can lead to a livelock situation in which an active commitSync is unable to make progress because its completion is blocked in the heartbeat thread. The fix is twofold:  1) We change ConsumerNetworkClient to use a fair lock to reduce the chance of each thread getting starved. 2) We eliminate the dependence on the lock in ConsumerNetworkClient for callback completion so that callbacks will not be blocked by an active poll().  I've left this as a WIP patch since I am still considering test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","hachikuji","2018-02-27T18:13:29Z","2018-03-01T19:10:05Z"
"","5060","Fixed ConsumerOffset#path","consumer offset path in zookeeper should be `s""/consumers/${group}/offsets/${topic}/${partition}""` instead of `s""/consumers/${group}/offset/${topic}/${partition}""`. Added `s` to the word `offset`.","closed","","maytals","2018-05-22T11:22:51Z","2020-02-18T05:26:40Z"
"","5190","KAFKA-7031 : Connect API shouldn't depend on jersey","Connect API currently depends on Jersey API as a side-effect of KIP-285. It should only depend on the JAX RS API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2018-06-11T18:18:46Z","2020-10-16T06:32:36Z"
"","5441","MINOR: Fixed default streams state dir location.","Co-authored-by: Mickael Maison  Co-authored-by: Simon Clark   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Malakhit","2018-08-01T16:06:33Z","2018-08-06T19:19:19Z"
"","4829","KAFKA-6750: Add listener name to authentication context (KIP-282)","Co-authored-by: Edoardo Comar ecomar@uk.ibm.com Co-authored-by: Mickael Maison mickael.maison@gmail.com  This KIP adds the String value of the ListenerName used for the connection to the AuthenticationContext.  This allows PrincipalBuilders to retrieve this value and build different Principal based on the source of the connection. This is especially interesting in deployments where inter-broker traffic is on a different network/listener than user traffic or when the same protocol is used by several listeners.  The change in itself is mostly ""plumbing"" as the listener name needs to be passed from `ChannelBuilders` all the way down to all classes implementing `AuthenticationContext`.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","mimaison","2018-04-05T16:35:37Z","2020-05-15T23:05:27Z"
"","5243","KAFKA-6203","Close file handlers before folder is renamed to avoid AccessDeniedException on Windows Unmap memory when file is closed to avoid IOException-s, this means that memory is unmapped if file is offline or it is closed.","open","","proarchii","2018-06-18T07:09:26Z","2019-03-09T11:54:00Z"
"","5427","MINOR: Code cleanup of 'clients' module","Cleanup involves * Removing redundant / unnecessary code * Fixing typos * Simplifying code when possible * Refactoring to use lambda expressions in some places  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","vahidhashemian","2018-07-26T20:15:31Z","2018-09-09T21:25:31Z"
"","4978","KAFKA-6878 NPE when querying global state store not in READY state","Check whether cache is null before retrieving from cache.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-05-08T17:17:09Z","2018-05-09T20:22:43Z"
"","5155","MINOR: follow-up test for create topic acl (KIP-277)","Check handling of a Metadata Request with a mix of existing and non-existing topics.  The particular codepath was not exercised by a test in the original PR. @ijuma fixed the code path on merge This small testcase covers that  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-06-06T16:13:54Z","2018-06-17T22:04:16Z"
"","4981","Changing log from debug to info for messages in ConsumerCoordinator","Changing some log messages from debug to info in ConsumerCoordinator. It is hard to troubleshoot rebalance issues without them.   Only log-related changes. Hence, just ran ./gradlew build  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","navina","2018-05-09T01:40:24Z","2018-05-09T02:04:03Z"
"","5467","KAFKA-7240: -total metrics in Streams are incorrect","Changes: * Add `org.apache.kafka.streams.processor.internals.metrics.CumulativeCount` analogous to `Count`, but not a `SampledStat` * Use `CumulativeCount` for -total metrics in streams instead of `Count`  Testing strategy: * Add a test in StreamsMetricsImplTest which fails on old, incorrect behavior  The contribution is my original work and I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","lendle","2018-08-06T22:08:26Z","2018-08-24T18:20:58Z"
"","5111","MINOR: Use thread name and task for sensor name","Changes to keep the operation name as is and make the sensor name unique.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-05-31T21:29:39Z","2018-06-01T21:48:44Z"
"","4697","KAFKA-6637: Avoid divide by zero exception when segment.ms is set to 0","Changes of this minor patch include: 1. When topic-level `segment.ms` is set to 0, no divide by zero error will be thrown from within `LogConfig.randomSegmentJitter` 2. Kafka ignores the effect of `log.roll.ms` when it was set to 0, still use `log.roll.hours` instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-03-13T06:57:56Z","2018-03-13T07:07:28Z"
"","5244","MINOR:Optimized the path compression in quick union","Changes are following: 1. Optimized the path compression logic in quick union Previously, the logic compressed the path one step up for a node at a time. Now, the new logic compresses the path to the root node at a time.","closed","streams,","xinzhg","2018-06-18T07:15:18Z","2018-06-26T21:19:56Z"
"","4716","KAFKA-6661: Ensure sink connectors don’t resume consumer when task is paused","Changed WorkerSinkTaskContext to only resume the consumer topic partitions when the connector/task is not in the paused state.  The context tracks the set of topic partitions that are explicitly paused/resumed by the connector, and when the WorkerSinkTask resumes the tasks it currently resumes all topic partitions *except* those that are still explicitly paused in the context. Therefore, the change above should result in the desired behavior.  Several debug statements were added to record when the context is called by the connector.  This can be backported to older releases, since this bug goes back to 0.10 or 0.9.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-03-15T00:44:28Z","2020-10-16T06:33:06Z"
"","4972","MINOR - Fix typo in Streams Dev Guide","Changed value in table from `null (tombstone)` to `null`","closed","docs,","joel-hamill","2018-05-07T18:57:55Z","2018-05-11T17:27:07Z"
"","5183","KAFKA-6948 - Change comparison to avoid overflow inconsistencies","Change timestamp comparison following what the Java documentation recommends to help preventing such errors.  @guozhangwang I create the new PR as requested 😄   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thisthat","2018-06-11T08:32:38Z","2021-07-23T01:56:55Z"
"","5078","KAFKA-6948 Change comparison to avoid overflow inconsistencies","Change timestamp comparison following what the Java documentation recommends to help preventing such errors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thisthat","2018-05-25T08:34:54Z","2018-06-11T08:02:13Z"
"","4599","MINOR: Fix streams broker compatibility test.","Change the string in the test condition to the one that is logged  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2018-02-20T15:03:20Z","2018-02-20T19:36:00Z"
"","4899","KAFKA-6807: Inconsistent method name.","Change the method name ""readTo"" to ""lesser"". The method is named as ""readTo"", but the method will return the variable with lesses value. Thus, the name ""readTo"" is inconsistent with the method body code. Rename the method as ""lesser"" should be better.","open","","Kui-Liu","2018-04-19T16:15:24Z","2018-04-19T16:15:24Z"
"","5416","KAFKA-2423: Expand scalafmt coverage to core","Change spotless target to find all scala files.  Also submit code changes from the output of ""./gradlew spotlessApply"".  Build and ran all unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rchiang","2018-07-23T22:43:34Z","2018-08-06T17:42:40Z"
"","5059","this is more a warn/error log than info","change log level","open","","lisa2lisa","2018-05-22T07:39:00Z","2018-05-22T07:39:00Z"
"","4485","KAFKA-6195: Resolve DNS aliases in bootstrap.server","Change described in KIP-235 https://cwiki.apache.org/confluence/display/KAFKA/KIP-235%3A+Add+DNS+alias+support+for+secured+connection  I license the work to the Apache Kafka project under the project's open source license.","closed","","lepolac","2018-01-29T17:19:03Z","2018-10-13T21:19:56Z"
"","4727","KAFKA-6672; ConfigCommand should create config change parent path if needed","Change `KafkaZkClient.createConfigChangeNotification` to ensure creation of the change directory. This fixes failing system tests which depend on setting SCRAM credentials prior to broker startup. Existing test case has been modified for new expected usage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-16T23:35:47Z","2018-03-17T05:29:23Z"
"","4798","MINOR: Catch null pointer exception for empty leader URL when assignment is null","Catch null pointer exception for empty leader URL when assignment is null.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","asdf2014","2018-03-30T09:12:07Z","2020-10-16T06:05:17Z"
"","5142","KAFKA-7000: update assignment in Consumer#position","Call `ConsumerCoordinator.poll` in `Consumer.position` to ensure we have updated assignment metadata before potentially throwing an exception regarding our assignment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","vvcephei","2018-06-05T23:37:05Z","2018-06-07T22:28:52Z"
"","5387","KAFKA-7180: Fixing the flaky test testHWCheckpointWithFailuresSingleLogSegment","By waiting until server1 has joined the ISR before shutting down server2  Rerun the test method many times after the code change, and there is no flakiness any more.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-07-18T17:15:18Z","2018-07-30T04:08:39Z"
"","4733","Minor: Broker should return the correct error message to consumer whe…","Broker always returns ""Errors.COORDINATOR_NOT_AVAILABLE"" to coordinator even though the root cause is another one. Currently, coordinator just logs the error so it hasn't introduced any bugs yet. I make this patch minor since the benefit is to correct the log content in coordinator.  The test ""testCreateResponseForFindCoordinatorRequest"" assume the error encounter by FindCoordinatorRequest is equal with the error in FindCoordinatorResponse.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-03-19T09:54:16Z","2018-03-20T01:44:44Z"
"","4531","KAFKA-4641: Add more unit test for stream thread","Before the patch, jacoco coverage test:   Element | Missed Instructions | Cov. | Missed Branches | Cov. | Missed | Cxty | Missed | Lines | Missed | Methods | Missed | Classes -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- Total | 3,386 of 22,177 | 84% | 336 of 1,639 | 79% | 350 | 1,589 | 526 | 4,451 | 103 | 768 | 1 | 102 StreamThread |   | 77% |   | 76% | 27 | 102 | 48 | 299 | 1 | 31 | 0 | 1  After the patch:   Element | Missed Instructions | Cov. | Missed Branches | Cov. | Missed | Cxty | Missed | Lines | Missed | Methods | Missed | Classes -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- Total | 3,329 of 22,180 | 84% | 329 of 1,639 | 79% | 345 | 1,590 | 516 | 4,452 | 102 | 769 | 1 | 102 StreamThread |   | 81% |   | 80% | 23 | 103 | 39 | 300 | 1 | 32 | 0 | 1    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-02-06T17:55:39Z","2020-04-25T00:03:50Z"
"","5306","KAFKA-7112: Only resume restoration if state is still PARTITIONS_ASSIGNED after poll","Before KIP-266, consumer.poll(0) would call `updateAssignmentMetadataIfNeeded(Long.MAX_VALUE)`, which makes sure that the rebalance is definitely completed, i.e. both onPartitionRevoked and onPartitionAssigned called within this `poll(0)`. After KIP-266, however, it is possible that only onPartitionRevoked will be called if timeout is elapsed. And hence we need to double check that state is still `PARTITIONS_ASSIGNED` after the `consumer.poll(duration)` call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-28T00:39:29Z","2018-06-28T17:20:32Z"
"","4813","KAFKA-6739: Ignore the presence of headers when down-converting from V2 to V1/V0","Because V1/V0 message formats do not expect a header, ignore their presence when down-converting V2 messages that contain headers. Added a test-case to verify down-conversion sanity in presence of headers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-04-03T00:01:38Z","2018-04-03T17:57:52Z"
"","4928","KAFKA-6292: Improved FileLogInputStream batch position checks in order to avoid type overflow related errors","Basically, my change was very simple - switch from sum operations to subtraction to avoid type casting in checks and type overflow during FlieLogInputStream work, especially in cases where property `log.segment.bytes` was set close to the `Integer.MAX_VALUE` and used as a `position` inside `nextBatch()` function.  All related Unit tests are working as intended. No new tests (probably) required.","closed","","SuppieRK","2018-04-25T12:51:31Z","2018-05-09T00:07:51Z"
"","5438","KAFKA-6963: KIP-310: Add a Kafka Source Connector to Kafka Connect","Based on earlier work by myself at: https://github.com/Comcast/MirrorTool-for-Kafka-Connect. I license the work to the project under the project's open source license.  Testing Done: - Added unit tests for source connector/task - Tested locally on docker stack using various configuration options, task workers and topic characteristics","closed","","rhysmccaig","2018-08-01T07:12:57Z","2018-10-26T00:42:11Z"
"","4789","KAFKA-6711: GlobalStateManagerImpl should not write offsets","Backport KAFKA-6711 to 1.0.x which is actually affected codes  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cemo","2018-03-28T17:18:05Z","2018-06-14T23:24:45Z"
"","5260","KAFKA-5764; Add toLowerCase support to sasl.kerberos.principal.to.loc…","Backport commit from Kafka 1.1 to Kafka 1.0 (Add toLowerCase support to sasl.kerberos.principal.to.local rule)  Add toLowerCase support to sasl.kerberos.principal.to.local rule (KIP-203)  ORIGINAL COMMIT INFORMATION:  Author: Manikumar Reddy   Reviewers: Jason Gustafson   Closes #3800 from omkreddy/KAFKA-5764-REGEX  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","umaanbazhagan","2018-06-20T23:31:43Z","2019-12-19T15:28:47Z"
"","5258","MINOR: Avoid using a big lock in pool#getAndMaybePut","Avoid using a big createLock in pool#getAndMaybePut by using the ConcurrentHashMap#computeIfAbsent function.","closed","performance,","cmccabe","2018-06-20T18:23:13Z","2018-06-24T02:27:56Z"
"","5237","KAFKA-7012: Don't process SSL channels without data to process","Avoid unnecessary processing of SSL channels when there are some bytes buffered, but not enough to make progress. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-15T23:15:53Z","2018-06-19T14:16:17Z"
"","5007","MINOR: Remove dependence on __consumer_offsets in AdminClient listConsumerGroups","Avoid dependence on the internal __consumer_offsets topic to handle `listConsumerGroups()` since it unnecessarily requires users to have Describe access on an internal topic. Instead we query each broker independently. For most clusters, this amounts to the same thing since the default number of partitions for __consumer_offsets is 50. This also provides better encapsulation since it avoids exposing the use of __consumer_offsets, which gives us more flexibility in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-11T16:08:33Z","2018-05-23T19:15:25Z"
"","4814","KAFKA-6560: Use single query for getters as well","As titled, use single query for getters as a follow-up of KAFKA-6560.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-04-03T01:17:22Z","2020-04-24T23:51:33Z"
"","4826","KAFKA-6747 Check whether there is in-flight transaction before aborting transaction","As Frederic reported on mailing list under the subject ""kafka-streams Invalid transition attempted from state READY to state ABORTING_TRANSACTION"", producer#abortTransaction should only be called when transactionInFlight is true.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2018-04-04T23:34:34Z","2018-06-12T17:45:31Z"
"","5028","KAFKA-3355: Fix GetOffsetShell doesn't work with SASL enabled Kafka","As described in [JIRA: KAFKA-3355](https://issues.apache.org/jira/browse/KAFKA-3355), i fixed this issue for kafka 0.10.1.1 in production systems, hoping it may help users with this version.","closed","","murong00","2018-05-17T03:11:19Z","2018-05-26T16:26:23Z"
"","4609","KAFKA-6578: Changed the Connect distributed and standalone main method to log all exceptions","Any exception thrown by calls within a `main()` method are not logged unless explicitly done so. This change simply adds a try-catch block around most of the content of the distributed and standalone `main()` methods.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2018-02-21T23:41:31Z","2018-02-23T06:29:49Z"
"","4590","MINOR: Fix file source task configs in system tests.","Another fall-through of `headers.converter` and `batch.size` properties. Here in `FileStreamSourceConnector` tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-02-19T20:36:45Z","2020-10-16T06:24:15Z"
"","4598","MINOR: Add registerController method to KafkaZkClient…","and change KafkaController to use this instead of lower level zk calls. As minor cleanup, remove InZk postfixes from registerBrokerInZk and updateBrokerInfoInZk.  As `checkedEphemeralCreate` is not used any more from outside of KafkaZkClient it's visibility was also reduced  ControllerIntegrationTest already covers this functionality well, it validates the refactor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","smurakozi","2018-02-20T14:20:39Z","2018-07-21T18:40:01Z"
"","5102","KAFKA-1614: Partition log directory and segments info exposed via JMX","An updated version for https://github.com/apache/kafka/pull/1196","open","","everpcpc","2018-05-31T03:27:36Z","2019-08-15T18:48:59Z"
"","5417","KAFKA-7194; Fix buffer underflow if onJoinComplete is retried after failure","An untimely wakeup can cause `ConsumerCoordinator.onJoinComplete` to throw a `WakeupException` before completion. On the next `poll()`, it will be retried, but this leads to an underflow error because the buffer containing the assignment data will already have been advanced. The solution is to duplicate the buffer passed to `onJoinComplete`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-07-23T23:14:22Z","2018-07-24T15:48:45Z"
"","5449","Fix a typo in delegation.token.expiry.time.ms docs","although it is obvious from the property name. Was:  > val DelegationTokenExpiryTimeMsDoc = ""The token validity time in seconds before the token needs to be renewed. Default value 1 day.""  Proposed:  > val DelegationTokenExpiryTimeMsDoc = ""The token validity time in **mili**seconds before the token needs to be renewed. Default value 1 day.""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sm4rk0","2018-08-02T15:02:21Z","2018-08-08T07:17:36Z"
"","5104","MINOR: Fix recently added method in SinkTaskContext to be backward compatible","Although Connect should always implement this interface and all its methods, other projects might implement this for tests or other environments, so we should make it backward compatible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-05-31T03:35:59Z","2020-10-16T06:02:57Z"
"","5255","MINOR: Use exceptions in o.a.k.common if possible and deprecate ZkUtils","Also: - Remove exceptions in `kafka.common` that are no longer used. - Keep `kafka.common.KafkaException` as it's still used by `ZkUtils`, `kafka.admin.AdminClient` and `kafka.security.auth` classes and we would like to maintain compatibility for now. - Add deprecated annotation to `kafka.admin.AdminClient`. The scaladoc stated that the class is deprecated, but the annotation was missing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-20T06:14:12Z","2018-06-20T12:05:56Z"
"","4770","MINOR: Remove dead code in LogCleaner.validateReconfiguration","Also tweak condition to be closer to error message.  No behaviour change, so no tests needed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-24T04:21:39Z","2018-03-28T04:03:40Z"
"","4993","MINOR: Remove deprecated valueTransformer.punctuate","Also removed the InternalValueTransformerWithKey / Supplier which is used to mock away the deprecated punctuate function.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-05-10T01:19:03Z","2020-04-24T23:55:11Z"
"","4809","MINOR: Rename RecordFormat to RecordVersion","Also include a few clean-ups:  * Method/variable/parameter renames to make them consistent with the class name * Return `ApiVersion` from `minSupportedFor` * Use `values` to remove some code duplication * Reduce duplication in `ApiVersion` by introducing the `shortVersion` method and building the versions map programatically * Avoid unnecessary `regex` in `ApiVersion.apply` * Added scaladoc to a few methods  Some of these were originally discussed in:  https://github.com/apache/kafka/pull/4583#pullrequestreview-98089400  Added a test for `ApiVersion.shortVersion`. Relying on existing tests for the rest since there is no change in behaviour.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-02T06:06:35Z","2018-05-11T05:06:23Z"
"","5231","MINOR: provide an example for deserialization exception handler","Also added a paragraph from data types to link to the example code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-14T20:42:47Z","2020-04-24T23:50:39Z"
"","4820","KAFKA-6684: Cast transform bytes","Allow to cast LogicalType to string by calling the serialized (Java) object's toString().     Added tests for `BigDecimal` and `Date` as whole record and as fields.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","amitsela","2018-04-03T16:17:48Z","2020-10-16T06:33:06Z"
"","5234","KAFKA-7060: Add override arguments to ConnectDistributed command line","Allow ConnectDistributed to accept an unlimited number of --override key=value command-line arguments.  This is an implementation of KIP-316.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kevin-laff","2018-06-15T00:00:57Z","2018-06-18T18:41:00Z"
"","4721","MINOR: improve trogdor commandline","Allow -c as a synonym for --agent.config and --coordinator.config.  Allow -n as a synonym for --node-name.  Add an example trogdor.conf file.","closed","","cmccabe","2018-03-16T00:17:27Z","2019-05-20T18:57:52Z"
"","4675","MINOR: Improve Trogdor client logging.","AgentClient and CoordinatorClient should have the option of logging failures to custom log4j objects.  There should also be builders for these objects, to make them easier to extend in the future.","closed","","cmccabe","2018-03-09T22:32:36Z","2019-05-20T18:57:59Z"
"","5027","KAFKA-6897: Prevent producer from blocking indefinitely after close","After successful completion of KafkaProducer#close, it is possible that an application calls KafkaProducer#send. If the send is invoked for a topic for which we do not have any metadata, the producer will block until `max.block.ms` elapses - we do not expect to receive any metadata update in this case because Sender (and NetworkClient) has already exited. It is only when RecordAccumulator#append is invoked that we notice that the producer has already been closed and throw an exception. If `max.block.ms` is set to Long.MaxValue (or a sufficiently high value in general), the producer could block awaiting metadata indefinitely.  This patch makes sure `Metadata#awaitUpdate` periodically checks if the network client has been closed, and if so bails out as soon as possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-05-17T01:46:48Z","2018-07-23T07:02:22Z"
"","4984","Minor: Remove the unused field in DelegatingClassLoader","After [3173](https://github.com/apache/kafka/commit/e0150a25e8), the field ""activePaths"" is not used anymore.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2018-05-09T07:39:08Z","2020-10-16T06:33:06Z"
"","5077","MINOR: AdminClient should respect retry backoff","AdminClient should backoff when retrying a Call. Fixed and added a unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-25T07:34:31Z","2018-05-26T00:57:04Z"
"","5122","KAFKA-6979: Add default.api.timeout.ms to KafkaConsumer (KIP-266)","Adds a configuration that specifies the default timeout for KafkaConsumer APIs that could block.","closed","","dhruvilshah3","2018-06-01T21:57:22Z","2018-06-12T23:29:51Z"
"","4562","MINOR: Resuming Tasks should not be initialized twice","additionally: removes race condition in StreamThreadTest plus cleanup","closed","streams,","mjsax","2018-02-13T02:03:51Z","2018-02-15T19:00:13Z"
"","5128","KAFKA-6938: Add documentation for accessing Headers on Kafka Streams Processor API","Adding documentation to access Processor Context on Processor API.","closed","docs,","jeqo","2018-06-03T13:58:42Z","2020-08-08T09:20:59Z"
"","5071","KAFKA-6935: Add config for allowing optional optimization","Adding configuration to `StreamsConfig` allowing for making topology optimization optional.  Added unit tests are verifying default values, setting correct value and failure on invalid values.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-05-23T14:58:29Z","2018-06-07T01:12:02Z"
"","5126","KAFKA-5919: Adding checks on ""version"" field for tools using it","Adding checks on ""version"" field for tools using it. This is a new version of the closed PR #3887 (to see for more comments and related discussion).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ppatierno","2018-06-02T13:25:28Z","2018-06-06T06:45:48Z"
"","5084","MINOR: Add upgrade notes for new consumer poll","Added upgrade notes for the new poll() API. I also added a few small cleanups from #4855.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-05-26T19:18:08Z","2018-05-31T00:28:18Z"
"","4918","KAFKA-6795: Added unit tests for ReplicaAlterLogDirsThread","Added unit tests for ReplicaAlterLogDirsThread. Mostly focused on unit tests for truncating logic.  Fixed  ReplicaAlterLogDirsThread.buildLeaderEpochRequest() to use future replica's latest epoch (not the latest epoch of replica it is fetching from). This follows the logic that offset for leader epoch request should be based on leader epoch of the follower (in this case it's the future local replica).   Also fixed PartitionFetchState constructor that takes offset and delay. The code ignored the delay parameter and used 0 for the delay. This constructor is used only by another constructor which passes delay = 0, which luckily works.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","apovzner","2018-04-24T03:48:54Z","2018-04-25T21:49:53Z"
"","5294","KAFKA-6944: Add system tests testing the new throttling behavior using older clients/brokers","Added two additional test cases to quota_test.py, which run between brokers and clients with different throttling behaviors. More specifically, 1. clients with new throttling behavior (i.e., post-KIP-219) and brokers with old throttling behavior (i.e., pre-KIP-219) 2. clients with old throttling behavior and brokers with new throttling behavior  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonlee2","2018-06-26T07:32:51Z","2018-06-27T23:50:58Z"
"","4834","KAFKA-6760: Responses not logged properly in controller","Added toString() to LeaderAndIsrResponse and StopReplicaResponse  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-04-07T15:27:11Z","2018-06-01T23:40:17Z"
"","4643","[KAFKA-6608] Add timeout parameter to methods which fetches and reset…","Added timeout parameters to methods.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","ConcurrencyPractitioner","2018-03-03T19:12:48Z","2018-05-12T18:28:11Z"
"","4903","MINOR: Fix formatting in --new-consumer deprecation warning","Added space between sentences, to make the text look nicer. All unit tests and integrations tests pass after the change.  **Before:** ``` $ bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list The [new-consumer] option is deprecated and will be removed in a future major release.The new consumer is used by default if the [bootstrap-server] option is provided. Note: This will not show information about old Zookeeper-based consumers. ```  **After:** ``` $ bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list The [new-consumer] option is deprecated and will be removed in a future major release. The new consumer is used by default if the [bootstrap-server] option is provided. Note: This will not show information about old Zookeeper-based consumers. ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","patrikerdes","2018-04-20T06:41:36Z","2018-04-23T19:08:53Z"
"","4510","Kafka 6363 add second check for end offset during restore","Added second check to ensure changelog topic is not written to during restore if it is a changelog topic for a state store.  This will be tricky to test, but I've pushed this PR to get feedback on the approach.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-02-01T22:45:06Z","2018-02-02T03:56:36Z"
"","4775","Kafka-6693: Added consumer workload to Trogdor","Added consumer only workload to Trogdor. The topics must already be pre-populated. The spec lets the user request topic pattern and range of partitions to assign to [startPartition, endPartition].  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2018-03-26T17:02:51Z","2018-04-10T16:07:00Z"
"","4673","Trogdor's ProducerBench does not fail if topics exists","Added configs to ProducerBenchSpec: topicPrefix: name of topics will be of format topicPrefix + topic index. If not provided, default is ""produceBenchTopic"". partitionsPerTopic: number of partitions per topic. If not provided, default is 1. replicationFactor: replication factor per topic. If not provided, default is 3.  The behavior of producer bench is changed such that if some or all topics already exist (with topic names = topicPrefix + topic index), and they have the same number of partitions as requested, the worker uses those topics and does not fail. The producer bench fails if one or more existing topics has number of partitions that is different from expected number of partitions.  Added unit test for WorkerUtils -- for existing methods and new methods.  Fixed bug in MockAdminClient, where createTopics() would over-write existing topic's replication factor and number of partitions while correctly completing the appropriate futures exceptionally with `TopicExistsException`.","closed","","apovzner","2018-03-09T20:37:35Z","2018-03-20T13:51:46Z"
"","5456","MINOR: System test for error handling and writes to DeadLetterQueue","Added a system test which creates a file sink with json converter and attempts to feed it bad records. The bad records should land in the DLQ if it is enabled, and the task should be killed or bad records skipped based on test parameters.  Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-08-03T03:31:36Z","2020-10-16T06:03:00Z"
"","4511","KAFKA-6364: second check for ensuring changelog topic not changed during restore","Added a second check for race condition where store changelog topic updated during restore, but not if a KTable changelog topic.   This will be tricky to test, but I wanted to push the PR to get feedback on the approach.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-02-01T23:01:38Z","2018-02-14T20:45:18Z"
"","4640","Added Payload class Trogdor to describe producer payload","Added a class that generates producer payload (key and value). Makes sure that the values are populated to target a realistic compression rate (0.3 - 0.4) if compression is used. The generated payload is deterministic and can be replayed from a given position. For now, all generated values are constant size, and key types can be configured to be either null or 8 bytes.   Added messageSize parameter to producer spec, that specifies produced key + message size.","closed","","apovzner","2018-03-02T22:51:58Z","2018-03-09T21:57:05Z"
"","4585","KAFKA-6554; Missing lastOffsetDelta validation before log append","Add validation checks that the offset range is valid and aligned with the batch count prior to appending to the log. I've added several unit tests to verify the various invalid cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-02-18T09:11:26Z","2018-02-20T09:32:10Z"
"","4855","KAFKA-5697: prevent poll() from blocking forever","Add the new stricter-timeout version of `poll` proposed in KIP-266.  The pre-existing variant `poll(long timeout)` would block indefinitely for metadata updates if they were needed, then it would issue a fetch and poll for `timeout` ms  for new records. The initial indefinite metadata block caused applications to become stuck when the brokers became unavailable. The existence of the timeout parameter made the indefinite block especially unintuitive.  This PR adds `poll(Duration timeout)` with the semantics: 1. iff a metadata update is needed:     1. send (asynchronous) metadata requests     2. poll for metadata responses (counts against timeout)         - if no response within timeout, **return an empty collection immediately** 2. if there is fetch data available, **return it immediately** 3. if there is no fetch request in flight, send fetch requests 4. poll for fetch responses (counts against timeout)     - if no response within timeout, **return an empty collection** (leaving async fetch request for the next poll)     - if we get a response, **return the response**  The old method, `poll(long timeout)` is deprecated, but we do not change its semantics, so it remains: 1. iff a metadata update is needed:     1. send (asynchronous) metadata requests     2. poll for metadata responses *indefinitely until we get it* 2. if there is fetch data available, **return it immediately** 3. if there is no fetch request in flight, send fetch requests 4. poll for fetch responses (counts against timeout)     - if no response within timeout, **return an empty collection** (leaving async fetch request for the next poll)     - if we get a response, **return the response**  One notable usage is prohibited by the new `poll`: previously, you could call `poll(0)` to block for metadata updates, for example to initialize the client, supposedly without fetching records. Note, though, that this behavior is not according to any contract, and there is no guarantee that `poll(0)` won't return records the first time it's called. Therefore, it has always been unsafe to ignore the response.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-04-12T00:07:11Z","2018-06-08T17:56:41Z"
"","4662","KIP-267: add MockProcessorContext","Add support for authors of Processor, Transformer, and ValueTransformer implementations to write unit tests.  We verify this component via unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-03-08T03:03:33Z","2018-03-30T14:56:06Z"
"","5134","MINOR: add regression tests for KTable mapValues and filter","Add some regression tests in the style of #5075 to prove the soundness of other changes made in #4919   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-06-04T22:22:29Z","2018-06-05T20:24:18Z"
"","5332","MINOR: Tighten FileRecords size checks to prevent overflow","Add some additional size validation to prevent overflows when using `FileRecords`. This may help us detect the cause of KAFKA-7130.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-07-04T00:41:02Z","2018-07-11T18:44:55Z"
"","5434","KAFKA-6620: Documentation about 'exactly_once' doesn't mention 'transaction.state.log.min.isr'","Add note on `transaction.state.log.min.isr` property, which can cause a problem when `transaction.state.log.replication.factor` is set to 1 for development without changing it to 1.  - note: [GlobalKTableEOSIntegrationTest.java](https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java#L66) - note: [GlobalThreadShutDownOrderTest.java](https://github.com/apache/kafka/blob/trunk/streams/src/test/java/org/apache/kafka/streams/integration/GlobalThreadShutDownOrderTest.java#L70)","closed","","dongjinleekr","2018-07-30T06:45:01Z","2018-09-28T21:41:51Z"
"","4730","KAFKA-6535: Set default retention ms for Streams repartition topics to Long.MAX_VALUE","add new default value ( RETENTION_MS_CONFIG = Long.MAX_VALUE) and fix unit tests","closed","streams,","khaireddine120","2018-03-18T12:37:41Z","2018-04-30T10:18:59Z"
"","4526","KAFKA-6528: Fix transient test failure in testThreadPoolResize","Add locking to access `AbstractFetcherThread#partitionStates` during dynamic thread update. Also make testing of thread updates that trigger retries more resilient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-05T14:27:13Z","2018-02-06T09:50:52Z"
"","4735","KAFKA-3365 Add a documentation field for types and update doc generation","Add documentation method to class Type. Implement it in primitive types and ArraysOf. Generate docs from list of types and include it in protocol.html","closed","","andrasbeni","2018-03-19T20:21:10Z","2018-04-18T16:18:46Z"
"","4595","MINOR: Add docs for ReplicationBytesInPerSec and ReplicationBytesOutPerSec","Add docs for the ReplicationBytesInPerSec and ReplicationBytesOutPerSec metrics. These metrics were introduced in KIP-153 (KAFKA-5194) but the docs were not updated.  Built site-docs, and viewed them in a web browser to confirm.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","wushujames","2018-02-20T09:28:26Z","2018-02-20T21:57:17Z"
"","5351","KAFKA-6999: Document read-write lock usage of caching enabled stores","Add description for the deadlock vulnerability of `ReadOnlyKeyValueStore`.","closed","streams,","dongjinleekr","2018-07-10T04:14:38Z","2018-07-28T14:56:32Z"
"","4653","KAFKA-6615; Add scripts for DumpLogSegments","Add bash and windows scripts for `kafka.tools.DumpLogSegments`. I verified the bash script manually.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-06T15:52:36Z","2018-03-06T23:37:51Z"
"","5200","[KAFKA-7038] Support AdminClient Example","Add AdminClient Example  include `describeCluster` , `createTopics` , `listTopics` , `describeTopics` and `deleteTopics` .   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","darionyaphet","2018-06-12T15:05:23Z","2020-11-04T07:13:40Z"
"","5197","KAFKA-7023: Add unit test","Add a unit test that validates after restoreStart, the options are set with bulk loading configs; and after restoreEnd, it resumes to the customized configs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-12T00:56:02Z","2020-04-24T23:57:28Z"
"","4674","MINOR: add DEFAULT_PORT for Trogdor Agent and Coordinator","Add a DEFAULT_PORT constant for the Trogdor Agent and Coordinator.","closed","","cmccabe","2018-03-09T22:06:51Z","2019-05-20T18:57:09Z"
"","4546","KAFKA-5327: Console Consumer should only poll for up to max messages","Add a check to ensure --max-messages, if set,  must be set no smaller than max.poll.records.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-02-08T02:16:16Z","2018-02-27T02:17:02Z"
"","5080","KAFKA-6900: Add thenCompose to KafkaFuture","Add `KafkaFuture.thenCompose` which behaves similarly to `KafkaFuture.thenAccept` except that it flattens Futures. This means that it's possible to do another async call inside thenCompose without getting a `KafkaFuture>`  It's implemented to have the same signature and behaviour as CompletableFuture.thenCompose  Tested both by writing unit tests but I've also done manual testing","closed","","RichoDemus","2018-05-25T10:43:10Z","2018-06-03T20:59:55Z"
"","5464","KAFKA-2423: Expand scalafmt coverage to core","Add ""spotlessRegex"" as a property.  Any files that match the property get added to the existing list of files to check.  Ran the `gradle spotlessCheck` and `gradle spotlessApply` with these options.  Verified file list manually. * `-PspotlessRegex='core/**/*.scala'` * `-PspotlessRegex='core/**/Replica*.scala core/**/Timer*.scala'` * `-PspotlessRegex='core/src/main/scala/kafka/controller/TopicDeletionManager.scala'`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rchiang","2018-08-06T17:52:25Z","2018-08-07T16:28:44Z"
"","5478","KAFKA-7255: Fix timing issue with create/update in SimpleAclAuthorizer","ACL updates currently  get `(currentAcls, currentVersion)` for the resource from ZK and do a conditional update using `(currentAcls+newAcl, currentVersion)`. This supports concurrent atomic updates if the resource path already exists in ZK. If the path doesn't exist, we currently do a conditional createOrUpdate using `(newAcl, -1)`. But `-1` has a special meaning in ZooKeeper for update operations - it means match any version. So two brokers adding acls using `(newAcl1, -1)` and `(newAcl2, -1)` will result in one broker creating the path and setting `newAcl1`, while the other broker can potentially update the path with `(newAcl2, -1)`, losing `newAcl1`. The timing window is very small, but we have seen intermittent failures in `SimpleAclAuthorizerTest.testHighConcurrencyModificationOfResourceAcls` as a result of this window.   This PR fixes the version used for conditional updates in ZooKeeper. It also replaces the confusing `ZkVersion.NoVersion=-1` used for set(any-version) and get(return not-found) with `ZkVersion.MatchAnyVersion` for set(any-version) and `ZkVersion.UnknownVersion` for get(return not-found) to avoid the return value from `get` matching arbitrary values in `set`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-08-08T10:01:25Z","2018-08-08T16:44:57Z"
"","4647","[WIP] KAFKA-6589 Extract Heartbeat thread from AbstractCoordinator","AbstractCoordinator interacts with the thread via a HeartbeatThreadManager.  The existing behavior was extracted to an abstract class + some glue code in the AbstractCoordinator.  Testing was done using the existing ConsumerCoordinatorTest and WorkerCoordinatorTest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","consumer,","smurakozi","2018-03-05T15:04:51Z","2018-03-13T16:03:37Z"
"","5114","MINOR: Fix JMX serialization by reverting #5011","A recent change returns an anonymous inner class which retains a reference to the outer class, KafkaMbean, which is not serializable.  This commit reverts #5011 as a proper fix needs to probably do serialization lazily as well by overriding `readObject` and `writeObject`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-06-01T18:29:22Z","2018-06-02T20:19:14Z"
"","4825","KAFKA-6650: Allowing transition to OfflineReplica state for replicas without leadership info","A partially deleted topic can end up with some partitions having no leadership info. For the partially deleted topic, a new controller should be able to finish the topic deletion by transitioning the rogue partition's replicas to OfflineReplica state. This patch adds logic to transition replicas to OfflineReplica state whose partitions have no leadership info.  Added a new test method to cover the partially deleted topic case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-04-04T21:00:53Z","2018-05-02T01:17:07Z"
"","5357","MINOR: Fixing JavaDoc for METRICS_RECORDING_LEVEL_CONFIG on ProducerClient","A minor fix, the JavaDoc for the `METRICS_RECORDING_LEVEL_CONFIG` was pointing to an outdated/incorrect value.","closed","","matzew","2018-07-12T13:40:24Z","2018-07-17T03:57:59Z"
"","5150","MINOR: fix typo in TransactionLog JavaDoc","a minor fix typo in TransactionLog JavaDoc","closed","","lambdaliu","2018-06-06T14:03:27Z","2019-02-15T02:10:46Z"
"","4691","MINOR: Some logging improvements for debugging replication","A few small logging improvements which help debugging replication issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-03-12T20:02:42Z","2018-03-19T11:08:13Z"
"","4769","KAFKA-6652: The controller should log failed attempts to transition a replica to OfflineReplica state if there is no leadership info","A controller can pick up a partially deleted topic where some partition znodes have been deleted by the previous controller. In that case, there will be no leadership info for the replica's partition, and hence trying to change the state of the replica to OfflineReplica will fail. This patch adds logs to indicate the failed attempts to change the state. There will be a separate PR to address the partially deleted topic issue.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-03-24T00:24:10Z","2018-03-24T02:57:34Z"
"","4551","KAFKA-6517: Avoid deadlock in ZooKeeperClient during session expiry","`ZooKeeperClient` acquires `initializationLock#writeLock` to establish a new connection while processing session expiry WatchEvent. `ZooKeeperClient#handleRequests` acquires  `initializationLock#readLock`, allowing multiple batches of requests to be processed concurrently, but preventing reconnections while processing requests. At the moment, `handleRequests` holds onto the readLock throughout the method, even while waiting for responses and inflight requests to complete. But responses cannot be delivered if event thread is blocked on the writeLock to process session expiry event. This results in a deadlock. During broker shutdown, the shutdown thread is also blocked since it needs the readLock to perform `ZooKeeperClient#unregisterStateChangeHandler`, which cannot be acquired if a session expiry had occurred earlier since this thread gets queued behind the event handler thread waiting for writeLock.  This PR fixes the issue by limiting locking in `ZooKeeperClient#handleRequests` to just the non-blocking send, so that session expiry handling doesn't get blocked.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-08T16:46:52Z","2018-07-23T14:54:04Z"
"","5147","MINOR: some fix in Javadocs of StateStore.WindowStore","`WindowStore` docs tell us that `put(K key, V value)` uses wall-clock time as the timestamp but it is not true. All window stores (`CachingWindowStore`, `ChangeLoggingWindowBytesStore`, `MeteredWindowStore` and `RocksDBWindowStore`) use `context.timestamp()` as if we use `put(K key, V value, long timestamp)`. ```java put(key, value, context.timestamp()); ``` So I think there is mistake here, correct me if I be wrong.","closed","streams,","v-gerasimov","2018-06-06T11:16:28Z","2018-07-02T03:16:45Z"
"","4771","KAFKA-6710: Remove Thread.sleep from LogManager.deleteLogs","`Thread.sleep` in `LogManager.deleteLogs` potentially blocks a scheduler thread for up to `log.segment.delete.delay.ms` with a default value of a minute. This PR skips delete when the first log is not yet ready to be deleted, freeing the scheduler thread. Logs are then deleted on the next delete iteration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-24T15:39:27Z","2018-03-24T20:54:10Z"
"","5389","MINOR: internal config objects should not be logged","`StreamsPartitionAssigner` and `InternalTopicManager` create config objects for internal usage only. Those config objects should not log their setting, because they don't reflect the actual used config. This can be confusing to uses as different log entries show different config values and the internal once always show default configs (could give the impression that user overwrites are not respected).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-07-18T18:24:13Z","2018-07-25T02:33:59Z"
"","5024","Use Set instead of List for list of topics","`Set` make sense here since we don't care about the order and we don't want duplicates.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-16T18:48:07Z","2018-05-17T15:54:46Z"
"","4570","KAFKA-6512: Discard references to buffers used for compression","`ProducerBatch` retains references to `MemoryRecordsBuilder` and cannot be freed until acks are received. Removing references to buffers used for compression after records are built will enable these to be garbage collected sooner, reducing the risk of OOM.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-14T11:18:47Z","2018-02-15T17:36:45Z"
"","5215","KAFKA-7055: Update InternalTopologyBuilder to throw TopologyException if a processor or sink is added with no upstream node attached","`InternalTopologyBuilder` throws an exception if a sink or a processor is added without at least one upstream node, as records cannot be forwarded downstream to an unconnected node. This does not prevent users from attempting to forward to unconnected nodes, but it does prevent them from attaching effectively useless downstream nodes, and the error message for forwarding to an unconnected node has been updated to be slightly more specific.  `connectProcessors` has also been removed from `InternalTopologyBuilder`, as it is only used in `KStreamImpl` for a stream-table join and in fact connecting the stream source/processor to the join processor with appropriate `valueGetter` (to retrieve values from the KTable) is sufficient to obtain values for both sides.","closed","streams,","nixsticks","2018-06-13T19:48:17Z","2018-08-04T03:05:24Z"
"","4852","MINOR: Fix doc - `FileMessageSet` was replaced by `FileRecords`","`FileMessageSet` no longer existed. It was already replaced by `FileRecords`. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiminhsieh","2018-04-11T04:18:01Z","2018-04-12T06:13:34Z"
"","5124","MINOR: Reduce in-memory copies of partition objects in onJoinComplete() and onJoinPrepare()","`ConsumerCoordinator.onJoinPrepare()` currently makes multiple copies of the set of assigned partitions. We can let `subscriptions.assignedPartitions()` return a view of the underlying partition set, copy it only once and re-use the copied value.","closed","","radai-rosenblatt","2018-06-02T00:10:49Z","2018-10-03T12:27:17Z"
"","5033","HOTFIX: Move Conusmed to o.a.k.streams.kstream","`Consumed` was mis-placed. I've moved it to `kstream` and added one statement in the upgrade guide.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-17T18:27:32Z","2020-04-24T23:48:11Z"
"","4661","KAFKA-6622 - fix performance issue in parsing consumer offsets","`batch.baseOffset` is an expensive operation (even says so in its javadoc), and yet was called for every single record in the batch.  this means that for N records in a gzipped batch, the entire batch will be unzipped N times. we've seen this take up to 3 hours for relatively small clusters.  here's an example: https://ibb.co/b9eGq7","closed","","radai-rosenblatt","2018-03-08T00:14:29Z","2018-03-09T22:21:07Z"
"","4606","MINOR: Fix typo in MockProducer JavaDoc","```java @{link java.util.concurrent.Future Future<RecordMetadata>} ``` should be ```java {@link java.util.concurrent.Future Future<RecordMetadata>} ``` instead.","closed","","daniel-shuy","2018-02-21T04:01:58Z","2018-08-15T04:08:52Z"
"","5117","KIP-290 Prefixed ACLs","[KIP-290](https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+wildcard+suffixed+ACLs) /  [KAFKA-6841](https://issues.apache.org/jira/browse/KAFKA-6841)  This PR supersedes https://github.com/apache/kafka/pull/5079. (As I lack rights to push to that PR).  Still to do (can be done in this or another PR): - [x] Doc updates, including upgrade.html - [x] AclCommand updates a.k.a. kafka-acls.sh - [x] Remove internal use of deprecated constructors, (want to make sure the tests pass using legacy first) - [x] Performance improvements in the SimpleAclAuthorizer.getMatchingAcls, (which currently visits every resource).  The following outstanding items have been moved to additional Jiras or deemed unnecessary. See https://github.com/apache/kafka/pull/5117#issuecomment-395066985. - Remove duplicate Java `Resource` class. - Remove duplicate Scala `Resource` class. - Remove  duplicate Scala `ResourceNameType` class and just Java one. - Use single `/kafka-acl-changes` path and store ResourceNameType a JSON Value. - Potentially replacing `Resource` in `AclBinding` with `ResourceFilter` or a `ResourceMatcher`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2018-06-01T18:50:11Z","2018-06-06T14:27:47Z"
"","5352","KAFKA-5638: Improve the Required ACL of ListGroups API (KIP-231)","[KIP-231](https://cwiki.apache.org/confluence/display/KAFKA/KIP-231%3A+Improve+the+Required+ACL+of+ListGroups+API)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-07-10T18:46:05Z","2018-08-13T06:59:32Z"
"","5186","[KAFKA-7033] Modify AbstractOptions's timeoutMs as Long type","[KAFKA-7033 | Modify AbstractOptions's timeoutMs as Long type](https://issues.apache.org/jira/browse/KAFKA-7033)  Currently AbstractOptions's timeoutMs is Integer and using Long to represent timeout Millisecond maybe better .  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","darionyaphet","2018-06-11T15:17:54Z","2018-07-09T06:55:21Z"
"","5061","[KAFKA-6930] Update KafkaZkClient debug log","[KAFKA-6930 | Update KafkaZkClient debug log](https://issues.apache.org/jira/browse/KAFKA-6930) ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","darionyaphet","2018-05-22T14:03:07Z","2018-05-25T23:26:54Z"
"","4762","[KAFKA-6707]The default value for config of Type.LONG should be *L","[https://issues.apache.org/jira/browse/KAFKA-6707](https://issues.apache.org/jira/browse/KAFKA-6707) The default value for config of Type.LONG should be *L","closed","","hejiefang","2018-03-23T09:06:58Z","2018-03-29T23:24:35Z"
"","4754","[KAFKA-6702]Wrong className in LoggerFactory.getLogger method","[https://issues.apache.org/jira/browse/KAFKA-6702](https://issues.apache.org/jira/browse/KAFKA-6702) Wrong className in LoggerFactory.getLogger method","closed","","hejiefang","2018-03-22T02:41:36Z","2018-03-26T02:11:55Z"
"","4772","[KAFKA-6702]Wrong className in LoggerFactory.getLogger method","[https://issues.apache.org/jira/browse/KAFKA-6702](https://issues.apache.org/jira/browse/KAFKA-6702) [KAFKA-6702]Wrong className in LoggerFactory.getLogger method","closed","","hejiefang","2018-03-26T01:20:47Z","2018-03-29T22:50:47Z"
"","4592","MINOR follow-up for KAFKA-6568","@ijuma This is the minor follow-up patch for #4580 to address your comments.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","becketqin","2018-02-19T22:40:48Z","2018-02-20T15:19:31Z"
"","5168","MINOR: Make regular expressions into static","@ijuma I made a new PR after several rebases and merges of trunk. It's the same as this: https://github.com/apache/kafka/pull/5042 except only 1 commit.  This should improve performance, as in-place means that the regex has to be compiled again each time the code path is traversed. Which is not performant.  A nice writeup on this topic can be found here: https://softwareengineering.stackexchange.com/questions/216320/java-regex-patterns-compile-time-constants-or-instance-members  And it's also recommended in Joshua Bloch's ""Effective Java 3rd Edition"" (Item 6: Avoid creating unnecessary objects)  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status (Are there code coverage reports?) - [x] Verify documentation (including upgrade notes)","closed","","KoenDG","2018-06-08T20:23:48Z","2018-08-14T12:30:14Z"
"","4480","MINOR: make it more explicit that poll() call is needed for the callback to run","@hachikuji","closed","","norwood","2018-01-26T23:34:22Z","2018-02-16T16:24:05Z"
"","4823","KAFKA-6742: TopologyTestDriver error when dealing with stores from GlobalKTable","@guozhangwang   While TopologyTestDriver works well with stores created from KTable it does not with stores from GlobalKTable. Moreover, for my testing purposes but I think it can be useful to others, I need to get access to the MockProducer inside TopologyTestDriver.  I have added 4 new tests to TopologyTestDriverTest, two for stores from KTable and two for stores from GlobalKTable.  While I was changing the TopologyTestDriver I've also make it implement java.io.Closeable.","closed","streams,","Vale68","2018-04-04T10:38:33Z","2018-04-17T16:42:56Z"
"","4927","KAFKA-6826 avoid range scans when forwarding values during aggregation","@guozhangwang","closed","streams,","xvrl","2018-04-25T10:22:36Z","2018-04-25T15:36:15Z"
"","5313","HOTFIX: KAFKA-7097; Set create time default to -1L","@apovzner @tedyu @hachikuji   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-06-29T20:21:28Z","2018-06-29T22:54:17Z"
"","4847","KAFKA-6770: Add New Protocol Versions to 1.1.0 documentation","1.1 introduced 2 new versions to existing API keys: - DescribeConfigs v1 - Fetch v7  The 3rd new field of FetchRequest v7 is called `forgetten_topics_data`. Can we fix the typo in the field name or is it considered public API now ?  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-04-10T13:34:50Z","2018-04-18T13:30:54Z"
"","5430","KAFKA-7192 Follow-up: update checkpoint to the reset beginning offset","1. When we reinitialize the state store due to no CHECKPOINT with EOS turned on, we should update the checkpoint to consumer.seekToBeginnning() / consumer.position() to avoid falling into endless iterations.  2. Fixed a few other logic bugs around needsInitializing and needsRestoring.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-27T21:06:24Z","2018-08-01T01:13:49Z"
"","4650","KAFKA-6611: PART I, Use JMXTool in SimpleBenchmark","1. Use JmxMixin for SimpleBenchmark (will remove the self reporting in #4744), only when loading phase is false (i.e. we are in fact starting the streams app).  2. Reported the full jmx reported metrics in log files, and in the returned data only return the max values: this is because we want to skip the warming up and cooling down periods that will have lower rate numbers, while max represents the actual rate at full speed.  3. Incorporates two other improves to JMXTool: https://github.com/apache/kafka/pull/1241 and https://github.com/apache/kafka/pull/2950   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-06T01:47:24Z","2020-04-24T23:57:00Z"
"","4854","KAFKA-6611, PART II: Improve Streams SimpleBenchmark","1. SimpleBenchmark:      1.a Do not rely on manual num.records / bytes collection on atomic integers.     1.b Rely on config files for num.threads, bootstrap.servers, etc.     1.c Add parameters for key skewness and value size.     1.d Refactor the tests for loading phase, adding tumbling-windowed count.     1.e For consumer / consumeproduce, collect metrics on consumer instead.     1.f Force stop the test after 3 minutes, this is based on empirical numbers of 10M records.  2. Other tests: use config for kafka bootstrap servers.  3. streams_simple_benchmark.py: only use scale 1 for system test, remove yahoo from benchmark tests.  Note that the JMX based metrics is more accurate than the manually collected metrics. For example:  ``` Module: kafkatest.benchmarks.streams.streams_simple_benchmark_test Class:  StreamsSimpleBenchmarkTest Method: test_simple_benchmark Arguments: {   ""scale"": 3,   ""test"": ""streamcount"" } ----  {   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]0"": "" 3691724/180042/20504.79332600171/20.64996886468678\n"",   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]1"": "" 3337273/180037/18536.595255419717/18.667835797999594\n"",   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]2"": "" 2971003/180029/16502.913419504635/16.61975533580484\n"",   ""jmx-avg0"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 473.9851851851854,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.03802973633453574,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 1.6337178935425791,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 41.49027662118575,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.0021335594038200878,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 21042.80038005481   },   ""jmx-avg1"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 354.0805555555556,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.030462783608341253,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 2.2233941132541286,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 35.82136735561879,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.002402570337120884,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 19100.53692641491   },   ""jmx-avg2"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 365.85555555555555,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.03052173041795845,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 1.1700956516025365,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 28.06739395694291,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.0016847557478484937,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 15594.917035789254   },   ""jmx-max0"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 547.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.06657567990413102,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 13.04,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 196.4955857339561,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.04672288495817908,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 99520.53241178217   },   ""jmx-max1"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 653.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.065242211710977,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 13.909090909090908,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 156.47243937161267,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.08381330685203575,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 83263.21836209696   },   ""jmx-max2"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 632.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.06459531038046638,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 8.25,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 173.80159786950733,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.07433900760593988,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 97828.82822902796   } }  ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-11T17:57:58Z","2020-04-24T23:51:29Z"
"","4744","KAFKA-6611, PART II: Improve Streams SimpleBenchmark","1. SimpleBenchmark:      1.a Do not rely on manual num.records / bytes collection on atomic integers.     1.b Rely on config files for num.threads, bootstrap.servers, etc.     1.c Add parameters for key skewness and value size.     1.d Refactor the tests for loading phase, adding tumbling-windowed count.     1.e For consumer / consumeproduce, collect metrics on consumer instead.     1.f Force stop the test after 3 minutes, this is based on empirical numbers of 10M records.  2. Other tests: use config for kafka bootstrap servers.  3. streams_simple_benchmark.py: only use scale 1 for system test, remove yahoo from benchmark tests.  Note that the JMX based metrics is more accurate than the manually collected metrics. For example:  ``` Module: kafkatest.benchmarks.streams.streams_simple_benchmark_test Class:  StreamsSimpleBenchmarkTest Method: test_simple_benchmark Arguments: {   ""scale"": 3,   ""test"": ""streamcount"" } ----  {   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]0"": "" 3691724/180042/20504.79332600171/20.64996886468678\n"",   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]1"": "" 3337273/180037/18536.595255419717/18.667835797999594\n"",   ""Streams Count Performance [records/latency/rec-sec/MB-sec counted]2"": "" 2971003/180029/16502.913419504635/16.61975533580484\n"",   ""jmx-avg0"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 473.9851851851854,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.03802973633453574,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 1.6337178935425791,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 41.49027662118575,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.0021335594038200878,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 21042.80038005481   },   ""jmx-avg1"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 354.0805555555556,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.030462783608341253,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 2.2233941132541286,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 35.82136735561879,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.002402570337120884,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 19100.53692641491   },   ""jmx-avg2"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 365.85555555555555,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.03052173041795845,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 1.1700956516025365,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 28.06739395694291,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.0016847557478484937,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 15594.917035789254   },   ""jmx-max0"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 547.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.06657567990413102,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 13.04,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 196.4955857339561,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.04672288495817908,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 99520.53241178217   },   ""jmx-max1"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 653.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.065242211710977,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 13.909090909090908,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 156.47243937161267,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.08381330685203575,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 83263.21836209696   },   ""jmx-max2"": {     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-latency-avg"": 632.0,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:commit-rate"": 0.06459531038046638,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-latency-avg"": 8.25,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:poll-rate"": 173.80159786950733,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-latency-avg"": 0.07433900760593988,     ""kafka.streams:type=stream-metrics,client-id=simple-benchmark-StreamThread-1:process-rate"": 97828.82822902796   } }  ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-03-21T00:11:40Z","2018-04-11T17:58:35Z"
"","5290","MINOR: Store metrics scope, total metrics","1. Rename metrics scope of rocksDB window and session stores; also modify the store metrics accordingly with guidance on its correlations to `metricsScope`.  2. Add the missing `total` metrics for per-thread, per-task, per-node and per-store sensors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-25T22:06:39Z","2018-06-28T21:32:43Z"
"","4486","MINOR: Code refacotring in KTable-KTable Join","1. Rename KTableKTableJoin to KTableKTableInnerJoin. Also removed `abstract` from other joins. 2. Merge KTableKTableJoinValueGetter.java into KTableKTableInnerJoin. 3. Use set instead of arrays in the stores function, to avoid duplicate stores to be connected to processors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-29T19:41:18Z","2020-04-24T23:47:28Z"
"","4991","KAFKA-6813: Remove deprecated APIs in KIP-182, Part III","1. Remove TopologyBuilder, TopologyBuilderException, KStreamBuilder,  2. Completed the leftover work of https://issues.apache.org/jira/browse/KAFKA-5660, when we remove TopologyBuilderException.  3. Added MockStoreBuilder to replace MockStateStoreSupplier, remove all XXStoreSupplier except StateStoreSupplier as it is still referenced in the logical streams graph.  4. Minor: rename KStreamsFineGrainedAutoResetIntegrationTest.java to FineGrainedAutoResetIntegrationTest.java.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-05-09T23:33:11Z","2020-04-24T23:57:17Z"
"","4976","KAFKA-6813: Remove deprecated APIs in KIP-182, Part II","1. Remove the deprecated StateStoreSuppliers, and the corresponding Stores.create() functions and factories: only the base StateStoreSupplier and MockStoreSupplier were still preserved as they are needed by the deprecated TopologyBuilder and KStreamBuilder. Will remove them in a follow-up PR.  2. Add TopologyWrapper.java as the original InternalTopologyBuilderAccessor was removed, but I realized it is still needed as of now.  3. Minor: removed StateStoreTestUtils.java and inline its logic in its callers since now with StoreBuilder it is just a one-liner.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-05-08T00:42:06Z","2020-04-24T23:57:16Z"
"","5382","KAFKA-3514: Remove min timestamp tracker","1. Remove MinTimestampTracker and its TimestampTracker interface. 2. In RecordQueue, keep track of the head record (deserialized) while put the rest raw bytes records in the fifo queue, the head record as well as the partition timestamp will be updated accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-18T00:27:40Z","2018-07-19T15:06:30Z"
"","5428","KAFKA-3514: Part III, Refactor StreamThread main loop","1. Refactor the StreamThread main loop, in the following:  * Fetch from consumer and enqueue data to tasks. * Check if any tasks should be enforced process. * Loop over processable tasks and process them for N iterations, and then check for 1) commit, 2) punctuate, 3) need to call consumer.poll * Even if there is not data to process in this iteration, still need to check if commit / punctuate is needed * Finally, try update standby tasks.  2. Add an optimization to only commit when it is needed (i.e. at least some process() or punctuate() was triggered since last commit).  3. Found and fixed a ProducerFencedException scenario: producer.send() call would never throw a ProducerFencedException directly, but it may throw a KafkaException whose ""cause"" is a ProducerFencedException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-07-26T23:22:45Z","2018-09-12T00:54:03Z"
"","5110","KAFKA-6562: (follow-up) Publish ""jackson-databind"" lib as provided scope dependency in clients maven artifact","1. jackson-databind library is mapped as a provided library by using maven plugin configuration-to-scope mapping feature. https://docs.gradle.org/current/javadoc/org/gradle/api/artifacts/maven/Conf2ScopeMappingContainer.html 2. Clients using default implementation of SASL/OAUTHBEARER mechanism must provide jackson-databind library to Kafka client runtime.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-05-31T19:35:39Z","2018-06-12T08:58:59Z"
"","5017","KAFKA-6729: Reuse source topics for source KTable's materialized store's changelog","1. In InternalTopologyBuilder#topicGroups, which is used in StreamsPartitionAssignor, look for book-kept `storeToChangelogTopic` map before creating a new internal changelog topics. In this way if the source KTable is created, its source topic stored in `storeToChangelogTopic` will be used.  2. Added unit test (confirmed that without 1) it will fail).  3. MINOR: removed TODOs that are related to removed KStreamBuilder.  4. MINOR: removed TODOs in StreamsBuilderTest util functions and replaced with TopologyWrapper.  5. MINOR: removed `StreamsBuilderTest#testFrom` as it is already covered by `TopologyTest#shouldNotAllowToAddSourcesWithSameName`, plus it requires `KStreamImpl.SOURCE_NAME` which should be a package private field of the `KStreamImpl`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-15T04:57:11Z","2020-04-24T23:57:22Z"
"","5398","KAFKA-3514: Part II, Choose tasks with data on all partitions to process","1. In each iteration, decide if a task is processable if all of its partitions contains data, so it can decide which record to process next.  1.a Add one exception that, if the task indeed have data on some but not all of its partitions, we only consider as not processable for some finite round of iterations.  1.b Add a task-level metric to record whenever we are forced to process a task that is only ""partially data available"", since it may leads to non-determinism.  2. Break the main loop on put-raw-data and process-them. Since now not all data put into the queue would be processed completely within a single iteration.  NOTE that within an iteration, if a task has exhausted one of its queue it will still be processed, since we only update `processable` list once in each iteration, I'm improving on this on the follow-up part III PR.  3. Found and fixed a bug in metrics recording: the taskName and sensorName parameters were exchanged.  4. Optimized task stream time computation again since our current partition stream time reasoning has been simplified.  5. Added unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-19T22:12:32Z","2018-08-10T17:25:09Z"
"","4539","KAFKA-6501: Dynamic broker config tests updates and metrics fix","1. Handle listener-not-found in MetadataCache since this can occur when listeners are being updated. To avoid breaking clients, this is handled in the same way as broker-not-available so that clients retry 2. Set retries=1000 for listener reconfiguration tests to avoid transient failures when metadata cache has not been updated  3. Remove IdlePercent metric when Processor is deleted, add test 4. Reduce log segment size used during reconfiguration to avoid timeout while waiting for log rolling 5.Test markPartitionsForTruncation after fetcher thread resize  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-02-07T14:51:38Z","2018-02-09T00:22:52Z"
"","4491","HOTFIX: Fix reset integration test hangs on busy wait","1. Found two root causes of the failure: a) appID is not set at constructor time and hence causing the condition method to fail, 2) class static properties are only populated once and hence when SSL tests are executed before non-SSL tests, the later will fail.  2. A bunch of log4j improvements along the debugging experience.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-30T23:11:42Z","2020-04-24T23:47:52Z"
"","5136","KAFKA-6993: Fix defective documentations for KStream/KTable methods","1. Fix the documentation of following methods, e.g., making more detailed description for the overloaded methods:  	- KStream#join 	- KStream#leftJoin 	- KStream#outerJoin 	- KTable#filter 	- KTable#filterNot 	- KTable#mapValues 	- KTable#transformValues 	- KTable#join 	- KTable#leftJoin 	- KTable#outerJoin  2. (trivial) with possible new type -> with possibly new type.","closed","streams,","dongjinleekr","2018-06-05T07:44:33Z","2018-06-06T21:07:19Z"
"","4858","Bug Hotfix: consumer poll blocked & FetchResponse cast error","1. fix bug: consumer poll(timeout) blocked infinitely when no available bootstrap server 2. fix bug: FindCoordinatorResponse cannot be cast to FetchResponse","closed","","koqizhao","2018-04-12T06:58:59Z","2018-04-12T15:26:41Z"
"","5298","KAFKA-7101: Consider session store for windowed store default configs","1. extend `isWindowStore` to consider session store as well. 2. extend the existing unit test accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-26T21:32:19Z","2018-07-03T05:00:28Z"
"","5322","KAFKA-5037 Infinite loop if all input topics are unknown at startup","1. At the beginning of assign, we first check that all the non-repartition source topics are included in the metadata. If not, we log an error at the leader and set an error in the Assignment userData bytes, indicating that leader cannot complete assignment and the error code would indicate the root cause of it.  2. Upon receiving the assignment, if the error is not NONE the streams will shutdown itself with a log entry re-stating the root cause interpreted from the error code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-07-03T07:05:22Z","2018-07-20T00:00:17Z"
"","4684","KAFKA-6634: Delay starting new transaction in task.initializeTopology","1. As titled, not starting new transaction since during restoration producer would have not activity and hence may cause txn expiration. 1.a. Also delay starting new txn in resuming until initializing topology.  2. Fixed a minor bug, that when resuming process hits a migration exception, we should remove that task from the running list if possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-03-11T18:28:45Z","2020-04-24T23:49:04Z"
"","5421","KAFKA-7192: Wipe out if EOS is turned on and checkpoint file does not exist","1. As titled and as described in comments. 2. Modified unit test slightly to insert for new keys in committed data to expose this issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-07-25T02:40:17Z","2018-08-01T01:13:42Z"
"","5018","KAFKA-4936: Add dynamic routing in Streams","1. Add the proposed APIs as described in https://cwiki.apache.org/confluence/display/KAFKA/KIP-303%3A+Add+Dynamic+Routing+in+Streams+Sink.  2. Modify StreamPartitioner#partition to include topic name along with this change, since now it is less reasonable to ignore the topic name since it may not be pre-known.  3. Update TopologyDescription#Sink implementation: when the topic name is static, print the topic name, otherwise print the dynamic topic name extractor class name.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","guozhangwang","2018-05-15T05:36:27Z","2020-06-13T00:02:52Z"
"","4878","KAFKA-6788: Combine queries for describe and delete groups in AdminCl…","1 Ask coordinator to list its groups 2 Try to delete/describe them all 3 Track which entries are successfully deleted or described 4 Continue over succesfully deleted/described entries","closed","","cyrusv","2018-04-16T06:59:54Z","2018-06-11T15:27:54Z"
"","5226","KAFKA-7210:  Add a system test to verify the log compaction","- Update TestLogCleaning tool to use Java consumer and rename as LogCompactionTester - Enable log compaction in System tests - Remove configs with values same as server defaults from ""kafka.properties"" template file. - Update the kafka.py logic to handle the duplicates between kafka.properties and server_prop_overides.","closed","","omkreddy","2018-06-14T13:45:35Z","2018-08-20T11:26:45Z"
"","5199","MINOR: Remove unnecessary old consumer usage in tests and other clean-ups","- Update some tests to use the Java consumer. - Remove ignored `ProducerBounceTest.testBrokerFailure`. This test is flaky and it has been superseded by `TransactionBounceTest`. - Use non-blocking poll for consumption methods in `TestUtils`.  This is a step on the road to remove the old consumers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-12T09:12:33Z","2018-06-13T07:51:29Z"
"","5278","KAFKA-7091: AdminClient should handle FindCoordinatorResponse errors","- Update KafkaAdminClient implementation to handle FindCoordinatorResponse errors - Remove scala AdminClient usage from core and streams tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-06-22T18:46:54Z","2018-07-03T15:37:29Z"
"","5463","KAFKA-7117: Support AdminClient API in AclCommand (KIP-332)","- Update AclCommandTest - update docs - Remove deprecated KafkaPrincipal.fromString() usage  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-06T15:59:50Z","2018-09-08T06:28:00Z"
"","5303","MINOR: Consolidate Topic create calls in Test classes","- Replace adminZkClient.createOrUpdateTopicPartitionAssignmentPathInZK calls with TestUtils.createTopic wherever applicable - Replace adminZkClient.createTopic calls with TestUtils.createTopic wherever applicable - Move non-deprecated tests to other test classes and deprecate AdminTest.scala  - Remove duplicate tests between AdminTest and AdminZkClientTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-06-27T16:31:03Z","2018-07-19T17:50:49Z"
"","5230","KAFKA-2983: Remove Scala consumers and related code","- Removed Scala consumers (`SimpleConsumer` and `ZooKeeperConsumerConnector`) and their tests. - Removed Scala request/response/message classes. - Removed any mention of new consumer or new producer in the code with the exception of MirrorMaker where the new.consumer option was never deprecated so we have to keep it for now. The non-code documentation has not been updated either, that will be done separately. - Removed a number of tools that only made sense in the context of the Scala consumers (see upgrade notes). - Updated some tools that worked with both Scala and Java consumers so that they only support the latter (see upgrade notes). - Removed `BaseConsumer` and related classes apart from `BaseRecord` which is used in `MirrorMakerMessageHandler`. The latter is a pluggable interface so effectively public API. - Removed `ZkUtils` methods that were only used by the old consumers. - Removed `ZkUtils.registerBroker` and `ZKCheckedEphemeral` since the broker now uses the methods in `KafkaZkClient` and no-one else should be using that method. - Updated system tests so that they don't use the Scala consumers except for multi-version tests. - Updated LogDirFailureTest so that the consumer offsets topic would continue to be available after all the failures. This was necessary for it to work with the Java consumer. - Some multi-version system tests had not been updated to include recently released Kafka versions, fixed it. - Updated findBugs and checkstyle configs not to refer to deleted classes and packages.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-14T18:42:09Z","2018-06-19T14:33:02Z"
"","5380","MINOR: remove unused MeteredKeyValueStore","- remove MeteredKeyValueStore - rename MeteredKeyValueByteStore to MeteredKeyValueStore - code cleanup (Java8 and others) - removed usage of deprecated `KafkaMetric#value()` - merged `InnerMeteredKeyValueStore` into `MeteredKeyValueStore` and removed `TypeConverter` interface","closed","streams,","mjsax","2018-07-17T21:51:26Z","2018-07-18T16:31:04Z"
"","4547","MINOR: Update jetty, jackson, gradle and jacoco","- Gradle update adds support for Java 10 - Jacoco update adds support for Java 9 - Jackson bug fix update adds more serialization robustness checks - Jetty bug fix update  Relying on existing tests for verification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-02-08T10:54:16Z","2018-02-08T17:39:39Z"
"","4572","MINOR: Fix Streams EOS system tests","- avoid loosing log/stdout/stderr files on restart  - reenable tests","closed","streams,","mjsax","2018-02-14T21:22:48Z","2018-02-16T21:16:57Z"
"","5419","MINOR: Remove TopicAndPartition  usage from tests","- also replace  TopicAndPartition with TopicPartition in MetadataCache ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-24T05:02:58Z","2018-07-24T08:27:20Z"
"","4520","fix ShellCheck issues","- also add ShellCheck test to CI  Any interest in this?  I did this as a first step towards replacing bash with posix shell, but later ran out of time when porting kafka-run-class.sh to posix shell (e.g. thanks to shopt nullglob). So I just ended up leaving bash in my docker container, but the shellcheck cleanup and CI might still be interesting for you.","closed","","MartinNowak","2018-02-03T15:54:19Z","2018-12-24T14:02:48Z"
"","4880","KAFKA-6054: Update Kafka Streams metadata to version 3","- adds Streams upgrade tests for 1.1 release","closed","streams,","mjsax","2018-04-16T14:23:58Z","2018-06-05T23:47:57Z"
"","5273","KAFKA-4217: Add KStream.flatTransform","- Adds flatTrasform method in KStream - Adds processor supplier and processor for flatTransform  This contribution is my original work and I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2018-06-22T09:38:59Z","2020-06-13T00:04:20Z"
"","5477","MINOR: Fixed log in Topology Builder.","- addressed some warnings shown by Intellij  https://github.com/JetBrains/intellij-community/blob/master/plugins/InspectionGadgets/src/inspectionDescriptions/ToArrayCallWithZeroLengthArrayArgument.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2018-08-08T06:06:54Z","2018-08-09T05:19:54Z"
"","4725","MINOR: Java 9/10 fixes, gradle and minor deps update","- Added dependencies so that Trogdor and Connect work with Java 9 and 10 - Updated Jacoco to 0.8.1 so that it works with Java 10 - Updated Gradle to 4.6 - A few minor version bumps (not related to Java9/10 fixes)  I tested manually that we can run `./gradlew test` with Java 10 after these changes. There are test failures as EasyMock and PowerMock will have to be updated to use a newer ASM version. But compiling successfully and most tests passing is progress. :)  I also tested manually that Trogdor can be started with Java 10. It previously failed with a ClassNotFoundError.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-16T16:37:29Z","2018-09-11T06:47:31Z"
"","4819","MINOR: Update max.connections.per.ip.overrides config docs","- Add a validation check to make sure max.connections.per.ip.overrides is configured when max.connections.per.ip is set zero.","closed","","omkreddy","2018-04-03T15:03:19Z","2018-07-03T15:44:13Z"
"","4719","KAFKA-6351: Prevent copying javassist library to Kafka distribution from tools project","-  javassist:3.21.0-GA library is coming from :connect:runtime project and javassist:3.20.0-GA library is coming from :tools project. This PR prevents copying javassist while creating Kafka distribution from tools project","closed","","omkreddy","2018-03-15T14:38:30Z","2020-10-13T14:11:10Z"
"","4731","KAFKA-6680: Fix issues related to Dynamic Broker configs","-   Fix kafkaConfig initialization if there are no dynamic configs defined in ZK. -  update DynamicListenerConfig.validateReconfiguration() to check new Listeners must be subset of listener map","closed","","omkreddy","2018-03-19T03:06:14Z","2018-07-03T15:44:17Z"
"","5109","KAFKA-6722: SensorAccess.getOrCreate should be more efficient","*There's JIRA created for this Pull Request: https://issues.apache.org/jira/browse/KAFKA-6722  Basically the read lock is completely unnecessary. If a temp variable is used, we can get rid of the read lock  *  *Unit test built in Kafka source is run when running gradle to build the code. *","closed","","wuqingjun","2018-05-31T19:23:30Z","2018-06-05T17:02:24Z"
"","5166","KAFKA-7023: Move prepareForBulkLoad() call after customized RocksDBConfigSetter","*Summary options.prepareForBulkLoad() and then use the configs from the customized customized RocksDBConfigSetter. This may overwrite the configs set in prepareBulkLoad call. The fix is to move prepareBulkLoad call after applying configs customized RocksDBConfigSetter.   *Summary of testing strategy (including rationale) Unit test, test on dev environment on recovery time.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Ishiihara","2018-06-08T17:22:45Z","2018-07-16T22:23:00Z"
"","4575","KAFKA-6555: Making state store queryable during restoration","*State store in Kafka streams are currently only queryable when StreamTask is in RUNNING state. The idea is to make it queryable even in the REBALANCING state as the time spend on restoration can be huge and making the data inaccessible during this time could be downtime not suitable for many applications. [https://issues.apache.org/jira/browse/KAFKA-6555]*  *Existing unit tests/integration tests are running fine*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  [unit_tests.html.zip](https://github.com/apache/kafka/files/1729175/unit_tests.html.zip)","closed","streams,","a-surana","2018-02-15T19:57:55Z","2020-03-04T01:15:09Z"
"","5405","KAFKA-7189 : Add a Merge Transformer for Kafka Connect","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Like the flatten, there is the need too for a merge transformer.  Example transformation : We want to add the offset and the partition for each record, and after that merge them into one field _metadata: ```""transforms"":""AddOffset, AddPartition, MergeFields"",   ""transforms.AddOffset.type"":""org.apache.kafka.connect.transforms.InsertField$Value"", ""transforms.AddOffset.offset.field"":""offset!"",  ""transforms.AddPartition.type"":""org.apache.kafka.connect.transforms.InsertField$Value"", ""transforms.AddPartition.partition.field"":""partition!"",  ""transforms.MergeFields.type"":""org.apache.kafka.connect.transforms.Merge$Value"", ""transforms.MergeFields.field.list"":""offset,partition"", ""transforms.MergeFields.field.root"":""_metadata"" ```  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  1. Test topLevelStrcuteRequired for records with Schema 2. Test topLevelMapRequired for schemaless records 3. Test Merge fields for a record value with schema for the 4 cases. 4. Test Merge fields for a schemaless record key.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","elarib","2018-07-20T13:10:35Z","2022-04-29T16:10:17Z"
"","4817","MINOR: Fix Findbugs issue about useless object in KafkaAdminClient","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Fix Findbugs issue  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  org.apache.kafka.clients.admin.KafkaAdminClientTest unit tests passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","attila-s","2018-04-03T13:17:29Z","2020-11-03T03:00:55Z"
"","4534","Kafka-3832: Kafka Connect's JSON Converter never outputs a null value","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  @rhauch , have added null condition in the method convertToJsonWithEnvelope ,to return null instead of empty Envelope, when enableSchemas=true.  Would also implement a similar change to toConnectData method, based on your feedback with changes made for fromConnectData method.  let me know, if there is any problem with the implementation or my understanding about the KAFKA-3832.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","Satyajitv","2018-02-06T22:12:50Z","2020-03-29T01:53:59Z"
"","4548","KAFKA-6541: Fixed a StackOverflow bug in kafka-coordinator-heartbeat-thread","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  - The bug is caused by infinite recursive calls to `ConsumerNetworkClient.disconnect()`   - In class `org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient`, I introduced a boolean field named `disconnecting` to prevent such calls happen  ```java     public void disconnect(Node node) {         synchronized (this) {             if(disconnecting){                 return;             }             disconnecting=true;             failUnsentRequests(node, DisconnectException.INSTANCE);                          client.disconnect(node.idString());         }          // We need to poll to ensure callbacks from in-flight requests on the disconnected socket are fired         pollNoWakeup();     } ```   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  No test to implement (I think this is a trivial fix)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","anhldbk","2018-02-08T11:17:35Z","2018-02-15T23:20:41Z"
"","5032","Kafka 6884 consumer group command should use new admin client","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  - ConsumerGroupCommand is updated to use the new AdminClient.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  - Executed existing unit tests: DescribeConsumerGroupTest, DeleteConsumerGroupTest, DeleteConsumerGroupsTest, ResetConsumerGroupOffsetTest), ListConsumerGroupTest - Created a kafka-console-consumer & manually verified ``bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group 1``  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-05-17T16:55:28Z","2018-07-17T18:56:40Z"
"","4739","MINOR: Fix potential resource leak in FileOffsetBackingStore","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Ran unit tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-03-20T11:14:49Z","2018-03-24T19:20:12Z"
"","4537","KAFKA-6430: Add buffer between Java data stream and gzip stream","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ying-zheng","2018-02-07T00:45:08Z","2018-02-16T23:15:36Z"
"","4525","[MINOR] Improve runtime / storage / metrics / config parts","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify documentation (including upgrade notes) - [x] Verify test coverage and CI build status","closed","","asdf2014","2018-02-05T12:22:48Z","2018-02-14T00:58:13Z"
"","5465","KAFKA-7252: fix Streams docs state.dir","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-08-06T19:18:21Z","2018-08-06T20:55:50Z"
"","5458","KAFKA-3514, Documentations: Add out of ordering in concepts.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-08-03T21:08:15Z","2018-09-11T23:28:55Z"
"","5442","MINOR: improve JavaDocs for Streams PAPI WordCountExample","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-08-01T23:06:23Z","2018-08-02T22:16:24Z"
"","5432","Fixed Spelling.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jimmycasey","2018-07-29T21:40:34Z","2018-08-06T20:25:19Z"
"","5431","[NOT MERGE] Dummy for testing","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-29T17:08:45Z","2020-04-24T23:57:33Z"
"","5426","MINOR: Caching layer should forward record timestamp (#5423)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-26T18:34:11Z","2018-08-01T22:22:06Z"
"","5424","0.10.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","QuanGangDeng","2018-07-26T06:32:04Z","2018-07-26T16:44:16Z"
"","5423","MINOR: Caching layer should forward record timestamp","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-07-25T23:08:47Z","2018-07-26T18:35:17Z"
"","5374","MINOR: update release.py","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-16T23:35:54Z","2020-08-04T00:14:59Z"
"","5372","MINOR: improve docs version numbers","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-16T21:18:41Z","2018-07-18T20:53:27Z"
"","5347","MINOR: update release script and fix docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-08T22:04:37Z","2018-07-16T23:11:52Z"
"","5335","add log","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shugangtest","2018-07-05T07:12:13Z","2018-07-10T02:39:00Z"
"","5331","MINOR: fix Streams quickstart","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-03T22:00:37Z","2018-07-04T00:06:54Z"
"","5330","MINOR: fix Streams quickstart","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-03T21:57:40Z","2018-07-04T00:06:29Z"
"","5329","MINOR: update release script","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-03T21:49:28Z","2018-07-16T23:10:18Z"
"","5328","MINOR: add release script","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-07-03T21:44:05Z","2018-07-16T23:09:40Z"
"","5327","MINOR: update for 0.10.2.2 release","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-07-03T19:25:08Z","2018-07-03T20:44:27Z"
"","5326","MINOR: updatd for 0.11.0.3 release","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-07-03T18:19:20Z","2018-07-03T20:45:03Z"
"","5321","KAFKA-7125: Allow multiple processor nodes for global store sub-topology","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-02T22:49:54Z","2020-04-24T23:54:44Z"
"","5285","1.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","birdgodtech","2018-06-24T02:15:53Z","2018-06-24T02:49:46Z"
"","5238","HOTFIX: remove sub-module 'kafka'","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-06-16T05:28:30Z","2018-06-16T05:52:39Z"
"","5219","KAFKA-6711: GlobalStateManagerImpl should not write offsets of in-memory stores in checkpoint file","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-06-14T00:01:50Z","2019-03-26T21:35:11Z"
"","5187","KAFKA-6860: Fix NPE in Kafka Streams with EOS enabled","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-06-11T17:41:59Z","2018-06-14T05:21:48Z"
"","5185","MINOR: remove duplicate jfreechart definition","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-06-11T12:22:02Z","2018-08-23T13:10:43Z"
"","5181","MINOR: update jackson dependency","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-06-11T04:34:15Z","2018-06-11T23:58:58Z"
"","5180","MINOR: update jackson dependencies","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-06-11T03:51:52Z","2018-06-12T01:56:48Z"
"","5179","MINOR: update jackson dependencies","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-06-11T03:42:08Z","2018-06-13T17:58:28Z"
"","5162","KAFKA-7020: Error when deleting topic with access denied exception","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wuqingjun","2018-06-07T23:18:18Z","2018-07-30T19:47:22Z"
"","5156","MINOR: need to update system test version after version bump","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-06-06T18:44:20Z","2018-06-06T23:07:01Z"
"","5132","MINOR: docs should point to latest version","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-06-04T18:29:41Z","2018-06-05T21:46:05Z"
"","5123","MINOR: Add missing configs for resilience settings","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-06-01T23:46:53Z","2020-04-24T23:50:38Z"
"","5096","KAFKA-6967: TopologyTestDriver does not allow pre-populating state stores that have change logging","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-05-30T03:52:47Z","2018-06-06T22:17:49Z"
"","5089","KAFKA-6420: Adding Msys support","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asutosh936","2018-05-28T05:25:14Z","2021-06-16T01:58:58Z"
"","5043","MINOR: Add docs for controller sensors added in KIP-237","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-05-20T00:18:35Z","2018-07-22T22:39:45Z"
"","5031","wip! Test closing from uncaughtExceptionHandler","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","astubbs","2018-05-17T14:50:23Z","2019-07-16T08:29:01Z"
"","5023","MINOR: add missing parameter `processing.guaratees` to Streams docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-05-16T17:48:16Z","2018-05-16T21:10:34Z"
"","5009","MINOR: Ensure sensor names are unique in Kafka Streams","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-05-11T20:55:47Z","2018-05-12T00:19:21Z"
"","5008","HOTFIX: RegexSourceIntegrationTest needs to cleanup shared output topic","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-05-11T20:49:29Z","2018-05-11T20:53:51Z"
"","4990","WIP: Initial impl for externalizing secrets in configs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rayokota","2018-05-09T22:50:09Z","2018-05-22T23:56:28Z"
"","4973","KAFKA-6813: Remove deprecated APIs in KIP-182, Part III [WIP]","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-07T23:18:24Z","2020-04-24T23:57:11Z"
"","4943","MINOR: Fix streams web doc configs tables","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","manjuapu","2018-04-27T23:05:41Z","2018-04-27T23:16:05Z"
"","4915","Minor: fix potential NPE in ConnectHeaders","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-04-23T06:31:02Z","2020-02-13T07:48:35Z"
"","4889","0.10.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ReasonDuan","2018-04-18T02:27:39Z","2018-04-18T02:28:43Z"
"","4886","0.9.0 view producer client","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chuailiwu","2018-04-17T09:44:29Z","2019-01-05T10:39:55Z"
"","4857","KAFKA 6775 - Added missing super.init method","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asutosh936","2018-04-12T01:48:18Z","2018-04-13T00:53:45Z"
"","4753","KAFKA 5609 - Added RollingFileAppender as per review comments","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","asutosh936","2018-03-22T01:54:40Z","2018-05-18T00:09:30Z"
"","4745","KAFKA 6673 - Implemented missing override equals method","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","asutosh936","2018-03-21T02:13:43Z","2018-05-09T17:34:51Z"
"","4738","MINOR: put registerChannel after connected succeeded","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","walkwalkwalk","2018-03-20T11:14:27Z","2018-04-08T05:46:28Z"
"","4706","Added NetworkClient/Selector Redirector","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thebigcorporation","2018-03-14T00:31:13Z","2018-03-14T00:32:28Z"
"","4687","KAFKA-5609","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","asutosh936","2018-03-12T00:52:47Z","2018-05-18T00:09:39Z"
"","4658","1.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tyleryu","2018-03-07T03:04:02Z","2018-03-07T15:50:26Z"
"","4652","1.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","q459997705","2018-03-06T09:25:22Z","2018-03-06T16:21:58Z"
"","4638","1.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tur4in","2018-03-02T13:25:20Z","2018-03-02T16:44:34Z"
"","4628","KAFKA-6486: Implemented LinkedHashMap in TimeWindows","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","asutosh936","2018-02-28T01:37:52Z","2018-03-19T03:44:12Z"
"","4619","KAFKA-5609 - Added Rolling File Appender","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","asutosh936","2018-02-26T00:27:45Z","2020-03-22T05:58:10Z"
"","4614","MINOR: Improve SimpleBenchmark (WIP)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-22T18:25:49Z","2018-03-09T22:19:28Z"
"","4581","MINOR: Fix typos in kafka.py","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asasvari","2018-02-16T13:11:15Z","2018-02-16T16:30:14Z"
"","4577","KAFKA-6566 SourceTask#stop() not called after exception raised in poll()","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","tedyu","2018-02-15T22:37:39Z","2020-03-29T02:20:04Z"
"","4576","KAFKA-5285: Use window and session key schema's comparator for rocksDB / Cache bytes [WIP]","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","guozhangwang","2018-02-15T22:10:52Z","2018-04-18T20:46:41Z"
"","4556","KAFKA-6552:“entity_type” not exactly  in description of kafka-configs.sh","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","auroraxlh","2018-02-11T09:32:45Z","2018-02-12T17:46:57Z"
"","4545","KAFKA-2423: Introduce Scalastyle","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rchiang","2018-02-08T02:08:36Z","2018-07-23T22:42:54Z"
"","4493","KAFKA-3625: add docs for Kafka Streams test-utils (follow up)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2018-01-31T01:26:06Z","2018-02-08T21:37:32Z"
"","4483","MINOR: Fix typo in KTable javadoc","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dminkovsky","2018-01-28T21:54:56Z","2018-02-01T18:19:21Z"
"","5039","MINOR: Remove duplicate code which is invoked twice","*More detailed description of your change* I have removed the logic that unnecessarily calls cache.remove twice.  *Summary of testing strategy* I have confirmed that there is no problem by executing Unit Test of NamedCache. `./gradlew -Dtest.single=NamedCacheTest streams:test`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Dark0096","2018-05-19T14:36:22Z","2018-05-30T18:09:12Z"
"","5099","MINOR:Fix table outer join test","*In the class test KTableImplTest the method shouldThrowNullPointerOnOuterJoinWhenMaterializedIsNull is testing the wrong method*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","emmanuelharel","2018-05-30T14:24:33Z","2018-06-01T18:59:03Z"
"","5307","KAFKA-7110: Add windowed changelog serde","*Currently the TimeWindowedSerde does not deserialize the windowed keys from a changelog topic properly. There are a few assumptions made in the TimeWindowedDeserializer that prevents the changelog windowed keys from being correctly deserialized. This PR will introduce a new WindowSerde to allow proper deserialization of changelog windowed keys. See https://issues.apache.org/jira/browse/KAFKA-7110 for more details.","closed","streams,","shawnsnguyen","2018-06-28T01:05:51Z","2019-01-09T00:00:00Z"
"","5034","KAFKA-6913: Add Connect converters and header converters for short, integer, long, float, and double (WIP)","*[KIP-305](https://cwiki.apache.org/confluence/display/KAFKA/KIP-305%3A+Add+Connect+primitive+number+converters) has been approved.*  Added converters and header converters for the primitive number types for which Kafka already had serializers and deserializers. All extend a common base class, `NumberConverter`, that encapsulates most of the shared functionality. Unit tests were added to check the basic functionality.  These classes are not used by any other Connect code, and must be explicitly used in Connect workers and connectors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-05-17T23:10:44Z","2020-10-16T06:24:18Z"
"","5001","KAFKA-6895: (WIP) Schema Inferencing for JsonConverter","**Work in Progress: Do not merge until KIP and this PR are both approved.**  KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-301%3A+Schema+Inferencing+for+JsonConverter  This introduces a new configuration for the JsonConverter class called ""schemas.infer.enable"". When ""schemas.enable"" is false and the ""schemas.infer.enable"" flag is set to true, the converter will infer the schema from the contents of the JSON record and return that as part of the SchemaAndValue object for Sink Connectors.  Author: Allen Tang   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","natengall","2018-05-10T22:35:29Z","2020-03-22T06:06:53Z"
"","5211","[KAFKA-7054] Kafka describe command should throw topic doesn't exist exception.","**User Interface Improvement :** If topic doesn't exist then Kafka describe command should throw topic doesn't exist exception, like alter and delete commands  ### Committer Checklist (excluded from commit message)   - [ ] Verify design and implementation   - [ ] Verify test coverage and CI build status   - [ ] Verify documentation (including upgrade notes)","closed","","ManoharVanam","2018-06-13T10:28:36Z","2018-12-21T09:20:04Z"
"","4610","KAFKA-6577: Fix Connect system tests and add debug messages","**NOTE: This should be backported to the `1.1` branch, and is currently a blocker for 1.1.**  The `connect_test.py::ConnectStandaloneFileTest.test_file_source_and_sink` system test is failing with the SASL configuration without a sufficient explanation. During the test, the Connect worker fails to start, but the Connect log contains no useful information. There are actual several things compounding to cause the failure and make it difficult to understand the problem.  First, the `tests/kafkatest/tests/connect/templates/connect_standalone.properties` is only adding in the broker's security configuration with the `producer.` and `consumer.` prefixes, but is not adding them with no prefix. The worker uses the AdminClient to connect to the broker to get the Kafka cluster ID and to manage the three internal topics, and the AdminClient is configured via top-level properties. Because the SASL test requires the clients all connect using SASL, the lack of broker security configs means the AdminClient was attempting and failing to connect to the broker. This is corrected by adding the broker's security configuration to the Connect worker configuration file at the top-level. (This was already being done in the `connect_distributed.properties` file.)  Second, the default `request.timeout.ms` for the AdminClient (and the other clients) is 120 seconds, so the AdminClient was retrying for 120 seconds before it would give up and thrown an error. However, the test was only waiting for 60 seconds before determining that the service failed to start. This can be corrected by setting `request.timeout.ms=10000` in the Connect distributed and standalone worker configurations.  Third, the Connect workers were recently changed to lookup the Kafka cluster ID before it started the herder. This is unlike the older uses of the AdminClient to find and manage the internal topics, where failure to connect was not necessarily logged correctly but nevertheless still skipped over, relying upon broker auto-topic creation to create the internal topics. (This may be why the test did not fail prior to the recent change to always require a successful AdminClient connection.) Although the worker never got this far in its startup process, the fact that we missed such an error since the prior releases means that failure to connect with the AdminClient was not being properly reported.  The `ConnectStandaloneFileTest.test_file_source_and_sink` system tests were run locally prior to this fix, and they failed as with the nightlies. Once these fixes were made, the locally run system tests passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-02-21T23:42:19Z","2020-10-16T06:05:17Z"
"","4936","KAFKA-6830: Add consumer fetch request metric for topics","**Description:** Introduce new metric (`ConsumerFetchRequestsPerSec`) for each topic. It is used to count only fetch requests received by broker instance that originated from clients (and not from replicas). This allows us to tell whether the topic is being actively consumed (not only synchronized from replicas).  **Testing strategy:** We want to verify that correct metric is growing depending on whether the partitions hosted by given broker instance are being read from (by clients).  Have a cluster with 2+ nodes (needed for replication).  a1) Create a topic with replication factor = 1. a2) Observe that `ConsumerFetchRequestsPerSec` does not change value (noone is consuming). a3) Start a consumer reading from that topic. Observe that `ConsumerFetchRequestsPerSec` grows. a4) Stop the consumer. Observe that `ConsumerFetchRequestsPerSec` is no longer growing.  b1) Create a topic with replication factor >= 2. b2) Observe that `ConsumerFetchRequestsPerSec` is not changing value. Observe that `TotalFetchRequestsPerSec` is growing. b3) Start a consumer reading from that topic. Observe that `ConsumerFetchRequestsPerSec` is growing. b4) Stop the consumer. Observe that `ConsumerFetchRequestsPerSec` is no longer growing. Observe that `TotalFetchRequestsPerSec` is still growing (replication is still going on).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","adamkotwasinski","2018-04-26T17:13:20Z","2019-05-24T23:13:44Z"
"","5010","KAFKA-6738: (WIP) Error Handling in Connect","**_This PR is a WIP. It has been created to serve as a high-level reference for discussions on the proposal at https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect._**  This feature aims to change the Connect framework to allow it to automatically deal with errors while processing records in a Connector. The following behavior changes are introduced here:  1. **Retry on Failure**: Retry the failed operation a configurable number of times, with backoff between each retry. 2. **Task Tolerance Limits**: Tolerate up to a configurable number of failures in a task.  We also add the following ways to report errors, along with sufficient context to simplify the  debugging process:  1. **Log Error Context**: The error information along with processing context is logged along with the standard application logs. 2. **Dead Letter Queue**: Produce the error information and processing context into a Kafka topic.  The logged information consists of the following bits: 1. Descriptions of the different stages (source/sink tasks, transformations and converters) in the connector, and their configs. 2. The record which caused the exception. 3. The exception and stack trace, if available. 4. Number of attempts (if applicable) made to execute the failed operation. 5. The time of error.  New metrics which will monitor the number of failures, and the behavior of the response handler are added.  The changes proposed here are **backward compatible**. The current behavior in Connect is to kill the task on the first error in any stage. This will remain the default behavior if the connector does not override any of the new configurations which are provided as part of this feature.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-05-11T22:22:21Z","2018-05-24T00:20:49Z"
"","4992","KAFKA-6738: Error Handling in Connect","**_This is a WIP, and is not ready for reviews._**  Signed-off-by: Arjun Satish   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wicknicks","2018-05-09T23:55:57Z","2018-05-10T00:57:57Z"
"","5454","KAFKA-7197 expand gradle build: include Scala 2.13","**_Final PR description (June 2019):_** - include Scala 2.13 into gradle build - handle future milestone and RC versions of Scala in a better way - if no Scala version is specified, default to scala 2.12 (bump from 2.11) - include certain Xlint options (removed by Scala 2.13) for Scala 2.11/2.12 build only - upgrade versions for dependencies:    - scalaLogging: 3.9.0 -->> 3.9.2   - scalatest:        3.0.7 -->> 3.0.8   - scoverage:       1.3.1 -->> 1.4.0   - - - -  **_Initial PR description (August 2018):_**  details:   * build expanded in order to use Scala 2.13 milestone version (version 2.13.0-M3 is used)   * 'scoverage' version upgraded: 1.3.1 -->> 1.4.0-M3  Note: _**./gradlew -PscalaVersion=2.13 jar -q**_ returns few errors (well, as expected):  ``` 8 errors found  FAILURE: Build failed with an exception.  * What went wrong: Execution failed for task ':core:compileScala'. > Compilation failed ```  Upgrade to Scala 2.13.0-M4 depends on:  - https://github.com/scoverage/scalac-scoverage-plugin/pull/229  **_Add Scala 2.13.0-M4 support_**","closed","","dejan2609","2018-08-02T22:47:09Z","2019-06-24T10:31:24Z"
"","4839","MINOR: Java 10 fixes so that the build passes","* Upgrade EasyMock to 3.6 which adds support for Java 10 by upgrading to ASM 6.1.1.  * Ensure that Jacoco is truly disabled for the `core` project. This was the original intent, since it's in Scala, but it had not been achieved. This is important because the Jacoco agent fails when it tries to instrument the classes compiled by scalac with Java 10.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-04-09T06:21:43Z","2018-09-11T06:47:16Z"
"","4812","KAFKA-6376: streams skip metrics","* unify skipped records metering * log warnings when things get skipped * tighten up metrics usage a bit   ### Testing strategy: Unit testing of the metrics and the logs should be sufficient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-04-02T22:38:37Z","2018-04-23T19:46:58Z"
"","4980","KAFKA-6868: Fix buffer underflow and expose group state in the consumer groups API","* The consumer groups API should expose group state.  This information is needed by administrative tools and scripts that access consume groups.  * The partition assignment will be empty when the group is rebalancing. Fix an issue where the adminclient attempted to deserialize this empty buffer.  * Remove nulls from the API and make all collections immutable.  * DescribeConsumerGroupsResult#all should return a result as expected, rather than Void  * Add Builder classes for MemberDescription and ConsumerGroupDescription, so that we can easily add new fields later without having a large number of constructors.  * Fix exception text for GroupIdNotFoundException, GroupNotEmptyException. It was being filled in as ""The group id The group id does not exist was not found"" and similar.  * Add integration test","closed","","cmccabe","2018-05-09T00:33:07Z","2019-05-20T18:55:38Z"
"","4655","MINOR: clean stateDirectory in TopologyTestDriver","* Specifically, on TopologyTestDriver#close() * Allow the test driver to clean up any persistant files that may   have been written by a state store in the topology. * Add a test verifying that persistent state does get cleaned up.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-03-06T23:05:38Z","2018-03-12T21:36:16Z"
"","5046","KAFKA-4423: Drop support for Java 7 (KIP-118) and update deps","* Set --source, --target and --release to 1.8. * Build Scala 2.12 by default. * Remove some conditionals in the build file now that Java 8 is the minimum version. * Bump the version of Jetty, Jersey and Checkstyle (the newer versions require Java 8). * Fixed issues uncovered by the new version if Checkstyle. * A couple of minor updates to handle an incompatible source change in the new version of Jetty. * Add dependency to jersey-hk2 to fix failing tests caused by Jersey upgrade. * Update release script to use Java 8 and to take into account that Scala 2.12 is now built by default. * While we're at it, bump the version of Gradle, Gradle plugins, ScalaLogging, JMH and apache directory api. * Minor documentation updates including the readme and upgrade notes. A number of Streams Java 7 examples can be removed subsequently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-20T04:38:57Z","2018-05-22T06:53:41Z"
"","5045","KAFKA-6921: Remove old Scala producer and related code","* Removed Scala producers, request classes, kafka.tools.ProducerPerformance, encoders, tests. * Updated ConsoleProducer to remove Scala producer support (removed `BaseProducer` and several options that are not used by the Java producer). * Updated a few Scala consumer tests to use the new producer (including a minor refactor of `produceMessages` methods in `TestUtils`). * Updated `ClientUtils.fetchTopicMetadata` to use `SimpleConsumer` instead of `SyncProducer`. * Removed `TestKafkaAppender` as it looks useless and it defined an `Encoder`. * Minor import clean-ups  No new tests added since behaviour should remain the same after these changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-20T03:32:06Z","2018-09-11T06:47:03Z"
"","4549","KAFKA-6424: QueryableStateIntegrationTest#queryOnRebalance should accept raw text","* Remove to .toLowerCase in word count stream * Add method to read text from file, and move text to resources file * Minor cleanup: change argument order in assertEquals  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","h314to","2018-02-08T11:49:55Z","2018-02-17T15:17:18Z"
"","4832","KAFKA-6474: Rewrite tests to use new public TopologyTestDriver [partial]","* Remove ProcessorTopologyTestDriver from TopologyTest     * Fix ProcessorTopologyTest     * Remove ProcessorTopologyTestDriver and InternalTopologyAccessor     * Partially refactored StreamsBuilderTest but missing one test     * Refactor KStreamBuilderTest     * Refactor AbstractStreamTest     * Further cleanup of AbstractStreamTest     * Refactor GlobalKTableJoinsTest     * Refactor InternalStreamsBuilderTest     * Fix circular dependency in build.gradle     * Refactor KGroupedStreamImplTest     * Partial modifications to KGroupedTableImplTest     * Refactor KGroupedTableImplTest     * Refactor KStreamBranchTest     * Refactor KStreamFilterTest     * Refactor KStreamFlatMapTest KStreamFlatMapValuesTest     * Refactor KStreamForeachTest     * Refactor KStreamGlobalKTableJoinTest     * Refactor KStreamGlobalKTableLeftJoinTest     * Refactor KStreamImplTest     * Refactor KStreamImplTest     * Refactor KStreamKStreamJoinTest     * Refactor KStreamKStreamLeftJoinTest     * Refactor KStreamKTableJoinTest     * Refactor KStreamKTableLeftJoinTest     * Refactor KStreamMapTest and KStreamMapValuesTest     * Refactor KStreamPeekTest and KStreamTransformTest     * Refactor KStreamSelectKeyTest     * Refactor KStreamTransformValuesTest     * Refactor KStreamWindowAggregateTest     * Add Depercation anotation to KStreamTestDriver and rollback failing tests in StreamsBuilderTest and KTableAggregateTest     * Refactor KTableFilterTest     * Refactor KTableForeachTest     * Add getter for ProcessorTopology, and simplify tests in StreamsBuilderTest     * Refactor KTableImplTest     * Remove unused imports     * Refactor KTableAggregateTest     * Fix style errors     * Fix gradle build     * Address reviewer comments:       - Remove properties new instance       - Remove extraneous line       - Remove unnecessary TopologyTestDriver instances from StreamsBuilderTest       - Move props.clear() to @After       - Clarify use of timestamp in KStreamFlatMapValuesTest       - Keep test using old Punctuator in KStreamTransformTest       - Add comment to clarify clock advances in KStreamTransformTest       - Add TopologyTestDriverWrapper class to access the protected constructor of TopologyTestDriver       - Revert KTableImplTest.testRepartition to KStreamTestDriver to avoid exposing the TopologyTestDriver processor topology       - Revert partially migrated classes: KTableAggregateTest, KTableFilterTest, and KTableImplTest     * Rebase on current trunk and fix conflicts","closed","streams,","h314to","2018-04-06T00:48:33Z","2018-04-27T15:27:45Z"
"","4799","MINOR: Remove magic number and extract Pattern instance from method as class field","* Remove magic number * Extract Pattern instance from method as class field * Add `@Override` declare  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","asdf2014","2018-03-30T10:08:53Z","2020-10-16T06:24:17Z"
"","5323","MINOR: Remove 1 minute minimum segment interval","* new minimum is 0, just like window size * refactor tests to use smaller segment sizes as well  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-03T16:05:12Z","2018-08-01T22:02:39Z"
"","4492","KAFKA-6499: Do not write offset checkpoint file with empty offset map","* In Checkpoint.write(), if the offset map passed in is empty, skip the writing of the file which would only contain version number and the empty size. From the reading pov, it is the same as no file existed.  * Add related unit tests.  * Minor fixes on log4j messages.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-01-31T01:12:03Z","2018-02-14T19:55:10Z"
"","4987","KAFKA-6863 Kafka clients should try to use multiple DNS resolved IP","* Implementation of KIP-302 * Based on the new client configuration `client.dns.lookup` a  Selector can use InetAddress.getAllByName to find all IPs and iterates over them when they fail to connect * Only use IPv4 or IPv6 addresses * Unit test  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   On attempting to connect to a resolved IP that does not respond, there appears to be a 75 second timeout - this is reflected in the unit test.  If the resolved IP refuses the connection the selector would move the next IP quickly   Note that we also fixed `SelectorTst.testConnectionRefused()`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-05-09T14:58:32Z","2018-10-12T22:25:53Z"
"","4795","KAFKA-6726: Fine Grained ACL for CreateTopics (KIP-277)","* Handling CreateTopicsRequest now requires Create auth on Topic resource and does not require Create on Cluster  * AclCommand --producer option adjusted   * Existing Unit and Integration tests adjusted accordingly  https://issues.apache.org/jira/browse/KAFKA-6726 https://cwiki.apache.org/confluence/display/KAFKA/KIP-277+-+Fine+Grained+ACL+for+CreateTopics+API  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-03-29T14:02:20Z","2018-06-06T18:39:57Z"
"","4917","MINOR: fix NamedCache metrics in Streams","* Fixes a bug in which all NamedCache instances in a process shared one parent metric.  * Also fixes a bug which incorrectly computed the per-cache metric tag (which was undetected due to the former bug).  * Drop the `StreamsMetricsConventions#xLevelSensorName` convention in favor of `StreamsMetricsImpl#xLevelSensor` to allow `StreamsMetricsImpl` to track thread- and cache-level metrics, so that they may be cleanly declared from anywhere but still unloaded at the appropriate time. This was necessary right now so that the `NamedCache` could register a thread-level parent sensor to be unloaded when the thread, not the cache, is closed.  * The above changes made it mostly unnecessary for the `StreamsMetricsImpl` to expose a reference to the underlying `Metrics` registry, so I did a little extra work to remove that reference, including removing inconsistently-used and unnecessary calls to `Metrics#close()` in the tests.  The existing tests should be sufficient to verify this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-04-23T22:05:18Z","2018-04-27T17:38:45Z"
"","4612","KAFKA-6581: Fix the ConsumerGroupCommand indefinite execution if one of the partition is unavailable.","* Checks if partition available before calling consumer.position * Adds timeout on consumer.position() call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","tools,","SahilAggarwal","2018-02-22T11:09:40Z","2018-02-28T04:38:45Z"
"","4578","KAFKA-6560: Replace range query with newly added single point query in Windowed Aggregation","* Add a new fetch(K key, long window-start-timestamp) API into ReadOnlyWindowStore. * Use the new API to replace the range fetch API in KStreamWindowedAggregate and KStreamWindowedReduce. * Added corresponding unit tests. * Also removed some redundant byte serdes in byte stores.  Running some modified simple benchmark with a very limited resource machine on windowed count (https://github.com/apache/kafka/pull/4614) have the following result:  Branch | MB / sec | Records / sec -- | -- | --  Trunk | 0.029 | 317.4 KAFKA-6560 | 5.84 | 63823.58  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-02-15T23:06:33Z","2020-04-24T23:56:58Z"
"","5283","MINOR: temporarily ignore system test until I can fix it","#5253 broke this test. We are working on a fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-06-22T23:09:07Z","2018-06-22T23:55:40Z"
"","5288","MINOR: fix standby stream time","#5253 broke standby restoration for windowed stores.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-06-25T18:01:50Z","2018-07-03T15:18:38Z"
"","5075","KAFKA-6813: return to double-counting for count topology names","#4919 unintentionally changed the topology naming scheme. This change returns to the prior scheme.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2018-05-24T19:26:06Z","2018-06-05T15:38:34Z"
"","4787","KAFKA-6724 ConsumerPerformance resets offsets on every startup","### Remove consumer offset reset on startup  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rootex-","2018-03-28T14:00:38Z","2018-06-03T13:07:21Z"
"","5365","Don't throw exceptions in KafkaHealthcheck.handleNewSession. Should fix KAFKA-4277","### Problem we've encountered  We've had a problem recently with Kafka brokers that apparently did not work. The last remarkable message we found in our logs was [this](https://github.com/apache/kafka/blob/1.0/core/src/main/scala/kafka/utils/ZkUtils.scala#L440) Exception, which contained [KafkaHealthcheck.register()](https://github.com/apache/kafka/blob/1.0/core/src/main/scala/kafka/server/KafkaHealthcheck.scala#L59) in its stack trace.  In our case, the ephemeral Zookeeper node for the broker that [handleNewSession](https://github.com/apache/kafka/blob/1.0/core/src/main/scala/kafka/server/KafkaHealthcheck.scala#L119) should create apparently still existed - Kafka restarted too quickly after it crashed before. The exception caused by the attempt to re-create this Zookeeper node eventually causes [ZkEventThread.run()](https://github.com/sgroschupf/zkclient/blob/0.9/src/main/java/org/I0Itec/zkclient/ZkEventThread.java#L63) to idle, since the Exception is caught there and merely logged. Since no further Events occur, the event thread apparently idles.  ### Solution and rationale  In order to have clean well-defined behavior in this case, we've decided that it would be best to finish the broker process in such a case. We've decided to `sys.exit(1)` and rely on the [shutdownHook](https://github.com/apache/kafka/blob/1.0/core/src/main/scala/kafka/Kafka.scala#L88) to do a proper shutdown.  This pull request contains the change described above. It should fix issue [KAFKA-4277](https://issues.apache.org/jira/browse/KAFKA-4277).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Sh4pe","2018-07-13T16:57:12Z","2020-10-19T05:42:09Z"
"","5188","KAFKA-7029: Update ReplicaVerificationTool to use Java Consumer","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-06-11T18:15:27Z","2018-06-12T07:46:25Z"
"","5115","MINOR: Remove usages of `scala.collection.JavaConversions`, and some typo fixes","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-06-01T18:39:03Z","2018-06-03T06:45:45Z"
"","5088","KAFKA-6955: Use Java AdminClient in DeleteRecordsCommand","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-05-27T21:19:07Z","2018-06-02T19:41:56Z"
"","4529","MINOR: Update copyright year in NOTICE","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  I'll also backport this proactively through a few older branches since it'll be required anyway if we ever want/need to do a bugfix release on them.","closed","","ewencp","2018-02-05T21:35:30Z","2018-02-06T02:48:29Z"
"","5479","MINOR: Add spotlessScalaCheck dependency to streams-scala test task","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-08T16:14:18Z","2018-08-20T09:29:15Z"
"","5472","MINOR: Fix Streams scala format violations","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-07T17:39:18Z","2018-08-08T06:05:41Z"
"","5470","KAFKA-7253 The connector type responded by worker is always null when…","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-08-07T04:04:44Z","2019-01-08T17:51:51Z"
"","5461","MINOR: increase timeout values in streams tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","omkreddy","2018-08-05T19:36:30Z","2018-08-06T16:25:33Z"
"","5459","MINOR: close producer instance in AbstractJoinIntegrationTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-04T11:22:43Z","2018-08-15T18:55:35Z"
"","5457","MINOR: log AdminClient configs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-08-03T18:11:36Z","2018-08-07T02:53:12Z"
"","5450","MINOR: clean up node and store sensors","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-08-02T18:22:46Z","2018-08-06T14:27:45Z"
"","5422","[WIP: DO NOT MERGE] KAFKA-3522: Enable StateStore upgrade path","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2018-07-25T18:44:40Z","2019-08-21T17:33:53Z"
"","5418","KAFKA-7195: Fix StreamStreamJoinIntegrationTest test failures","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-24T01:08:42Z","2018-08-04T19:28:55Z"
"","5413","MINOR: update DEFAULT_FIX_VERSION to 1.1.2 in kafka-merge-pr.py","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-07-22T02:36:35Z","2018-07-22T22:36:59Z"
"","5402","MINOR: Fix format violations in streams scala tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-20T06:54:11Z","2018-07-20T16:05:32Z"
"","5401","MINOR: Implement toString() in config validator classes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-20T03:41:19Z","2018-08-04T11:25:58Z"
"","5399","KAFKA-5037 Follow-up: move Scala test to Java","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-19T22:57:26Z","2018-07-20T17:38:00Z"
"","5396","MINOR: Fix transient test failure in SslTransportLayerTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-19T20:25:35Z","2018-07-20T08:16:21Z"
"","5386","MINOR: Remove references to version 1.2 in docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-18T17:02:19Z","2018-07-18T17:08:00Z"
"","5381","HOTFIX: Minor web docs fixes on message header","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-17T21:52:53Z","2018-07-18T00:31:35Z"
"","5375","MINOR: update release.py","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-07-17T02:41:35Z","2018-07-22T22:36:36Z"
"","5368","MINOR: Update build.gradle and release.py to upload streams-scala_2.12","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-07-15T16:21:09Z","2018-07-19T10:33:35Z"
"","5366","KAFKA-7161: check invariant: oldValue is in the state","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2018-07-13T18:04:41Z","2018-08-01T22:02:30Z"
"","5355","KAFKA-7147; ReassignPartitionsCommand should be able to connect to broker over SSL","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-07-10T21:29:17Z","2018-12-07T02:48:45Z"
"","5349","KAFKA-7139 Support to exclude the internal topics in kafka-topics.sh …","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-07-09T10:24:07Z","2018-08-25T19:08:25Z"
"","5348","KAFKA-6161 Add default implementation to close() and configure() for …","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","chia7712","2018-07-09T01:43:33Z","2020-06-13T00:03:49Z"
"","5343","MINOR: Upgrade rocksdb to 5.14.2","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-07-06T19:39:00Z","2018-07-09T04:14:52Z"
"","5333","[KAFKA-6127] Streams should never block infinitely","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ConcurrencyPractitioner","2018-07-04T06:29:27Z","2019-01-02T03:14:05Z"
"","5325","MINOR: Include quota related public interfaces to javadocs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-07-03T17:34:29Z","2018-07-04T06:40:39Z"
"","5324","MINOR: Remove deprecated ZkUtils usage from EmbeddedKafkaCluster","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","omkreddy","2018-07-03T16:54:36Z","2018-07-20T03:06:08Z"
"","5319","KAFKA-7140: Remove deprecated poll usages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-07-02T12:58:39Z","2018-08-23T13:10:48Z"
"","5315","Minor: document thread safety of KafkaConsumer#metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-07-01T05:51:10Z","2020-03-17T08:59:45Z"
"","5280","MINOR: Fix expected output in Streams quickstart","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vahidhashemian","2018-06-22T20:46:20Z","2018-06-23T00:23:31Z"
"","5274","[KAFKA-7087] Add preferred leader report to describe","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","viktorsomogyi","2018-06-22T15:14:12Z","2018-10-17T00:02:49Z"
"","5262","MINOR: Update version for doc to 2.0.0","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-21T10:05:27Z","2018-06-21T10:18:32Z"
"","5261","KAFKA-7084 NewTopicBuilder#config should accept Map r…","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-06-21T09:40:25Z","2020-03-17T09:00:12Z"
"","5252","KAFKA-6938: Add headers support in new api section","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-06-19T20:27:23Z","2018-06-19T21:24:15Z"
"","5251","MINOR: Remove new consumer references in the code","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vahidhashemian","2018-06-19T17:58:30Z","2018-06-20T21:09:42Z"
"","5249","MINOR: Update examples / tutorials / docs with Java 8 syntax","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-19T00:10:48Z","2018-06-21T17:03:07Z"
"","5223","MINOR: Include StickyAssignor in system tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-06-14T04:50:20Z","2019-05-11T18:13:08Z"
"","5212","Minor: Show the specified value in logging the deprecation warnings o…","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-06-13T15:45:41Z","2019-01-05T10:57:31Z"
"","5202","KAFKA-7048 NPE when creating connector","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2018-06-12T17:27:31Z","2020-10-16T06:02:59Z"
"","5195","KAFKA-7021: Update upgrade guide section for reusing source topic","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","guozhangwang","2018-06-12T00:07:32Z","2018-06-15T15:52:41Z"
"","5193","KAFKA-7037: Improve the topic command description of `--topic` option","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-06-11T21:00:59Z","2018-11-28T10:48:50Z"
"","5192","KAFKA-7030: Add configuration to disable message down-conversion (KIP-283)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-06-11T20:27:39Z","2018-06-14T06:38:55Z"
"","5182","KAFKA-7032 The TimeUnit is neglected by KakfaConsumer#close(long, Tim…","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-06-11T06:49:33Z","2018-06-15T11:16:04Z"
"","5174","WIP: Remove old consumers","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-06-09T17:50:26Z","2018-09-11T06:46:48Z"
"","5171","MINOR: Clean up","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiminhsieh","2018-06-08T23:49:47Z","2018-06-12T15:01:07Z"
"","5167","KAFKA-7022: Validate that log.segment.bytes is big enough for one batch","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rajinisivaram","2018-06-08T20:19:57Z","2018-06-10T06:53:23Z"
"","5165","MINOR: Update TestLogCleaning tool to use java consumer","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-06-08T15:33:10Z","2018-06-08T16:29:14Z"
"","5154","MINOR: Fix version in 2.0.0 streams doc","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-06T15:54:02Z","2018-06-06T16:01:02Z"
"","5153","MINOR: Bump version to 2.1.0-SNAPSHOT","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-06T15:41:01Z","2018-06-06T16:24:53Z"
"","5137","MINOR: Rename package `internal` to `internals` for consistency","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-06-05T09:03:09Z","2018-06-05T20:07:30Z"
"","5130","KAFKA-4690: Return new error code for DeleteTopicRequest when topic deletion disabled.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-06-04T17:24:46Z","2018-07-26T17:00:54Z"
"","5127","HOTFIX: Fix compilation error in ConsumerPerformance","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-06-02T23:55:58Z","2018-06-03T00:01:44Z"
"","5118","MINOR: Remove deprecated KafkaStreams constructors in docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2018-06-01T19:40:15Z","2018-06-04T20:43:52Z"
"","5098","MINOR: Hygiene fixes in KafkaFutureImpl","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-05-30T10:55:03Z","2018-12-22T08:01:50Z"
"","5093","KAFKA-5905: Remove deprecated PrincipalBuilder and DefaultPrincipalBuilder","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-05-29T15:48:51Z","2021-07-09T15:36:07Z"
"","5092","KAFKA-5523: Remove ReplayLogProducer tool","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-05-29T15:34:46Z","2018-06-03T08:52:12Z"
"","5082","KAFKA-6950: Delay response to failed client authentication to prevent potential DoS issues (KIP-306)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2018-05-26T00:52:42Z","2018-09-14T03:33:04Z"
"","5076","KAFKA-6941 when passing port = 0 to worker, the advertisedPort still …","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-05-25T01:49:21Z","2020-03-17T09:00:15Z"
"","5070","MINOR: Improve error when append fails due to unexpected first/last offset","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-05-23T14:45:35Z","2018-06-05T17:42:48Z"
"","5062","MINOR: Print metric name if testMetricsLeak fails","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-05-22T17:39:05Z","2018-06-21T19:14:37Z"
"","5054","KAFKA-2951: Add a testcase to verify produce, consume with acls for topic/group wildcard resources.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-05-21T18:08:11Z","2018-05-26T06:49:32Z"
"","5053","Update doc of curried joins and aggregates","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joan38","2018-05-21T17:32:04Z","2018-05-30T21:49:09Z"
"","5038","KAFKA-6729: Follow up; disable logging for source KTable.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-18T22:50:30Z","2020-04-24T23:57:23Z"
"","5036","KAFKA-6917: Process txn completion asynchronously to avoid deadlock","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-18T14:10:41Z","2018-05-18T23:25:53Z"
"","5030","KAFKA-6912: Add test for authorization with custom principal types","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-17T13:54:37Z","2018-06-03T01:29:52Z"
"","5021","KAFKA-5994: Log ClusterAuthorizationException for all ClusterAction requests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-05-16T02:52:34Z","2019-01-10T13:50:54Z"
"","5006","MINOR: doc change for deprecate removal","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-05-11T16:04:57Z","2020-04-24T23:50:35Z"
"","4999","KAFKA-6893: Create processors before starting acceptor in SocketServer","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-10T18:39:06Z","2018-05-10T21:18:00Z"
"","4979","MINOR: add equals()/hashCode() for Produced/Consumed","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","norwood","2018-05-08T20:29:32Z","2018-05-08T23:07:34Z"
"","4954","MINOR: Update dynamic broker configuration doc for truststore update","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-02T11:30:44Z","2018-05-10T15:55:16Z"
"","4953","KAFKA-6834: Handle compaction with batches bigger than max.message.bytes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-05-02T08:16:29Z","2018-05-09T10:46:37Z"
"","4952","Minor: Replace magic string in JsonConverter by constants","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2018-05-02T02:17:03Z","2020-03-03T03:51:35Z"
"","4941","HOTFIX: rename run_test to execute in streams simple benchmark","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-27T18:59:00Z","2020-04-24T23:48:05Z"
"","4929","KAFKA-6825: Make StreamsConfig#DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG public","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-25T15:40:02Z","2020-04-24T23:48:04Z"
"","4921","MINOR: Fix sasl.jaas.config doc string","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-04-24T17:56:18Z","2018-07-03T15:44:02Z"
"","4920","KAFKA-6526: Enable unclean leader election without controller change","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-24T15:51:57Z","2018-05-01T14:27:03Z"
"","4910","MINOR: Remove deprecated KTable#writeAs, print, foreach, to, through","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-04-21T18:47:18Z","2020-04-24T23:55:09Z"
"","4890","KAFKA-6800: Update SASL/PLAIN and SCRAM docs to use KIP-86 callbacks","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-04-18T13:11:55Z","2018-06-11T10:29:03Z"
"","4784","MINOR: Remove 1.2.0 changes from 1.1 streams doc","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-28T09:56:26Z","2018-03-28T16:03:01Z"
"","4751","MINOR: remove extraneous double initialization of Stats.index","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cburroughs","2018-03-21T19:24:14Z","2018-03-24T19:39:10Z"
"","4750","MINOR: clarify ProducerPerformance throughput units","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","cburroughs","2018-03-21T19:12:12Z","2018-04-05T07:22:25Z"
"","4748","KAFKA-6700; allow producer performance test to run for a fixed duration","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","cburroughs","2018-03-21T15:28:13Z","2018-03-21T15:41:38Z"
"","4747","MINOR: Fix AdminClient.describeConfigs() of listener configs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-21T14:23:18Z","2018-03-22T20:05:46Z"
"","4740","MINOR: Document workaround for KAFKA-6680 for 1.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-20T14:52:48Z","2018-03-21T11:21:06Z"
"","4726","MINOR: Fix some compiler warnings","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-03-16T16:43:01Z","2018-03-19T15:04:09Z"
"","4671","MINOR: Fix record conversion time in metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-03-09T13:31:21Z","2018-03-09T16:50:35Z"
"","4663","KAFKA-6624; Prevent concurrent log flush and log deletion","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-03-08T04:50:27Z","2018-03-14T19:13:11Z"
"","4648","KAFKA-3806: Increase offsets retention default to 7 days (KIP-186)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","ewencp","2018-03-05T19:34:18Z","2018-03-09T19:37:40Z"
"","4528","MINOR: Update release script with new remote, better error handling, correct mvn deploy profile","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-02-05T21:07:46Z","2018-02-06T17:40:10Z"
"","4490","KAFKA-6452: Add documentation for delegation token authentication","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-01-30T19:46:59Z","2018-07-03T15:44:48Z"
"","4815","KAFKA-6728: Corrected the worker’s instantiation of the HeaderConverter","## Summary of the problem When the `header.converter` is not specified in the worker config or the connector config, a bug in the `Plugins` test causes it to never instantiate the `HeaderConverter` instance, even though there is a default value.  This is a problem as soon as the connector deals with headers, either in records created by a source connector or in messages on the Kafka topics consumed by a sink connector. As soon as that happens, a NPE occurs.  A workaround is to explicitly set the `header.converter` configuration property, but this was added in AK 1.1 and thus means that upgrading to AK 1.1 will not be backward compatible and will require this configuration change.  ## The Changes  The `Plugins.newHeaderConverter` methods were always returning null if the `header.converter` configuration value was not specified in the supplied connector or worker configuration. Thus, even though the `header.converter` property has a default, it was never being used.  The fix was to only check whether a `header.converter` property was specified when the connector configuration was being used, and if no such property exists in the connector configuration to return null. Then, when the worker configuration is being used, the method simply gets the `header.converter` value (or the default if no value was explicitly set).  Also, the ConnectorConfig had the same default value for the `header.converter` property as the WorkerConfig, but this resulted in very confusing log messages that implied the default header converter should be used even when the worker config specified the `header.converter` value. By removing the default, the log messages now make sense, and the Worker still properly instantiates the correct header converter.  Finally, updated comments and added log messages to make it more clear which converters are being used and how they are being converted.  ## Testing  Several new unit tests for `Plugins.newHeaderConverter` were added to check the various behavior. Additionally, the runtime JAR with these changes was built and inserted into an AK 1.1 installation, and a source connector was manually tested with 8 different combinations of settings for the `header.converter` configuration:  1. default value 1. worker configuration has `header.converter` explicitly set to the default 1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin 1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin 1. connector configuration has `header.converter` explicitly set to the default 1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin 1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin 1. worker configuration has `header.converter` explicitly set to the default, and the connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin  The worker created the correct `HeaderConverter` implementation with the correct configuration in all of these tests.  Finally, the default configuration was used with the aforementioned custom source connector that generated records with headers, and an S3 connector that consumes the records with headers (but didn't do anything with them). This test also passed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2018-04-03T01:37:27Z","2020-10-16T06:24:17Z"
"","4818","KAFKA-6743 ConsumerPerformance fails to consume all messages","## ConsumerPerformance fails to consume all messages  on topics with large number of partitions due to a relatively short default polling loop timeout (1000 ms) that is not reachable and modifiable by the end user.   ### Demo Create a topic of 10 000 partitions, send a 50 000 000 of 100 byte records using kafka-producer-perf-test and consume them using kafka-consumer-perf-test (ConsumerPerformance). You will likely notice that the number of records returned by the kafka-consumer-perf-test is many times less than expected 50 000 000.   This happens due to specific ConsumerPerformance implementation. As the result, in some rough cases it may take a long enough time to process/iterate through the records polled in batches, thus, the time may exceed the default hardcoded polling loop timeout and this is probably not what we want from this utility.   ### Possible options 1) Increasing polling loop timeout in ConsumerPerformance implementation. It defaults to 1000 ms and is hardcoded, thus cannot be changed but we could export it as an OPTIONAL kafka-consumer-perf-test parameter to enable it on a script level configuration and available to the end user. 2) Decreasing max.poll.records on a Consumer config level. This is not a fine option though since we do not want to touch the default settings.  [KAFKA-6743: ConsumerPerformance fails to consume all messages on topics with large number of partitions](https://issues.apache.org/jira/browse/KAFKA-6743) [KIP-281: ConsumerPerformance: Increase Polling Loop Timeout and Make It Reachable by the End User](https://cwiki.apache.org/confluence/display/KAFKA/KIP-281%3A+ConsumerPerformance%3A+Increase+Polling+Loop+Timeout+and+Make+It+Reachable+by+the+End+User)  ### Result `bin/kafka-consumer-perf-test.sh... --polling-loop-timeout  `   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rootex-","2018-04-03T14:45:06Z","2018-06-02T23:43:54Z"
"","5420","[MINOR] Improve consumer logging on LeaveGroup","","closed","","dhruvilshah3","2018-07-24T17:01:56Z","2019-01-22T15:04:56Z"
"","5403","[WIP]","","closed","","dhruvilshah3","2018-07-20T08:47:43Z","2018-07-20T08:48:25Z"
"","5400","KAFKA-7185: Allow empty resource name when matching ACLs","","closed","","dhruvilshah3","2018-07-19T23:37:11Z","2018-07-20T23:38:05Z"
"","5393","KAFKA-7183: Add a trogdor test that creates many connections to brokers","","closed","","cmccabe","2018-07-18T22:34:45Z","2019-05-20T18:54:54Z"
"","5392","MINOR: fix upgrade docs for Streams","","closed","","mjsax","2018-07-18T21:13:28Z","2018-07-19T15:46:00Z"
"","5378","MINOR: More efficient midpoint calc for AbstractIndex","","closed","","cmccabe","2018-07-17T16:50:46Z","2019-11-17T18:09:26Z"
"","5314","Soak6","","closed","","cmccabe","2018-06-29T21:43:42Z","2019-05-20T18:55:01Z"
"","5271","Add note about num.standby.replicas","","closed","","JimGalasyn","2018-06-22T00:13:24Z","2018-06-26T15:29:36Z"
"","5250","MINOR: Add system test for verifying lazy down-conversion implementation (KIP-283)","","open","core,","dhruvilshah3","2018-06-19T00:53:48Z","2020-01-30T12:57:06Z"
"","5240","KAFKA-7063: Update documentation to remove references to old producers and consumers","","closed","","omkreddy","2018-06-16T18:17:17Z","2018-08-22T02:44:08Z"
"","5140","KAFKA-6983: Error while deleting segments - The process cannot access the file because it is being used by another process","","open","","wuqingjun","2018-06-05T20:56:59Z","2018-06-08T06:23:04Z"
"","5113","[MINOR] Use SL4J string interpolation instead of string concatenation","","closed","","dhruvilshah3","2018-06-01T00:54:54Z","2018-06-12T06:02:20Z"
"","5100","MINOR: update consumer javadoc for position method","","closed","","omkreddy","2018-05-30T15:58:13Z","2018-06-27T23:59:15Z"
"","5091","KAFKA-6962 added better docs to DESCRIBE_CONFIGS_REQUEST_RESOURCE_V0","","closed","","kdrakon","2018-05-29T03:22:11Z","2019-11-20T00:20:04Z"
"","5041","KAFKA-3143: Controller should transition offline replicas on startup","","closed","controller,","omkreddy","2018-05-19T19:31:02Z","2019-05-21T15:21:18Z"
"","5022","KAFKA-6905: Document that Processors may be re-used by Streams","","closed","docs,","glasser","2018-05-16T07:30:08Z","2018-05-16T21:11:21Z"
"","5013","[KAFKA-6730] Simplify State Store Recovery","","closed","streams,","ConcurrencyPractitioner","2018-05-12T18:41:16Z","2018-06-05T20:35:48Z"
"","5012","KAFKA-4696: Streams standby task assignment should be state-store aware","","closed","streams,","ConcurrencyPractitioner","2018-05-12T18:40:11Z","2019-05-23T18:42:14Z"
"","5005","MINOR: Update consumer javadocs","","closed","","omkreddy","2018-05-11T15:54:34Z","2018-05-28T19:08:15Z"
"","4995","KAFKA-6891 fixed config constraints in KafkaConnect + refactoring","","open","connect,","oleg-smith","2018-05-10T08:26:36Z","2020-03-22T06:27:56Z"
"","4957","KAFKA-6835: Enable topic unclean leader election to be enabled without controller change","","closed","","omkreddy","2018-05-02T14:44:16Z","2018-08-20T16:54:11Z"
"","4916","MINOR: Fix TRACE logging in ReplicaManager","","closed","","cmccabe","2018-04-23T17:55:44Z","2019-05-20T18:56:29Z"
"","4901","[KAFKA-6730] Simplify state store recovery","","closed","streams,","ConcurrencyPractitioner","2018-04-19T17:50:50Z","2018-05-16T04:21:50Z"
"","4900","KAFKA-6782: solved the bug of restoration of aborted messages for GlobalStateStore and KGlobalTable","","closed","streams,","Gitomain","2018-04-19T16:19:21Z","2018-06-12T20:31:10Z"
"","4897","KAFKA-6394: Add a check to prevent misconfiguration of advertised listeners","","closed","","omkreddy","2018-04-19T13:42:47Z","2019-03-19T21:08:56Z"
"","4892","MINOR: Fix Trogdor tests, partition assignments","","closed","","cmccabe","2018-04-18T21:38:57Z","2019-05-20T18:55:55Z"
"","4866","KAFKA-6778: AdminClient.describeConfigs() should return error for non-existent topics","","closed","","omkreddy","2018-04-13T09:37:38Z","2018-07-03T15:44:05Z"
"","4862","KAFKA-6785: Add Trogdor documentation","","closed","","cmccabe","2018-04-12T17:49:58Z","2019-05-20T18:55:54Z"
"","4850","KAFKA-6771. Make specifying partitions more flexible","","closed","","cmccabe","2018-04-10T21:38:41Z","2018-04-16T07:55:14Z"
"","4849","KAFKA-4883: handle NullPointerException while parsing login modue control flag","","closed","","omkreddy","2018-04-10T15:12:18Z","2018-07-03T15:44:56Z"
"","4844","MINOR: Add NullPayloadGenerator to Trogdor","","closed","","cmccabe","2018-04-10T00:04:42Z","2018-04-10T19:48:39Z"
"","4838","KAFKA-6752: Enable unclean leader election metric","","closed","","omkreddy","2018-04-08T18:01:53Z","2018-07-03T15:45:18Z"
"","4824","KAFKA-6741:  Disable Selector's idle connection timeout in testNetworkThreadTimeRecorded() test","","closed","","omkreddy","2018-04-04T19:12:59Z","2018-07-03T15:44:54Z"
"","4811","MINOR: Add Timed wait to SslTransportLayerTest.testNetworkThreadTimeRecorded","","closed","","omkreddy","2018-04-02T09:40:50Z","2018-07-03T15:44:14Z"
"","4781","MINOR: Fix ReassignPartitionsClusterTest.testHwAfterPartitionReassignment test","","closed","","omkreddy","2018-03-27T18:07:35Z","2018-07-03T15:44:16Z"
"","4780","MINOR: StreamsConfig `upgrade.from` should not be in list of deprecated configs","","closed","streams,","mjsax","2018-03-27T05:17:50Z","2018-03-27T20:46:21Z"
"","4779","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-27T03:49:48Z","2018-04-07T00:00:53Z"
"","4778","MINOR: Fix typos in docs and address some missed review comments","","closed","streams,","mjsax","2018-03-27T00:59:11Z","2018-03-27T20:44:34Z"
"","4773","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-26T01:55:47Z","2018-06-05T23:47:59Z"
"","4768","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-23T23:48:05Z","2018-06-05T23:48:02Z"
"","4761","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-23T04:39:00Z","2018-06-05T23:48:04Z"
"","4758","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-22T18:07:50Z","2018-03-27T04:46:36Z"
"","4749","MINOR: remove obsolete warning in StreamsResetter","","closed","streams,","mjsax","2018-03-21T18:04:53Z","2018-03-21T23:26:18Z"
"","4746","KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0","","closed","streams,","mjsax","2018-03-21T06:03:44Z","2018-06-05T23:48:05Z"
"","4741","KAFKA-6694: The Trogdor Coordinator should support filtering task responses","","closed","","cmccabe","2018-03-20T21:27:21Z","2019-05-20T19:06:44Z"
"","4737","KAFKA-6688. The Trogdor coordinator should track task statuses","","closed","","cmccabe","2018-03-20T05:17:26Z","2019-05-20T19:07:03Z"
"","4728","MINOR: fix flaky Streams EOS system test","","closed","streams,","mjsax","2018-03-16T23:49:12Z","2018-03-18T01:03:55Z"
"","4713","MINOR: fix KafkaStreams#cleanUp(): should throw and fail for lock conflict","","closed","streams,","mjsax","2018-03-14T21:14:00Z","2020-03-17T03:37:02Z"
"","4712","MINOR: Trogdor should not assume an agent co-located with the controller","","closed","","cmccabe","2018-03-14T21:03:30Z","2019-05-20T18:57:54Z"
"","4701","MINOR: Remove unused server exceptions","","closed","","omkreddy","2018-03-13T17:24:51Z","2018-07-03T15:44:23Z"
"","4688","MINOR: Avoid nulls when deserializing Trogodor JSON","","closed","","cmccabe","2018-03-12T18:18:20Z","2019-05-20T18:57:55Z"
"","4667","MINOR: Streams doc example should not close store","","closed","docs,","mjsax","2018-03-08T22:47:57Z","2018-03-13T07:45:03Z"
"","4636","KAFKA-6054: Add 'version probing' to Kafka Streams rebalance","","closed","streams,","mjsax","2018-03-02T07:39:23Z","2018-06-05T23:48:10Z"
"","4615","[KAFKA-4696] Streams standby task assignment should be state-store aware","","closed","streams,","ConcurrencyPractitioner","2018-02-22T22:35:17Z","2018-05-16T04:21:15Z"
"","4594","MINOR: Fix Streams-Broker-Compatibility system test","","closed","streams,","mjsax","2018-02-20T03:25:57Z","2018-02-21T23:58:01Z"
"","4593","MINOR: follow up on Streams EOS system tests","","closed","streams,","mjsax","2018-02-19T23:51:02Z","2018-02-21T18:24:59Z"
"","4582","MINOR: fixes lgtm.com warnings","","closed","streams,","mjsax","2018-02-16T23:46:03Z","2018-02-26T03:44:39Z"
"","4566","MINOR: Update log condition in JassContext.loadServerContext method","","closed","","omkreddy","2018-02-13T18:51:46Z","2018-07-03T15:44:46Z"
"","4532","MINOR: improve security docs for Kafka Streams","","closed","docs,","mjsax","2018-02-06T19:05:25Z","2018-02-08T19:51:35Z"
"","4523","[KAFKA-5996] JsonConverter generates Mismatching schema DataException","","closed","connect,","ConcurrencyPractitioner","2018-02-04T04:38:27Z","2020-10-16T06:24:15Z"
"","4522","[Kafka-5996] JsonConverter generates ""Mismatching schema"" DataException","","closed","","ConcurrencyPractitioner","2018-02-04T04:30:30Z","2018-02-04T08:31:42Z"
"","4518","MINOR: Add missing generics and surpress warning annotations","","closed","streams,","mjsax","2018-02-02T23:17:07Z","2018-02-08T21:32:28Z"
"","4505","[MINOR]: Bump trunk versions to 1.2-SNAPSHOT","","closed","","dguy","2018-02-01T09:41:04Z","2018-02-01T11:35:58Z"
"","4502","KAFKA-3625: TopologyTestDriver must process output for wall-clock-time punctuations and on close()","","closed","streams,","mjsax","2018-02-01T01:17:20Z","2018-02-09T23:32:51Z"
"","4500","MINOR: update upgrade notes for Streams API; message format 0.10 requiered","","closed","docs,","mjsax","2018-01-31T22:03:55Z","2018-02-07T19:49:46Z"