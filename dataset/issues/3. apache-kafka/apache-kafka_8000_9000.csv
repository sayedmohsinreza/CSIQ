"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","3585","KAFKA-5659. Fix error handling, efficiency issue in AdminClient#describeConfigs","…when only broker info is requested","closed","","cmccabe","2017-07-26T21:23:58Z","2019-05-20T19:08:49Z"
"","3506","KAFKA-5565. Add a broker metric specifying the number of consumer gro…","…up rebalances in progress","closed","","cmccabe","2017-07-07T22:13:35Z","2019-05-20T18:45:59Z"
"","3664","KAFKA-5733: ensure clean RocksDB directory before setting prepareForB…","…ulkload settings","closed","","bbejeck","2017-08-14T14:47:30Z","2017-08-24T03:02:27Z"
"","4163","MINOR: build.gradle: sourceCompatibility, targetCompatibility to allp…","…rojects","closed","","cmccabe","2017-10-31T16:32:43Z","2017-11-02T21:59:14Z"
"","3679","KAFKA-5744: ShellTest: add tests for attempting to run nonexistent pr…","…ogram, error return","closed","","cmccabe","2017-08-16T22:47:24Z","2019-05-20T18:58:20Z"
"","3557","KAFKA-5623: ducktape kafka service: do not assume Service contains nu…","…m_nodes","closed","","cmccabe","2017-07-21T00:03:11Z","2019-05-20T18:44:54Z"
"","3822","KAFKA-5792: Fix Transient failure in KafkaAdminClientTest.testHandleT…","…imeout","closed","","cmccabe","2017-09-08T23:04:43Z","2019-05-20T18:46:17Z"
"","3871","KAFKA-5909; Removed the source jars from classpath while executing CL…","…I tools.  - redundant `for` loops removed. - I did this change with assumption that there are no priorities in jar while assigning it to the classpath.","closed","","kamalcph","2017-09-15T11:29:38Z","2018-02-12T04:59:48Z"
"","4230","KAFKA-5563: Moved comparison of connector name from url against name …","…from config to own function and added check to create connector call.","closed","","soenkeliebau","2017-11-17T09:43:49Z","2017-11-26T01:52:22Z"
"","4399","MINOR: LogCleanerManager.cleanableOffsets should create objects …","…for dirty non-active segments only when `log.cleaner.min.compaction.lag.ms` is greater than 0  With `log.cleaner.min.compaction.lag.ms`'s default value of 0, there is no need to hold heap objects for those dirty non-active segments. This could reduce the heap size and also avoid the unnecessary monitor lock retrieval.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2018-01-07T03:40:55Z","2018-01-25T17:49:51Z"
"","3538","KAFKA-5600: fix GroupMetadataManager doesn't read offsets of segmente…","…d log correctly  the while loop was too big and need to be closed earlier to see the fix, ignore whitespace since most of it is indentation  this bug was introduced by commit 5bd06f1d542e6b588a1d402d059bc24690017d32","closed","","bjrke","2017-07-17T12:22:10Z","2017-07-17T18:54:50Z"
"","3815","KAFKA-5846; Used Singleton NoOpConsumerRebalanceListener in the Kafka…","…Consumer#subscribe call.  - I haven't removed the internal class 'NoOpConsumerRebalanceListener', as it might conflict with the existing users who's using it.","closed","","kamalcph","2017-09-08T12:29:29Z","2018-10-17T09:36:46Z"
"","4105","KAFKA-6102: Consolidate MockTime implementations between connect and …","…clients","closed","connect,","cmccabe","2017-10-20T21:50:10Z","2020-10-16T06:29:13Z"
"","3848","KAFKA-5692: Change PreferredReplicaLeaderElectionCommand to use Admin…","…Client  See also KIP-183.  The contribution is my original work and I license the work to the project under the project's open source license.  This implements the following algorithm:  1. AdminClient sends ElectPreferredLeadersRequest. 2. KafakApis receives ElectPreferredLeadersRequest and delegates to    ReplicaManager.electPreferredLeaders() 3. ReplicaManager delegates to KafkaController.electPreferredLeaders() 4. KafkaController adds a PreferredReplicaLeaderElection to the EventManager, 5. ReplicaManager.electPreferredLeaders()'s callback uses the    delayedElectPreferredReplicasPurgatory to wait for the results of the    election to appear in the metadata cache. If there are no results    because of errors, or because the preferred leaders are already leading    the partitions then a response is returned immediately.  In the EventManager work thread the preferred leader is elected as follows:  1. The EventManager runs PreferredReplicaLeaderElection.process() 2. process() calls KafkaController.onPreferredReplicaElectionWithResults() 3. KafkaController.onPreferredReplicaElectionWithResults()    calls the PartitionStateMachine.handleStateChangesWithResults() to    perform the election (asynchronously the PSM will send LeaderAndIsrRequest    to the new and old leaders and UpdateMetadataRequest to all brokers)    then invokes the callback.  Note: the change in parameter type for CollectionUtils.groupDataByTopic(). This makes sense because the AdminClient APIs use Collection consistently, rather than List or Set. If binary compatiblity is a consideration the old version should be kept, delegating to the new version.  I had to add PartitionStateMachine.handleStateChangesWithResults() in order to be able to process a set of state changes in the PartitionStateMachine *and get back individual results*. At the same time I noticed that all callers of existing handleStateChange() were destructuring a TopicAndPartition that they already had in order to call handleStateChange(), and that handleStateChange() immediately instantiated a new TopicAndPartition. Since TopicAndPartition is immutable this is pointless, so I refactored it. handleStateChange() also now returns any exception it caught, which is necessary for handleStateChangesWithResults()","closed","","tombentley","2017-09-13T15:59:35Z","2019-01-28T08:24:36Z"
"","4472","KAFKA-6481: Improving performance of the function ControllerChannelManager.addUpd…","…ateMetadataRequestForBrokers  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2018-01-25T00:15:10Z","2018-02-17T01:20:02Z"
"","3794","KAFKA-5838. Speed up running system tests in docker a bit with better…","… sshd options","open","","cmccabe","2017-09-05T18:14:42Z","2019-11-15T19:35:22Z"
"","3741","KAFKA-5762; LogContext used to capture the clientId implicitly in the…","… logs.","closed","","kamalcph","2017-08-25T14:52:16Z","2017-08-30T16:54:21Z"
"","3785","KAFKA-5830: kafka-configs.sh should allow deletion of all configs for…","… an entity  Added a --delete-all-configs flag to delete all configurations for a given entity","closed","","mimaison","2017-09-04T16:35:03Z","2018-04-18T13:31:23Z"
"","3860","KAFKA-5894: add the notion of max inflight requests to async ZooKeeperClient","ZooKeeperClient is a zookeeper client that encourages pipelined requests to zookeeper. We want to add the notion of max inflight requests to the client for several reasons: 1. to bound memory overhead associated with async requests on the client. 2. to not overwhelm the zookeeper ensemble with a burst of requests.","closed","","onurkaraman","2017-09-14T17:48:29Z","2017-11-07T00:50:30Z"
"","4004","KAFKA-6003: Accept appends on replicas unconditionally when local producer state doesn't exist","Without this patch, if the replica's log was somehow truncated before the leader's, it is possible for the replica fetcher thread to continuously throw an OutOfOrderSequenceException because the incoming sequence would be non-zero and there is no local state.  This patch changes the behavior so that the replica state is updated to the leader's state if there was no local state for the producer at the time of the append.","closed","","apurvam","2017-10-02T22:44:18Z","2017-10-05T20:03:45Z"
"","4407","KAFKA-6287: Consumer group command should list simple consumer groups","With this patch, simple consumer groups which only use Kafka for offset storage will be viewable using the `--list` option. In addition, this patch fixes a bug in the offset loading logic which caused us to lose the protocol type of empty groups on coordinator failover. I also did some cleanup of the various consumer group command test cases.  For testing, I have added new integration tests which cover listing and describing simple consumer groups. I also added unit tests to cover loading empty groups with assertions on the protocol type.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-01-09T22:30:48Z","2018-01-24T02:58:50Z"
"","4029","KAFKA-6016: Make the reassign partitions system test use the idempotent producer","With these changes, we are ensuring that the partitions being reassigned are from non-zero offsets. We also ensure that every message in the log has producerId and sequence number.   This means that it successfully reproduces https://issues.apache.org/jira/browse/KAFKA-6003, as can be seen below:  ```  [2017-10-05 20:57:00,466] ERROR [ReplicaFetcher replicaId=1, leaderId=4, fetcherId=0] Error due to (kafka.server.ReplicaFetcherThread) kafka.common.KafkaException: Error processing data for partition test_topic-16 offset 682         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:203)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1$$anonfun$apply$2.apply(AbstractFetcherThread.scala:171)         at scala.Option.foreach(Option.scala:257)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:171)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2$$anonfun$apply$mcV$sp$1.apply(AbstractFetcherThread.scala:168)         at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply$mcV$sp(AbstractFetcherThread.scala:168)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:168)         at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$2.apply(AbstractFetcherThread.scala:168)         at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:218)         at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:166)         at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:109)         at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64) Caused by: org.apache.kafka.common.errors.UnknownProducerIdException: Found no record of producerId=1000 on the broker. It is possible that the last message with the producerId=1000 has been removed due to hitting the retention limit. ```","closed","","apurvam","2017-10-05T21:18:41Z","2017-10-10T17:40:55Z"
"","3703","KAFKA-5755: KafkaProducer should be refactored to use LogContext","With LogContext, each producer log item is automatically prefixed with client id and transactional id. @hachikuji  Since a new parameter `logContext` is passed into `Sender` constructor which breaks the max parameter count in checkstyle, I also increased it to 14 now.  And since all the static methods where `log` is used are private in KafkaProducer, I did not check the nullness for it in those methods.","closed","","huxihx","2017-08-21T03:02:59Z","2017-08-28T18:08:35Z"
"","4462","MINOR: Update consumer group command documentation with newly supported options","With KIP-175 released as of 1.0.0 there are a number of consumer group command options that can be used to describe a consumer group. This PR updates the documentation on consumer group command to mention those options.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-01-23T23:07:10Z","2018-01-25T17:25:30Z"
"","3611","MINOR: Added a couple of unit tests for KStreamPrint node when values are bytes","With current tests, the deserialization inside the KStreamPrint node processor which happens when key and/or values are byte[] isn't tested. This PR fixes that.","closed","","ppatierno","2017-08-02T15:18:06Z","2017-08-04T11:55:25Z"
"","4241","KAFKA-4893: Fix conflict between async topic deletion and max topic length","With async topic deletion the topic partition folder name is affixed with a '.', a UUID, and '-delete'. If topic name length is close to its current limit (249) this could cause an issue because the folder name size goes over 255. This PR implements the [suggestion solution](https://issues.apache.org/jira/browse/KAFKA-4893?focusedCommentId=15927155) by @onurkaraman in the JIRA. This implementation is automatically backward compatible, and cleans up any folder marked for deletion using the old method (affixing the folder name).","open","","vahidhashemian","2017-11-21T00:39:42Z","2018-07-12T21:27:15Z"
"","4173","KAFKA-6156: Metric tag name should not contain colons.","Windows directory paths often contain colons which are now allowed in yammer metrics. Should convert to its corresponding Unix style path before creating metrics.","closed","","huxihx","2017-11-03T06:17:25Z","2017-11-09T09:55:25Z"
"","4247","KAFKA-6250: Use existing Kafka Connect internal topics without requiring ACL","When using Kafka Connect with a cluster that doesn't allow the user to create topics (due to ACL configuration), Connect fails when trying to create its internal topics, even if these topics already exist. This is incorrect behavior according to the documentation, which mentions that R/W access should be enough.  This happens specifically when using Aiven Kafka, which does not permit creation of topics via the Kafka Admin Client API.  The patch ignores the returned error, similar to the behavior for older brokers that don't support the API.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","gavrie","2017-11-22T07:03:05Z","2020-10-16T06:24:12Z"
"","4343","KAFKA-6383: complete shutdown for CREATED StreamThreads","When transitioning StreamThread from CREATED to PENDING_SHUTDOWN free up resources from the caller, rather than the stream thread, since in this case the stream thread was never actually started.  In KakfaStreams.close, shut down the streams threads from the close thread. StreamThread.shutdown may now block, so call this from the close thread so that the timeout is honored.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2017-12-19T18:49:08Z","2018-01-02T17:32:29Z"
"","3961","KAFKA-5976: RequestChannel.sendResponse should record correct size for NetworkSend.","When TRACE logging is enabled, RequestChannel.sendResponse records incorrect size for `Send` due to the fact that `response.responseSend` is of scala.Option type.","closed","","huxihx","2017-09-26T09:12:27Z","2017-10-09T03:46:31Z"
"","3533","KAFKA-5596: TopicCommand methods fail to handle topic name with "".""","When there is one topic which name includes `.`, then we invoke `list`, `describe`, `alter`, `delete` functions and set this topic name, those functions will not only operate this topic, but also operate all other topics that have same name regex pattern. For example, there are two topics on Kafka cluster: - test.a - test_a  If you run command `--describe --topic test.a`, it will return both of `test.a` and `test_a`. Other functions have the same effect.","closed","","sh-z","2017-07-15T17:39:40Z","2017-07-17T07:30:11Z"
"","3532","KAFKA-5596: TopicCommand methods fail to handle topic name with "".""","When there is one topic which name includes `.`, then we invoke `list`, `describe`, `alter`, `delete` functions and set this topic name, those functions will not only operate this topic, but also operate all other topics that have same name regex pattern. For example, there are two topics on Kafka cluster: - test.a - test_a  If you run command `--describe --topic test.a`, it will return both of `test.a` and `test_a`. Other functions have the same effect.","closed","","sh-z","2017-07-15T16:37:15Z","2017-07-15T17:31:11Z"
"","3530","KAFKA-5595: Ensure client connection ids are not reused too quickly","When there are broker delays that cause a response to take longer than `connections.max.idle.ms`, connections may be closed by the broker (as well as by the client) before the response is processed. If the port is reused, broker may send the outstanding response to a new connection with the reused port. The new connection will end up with correlation id mismatch, requiring process restart. This is also a security exposure since clients receive response intended for the wrong connection.","closed","","rajinisivaram","2017-07-15T13:36:24Z","2017-08-14T16:17:59Z"
"","4356","KAFKA-4335: Add batch.size to FileStreamSource connector to prevent OOM","When the source file of `FileStreamSource` is a large file, `FileStreamSourceTask.poll()` will result in OOM. This pull request added `batch.size` parameter which can restrict the poll size.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","phstudy","2017-12-25T08:39:45Z","2020-10-16T06:33:47Z"
"","4349","KAFKA-6366: Fix stack overflow in consumer due to many offset commits during coordinator disconnect","When the coordinator is marked unknown, we explicitly disconnect its connection and cancel pending requests. Currently the disconnect happens before the coordinator state is set to null, which means that callbacks which inspect the coordinator state will see it still as active. If there are offset commit requests which need to be cancelled, each request callback will inspect the coordinator state and attempt to mark the coordinator dead again. In the worst case, if there are many pending offset commit requests that need to be cancelled, this can cause a stack overflow. To fix the problem, we need to set the coordinator to null prior to cancelling pending requests.  I have added a test case which reproduced the stack overflow with many pending offset commits. I have also added a basic test case to verify that callbacks for in-flight or unsent requests see the coordinator as unknown which prevents them from attempting to resend.  Note that I have also included some minor cleanups which I noticed along the way.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-12-21T02:05:21Z","2018-01-20T21:28:25Z"
"","4208","MINOR: Not enough replica exception should never happen for delete records","When reviewing https://github.com/apache/kafka/pull/4132, I felt that NOT_ENOUGH_REPLICAS should never happen actually. Hence proposing to remove it from the listed error code as well in the broker-side capture clause.  Testing added in 4132 should have been sufficient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-11-13T19:02:12Z","2020-04-24T23:54:57Z"
"","4473","[KAFKA-6482] when produce send a invalidity timestamps, broker will be not delete …","when produce send a invalidity timestamps, broker will be not delete files forever...   https://issues.apache.org/jira/browse/KAFKA-6482","open","","leonhong","2018-01-25T08:56:42Z","2018-03-02T19:31:08Z"
"","3983","KAFKA-5986: Streams State Restoration never completes when logging is disabled","When logging is disabled and there are state stores the task never transitions from restoring to running. This is because we only ever check if the task has state stores and return false on initialization if it does. The check should be if we have changelog partitions, i.e., we need to restore.","closed","","dguy","2017-09-28T12:13:56Z","2017-09-29T14:26:16Z"
"","3955","KAFKA-5969: Use correct error message when the JSON file is invalid","When invalid JSON file is passed to the bin/kafka-preferred-replica-election.sh / PreferredReplicaLeaderElectionCommand tool it gives a misleading error `Preferred replica election data is empty`. This PR replaces it with the correct error message.","open","","scholzj","2017-09-25T09:13:31Z","2018-03-02T19:30:46Z"
"","3586","KAFKA-5665: Heartbeat thread should use correct interruption method to restore status","When interrupting the background heartbeat thread, `Thread.interrupted();` is used. Actually, `Thread.currentThread().interrupt();` should be used to restore the interruption status. An alternative way to solve is to remove `Thread.interrupted();` since HeartbeatThread extends Thread and all code higher up on the call stack is controlled, so we could safely swallow this exception. Anyway, `Thread.interrupted();` should not be used here. It's a test method not an action.","closed","","huxihx","2017-07-27T03:54:41Z","2017-08-04T03:27:43Z"
"","3526","KAFKA-5587: Remove channel only after staged receives are delivered","When idle connections are closed, ensure that channels with staged receives are retained in `closingChannels` until all staged receives are completed. Also ensure that only one staged receive is completed in each poll, even when channels are closed.","closed","","rajinisivaram","2017-07-14T09:55:39Z","2017-07-18T09:38:17Z"
"","4387","KAFKA-6422 Mirror maker will throw null pointer exception when the message value is null","when enable trace level log in mirror maker, if the message value is null. it will throw null pointer exception and shutdown the mirror maker.","closed","","lisa2lisa","2018-01-04T09:37:58Z","2018-01-09T07:39:59Z"
"","4386","MINOR mirror make will throw null pointer exception when value is null and shutdown","when enable trace level log for mirror maker, when the message value is null, it will throw null pointer exceptions. and the mirror maker will shut down because of that.","closed","","lisa2lisa","2018-01-04T09:32:29Z","2018-01-04T17:38:49Z"
"","4325","Check null message","When enable the trace level log in mirror maker,  the message could contain null value, and this will throw null pointer exception.  *Summary of testing strategy (including rationale) 1. Create message contain null value and normal message 2. Passing throw testing case 3. and make sure don't not throw any thing","closed","","lisa2lisa","2017-12-14T15:35:44Z","2018-01-04T17:35:17Z"
"","3897","KAFKA-5929: Save pre-assignment to file to avoid too long text to display when do topic partition reassign","When do partition reassign - before pr Pre-assignment will be printed directly. It is not friendly when the text is too long.  - after pr Pre-assignment will still be printed directly, but will be save to a file at the same time, naming with suffix "".rollback"" of ""reassignment-json-file"". For example:  ``` ./kafka-reassign-partitions.sh --reassignment-json-file test.json ... ``` then we may get a file **test.json.rollback**","closed","","uncleGen","2017-09-19T07:41:20Z","2019-04-23T06:00:43Z"
"","4248","KAFKA-6258; SSLTransportLayer should keep reading from socket until either the buffer is full or the socket has no more data","When consumer uses plaintext and there is remaining data in consumer's buffer, consumer.poll() will read all data available from the socket buffer to consumer buffer. However, if consumer uses ssl and there is remaining data, consumer.poll() may only read 16 KB (the size of SslTransportLayer.appReadBuffer) from socket buffer. This will reduce efficient of consumer.poll() by asking user to call more poll() to get the same amount of data.  Furthermore, we observe that for users who naively sleep a constant time after each consumer.poll(), some partition will lag behind after they switch from plaintext to ssl. Here is the explanation why this can happen.  Say there are 1 partition of 1MB/sec and 9 partition of 32KB/sec. Leaders of these partitions are all different and consumer is consuming these 10 partitions. Let's also assume that socket read buffer size is large enough and consume sleeps 1 sec between consumer.poll(). 1 sec is long enough for consumer to receive the FetchResponse back from broker.  When consumer uses plaintext, each consumer.poll() will read all data from the socket buffer and it means 1 MB data is read from each partition.  When consumer uses ssl, each consumer.poll() is likely to find that there is some data available in the memory. In this case consumer only reads 16 KB data from other sockets, particularly the socket for the broker with the large partition. Then the throughput of the large partition will be limited to 16KB/sec.  Arguably user should not sleep 1 sec if its consumer is lagging behind. But on Kafka dev side it is nice to keep the previous behavior and optimize consumer.poll() to read as much data from socket as possible.","closed","","lindong28","2017-11-22T07:14:22Z","2018-03-14T19:14:53Z"
"","3866","KAFKA-5874: TopicCommand should check at least one parameter is given...","When altering topics, TopicCommand should ensure that at least one of parameters in `partitions`, `config` or `delete-config` must be specified.","open","","huxihx","2017-09-15T02:16:16Z","2018-03-02T19:30:40Z"
"","4276","KAFKA-6260: Ensure selection keys are removed from all collections on socket close","When a socket is closed, we must remove corresponding selection keys from internal collections. This fixes an NPE which is caused by attempting to access the selection key's attached channel after it had been cleared after disconnecting.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-11-29T19:17:10Z","2017-12-01T19:36:15Z"
"","4235","KAFKA-6207 : Include start of record when RecordIsTooLarge","When a message is too large to be sent (at org.apache.kafka.clients.producer.KafkaProducer#doSend), the RecordTooLargeException should carry the start of the record (for example, the first 1KB) so that the calling application can debug which message caused the error. To resolve this, added functionality to log the first 1Kb of message, if RecordIsTooLarge exception is thrown.  Thanks, Sumit Jawale  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","samjawale","2017-11-19T10:00:35Z","2017-11-19T10:26:32Z"
"","4234","KAFKA-6207 : Include start of record when RecordIsTooLarge","When a message is too large to be sent (at org.apache.kafka.clients.producer.KafkaProducer#doSend), the RecordTooLargeException should carry the start of the record (for example, the first 1KB) so that the calling application can debug which message caused the error.  To resolve this, I created a new function which will get the first 1KB of the message (if the RecordTooLargeException is occurring), and the same will be logged...  Thanks, Sumit Jawale  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","samjawale","2017-11-19T07:32:10Z","2017-11-19T11:17:22Z"
"","4237","KAFKA-6207 : Include start of record when RecordIsTooLarge","When a message is too large to be sent (at org.apache.kafka.clients.producer.KafkaProducer#doSend), the RecordTooLargeException should carry the start of the record (for example, the first 1KB) so that the calling application can debug which message caused the error.  To resolve this, added functionality to log the first 1Kb of message, if RecordIsTooLarge exception is thrown.  Thanks, Sumit Jawale  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","samjawale","2017-11-19T21:22:12Z","2018-03-02T19:30:57Z"
"","4375","KAFKA-6415: Use WARN log level for Metadata in system test","When a log entry is appended to a Kafka topic using `KafkaLog4jAppender`, the producer.send operation may block waiting for metadata. This can result in deadlocks in a couple of scenarios if a log entry from the producer network thread is also at a log level that results in the entry being appended to a Kafka topic. 1. Producer's network thread will attempt to send data to a Kafka topic and this is unsafe since producer.send may block waiting for metadata, causing a deadlock since the thread will not process the metadata request/response. 2. `KafkaLog4jAppender#append` is invoked while holding the lock of the logger. So the thread waiting for metadata in the initial send will be holding the logger lock. If the producer network thread has.a log entry that needs to be appended, it will attempt to acquire the logger lock and deadlock.  This is a temporary workaround to avoid deadlocks in system tests by setting log level to WARN for `Metadata` in `VerifiableLog4jAppender`. The fix has been verified using the system tests log4j_appender_test.py which started failing when the info-level log entry was introduced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-02T15:15:06Z","2018-01-03T19:19:42Z"
"","3641","KAFKA-5704 Corrected Connect distributed startup behavior to allow older brokers to auto-create topics","When a Connect distributed worker starts up talking with broker versions 0.10.1.0 and later, it will use the AdminClient to look for the internal topics and attempt to create them if they are missing. Although the AdminClient was added in 0.11.0.0, the AdminClient uses APIs to create topics that existed in 0.10.1.0 and later. This feature works as expected when Connect uses a broker version 0.10.1.0 or later.  However, when a Connect distributed worker starts up using a broker older than 0.10.1.0, the AdminClient is not able to find the required APIs and thus will throw an UnsupportedVersionException. Unfortunately, this exception is not caught and instead causes the Connect worker to fail even when the topics already exist.  This change handles the UnsupportedVersionException by logging a debug message and doing nothing. The existing producer logic will get information about the topics, which will cause the broker to create them if they don’t exist and broker auto-creation of topics is enabled. This is the same behavior that existed prior to 0.11.0.0, and so this change restores that behavior for brokers older than 0.10.1.0.  This change also adds a system test that verifies Connect works with a variety of brokers and is able to run source and sink connectors. The test verifies that Connect can read from the internal topics when the connectors are restarted.","closed","connect,","rhauch","2017-08-07T19:35:09Z","2020-10-16T06:29:10Z"
"","3566","KAFKA-5629: Changed behavior of ConsoleConsumer when auto.offset.reset parameter is specified on the command line","when ""auto.offset.reset"" property is specified on the command line but overridden by the code during startup. Currently the ConsoleConsumer silently overrides that setting, which can create confusing behavior.","closed","","soenkeliebau","2017-07-24T11:40:05Z","2017-08-09T20:27:42Z"
"","4366","Add note about segment.ms to log cleaner docs","We were bitten by missing the essential `segment.ms` configuration option when using log compaction. Add a note about that to the ""Configuring The Log Cleaner"" section of the documentation.","closed","","thijsc","2017-12-29T06:40:12Z","2019-02-05T13:29:19Z"
"","4348","MINOR: Fix race condition in Streams EOS system test","We should start the process only within the `with` block, otherwise the bytes parameter would cause a race condition that result in false alarms of system test failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-12-21T01:58:08Z","2020-04-24T23:47:11Z"
"","3878","KAFKA-5913: Add hasOffset() and hasTimestamp() methods to RecordMetadata","We return this exception from `RecordMetadata.offset()` or `RecordMetadata.timestamp()` if these pieces of metadata were not returned by the broker.   This will happen, for instance, when the broker returns a `DuplicateSequenceException`.","closed","","apurvam","2017-09-16T01:17:28Z","2017-09-21T21:41:29Z"
"","4143","MINOR: PartitionReassignmentHandler should only generate event when znode is created","We only need to generate the event when the znode is created or deleted. In the former case, we start the reassignment while in the latter we re-register the watcher (necessary for the Controller to detect future reassignments).  During Controller failover, we restart the reassignment without generating an event so it's not affected by this change.  Also use the Controller cache (`ControllerContext.partitionsBeingReassigned`) in `removePartitionFromReassignedPartitions` instead of reloading the data from ZooKeeper.  Overall, we would previously load the reassignment data from ZooKeeper twice per completed partition whereas now as don't do it at all. As an example, say there were 30k partitions being reassigned, these changes save the allocation of 900 million `TopicAndPartition` and `Seq[Int]` (replicas) instances (could easily amount to 20-40 GB depending on the topic name length). This matters most in cases where the partitions being reassigned don't have much data allowing the reassignment to complete reasonably fast for many of the partitions.","closed","","ijuma","2017-10-27T10:33:45Z","2017-12-22T18:21:57Z"
"","3894","KAFKA-5928: Avoid redundant requests to zookeeper when reassign topic partition","We mistakenly request topic level information according to partitions config in the assignment json file. For example https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala#L550: ``` val validPartitions = proposedPartitionAssignment.filter { case (p, _) => validatePartition(zkUtils, p.topic, p.partition) }  ``` If reassign 1000 partitions (in 10 topics), we need to request zookeeper 1000 times here. But actually we only need to request just 10 (topics) times. We test a large-scale assignment, about 10K partitions. It takes tens of minutes. After optimization, it will reduce to less than 1minute.","closed","","uncleGen","2017-09-19T03:06:32Z","2018-08-07T03:06:59Z"
"","3954","KAFKA-5758: Don't fail fetch request if replica is no longer a follower for a partition","We log a warning instead, which is what we also do if the partition hasn't been created yet.  A few other improvements: - Return updated high watermark if fetch is returned immediately. This seems to be more intuitive and is consistent with the case where the fetch request is served from the purgatory. - Centralise offline partition handling - Remove unnecessary `tryCompleteDelayedProduce` that would already have been done by the called method - A few other minor clean-ups","closed","","ijuma","2017-09-25T07:34:17Z","2017-12-22T18:24:26Z"
"","4189","KAFKA-6146: minimize the number of triggers enqueuing PreferredReplicaLeaderElection events","We currently enqueue a PreferredReplicaLeaderElection controller event in PreferredReplicaElectionHandler's handleCreation, handleDeletion, and handleDataChange. We can just enqueue the event upon znode creation and after preferred replica leader election completes. The processing of this latter enqueue will register the exist watch on PreferredReplicaElectionZNode and perform any pending preferred replica leader election that may have occurred between completion and registration.","closed","","onurkaraman","2017-11-07T21:44:48Z","2017-11-09T00:58:50Z"
"","3725","HOTFIX: reduce streams benchmark input records to 10 million","We are occasionally hitting some timeouts due to processing not finishing. So rather than failing the build for these reasons it would be better to reduce the runtime.","closed","","dguy","2017-08-23T16:24:32Z","2017-08-23T19:54:29Z"
"","4397","KAFKA-6252: Close the metric group to clean up any existing metrics","We are closing the metricGroups created in a Worker, Source task and Sink task before populating them with new metrics. This helps in cases where an Exception is thrown when previously created groups were not cleaned up correctly.   Signed-off-by: Arjun Satish   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2018-01-05T22:56:33Z","2020-10-16T06:05:14Z"
"","4398","KAFKA-6428: Generate findbugs output for CI and fail builds for 'high' level warnings","We already had findbugs running and it looks like sufficient warnings should cause errors. This PR does a few things. First, it changes to generating xml reports (which CI likes) by default. We already seem to have the Jenkins plugins setup to consume these, so we should immediately start seeing the output on Jenkins. This seems better than the current html default since most devs probably aren't looking at the html reports unless they are specifically looking at findbugs issues. Second, it explicitly sets the report level we want to trigger failures on.  I think we were already failing the build based on the current settings, we just didn't have any high-level warnings. But this sets us up not to only fail the builds but also have some visibility via jenkins reports.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-01-06T03:47:39Z","2018-01-08T21:57:21Z"
"","3764","view diff","view diff","closed","","chuailiwu","2017-08-31T07:08:19Z","2017-12-22T01:48:33Z"
"","3919","[MINOR] Where possible, introduce EnumMap.","Very simple PR, introduce EnumMap in 2 places of the code.  EnumMap: https://docs.oracle.com/javase/7/docs/api/java/util/EnumMap.html  EnumMap is meant to be used when the key of a Map is an Enum.  From the documentation:   > Enum maps are represented internally as arrays. This representation is extremely compact and efficient.  Should use less memory per instance and also be faster in lookup.","closed","","KoenDG","2017-09-20T08:55:07Z","2017-12-29T12:39:18Z"
"","3997","Applying AC transformation using Rascal","Using Rascal Software retrieved from https://github.com/refactoring-towards-language-evolution/rascal-Java8. AC transformation was applied and the result was the following log:  - Number of files:  1520 - Processed Files: 11 - Exported Files:   10 - Total of files changed: 13 - Total of transformations: 14 - Errors: 3","open","","jmarcelonunes","2017-09-30T00:52:35Z","2018-03-02T19:30:49Z"
"","4246","MINOR: improve flaky Streams tests","Use TestUtil test directory for state directory instead of default /tmp/kafka-streams  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-22T01:24:30Z","2017-11-22T22:06:30Z"
"","4058","MINOR: Fixed format string","Use Scala string templates instead of format","closed","","mimaison","2017-10-11T17:10:51Z","2018-04-18T13:27:50Z"
"","3869","MINOR: Make the state change log more consistent","Use logIdent to achieve this.  Also fixed an issue where we were logging about replicas going offline with an empty set of replicas (i.e. no replicas had gone offline so no need to log).","closed","","ijuma","2017-09-15T10:50:49Z","2017-12-22T18:22:56Z"
"","3516","KAFKA-5562: execute state dir cleanup on single thread","Use a single `StateDirectory` per streams instance. Use threadId to determine which thread owns the lock. Only allow the owning thread to unlock. Execute cleanup on a scheduled thread in `KafkaStreams`","closed","","dguy","2017-07-10T17:04:07Z","2017-08-16T13:23:04Z"
"","4268","KAFKA-6274: Use topic plus dash as prefix of auto-generated state store names","Use `topic-` as the prefix of the auto-generated state store name.  Also add a unit test for this functionality.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-11-27T18:51:50Z","2018-02-14T19:52:00Z"
"","3609","MINOR: Make the state change log more consistent","Use `logIdent` to achieve this.  Also fixed an issue where we were logging about replicas going offline with an empty set of replicas (i.e. no replicas had gone offline so no need to log).","closed","","ijuma","2017-08-02T10:45:01Z","2017-09-17T15:20:17Z"
"","4345","KAFKA-6390: Update ZooKeeper to 3.4.11, Gradle and other minor updates","Updates: - Gradle, gradle plugins and maven artifact updated - Bug fix updates for ZooKeeper, Jackson, EasyMock and Snappy  Not updated: - RocksDB as it often causes issues, so better done separately - args4j as our test coverage is weak and the update was a feature release  Also fixed scala-reflect version to match scala-library.  Release notes for ZooKeeper 3.4.11: https://zookeeper.apache.org/doc/r3.4.11/releasenotes.html  A notable fix is improved handling of UnknownHostException: https://issues.apache.org/jira/browse/ZOOKEEPER-2614  Manually tested that IntelliJ import and build still works. Relying on existing test suite otherwise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-12-20T10:31:23Z","2018-01-02T19:52:14Z"
"","3791","MINOR: Update dependencies for 1.0.0 release","Updates dependencies to their latest versions.","closed","","andrasbeni","2017-09-05T14:43:28Z","2017-09-08T13:49:32Z"
"","4286","MINOR: fix broker compatibility tests","Updated the System test `stream_broker_compatibility_test.py` to address system test failures as we have removed explicit broker version checking  - Ignore the `0.8.2.2` and `0.9.0.0` tests because the `NetworkClient` only logs `UnsupportedVersionException`s that occur and will continue to retry connecting.  Once issue https://issues.apache.org/jira/browse/KAFKA-6297 is addressed, we may re-enable these tests. - Updated existing tests expected error messages - Updated Streams code in test for to make sure we fail fast for incompatible brokers  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2017-12-01T21:19:19Z","2017-12-03T17:07:30Z"
"","3896","KAFKA-5914: add message format version and message max bytes to metadata response","Updated the `TopicResponse` part of the `MetadataResponse` to include the message format version and the message max bytes.  One problem here is that we use the `TopicConfigHandler` to listen for topic changes. However this is not invoked for topic _creates_ since the change notification path is not updated during creates. I am not sure what the right solution is here. Intuitively, we should update the the change notification path for topic creates, but not sure if that has compatibility (or other) implications.  TODO: 1. Add a more complete integration test where the client sends a real `MetadataRequest` and receives the proper `MetadataResponse`. 2. Update the code to pull the global topic config during startup. The present patch only contains configs for the topics present on each broker. 3. Update `AdminUtils.createTopic` to update the change notification path so that each broker gets notified of topic creations and can update its topic config cache.","open","","apurvam","2017-09-19T06:09:54Z","2018-03-02T19:30:42Z"
"","4028","Update docs to reflect kafka trademark status","Updated a couple places in docs with the 'registered' trademark symbol.","closed","","derrickdoo","2017-10-05T20:12:48Z","2017-10-05T20:33:01Z"
"","3633","MINOR: streams memory management docs","update streams memory management docs","closed","","dguy","2017-08-07T09:25:07Z","2017-08-16T13:22:58Z"
"","4409","Merge pull request #1 from apache/trunk","update  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","terry19850829","2018-01-10T08:42:48Z","2018-01-10T08:46:02Z"
"","3712","MINOR: Fix transient failure in SocketServerTest.testConnectionIdReuse","Two requests sent together may not always trigger a staged receive since the requests may not be received in a single poll and the channel is muted when receives are complete. Hence attempt to stage multiple times until a receive is staged to make the test more stable.","closed","","rajinisivaram","2017-08-22T14:54:11Z","2017-08-25T05:16:35Z"
"","3926","KAFKA-5934 Cross build for Scala 2.12","Tried to do some simple refactorings to publish Kafka 0.8.x for Scala 2.12 as it's still used quite a lot in the industry. Let me know if more needs to be done, I don't use gradle day to day plus there probably was a change in gradle 4 which required a change to  build.gradle and I bumped the scala modules from 1.0.2 to 1.0.6.","closed","","schrepfler","2017-09-20T21:25:34Z","2017-12-22T01:54:35Z"
"","3632","KAFKA-5707: TopicCommand and ConfigCommand should remove useless `--force` option.","TopicCommand and ConfigCommand should remove useless `--force` option.","closed","","huxihx","2017-08-07T01:42:09Z","2017-08-07T08:37:51Z"
"","3969","KAFKA-5888: System test to check ordering of messages with transactions and max.in.flight > 1","To check ordering, we augment the existing transactions test to read and write from topics with one partition. Since we are writing monotonically increasing numbers, the topics should always be sorted, making it very easy to check for out of order messages.","closed","","apurvam","2017-09-27T04:24:57Z","2017-09-28T20:05:15Z"
"","3780","MINOR: Updated KafkaConsumer.commitSync() javadoc","Throws IllegalArgumentException is the offset is negative","closed","","mimaison","2017-09-03T17:41:49Z","2018-04-18T13:27:52Z"
"","3920","KAFKA-5940: Throw exception if the JSON file is invalid","Throw an exception when the JSON file with topics/partitions and offsets is incorrect (= when Json.parseFull(...) returns None).","closed","","scholzj","2017-09-20T11:02:40Z","2017-09-29T12:42:28Z"
"","4049","[KAFKA-5212] Consumer ListOffsets request can starve group heartbeats","Through the identification of the poll method in ConsumerCoordinator as the place where the heartbeat is sent, I modified the metadata so that it polled periodically every interval for a heartbeat.","closed","","ConcurrencyPractitioner","2017-10-10T03:47:35Z","2018-01-09T23:57:10Z"
"","3999","MINOR: Add attributes `processedKeys` and `processedValues` to MockProcessorSupplier","This would allow for easier testing of topologies using the following pattern: ```Scala // in Scala val builder = new KStreamBuilder val stream: KStream[K, V] = builder.stream(KSerde, VSerde, topic)  val processedStream: KStream[K, VR] = createTopology(stream, builder)  val processorSupplier = new MyMockProcessorSupplier[K, VR] processedStream.process(processorSupplier)  val streamDriver = new MyKStreamTestDriver(builder, TestUtils.tempDirectory()) streamDriver.setTime(0L)  streamDriver.process(topic, somethingK, somethingV) streamDriver.flushState()  val results = (processorSupplier.processedKeys zip processorSupplier.processedValues).toMap results(expectedK) should be(expectedVR) ``` Without breaking any existing tests that rely on the `processed` `ArrayList`. Of course it's not as elegant as rewriting the logic here, as we're (almost) duplicating the information in the `processed` array.  @guozhangwang any thoughts?","closed","streams,","mewwts","2017-10-02T07:17:08Z","2018-02-08T00:33:28Z"
"","3877","MINOR: Disable KafkaAdminClientTest.testHandleTimeout","This test is super flaky in the PR builder. https://issues.apache.org/jira/browse/KAFKA-5792 tracks the fix.","closed","","apurvam","2017-09-15T20:10:39Z","2017-09-15T21:14:34Z"
"","4184","MINOR: Remove FanoutIntegrationTest.java","This test has been completely subsumed by the coverage of reset integration test, and hence can be removed.","closed","","guozhangwang","2017-11-06T22:41:09Z","2020-04-24T23:38:58Z"
"","3543","MINOR: Remove flaky assertion in verifyCloseOldestConnectionWithStagedReceives","This seems to fail a lot in Jenkins although it always passes locally for me. Removing the assertion to restore Jenkins stability while we investigate in more detail.","closed","","ijuma","2017-07-18T07:20:08Z","2017-08-22T06:39:20Z"
"","4180","Update  kafka-server-stop.sh","This script does not work on Kafka1.0. Kafka1.0 work well to use this script to stop the process.","open","","pengkeboy","2017-11-06T07:26:02Z","2018-03-02T19:30:55Z"
"","4378","Revert ""KAFKA-6383: complete shutdown for CREATED StreamThreads (#4343)""","This reverts commit 47db063c310cf47e4c544196acab2abfe62977b0.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2018-01-02T21:27:15Z","2018-01-02T22:24:07Z"
"","4224","KAFKA-6170; KIP-220 Part 2: Break dependency of Assignor on StreamThread","This refactoring is discussed in https://github.com/apache/kafka/pull/3624#discussion_r132614639. More specifically:  1. Moved the access of `StreamThread` in `StreamPartitionAssignor` to `TaskManager`, removed any fields stored in `StreamThread` such as `processId` and `clientId` that are only to be used in `StreamPartitionAssignor`, and pass them to `TaskManager` if necessary. 2. Moved any in-memory states, `metadataWithInternalTopics`, `partitionsByHostState`, `standbyTasks`, `activeTasks` to `TaskManager` so that `StreamPartitionAssignor` becomes a stateless thin layer that access TaskManager directly. 3. Remove the reference of `StreamPartitionAssignor` in `StreamThread`, instead consolidate all related functionalities such as `cachedTasksIds ` in `TaskManager` which could be retrieved by the `StreamThread` and the `StreamPartitionAssignor` directly. 4. Finally, removed the two interfaces used for `StreamThread` and `StreamPartitionAssignor`.  5. Some minor fixes on logPrefixes, etc.  Future work: when replacing the StreamsKafkaClient, we would let `StreamPartitionAssignor` to retrieve it from `TaskManager` directly, and also its closing call do not need to be called (`KafkaStreams` will be responsible for closing it).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-11-16T00:16:42Z","2018-02-14T19:51:54Z"
"","3813","MINOR: Move request/response schemas to the corresponding object representation","This refactor achieves the following:  1. Breaks up the increasingly unmanageable `Protocol` class and moves schemas closer to their actual usage. 2. Removes the need for redundant field identifiers maintained separately in `Protocol` and the respective request/response objects. 3. Provides a better mechanism for sharing common fields between different schemas (e.g. topics, partitions, error codes, etc.). 4. Adds convenience helpers to `Struct` for common patterns (such as setting a field only if it exists).","closed","","hachikuji","2017-09-08T03:19:08Z","2017-09-19T04:15:59Z"
"","3925","KAFKA-5911 Avoid creation of extra Map for futures in KafkaAdminClient","This reduces creation of garbage while keeping thread safety untouched.","open","","tedyu","2017-09-20T21:10:10Z","2017-10-04T14:01:44Z"
"","4354","KAFKA-6386:use Properties instead of StreamsConfig in KafkaStreams constructor","This pull request targets https://issues.apache.org/jira/browse/KAFKA-6386 The minor fix to deprecate usage of `StreamsConfig` in favor of `java.util.Properties`. I created separate public constructors using `Properties` in order to replace the old ones,  and prioritize new functions in the `KafkaStreams.java` file.  Since this is my first time doing open source contribution, I'm very happy to get any comment or pointer to be more professional and get better next time, thank you Guozhang @guozhangwang and Liquan @Ishiihara!  testing strategy: existing unit test should be suffice to cover this change.","closed","streams,","abbccdda","2017-12-23T12:38:12Z","2018-03-28T03:21:14Z"
"","3995","KAFKA-5514 KafkaConsumer ignores default values in Properties object because of incorrect use of Properties object.","This problem plagues anywhere Properties are being misused. I have this issue with KafkaProducer(Properties properties) constructor. I bet this problem spans across more classes just like this KafkaConsumer(Properties properties).  Clarification:  putAll is implemented in Hashtable.java and at that point we are already too late. When we cast to Map/Hashtable we lose  Properties's 'defaults member which is another Properties object. Whenever you use Properties's getProperty() method, if the Property object does not explicity contain the property, it will refer to it's internal defaults Properties object which can be set as so: Properties myProps = new Properties(oldProperties) if oldProperties has a key:value of foo:bar calling myProps.get(foo) will not return bar whereas, calling myProps.getProperty(foo) will return bar  Reference:   ``` public class Properties extends Hashtable { ...     /**      * A property list that contains default values for any keys not      * found in this property list.      *      * @serial      */     protected Properties defaults;      /**      * Creates an empty property list with the specified defaults.      *      * @param   defaults   the defaults.      */     public Properties(Properties defaults) {         this.defaults = defaults;     } ...     public String getProperty(String key) {         Object oval = super.get(key);         String sval = (oval instanceof String) ? (String)oval : null;         return ((sval == null) && (defaults != null)) ? defaults.getProperty(key) : sval;     } ... ```    Here is the fix:  ```         for (final String name: properties.stringPropertyNames()) {             if (properties.get(name) == null) {                 properties.put(name, properties.getProperty(name));             }         } ```   This pulls all the 'default' inner Properties object's entries up into the outer level Properties object. This should be applied to all the public methods that are being passed in a Properties object See https://github.com/Galus/kafka/commit/8af611dfd9c8f3cb37f1d2b400890525c5a646d3 and https://github.com/Galus/kafka/commit/130b4cde2da4d9ede7aa6d6a99997af32d1a00fc   Similar Issues addressing this issue: KAFKA-2184, KAFKA-3049, KAFKA-3161 @Kamal15   This contribution is my original work and I hereby license it for use by the apache kafka project's open source license.","open","","Galus","2017-09-29T19:48:30Z","2018-03-02T19:30:48Z"
"","3727","KAFKA-5754 : Refactor Streams to use LogContext","This PR utilizes `org.apache.kafka.common.utils.LogContext` for logging in `KafkaStreams`. @hachikuji, @ijuma please review this and let me know your thoughts.","closed","","umesh9794","2017-08-24T08:59:35Z","2017-09-18T08:55:11Z"
"","4259","KAFKA-5631 : Use Jackson for serialising to JSON","This PR replaces the existing `Json.encode` to use Jackson serialization. Since the change is spread more than one module, it relies on the existing tests written for those modules.   If required, I can write new tests as well. Please suggest.","closed","","umesh9794","2017-11-23T07:03:38Z","2017-12-13T03:22:35Z"
"","4216","KAFKA-5859: Avoid retaining AbstractRequest in RequestChannel.Response","This PR removes the need to keep a reference to the parsed `AbstractRequest` after it's been handled in `KafkaApis`.  A reference to `RequestAndSize` which holds the parsed `AbstractRequest` in  `RequestChannel.Request` was kept in scope as a consequence of being passed into the `RequestChannel.Response` after being handled.    The Jira ticket [KAFKA-5859](https://issues.apache.org/jira/browse/KAFKA-5859) suggests removing this reference as soon as it's no longer needed.  I considered several implementations and I settled on creating a new type that contains all the relevant information of the Request that is required after it has been handled.  I think this approach allows for the least amount of invasive changes in the Request/Response lifecycle while retaining the immutability of the `RequestChannel.Request`.  A new type called `RequestChannel.RequestSummary` now contains much of the information that was in `RequestChannel.Request` before.  The `RequestChannel.Request` now generates a `RequestChannel.RequestSummary` that is passed into the `RequestChannel.Response` after being handled in `KafkaApis`.  `RequestChannel.RequestSummary` contains information such as:  * A detailed and non-detailed description of the request * Metrics associated with the request * Helper methods to update various Request metrics * A special case describing whether or not the original Request was a `FetchRequest` and whether it was from a follower.  This information is required in the `updateRequestMetrics` metrics helper method.  This change does not make any behaviour changes so no additional tests were added.  I've verified that all unit and integration tests pass and no regressions were introduced.  I'm interested in seeing the before and after results of the Confluent Kafka system tests as described in step 11 of the [Contributing Code Changes](https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes#ContributingCodeChanges-PullRequest) section.  I would like to request access to kick off this system tests suite if you agree that it's relevant to this ticket.  This is my first contribution to this project.  I picked up this issue because it was marked with the newbie flag and it seemed like a good opportunity to learn more about about the request and response lifecycle in the Kafka broker.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","seglo","2017-11-15T04:42:07Z","2018-03-02T19:30:56Z"
"","4321","KAFKA-6342 : Move workaround for JSON parsing of non-escaped strings","This PR moves the JSON parsing workaround of [this PR](https://github.com/apache/kafka/pull/4303) to new method and uses this method in `ZkClient` etc. classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","core,","umesh9794","2017-12-13T11:27:35Z","2020-04-30T13:44:04Z"
"","4252","Migrate Streams Dev Guide content to AK","This PR migrates content from CP Streams Dev Guide.   Here is the top-level page: ![image](https://user-images.githubusercontent.com/11722533/33904945-df9cf804-df31-11e7-93aa-52385961522c.png)  Here is a child page: ![image](https://user-images.githubusercontent.com/11722533/33904976-f2eafabe-df31-11e7-918c-fbf95db0f76b.png)  See related: https://github.com/apache/kafka-site/pull/112","closed","","joel-hamill","2017-11-22T20:58:24Z","2017-12-21T19:18:18Z"
"","3489","KAFKA-5556 : KafkaConsumer.commitSync throws IllegalStateException: A…","This PR makes `commitOffsetsSync` method check whether future is completed after client's poll or not.   Tests: All existing tests especially ""`testCommitOffsetSyncCallbackWithNonRetriableException`"" is passed. Not sure if we need to add any dedicated tests for this minor change.   Awaiting your review comments.","closed","","umesh9794","2017-07-05T10:52:40Z","2017-07-20T16:08:42Z"
"","4190","MINOR: Fix Javadoc Issues","This PR mainly fixes some broken links and invalid references in the clients Javadoc","closed","","vahidhashemian","2017-11-07T22:05:06Z","2018-01-28T02:08:51Z"
"","3761","KAFKA-5763: Refactor NetworkClient to use LogContext","This PR lets logging client id in every log line in NetworkClient","closed","","adyach","2017-08-30T19:40:27Z","2017-09-11T14:40:18Z"
"","4309","KAFKA-4218: Enable access to key in ValueTransformer and ValueMapper","This PR is the partial implementation for KIP-149. As the discussion for this KIP is still ongoing, I made a PR on the ""safe"" portions of the KIP (so that it can be included in the next release) which are 1) `ValueMapperWithKey`, 2) `ValueTransformerWithKeySupplier`, and 3) `ValueTransformerWithKey`.","closed","","jeyhunkarimov","2017-12-09T01:08:10Z","2018-01-16T18:48:59Z"
"","3864","KAFKA-5899: Added Connect metrics for connectors","This PR is the first of several subtasks for [KAFKA-2376](https://issues.apache.org/jira/browse/KAFKA-2376) to add metrics to Connect worker processes. See that issue and [KIP-196 for details](https://cwiki.apache.org/confluence/display/KAFKA/KIP-196%3A+Add+metrics+to+Kafka+Connect+framework).  This PR adds metrics for each connector using Kafka’s existing `Metrics` framework. This is the first of several changes to add several groups of metrics, this change starts by adding a very simple `ConnectMetrics` object that is owned by each worker and that makes it easy to define multiple groups of metrics, called `ConnectMetricGroup` objects. Each metric group maps to a JMX MBean, and each metric within the group maps to an MBean attribute.  Future PRs will build upon this simple pattern to add metrics for source and sink tasks, workers, and worker rebalances.","closed","connect,","rhauch","2017-09-14T22:45:43Z","2020-10-16T06:05:12Z"
"","3636","KAFKA-5684: KStreamPrintProcessor as customized KStreamPeekProcessor","This PR is intended for having KStreamPrint derived from KStreamPeek and avoiding the ""instance of"" check on byte[ ] every process call.","closed","","ppatierno","2017-08-07T12:58:49Z","2017-08-16T07:10:53Z"
"","4308","catch and log exceptions thrown in waiters added to KafkaFuture","This PR is based on the changes in #4033 so we can hopefully merge them together as part of KAFKA-6018   Exceptions thrown in actions added with KafkaFuture.addWaiter(...) could prevent other waiting actions from executing, possibly breaking KafkaFuture.allOf(), so it seems advisable to wrap actions to catch and log exceptions before making this API public.  cc @cmccabe @ijuma @hachikuji","closed","","xvrl","2017-12-08T22:10:51Z","2018-01-21T16:58:26Z"
"","4410","KAFKA-4969: Attempt to evenly distribute load of tasks","This PR is an initial attempt to evenly distribute tasks with heavy processing across clients using a somewhat naive approach.  The rationale is by making sure each task is not comprised entirely of the same `topicGroupId`s,  then if there is one sub-topology doing heavy processing and another sub-topology that is relatively light, the processing load is somewhat evenly distributed.  This process only looks at active tasks; standby tasks are not given this consideration as we can end up in a state where clients have the same task assignments i.e [aT1, sT2] [aT2, sT1].  We plan to do a follow-on task at a later date where we weigh tasks with state stores to distribute tasks with state stores evenly.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2018-01-10T22:01:31Z","2018-01-30T23:35:16Z"
"","4357","MINOR: refactored code duplicates in several files (Streams project)","This PR is a reworked and improved version of [3049](https://github.com/apache/kafka/pull/3049).  PR offers following to Kafka Streams project: * refactored code duplication from **InternalTopologyBuilder** (extracted common function) * refactored code duplication from **GlobalProcessorContextImpl** and from **ProcessorContextImpl** to parent class **AbstractProcessorContext** (moved into parent) * exchanged concrete implementations with interfaces to make code more maintainable","closed","","wlsc","2017-12-25T10:56:06Z","2018-01-02T18:54:29Z"
"","3962","KAFKA-5973 Exit when ShutdownableThread encounters uncaught exception","This PR installs UncaughtExceptionHandler which calls Exit.exit() .  According to discussion on KAFKA-5973, exiting seems to be the consensus in this scenario.","closed","","tedyu","2017-09-26T16:16:32Z","2017-09-27T17:35:07Z"
"","4429","KAFKA-4029: SSL support for Connect REST API","This PR implements the JIRA issue [KAFKA-4029: SSL support for Connect REST API](https://issues.apache.org/jira/browse/KAFKA-4029) / [KIP-208](https://cwiki.apache.org/confluence/display/KAFKA/KIP-208%3A+Add+SSL+support+to+Kafka+Connect+REST+interface).  Summary of the main changes: - Jetty `HttpClient` is used as HTTP client instead of the one shipped with Java. That allows to keep the SSL configuration for Server and Client be in single place (both use the Jetty `SslContextFactory`). It also has much richer configuration than the JDK client (it is easier to configure things such as supported cipher suites etc.). - The `RestServer` class has been broker into 3 parts. `RestServer` contains the server it self. `RestClient` contains the HTTP client used for forwarding requests etc. and `SSLUtils` contain some helper classes for configuring SSL. One of the reasons for this was Findbugs complaining about the class complexity. - A new method `valuesWithPrefixAllOrNothing` has been added to `AbstractConfig` to make it easier to handle the situation that we want to use either only the prefixed SSL options or only the non-prefixed. But not mixed them.","closed","connect,","scholzj","2018-01-16T23:09:29Z","2020-10-16T06:33:47Z"
"","3832","KAFKA-5854: Handle SASL authentication failures as non-retriable exceptions in clients","This PR implements the client-side of KIP-152, by modifying `KafkaConsumer`, `KafkaProducer`, and `ConsumerGroupCommand` to throw a non-retriable exception when SASL authentication fails.  This PR is co-authored with @rajinisivaram.","closed","","vahidhashemian","2017-09-12T00:08:54Z","2017-09-20T21:54:00Z"
"","3960","KAFKA-5975: No response when deleting topics and delete.topic.enable=false","This PR implements [KIP-322](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=87295558).","closed","","mmolimar","2017-09-26T02:12:50Z","2018-08-25T15:50:59Z"
"","4479","KIP-229: DeleteGroups API","This PR implements [KIP-229](https://cwiki.apache.org/confluence/display/KAFKA/KIP-229%3A+DeleteGroups+API).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2018-01-26T19:36:36Z","2018-01-31T23:26:20Z"
"","4467","KAFKA-5540 : Deprecate and remove internal converter configs","This PR deprecates the following configs:   `internal.key.converter` `internal.value.converter`  And removes their usage from connect-*.properties files.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","umesh9794","2018-01-24T05:54:46Z","2020-03-29T01:19:24Z"
"","4165","KAFKA-6086: Provide for custom error handling when Kafka Streams fails to produce","This PR creates and implements the `ProductionExceptionHandler` as described in [KIP-210](https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce).  I've additionally provided a default implementation preserving the existing behavior. I fixed various compile errors in the tests that resulted from my changing of method signatures, and added tests to cover the new behavior.","closed","","farmdawgnation","2017-10-31T17:57:48Z","2017-12-16T19:08:15Z"
"","3671","KAFKA-5723[WIP]: Refactor BrokerApiVersionsCommand to use the new AdminClient","This PR brings refactoring to new AdminClient java class for BrokerApiVersionsCommand. The code was not tested, because I just want to make sure, that I am going in the right direction with the implementation, at the end tests will be in place. There are also no java doc for the same reasons. I took a look at #3514 to be more consistent with the implementation for the whole topic of AdminClient refactoring, so I grabbed argparse4j.","open","","adyach","2017-08-15T14:56:23Z","2018-03-02T19:30:31Z"
"","3570","KAFKA-4218: KIP-149, Enabling withKey interfaces in streams","This PR aims to provide key access to ValueMapper, ValueJoiner, ValueTransformer, Initializer and Reducer interfaces. More details can be found in [here](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner)","closed","","jeyhunkarimov","2017-07-25T04:58:41Z","2017-12-09T01:00:37Z"
"","4453","KAFKA-6035: Avoid creating changelog topics for state stores that are directly piped to a sink topic","This PR aims to optimize topology by eliminating state stores that are directly piped to a sink topic.  There few things to note:  - If topology sink operator is `toStream`, then we consider the operator just before `toStream` and check if it has state store.  - Just removing the state store from `storeToChangelogTopic` can cause a problem in case of failure, especially with `EoS`. So, what we propose here is that if the sink operator contains state store, then i) we remove selected state stores from `storeToChangelogTopic` and put it into `stateStoreToChangelogTopicOnlyForRestoring`.  The first step will make sure that selected state stores will not `write` to changelog topic. The second step is only needed when the state store `reads` from changelog topic (say, in case of failure).","open","streams,","jeyhunkarimov","2018-01-20T16:33:42Z","2018-03-02T19:31:06Z"
"","3600","KAFKA-4218: Add withkey methods to KTable","This PR aims to add withKey methods to KTable interface.  This PR is part of [KIP-149](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner). I separated the complete PR into 4 parts as discussed in [here](https://github.com/apache/kafka/pull/3570#issuecomment-317645747). So, this PR concentrates on adding withKey methods to KTable interface.","closed","kip,","jeyhunkarimov","2017-07-30T10:28:05Z","2021-02-06T13:49:33Z"
"","3598","KAFKA-4218: Add withkey methods to KStream","This PR aims to add withKey methods to KStream interface.  This PR is part of [KIP-149](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner). I separated the complete PR into 4 parts as discussed in [here](https://github.com/apache/kafka/pull/3570#issuecomment-317645747). So, this PR concentrates on adding withKey methods to KStream interface.","closed","kip,","jeyhunkarimov","2017-07-30T10:21:42Z","2021-02-06T13:49:14Z"
"","3601","KAFKA-4218: Add withkey methods to KGroupedTable","This PR aims to add withKey methods to KGroupedTable interface.  This PR is part of [KIP-149](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner). I separated the complete PR into 4 parts as discussed in [here](https://github.com/apache/kafka/pull/3570#issuecomment-317645747). So, this PR concentrates on adding withKey methods to KGroupedTable interface.","closed","kip,","jeyhunkarimov","2017-07-30T10:29:49Z","2021-02-06T13:49:45Z"
"","3599","KAFKA-4218: Add withkey methods to KGroupedStream","This PR aims to add withKey methods to KGroupedStream interface.  This PR is part of [KIP-149](https://cwiki.apache.org/confluence/display/KAFKA/KIP-149%3A+Enabling+key+access+in+ValueTransformer%2C+ValueMapper%2C+and+ValueJoiner). I separated the complete PR into 4 parts as discussed in [here](https://github.com/apache/kafka/pull/3570#issuecomment-317645747). So, this PR concentrates on adding withKey methods to KGroupedStream interface.","closed","kip,","jeyhunkarimov","2017-07-30T10:26:19Z","2021-02-06T13:49:26Z"
"","3612","KAFKA-2507, KAFKA-2959: Remove legacy ControlledShutdown request/response objects","This patch replaces the legacy ControlledShutdown objects in `kafka.api` with the alternatives in `org.apache.kafka.common.requests`. Since this was the last API that needed updating, we have also dropped the reference in `RequestChannel.Request` to the legacy object type.","closed","","hachikuji","2017-08-03T00:48:20Z","2017-08-03T23:21:25Z"
"","4376","MINOR: Nicer builder for MemoryRecords","This patch provides a nicer pattern for constructing MemoryRecords. Summary:  - Removed all of the static factory methods in MemoryRecords. - Created a high-level RecordsBuilder which is batch-aware. - The lower-level MemoryRecordsBuilder has been renamed to RecordBatchWriter and I've attempted to clean up some of its internal bookkeeping.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2018-01-02T18:36:31Z","2018-12-22T23:02:42Z"
"","4141","KAFKA-6134: Read partition reassignment lazily on event handling","This patch prevents an O(n^2) increase in memory utilization during partition reassignment. Instead of storing the reassigned partitions in the `PartitionReassignment` object (which is added after ever individual partition reassignment), we read the data fresh from ZK when processing the event.","closed","","hachikuji","2017-10-27T02:28:45Z","2017-10-27T05:10:13Z"
"","3528","Kafka-4763 (Used for triggering test only)","This patch is used only for triggering test. No need for review.","closed","","lindong28","2017-07-14T18:51:02Z","2017-07-22T19:39:05Z"
"","3498","Kafka-4763 (Used for triggering test only)","This patch is used only for triggering test. No need for review.","closed","","lindong28","2017-07-06T21:16:49Z","2017-07-07T21:32:21Z"
"","3716","KAFKA-5342: Clarify fatal/abortable exceptions used in producer","This patch improves documentation on the handling of errors for the idempotent/transactional producer. It also fixes a couple minor inconsistencies and improves test coverage. In particular: - UnsupportedForMessageFormat should be a fatal error for TxnOffsetCommit responses - UnsupportedVersion should be fatal for Produce responses and should be returned instead of InvalidRequest","closed","","hachikuji","2017-08-22T21:57:08Z","2017-08-24T23:05:34Z"
"","3573","KAFKA-5630; The consumer should block on courrupt records and keeping throw exception","This patch handles the case that a CorruptRecordException is thrown from the iterator directly. The fix is a little tricky as exceptions can be thrown from a few different scenarios. The current approach is to let the same record go through the exact same process as last time when exception is thrown, so the exception will be thrown at the same step. The only problem for that is the iterator state will change once it throws an exception. To handle that we cache the first iterator exception and put it into the suppressed exception of the IllegalStateException thrown in the future.","closed","","becketqin","2017-07-25T20:16:17Z","2017-08-01T15:59:54Z"
"","3676","MINOR: Ensure consumer logging has clientId/groupId context","This patch ensures that the consumer groupId and clientId are available in all log messages which makes debugging much easier when a single application has multiple consumer instances. To make this easier, I've added a new `LogContext` object which builds a log prefix similar to the broker-side `kafka.utils.Logging` mixin. Additionally this patch changes the log level for a couple minor cases:  - Consumer wakeup events are now logged at DEBUG instead of TRACE - Heartbeat enabling/disabling is now logged at DEBUG instead of TRACE","closed","","hachikuji","2017-08-16T20:35:31Z","2017-08-19T18:23:48Z"
"","3673","MINOR: Consolidate broker request/response handling","This patch contains a few small improvements to make request/response handling more consistent. Primarily it consolidates request/response serialization logic so that `SaslServerAuthenticator` and `KafkaApis` follow the same path. It also reduces the amount of custom logic needed to handle unsupported versions of the ApiVersions requests.","closed","","hachikuji","2017-08-15T23:02:58Z","2017-08-25T17:27:08Z"
"","3575","KAFKA-5634; Do not allow segment deletion beyond high watermark","This patch changes the segment deletion behavior to take the high watermark of the partition into account. In particular, segments containing offsets equal to or larger than the high watermark are no longer eligible for deletion. This is needed to ensure that the log start offset reported in fetch responses does not get ahead of the high watermark.  Impact: segment deletion may be delayed compared to existing behavior since the broker must await advancement of the high watermark. For topics with heavy load, this may make the active segment effectively ineligible for deletion since the high watermark may never catch up to the log end offset.","closed","","hachikuji","2017-07-26T00:46:04Z","2017-08-04T01:27:32Z"
"","4179","KAFKA-6175; AbstractIndex should cache index file to avoid unnecessary disk access during resize()","This patch also adds the a test for test the log deletion after close.","closed","","lindong28","2017-11-05T21:41:03Z","2017-12-13T01:59:22Z"
"","4392","KAFKA-3473; More Controller Health Metrics (KIP-237)","This patch adds a few metrics that are useful for monitoring controller health. See KIP-237 for more detail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2018-01-04T19:33:18Z","2018-05-22T17:43:01Z"
"","3605","KAFKA-2111: Add help arguments and required fields","This patch addresses the less invasive standardization of command line arguments as an offshoot of KIP-14. The following changes have been addressed in this patchset: - Used the required() method of JOptSimple to identify required options   for commands. Currently, an option is identified as required by   mentioning the word REQUIRED in the description of the option. - Used the requiredIf() and requiredUnless() methods to capture option   requirements which met the condition - required if/unless some other   option is present - Removed calls to CommandLineUtils.checkRequiredArgs(). Since we use   JOptSimple's required() function to mark options as required, when   we parse the arguments using OptionParser, it automatically checks   the required arguments in that call. So there is no need to do this   check again externally. - Added a help option to all the commands that were missing it. For   some of the commands that had the help option, included the call to   forHelp() method to mark the option as a help option. This means that   specifying help option on the command line will not cause parsing to   fail because of missing required options. - Added try-catch statements to correctly capture OptionException. In   the case an OptionException is caught, the command usage information   will be printed along with the exception message. The stacktrace for   all other exceptions will be printed to System.err. This addresses   the issue in KAFKA-4220 - Made some minor changes to the description text and added some more   description for some of the options","open","","johnma14","2017-08-01T13:54:51Z","2018-03-02T19:30:29Z"
"","4460","KAFKA-6345: Keep a separate count of in-flight requests to avoid ConcurrentModificationException.","This keeps a separate count of the number of in flight requests so that sensor threads will not need to deal with ConcurrentModfiicationException.  This would probably still be correct with volatile rather than AtomicInteger, but FindBugs flags the use of volatile as the count is incremented and decremented.","closed","","smccauliff","2018-01-23T00:22:51Z","2018-01-30T18:21:45Z"
"","4008","MINOR: Add / to connect docs, because HTML render doesn't respect blank lines","This just adds ``/`` to paragraphs in Kafak Connect docs.  @ewencp are you a good person to review this?","closed","","tombentley","2017-10-03T13:39:10Z","2017-10-03T17:26:57Z"
"","3681","KAFKA-5733: RocksDB bulk load with lower number of levels.","This is to complete Bill's PR #3664 on KAFKA-5733, incorporating the suggestion in https://github.com/facebook/rocksdb/issues/2734.  Some minor changes: move `open = true` in `openDB`.","closed","","guozhangwang","2017-08-16T23:55:58Z","2018-02-14T19:49:20Z"
"","3593","KAFKA-5664: Disable auto offset commit in ConsoleConsumer if no group is provided","This is to avoid polluting the Consumer Coordinator cache as the auto-generated group and its offsets are unlikely to be reused.","closed","","vahidhashemian","2017-07-28T21:51:18Z","2018-05-29T23:54:37Z"
"","4416","KAFKA-5722: [WIP] New ConfigCommand that uses AdminClient","This is the pull request to include ListQuotas, DescribeQuotas and AlterQuotas protocols with a new ConfigCommand in the tools module.  Testing: unit tests and integration tests to verify the end-to-end behavior of the new protocols. Also unit tests in both tools and core modules to verify the correctness of each components.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2018-01-12T11:26:51Z","2019-12-14T09:31:36Z"
"","4132","KAFKA-5925: Adding records deletion operation to the new Admin Client API","This is the PR related to the [KIP-204](https://cwiki.apache.org/confluence/display/KAFKA/KIP-204+%3A+Adding+records+deletion+operation+to+the+new+Admin+Client+API) in order to add the `deleteRecords` operation to the new Admin Client (it's already available in the ""legacy"" one). Other than that, unit test and integration tests are added as well (such integration tests come from the ""legacy"" integration tests in order to test the new addition in the same way as the ""legacy"" one).","closed","","ppatierno","2017-10-25T10:40:51Z","2017-11-17T08:20:24Z"
"","3916","KAFKA-5765 Move merge() from StreamsBuilder to KStream","This is the polished version.  1. The old merge() method in StreamsBuilder has been removed, 2. The merge() method in KStreamBuilder was changed so that it would use the single variable argument  rather than several variable arguments in the KStreamImpl implementation 3. The merge() method in KStream has been declared as final and tests have been added to test correctness.","closed","","ConcurrencyPractitioner","2017-09-20T03:14:53Z","2017-10-04T18:07:06Z"
"","4301","KAFKA-6323: punctuate with WALL_CLOCK_TIME triggered immediately","This is the only way I found to fix the issue without altering the API.  @mihbor @mjsax   the contribution is my original work and I license the work to the project under the project's open source license","closed","streams,","fredfp","2017-12-07T08:37:54Z","2018-01-31T04:10:41Z"
"","4362","KAFKA-5890 records.lag should use tags for topic and partition rather than using metric name.","This is the implementation of KIP-225. It marks the previous metrics as deprecated in the documentation and adds new metrics using tags.  Testing verifies that both the new and the old metric report the same value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lahabana","2017-12-27T23:13:55Z","2018-07-20T06:36:15Z"
"","3806","KAFKA-5797: Handle metadata not available in store registration","This is the backport of #3748 for trunk","closed","","guozhangwang","2017-09-07T06:36:58Z","2017-11-06T22:45:58Z"
"","4226","MINOR: Arrays.toList replaced with Collections.singletonList() where possible.","This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours. It's not actual functionality, it is an internal rewrite based on suggestions provided by the static code analysis built into the Intellij IDE.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  In places where Arrays.asList() is given exactly 1 argument, replace it with Collections.singletonList().  Internally, this is a smaller object, so it uses a bit less memory at runtime. An important thing to note is that you cannot add anything to such a list. So if it is returned through a method into a random `List<>` object, performing `add()` on it will throw an exception. I checked every usage of all replaced instances, they're only ever returned to be read, so this should not occur. A  One might say this is a micro-optimization, I would say that's true, it is. It's up to the maintainers whether or not they want this in. Personally I checked all code usages and did not find a point where an exception would be caused because of these changes. So it should not break anything and provide a tiny improvement in terms of memory footprint at runtime.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KoenDG","2017-11-16T20:07:49Z","2017-12-31T14:33:01Z"
"","4369","MINOR: Replace empty string concattenation with String.valueOf() where possible.","This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  Using an empty string concat in order to achieve the String representation of the value you want is bad for 2 reasons, as explained here: (https://stackoverflow.com/questions/1572708/is-conversion-to-string-using-int-value-bad-practice  1. Readability: it shows what you're trying to do.  2. Depending on your compiler, it might attempt to create your String by first creating a StringBuffer, appending your value to it and then doing `.toString()` on that. Which is inefficient.  Also, the `Metrics.java` file had an empty string being added for the sole reason that the page width forced a string to continue on a new line. Removed that.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KoenDG","2017-12-31T15:05:58Z","2018-01-27T10:04:45Z"
"","4377","MINOR: Refactor use of regular expressions to use statically compiled variables instead of dynamic regex.","This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  This PR is focussed on finding all places in the code where regular expressions are used dynamically and converting them to use statically compiled `Pattern` objects instead.  This prevents the regular expression from having to be compiled every single time it is used.  An explanation with code example can be seen here: https://stackoverflow.com/a/1721778  The refactoring was done by going into the source code of String.java and Pattern.java. The code now seen here does it exactly that way I found it in the Oracle JDK source code zip file that's provided along with the install.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KoenDG","2018-01-02T20:20:17Z","2018-01-28T09:57:52Z"
"","4379","MINOR: String concat used in StringBuilder","This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  A small thing: static analysis ran into a few occurrences of string concatenation being used as argument for a StringBuilder `append()` method. This seems to entirely defeat the purpose of using StringBuilder in the first place.  So I replaced the concatenation with `append()` calls instead, for consistency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KoenDG","2018-01-02T22:00:22Z","2018-01-28T09:57:57Z"
"","4303","KAFKA-6319: Quote strings stored in JSON configs","This is required for ACLs where SSL principals contain special characters (e.g. comma) that are escaped using backslash. The strings need to be quoted for JSON to ensure that the JSON stored in ZK is valid. Since ACLs may have been stored in ZK without escaping with previous versions of Kafka, a workaround has been added to parse those. Also converted `SslEndToEndAuthorizationTest` to use a principal with special characters to ensure that this path is tested.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2017-12-07T20:30:17Z","2017-12-12T10:28:18Z"
"","3968","KAFKA-5952 Refactor Consumer Fetcher metrics","This is ready for review. This does not have to be merged into 1.0.0, it can simply go to trunk.","open","","wushujames","2017-09-27T02:42:01Z","2018-03-02T19:30:47Z"
"","3486","KAFKA-5554 doc config common","This is more a request for review than it is a request to merge, as I don't think this is complete yet. However, this illustrates the idea of drawing attention to those settings which pertain to common use cases.  It would work even better with the merge of https://github.com/apache/kafka/pull/3436, since then I could link directly to the doc for each setting.","closed","","tombentley","2017-07-04T13:59:33Z","2020-02-25T11:56:39Z"
"","3996","KAFKA-5746; Return 0.0 from Metric.value() instead of throwing exception","This is less likely to break custom metric reporters and since the method is deprecated, people will be warned about this potential issue.","closed","","ijuma","2017-09-29T20:18:21Z","2017-12-22T18:22:17Z"
"","3720","[DO NOT MERGE] KAFKA-3705: non-key joins","This is just for reviewing the diff easily to see how it is done by @jfilipiak @Kaiserchen","closed","streams,","guozhangwang","2017-08-23T00:44:17Z","2019-10-15T18:47:57Z"
"","3684","KIP-81: KAFKA-4133: Bound memory usage of the Consumer","This is in-progress but there should be enough to start the reviews.  Although the KIP doesn't mention metrics, I think we should expose the memory pool usage metrics like in KIP-72. Also this need more tests","closed","","mimaison","2017-08-17T10:09:07Z","2018-04-18T14:12:00Z"
"","3509","[DO NOT MERGE] Upgrade to 5.4.5","This is for testing.","closed","","guozhangwang","2017-07-10T00:22:41Z","2017-07-15T22:06:43Z"
"","3607","[DO NOT MERGE] Existing StreamThread exception handling issues","This is for @dguy as a reference while working on the first step of KAFKA-5152, as a list of existing issues that need to be address at stream thread layer.","closed","","guozhangwang","2017-08-02T00:10:15Z","2017-11-06T22:45:06Z"
"","4214","MINOR: Make PushHttpMetricsReporter API compatible with releases back to 0.8.2.2","This is follow up to #4072 which added the PushHttpMetricsReporter and converted some services to use it. We somehow missed some compatibility issues that made the ProducerPerformance tool fail when using a newer tools jar with older common/clients jar, which we do with some system tests so we have all the features we need in the tool but can build compatibility tests for older releases.  This just adjusts some API usage to make the tool compatible with all previous releases.  I have a full run of the tests starting [here](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1122/)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2017-11-15T01:36:01Z","2017-11-15T21:30:17Z"
"","3922","Kafka 5854: Handle authentication failures in KafkaAdminClient","This is currently on top of the commits from #3832.","closed","","rajinisivaram","2017-09-20T15:31:49Z","2017-09-20T22:15:02Z"
"","3746","MINOR: Fix doc typos and grammar","This is contributed by mihbor on various doc fixes including:  https://github.com/apache/kafka/pull/3224 https://github.com/apache/kafka/pull/3226 https://github.com/apache/kafka/pull/3229","closed","","guozhangwang","2017-08-26T23:32:23Z","2017-11-06T22:45:20Z"
"","3842","KAFKA-5301 Improve exception handling on consumer path","This is an improvised approach towards fixing @guozhangwang 's second issue.  I have changed the method return type as well as override such that it returns exception. If the exception returned is not null (the default value), than we skip the callback.","closed","","ConcurrencyPractitioner","2017-09-13T03:22:26Z","2017-09-18T10:38:12Z"
"","4329","MINOR: stabilize flaky system tests","This is a workaround until KIP-91 is merged. We tried increasing the timeout multiple times already but tests are still flaky.","closed","","mjsax","2017-12-15T18:20:26Z","2017-12-19T01:52:37Z"
"","4330","[WIP] KAFKA-6359: KIP-236 interruptible reassignments","This is a WIP for KIP-236. All the existing tests (via the `/admin/reassign_partitions` path) still pass, and I've added a couple of tests for the new path (via `/admin/reassignment_requests`), but it needs a lot more tests of the new path.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2017-12-15T18:26:16Z","2020-02-25T11:54:58Z"
"","4442","Add TestKit","This is a proof of concept of some ideas for making a public Kafka cluster testing interface.  The main idea here is that you have a public, stable interface where you can easily spin up a Kafka cluster for testing with code like this:  ``` try (MiniKafkaCluster cluster = new MiniKafkaClusterBuilder()     .addZookeeperNode(new MiniZookeeperNodeBuilder())     .addNode(new MiniKafkaNodeBuilder())     .build()) {   // do stuff with the brokers, zookeepers, etc. } ```  Because the MiniKafkaCluster extends AutoCloseable, it can be used in a Java7 try-with-resources block.  It will be automatically closed when it goes out of scope.  Because we use builders everywhere, it is possible to easily add new parameters to any of the classes without breaking backwards compatibility.  For example, we could add a way to configure the port or address that each Kafka node binds to when it starts up, and so forth.  When parameters are not supplied, they should default to something reasonable.  For example, if no Kafka log directories are specified, we create a single Kafka log directory for each broker.  When broker ids are not specified, we default to automatic ID assignment through ZooKeeper.  Tests can shut down brokers or zookepeer nodes as part of the test by invoking `shutdown` on the relevant Java object. They services can be restarted by calling `start`.  During cluster startup and shutdown, we parallelize operations that take a long time, like starting a broker or shutting down a broker.  This cuts test times.  This is a rough sketch that doesn't include clients or SSL / SASL.","open","","cmccabe","2018-01-18T22:43:26Z","2018-03-02T19:31:06Z"
"","4020","KAFKA-6003: Accept appends on replicas and when rebuilding the log unconditionally","This is a port of #4004 for the 0.11.0 branch.  With this patch so that we _only_ validate appends which originate from the client. In general, once the append is validated and written to the leader the first time, revalidating it is undesirable since we can't do anything if validation fails, and also because it is hard to maintain the correct assumptions during validation, leading to spurious validation failures.  For example, when we have compacted topics, it is possible for batches to be compacted on the follower but not on the leader. This case would also lead to an OutOfOrderSequencException during replication. The same applies to when we rebuild state from compacted topics: we would get gaps in the sequence numbers, causing the OutOfOrderSequence.","closed","","apurvam","2017-10-04T21:01:28Z","2017-10-09T17:46:01Z"
"","3853","MINOR: Capitalise topicPurgatory name","This is a minor change, to capitalise topicPurgatory name to have consistent logging. Please find attached snapshot   @junrao @granthenke @ijuma Please review the change.","open","","chetnachaudhari","2017-09-13T22:26:39Z","2018-03-02T19:30:39Z"
"","3771","MINOR: logging improvements","This is a manual cherry-pick of https://github.com/apache/kafka/pull/3769 for 0.11.0","closed","","guozhangwang","2017-08-31T17:49:16Z","2017-11-06T22:46:01Z"
"","4146","MINOR: Tighten up locking when aborting expired transactions","This is a followup to #4137","closed","","apurvam","2017-10-27T16:56:20Z","2017-10-31T17:26:22Z"
"","4421","KAFKA-6398: Return value getter based on KTable materialization status","This is a bug fix that is composed of two parts:  1. The major part is, for all operators that is generating a KTable, we should construct its value getter based on whether the KTable itself is materialized. 1.a If yes, then query the materialized store directly for value getter. 1.b If not, then hand over to its parents value getter (recursively) and apply the computation to return.  2. The minor part is, in KStreamImpl, when joining with a table, we should connect with table's `valueGetterSupplier().storeNames()`, not the `internalStoreName()` as the latter always assume that the KTable is materialized, but that is not always true.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-13T01:22:11Z","2020-04-25T00:04:11Z"
"","3558","MINOR: Safer handling of requests prior to SASL authentication","This implements two improvements for request handling prior to SASL authentication:  1. Only parse request types that are allowed prior to authentication. 2. Limit the maximum request size (the default is 100Mb).","closed","","hachikuji","2017-07-21T00:24:37Z","2017-08-14T13:59:35Z"
"","4034","MINOR: Remove TLS renegotiation code","This has been disabled since the start and since it's removed in TLS 1.3, there are no plans to ever support it.","closed","","ijuma","2017-10-06T10:39:17Z","2017-12-22T18:23:57Z"
"","4388","HOTFIX: Fix lgtm.com alerts (dead code and out-of-bounds error)","This fixes two alerts flagged on [lgtm.com](https://lgtm.com) for Apache Kafka.  1. This [dead code alert](https://lgtm.com/projects/g/apache/kafka/snapshot/6abe9363dbc8d7295055865233071ef51dce1f3a/files/tools/src/main/java/org/apache/kafka/trogdor/rest/RestExceptionMapper.java?sort=name&dir=ASC&mode=heatmap&excluded=false#xbfc4f8e26024a8f4:1) where `InvalidTypeIdException` indirectly extends `JsonMappingException`. The flagged condition with the type test appears after the type test for the latter and thus makes its body dead. I opted to change the order of the tests. Please let me know if this is the intended behavior.  2. The second commit addresses this [out-of-bounds alert](https://lgtm.com/projects/g/apache/kafka/snapshot/6abe9363dbc8d7295055865233071ef51dce1f3a/files/clients/src/main/java/org/apache/kafka/common/security/kerberos/KerberosRule.java?sort=name&dir=ASC&mode=heatmap&excluded=false#x40b686b508503559:1).  More alerts can be found [here](https://lgtm.com/projects/g/apache/kafka/alerts/). Note that my colleague Aditya Sharad addressed some of those in the now outdated #2939.  @mjsax I don't know any committers here, apart from you, so maybe you could have a look? How are you doing by the way? Long time no see...","closed","","zbazztian","2018-01-04T14:26:14Z","2018-02-15T01:02:57Z"
"","3773","MINOR: Fix unit test failures from last commit, cherry-pick fix for SASL principal","This fixes PlainSaslServerTest from the last commit which uses a class not present in 0.10.2. Also cherry-picked commit 9934d28 to 0.10.2 since that is required for multiple mechanism support (SaslMultiMechanismConsumerTest was failing with the changes without this).","closed","","rajinisivaram","2017-08-31T19:23:50Z","2017-09-01T01:20:46Z"
"","3572","MINOR: Remove unused GroupState.state field","This field doesn't seem to be used and the value for `AwaitingSync` seems to be wrong (it seems like it should have been `2` instead of `5`).","closed","","ijuma","2017-07-25T13:20:31Z","2017-08-22T06:37:31Z"
"","4417","MINOR: Fix error message in KafkaConfig validation","This error message contradicts the documentation of the individual properties, their default values and of course how Kafka really works. Although equality is not a desired configuration, I decided to include it in the error message as the check itself allows equality.","closed","","andrasbeni","2018-01-12T12:52:12Z","2018-01-18T17:23:24Z"
"","3639","MINOR: Standardize logging of Worker-level messages from Tasks and Connectors","This ensures all logs have the connector/task ID, whether tasks are source or sink, and formats them consistently.","closed","connect,","ewencp","2017-08-07T17:37:51Z","2020-10-16T06:05:11Z"
"","3668","KAFKA-5679: Add logging for broker termination due to SIGTERM or SIGINT","This depends on sun.misc.Signal and sun.misc.SignalHandler, which may be removed in future releases. But along with sun.misc.Unsafe, these classes are available in Java 9, so they are safe to use for now.","closed","","rajinisivaram","2017-08-15T07:56:39Z","2017-09-26T14:12:08Z"
"","3513","Updated consumer properties","This consumer properties is for the old consumer. Updated to the new one. (it uses boostrap servers not the zookeeper)","closed","","klalafaryan","2017-07-10T10:01:52Z","2018-02-24T23:04:05Z"
"","4293","KAFKA-6308: Connect Struct should use deepEquals/deepHashCode","This changes the Struct's equals and hashCode method to use Arrays#deepEquals and Arrays#deepHashCode, respectively. This resolves a problem where two structs with values of type byte[] would not be considered equal even though the byte arrays' contents are equal. By using deepEquals, the byte arrays' contents are compared instead of ther identity.  Since this changes the behavior of the equals method for byte array values, the behavior of hashCode must change alongside it to ensure the methods still fulfill the general contract of ""equal objects must have equal hashCodes"".  Test rationale: All existing unit tests for equals were untouched and continue to work. A new test method was added to verify the behavior of equals and hashCode for Struct instances that contain a byte array value. I verify the reflixivity and transitivity of equals as well as the fact that equal Structs have equal hashCodes and not-equal structs do not have equal hashCodes.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","tobiasgies","2017-12-05T10:43:02Z","2020-10-16T06:24:13Z"
"","3992","MINOR: Allow schedule and commit in MockProcessorContext","This change allows for testing custom Processors and Transformers that call `schedule` and `commit` using KStreamTestDriver, by _not_ throwing `UnsupportedOperationException`.   This PR is my original work.","closed","","mewwts","2017-09-29T12:15:38Z","2017-10-06T21:32:00Z"
"","3998","Fix array index out of bounds","This array access might be out of bounds, as the index might be equal to the array length.","closed","","yew1eb","2017-09-30T10:47:58Z","2017-12-09T08:41:55Z"
"","3606","MINOR: Use commitId property if set","This allows a build system to set the correct commit ID when .git/HEAD would be wrong if there are local commits for build purposes.","closed","","maxzheng","2017-08-01T23:09:10Z","2017-08-03T22:36:20Z"
"","3646","MINOR: Remove unneeded error handlers in deprecated request objects","These handlers were previously used on the broker to handle uncaught exceptions, but now the broker users the new Java request objects exclusively.","closed","","hachikuji","2017-08-09T00:12:43Z","2017-08-10T18:09:54Z"
"","4474","Add missing backslash","There're missing backslash when running the quick start tutorial.","closed","","yujhe","2018-01-25T13:18:53Z","2018-01-27T00:54:03Z"
"","4151","KAFKA-3073: Add topic regex support for Connect sinks","There are more methods that had to be touched than I anticipated when writing [the KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-215%3A+Add+topic+regex+support+for+Connect+sinks).  The implementation here is now complete and includes a test that verifies that there's a call to `consumer.subscribe(Pattern, RebalanceHandler)` when `topics.regex` is provided.","closed","connect,","jklukas","2017-10-28T01:14:20Z","2020-10-16T06:24:11Z"
"","4256","KAFKA-6238: Fix 1.0.0 upgrade instructions relating to the message format version","The upgrade instructions concerning the message format versions did not account for upgrades from versions prior to 0.11.0.x.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-11-23T01:01:34Z","2017-11-27T12:52:57Z"
"","3890","MINOR: Fix replica_verification_tool.py to handle slight change in output format","The string representation of TopicPartition was changed to be {topic}-{partitition} consistently in the following commit:  f6f56a645bb1c5ec6810c024ba517e43bf77056c","closed","","ijuma","2017-09-18T12:18:13Z","2017-12-22T18:22:50Z"
"","4310","DOCU: Add authorizer.class.name to the security section in documentation","The section _7.4 Authorization and ACLs_ in Kafka documentation describes how to use ACLs. But it doesn't seem to contain the most important part - configuring the Authorizer. That might be a bit confusing for the users. This PR adds this to the documentation.","closed","","scholzj","2017-12-10T19:45:00Z","2018-01-26T19:40:35Z"
"","4469","KAFKA-6478: Added quotes around the class path","The script otherwise fails when the classpath contains paths with spaces.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Bert-R","2018-01-24T10:35:43Z","2019-01-26T04:37:32Z"
"","3774","MINOR: Increase timeout of Zookeeper service in system tests","The previous timeout was 10 seconds, but system test failures have occurred when Zookeeper has started after about 11 seconds. Increasing the timeout to 30 seconds, since most of the time this extra time will not be required, and when it is it will prevent a failed system test.  In addition to merging to `trunk`, please backport to the `0.11.x` and `0.10.2.x` branches.","closed","","rhauch","2017-08-31T21:42:33Z","2017-08-31T21:55:13Z"
"","3596","MINOR: Null out all buffer references in RequestChannel.Request","The previous code kept two references to `Buffer` and only nulled out one of them.  As part of this, I removed the `case` modifier from `RequestChannel.{Request, Response}`. They don't seem to be good matches given the types of fields they contain (mutable buffers and opaque `Send` instances).  Also removed a couple of unused files in `kafka.network`.","closed","","ijuma","2017-07-29T08:23:25Z","2017-08-22T06:37:15Z"
"","4167","KAFKA-5950: AdminClient should retry based on returned error codes","The PR will add retries for KafkaAdminClient's retriable error responses.","closed","","adyach","2017-11-01T16:11:07Z","2019-05-11T07:47:32Z"
"","3900","KAFKA-4320: Clarify the log compaction config property per-topic and per-broker","The per-topic config property is different from the per-broker. This PR makes it a bit more clear.","open","","scholzj","2017-09-19T09:36:57Z","2018-03-02T19:30:43Z"
"","3888","MINOR: Fix typo in mapper parameter of flatMapValues","The parameter is already called `mapper` in the KStreamImpl class. I think it was probably named `processor` here because it was copy/pasted from some other signature. This sees trivial enough to not require a jira as per the contribution guidelines.","closed","","cddr","2017-09-18T10:28:05Z","2017-09-18T14:32:15Z"
"","4083","MINOR: Improve a Windows quickstart instruction","The output of `wmic` can be very long and could truncate the search keywords in the existing command. If those keywords are truncated no process is returned in the output. An update is suggested to the command by which the query is performed inside the `wmic` command itself instead of using pipes and `find`.","closed","","vahidhashemian","2017-10-17T20:32:22Z","2017-10-26T20:47:46Z"
"","4455","KAFKA-6464: Fix Base64URL encode padding issue under JRE 1.7","The org.apache.kafka.common.utils.Base64 class defers Base64 encoding/decoding to the java.util.Base64 class beginning with JRE 1.8 but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.  The implementation of the encodeToString(bytes[]) method returned under JRE 1.7 by Base64.urlEncoderNoPadding() blindly removed the last two trailing characters of the Base64 encoding under the assumption that they would always be the string ""=="" but that is incorrect; padding can be ""="", ""=="", or non-existent. This commit fixes that problem.  The commit also adds a Base64.urlDecoder() method that defers to java.util.Base64 under JRE 1.8+ but leverages javax.xml.bind.DatatypeConverter under JRE 1.7.  Finally, there is a unit test to confirm that encode/decode are inverses in both the Base64 and Base64URL cases.  Signed-off-by: Ron Dagostino","closed","","rondagostino","2018-01-22T14:29:26Z","2018-01-30T14:13:42Z"
"","3936","KAFKA-5956: use serdes from materialized in table and globalTable","The new overloads `StreamBuilder.table(String, Materialized)` and `StreamsBuilder.globalTable(String, Materialized)` need to set the serdes from `Materialized` on the internal `Consumed` instance that is created, otherwise the defaults will be used and may result in serialization errors","closed","","dguy","2017-09-21T18:27:42Z","2017-09-22T12:46:57Z"
"","4280","KAFKA-6289: NetworkClient should not expose failed internal ApiVersions requests","The NetworkClient internally ApiVersion requests to each broker following connection establishment. If this request happens to fail (perhaps due to an incompatible broker), the NetworkClient includes the response in the result of poll(). Applications will generally not be expecting this response which may lead to failed assertions (or in the case of AdminClient, an obscure log message).  I've added test cases which await the ApiVersion request sent by NetworkClient to be in-flight, and then disconnect the connection and verify that the response is not included from poll().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-12-01T05:10:00Z","2017-12-08T10:55:04Z"
"","4114","MINOR: Changed visibility of methods in ClusterConnectionStates to private","The methods resetReconnectBackoff and updateReconnectBackoff in ClusterConnectionStates both take an instance of a private inner class as parameter and thus cannot be called from outside the class anyway.","closed","","soenkeliebau","2017-10-22T20:50:37Z","2017-10-22T23:56:25Z"
"","4383","MINOR: Improve exception messages when state stores cannot be accessed.","The messages were previously identical which is a bit of a pain since you have to track down line numbers to see exactly why the exception happened. The messages are also confusing and I think misleading in the first case, where the issue likely isn't that the state store actually moved just that a rebalance is in progress.  Hopefully these are clearer and provide a bit more debugging info.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2018-01-03T19:42:19Z","2018-01-08T19:15:00Z"
"","4185","MINOR: Update Scala 2.11 to 2.11.12","The main change is Java 9 support.","closed","","ijuma","2017-11-06T23:43:07Z","2017-11-15T14:37:14Z"
"","3933","MINOR: fix the information states in the log","the log is not that accurate, at that point topic partition should be in paused state.","open","","lisa2lisa","2017-09-21T14:27:12Z","2018-06-04T22:02:52Z"
"","3898","KAFKA-4320: Log cleaner is enabled by default","The log cleaner is enabled by default since 0.9.0.1. But the 0.9.0 docs are not updated. This PR updates the docs to say that it is enabled by default.","closed","","scholzj","2017-09-19T09:36:41Z","2022-02-10T16:44:36Z"
"","3899","KAFKA-4320: Log cleaner is enabled by default","The log cleaner is enabled by default since 0.9.0.1. But the 0.10.0 docs are not updated. This PR updates the docs to say that it is enabled by default.","closed","","scholzj","2017-09-19T09:36:47Z","2022-02-10T16:44:26Z"
"","3714","close iterator on doc example","The iterator interface usage has some examples missing explicit close operation after usage. We should remind the user to do so because un-closed iterator will leave the underlying file descriptor open, thus eating up memory. @guozhangwang @Ishiihara","closed","","abbccdda","2017-08-22T20:50:11Z","2017-09-07T05:23:43Z"
"","4042","KAFKA-5835; CommitFailedException exception versus KafkaConsumer flow","The invalid offset/out of range offset is captured in IllegalArgumentException/RunTimeException. The CommitFailedException only happens as called out in CommitFailedException javadoc. Making the invalid offset exception flow explicit, updated KafkaConsumer javadoc.  Please let me know if it makes sense.Thanks","open","","rekhajoshm","2017-10-09T03:22:13Z","2018-03-02T19:30:50Z"
"","4097","MINOR: Update docs with regards to max.in.flight and idempotent producer","The idempotent producer doesn't change that setting any more and the accepted range has changed.","closed","","ijuma","2017-10-19T09:48:03Z","2017-12-22T18:23:49Z"
"","4066","MINOR: Don't register signal handlers if running on Windows","The following happens on Windows for `HUP`:  [2017-10-11 21:45:11,642] FATAL  (kafka.Kafka$) java.lang.IllegalArgumentException: Unknown signal: HUP         at sun.misc.Signal.(Unknown Source)         at kafka.Kafka$.registerHandler$1(Kafka.scala:67)         at kafka.Kafka$.registerLoggingSignalHandler(Kafka.scala:73)         at kafka.Kafka$.main(Kafka.scala:82)         at kafka.Kafka.main(Kafka.scala)  I thought it was safer not to register them at all since the additional logging is a nice to have and we haven't tested it on Windows.  Also changed map to be concurrent and removed stray printStackTrace in test.","closed","","ijuma","2017-10-12T12:02:40Z","2017-12-22T18:23:56Z"
"","4463","MINOR: Fix typo in consumer group command error","The error message lists '--reset-offset' as one of the possible options, but the option is in fact called '--reset-offsets'.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","loganmhb","2018-01-23T23:28:20Z","2018-01-25T17:16:16Z"
"","4289","KAFKA-6301 changing regular expression in ops.html from '*' to '.*'","The documentation for section ""Mirroring data between clusters"" states the following: Or you could mirror all topics using --whitelist '*' The regular expression should be '.*' instead.  This fix makes the change directly to the ops.html file.","closed","","waleedfateem","2017-12-03T19:51:49Z","2022-02-10T16:40:49Z"
"","4150","MINOR: Add valid values for message.timestamp.type","The documentation for `message.timestamp.type` is missing valid values (https://kafka.apache.org/documentation/#topicconfigs). This change adds valid values for that config","open","","makearl","2017-10-28T00:51:53Z","2018-03-02T19:30:54Z"
"","3843","MINOR: Fix JavaDoc for StreamsConfig.PROCESSING_GUARANTEE_CONFIG","The contribution is my original work and I license the work to the project under the project's open source licence.","closed","","leigh-perry","2017-09-13T11:06:50Z","2017-09-13T16:54:21Z"
"","4227","MINOR: Log unexpected exceptions in Connect REST calls that generate 500s at a higher log level","The ConnectExceptionMapper was originally intended to handle ConnectException errors for some expected cases where we just want to always convert them to a certain response and the ExceptionMapper was the easiest way to do that uniformly across the API. However, in the case that it's not an expected subclass, we should log the information at the error level so the user can track down the cause of the error.  This is only an initial improvement. We should probably also add a more general ExceptionMapper to handle other exceptions we may not have caught and converted to ConnectException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ewencp","2017-11-16T23:51:24Z","2020-10-16T06:24:12Z"
"","4050","KAFKA-6041: ""--producer.config"" option doesn't work""","The configured parameters in the config\producer.properties file will be overwritten by the parameter default values.  For example:  //modify the config\producer.properties file: bootstrap.servers=kafkahost:9092 linger.ms=2000 batch.size=1024  //excute the bin\Kafka-console-producer.sh  bin\Kafka-console-producer --topic test --producer.config config/producer.properties  //while excuting the ConsoleProducer Class, the batch.size is set  to 200, the linger.ms is set to 1000, the bootstrap.servers is set to localhost:9092.    The correct way is to overwrite the default parameters values by configured values.","open","","wujianping10043419","2017-10-10T09:06:32Z","2018-03-02T19:30:50Z"
"","3663","KAFKA-4643: Improve test coverage of StreamsKafkaClient","The commit brings improved test coverage for StreamsKafkaClientTest.java","closed","","adyach","2017-08-14T09:46:58Z","2017-08-14T17:26:10Z"
"","3659","KAFKA-4643: Improve test coverage of StreamsKafkaClient","The commit brings improved test coverage for StreamsKafkaClientTest.java","closed","","adyach","2017-08-11T14:39:50Z","2017-08-14T09:47:27Z"
"","4361","KAFKA-6405:Fix incorrect comment in MetadataUpdater","The comment for 'handleDisconnection' says it can return true or false, but the return type is void.  ```java     /**      * If `request` is a metadata request, handles it and return `true`. Otherwise, returns `false`.      *      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own      * requests with special handling for disconnections of such requests.      * @param destination      */     void handleDisconnection(String destination); ```  change to ```java     /**      * Handle disconnections for metadata requests.      *      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own      * requests with special handling for disconnections of such requests.      * @param destination      */     void handleDisconnection(String destination); ```","closed","","Guangxian","2017-12-27T07:52:28Z","2018-02-08T01:44:40Z"
"","3587","MINOR: Support versions with 3 segments in _kafka_jar_versions","The bump from 0.11.1.0-SNAPSHOT to 1.0.0-SNAPSHOT broke a couple of system tests:  * TestVerifiableProducer.test_simple_run * KafkaVersionTest.test_multi_version","closed","","ijuma","2017-07-27T13:58:33Z","2017-08-22T06:37:14Z"
"","3584","KAFKA-5658. Fix AdminClient request timeout handling bug resulting in continual BrokerNotAvailableExceptions","The AdminClient does not properly clear calls from the callsInFlight structure. Later, in an effort to clear the lingering call objects, it closes the connection they are associated with. This disrupts new incoming calls, which then get BrokerNotAvailableException.  This patch fixes this bug by properly removing completed calls from the callsInFlight structure.  It also adds the Call#aborted flag, which ensures that we only abort a connection once-- even if there is a similar bug in the future which causes old Call objects to linger.","closed","","cmccabe","2017-07-26T21:06:49Z","2019-05-20T18:44:37Z"
"","3868","KAFKA-5908: fix range query in CompositeReadOnlyWindowStore","The `NextIteratorFunction` in `CompositeReadOnlyWindowStore` was incorrectly using the `timeFrom` as the `timeTo`","closed","","dguy","2017-09-15T09:05:03Z","2017-09-15T13:20:51Z"
"","3985","KAFKA-5987: Maintain order of metric tags in generated documentation","The `MetricNameTemplate` is changed to used a `LinkedHashSet` to maintain the same order of the tags that are passed in. This tag order is then maintained when `Metrics.toHtmlTable` generates the MBean names for each of the metrics.  The `SenderMetricsRegistry` and `FetcherMetricsRegistry` both contain templates used in the producer and consumer, respectively, and these were changed to use a `LinkedHashSet` to maintain the order of the tags.  Before this change, the generated HTML documentation might use MBean names like the following and order them:  ``` kafka.connect:type=sink-task-metrics,connector={connector},partition={partition},task={task},topic={topic} kafka.connect:type=sink-task-metrics,connector={connector},task={task} ``` However, after this change, the documentation would use the following order: ``` kafka.connect:type=sink-task-metrics,connector={connector},task={task} kafka.connect:type=sink-task-metrics,connector={connector},task={task},topic={topic},partition={partition} ```  This is more readable as the code that is creating the templates has control over the order of the tags.  Note that JMX MBean names use ObjectName that does not maintain order of the properties (tags), so this change should have no impact on the actual JMX MBean names used in the metrics.  cc @wushujames","closed","","rhauch","2017-09-28T20:12:49Z","2018-02-05T20:14:59Z"
"","3875","MINOR: Use full package name when classes referenced in documentation","The `metric.reporters` description in the documentation says to implement the `MetricReporter` class, but the actual class is `MetricsReporter`. [MetricsReporter.java](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/metrics/MetricsReporter.java)  The configurations documentation is also inconsistent as some references to classes do not have the full package name while others do.  @ijuma","closed","","KevinLiLu","2017-09-15T17:54:57Z","2017-09-28T10:42:03Z"
"","3867","deleted","The `metric.reporters` description in the documentation says to implement the `MetricReporter` class, but the actual class is `MetricsReporter`.  [MetricsReporter.java](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/metrics/MetricsReporter.java)  The broker configuration section also uses the client description so it says to implement the `MetricReporter` interface, but the actual class is `KafkaMetricsReporter`. Fixing this seems to require setting the description at: https://github.com/apache/kafka/blob/dfd625daa36de2e34e6c596967775394c55bc605/core/src/main/scala/kafka/server/KafkaConfig.scala#L638  If that is the right place, I can add a commit to edit it.  @jkreps","closed","","KevinLiLu","2017-09-15T05:52:50Z","2018-12-02T23:52:44Z"
"","4271","KAFKA-5526: Additional `--describe` views for ConsumerGroupCommand (KIP-175)","The `--describe` option of ConsumerGroupCommand is expanded, as proposed in [KIP-175](https://cwiki.apache.org/confluence/display/KAFKA/KIP-175%3A+Additional+%27--describe%27+views+for+ConsumerGroupCommand), to support: * `--describe` or `--describe --offsets`: listing of current group offsets * `--describe --members` or `--describe --members --verbose`: listing of group members * `--describe --state`: group status  Example: With a single partition topic `test1` and a double partition topic `test2`, consumers `consumer1` and `consumer11` subscribed to `test`, consumers `consumer2` and `consumer22` and `consumer222` subscribed to `test2`, and all consumers belonging to group `test-group`, this is an output example of the new options above for `test-group`:  ``` --describe, or --describe --offsets:  TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID test2           0          0               0               0               consumer2-bad9496d-0889-47ab-98ff-af17d9460382  /127.0.0.1      consumer2 test2           1          0               0               0               consumer22-c45e6ee2-0c7d-44a3-94a8-9627f63fb411 /127.0.0.1      consumer22 test1           0          0               0               0               consumer1-d51b0345-3194-4305-80db-81a68fa6c5bf  /127.0.0.1      consumer1 ```  ``` --describe --members  CONSUMER-ID                                      HOST            CLIENT-ID       #PARTITIONS consumer2-bad9496d-0889-47ab-98ff-af17d9460382   /127.0.0.1      consumer2       1 consumer222-ed2108cd-d368-41f1-8514-5b72aa835bcc /127.0.0.1      consumer222     0 consumer11-dc8295d7-8f3f-4438-9b11-7270bab46760  /127.0.0.1      consumer11      0 consumer22-c45e6ee2-0c7d-44a3-94a8-9627f63fb411  /127.0.0.1      consumer22      1 consumer1-d51b0345-3194-4305-80db-81a68fa6c5bf   /127.0.0.1      consumer1       1 ```  ``` --describe --members --verbose  CONSUMER-ID                                      HOST            CLIENT-ID       #PARTITIONS     ASSIGNMENT consumer2-bad9496d-0889-47ab-98ff-af17d9460382   /127.0.0.1      consumer2       1               test2(0) consumer222-ed2108cd-d368-41f1-8514-5b72aa835bcc /127.0.0.1      consumer222     0               - consumer11-dc8295d7-8f3f-4438-9b11-7270bab46760  /127.0.0.1      consumer11      0               - consumer22-c45e6ee2-0c7d-44a3-94a8-9627f63fb411  /127.0.0.1      consumer22      1               test2(1) consumer1-d51b0345-3194-4305-80db-81a68fa6c5bf   /127.0.0.1      consumer1       1               test1(0) ```  ``` --describe --state  COORDINATOR (ID)         ASSIGNMENT-STRATEGY       STATE                #MEMBERS localhost:9092 (0)       range                     Stable               5 ```  Note that this PR also addresses the issue reported in [KAFKA-6158](https://issues.apache.org/jira/browse/KAFKA-6158) by dynamically setting the width of columns `TOPIC`, `CONSUMER-ID`, `HOST`, `CLIENT-ID` and `COORDINATOR (ID)`. This avoid truncation of column values when they go over the current fixed width of these columns.  The code has been restructured to better support testing of individual values and also the console output. Unit tests have been updated and extended to take advantage of this restructuring.","closed","","vahidhashemian","2017-11-28T20:28:00Z","2017-12-22T06:34:40Z"
"","4091","Test branch","Test","closed","","dnangel026","2017-10-18T22:10:22Z","2020-01-24T22:54:52Z"
"","4441","[KAFKA-5117]: Password Mask to Kafka Connect REST Endpoint","Tasks Remaining: - [x] Write functionality - [x] Write tests - [ ] Opt-in config  I've opened this PR up upon discovering: https://github.com/confluentinc/kafka-connect-jdbc/issues/348  Upon this being accepted, could I request this be cherry-picked into the `0.11` branch?  Before:  ![image](https://user-images.githubusercontent.com/4412200/35118883-2358c23e-fc48-11e7-9283-77f695c7dfba.png)  After: ![image](https://user-images.githubusercontent.com/4412200/35118868-137e82ae-fc48-11e7-8c34-777df0322f2e.png)   Credits to: @qiao-meng-zefr for starting this work.","closed","connect,","Tang8330","2018-01-18T20:02:09Z","2019-10-14T22:53:45Z"
"","4313","MINOR: broker down for significant amt of time system test","System test where a broker is offline more than the configured timeouts.  In this case: - Max poll interval set to 45 secs - Retries set to 2 - Request timeout set to 15 seconds - Max block ms set to 30 seconds  The broker was taken off-line for 70 seconds or more than double request timeout * num retries  [passing system test results](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2017-12-11--001.1513034559--bbejeck--KSTREAMS_1179_broker_down_for_significant_amt_of_time--6ab4802/report.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2017-12-11T23:34:02Z","2017-12-19T23:39:17Z"
"","4299","KAFKA-6300 SelectorTest may fail with ConcurrentModificationException","Synchronization is added w.r.t. sockets ArrayList to avoid ConcurrentModificationException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2017-12-06T17:49:52Z","2017-12-06T19:44:48Z"
"","4288","KAFKA-6300 SelectorTest may fail with ConcurrentModificationException","Synchronization is added w.r.t. sockets ArrayList to avoid ConcurrentModificationException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2017-12-03T00:15:04Z","2017-12-06T17:51:26Z"
"","4282","KAFKA-6293 Support for Avro formatter in ConsoleConsumer","Support for Avro formatter in ConsoleConsumer With Confluent Schema Registry as per https://issues.apache.org/jira/browse/KAFKA-6293  This adds the ability the display Avro payloads when listening for messages in kafka-console-consumer.sh. This proposed PR will display Avro payloads (in JSON) when executed with the following parameters: bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mytopic --confluent-server localhost:8081 --formatter kafka.tools.AvroMessageFormatter  This contribution is my original work and I license the work to the project under the project's open source license.  All tests passed: BUILD SUCCESSFUL in 21m 18s 90 actionable tasks: 60 executed, 30 up-to-date","closed","","ethiebaut","2017-12-01T13:50:47Z","2018-07-01T09:51:34Z"
"","3539","KAFKA-5602: ducker-ak: support --custom-ducktape","Support a --custom-ducktape flag which allows developers to install their own versions of ducktape into Docker images.  This is helpful for ducktape development.","closed","","cmccabe","2017-07-17T17:05:40Z","2019-05-20T18:44:46Z"
"","3737","KAFKA-5771: org.apache.kafka.streams.state.internals.Segments#segments method returns incorrect results when segments were added out of order","Suggested fix for the bug","closed","","radzish","2017-08-25T09:47:51Z","2017-08-25T13:00:39Z"
"","3535","KAFKA-5597: Autogenerate producer sender metrics.","Subtask of https://issues.apache.org/jira/browse/KAFKA-3480  The changes are very similar to what was done for the consumer in https://issues.apache.org/jira/browse/KAFKA-5191 (pull request https://github.com/apache/kafka/pull/2993)  A screenshot of the docs are here: ![producer metrics docs](https://user-images.githubusercontent.com/677529/28245950-96b0c20a-69c6-11e7-8631-11fd55a0ad92.png)","closed","","wushujames","2017-07-16T08:32:34Z","2017-09-06T00:40:48Z"
"","4025","KAFKA-5989: resume consumption of tasks that have state stores but no changelogging","Stores where logging is disabled where never consumed as the partitions were paused, but never resumed.","closed","","dguy","2017-10-05T15:30:13Z","2017-10-05T20:03:24Z"
"","4002","KAFKA-5989: resume consumption of tasks that have state stores but no changelogging","Stores where logging is disabled where never consumed as the partitions were paused, but never resumed.","closed","","dguy","2017-10-02T17:39:52Z","2017-10-05T15:27:41Z"
"","3670","MINOR: fixed StickyAssignor javadoc","StickyAssignor javadoc has a bunch of formatting issues which make it pretty hard to read: http://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/StickyAssignor.html  cc @vahidhashemian","closed","","mimaison","2017-08-15T10:28:56Z","2018-04-18T13:31:32Z"
"","4363","MINOR: wait for broker startup for system tests","Starting up a Kafka cluster with multiple nodes can lead to a race condition if an application wants to create a topic but not all brokers are available yet.  For example: ``` org.apache.kafka.streams.errors.StreamsException: Could not create topic SmokeTest-cntByCnt-repartition. 	at  ... Caused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 2. 	at ... Caused by: org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 2. ```  This should be fixed by making sure all brokers are registered at ZK before creating topics.","closed","","mjsax","2017-12-28T22:53:05Z","2018-02-24T00:20:05Z"
"","3514","KAFKA-5561: Rewrite TopicCommand using the new Admin client (** WIP do not merge **)","Start on porting TopicCommand from Scala to Java using Admin Client API as well  Please do not merge it's just for evaluation with other contributors and committers as starting point for a migration from Scala to Java for tools. Other considerations here :   https://issues.apache.org/jira/browse/KAFKA-5536","open","","ppatierno","2017-07-10T10:56:15Z","2018-03-02T19:30:25Z"
"","3643","MINOR: Error when incompatible console producer configs.","Spinned off from [KAFKA-2526](https://issues.apache.org/jira/browse/KAFKA-2526), to throw an exception if the console producer is passed a key serializer or a value serializer. To quote a comment on the JIRA: > The longer term solution with a KIP is obviously a lot more involved, but you should feel free to work on it; it would be a useful improvement. Since there would be two separate steps, you'd probably want to either file a separate JIRA for the better error messages and leave this one to the KIP or just file that fix as a MINOR PR.  Hence opening as a MINOR PR cc @ewencp","closed","","aishraj","2017-08-08T14:54:56Z","2018-04-07T16:38:52Z"
"","3769","MINOR: Log encountered exception during rebalance","Some other minor changes:  1. Do not throw the exception form callback as it would only be swallowed by consumer coordinator; remembering it and re-throw in the next loop is good enough. 2. Change Creating to Defining in Stores to avoid confusions that the stores have already been successfully created at that time. 3. Do not need unAssignChangeLogPartitions as the restore consumer will be unassigned already inside changelog reader.","closed","","guozhangwang","2017-08-31T17:06:18Z","2017-11-06T22:45:31Z"
"","3909","MINOR: Consistent terminal period in Errors.defaultExceptionMessage","Some default messages ended with a period and some didn't, so make things consistent.  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","tombentley","2017-09-19T20:47:59Z","2020-02-27T11:29:45Z"
"","4182","KAFKA-6163 fail fast on startup","skip loading remaining logs if we encounter an unrecoverable error on startup  @hachikuji @ijuma","open","","xvrl","2017-11-06T16:48:43Z","2018-03-02T19:30:55Z"
"","3863","MINOR: Use SecurityProtocol in AuthenticationContext","Since we removed the unused `TRACE` option from `SecurityProtocol`, it now seems safer to expose it from `AuthenticationContext`. Additionally this patch exposes javadocs under security.auth and relocates the `Login` and `AuthCallbackHandler` to a non-public package.","closed","","hachikuji","2017-09-14T21:07:24Z","2017-10-04T16:24:53Z"
"","3935","MINOR: improvement on top of KAFKA-5793: Tighten up the semantics of the OutOfOrderSequenceException","Simplified the condition in Sender#failBatch() Added log in TransactionManager#updateLastAckedOffset()","closed","","tedyu","2017-09-21T17:02:35Z","2017-09-22T18:13:28Z"
"","4213","KAFKA-4115: Increasing the heap settings for connect-distributed script","Signed-off-by: Arjun Satish","closed","connect,","wicknicks","2017-11-14T23:43:03Z","2020-10-16T06:05:13Z"
"","4205","KAFKA-4827: Correctly encode special chars while creating URI objects","Signed-off-by: Arjun Satish","closed","connect,","wicknicks","2017-11-10T18:04:41Z","2020-10-16T06:24:11Z"
"","3613","KAFKA-2360: Extract producer-specific configs out of the common PerfConfig","Separate `batch.size`, `message-size` and `compression-code` from PerfConfig to a newly-created ProducerPerfConfig in order to hide them in ConsumerPerf tool.","closed","","huxihx","2017-08-03T03:13:21Z","2017-08-08T23:39:34Z"
"","3870","KAFKA-5856: Add AdminClient.createPartitions()","See KIP-195.  The contribution is my original work and I license the work to the project under the project's open source license.  This patch adds AdminClient.createPartitions() and the network protocol is uses. The broker-side algorithm is as follows:  1. KafkaApis makes some initial checks on the request, then delegates to the    new AdminManager.createPartitions() method. 2. AdminManager.createPartitions() performs some validation then delegates to    AdminUtils.addPartitions().  Aside: I felt it was safer to add the extra validation in AdminManager.createPartitions() than in AdminUtils.addPartitions() since the latter is used on other code paths which might fail differently with the introduction of extra checks.  3. AdminUtils.addPartitions() does its own checks and adds the partitions. 4. AdminManager then uses the existing topic purgatory to wait for the    PartitionInfo available from the metadata cache to become consistent with    the new total number of partitions.  The messages of exceptions thrown in AdminUtils affecting this new API have been made consistent with initial capital letter and terminating period. A few have been reworded for clarity. I've also standardized on using String.format().  cc @ijuma","closed","","tombentley","2017-09-15T11:05:57Z","2017-09-21T09:28:39Z"
"","3708","KAFKA-4764: Wrap SASL tokens in Kafka headers to improve diagnostics (KIP-152)","SASL handshake protocol changes from KIP-152.","closed","","rajinisivaram","2017-08-21T21:30:03Z","2017-09-15T16:18:22Z"
"","3938","HOTFIX: ConsumerGroupCommand - Offset and partition numbers are not converted to long and int correctly","Running the command line with --from-file option causes the following exception:  java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer  Reason: asInstanceOf used for the conversion.  Also, unit test is using --to-earliest and --from-file together when executing the test. This is executing --to-earliest option only and ignoring --from-file option. Since the preparation part is also using --to-earliest to create the file, this unit test passes without testing --from-file option. Fixed the unit test too.","closed","","eu657","2017-09-21T20:18:44Z","2017-09-22T21:37:39Z"
"","4188","Add a Validator for NonNull configurations and remove redundant null checks on lists","Return an empty list instead of null to make handling these easier","closed","","lahabana","2017-11-07T18:21:28Z","2018-01-25T09:57:15Z"
"","3667","KAFKA-5606: Review consumer's RequestFuture usage pattern","Replacing succeeded, failed and retry with a status method returning an enum with 'SUCCEEDED', 'FAILED', 'RETRY' and 'NOT_RETRY'","closed","","jedichien","2017-08-15T02:48:25Z","2017-09-07T02:59:26Z"
"","3991","KAFKA-5968: Create/remove request metrics during broker startup/shutdown","Replaces the static `RequestMetrics` object with a class so that metrics are created and removed during broker startup and shutdown to avoid metrics tests being affected by metrics left behind by previous tests. Also reinstates `kafka.api.MetricsTest` which was failing frequently earlier due to tests removing the static request metrics.","closed","","rajinisivaram","2017-09-29T11:09:09Z","2017-09-29T14:13:14Z"
"","4368","MINOR: Replace Arrays.asList with Collections.singletonList where possible.","Replacement PR for #4226 where I screwed up the git merges.  This is something I did after my working hours, I would ask people reviewing this do the same, don't take time for this during your work hours. It's not actual functionality, it is an internal rewrite based on suggestions provided by the static code analysis built into the Intellij IDE.  I try to keep such a PR as limited as possible, for clarity of reading.  ==========  In places where Arrays.asList() is given exactly 1 argument, replace it with Collections.singletonList().  Internally, this is a smaller object, so it uses a bit less memory at runtime. An important thing to note is that you cannot add anything to such a list. So if it is returned through a method into a random `List<>` object, performing `add()` on it will throw an exception. I checked every usage of all replaced instances, they're only ever returned to be read, so this should not occur. A  One might say this is a micro-optimization, I would say that's true, it is. It's up to the maintainers whether or not they want this in. Personally I checked all code usages and did not find a point where an exception would be caused because of these changes. So it should not break anything and provide a tiny improvement in terms of memory footprint at runtime.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)  pinging @ijuma about the replacement","closed","","KoenDG","2017-12-31T14:47:10Z","2018-01-01T19:50:36Z"
"","4254","KAFKA-6074 Use ZookeeperClient in ReplicaManager and Partition","Replace ZkUtils with KafkaZkClient in ReplicaManager and Partition  Utilize existing unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2017-11-22T22:30:08Z","2017-11-24T11:33:25Z"
"","4232","KAFKA-6233 :Removed unnecessary null check","Removed unnecessary null check if (encodingValue != null && encodingValue instanceof String) null instanceof String returns false hence replaced the check with if (encodingValue instanceof String)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sagarchavan3172","2017-11-18T20:53:05Z","2017-11-19T18:56:31Z"
"","4218","Update a protocol.html error","Removed two words that indicated that the primitive types of the protocol would be listed here, which is not yet the case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Joannis","2017-11-15T15:00:42Z","2020-03-26T10:01:27Z"
"","3887","KAFKA-5919: Adding checks on ""version"" field for tools using it","Removed ignored ""version"" field in JSON file for deleting records","closed","","ppatierno","2017-09-18T09:51:36Z","2018-06-06T06:45:44Z"
"","3568","MINOR: updated configs to use one try/catch for serdes","removed `try/catch` from `keySerde` and `valueSerde` methods so only the `try\catch` blocks in `defaultKeySerde` and `defaultValueSerde` perform error handling resulting in correct error message.","closed","","bbejeck","2017-07-24T21:36:23Z","2017-07-26T19:59:43Z"
"","3742","[KAFKA-4380] Document the purpose of clean shutdown file in the code.","Remove the previous TODO to remove the clean shutdown file with some of the discussion from https://github.com/apache/kafka/pull/2104.","closed","","holdenk","2017-08-25T18:07:06Z","2017-08-27T08:14:36Z"
"","4239","KAFKA-6214: enable use of in-memory store for standby tasks","Remove the flag in `ProcessorStateManager` that checks if a store is persistent when registering it as a standby task. Updated the smoke test to use an in-memory store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2017-11-20T17:30:01Z","2017-11-22T01:57:03Z"
"","3783","KAFKA-5515: Remove date formatting from Segments","Remove date formatting from `Segments` and use the `segementId` instead. Add tests to make sure can load old segments. Rename old segment dirs to new formatting at load time.","closed","","dguy","2017-09-04T13:16:12Z","2017-09-18T11:13:35Z"
"","4133","Fix typo dev guide title","related to https://github.com/apache/kafka-site/pull/103","closed","","joel-hamill","2017-10-25T14:50:35Z","2017-10-25T17:13:20Z"
"","3592","KAFKA-5673: refactor KeyValueStore hierarchy to make MeteredKeyValueStore outermost","refactor StateStoreSuppliers such that a `MeteredKeyValueStore`  is the outermost store.","closed","","dguy","2017-07-28T16:27:34Z","2017-08-16T13:22:23Z"
"","4278","KAFKA-6590: Fix bug in aggregation of consumer fetch bytes and counts metrics","records-consumed-rate fetch metric shows different value than aggregated topic-level records-consumed-rate metrics. Looks like a typo when reporting bytes and records in Fetcher class.","closed","","kiest","2017-11-30T12:21:07Z","2018-02-25T02:43:11Z"
"","4296","KAFKA-6313: Add SLF4J as direct dependency to Kafka core","Recent changes are now directly using the SLF4J API, so we should have a direct dependency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2017-12-06T15:09:54Z","2017-12-07T08:38:38Z"
"","4056","KAFKA-6051 Close the ReplicaFetcherBlockingSend earlier on shutdown","Rearranged the testAddPartitionDuringDeleteTopic() test to keep the likelyhood of the race condition.","closed","","mayt","2017-10-11T10:42:28Z","2017-10-18T17:00:47Z"
"","4176","KAFKA-6168 Connect Schema comparison is slow for large schemas","Re-arrange order of comparisons in equals() to evaluate non-composite fields first Cache hash code","closed","connect,","tedyu","2017-11-04T02:05:47Z","2020-10-16T06:05:13Z"
"","4217","MINOR: Updated KafkaZkClient.pathExists() to use ExistsRequest","rather than using GetDataRequest","closed","","mimaison","2017-11-15T14:33:32Z","2018-04-18T13:27:43Z"
"","4156","Merge pull request #2 from apache/trunk","pull","closed","","hejiefang","2017-10-30T09:22:20Z","2017-12-22T01:44:07Z"
"","3620","KAFKA-5700: Producer should not drop header information when splitting batches","Producer should not drop header information when splitting batches.  This PR also corrects a minor typo in Sender.java, where `spitting and retrying` should be `splitting and retrying`.","closed","","huxihx","2017-08-04T03:23:36Z","2017-08-07T05:32:20Z"
"","3717","KAFKA-5731 Corrected how the sink task worker updates the last committed offsets (0.10.2)","Prior to this change, it was possible for the synchronous consumer commit request to be handled before previously-submitted asynchronous commit requests. If that happened, the out-of-order handlers improperly set the last committed offsets, which then became inconsistent with the offsets the connector task is working with.  This change ensures that the last committed offsets are updated only for the most recent commit request, even if the consumer reorders the calls to the callbacks.  This change also backports the fix for KAFKA-4942, which was minimal that caused the new tests to fail.  **This is for the `0.10.2` branch; see #3662 for the equivalent and already-approved PR for `trunk` and #3672 for the equivalent and already-approved PR for the `0.11.0` branch.**","closed","","rhauch","2017-08-22T23:34:04Z","2017-08-23T01:35:27Z"
"","3672","KAFKA-5731 Corrected how the sink task worker updates the last committed offsets (0.11.0)","Prior to this change, it was possible for the synchronous consumer commit request to be handled before previously-submitted asynchronous commit requests. If that happened, the out-of-order handlers improperly set the last committed offsets, which then became inconsistent with the offsets the connector task is working with.  This change ensures that the last committed offsets are updated only for the most recent commit request, even if the consumer reorders the calls to the callbacks.  **This is for the `0.11.0` branch; see #3662 for the equivalent and already-approved PR for `trunk`.**","closed","","rhauch","2017-08-15T22:11:25Z","2017-08-16T01:48:26Z"
"","3662","KAFKA-5731 Corrected how the sink task worker updates the last committed offsets","Prior to this change, it was possible for the synchronous consumer commit request to be handled before previously-submitted asynchronous commit requests. If that happened, the out-of-order handlers improperly set the last committed offsets, which then became inconsistent with the offsets the connector task is working with.  This change ensures that the last committed offsets are updated only for the most recent commit request, even if the consumer reorders the calls to the callbacks.","closed","connect,","rhauch","2017-08-13T23:41:10Z","2020-10-16T06:29:11Z"
"","4233","KAFKA-6181 Examining log messages with {{--deep-iteration}} should show superset of fields","Printing log data on Kafka brokers using kafka.tools.DumpLogSegments --deep-iteration option doesn't print all the fields. Adding missing fields in the log data on kafka brokers Adding following fields in the deep-interation option:  baseOffset lastOffset baseSequence lastSequence producerEpoch partitionLeaderEpoch size crc  thanks, Nikhil  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","deorenikhil","2017-11-19T07:13:25Z","2018-03-02T19:30:57Z"
"","3715","KafkaService: print node hostname on failure","Print node hostname on failure.  Check for both ""kafka.Kafka"" and ""metrics.SupportedKafka"" process names.","closed","","cmccabe","2017-08-22T21:29:58Z","2019-05-20T18:58:09Z"
"","4073","KAFKA-6060: Add workload generation capabilities to Trogdor","Previously, Trogdor only handled ""Faults.""  Now, Trogdor can handle ""Tasks"" which may be either faults, or workloads to execute in the background.  The Agent and Coordinator have been refactored from a mutexes-and-condition-variables paradigm into a message passing paradigm.  No locks are necessary, because only one thread can access the task state or worker state.  This makes them a lot easier to reason about.  The MockTime class can now handle mocking deferred message passing (adding a message to an ExecutorService with a delay).  I added a MockTimeTest.  MiniTrogdorCluster now starts up Agent and Coordinator classes in paralle in order to minimize junit test time.  RPC messages now inherit from a common Message.java class.  This class handles implementing serialization, equals, hashCode, etc.  Remove FaultSet, since it is no longer necessary.  Previously, if CoordinatorClient or AgentClient hit a networking problem, they would throw an exception.  They now retry several times before giving up.  Additionally, the REST RPCs to the Coordinator and Agent have been changed to be idempotent.  If a response is lost, and the request is resent, no harm will be done.","closed","","cmccabe","2017-10-13T21:08:20Z","2019-05-20T19:07:34Z"
"","3522","MINOR: consume from outputTopic in EosIntegrationTest.runSimpleCopyTest","Previously, the code mistakenly consumed from inputTopic, which worked, but didn't actually verify that the messages were correctly copied from inputTopic to outputTopic.","closed","","dicej","2017-07-12T17:47:03Z","2017-07-12T20:08:18Z"
"","3946","KAFKA-5802 : ScramServerCallbackHandler#handle should check username not being null before calling credentialCache.get()","Pretty trivial and tested with `gradle clean client:test`","open","","umesh9794","2017-09-22T10:05:32Z","2018-03-02T19:30:45Z"
"","3904","[MINOR] Added equals() method to Stamped","Please don't let analyzing this PR take away from actual working time. I did this in my spare time, it's not intended to take away from your working hours. It may well be that this is a non-issue.  As the title states: I added equals() method to Stamped.java because a class that implements Comparable should always have this.  This was pointed out to me by Intellij's code analysis, which warned that classes implementing Comparable should always implement both compareTo() and equals(). This is because an end-user can at some point add objects of that class to java.util.SortedSet. If the compareTo() and equals() implementations are not consistent, that would violate the contract of java.util.Set, which is defined in terms of equals().  findBugs also complained about PunctuationScheduler, a subclass of Stamped, nothing having equals, so I also implemented that.  The equals() in Stamped.java is written to be consistent with how the already-existing compareTo() works in that class. The hashCode() is autogenerated, with only the timestamp field, as that is the only one used by compareTo() and equals().  findBugs complained about PunctuationSchedule.java, a subclass of Stamped.java, also needing these methods if Stamped.java had them. So I had equals() and hashCode() auto-generated for them.  ./gradlew test ran fine.","closed","","KoenDG","2017-09-19T16:06:17Z","2017-11-16T19:11:26Z"
"","4274","KAFKA-6283: Configuration of custom SCRAM SaslServer implementations","Pass the jaasContext to the ScramServerCallbackHandler, so that custom implementations of a SCRAM SaslServer have access to the JAAS configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2017-11-29T17:28:15Z","2017-12-11T14:11:37Z"
"","4075","MINOR: reduce partition state machine debug logging","PartitionStateMachine.electLeaderForPartition logs all partition states in the cluster. This leads to quadratic logging behavior since PartitionStateMachine.electLeaderForPartition itself gets called on a per-partition basis.  This patch reduces the logging so that only the single partition undergoing leader election gets its state logged.","closed","","onurkaraman","2017-10-16T17:39:28Z","2017-11-01T12:13:19Z"
"","3772","KAFKA-5817: Add Serialized class and overloads to KStream#groupBy and KStream#groupByKey","Part of KIP-182 - Add the `Serialized` class - implement overloads of `KStream#groupByKey` and KStream#groupBy` - deprecate existing methods that have more than default arguments","closed","","dguy","2017-08-31T18:26:14Z","2017-09-06T23:20:34Z"
"","3768","KAFKA-5815: add Printed class and KStream#print(printed)","Part of KIP-182 - Add `Printed` class and `KStream#print(Printed)` - deprecate all other `print` and `writeAsText` methods","closed","","dguy","2017-08-31T15:54:34Z","2017-09-08T17:23:33Z"
"","3767","KAFKA-5650: add StateStoreBuilder interface and implementations","Part of KIP-182  - Add `StateStoreBuilder` interface and `WindowStateStoreBuilder`, `KeyValueStateStoreBuilder`, and `SessionStateStoreBuilder` implementations - Add `StoreSupplier`, `WindowBytesStoreSupplier`, `KeyValueBytesStoreSupplier`, `SessionBytesStoreSupplier` interfaces and implementations - Add new methods to `Stores` to create the newly added `StoreSupplier` and `StateStoreBuilder` implementations - Update `Topology` and `InternalTopology` to use the interfaces","closed","","dguy","2017-08-31T14:03:25Z","2017-09-07T08:41:40Z"
"","3988","KAFKA-5967 Ineffective check of negative value in CompositeReadOnlyKeyValueStore#approximateNumEntries()","package name: org.apache.kafka.streams.state.internals Minor change to approximateNumEntries() method in CompositeReadOnlyKeyValueStore class.  long total = 0;    for (ReadOnlyKeyValueStore store : stores) {           total += store.approximateNumEntries();    }  return total < 0 ? Long.MAX_VALUE : total;  The check for negative value seems to account for wrapping. However, wrapping can happen within the for loop. So the check should be performed inside the loop.","closed","","shivsantham","2017-09-28T23:43:48Z","2017-10-04T17:21:09Z"
"","3527","MINOR: Mention systemTestLibs in docker system test instructions","Otherwise NoClassFoundExceptions are thrown for tests that attempt to start MiniKdc.","closed","","ijuma","2017-07-14T14:16:49Z","2017-08-22T06:37:59Z"
"","3701","MINOR: Document how to create source streams and tables","Originally reviewed as part of https://github.com/apache/kafka/pull/3490.","closed","","enothereska","2017-08-20T09:58:10Z","2017-08-22T14:52:09Z"
"","4468","KAFKA-6468 Read replication-offset-checkpoint once","Only read the high watermark checkpoint file (replication-offset-checkpoint) once. Before this patch, this file is read every time the broker handles LeaderAndIsrRequest. See kafka.cluster.Partition#getOrCreateReplica(Int, Boolean).  On my local test cluster of three brokers with around 40k partitions, the initial LeaderAndIsrRequest refers to every partition in the cluster, and it can take 20 to 30 minutes to create all of the replicas because the replication-offset-checkpoint is nearly 2MB.  Changing this code so that we only read this file once on startup reduces the time to create all replicas to around one minute.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","performance,","ambroff","2018-01-24T06:02:03Z","2020-05-05T16:30:39Z"
"","4250","KAFKA-6261: Fix exception thrown by request logging if acks=0","Only expect responseAsString to be set if request logging is enabled _and_ responseSend is defined.  Also fixed a couple of issues that would manifest themselves if trace logging is enabled:  - `MemoryRecords.toString` should not throw exception if data is corrupted - Generate `responseString` correctly if unsupported api versions request is received.  Unit tests were added for every issue fixed. Also changed SocketServerTest to run with trace logging enabled as request logging breakage has been a common issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-22T16:11:27Z","2017-12-22T18:21:39Z"
"","4324","KAFKA-6360: Clear RocksDB Segments when store is closed","Now that we support re-initializing state stores, we need to clear the segments when the store is closed so that they can be re-opened.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2017-12-14T10:18:23Z","2018-06-15T05:17:07Z"
"","3619","MINOR: Update dependencies for 1.0.0 release","Notable updates:  1. Gradle 4.1 includes a number of performance and CLI improvements as well as initial Java 9 support.  2. Scala 2.12.3 has substantial compilation time improvements.  3. lz4-java 1.4 allows us to remove a workaround in KafkaLZ4BlockInputStream (not done in this PR).  4. snappy-java 1.1.4 improved performance of compression (5%) and decompression (20%). There was a slight increase in the compressed size in one of our tests.  Not updated:  1. PowerMock due to a couple of regressions. I investigated one of them and filed https://github.com/powermock/powermock/issues/828.  2. Jackson, which will be done via #3631.  3. Rocksdb, which will be done via #3519.","closed","","ijuma","2017-08-04T01:14:34Z","2017-08-22T06:37:09Z"
"","3738","KAFKA-5790: SocketServer.processNewResponses should not skip a response if exception is thrown","Not sure why the try/finally was there in the first place, but it seems to cause the next response after an exception to be skipped.  This could be a source of correlation id errors we've seen before as well.","closed","","ijuma","2017-08-25T10:16:53Z","2017-09-05T08:37:46Z"
"","4044","KAFKA-6026: Fix for indefinite wait in KafkaFutureImpl","Not passing 0 to wait in KafkaFuture implementation. The case where the timeout argument is 0 is already taken care of in the definition of waitTimeMs which is at least 1. The contribution is my original work and I license the work to the project under the project's open source license","closed","","bartdevylder","2017-10-09T15:12:11Z","2017-10-10T01:00:00Z"
"","4320","[WIP] Add logs to debug testHighConcurrencyModificationOfResourceAcls test case","Not able to reproduce locally. added few logs to check on jenkins run looks like some synchronization issue  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-12-13T10:29:25Z","2018-07-03T15:44:41Z"
"","3751","KAFKA-4915: do not use word ""error"" while logging with non-error level","Normally if logging event is an error, then it should have log level ""ERROR"". If it's not an error, it should not have ""Error"" in the log message.  So I changed wordings a bit.  Also I've deliberately skipped several places, where error is the actual entity name received/sent.","open","","dernasherbrezon","2017-08-28T19:54:54Z","2018-03-02T19:30:34Z"
"","3711","MINOR: Stateless transformation documentation","Needs to come after https://github.com/apache/kafka/pull/3701 Originally reviewed as part of #3490.","closed","","enothereska","2017-08-22T08:52:49Z","2017-08-24T14:11:09Z"
"","4039","MINOR: Bump the request timeout for the transactional message copier","Multiple inflights means that when there are rolling bounces or other cluster instability, there is an increased likelihood of having previously tried batch expire in the accumulator. This is a fatal error for a transactional producer, causing the `TransactionalMessageCopier` to exit. To work around this, we bump the request timeout. We can get rid of this when KIP-91 is merged.","closed","","apurvam","2017-10-07T17:51:34Z","2017-10-12T23:56:14Z"
"","3908","KAFKA-5933: Move timeout and validate_only protocol fields into CommonFields class","Most of the fields which are shared by multiple protocol messages (requests ad responses) are in the CommonFields class. However there are still some fields used multiple times which are not there yet: * `timeout` * `validate_only`  It would be good to move also these two fields into the CommonFields class so that they can be easily shared by different messages.","open","","scholzj","2017-09-19T19:14:21Z","2018-03-02T19:30:44Z"
"","4311","KAFKA-6298 - Line numbers on log messages are incorrect","Modified LogContext.KafkaLogger to add support for location aware logging. If LocationAwareLogger is not available then fallback to the Logger.","closed","","mrnakumar","2017-12-11T15:47:29Z","2018-01-04T16:59:54Z"
"","4200","MINOR: Update to Scala 2.11.12 (part 2)","Missed a few places in the previous commit.","closed","","ijuma","2017-11-09T16:47:45Z","2017-12-22T18:21:45Z"
"","4085","HOTFIX: poll with zero millis during restoration","Mirror of #4096 for 0.11.01.  During the restoration phase, when thread state is still in PARTITION_ASSIGNED not RUNNING yet, call poll() on the normal consumer with 0 millisecond timeout, to unblock the restoration of other tasks as soon as possible.","closed","","guozhangwang","2017-10-17T22:07:55Z","2017-11-07T23:23:34Z"
"","4086","[WIP] KAFKA-6085: Pause all partitions before tasks are initialized","Mirror of #4085 against trunk. This PR contains two fixes (one major and one minor):  Major: on rebalance, pause all partitions instead of the partitions for tasks with state stores only, so that no records will be returned in the same `pollRecords()` call.  Minor: during the restoration phase, when thread state is still PARTITION_ASSIGNED, call consumer.poll with hard-coded pollMs = 0.","closed","","guozhangwang","2017-10-18T01:28:12Z","2017-10-19T05:11:35Z"
"","4265","KAFKA-6065: Latency metric for KafkaZkClient","Measures the latency of each request.  Updated existing `ZkUtils` test to use `KafkaZkClient` instead.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-26T01:33:54Z","2017-12-22T18:21:22Z"
"","3537","KAFKA-5599: ConsoleConsumer : --new-consumer option as deprecated","Marking --new-consumer as deprecated in the help of ConsoleConsumer Printing a warning on using the new consumer but adding the --new-consumer option","closed","","ppatierno","2017-07-17T10:58:16Z","2017-08-04T11:52:52Z"
"","4432","KAFKA-6456 JavaDoc clarification for SourceTask#poll()","Making clear that implementations of poll() shouldn't block indefinitely in order to allow the task instance to transition to PAUSED state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gunnarmorling","2018-01-17T10:30:48Z","2020-10-16T06:24:14Z"
"","4001","KAFKA-6001: remove  from Materialized usages","Make the API simpler by removing `` from usages of `Materialized`. This is already enforced when by `Materialized.as(KeyValueBytesStore)` etc.","closed","","dguy","2017-10-02T15:59:26Z","2017-10-04T21:02:06Z"
"","3729","KAFKA-5749: Add MeteredSessionStore and ChangeloggingSessionBytesStore.","Make MeteredSessionStore the outermost store.","closed","","dguy","2017-08-24T15:17:24Z","2017-08-26T08:10:19Z"
"","4102","MINOR: Update Scala to 2.12.4","Mainly for Java 9 fixes and improved compilation times (5-10% reduction):  http://www.scala-lang.org/news/2.12.4","closed","","ijuma","2017-10-20T10:00:32Z","2017-12-22T18:23:46Z"
"","4041","KAFKA-6023 ThreadCache#sizeBytes() should check overflow","long sizeBytes() {         long sizeInBytes = 0;         for (final NamedCache namedCache : caches.values()) {             sizeInBytes += namedCache.sizeInBytes();         }         return sizeInBytes;     } The summation w.r.t. sizeInBytes may overflow. Check similar to what is done in size() should be performed.","closed","","shivsantham","2017-10-09T01:48:58Z","2017-10-18T17:49:59Z"
"","3740","MINOR: reduce logging to trace in NetworkClient when an old server API is being used","logging in `NetworkClient#doSend` at debug level is spamming the logs when you have a producer that is sending many requests. It makes it extremely difficult to debug. Reducing to trace to remove the noise from the logs","closed","","dguy","2017-08-25T11:27:55Z","2017-10-04T21:52:48Z"
"","4465","KAFKA-6244: Dynamic update of log cleaner configuration","Log cleaner config update as described in KIP-226. Config updates are handled by stopping cleaner threads and starting new ones with the new config. This keeps the code simple and ensures that if any of the threads had terminated earlier due to an exception, new ones are created to match the configured thread count.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-24T00:18:04Z","2018-01-26T22:38:47Z"
"","3762","KAFKA-5807 - Check Connector.config() and Transformation.config() returns a valid ConfigDef","Little back story on this. Was helping a user over email. This could be much easier to debug if we assume that the connector developer might not return valid configs. For example Intellij will generate a stub that returns a null. This was the case that inspired this JIRA.","closed","connect,","jcustenborder","2017-08-30T21:00:08Z","2018-05-22T20:57:44Z"
"","4454","KIP-222 - Add Consumer Group operations to Admin API","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-222+-+Add+Consumer+Group+operations+to+Admin+API","closed","","jeqo","2018-01-21T23:18:15Z","2020-08-08T09:20:55Z"
"","4159","KAFKA-5520: KIP-171 - Extend Consumer Group Reset Offset for Stream Application - Updated","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application  Merge changes from KIP-198  Ref: https://github.com/apache/kafka/pull/3831","closed","","jeqo","2017-10-30T13:24:29Z","2020-08-08T09:20:31Z"
"","3831","KAFKA-5520: KIP-171 - Extend Consumer Group Reset Offset for Stream Application","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application","closed","","jeqo","2017-09-11T23:19:00Z","2020-08-08T09:20:33Z"
"","4327","KAFKA-6370: KafkaMetricsGroup.toScope should filter out tags null value","KafkaMetricsGroup.toScope should filter out tags with value of `null` to avoid NullPointerException thrown.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2017-12-15T08:07:45Z","2018-02-02T03:35:35Z"
"","3512","KAFKA-5574: add thread.id header in show-detailed-stats report","kafka-consumer-perf-test.sh report missing one header column:   time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec 2017-07-09 21:40:40:369, 0, 0.1492, 2.6176, 5000, 87719.2982 2017-07-09 21:40:40:386, 0, 0.2983, 149.0479, 10000, 5000000.0000 2017-07-09 21:40:40:387, 0, 0.4473, 149.0812, 15000, 5000000.0000 there's one more column between ""time"" and ""data.consumed.in.MB"", should be thread.id .","closed","","cnZach","2017-07-10T05:40:29Z","2017-10-23T07:18:41Z"
"","4306","KAFKA-6331; Fix transient failure in AdminClientIntegrationTest.testAlterReplicaLogDirs","KAFKA-6331; Fix transient failure in AdminClientIntegrationTest.testAlterReplicaLogDirs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lindong28","2017-12-08T18:36:03Z","2018-03-14T19:15:44Z"
"","4251","KAFKA-6253: Improve sink connector topic regex validation","KAFKA-3073 added topic regex support for sink connectors. The addition requires that you only specify one of topics or topics.regex settings. This is being validated in one place, but not during submission of connectors. This PR adds validation at `AbstractHerder.validateConnectorConfig` and `WorkerConnector.initialize`.  This adds a test of the new behavior to `AbstractHerderTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","jklukas","2017-11-22T18:19:19Z","2020-10-16T06:24:12Z"
"","3765","KAFKA-5642: Use async ZookeeperClient in Controller","Kafka today uses ZkClient, a wrapper client around the raw Zookeeper client. This library only exposes synchronous apis to the user. Synchronous apis mean we must wait an entire round trip before doing the next operation.  This becomes problematic with partition-heavy clusters, as we find the controller spending a significant amount of time just sending many sequential reads and writes to zookeeper at the per-partition granularity. This especially becomes an issue during: - controller failover, where the newly elected controller effectively reads all zookeeper state. - broker failures and controlled shutdown. The controller tries to elect a new leader for partitions previously led by the broker. The controller also removes the broker from isr on partitions for which the broker was a follower. These all incur partition-granular reads and writes to zookeeper.  As a first step in addressing these issues, we built a low-level wrapper client called ZookeeperClient in KAFKA-5501 that encourages pipelined, asynchronous apis.  This patch converts the controller to use the async ZookeeperClient to improve controller failover, broker failure handling, and controlled shutdown times.  Some notable changes made in this patch: - All ControllerEvents now defer access to zookeeper at processing time instead of enqueue time as was intended with the single-threaded event queue model patch from KAFKA-5028. This results in a fresh view of the zookeeper state by the time we process the event. This reverts the hacks from KAFKA-5502 and KAFKA-5879. - We refactored PartitionStateMachine and ReplicaStateMachine to process multiple partitions and replicas in batch rather than one-at-a-time so that we can send a batch of requests over to ZookeeperClient to pipeline. - We've decouple ZookeeperClient handler registration from watcher registration. Previously, these two were coupled, which meant handler registrations actually sent out a request to the zookeeper ensemble to do the actual watcher registration. In KafkaController.onControllerFailover, we register partition modification handlers (thereby registering watchers) and additionally lookup the partition assignments for every topic in the cluster. We can shave a bit of time off failover if we merge these two operations. We can do this by decoupling ZookeeperClient handler registration from watcher registration. This means ZookeeperClient's registration apis have been changed so that they are purely in-memory operations, and they only take effect when the client sends ExistsRequest, GetDataRequest, or GetChildrenRequest. - We've simplified the logic for updating LeaderAndIsr such that if we get a BADVERSION error code, the controller will now just retry in the next round by reading the new state and trying the update again. This simplifies logic when updating the partition leader epoch, removing replicas from isr, and electing leaders for partitions. - We've implemented KAFKA-5083: always leave the last surviving member of the ISR in ZK. This means that if people re-disabled unclean leader election, we can still try to elect the leader from the last in-sync replica. - ZookeeperClient's handlers have been changed so that their methods default to no-ops for convenience. - All znode paths and definitions for znode encoding and decoding have been consolidated as static methods in ZkData.scala. - The partition leader election algorithms have been refactored as pure functions so that they can be easily unit tested. - PartitionStateMachine and ReplicaStateMachine now have unit tests.","closed","","onurkaraman","2017-08-31T07:10:10Z","2017-11-01T12:26:22Z"
"","4131","Remove maven central repository, use only jcenter","jcenter is a super set on top of maven central, so having both of those repositories is redundant, and the preferred one should be jcenter.","open","","yinonavraham","2017-10-25T08:46:55Z","2018-03-02T19:30:53Z"
"","4287","KAFKA-6118: Fix transient failure testTwoConsumersWithDifferentSaslCredentials","It's rare, but it can happen that the initial FindCoordinator request returns before the first Metadata request. Both authorization errors are fine for this test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-12-02T06:04:58Z","2017-12-02T07:55:36Z"
"","3801","MINOR: Include response in request log","It's implemented such that there is no overhead if request logging is disabled.  Also: - Reduce metrics computation duplication in `updateRequestMetrics` - Change a couple of log calls to use string interpolation instead of `format` - Fix a few compiler warnings related to unused imports and unused default arguments.","closed","","ijuma","2017-09-06T12:43:18Z","2017-09-17T15:20:36Z"
"","4262","MINOR: Temporarily disable testLogStartOffsetCheckpoint","It's failing often and it seems like there are multiple reasons. PR #4238 will re-enable it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-24T14:31:43Z","2017-12-22T18:21:37Z"
"","4172","MINOR: Remove clients/out directory","It was committed inadvertently.","closed","","ijuma","2017-11-02T20:34:59Z","2017-12-22T18:22:01Z"
"","4297","KAFKA-6317: Maven artifact for kafka should not depend on log4j","It should only depend on slf4j-api (like kafka-clients). The release tarball still includes log4j and slf4j-log4j12.  Manually verified that there are no duplicate dependencies in the release tarball and `./gradlew core:dependencies` looks good.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-12-06T17:00:37Z","2017-12-22T18:13:54Z"
"","3989","KAFKA-5746; Fix conversion count computed in `downConvert`","It should be the number of records instead of the number of batches.  A few additional clean-ups: - Minor renames - Removed unused variable - Some test fixes - Ignore a flaky test","closed","","ijuma","2017-09-29T01:44:49Z","2017-12-22T18:22:21Z"
"","4117","MINOR: Configure owasp.dependencycheck gradle plugin","It seems to output a few false positives, but still worth verifying.","closed","","ijuma","2017-10-23T10:51:23Z","2017-12-22T18:23:43Z"
"","4202","MINOR: Exclude Committer Checklist section from commit message","It seems like it's sufficient to be able to refer to it in the PR.  ### Committer Checklist - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-10T11:51:36Z","2017-12-22T18:21:43Z"
"","4022","KAFKA-6015: Fix NullPointerException in RecordAccumulator","It is possible for batches with sequence numbers to be in the `deque` while at the same time the in flight batches in the `TransactionManager` are removed due to a producerId reset.  In this case, when the batches in the `deque` are drained, we will get a `NullPointerException` in the background thread due to this line:   ```java if (first.hasSequence() && first.baseSequence() != transactionManager.nextBatchBySequence(first.topicPartition).baseSequence()) ```  Particularly, `transactionManager.nextBatchBySequence` will return null, because there no inflight batches being tracked.   In this patch, we simply allow the batches in the `deque` to be drained if there are no in flight batches being tracked in the TransactionManager. If they succeed, well and good. If the responses come back with an error, the batces will be ultimately failed in the producer with an `OutOfOrderSequenceException` when the response comes back.","closed","","apurvam","2017-10-05T06:39:18Z","2017-10-06T22:06:11Z"
"","3744","Add group-id to the metrics tags","It is better to have group-id in the JMX metrics. It would improve debuggability of systems: ```kafka_consumer_consumer_fetch_manager_metrics_test_0_records_lag{client_id=""consumer-1"",group_id=""group-1"",instance=""service:8080"",job=""prometheus""}```","open","","EtaCassiopeia","2017-08-25T20:12:28Z","2018-06-21T08:22:09Z"
"","3728","Add group-id to the metrics tags","It is better to have group-id in the JMX metrics. It would improve debuggability of systems: ```kafka_consumer_consumer_fetch_manager_metrics_test_0_records_lag{client_id=""consumer-1"",group_id=""group-1"",instance=""service:8080"",job=""prometheus""}```","closed","","EtaCassiopeia","2017-08-24T14:08:29Z","2017-08-25T19:40:36Z"
"","3923","MINOR: Upgrade to Gradle 4.2","It includes the usual performance improvements, but the nicest improvement for me is that the findBugs plugin no longer outputs 10000+ lines in Jenkins builds:  https://docs.gradle.org/4.2/release-notes.html#findbugs-plugin-does-not-render-analysis-progress-anymore","closed","","ijuma","2017-09-20T15:51:07Z","2017-12-22T18:22:34Z"
"","3856","MINOR: Remove unused SecurityProtocol.TRACE","It adds complexity for no benefit since we don't use it anywhere.  Also removed a few unused imports, variables and default parameters.","closed","","ijuma","2017-09-14T09:40:13Z","2017-09-17T15:20:34Z"
"","3634","MINOR: Deprecate LogConfig.Compact","It actually refers to the `delete` cleanup policy.","closed","","ijuma","2017-08-07T11:34:43Z","2017-08-22T06:36:55Z"
"","3876","KAFKA-5896: Force Connect tasks to stop via thread interruption","Interrupt the thread of Kafka Connect tasks that do not stop within the timeout via `Worker::stopAndAwaitTasks()`. Previously tasks would be asked to stop via setting a `stopping` flag. It was possible for tasks to ignore this flag if they were, for example, waiting for a lock or blocked on I/O.  This prevents issues where tasks may end up with multiple threads all running and attempting to make progress when there should only be a single thread running for that task at a time.  Fixes KAFKA-5896  /cc @rhauch @tedyu","closed","connect,","56quarters","2017-09-15T18:33:37Z","2018-05-01T16:00:32Z"
"","4415","KAFKA-6205: initialize topology after state stores restoration completed","Initialize topology after state store restoration.   Although IMHO updating some of the existing tests demonstrates the correct order of operations, I'll probably add an integration test, but I wanted to get this PR in for feedback on the approach.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage, and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2018-01-11T19:27:11Z","2018-06-13T00:28:42Z"
"","4238","KAFKA-6234: Increased timeout value for lowWatermark response to avoid test failing occasionally","Increase timeout to fix flaky integration test testLogStartOffsetCheckpoint.","closed","","soenkeliebau","2017-11-20T06:30:05Z","2018-04-12T22:45:39Z"
"","4312","MINOR: Increase number of messages in replica verification tool test","Increase the number of messages produced to make the test more reliable. The test failed in a recent build and also fails intermittently when run locally. Since the producer uses acks=0 and the test stops as soon as a lag is observed, the change shouldn't have a big impact on the time taken to run when lag is observed sooner.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2017-12-11T17:08:11Z","2017-12-11T19:16:36Z"
"","4364","KAFKA-6256: fix flaky test KStreamKTableJoinIntegrationTest.shouldCountClicksPerRegionWithNonZeroByteCache","Increase commit interval to make it less likely that we flush the cache in-between. To make it fool-proof, only compare the ""final"" result records if cache is enabled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-12-29T01:14:15Z","2018-01-03T19:12:42Z"
"","4291","MINOR: increase request timeout for streams bounce test","Increase `REQUEST_TIMOUT_MS` to improve a flaky system test until KIP-91 merged    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2017-12-04T14:36:33Z","2017-12-04T17:58:54Z"
"","4038","[MINOR] Include isolationLevel in toString of FetchRequest","Include `isolationLevel` in `toString` of `FetchRequest`  This is a follow-up to https://issues.apache.org/jira/browse/KAFKA-4818.","closed","","jaceklaskowski","2017-10-07T14:53:39Z","2017-10-14T08:13:20Z"
"","3589","KAFKA-3623: KStreamTestDriver extends ExternalResource","In the streams project, there are a number of unit tests that has duplicate code with respect to the tearDown() method, in which it tries to close the KStreamTestDriver connection. The goal of this changeset is to eliminate this duplication by converting the KStreamTestDriver class to an ExternalResource class which is the base class of JUnit Rule.  In every unit tests that calls KStreamTestDriver, we annotate the KStreamTestDriver using @Rule annotation. In the KStreamTestDriver class, we override the after() method. This after() method in turn calls the close() method which was previously called in the tearDown() method in the unit tests. By annotating the KStreamTestDriver as a @Rule, the after() method will be called automatically after every testcase.","closed","","johnma14","2017-07-27T22:20:52Z","2017-08-01T23:17:41Z"
"","4400","KAFKA-6303: Potential lack of synchronization in NioEchoServer","In the run() method:                                  SocketChannel socketChannel = ((ServerSocketChannel) key.channel()).accept();                                 socketChannel.configureBlocking(false);                                 newChannels.add(socketChannel);  Modification to newChannels should be protected by synchronized block.  - [ ] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","shivsantham","2018-01-07T06:36:43Z","2018-01-08T12:16:53Z"
"","3755","KAFKA-5806. Fix transient unit test failure in trogdor coordinator shutdown","In the coordinator, we should check that 'shutdown' is not true before going to sleep waiting for the condition.","closed","","cmccabe","2017-08-29T19:33:24Z","2019-05-20T19:08:59Z"
"","4451","KAFKA-6461 TableTableJoinIntegrationTest is unstable if caching is enabled","In test output, NPE is observed on the following line in apply() method: ```             if (value.equals(expected)) { ``` We should check that value is not null before calling equals()  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-01-19T22:23:00Z","2018-01-23T23:21:40Z"
"","4109","KAFKA-6024 - Move validation in KafkaConsumer ahead of acquireAndEnsu…","In several methods, parameter validation is done after calling acquireAndEnsureOpen() in Kafka Consumer :      public void seek(TopicPartition partition, long offset) {         acquireAndEnsureOpen();         try {             if (offset < 0)                 throw new IllegalArgumentException(""seek offset must not be a negative number"");  Since the value of parameter would not change per invocation, it seems performing validation ahead of acquireAndEnsureOpen() call would be better.","closed","","shivsantham","2017-10-21T07:07:52Z","2018-02-25T06:48:54Z"
"","4298","Remove ignore-case grep option from kafka-server-stop.sh","In order to avoid killing kafka-manager running on the same machine, because it will match these strings from command line: lib/org.apache.kafka.kafka_2.11-0.10.0.1.jar lib/org.apache.kafka.kafka-clients-0.10.0.1.jar  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sm4rk0","2017-12-06T17:29:29Z","2019-02-08T11:18:21Z"
"","3675","KAFKA-5152: perform state restoration in poll loop","In onPartitionsAssigned:  release all locks for non-assigned suspended tasks. resume any suspended tasks. Create new tasks, but don't attempt to take the state lock. Pause partitions for any new tasks. set the state to PARTITIONS_ASSIGNED In StreamThread#runLoop  poll if state is PARTITIONS_ASSIGNED 2.1 attempt to initialize any new tasks, i.e, take out the state locks and init state stores 2.2 restore some data for changelogs, i.e., poll once on the restore consumer and return the partitions that have been fully restored 2.3 update tasks with restored partitions and move any that have completed restoration to running 2.4 resume consumption for any tasks where all partitions have been restored. 2.5 if all active tasks are running, transition to RUNNING and assign standby partitions to the restoreConsumer.","closed","","dguy","2017-08-16T13:28:26Z","2017-08-22T18:14:35Z"
"","4449","MINOR: Fix MetadataResponse#toStruct serialization for null leaders","In MetadataResponse deserialization, if the partition leader key is set to -1, the leader is set to null.  The MetadataResponse#toStruct code should handle this correctly as well.  RequestResponseTest should test this as well.","closed","","cmccabe","2018-01-19T18:48:08Z","2019-05-20T18:57:33Z"
"","4292","Is there something wrong? (for kafka 1.0)","In kafka stream source code: InternalTopologyBuilder.java  ![image](https://user-images.githubusercontent.com/30587995/33597792-79425f28-d9db-11e7-81f6-15e7f81df264.png)  I think it should be: **globalGroups.add(node);**","closed","","helowken","2017-12-05T08:44:44Z","2017-12-22T20:10:10Z"
"","3982","MINOR: Fix error message in exception when records have schemas in Connect's Flatten transformation","In case of an error while flattening a record with schema, the Flatten transform was reporting an error about a record without schema, as follows:   ``` org.apache.kafka.connect.errors.DataException: Flatten transformation does not support ARRAY for record without schemas (for field ...) ```  The expected behaviour would be an error message specifying ""with schemas"".   This looks like a simple copy/paste typo from the schemaless equivalent methods, in the same file","closed","connect,","sv3ndk","2017-09-28T11:52:54Z","2020-03-29T04:33:20Z"
"","3653","KAFKA-5152: move state restoration out of rebalance and into poll loop","In `onPartitionsAssigned`:  1. release all locks for non-assigned suspended tasks. 2. resume any suspended tasks. 3. Create new tasks, but don't attempt to take the state lock. 4. Pause partitions for any new tasks. 5. set the state to `PARTITIONS_ASSIGNED`  In `StreamThread#runLoop` 1. poll 2. if state is `PARTITIONS_ASSIGNED`  2.1  attempt to initialize any new tasks, i.e, take out the state locks and init state stores  2.2 restore some data for changelogs, i.e., poll once on the restore consumer and return the partitions that have been fully restored  2.3 update tasks with restored partitions and move any that have completed restoration to running  2.4 resume consumption for any tasks where all partitions have been restored.  2.5 if all active tasks are running, transition to `RUNNING` and assign standby partitions to the restoreConsumer.","closed","","dguy","2017-08-10T11:46:30Z","2017-08-16T10:14:34Z"
"","3905","MINOR: change task initialization logging levels","In `AssignedTasks` log at debug all task ids that are yet to be initialized. In `StreamsTask` log at trace when the task is initialized.","closed","","dguy","2017-09-19T16:17:55Z","2017-09-20T11:08:39Z"
"","4206","KAFKA-6122: Global Consumer should handle TimeoutException","Implements KIP-224: - adding new StreamsConfig `retires` - uses `retires` and `retry.backoff.ms` to handle TimeoutException in GlobalStateManager - adds two new tests to trigger TimeoutException in global consumer - some minor code cleanup to reduce number of parameters we need to pass around","closed","","mjsax","2017-11-10T21:13:07Z","2017-11-20T17:58:25Z"
"","3686","KAFKA-5738: Add cumulative count for rate metrics (KIP-187)","Implementation of https://cwiki.apache.org/confluence/display/KAFKA/KIP-187+-+Add+cumulative+count+metric+for+all+Kafka+rate+metrics  Also made locking in Sensor for `CompoundStat` consistent with simple `Stat`, avoiding locking the whole sensor.","closed","","rajinisivaram","2017-08-17T10:58:46Z","2017-09-14T23:09:07Z"
"","4305","KAFKA-6318: StreamsResetter should return non-zero return code on error","If users specify a non-existing topic as input parameter, StreamsResetter only prints out an error message that the topic was not found, but return code is still zero. We should return a non-zero return code for this case.","closed","","shivsantham","2017-12-08T18:08:34Z","2018-01-02T18:46:07Z"
"","4219","MINOR: Shutdown ControllerEventThread via event instead of interruption","If the ControllerEventThread is interrupted when a request is being sent, it may lead to an IllegalStateException being thrown. This, in turn, can lead to a NullPointerException in unregisterPartitionReassignmentIsrChangeHandlers,  To avoid these issues, we make the ControllerEventThread uninterruptable and we shut it down by clearing the queue and enqueuing a special event.  To make the code more robust, we also set ReassignedPartitionsContext.reassignIsrChangeHandler during construction instead of setting it to null first.  Finally, misleading log messages in ephemeral node creation have been clarified.  For reference, the relevant log lines from the relevant flaky test:  ```text [2017-11-15 10:30:13,869] ERROR Error while creating ephemeral at /controller with return code: OK (kafka.zk.KafkaZkClient$CheckedEphemeral:101) [2017-11-15 10:30:14,155] ERROR Haven't been able to send leader and isr requests, current state of the map is Map(101 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 100 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 102 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false))). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101) [2017-11-15 10:30:14,156] ERROR Haven't been able to send metadata update requests to brokers Set(102, 103, 104, 101, 105), current state of the partition info is Map(topic1-0 -> PartitionState(controllerEpoch=1, leader=101, leaderEpoch=2, isr=[101], zkVersion=2, replicas=[100, 102, 101], offlineReplicas=[100])). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101) [2017-11-15 10:30:14,158] ERROR [Controller id=101] Forcing the controller to resign (kafka.controller.KafkaController:101) [2017-11-15 10:30:14,158] ERROR [Controller id=101] Error completing reassignment of partition topic1-0 (kafka.controller.KafkaController:107) java.lang.NullPointerException 	at kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:784) 	at kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:783) ```  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-15T15:03:26Z","2017-12-22T18:21:28Z"
"","3838","[KAFKA-1194 Invokes unmap if on Windows OS]","If on Windows OS, forces unmapping of mmap so that segment suffixes can be changed.","closed","","manmedia","2017-09-12T17:51:13Z","2019-07-03T10:34:49Z"
"","4304","KAFKA-6323: document that punctuation is called immediately.","If KAFKA-6323 is not a bug, then it needs better documentation.  Alternative to https://github.com/apache/kafka/pull/4301  @mihbor @mjsax","closed","","fredfp","2017-12-08T03:34:54Z","2017-12-20T09:33:07Z"
"","3747","KAFKA-5787: StoreChangelogReader needs to restore partitions that were added post initialization","If a task fails during initialization due to a LockException, its changelog partitions are not immediately added to the StoreChangelogReader as the thread doesn't hold the lock. However StoreChangelogReader#restore will be called and it sets the initialized flag. On a subsequent successfull call to initialize the new tasks the partitions are added to the StoreChangelogReader, however as it is already initialized these new partitions will never be restored. So the task would remain in a non-running state forever.","closed","","dguy","2017-08-27T09:58:26Z","2017-08-29T08:45:33Z"
"","3736","KAFKA-5787: StoreChangelogReader needs to restore partitions that were added post initialization","If a task fails during initialization due to a LockException, its changelog partitions are not immediately added to the StoreChangelogReader as the thread doesn't hold the lock. However StoreChangelogReader#restore will be called and it sets the initialized flag. On a subsequent successfull call to initialize the new tasks the partitions are added to the StoreChangelogReader, however as it is already initialized these new partitions will never be restored. So the task would remain in a non-running state forever.","closed","","dguy","2017-08-25T09:38:16Z","2017-08-29T17:50:23Z"
"","3931","Minor: Fix testUniqueErrorCodes unit test failure","I'm not really sure if I'm doing the right thing here, but I thought I'd give it a shot and try to fix the unit test breakage. Running ```./gradlew :clients:unitTest``` now passes.  Fix error code collision accidentally introduced when merging in https://github.com/apache/kafka/commit/5f6393f9b17cce17ded7a00e439599dfa77deb2d#diff-b119227df7efa3ffeb7fe69e49ff1afeR541  There were two Error 59's  Requesting review by @tombentley and @ijuma","closed","","wushujames","2017-09-21T07:24:11Z","2017-09-21T07:41:18Z"
"","3886","MINOR: Code cleanup, subject: log statements.","I'm doing this in my spare time, so don't let reviewing this PR take away actual work time. This is just me going over the code with the Intellij analyzer and implementing the most easily implementable fixes.  This PR is focused only on seemingly erronous log statements.  1: A log statement that has 4 arguments supplied but only 3 `{}` statements  2: A log statement that checks is debug is enabled, but then logs on `info` level.","closed","","KoenDG","2017-09-18T09:37:48Z","2017-09-18T11:32:21Z"
"","3552","MINOR: Simplify SensorAccess usage","I was investigating an exception in this code and found a few opportunities for making it clearer.  I also added the `out` folder to `.gitignore` as IntelliJ sometimes uses that as the build folder.","closed","","ijuma","2017-07-20T10:58:01Z","2017-08-22T06:39:35Z"
"","3799","KAFKA-5597 Alternate way to do Metrics docs generation","I was about to start on the next round of autogeneration of metrics docs, but I wanted to @guozhangwang 's opinion on this first. This is a possible alternate way to do autogeneration of metrics docs, that possibly looks a little nicer for the developer. Posting this to get some feedback on if the original way looks better, or if this new way looks better.  Instead of having the metrics registry and the org.apache.kafka.common.metrics.Metrics object be separate things, have the metrics registry hold a copy of the Metrics object. That way, all the metricInstance stuff is hidden, and we don't have to make sure that the metrics registry and the Metrics object are configured identicailly (with the same tags).  I personally think this looks a little better.","closed","","wushujames","2017-09-06T07:32:39Z","2017-09-27T22:33:42Z"
"","3581","KAFKA-5648: make Merger extend Aggregator","I suggest that Merger should extend Aggregator. reason: Both classes usually do very similar things. A merger takes two sessions and combines them, an aggregator takes an existing session and aggregates new values into it. in some use cases it is actually the same thing, e.g.:  -> .map() to > -> .groupByKey().aggregate() to > In this case both merger and aggregator do the same thing: take two lists and combine them into one. With the proposed change we could pass the Merger as both the merger and aggregator to the .aggregate() method and keep our business logic within one merger class. Or in other words: The Merger is simply an Aggregator that happens to aggregate two objects of the same class","open","kip,","cvaliente","2017-07-26T15:09:52Z","2020-06-12T23:04:29Z"
"","3777","KAFKA-5820 Remove unneeded synchronized keyword in StreamThread","I removed synchronized keyword from 3 methods.  I ran the change thru streams module where test suite passed.","closed","","tedyu","2017-09-01T15:30:15Z","2017-09-08T00:10:16Z"
"","4111","KAFKA-6073: Use ZookeeperClient in KafkaApis","I kept zkUtils for the call to AdminUtils.createTopic(). AdminUtils can be done in another PR.  Is there a reason why we use TopicAndPartition instead of TopicPartition in KafkaControllerZkUtils ?","closed","","mimaison","2017-10-21T18:46:47Z","2018-04-18T13:27:48Z"
"","3880","KAFKA-5765 Move merge() from StreamsBuilder to KStream","I have defined a {{merge()}} method to KStream.  KStreamImpl overrides the {{merge()}} method.","closed","","ConcurrencyPractitioner","2017-09-16T23:42:37Z","2017-09-20T02:15:42Z"
"","3745","[KAFKA-4468] Correctly calculate the window end timestamp after read from state stores","I have decided to use the following approach to fixing this bug:  1) Since the Window Size in WindowedDeserializer was originally unknown, I have initialized a field _windowSize_ and created a constructor to allow it to be instantiated  2) The default size for __windowSize__ is _Long.MAX_VALUE_. If that is the case, then the  deserialize method will return an Unlimited Window, or else will return Timed one.  3) Temperature Demo was modified to demonstrate how to use this new constructor, given that the window size is known.","closed","","ConcurrencyPractitioner","2017-08-26T20:25:43Z","2017-09-12T23:43:36Z"
"","4065","MINOR: Use OS-assigned port, in case 9092 is already bound","I found this by running the tests while I happened to have a kafka broker running.  @mjsax please could you review?","closed","","tombentley","2017-10-12T11:57:17Z","2017-10-12T13:25:00Z"
"","3883","KAFKA-5918: Fix minor typos and errors in the Kafka Streams turotial","I found several minor issues with the Kafka Streams tutorial: * Some typos   * ""As shown above, it illustrate that the constructed ..."" instead of ""As shown above, it illustrate_s_ that the constructed ...""   * ""same as Pipe.java below"" instead of ""same as Pipe.java _above_""   * Wrong class name in the `LineSplit` example * Incorrect imports for the code examples   * Missing `import org.apache.kafka.streams.kstream.KStream;` in `LineSplit` and `WordCount` example * Unnecessary (and potentially confusing) split by whitespaces in the `WorkCount` class (the split into words happened already in `LineSplit`)","closed","","scholzj","2017-09-17T21:27:54Z","2017-09-18T23:59:36Z"
"","3518","WIP: SimpleRegexAclAuthorizer","I extended the SimpleAclAuthorizer to support regex. In order to keep backwards compatibility, I added a prefix to the Resource name (default is ""r:"") to allow otherwise-valid regex patterns to be used as Resource names without applying regex matching logic.  Given the criticality of this, any feedback would be _greatly_ appreciated.  I had to move a bunch of stuff from private -> protected in order to maximize code reuse.","closed","","nnordrum","2017-07-11T04:53:22Z","2021-06-23T16:05:05Z"
"","4475","[KAFKA-6485] 'ConsumerGroupCommand' performance optimization for old consumer describe group","https://issues.apache.org/jira/browse/KAFKA-6485  ConsumerGroupCommand describegroup performance optimization. performance improvement 3 times compare trunk(1.0+). and performance improvement 10 times compare 0.10.2.1  `./kafka-consumer-groups.sh --zookeeper 127.0.0.1 --group cp-mirror-consumer-group --describe ` below is truck version: ![1ada9efa5-850a-4495-83b6-9a28f8885512](https://user-images.githubusercontent.com/231336/35393921-4f49c274-0221-11e8-94dc-99165cd7dc36.png)  below is  performance improvement 3 times: ![75a51a9f-d2c3-47fa-81d3-519343aa4d84](https://user-images.githubusercontent.com/231336/35393939-595fb5b6-0221-11e8-834a-31b773333f6e.png)  below is  performance improvement 10 times for 0.10.2.1: ![3a89bcef-ac5f-4383-94ae-36897ae2da26](https://user-images.githubusercontent.com/231336/35394082-b73ce528-0221-11e8-822e-3c8d843547e4.png)","closed","","leonhong","2018-01-25T14:30:29Z","2019-01-05T10:34:36Z"
"","4390","Menu order in Streams docs incorrect","https://issues.apache.org/jira/browse/KAFKA-6419  Related: https://github.com/apache/kafka-site/pull/115","closed","","joel-hamill","2018-01-04T17:57:03Z","2018-01-08T19:45:14Z"
"","3743","KAFKA-5494: enable idempotence with max.in.flight.requests.per.connection > 1","Here we introduce client and broker changes to support multiple inflight requests while still guaranteeing idempotence. Two major problems to be solved:  1. Sequence number management on the client when there are request failures. When a batch fails,  future inflight batches will also fail with `OutOfOrderSequenceException`. This must be handled on the client with intelligent sequence reassignment. We must also deal with the fatal failure of some batch: the future batches must get different sequence numbers when the come back. 2. On the broker, when we have multiple inflights, we can get duplicates of multiple old batches. With this patch, we retain the record metadata for 5 older batches.   I have added `TODO(reviewers)` comments for specific decisions in the code which are worth discussing.  TODO:  1. Add more unit tests, especially for loading different snapshot versions correctly, more client side unit tests, more broker side tests to validate that we are caching the correct number of batches (some of this is already there). 2. Update the system tests to check for ordering.  3. Run a tight loop of system tests.  4. Add comments about the assumptions made around the network client semantics of send/receive. 5. Gracefully handle changes in the producer id. In particular, we should never change the producer id once a batch has been sent, only the sequence number. This protects against duplicates and is a generalization of the previous approach to not reset producer state on retry.","closed","","apurvam","2017-08-25T19:34:08Z","2020-09-24T14:25:20Z"
"","3824","MINOR: Fix help in consumer group tool","help in kafka-consumer-groups.sh change","closed","","ueokande","2017-09-10T22:44:20Z","2018-01-26T19:28:38Z"
"","3952","KAFKA-5061:clientid should be set for Connect Producers and Consumers","Have added the new config in ConnectorConfig with priority ""LOW"", providing an option to override client.id on a per-connector basis. And set client.id , using (worker group ID + task ID) by default, when client.id is not provided in connector configuration, for distributed mode.","closed","connect,","Satyajitv","2017-09-24T05:39:44Z","2019-02-21T15:42:36Z"
"","4244","MINOR: improve flaky Streams system test","Handle TimeoutException in Producer callback and retry sending input data  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-22T00:00:23Z","2017-11-22T22:04:44Z"
"","3945","MINOR: Move ChannelState.exception() to its own class","Given that Java doesn't support pattern matching, the benefit is not as clear.  Also: - Renamed `state` to `value` to avoid method calls like `state.state` - Implemented `ChannelState.toString` - Removed some unused imports.","open","","ijuma","2017-09-22T09:37:14Z","2018-03-02T19:30:45Z"
"","4352","MINOR: Use GitHub git repo for push by default","GitBox (write support for the GItHub git repository) has been enable as per:  https://issues.apache.org/jira/browse/INFRA-15676  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-12-22T01:08:29Z","2017-12-22T01:19:19Z"
"","3524","KAFKA-5588: uselss --new-consumer option","Get rid of the --new-consumer option for the ConsoleConsumer","closed","","ppatierno","2017-07-13T07:54:40Z","2017-08-04T12:53:22Z"
"","4257","KAFKA-6174; Add methods in Options classes to keep binary compatibility with 0.11","From 0.11 to 1.0, we moved `DescribeClusterOptions timeoutMs(Integer timeoutMs)` from DescribeClusterOptions to AbstractOptions. User reports that code compiled against 0.11.0 fails when it is executed with 1.0 kafka-clients jar. This patch adds back these methods to keep binary compatibility with 0.11.","closed","","lindong28","2017-11-23T01:15:56Z","2017-12-13T01:59:10Z"
"","4018","KAFKA-6010: Relax record conversion time test to avoid build failure","For record conversion tests, check time >=0 since conversion times may be too small to be measured accurately. Since default value is -1, the test is still useful. Also increase message size in SslTransportLayerTest#testNetworkThreadTimeRecorded to avoid failures when processing time is too small.","closed","","rajinisivaram","2017-10-04T19:13:31Z","2017-10-04T21:39:22Z"
"","4108","KAFKA-6101 Reconnecting to broker does not exponentially backoff","For reconnection, share the nodeState if one exists. Only update its state and lastConnectAttemptMs.","closed","","tedyu","2017-10-21T02:12:42Z","2017-10-23T13:42:15Z"
"","4424","KAFKA-6378 KStream-GlobalKTable null KeyValueMapper handling","For KStream-GlobalKTable joins let `null` `KeyValueMapper` results indicate no match  For KStream-GlobalKTable joins, a `KeyValueMapper` is used to derive a key from the stream records into the `GlobalKTable`. For some stream values there may be no valid reference to the table stream. This patch allows developers to use `null` return values to indicate there is no possible match. This is possible in this case since `null` is never a valid key value for a `GlobalKTable`. Without this patch, providing a `null` value caused the stream to crash on Kafka 1.0.  I added unit tests for KStream-GlobalKTable left and inner joins, since they were missing. I also covered this additional scenario where `KeyValueMapper` returns `null` to insure it is handled correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","andybryant","2018-01-16T00:23:20Z","2018-01-31T11:16:01Z"
"","4195","KAFKA-5811: Add Kibosh integration for Trogdor and Ducktape","For ducktape: add Kibosh to the testing Dockerfile. Create files_unreadable_fault_spec.py.  For trogdor: create FilesUnreadableFaultSpec.java. Add a unit test of using the Kibosh service.","closed","","cmccabe","2017-11-08T18:32:29Z","2019-06-20T11:12:36Z"
"","3525","KAFKA-5431: cleanSegments should not set length for cleanable segment files","For a compacted topic with preallocate enabled, during log cleaning, LogCleaner.cleanSegments does not have to pre-allocate the underlying file size since we only want to store the cleaned data in the file.  It's believed that this fix should also solve KAFKA-5582.","closed","","huxihx","2017-07-13T08:37:57Z","2017-07-25T02:09:54Z"
"","4401","Updating docs for streams app reset tool","Following up https://github.com/apache/kafka/pull/4159#issuecomment-354942083   Updating documentation for Streams Application Resetter Tool:  * Added upgrade notes that StreamsResetter API has been updated. * Describe options to reset offsets for input-topics in Tool docs.","closed","","jeqo","2018-01-08T15:19:48Z","2020-08-08T09:20:53Z"
"","3972","MINOR: Add documentation about each of the messages in the protocol","Following recent refactoring of `Protocol` I suggested to add documentation for the messages to explain the purpose of each message and the participants in the exchange. This is roughly what I was thinking. If this addition is desirable, I will need some help with the actual documentation for some of the request -- If you leave comments on the PR I'll update the doc accordingly.","closed","","tombentley","2017-09-27T11:02:53Z","2020-02-25T11:55:59Z"
"","4196","MINOR: KafkaZkClient refactor. Use match instead of if/else chains","Follow up from https://github.com/apache/kafka/pull/4111","closed","","mimaison","2017-11-08T21:00:31Z","2018-04-18T13:27:46Z"
"","4243","MINOR: Update Powermock to fix PushHttpMetricsReporterTest failures","Fixes test failures where old versions of Powermock don't handle nested classes accessing parent field members when using mockStatic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2017-11-21T19:03:51Z","2017-11-22T16:36:59Z"
"","4389","Fix /image 404s in streams doc","Fixes image 404s in streams docs (e.g. https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html).  Related https://github.com/apache/kafka-site/pull/114","closed","","joel-hamill","2018-01-04T17:30:44Z","2018-01-08T17:40:30Z"
"","4317","KAFKA-6349: fix concurrent modification exception in AbstractStateManager during restore","Fixes a `ConcurrentModificationException` in`AbstractStateManager` that is triggered when a `StateStore` is re-initialized and there are multiple stores in the context.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2017-12-12T14:29:40Z","2017-12-13T03:26:40Z"
"","3650","KAFKA-5717: InMemoryKeyValueStore should delete keys with null values during restore","Fixed a bug in the InMemoryKeyValueStore restoration where a key with a `null` value is written in to the map rather than being deleted.","closed","","dguy","2017-08-09T13:26:28Z","2017-08-09T19:04:58Z"
"","4318","MINOR: fix warn logging in RecordCollectorImpl","Fix warn log message in RecordCollectorImpl so it prints the exception message rather than `{}`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dguy","2017-12-12T15:01:18Z","2017-12-13T03:16:02Z"
"","4440","Fix typo: ""actually recovery"" > ""actually recover""","Fix typo: ""actually recovery"" > ""actually recover""","closed","","jeffwidman","2018-01-18T19:49:32Z","2018-01-18T22:45:31Z"
"","4426","MINOR: fix typo","Fix typo on Streams page","closed","","joel-hamill","2018-01-16T17:15:39Z","2018-01-16T18:31:01Z"
"","3685","KAFKA-5668: fetch across stores in CompositeReadOnlyWindowStore & CompositeReadOnlySessionStore","Fix range queries in `CompositeReadOnlyWindowStore` and `CompositeReadOnlySessionStore` to fetch across all stores (was previously just looking in the first store)","closed","","dguy","2017-08-17T10:31:13Z","2017-08-18T17:01:03Z"
"","4403","Fix Streams navigation, menu order, and missig maven content","Fix navigation  Update libraries and maven topics","closed","","joel-hamill","2018-01-08T22:07:17Z","2018-01-09T17:24:37Z"
"","4240","KAFKA-6247. Fix system test dependency issues","Fix an omission where Kibosh was not getting installed on Vagrant instances running in AWS.  Centralize test dependency versions in tests/versions.sh.  Fix an issue where the Dockerfile was unable to download old Apache Kafka releases.  See the discussion on KAFKA-6233.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cmccabe","2017-11-20T19:02:55Z","2019-05-20T19:07:16Z"
"","3849","KAFKA-5886: Implement KIP-91 delivery.timeout.ms","First shot at implementing kip-91 `delivery.timeout.ms`. Summary  1. Added `delivery.timeout.ms` config. Default 120,000 2. Changed `retries` to `MAX_INT`. 3. batches may expire whether they are inflight or not. So muted is no longer used in `RecordAccumulator.expiredBatches`. 4. In some rare situations batch.done may be called twice. Attempted transitions from failed to succeeded are logged. Successful to successful is an error (exception as before). Other transitions (failed-->aborted, aborted-->failed) are ignored. 5. The old test from `RecordAccumulatorTest` is removed. It has three additional tests. `testExpiredBatchSingle`, `testExpiredBatchesSize`, `testExpiredBatchesRetry`. All of them test that expiry is independent of muted.","closed","","sutambe","2017-09-13T17:40:32Z","2019-02-18T19:07:10Z"
"","3891","[WIP input needed] MINOR: Further code cleanup involving log statements","First of all, terribly sorry for not having included this in the previous PR, that one only focused on the streams folder, this one is on the entire project. I realized my mistake too late.  I do need some input for this one, I left 2 TODO statements in the code that I'll point out.  This commit largely replaces string concatenation with placeholders in debug and trace log statements. Other things I'll point out with comments.  For historical purposes, on why replacing string concatenation with placeholders: When running an application on a certain log level, one wants to avoid log statements of more detailed levels actually being evaluated, both due to potential size issues and time spent doing it, since they're not actually going to be printed. If the message being passed to the log method utilizes string concatenation directly in the passed argument, that means the string will be fully evaluated before being passed along to the log method, to then potentially not even be shown, if the application is running on a different log level.  Using placeholders prevents this from happening.  EDIT: Ran the tests:  BUILD SUCCESSFUL in 30m 59s 90 actionable tasks: 90 executed","closed","","KoenDG","2017-09-18T12:45:46Z","2017-11-16T19:29:14Z"
"","4144","MINOR: Ensure that the producer in testAlterReplicaLogDirs is always closed","Failure to close the producer could cause a transient failure, more details below.  The request timeout was only 2 seconds, exceptions thrown were not propagated and the producer would not be closed. If the exception was thrown during `send`, we did not increment `numMessages` allowing the test to pass.  I have increased the timeout to 10 seconds and made sure that exceptions are propagated.  Example of the error:  ```text kafka.api.SaslSslAdminClientIntegrationTest > classMethod STARTED  kafka.api.SaslSslAdminClientIntegrationTest > classMethod FAILED     java.lang.AssertionError: Found unexpected threads, allThreads=Set(metrics-meter-tick-thread-2, Signal Dispatcher, main, Reference Handler, scala-execution-context-global-164, kafka-producer-network-thread | producer-1, scala-execution-context-global-166, Test worker, scala-execution-context-global-1249, /0:0:0:0:0:0:0:1:58910 to /0:0:0:0:0:0:0:1:43025 workers Thread 2, Finalizer, /0:0:0:0:0:0:0:1:58910 to /0:0:0:0:0:0:0:1:43025 workers Thread 3, scala-execution-context-global-163, metrics-meter-tick-thread-1) ```","closed","","ijuma","2017-10-27T13:02:10Z","2017-12-22T18:22:10Z"
"","3624","KAFKA-5702: extract refactor StreamThread","Extracted `TaskManager` to handle all task related activities. Make `StandbyTaskCreator`, `TaskCreator`, and `RebalanceListener` static classes so they must define their dependencies and can be testing independently of `StreamThread` Added interfaces between `StreamPartitionAssignor` & `StreamThread` to reduce coupling.","closed","","dguy","2017-08-04T14:47:14Z","2017-08-16T13:22:55Z"
"","3556","KAFKA-5620","Expose the ClassCastException as the cause for the SerializationException.","closed","","jcustenborder","2017-07-20T16:49:21Z","2017-08-26T23:09:54Z"
"","4010","KAFKA-5909; Removed the source jars from classpath","Executing CLI tools don't require source jars so removed it from the classpath.","closed","","kamalcph","2017-10-03T16:56:58Z","2018-05-14T06:24:10Z"
"","4457","KAFKA-6148: ClassCastException in connectors that include kafka-clients","Exclusion for packages that need not be loaded in isolation needs to be extended to all the `org.apache.kafka` packages (that do not belong to transforms and the other whitelisted packages). Most notably, this refers to any classes in `kafka-clients` package.   Given this exclusion and the fact that scanning for yet another type of package would increase the scanning time, issuing a warning as well any time a Connect plugin wrongfully includes such a dependency does not seem straightforward at this point.   I'm issuing this PR with the ""ignore"" functionality at the moment.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-01-22T21:59:05Z","2020-10-16T06:24:14Z"
"","4000","KAFKA-5445: Document exceptions thrown by AdminClient methods","Exceptions are processed internally in KafkaAdminClient without throwing them to the client code, hence the documentation of the exception is done in unusual way.","open","","adyach","2017-10-02T12:49:14Z","2018-08-22T21:13:35Z"
"","4138","MINOR: Revert EmbeddedZooKeeper rename","Even though this class is internal, it's widely used by other projects and it's better to avoid breaking them until we have a publicly supported test library.","closed","","ijuma","2017-10-26T10:55:52Z","2017-12-22T18:23:34Z"
"","4120","Make CONSOLE_OUTPUT_FILE configurable","Even if Kafka is not configured without the console appender the CONSOLE_OUTPUT_FILE is generated. Allowing for this to be configurable offers more flexibility including redirecting `nohup.out` or the resulting redirect to `/dev/null`","closed","","rnpridgeon","2017-10-23T14:46:14Z","2022-02-12T10:53:44Z"
"","4347","KAFKA-6391 ensure topics are created with correct partitions BEFORE building the…","ensure topics are created with correct partitions BEFORE building the metadata for our stream tasks  First ensureCoPartitioning() on repartitionTopicMetadata before creating allRepartitionTopicPartitions  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cvaliente","2017-12-20T15:52:29Z","2018-01-02T22:44:30Z"
"","3500","KAFKA-5566: fix unit test shouldAllowToQueryAfterThreadDied","Ensure only one thread dies, not both.","closed","","enothereska","2017-07-07T10:59:54Z","2017-07-07T12:30:58Z"
"","4263","KAFKA-6241: Enable dynamic updates of broker SSL keystore","Enable dynamic broker configuration (see KIP-226 for details). Includes  - Base implementation to allow specific broker configs and custom configs to be dynamically updated  - Extend DescribeConfigsRequest/Response to return all synonym configs and their sources in the order of precedence  - Extend AdminClient to alter dynamic broker configs  - Dynamic update of SSL keystores  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2017-11-24T14:56:41Z","2018-08-14T14:16:29Z"
"","4396","MINOR: Enable deep-iteration to print data in DumpLogSegments","Enable deep-iteration option when print-data-log is enabled in `DumpLogSegments`. Otherwise data is not printed. System test was wrongly reporting that data was lost since no payloads were printed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-05T17:00:24Z","2018-02-16T23:20:17Z"
"","3812","KAFKA-5657 Connect REST API should include the connector type when describing a connector","Embed the type of connector in ConnectorInfo","closed","connect,","tedyu","2017-09-07T23:06:21Z","2020-10-16T06:05:12Z"
"","4466","KAFKA-6245: Dynamic update of topic config defaults","Dynamic update of default topic configs as described in KIP-226.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-24T00:41:29Z","2018-01-27T20:22:06Z"
"","4471","KAFKA-6242: Dynamic resize of various broker thread pools","Dynamic resize of broker thread pools as described in KIP-226:  - num.network.threads  - num.io.threads  - num.replica.fetchers  - num.recovery.threads.per.data.dir  - background.threads  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-24T18:18:51Z","2018-01-30T17:29:27Z"
"","4464","KAFKA-6243: Enable dynamic updates of broker metrics reporters","Dynamic metrics reporter updates described in KIP-226. This includes:  - Addition and removal of metrics reporters  - Reconfiguration of custom metrics reporter configs  - Tests for metrics reporter updates at default cluster-level and as per-broker config for testing  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2018-01-23T23:48:01Z","2018-01-30T20:55:32Z"
"","3724","KAFKA-5769: Transient test failure org.apache.kafka.streams.integration.KStreamRepartitionJoinTest.shouldCorrectlyRepartitionOnJoinOperationsWithZeroSizedCache","Don't start the consumer until streams has transitioned to the running state","closed","","dguy","2017-08-23T13:22:11Z","2017-08-24T12:28:05Z"
"","3967","KAFKA-5951: Autogenerate Producer RecordAccumulator metrics","Don't review this until https://github.com/apache/kafka/pull/3799/files is merged.  This PR layers on top of https://github.com/apache/kafka/pull/3799/files. This will have to be rebased, once https://github.com/apache/kafka/pull/3799/files is merged into trunk and 1.0.0","open","","wushujames","2017-09-27T01:50:00Z","2018-03-02T19:30:47Z"
"","4358","KAFKA-6343 Documentation update in OS-level tuning section: add vm.ma…","Documentation update in OS-level tuning section: add vm.max_map_count bullet point  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rootex-","2017-12-26T13:54:17Z","2018-08-22T22:02:14Z"
"","4443","KAFKA-6461: fix TableTableJoinIntegrationTest [WIP]","DO NOT MERGE -- WIP for debugging","closed","","mjsax","2018-01-18T22:54:37Z","2018-01-30T02:21:25Z"
"","3492","HOTFIX: Hotfix for trunk failing","Disable 3 tests that need rethinking.","closed","","enothereska","2017-07-06T10:37:43Z","2017-07-06T12:26:35Z"
"","4406","KAFKA-4247: Prevent CLASSPATH from beginning with a single colon","Different fix for problem addressed by https://github.com/apache/kafka/pull/1953. Should prevent the CLASSPATH environment variable from being prefixed by a single colon before the JVM is invoked in the run-class script, which will then prevent the current working directory from being unintentionally included in the classpath when using the Reflections library.  If the current working directory should still be included in the classpath, it just needs to be explicitly specified either with its fully-qualified pathname or as a single dot (""."").","closed","","C0urante","2018-01-09T18:24:24Z","2018-01-09T22:56:44Z"
"","3906","KAFKA-5735: KIP-190: Handle client-ids consistently","Developed with @edoardocomar","closed","","mimaison","2017-09-19T18:00:44Z","2018-04-18T13:31:21Z"
"","3661","KAFKA-4585: Lower the Minimum Required ACL Permission of OffsetFetch (KIP-163)","Details can be found in the [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-163%3A+Lower+the+Minimum+Required+ACL+Permission+of+OffsetFetch).","closed","","vahidhashemian","2017-08-11T20:47:01Z","2017-09-07T18:42:35Z"
"","3865","KAFKA-5793: Tighten up the semantics of the OutOfOrderSequenceException","Description of the solution can be found here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Exactly+Once+-+Solving+the+problem+of+spurious+OutOfOrderSequence+errors","closed","","apurvam","2017-09-15T01:05:28Z","2017-09-22T18:12:55Z"
"","4430","KAFKA-6138 Simplify StreamsBuilder#addGlobalStore","deprecated StreamsBuilder#addGlobalStore and InternalStreamsBuilder#addGlobalStore add new StreamsBuilder#addGlobalStore and InternalStreamsBuilder#addGlobalStore without sourceName and processorName as parameter generate sourceName and processorName by using InternalStreamsBuilder#newProcessorName  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","panuwat-sc","2018-01-17T02:39:26Z","2018-01-31T21:53:42Z"
"","4115","KAFKA-6105: load client properties in proper order for kafka.tools.EndToEndLatency","Currently, the property file is loaded first, and later a auto generated group.id is used: consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, ""test-group-"" + System.currentTimeMillis()) so even user gives the group.id in a property file, it is not picked up.  we need to load client properties in proper order, so that we allow user to specify group.id and other properties, excludes only the properties provided in the argument list.","closed","","cnZach","2017-10-23T07:18:20Z","2017-10-23T07:40:52Z"
"","4125","KAFKA-6105: load client properties in proper order for EndToEndLatency tool","Currently, the property file is loaded first, and later a auto generated group.id is used: consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, ""test-group-"" + System.currentTimeMillis())  so even user gives the group.id in a property file, it is not picked up.  Change it to load client properties in proper order: set default values first, then try to load the custom values set in client.properties file.","open","","cnZach","2017-10-24T05:52:35Z","2020-02-14T09:46:02Z"
"","4116","KAFKA-6105: load client properties in proper order for EndToEndLatency tool","Currently, the property file is loaded first, and later a auto generated group.id is used: `consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, ""test-group-"" + System.currentTimeMillis())`  so even user gives the group.id in a property file, it is not picked up.  Change it to load client properties in proper order: set default values first, then try to load the custom values set in client.properties file.","closed","","cnZach","2017-10-23T08:15:45Z","2017-10-24T05:50:00Z"
"","3941","KAFKA-3999: Record raw size of fetch responses as part of consumer metrics (KIP-264)","Currently, only the decompressed size of fetch responses is recorded. This PR adds a sensor to keep track of the raw size as well.","open","","vahidhashemian","2017-09-22T00:03:12Z","2018-08-01T23:48:04Z"
"","4094","MINOR: Correct KafkaProducer Javadoc spelling of property 'max.in.flight.requests.per.connection'","Currently, in branches _trunk_, _0.11.0_, and _1.0_ the property **max.in.flight.requests.per.connection** is incorrectly misspelled as _max.inflight.requests.per.connection_  @harshach @ijuma @guozhangwang can you please review. Thank you.","closed","","hmcl","2017-10-19T02:35:24Z","2017-10-19T09:37:08Z"
"","3648","MINOR: ConsumerGroupCommand should always show partitions in ascending order","Currently, ConsumerGroupCommand shows in ascending order for partitions with active consumer assigned, but failed to do so for partitions without consumer assigned. The behavior should keep same.","open","","huxihx","2017-08-09T03:17:45Z","2018-03-02T19:30:30Z"
"","4326","KAFKA-6362: maybeAutoCommitOffsetsAsync should try to discover coordinator","Currently, `maybeAutoCommitOffsetsAsync` may not retry to find out coordinator even after the coordinator goes back to service. As a result, all asynchronous offset commits will fail.  This patch refines `maybeAutoCommitOffsetsAsync` to have it periodically retry the coordinator discovering.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2017-12-15T02:43:18Z","2018-08-22T03:18:45Z"
"","3608","KAFKA-5691- handle NOAUTH in zkUtils.CreateRecursive","Currently old consumers are unable to register with secure ZK installations. Handling NOAUTH prevents failures when checking /consumers","closed","","rnpridgeon","2017-08-02T03:22:05Z","2018-05-09T14:05:44Z"
"","4478","KAFKA-6489; Fetcher.retrieveOffsetsByTimes() should batch the metadata fetch.","Currently if users call KafkaConsumer.offsetsForTimes() with a large set of partitions. The consumer will add one topic at a time for the metadata refresh. We should add all the topics to the metadata topics and just do one metadata refresh instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","becketqin","2018-01-26T04:47:02Z","2018-02-01T17:00:17Z"
"","4372","KAFKA-6412 Improve synchronization in CachingKeyValueStore methods","Currently CachingKeyValueStore methods are synchronized at method level.  It seems we can use read lock for getter and write lock for put / delete methods.  For getInternal(), if the underlying thread is streamThread, the getInternal() may trigger eviction. This can be handled by obtaining write lock at the beginning of the method for streamThread.  The jmh patch is attached to JIRA: https://issues.apache.org/jira/secure/attachment/12905140/6412-jmh.v1.txt  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2018-01-01T15:12:19Z","2018-01-10T15:04:27Z"
"","3655","KAFKA-5724: AbstractPartitionAssignor should support assignment for topics with non-consecutive partitions","Current design does consider the siutation when user creates a topic via KafkaAdminClient whose partitions are not consecutive or zero-based. In such case, consumer does not work since assignor failed to assign partitions.","open","","huxihx","2017-08-11T07:25:08Z","2018-03-02T19:30:31Z"
"","4153","KAFKA-6141: add disabling of useless embedded zookeeper error logs","Created a function which sets  `zkShutdownHandler` filed of a `ZooKeeperServer` object via reflection(because the field and `ZooKeeperServerShutdownHandler` is not accessible).  So it is a solution because useless logs is eliminated.","closed","","Sammers21","2017-10-28T10:16:45Z","2018-01-13T00:59:11Z"
"","3810","KAFKA-5816: [FOLLOW UP] - create ProducedInternal class","Create `ProducedInternal` and remove getters from `Produced`","closed","","dguy","2017-09-07T17:27:23Z","2017-09-11T11:02:29Z"
"","3750","KafkaConsumer was hung when bootstrap servers was not existed","Could anyone help me on this?  We have an issue if we entered an non-existed host:port for bootstrap.servers property on KafkaConsumer. The created KafkaConsumer was hung forever.  **the debug message:** java.net.ConnectException: Connection timed out: no further information 	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) 	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) 	at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50) 	at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:95) 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:359) 	at org.apache.kafka.common.network.Selector.poll(Selector.java:326) 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:432) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:232) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:199) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:134) 	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:223) 	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:200) 	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:286) 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1078) 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1043) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382) 	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192) [2017-08-28 09:20:56,400] DEBUG Node -1 disconnected. (org.apache.kafka.clients.NetworkClient) [2017-08-28 09:20:56,400] WARN Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) [2017-08-28 09:20:56,400] DEBUG Give up sending metadata request since no node is available (org.apache.kafka.clients.NetworkClient) [2017-08-28 09:20:56,450] DEBUG Initialize connection to node -1 for sending metadata request (org.apache.kafka.clients.NetworkClient)","closed","","kyleabcha","2017-08-28T03:08:10Z","2017-10-11T02:54:39Z"
"","3540","Streams landing page","Content and assets for the updated Streams API landing page","closed","","derrickdoo","2017-07-17T22:00:51Z","2017-07-22T02:17:22Z"
"","3647","KAFKA-4501: Java 9 compilation and runtime fixes","Compilation error fixes: - Avoid ambiguity error when appending to Properties in Scala code (https://github.com/scala/bug/issues/10418) - Use position() and limit() to fix ambiguity issue ( https://github.com/scala/bug/issues/10418#issuecomment-316364778) - Disable findBugs if Java 9 is used ( https://github.com/findbugsproject/findbugs/issues/105)  Compilation warning fixes: - Avoid deprecated Class.newInstance in Utils.newInstance - Silence a few Java 9 deprecation warnings - var -> val and unused fixes  Runtime error fixes: - Introduce Base64 class that works in Java 7 and Java 9  Also: - Set --release option if building with Java 9  Note that tests involving EasyMock (https://github.com/easymock/easymock/issues/193) or PowerMock (https://github.com/powermock/powermock/issues/783) will fail as neither supports Java 9 currently.","closed","","ijuma","2017-08-09T00:23:52Z","2017-08-22T06:37:02Z"
"","3907","KAFKA-4950: Fix ConcurrentModificationException on assigned-partitions metric","Code change: - prevent `java.util.ConcurrentModificationException` being thrown when fetching the consumer coordinator assigned-partitions metric value from a `MetricsReporter` (e.g. a reporter exporting metrics periodically running in a separate thread) because of a race condition by using a volatile field for storing the number of assigned partitions: ``` java.util.ConcurrentModificationException: null         at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)         at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)         at java.util.AbstractCollection.addAll(AbstractCollection.java:343)         at java.util.HashSet.(HashSet.java:119)         at org.apache.kafka.common.internals.PartitionStates.partitionSet(PartitionStates.java:66)         at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedPartitions(SubscriptionState.java:293)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$ConsumerCoordinatorMetrics$1.measure(ConsumerCoordinator.java:880)         ...         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) ``` - new unit test to reproduce the issue and detect potential future regression  I am using a volatile field on `SubscriptionState` rather than changing the `PartitionStates.map` field to some thread safe `LinkedHashMap` alternative to avoid bringing an unnecessary concurrent structure to other components relying on `PartitionStates`.","closed","","slaunay","2017-09-19T18:25:05Z","2018-08-08T22:43:44Z"
"","4228","MINOR: improve StateStore JavaDocs","Clarify that state directory must use `storeName`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-17T01:41:49Z","2017-11-17T21:24:04Z"
"","3691","HOTFIX: Cherrypicking state deadlock fix from 0.11","Cherrypicking additional changes made as part of this PR https://github.com/apache/kafka/pull/3622, back to trunk.","closed","","enothereska","2017-08-18T14:32:18Z","2017-08-22T14:51:42Z"
"","3622","HOTFIX: state transition cherry picking","Cherry picked from https://github.com/apache/kafka/pull/3432","closed","","enothereska","2017-08-04T08:22:29Z","2017-08-15T16:08:11Z"
"","4033","KAFKA-6018: Make KafkaFuture.Future an interface","Changing KafkaFuture.Future and KafkaFuture.BiConsumer into an interface makes them a functional interface.  This makes them Java 8 lambda compatible.","closed","","steven-aerts","2017-10-06T10:27:30Z","2018-01-30T23:57:18Z"
"","3548","KAFKA-5790, KAFKA-5607; Improve error handling in SocketServer to avoid issues later","Changes:   1. When an exception is encountered in any of the methods in `Processor` while processing a channel, log the exception and close the connection. Continue to process other channels.   2. Fixes KAFKA-5790: SocketServer.processNewResponses should not skip a response if exception is thrown.   3. For `IllegalStateException` and `IOException` in `poll()`, don't close the `Selector`. Log the exception and continue.   4. Close channel on any failed send in `Selector`.   5. When closing channel fails or is closed, leave channel state as-is, indicating the state in which the channel was moved to closing.   6. Add tests for various failure scenarios.   7. Fix timing issue in `SocketServerTest.testConnectionIdReuse` by waiting for new connections to be processed by the server.","closed","","rajinisivaram","2017-07-19T10:43:31Z","2017-09-08T17:44:40Z"
"","4017","Rename streams tutorial and quickstart","Changed these topic titles: - Write your own Streams Applications -> Tutorial: Write a Streams Application - Play with a Streams Application -> Run the Streams Demo Application","closed","","joel-hamill","2017-10-04T18:27:32Z","2017-10-05T16:32:17Z"
"","4225","KAFKA-6218 : Optimize condition in if statement to reduce the number of comparisons","Changed the condition in **if** statement  **(schema.name() == null || !(schema.name().equals(LOGICAL_NAME)))** which requires two comparisons in worst case with **(!LOGICAL_NAME.equals(schema.name()))**  which requires single comparison in all cases and _avoids null pointer exception. ![kafka_optimize_if](https://user-images.githubusercontent.com/32234013/32872271-afe0b954-ca3a-11e7-838d-6a3bc416b807.JPG) _  ### Committer Checklist (excluded from commit message) - [+ ] Verify design and implementation  - [+ ] Verify test coverage and CI build status - [+ ] Verify documentation (including upgrade notes)","closed","connect,","sachinbhalekar","2017-11-16T03:25:13Z","2020-10-16T06:24:11Z"
"","3507","KAFKA-5550","Changed call to use the overload of ConnectSchema.validate method with the field name passed in. Ensure that field in put call is not null.","closed","connect,","jcustenborder","2017-07-08T04:12:08Z","2020-10-16T06:29:10Z"
"","4209","Add cell-sec entry point","Change-Id: I3daa7bba925b9f052cec6de0dd385c98292ca45a  Entry points for cell based security","closed","","iso88592","2017-11-14T12:33:05Z","2017-11-14T12:33:30Z"
"","3855","MINOR: Logging changes","Capitalised few log lines to match with rest of the code base. Please review the change @junrao","open","","chetnachaudhari","2017-09-13T23:07:32Z","2018-03-02T19:30:39Z"
"","4197","KAFKA-6190 GlobalKTable never finishes restoring when consuming transactional messages","Calculate offset using consumer.position() in GlobalStateManagerImp#restoreState","closed","","alexjg","2017-11-08T23:39:47Z","2017-11-10T06:43:24Z"
"","3652","MINOR: change log level in ThreadCache to trace","cache eviction logging at debug level is too high volume. This was already done on trunk but didn't make it into 0.11","closed","","dguy","2017-08-10T07:43:31Z","2017-08-16T13:22:25Z"
"","3550","KAFKA-5610: KafkaApis.HandleWriteTxnMarkerRequest should return UNKNOWN_TOPIC_OR_PARTITION on partition emigration.","Before this patch, we would return the non-retriable `UNSUPPORTED_FOR_MESSAGE_FORMAT` error when leadership changed, causing markers to be lost.","closed","","apurvam","2017-07-19T22:12:45Z","2017-07-20T20:14:17Z"
"","3901","MINOR: Fix unresolvable address in `testUnresolvableConnectString`","Before this change, the test always failed on my MacBook Pro.","closed","","ijuma","2017-09-19T09:47:17Z","2017-12-22T18:22:48Z"
"","3654","KAFKA-5562: Do streams state directory cleanup on a single thread","Backported from trunk: https://github.com/apache/kafka/pull/3516","closed","","dguy","2017-08-10T11:53:12Z","2017-08-16T13:22:19Z"
"","3690","Backport KAFKA-5742 support ZK chroot in system tests","backport of KAFKA-5742 (#3677) to 0.11.0","closed","","xvrl","2017-08-17T22:24:30Z","2017-08-21T04:58:01Z"
"","4272","KAFKA-4827: Porting fix for KAFKA-4827 to v0.10 and v0.11","Back porting the fix in this PR (https://github.com/apache/kafka/pull/4205) to v0.10 and v0.11.","closed","connect,","wicknicks","2017-11-28T22:36:20Z","2020-10-16T06:24:13Z"
"","4204","KAFKA-5238: BrokerTopicMetrics can be recreated after topic is deleted","Avoiding a DelayedFetch recreate the metrics when a topic has been deleted  developed with @mimaison  added unit test borrowed from @ijuma JIRA","open","","edoardocomar","2017-11-10T17:31:23Z","2020-06-18T16:47:11Z"
"","4207","MINOR: Add HttpMetricsReporter for system tests","Author: Ewen Cheslack-Postava   Reviewers: Apurva Mehta , Ismael Juma   Closes #4072 from ewencp/http-metrics  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ewencp","2017-11-13T02:23:42Z","2017-11-20T23:45:27Z"
"","3756","KAFKA-5261 added cached authorizer","attempt to improve Kafka performance by caching the results of the authorisation calls.","open","","simplesteph","2017-08-29T20:26:32Z","2018-03-02T19:30:35Z"
"","4395","MINOR: Add documentation for KAFKA-6086 (ProductionExceptionHandler)","Attempt number 2 at adding documentation related to KAFKA-6086, the `ProductionExceptionHandler`.  Tagging @guozhangwang and @mjsax for review!","closed","","farmdawgnation","2018-01-05T15:06:39Z","2018-01-15T18:36:10Z"
"","3595","MINOR: Request log should be enabled by debug level","At some point, we lost the ability to output request logging at debug level (which is a little less verbose than at trace level).","closed","","ijuma","2017-07-29T07:25:29Z","2017-08-22T06:37:19Z"
"","4450","MINOR Fix AsyncProducerTest bug that hits when logging is turned up","AsyncProducerTest gets an error about an incorrect mock when the logging level is turned up.  Instead of using a mock, just create a real SyncProducerConfig object, since the object is simple to create.","closed","","cmccabe","2018-01-19T19:45:25Z","2018-04-11T21:50:12Z"
"","4134","KAFKA-6075 Kafka cannot recover after an unclean shutdown","As Vahid commented, Files.deleteIfExists(file.toPath) seems to destabilize Windows environment.  This PR reverts to calling delete() directly.","closed","","tedyu","2017-10-25T16:37:20Z","2018-07-11T17:35:19Z"
"","3749","[KAFKA-4793] Restart tasks for connector endpoint","As suggested in KAFKA-4793 users may sometimes want to restart all of the tasks associated with a given connection. This proposed change adds a new endpoint to ConnectorsResource to allow one to restart all of the tasks associated with a given connector. This is slightly different than the original request of KAFKA-4793 with the comments of Ewen Cheslack-Postava  in mind that the migration of modifying the existing endpoint might have some challenges.  Note: this is one of my early contributions to Kafka, so if I'm still learning the code base -- if I've done something silly please let me know :) (I mean as always but especially so here :))","closed","connect,","holdenk","2017-08-28T00:05:10Z","2021-06-01T18:36:03Z"
"","4127","MINOR use proper template classes for internalSelectKey()","As pointed out in this thread: http://search-hadoop.com/m/Kafka/uyzND1fy2K7I85G1?subj=Kafka+source+code+Build+Error , Eclipse shows syntax error for the following: ```                         return new KeyValue<>(mapper.apply(key, value), value); ```","open","","tedyu","2017-10-24T20:09:22Z","2017-10-24T21:21:45Z"
"","3844","MINOR: Use sha512 instead of sha2 suffix in release artifacts","As per Apache guidelines:  http://www.apache.org/dev/release-distribution#sigs-and-sums","closed","","ijuma","2017-09-13T11:35:26Z","2017-09-17T15:20:25Z"
"","3879","KAFKA-5915: Support unmapping of mapped/direct buffers in Java 9","As mentioned in MappedByteBuffers' class documentation, its implementation was inspired by Lucene's MMapDirectory:  https://github.com/apache/lucene-solr/blob/releases/lucene-solr/6.6.1/lucene/core/src/java/org/apache/lucene/store/MMapDirectory.java#L315  Without this change, unmapping fails with the following message:  > java.lang.IllegalAccessError: class kafka.log.AbstractIndex (in unnamed module @0x45103d6b) cannot access class jdk.internal.ref.Cleaner (in module java.base) because module java.base does not export jdk.internal.ref to unnamed module @0x45103d6b","closed","","ijuma","2017-09-16T02:08:05Z","2017-12-22T18:22:29Z"
"","3861","KAFKA-5895 Gradle 3.0+ is needed on the build","As discussed in [KAFKA-5895](https://issues.apache.org/jira/browse/KAFKA-5895), this trivial PR is simply adding a hint that Gradle 3.0+ is now needed","closed","","matzew","2017-09-14T18:39:54Z","2017-12-22T01:52:13Z"
"","4281","[WIP] KAFKA-5693 rationalise policy interfaces","As described in KIP-201 (not yet accepted), this:  * deprecates the CreateTopicPolicy and AlterConfigPolicy  * adds a new TopicManagementPolicy.  * adds validateOnly() option to AdminClient.deleteTopics() and AdminClient.deleteRecords()  The existing policy tests are duplicated to test both old and new policy interfaces. A new DeleteRecordsRequestTest (and *WithPolicy subclass) are added to further test delete records with and without policy. A new DeleteTopicsRequestTestWithPolicy is added, subclassing the existing (but updated) DeleteTopicsRequestTest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2017-12-01T10:35:44Z","2022-02-10T16:49:37Z"
"","3521","KAFKA-5584: fix integer overflow in Log.size","As described in [KAFKA-5584](https://issues.apache.org/jira/browse/KAFKA-5584) the integer overflow in `Log.size` may lead to wrong metrics and broken size-based retention via `log.retention.bytes` or `retention.bytes` on rather large topic partitions.","closed","","kongo2002","2017-07-12T10:12:23Z","2017-07-13T00:39:47Z"
"","4456","KAFKA-6354 Update KStream JavaDoc using new State Store API","As a newbie to Kafka, I make this docs update PR and in the meanwhile to get familiar with kafka and apis :)","closed","docs,","PnPie","2018-01-22T21:19:32Z","2018-02-02T19:28:01Z"
"","3881","MINOR: Update powermock and enable its tests when running with Java 9","Also: 1. Fix WorkerTest to use the correct `Mock` annotations. `org.easymock.Mock` is not supported by PowerMock 2.x. 2. Rename `powermock` to `powermockJunit4` in `dependencies.gradle` for clarity.","closed","","ijuma","2017-09-17T15:18:51Z","2017-12-22T18:22:54Z"
"","4261","MINOR: Avoid intermediate strings when parsing/decoding ZK JSON","Also: - Fix bug in result type of `createSequentialPersistentPath` - Remove duplicated code from `ReplicationUtils` - Move `propagateIsrChanges` from `ReplicationUtils` to `KafkaZkClient` - Add tests - Minor clean-ups  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-24T13:59:18Z","2017-12-22T18:14:02Z"
"","4154","KAFKA-4203: Align broker default for max.message.bytes with Java producer default","Also: * Improve error message * Add test * Minor code quality fixes  Verified that the test fails if the broker default for max message bytes is lower or higher than the currently set value.","closed","","ijuma","2017-10-28T15:47:11Z","2020-01-29T21:03:36Z"
"","3808","MINOR: Always specify the keystore type in system tests","Also throw an exception if a null keystore type is seen in `SecurityStore`. This should never happen.  The default keystore type has changed in Java 9 ( http://openjdk.java.net/jeps/229), so we need to be explicit to have consistent behaviour across Java versions.","closed","","ijuma","2017-09-07T14:40:51Z","2017-09-17T15:20:43Z"
"","4142","MINOR: Use Scala Future in CoreUtils test","Also rename UtilsTest to CoreUtilsTest and note that `getOrElseUpdate` has the right behaviour in Scala 2.12.","closed","","ijuma","2017-10-27T02:47:52Z","2017-12-22T18:23:32Z"
"","3658","MINOR: AdminClient should register with `AppInfoParser`","Also make ""created"" message more consistent across clients.","closed","","ijuma","2017-08-11T12:39:51Z","2017-08-22T06:37:08Z"
"","3501","MINOR: Improve versioning in docs when a full version is required","Also fix quickstart.","closed","","ijuma","2017-07-07T13:35:42Z","2017-09-05T08:38:17Z"
"","4061","KAFKA-6032: Unit Tests should be independent of locale settings","Allows to run the tests on linux servers without needing the locale set to US.","open","","gilles-degols","2017-10-11T21:34:54Z","2018-03-02T19:30:51Z"
"","3508","KAFKA-5572 - ConfigDef should be able to escape comma(s)","Allow escaping ',' character with '\,' as an escape sequence.","closed","","jcustenborder","2017-07-08T16:22:52Z","2018-05-16T16:43:41Z"
"","4423","MINOR: Add async and different sync startup modes in connect service test class","Allow Connect Service in system tests to start asynchronously.   Specifically, allow for three startup conditions:  1. No condition - start async and return immediately.  2. Semi-async - start immediately after plugins have been discovered successfully.  3. Sync - start returns after the worker has completed startup. This is the current mode, but its condition is improved by checking that the port of Connect's REST interface is open, rather than that a log line has appeared in the logs.   An associated system test run has been started here:  https://jenkins.confluent.io/job/system-test-confluent-platform-branch-builder/586/  @ewencp @rhauch, I'd appreciate your review.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-01-16T00:04:23Z","2020-10-16T06:05:15Z"
"","3753","Allow timestamp parameter in `ProcessorTopologyTestDriver.process`","All current implementations process records using the same timestamp. This makes it difficult to test operations that require time windows, like `KStream-KStream joins`.   This change would allow tests to simulate records created at different times, thus making it possible to test operations like the above mentioned joins.","closed","","sebigavril","2017-08-29T14:38:53Z","2017-09-01T04:47:12Z"
"","4175","KAFKA-6161 add base classes for (De)Serializers with empty conf methods","All (de)serializers, which have empty configure() and/or close() methods, are now inherit NoConf(De)Serializer. Also, such classes are created for extended (de)serializers.","closed","","evis","2017-11-03T15:38:46Z","2018-06-04T05:20:41Z"
"","4470","KAFKA-6418: AdminClient should handle empty or null topic names better","AdminClient should return an InvalidTopicException when the topic name supplied by an AC user is empty or null.  Previously, the client would try to serialize these invalid topic names, and the whole batch request would fail, including unrelated topics.  Also add a unit test.","closed","","cmccabe","2018-01-24T18:02:14Z","2019-05-20T18:57:29Z"
"","4295","KAFKA-6299. Fix AdminClient error handling when metadata changes","AdminClient should only call Metadata#requestUpdate when needed.  When AdminClient gets a NOT_CONTROLLER error, it should refresh its metadata and retry the request, rather than making the end-user deal with NotControllerException.  Move AdminClient's metadata management outside of NetworkClient and into AdminMetadataManager.  This will make it easier to do more sophisticated metadata management in the future, such as implementing a NodeProvider which fetches the leaders for topics.  Rather than manipulating newCalls directly, the AdminClient service thread now drains it directly into pendingCalls.  This minimizes the amount of locking we have to do, since pendingCalls is only accessed from the service thread.","closed","","cmccabe","2017-12-06T00:42:07Z","2019-05-20T18:55:45Z"
"","3937","KAFKA-5856 AdminClient.createPartitions() follow up","Adds support for noop requests. When assignments are given with a request that would be a noop we validate that the given assignments match the actual ones, so that the state of the partitions after a successful call definitely matches what was requested.  I can put the additional Javadoc on the exception in a separate PR if you prefer.  The tests have the [improvements requested](https://github.com/apache/kafka/pull/3930#issuecomment-331130475) by @ijuma  The Javadoc is improved too. Putting the expected exceptions on the AdminClient method is rather distant from where they're actually throw (the Future from the Map from the Results from the call), but it keeps the documentation about the method as a whole in one place. I didn't know whether to include the detailed possible causes for each Exception. The more detail the harder it is to maintain, but the more useful to the client.  /cc @ijuma","closed","","tombentley","2017-09-21T19:58:36Z","2017-10-04T18:01:53Z"
"","3705","KAFKA-5746: Add new metrics to support health checks (KIP-188)","Adds new metrics to support health checks: 1. Error rates for each request type, per-error code 2. Request size and temporary memory size 3. Message conversion rate and time 4. Successful and failed authentication rates 5. ZooKeeper latency and status 6. Client version","closed","","rajinisivaram","2017-08-21T13:29:40Z","2017-09-28T21:01:02Z"
"","4336","MINOR: Add documentation of error handling logic added in 1.1.0","Adds documentation of exception handling logic recently introduced into Kafka Streams.","closed","","farmdawgnation","2017-12-16T19:36:55Z","2018-01-05T13:37:34Z"
"","4459","KAFKA-6467: Enforce layout of dependencies within a connect plugin to be deterministic","Adds alphanumeric ordering of dependencies as they added to a Connect plugin's class loader path.   This makes the layout of the dependencies consistent across systems and deployments. Dependencies should still, in principle, not include conflicts and ideally order should not matter.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-01-23T00:14:40Z","2020-10-16T06:05:15Z"
"","4145","KAFKA-4928: Add integration test for DumpLogSegments","Adding tests for `kafka.tools.DumpLogSegments`","closed","","Sammers21","2017-10-27T15:18:40Z","2020-02-11T11:31:28Z"
"","4220","KAFKA-6210: IllegalArgumentException if 1.0.0 is used for inter.broker.protocol.version or log.message.format.version","Added unit test for ApiVersion and testApiVersions from Scala to Java.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-15T16:07:17Z","2017-12-22T18:21:40Z"
"","3551","MINOR: added logging feature for ConfigEntry","Added toString() ovveride to ConfigEntry for logging","closed","","ppatierno","2017-07-20T08:48:32Z","2017-07-20T10:56:58Z"
"","3490","MINOR: Pass over DSL documentation up to stateful transformations","Added the following documentation: - creating source streams from Kafka - stateless transformations - moved previous three sections under ""stateful transformations""","closed","","enothereska","2017-07-05T12:53:58Z","2017-08-24T07:16:55Z"
"","3577","MINOR: Added some tips for running a single test file, test class and/or test method","Added some tips for running a single test file, test class and/or test method on the documentation landing page about tests","closed","","ppatierno","2017-07-26T08:38:04Z","2017-09-18T07:56:41Z"
"","4222","Adding secondary nav to Streams page","Added secondary navigation to Streams sub-pages:  - quickstart.html  - core-concepts.html  - developer-guide.html  - tutorial.html  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","manjuapu","2017-11-15T21:34:22Z","2017-11-15T23:25:00Z"
"","4011","KAFKA-5903: Added Connect metrics to the worker and distributed herder","Added metrics to the Connect worker and rebalancing metrics to the distributed herder.  This is built on top of #3987, and I can rebase this PR once that is merged.","closed","connect,","rhauch","2017-10-04T05:22:33Z","2020-10-16T06:29:13Z"
"","3911","KAFKA-5900: Add task metrics common to both sink and source tasks","Added metrics that are common to both sink and source tasks.    Marked as ""**WIP**"" since this PR is built upon #3864, and will need to be rebased once that has been merged into `trunk`. However, I would still appreciate initial reviews since this PR is largely additive.","closed","connect,","rhauch","2017-09-19T22:11:17Z","2020-10-16T06:05:12Z"
"","3488","KAFKA-5557: Using a logPrefix inside the StreamPartitionAssignor","Added logPrefix for avoiding stream thread name formatting replicated more times","closed","","ppatierno","2017-07-05T09:25:41Z","2017-07-06T07:51:35Z"
"","3578","KAFKA-5643: Using _DUCKTAPE_OPTIONS has no effect on executing tests","Added handling of _DUCKTAPE_OPTIONS (mainly for enabling debugging)","closed","","ppatierno","2017-07-26T09:54:15Z","2017-08-07T05:45:53Z"
"","4381","MINOR: Added functionality to discover Kafka as OSGi package","Added functionality to discover Kafka as an OSGi supported package. Currently Kafka is not supported for usage in bndrun files because of it's missing Bundle-SymbolicName.","closed","","rlenferink","2018-01-03T15:12:45Z","2018-01-03T15:48:18Z"
"","3495","KAFKA-3362: Update protocol schema and field doc strings","Added doc to protocol fields","closed","","andrasbeni","2017-07-06T13:03:38Z","2018-11-30T11:31:26Z"
"","3555","KAFKA-5619: Make --new-consumer option as deprecated in all tools","Added deprecation and warning message on --new-consumer option usage with ConsumerGroupCommand and ConsumerPerformance tools.","closed","","ppatierno","2017-07-20T13:37:13Z","2017-08-04T11:53:53Z"
"","3959","KAFKA-5901: Added Connect metrics specific to source tasks","Added Connect metrics specific to source tasks, and builds upon #3864 and #3911 that have already been merged into `trunk`.","closed","connect,","rhauch","2017-09-25T19:38:32Z","2020-10-16T06:29:11Z"
"","3975","KAFKA-5902: Added sink task metrics","Added Connect metrics specific to source tasks, and builds upon #3864 and #3911 that have already been merged into `trunk`, and #3959 that has yet to be merged.  I'll rebase this PR when the latter is merged.","closed","connect,","rhauch","2017-09-27T17:33:55Z","2020-10-16T06:29:12Z"
"","4019","KAFKA-6011 AppInfoParser should only use metrics API and should not register JMX mbeans directly","Added app ID to metrics API.  The JMX can be dropped post 1.0.0","open","","tedyu","2017-10-04T20:12:11Z","2017-10-04T20:17:59Z"
"","3760","MINOR: add table of contents","Added a simple table of contents for the developer section.","closed","","enothereska","2017-08-30T15:24:16Z","2017-09-05T06:39:08Z"
"","4113","KAFKA-6104: Added unit tests for ClusterConnectionStates.","Added a few unit tests for ClusterConnectionStates.","closed","","soenkeliebau","2017-10-22T20:17:35Z","2017-10-23T15:35:28Z"
"","3511","KAFKA-5575 - Add SchemaBuilder.from","Added `SchemaBuilder.from` method which allows creating a schema builder prepopulated with the details from the schema. Added tests for structs, maps, arrays, primitives, and logical types.","open","connect,","jcustenborder","2017-07-10T05:37:39Z","2018-03-02T19:30:25Z"
"","3784","KAFKA-5832: add Consumed and change StreamBuilder to use it","Added `Consumed` class. Updated `StreamBuilder#stream`, `StreamBuilder#table`, `StreamBuilder#globalTable`","closed","","dguy","2017-09-04T16:03:20Z","2017-09-08T07:23:32Z"
"","3809","KAFKA-5853: implement WindowedKStream","Add the `WindowedKStream` interface and implementation of methods that don't require `Materialized`","closed","","dguy","2017-09-07T16:27:27Z","2017-09-08T16:20:28Z"
"","3770","KAFKA-5816: add Produced class, KStream#to(topic, Produced), and KStream#through(topic, Produced)","Add the `Produced` class and `KStream` overloads that use it: `KStream#to(String, Produced)` `KStream#through(String, Produced)` Deprecate all other to and through methods accept the single param methods that take a topic param","closed","","dguy","2017-08-31T17:34:49Z","2017-09-07T07:55:45Z"
"","3776","KAFKA-5819: Add Joined class and relevant KStream join overloads","Add the `Joined` class and the overloads to `KStream` that use it. Deprecate existing methods that have `Serde` params","closed","","dguy","2017-09-01T09:47:15Z","2017-09-06T09:57:12Z"
"","3807","KAFKA-5852: Add filter, filterNot, mapValues and Materialized to KTable","Add overloads of `filter`, `filterNot`, `mapValues` that take `Materialized` as a param to `KTable`. Deprecate overloads using `storeName` and `storeSupplier`","closed","","dguy","2017-09-07T12:37:09Z","2017-09-08T21:03:35Z"
"","3827","KAFKA-5654: add materialized count, reduce, aggregate to KGroupedStream","Add overloads of `count`, `reduce`, and `aggregate` that are `Materialized` to `KGroupedStream`. Refactor common parts between `KGroupedStream` and `WindowedKStream`","closed","","dguy","2017-09-11T14:23:48Z","2017-09-18T10:55:56Z"
"","3829","KAFKA-5655: materialized count, aggregate, reduce to KGroupedTable","Add overloads of `count`, `aggregate`, `reduce` using `Materialized` to `KGroupedTable` deprecate other overloads","closed","","dguy","2017-09-11T16:31:32Z","2017-09-12T16:22:10Z"
"","3837","KAFKA-5873: add materialized overloads to StreamsBuilder","Add overloads for `table` and `globalTable` that use `Materialized`","closed","","dguy","2017-09-12T16:31:14Z","2017-09-18T14:59:02Z"
"","3692","KAFKA-5689:Add MeteredWindowStore and refactor store hierarchy","Add MeteredWindowStore and ChangeLoggingWindowBytesStore. Refactor Store hierarchy such that Metered is always the outermost store Do serialization in MeteredWindowStore","closed","","dguy","2017-08-18T15:58:51Z","2017-08-22T10:23:25Z"
"","4448","MINOR: additional check to follower fetch handling","add check to KafkaApis, add unit test specific to follower fetch developed with @mimaison backport to 0.10.2 of #4433  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-01-19T12:31:20Z","2018-01-19T16:23:58Z"
"","4447","MINOR: additional check to follower fetch handling","add check to KafkaApis, add unit test specific to follower fetch developed with @edoardocomar  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-01-19T11:55:09Z","2018-04-18T13:28:04Z"
"","4433","MINOR: additional check to follower fetch handling","add check to KafkaApis add unit test specific to follower fetch update tool  developed with @mimaison   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2018-01-17T18:08:20Z","2018-02-02T16:34:43Z"
"","4069","KAFKA-6057: Users forget `--execute` in the offset reset tool","Add a small warning note when the user does not use the --execute flag.","closed","","gilles-degols","2017-10-12T22:16:05Z","2018-02-26T17:00:40Z"
"","4009","KAFKA-5651: [FOLLOW-UP] add with method to Materialized","Add a `with(Serde keySerde, Serde valSerde)` to `Materialized` for cases where people don't care about the state store name.","closed","","dguy","2017-10-03T16:17:34Z","2017-10-06T22:40:02Z"
"","3902","KAFKA-5922: Add SessionWindowedKStream","Add `SessionWindowedKStream` and implementation. Deprecate existing `SessionWindow` `aggregate` methods on `KGroupedStream`","closed","","dguy","2017-09-19T11:04:10Z","2017-09-21T08:12:15Z"
"","3825","KAFKA-5817: [FOLLOW-UP] add SerializedInternal","Add `SerializedInternal` class and remove getters from `Serialized`","closed","","dguy","2017-09-11T09:20:30Z","2017-09-11T20:08:31Z"
"","4191","KAFKA-6184: report a metric of the lag between the consumer offset ...","Add `records-lead` and partition-level `{topic}-{partition}.records-lead-min|avg` for fetcher metrics.  @junrao  Please kindly review. Thanks.","closed","","huxihx","2017-11-08T08:23:12Z","2018-02-06T18:42:23Z"
"","3889","KAFKA-5921: add Materialized overloads to windowed kstream","Add `Materialized` overloads to `WindowedKStream`. Deprecate existing methods on `KGroupedStream`","closed","","dguy","2017-09-18T11:27:08Z","2017-09-19T10:00:48Z"
"","3802","KAFKA-5844: add groupBy(selector, serialized) to Ktable","add `KTable#groupBy(KeyValueMapper, Serialized)` and deprecate the overload with `Serde` params","closed","","dguy","2017-09-06T13:51:32Z","2017-09-07T11:37:12Z"
"","3826","KAFKA:5653: add join overloads to KTable","Add `join`, `leftJoin`, `outerJoin` overloads that use `Materialized` to `KTable`","closed","","dguy","2017-09-11T10:54:27Z","2017-09-12T15:02:58Z"
"","3798","KAFKA-5841: AbstractIndex should offer `makeReadOnly` method","AbstractIndex should offer `makeReadOnly` method that changed the underlying MappedByteBuffer read-only.","open","","huxihx","2017-09-06T06:50:38Z","2018-03-02T19:30:36Z"
"","4413","[KAFKA-6265] GlobalKTable missing #queryableStoreName()","A spinoff of original pull request #4340 for resolving conflicts.","closed","","ConcurrencyPractitioner","2018-01-11T02:17:00Z","2018-01-11T17:56:01Z"
"","4258","[KAFKA-4499] Add all() and fetchAll() API for querying","A rebased version of the code.","closed","","ConcurrencyPractitioner","2017-11-23T04:20:30Z","2018-01-04T00:21:27Z"
"","3987","KAFKA-5990: Enable generation of metrics docs for Connect","A new mechanism was added recently to the Metrics framework to make it easier to generate the documentation. It uses a registry with a MetricsNameTemplate for each metric, and then those templates are used when creating the actual metrics. The metrics framework provides utilities that can generate the HTML documentation from the registry of templates.  This change moves the recently-added Connect metrics over to use these templates and to then generate the metric documentation for Connect.  This PR is based upon #3975 and can be rebased once that has been merged.","closed","connect,","rhauch","2017-09-28T23:42:43Z","2020-10-16T06:29:12Z"
"","4137","KAFKA-6119: Bump epoch when expiring transactions in the TransactionCoordinator","A description of the problem is in the JIRA. I have added an integration test which reproduces the original scenario, and also added unit test cases.","closed","","apurvam","2017-10-26T01:07:31Z","2017-10-27T17:40:15Z"
"","4095","KAFKA-5140: Fix reset integration test","A couple of root causes of this flaky test is fixed:  1. The MockTime was incorrectly used across multiple test methods within the class, as a class rule. Instead we set it on each test case; also remove the scala MockTime dependency.  2. List topics may not contain the deleted topics while their ZK paths are yet to be deleted; so the delete-check-recreate pattern may fail to successfully recreate the topic at all. Change the checking to read from zk path directly instead.  Another minor fix is to remove the misleading wait condition error message as the accumData is always empty.","closed","","guozhangwang","2017-10-19T04:46:27Z","2017-11-06T22:46:04Z"
"","3491","HOTFIX: Fixes to metric names","A couple of fixes to metric names to match the KIP - Removed extra strings in the metric names that are already in the tags - add a separate metric for ""all""","closed","","enothereska","2017-07-06T08:09:43Z","2017-08-04T06:06:48Z"
"","4021","KAFKA-5972 Flatten SMT does not work with null values","A bug in Flatten SMT while doing tests with different SMTs that are provided out-of-box. Flatten SMT does not work as expected with schemaless JSON that has properties with null values.  Example json:   {A={D=dValue, B=null, C=cValue}} The issue is in if statement that checks for null value.  CURRENT VERSION:   for (Map.Entry entry : originalRecord.entrySet()) {             final String fieldName = fieldName(fieldNamePrefix, entry.getKey());             Object value = entry.getValue();             if (value == null) {                 newRecord.put(fieldName(fieldNamePrefix, entry.getKey()), null);                 return;             }  PROPOSED VERSION:   for (Map.Entry entry : originalRecord.entrySet()) {             final String fieldName = fieldName(fieldNamePrefix, entry.getKey());             Object value = entry.getValue();             if (value == null) {                 newRecord.put(fieldName(fieldNamePrefix, entry.getKey()), null);                 continue;             }","closed","connect,","shivsantham","2017-10-04T23:03:09Z","2020-03-11T23:14:27Z"
"","4279","KAFKA-6284: Fixed system test for Connect REST API","`topics.regex` was added in KAFKA-3073. This change fixes the test that invokes `/validate` to ensure that all the configdefs are returned as expected.","closed","connect,","mikkin","2017-11-30T17:47:52Z","2020-10-16T06:24:13Z"
"","3823","MINOR: Fix transient failure in SelectorTest.testCloseConnectionInClosingState","`SelectorTest.testCloseConnectionInClosingState` creates a channel with some staging receives and moves time forward to expire the channel. To ensure that the channel will be expired on the next poll, the channel must be muted to avoid expiry time being updated if more data is available for read.","closed","","rajinisivaram","2017-09-10T17:52:24Z","2017-09-10T19:39:11Z"
"","4223","when closing a socket in response to an IOException, also print the root issue if closing fails","`Selector.pollSelectionKeys()` attempts to close the channel in response to an Exception (lets call this exception the root issue).  if the root issue itself is an IOException, its printed to log at debug level (which is usually off for production users):  ```java  catch (Exception e) {     String desc = channel.socketDescription();     if (e instanceof IOException)        log.debug(""Connection with {} disconnected"", desc, e);   <----- does not appear in real-life log     else if (e instanceof AuthenticationException) // will be logged later as error by clients        log.debug(""Connection with {} disconnected due to authentication exception"", desc, e);     else        log.warn(""Unexpected error from {}; closing connection"", desc, e);     close(channel, true);  } ```  for some cases, close itself would throw an exception. this exception is printed to log as a warning (`Selector.doClose()`): ```java try {    channel.close(); } catch (IOException e) {    log.error(""Exception closing connection to node {}:"", channel.id(), e); } ```  this tends to actually show up in user log, looking something like this (note - line numbers are from kafka 10.2.*): ``` java.io.IOException: Connection reset by peer 	at sun.nio.ch.FileDispatcherImpl.write0(Native Method) 	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) 	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) 	at sun.nio.ch.IOUtil.write(IOUtil.java:65) 	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470) 	at org.apache.kafka.common.network.SslTransportLayer.flush(SslTransportLayer.java:195) 	at org.apache.kafka.common.network.SslTransportLayer.close(SslTransportLayer.java:163) 	at org.apache.kafka.common.utils.Utils.closeAll(Utils.java:731) 	at org.apache.kafka.common.network.KafkaChannel.close(KafkaChannel.java:54) 	at org.apache.kafka.common.network.Selector.doClose(Selector.java:540) 	at org.apache.kafka.common.network.Selector.close(Selector.java:531) 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:378) 	at org.apache.kafka.common.network.Selector.poll(Selector.java:303) 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:349) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:233) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:131) 	at java.lang.Thread.run(Thread.java:745) ```  this issue spams user's logs and is not really helpful in diagnosing the real underlying cause, which (after turning debug logs on) turned out to be (for this particular case): ``` javax.net.ssl.SSLHandshakeException: General SSLEngine problem 	at sun.security.ssl.Handshaker.checkThrown(Handshaker.java:1409) 	at sun.security.ssl.SSLEngineImpl.checkTaskThrown(SSLEngineImpl.java:535) 	at sun.security.ssl.SSLEngineImpl.writeAppRecord(SSLEngineImpl.java:1214) 	at sun.security.ssl.SSLEngineImpl.wrap(SSLEngineImpl.java:1186) 	at javax.net.ssl.SSLEngine.wrap(SSLEngine.java:469) 	at org.apache.kafka.common.network.SslTransportLayer.handshakeWrap(SslTransportLayer.java:382) 	at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:243) 	at org.apache.kafka.common.network.KafkaChannel.prepare(KafkaChannel.java:69) 	at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:350) 	at org.apache.kafka.common.network.Selector.poll(Selector.java:303) 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:349) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:233) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:131) 	at java.lang.Thread.run(Thread.java:745) Caused by: javax.net.ssl.SSLHandshakeException: General SSLEngine problem 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) 	at sun.security.ssl.SSLEngineImpl.fatal(SSLEngineImpl.java:1728) 	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:304) 	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:296) 	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1478) 	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:212) 	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:957) 	at sun.security.ssl.Handshaker$1.run(Handshaker.java:897) 	at sun.security.ssl.Handshaker$1.run(Handshaker.java:894) 	at java.security.AccessController.doPrivileged(Native Method) 	at sun.security.ssl.Handshaker$DelegatedTask.run(Handshaker.java:1347) 	at org.apache.kafka.common.network.SslTransportLayer.runDelegatedTasks(SslTransportLayer.java:336) 	at org.apache.kafka.common.network.SslTransportLayer.handshakeUnwrap(SslTransportLayer.java:417) 	at org.apache.kafka.common.network.SslTransportLayer.handshake(SslTransportLayer.java:270) 	... 7 more Caused by: sun.security.validator.ValidatorException: No trusted certificate found 	at sun.security.validator.SimpleValidator.buildTrustedChain(SimpleValidator.java:384) 	at sun.security.validator.SimpleValidator.engineValidate(SimpleValidator.java:133) 	at sun.security.validator.Validator.validate(Validator.java:260) 	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) 	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:281) 	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:136) 	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1465) 	... 16 more ``` this patch allows submitting the root cause exception (the one caught in pollSelectionKeys) to close so that in the event close encounters an exception the root cause is also printed alongside the close exception.","closed","","radai-rosenblatt","2017-11-15T23:22:58Z","2018-04-18T01:50:39Z"
"","4350","Cached hashCode of a Node instance since it is immutable","`Node` structure is immutable so it is possible to cache `hashCode` of a `Node` instance as it's done in the `TopicPartition` class. Faced with the producer performance degradation in case of high load and large number of brokers (100), topics (150) and partitions (350). Made several diagnostic records with the java flight recorder and found that the method `HashSet::contains` in [`RecordAccumulator::ready`](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L423) takes about 40% of the whole time of the application. It is caused by re-calculating a hash code of a [leader](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L433) (`Node` instance) for every [batch entry](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java#L429). The cached hash code solved this issue and the corresponding time of `HashSet::contains` in `RecordAccumulator::ready` decreased to ~2%.","closed","","esevastyanov","2017-12-21T16:51:01Z","2018-02-15T22:11:00Z"
"","3588","HOTFIX: fix threading issue in MeteredKeyValueStore","`MeteredKeyValueStore` wasn't thread safe. Interleaving operations could modify the state, i.e, the `key` and/or `value` which could result in incorrect behaviour.","closed","","dguy","2017-07-27T14:54:49Z","2017-08-16T13:23:02Z"
"","4346","appends Materialized#with to include the ability to specify the store name as well as the serdes","`Materialized#with` doesn't allow you to specify both a store name and the key/value serdes. If you specify the name using `#as`, then the serdes are implied/inferred to be for `Serde` and it becomes ugly to use `#withKeySerde`, `#withValueSerde`, etc. This overload of `Materialized#with` allows both the store name *and* the specific Serdes desired to be specified in one go.  I've updated `MaterializedTest` with the expected new behaviour as well as the old behaviour (i.e. `null` store name when not specified).  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","kdrakon","2017-12-20T12:18:17Z","2018-02-08T20:06:39Z"
"","4428","KAFKA-6277: Ensure loadClass for plugin class loaders is thread-safe.","`loadClass` needs to be synchronized to protect subsequent calls to `defineClass`.   Details in the javadoc of this PR as well as here too: https://docs.oracle.com/javase/7/docs/technotes/guides/lang/cl-mt.html  /cc @ewencp @rhauch   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2018-01-16T20:40:32Z","2020-10-16T06:05:15Z"
"","3821","MINOR: Catch JsonMappingException subclass","`InvalidTypeIdException` cannot be catched currently","closed","","rhardouin","2017-09-08T22:34:35Z","2018-01-08T12:01:49Z"
"","3754","KAFKA-5804: retain duplicates in ChangeLoggingWindowBytesStore","`ChangeLoggingWindowBytesStore` needs to have the same `retainDuplicates` functionality as `RocksDBWindowStore` else data could be lost upon failover/restoration.","closed","","dguy","2017-08-29T15:20:02Z","2017-08-30T12:54:32Z"
"","4477","KAFKA-6451: Simplifying KStreamReduce and KStreamAggregate","[KAFKA-6451](https://issues.apache.org/jira/browse/KAFKA-6451)  Simplified KStreamReduce and KStreamAggregate. Updated comments in KStreamAggregate.","closed","","tanvijaywant31","2018-01-26T00:12:47Z","2018-01-30T01:39:43Z"
"","4438","KAFKA-4932 add UUID serializer / deserializer","[KAFKA-4932](https://issues.apache.org/jira/browse/KAFKA-4932)  Added a UUID Serializer / Deserializer.  Added the UUID type to the SerializationTest","closed","streams,","brandonkirchner","2018-01-18T18:35:35Z","2018-09-10T00:24:16Z"
"","3872","KAFKA-5716: Recent polled offsets may not be written/flushed at SourceTask.commit","@rhauch @tedyu @hachikuji   For now a test showing that the claimed problem is true. Should definitely not be committed - its a failing test. It fails trying to assert SourceTask.commit javadoc: Commit the offsets, up to the offsets that have been returned by {@link #poll()}  The contribution is my (@steff1193) original work and I license the work to the project under the project's open source license.","open","connect,","steff1193","2017-09-15T13:20:38Z","2020-03-22T06:27:49Z"
"","4435","MINOR need to get a new transformer for each get() call. can't share'em","@jeyhunkarimov @mjsax @guozhangwang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","norwood","2018-01-18T00:15:51Z","2018-01-18T16:45:23Z"
"","4171","MINOR: Change version format in release notes python code","@ijuma @ewencp","closed","","guozhangwang","2017-11-02T19:23:39Z","2017-11-06T22:46:12Z"
"","4157","KAFKA-6130: VerifiableConsumer with --max-messages","@hachikuji can you review this? Do you know the best place to write a test, since there doesn't seem to be a unit test for the `VerifiableConsumer`?","closed","","tombentley","2017-10-30T11:09:47Z","2017-10-30T17:00:47Z"
"","4098","verifiable_consumer: fix var typo in assert","@hachikuji","closed","","edenhill","2017-10-19T14:27:32Z","2017-10-19T15:43:55Z"
"","4059","Redesign of Streams page to include video & logos","@guozhangwang Please review.","closed","","manjuapu","2017-10-11T19:59:53Z","2017-10-11T22:17:51Z"
"","4164","Adding Trivago logo","@guozhangwang Please review","closed","","manjuapu","2017-10-31T17:28:25Z","2017-10-31T20:26:48Z"
"","4161","Adding lighthouse logos and nav bar","@guozhangwang Please review","closed","","manjuapu","2017-10-30T20:33:16Z","2017-10-31T17:28:43Z"
"","4160","Customer logo stream","@guozhangwang Please review","closed","","manjuapu","2017-10-30T19:52:24Z","2017-10-30T20:19:00Z"
"","3637","KAFKA-4879 KafkaConsumer.position may hang forever when deleting a topic","@guozhangwang  I created this PR but it lacks the test. I also has the test but I wanted to ask which Test file should be the best candidate for this? Also there is a TODO line which I created because that method already has a timeout parameter, what do you think how can we proceed there?","closed","","baluchicken","2017-08-07T15:04:43Z","2017-12-22T06:32:00Z"
"","3645","MINOR: support retrieving cluster_id in system tests","@ewencp would be great to cherry-pick this back into 0.11.x if possible","closed","","xvrl","2017-08-08T23:51:52Z","2017-08-09T03:59:48Z"
"","3818","MINOR: KAFKA-5742 follow-up, fix incorrect method name","@ewencp @ijuma, also needs backport to 0.11.0","closed","","xvrl","2017-09-08T17:39:59Z","2017-09-08T18:07:09Z"
"","4123","MINOR: reset state in cleanup, fixes jmx mixin flakiness","@ewencp @ijuma","closed","","xvrl","2017-10-24T00:40:00Z","2017-10-25T20:36:14Z"
"","3963","MINOR:Updated Rabobank description","@dguy Please review","closed","","manjuapu","2017-09-26T16:51:12Z","2017-09-27T08:35:16Z"
"","4169","MINOR: Update docs for new version","1. Update the Streams hello world examples with the new API. 2. Update the version references in various places. 3. Update version templates to 1.1.x.","closed","","guozhangwang","2017-11-01T23:25:55Z","2017-11-06T22:46:13Z"
"","3713","MINOR: simplify state transition for Kafka Streams and stream threads","1. StreamThread: prevent `PARTITIONS_REVOKED` to transit to itself in `setState` by returning false. And only execute the task suspension logic when `setState(PARTITIONS_REVOKED)` returns true in `onPartitionsRevoked`.  2. StreamThread: minor, renaming `shutdown` to `completeShutdown`, and `close` to `shutdown`, `stillRunning` to `isRunning`, `isInitialized` to `isRunningAndNotRebalancing`.  3. GlobalStreamThread: tighten the transition a bit in `setState`. Force transiting to `PENDING_SHUTDOWN` and `DEAD` when initialization failed.  4. GlobalStreamThread: minor, add logPrefix to StateConsumer. Also removing its state change listener when closing the thread.  5. KafkaStreams: because of 1) above we can now prevent its `REBALANCING` to `REBALANCING`.   6. KafkaStreams: prevent `CREATED` to ever go to `REBALANCING` first to force it transit to `RUNNING` when starting. Also prevent `CREATED` to go to `ERROR`.  7. KafkaStreams: collapse `validateStartOnce` and `checkFirstTimeClosing ` into `setState`.  8. KafkaStreams: in `close` and `start`, only execute the logic when `setState` succeeds.","closed","","guozhangwang","2017-08-22T20:27:55Z","2017-11-06T22:45:21Z"
"","3618","KAFKA-5698: Sort processor nodes based on its sub-tree size","1. Sort processor nodes within a sub-topology by its sub-tree size: nodes with largest sizes are source nodes and hence printed earlier.  2. Sort sub-topologies by ids; sort global stores by the source topic names.  3. Open for discussion: start newlines for predecessor and successor.  4. Minor: space between processor nodes and stores / topics; maintain `[]` for the topic names.","closed","","guozhangwang","2017-08-04T00:45:57Z","2017-11-06T22:45:59Z"
"","4419","MINOR: Improve Join integration test coverage, PART II","1. Replaced KStreamKTableJoinIntegrationTest with the abstract based StreamTableJoinIntegrationTest. Added details on per-step verifications. 2. Minor renaming on GlobalKTableIntegrationTest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-12T23:42:07Z","2020-04-24T23:47:20Z"
"","3748","KAFKA-5797: Delay checking of partition existence in StoreChangelogReader","1. Remove timeout-based validatePartitionExists from StoreChangelogReader; instead only try to refresh metadata once after all tasks have been created and their topology initialized (hence all stores have been registered). 2. Add the logic to refresh partition metadata at the end of initialization if some restorers needing initialization cannot find their changelogs, hoping that in the next run loop these stores can find their changelogs.  As a result, after `initialize` is called we may not be able to start initializing all the `needsInitializing` ones.  As an optimization, we would not call `consumer#partitionsFor` any more, but only `consumer#listTopics` fetching all the topic metadata; so the only blocking calls left are `listTopics` and `endOffsets`, and we always capture timeout exceptions around these two calls, and delay to retry in the next run loop after refreshing the metadata. By doing this we can also reduce the number of request round trips between consumer and brokers.","closed","","guozhangwang","2017-08-27T18:16:00Z","2017-11-06T22:45:26Z"
"","3640","KAFKA-5701: fix flaky unit test","1. Remove separate thread from test failing periodically due to race condition. 2. Remove anonymous `AbstractNotifyingBatchingRestoreCallback` declare as concrete inner class `RocksDBBatchingRestoreCallback` and set as package private variable.  Class is static so it has to initialize it's dependency on `RocksDBStore`","closed","","bbejeck","2017-08-07T18:31:41Z","2017-08-07T22:24:05Z"
"","3603","KAFKA-5671 Followup: Remove reflections in unit test classes","1. Remove rest deprecation warnings in streams:jar.  2. Consolidate all unit test classes' reflections to access internal topology builder from packages other than `o.a.k.streams`. We need to refactor the hierarchies of StreamTask, StreamThread and KafkaStreams to remove these hacky reflections.  3. Minor fixes such as reference path, etc.  4. Minor edits on web docs for the describe function under developer-guide.","closed","","guozhangwang","2017-08-01T00:35:51Z","2017-11-06T22:45:08Z"
"","4436","MINOR: Improve on reset integration test","1. Refactor AbstractResetIntegrationTest with producer / consumer / streams config, along with common client config embedded with ssl configs if necessary. 2. Use specific test id to replace the testNo suffix. 3. In testReprocessingFromScratchAfterResetWithIntermediateUserTopic, additional check on intermediate result. 4. Add the same result for SSL test suite as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-18T01:38:28Z","2020-04-24T23:47:26Z"
"","3928","KAFKA-5947: Handle authentication failure in admin client, txn producer","1. Raise AuthenticationException for authentication failures in admin client 2. Handle AuthenticationException as a fatal error for transactional producer 3. Add comments to authentication exceptions","closed","","rajinisivaram","2017-09-20T23:17:51Z","2017-09-21T20:52:08Z"
"","3918","KAFKA-5920: Handle SSL handshake failures as authentication exceptions","1. Propagate `SSLException` as `SslAuthenticationException` to enable clients to report these and avoid retries 2. Updates to `SslTransportLayer` to process bytes received even if end-of-stream 3. Some tidy up of authentication handling 4. Report exceptions in SaslClientAuthenticator as AuthenticationExceptions","closed","","rajinisivaram","2017-09-20T08:33:38Z","2017-09-22T20:00:01Z"
"","3534","KAFKA-5006: Improve thrown exception","1. Only log an ERROR on the first encountered exception from the callback.  2. Wrap the exception message with the first thrown message information, and throw the exception whenever `checkException` is called.  Therefore, for the `store.put` call, it will throw a `KafkaException` with the error message a bit more intuitive.","closed","","guozhangwang","2017-07-15T23:39:48Z","2017-11-06T22:45:01Z"
"","3485","KAFKA-5531: throw concrete exceptions in streams tests","1. Now instead of just generic `Exception` methods declare more concrete exceptions throwing or don't declare any throwing at all, if not needed. 2. `SimpleBenchmark.run()` throws `RuntimeException` 3. `SimpleBenchmark.produce()` throws `IllegalArgumentException` 4. Expect `ProcessorStateException` in `StandbyTaskTest.testUpdateNonPersistentStore()`  /cc @enothereska","closed","","evis","2017-07-04T11:57:21Z","2017-09-13T00:46:26Z"
"","3515","MINOR: Make streams quick start more interactive","1. Make the WordCountDemo application to not stop automatically but via ""ctrl-C"". 2. Update the quickstart html file to let users type input messages one-by-one, and observe added output in an interactive manner. 3. Some minor fixes on the parent documentation page pointing to streams sub-pages, added a new recommended Scala version number.","closed","","guozhangwang","2017-07-10T16:42:20Z","2017-11-06T22:45:02Z"
"","4384","KAFKA-6398: fix KTable.filter that does not include its parent's queryable storename","1. Include the parent's queryable store name in KTable.filter if this operator is not materialized. 2. Augment InternalTopologyBuilder checking on null processor / store names from the enum. 3. Unit test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-03T20:12:14Z","2020-04-24T23:49:54Z"
"","4315","KAFKA-6150: KIP-204 part III; Change repartition topic segment size and ms","1. Create default internal topic configs in StreamsConfig, especially for repartition topics change the segment size and time to smaller value. 2. Consolidate the default internal topic settings to InternalTopicManager and simplify InternalTopicConfig correspondingly. 3. Add an integration test for purging data. 4. MINOR: change TopologyBuilderException to IllegalStateException in StreamPartitionAssignor (part of https://issues.apache.org/jira/browse/KAFKA-5660).  Here are a few public facing APIs that get added:  1. AbstractConfig#originalsWithPrefix(String prefix, boolean strip): this for simplify the logic of passing admin and topic prefixed configs to consumer properties. 2. KafkaStreams constructor with Time object for convienent mocking in tests.  Will update KIP-204 accordingly if people re-votes these changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-12-12T01:29:29Z","2018-02-14T19:50:55Z"
"","3732","MINOR: doc changes for KIP-138","1. Core concepts (added the stream time definition), upgrade guide and developer guide. 2. Related Java docs changes.","closed","","guozhangwang","2017-08-24T23:56:39Z","2017-11-06T22:45:36Z"
"","3657","Kafka-5676: MockStreamsMetrics should be in o.a.k.test","1. Change package for MockStreamsMetrics class 2. Removed metric object from  MockStreamsMetrics constcruor","open","streams,","DevelopersWithPassion","2017-08-11T10:54:49Z","2018-12-17T04:57:55Z"
"","3574","HOTFIX: handle commit failed exception on stream thread's suspend task","1. Capture `CommitFailedException` in `StreamThread#suspendTasksAndState`.  2. Remove `Cache` from AbstractTask as it is not needed any more; remove not used cleanup related variables from StreamThread (cc @dguy to double check).  3.  Also fix log4j outputs for error and warn, such that for WARN we do not print stack trace, and for ERROR we remove the dangling colon since the exception stack trace will start in newline.  4. Update one log4j entry to always print as WARN for errors closing a zombie task (cc @mjsax ).","closed","","guozhangwang","2017-07-25T22:52:01Z","2017-11-06T22:45:05Z"
"","4096","HOTFIX: Poll with zero milliseconds during restoration phase","1. After the poll call, re-check if the state has been changed or not; if yes, initialize the tasks again. 2. Minor log4j improvements.","closed","","guozhangwang","2017-10-19T05:16:28Z","2017-11-06T22:46:03Z"
"","4071","MINOR: a few web doc and javadoc fixes","1. Added missing Javadocs in public interfaces. 2. Added missing upgrade web docs. 3. Minor improvements on exception messages.","closed","","guozhangwang","2017-10-13T05:48:17Z","2017-11-06T22:45:46Z"
"","4338","MINOR: Web docs for KIP-220","1. added functions for KafkaStreams and KafkaClientSupplier. 2. added prefix for admin client in StreamsConfig.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-12-18T20:09:45Z","2017-12-22T19:54:49Z"
"","3687","MINOR: add upgrade section for 1.0.0","1. Add upgrade section for 1.0.0, including Streams API changes section. 2. Add metrics name changes section.","closed","","guozhangwang","2017-08-17T17:45:14Z","2017-11-06T22:45:18Z"
"","4270","KAFKA-6150: Purge repartition topics","1. Add the repartition topics information into ProcessorTopology: personally I do not like leaking this information into the topology but it seems not other simple way around. 2. StreamTask: added one more function to expose the consumed offsets from repartition topics only. 3. TaskManager: use the AdminClient to send the gathered offsets to delete only if a) previous call has completed and client intentionally ignore-and-log any errors, or b) no requests have ever called before.  NOTE that this code depends on the assumption that purge is only called right after the commit has succeeded, hence we presume all consumed offsets are committed.  4. MINOR: Added a few more constructor for ProcessorTopology for cleaner unit tests. 5. MINOR: Extracted MockStateStore out of the deprecated class. 6. MINOR: Made a pass over some unit test classes for clean ups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-11-28T18:00:36Z","2018-02-14T19:51:01Z"
"","4211","KAFKA-6170; KIP-220 Part 1: Add AdminClient to Streams","1. Add The AdminClient into Kafka Streams, which is shared among all the threads. 2. Add ADMIN_PREFIX to StreamsConfig. 3. Also made a few tweaks on the metrics of the AdminClient, which is slightly different from the StreamsKafkaClient (note these changes will not be reflected in this PR but only take place when we eventually replace StreamsKafkaClient): 3.1. ""clientId"" tag will be set as ""clientId-admin"": in StreamsKafkaClient it is whatever user sets, and hence could even be null. 3.2. ""groupPrefix"" will be set as ""admin-client"": in StreamsKafkaClient it will be ""kafka-client"".  So the metrics from `StreamsKafkaClient` to `AdminClient` would be changed from  `kafka.admin.client:type=kafka-client-metrics,client-id=`  to   `kafka.admin.client:type=admin-client-metrics,client-id=myApp-UUID-admin`   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-11-14T21:46:19Z","2018-02-14T19:51:50Z"
"","4331","MINOR: Improve Join integration test coverage, PART I","0. Rename `JoinIntegrationTest` to `StreamStreamJoinIntegrationTest`, which is only for KStream-KStream joins. 1. Extract the `AbstractJoinIntegrationTest` which is going to be used for all the join integration test classes, parameterized with and without caching. 2. Merge `KStreamRepartitionJoinTest.java` into `StreamStreamJoinIntegrationTest.java` with augmented stream-stream join. 3. Add `TableTableJoinIntegrationTest` with detailed per-step expected results and removed `KTableKTableJoinIntegrationTest`.  Findings of the integration test:  1. Confirmed KAFKA-4309 with caching turned on. 2. Found bug KAFKA-6398. 3. Found bug KAFKA-6443. 4. Found a bug that in CachingKeyValueStore, we would flush before putting the record into the underlying store, when the store is going to be used in the downstream processors with flushing it would result in incorrect results, fixed the issue along with this PR. 5. Consider a new optimization described in KAFKA-6286.  Future works including stream-table joins will be in other PRs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2017-12-15T22:26:16Z","2020-04-24T23:47:18Z"
"","3630","KAFKA-5727: Add Streams quickstart tutorial as an archetype project","0. Minor fixes on the existing examples to merge all on a single input topic; also do not use `common.utils.Exit` as it is for internal usage only.  1. Add the archetype project for the quickstart. Steps to try it out:    a. `mvn install` on the quickstart directory.   b. `mvn archetype:generate \ -DarchetypeGroupId=org.apache.kafka \ -DarchetypeArtifactId=streams-quickstart-java \ -DarchetypeVersion=1.0.0-SNAPSHOT \ -DgroupId=streams-quickstart \ -DartifactId=streams-quickstart \ -Dversion=0.1 \ -Dpackage=StreamsQuickstart \ -DinteractiveMode=false` at any directory to create the project.   c. build the streams jar with version `1.0.0-SNAPSHOT` to local maven repository with `./gradlew installAll`; `cd streams-quickstart; mvn clean package`   d. create the input / output topics, start the console producer and consumer.   e. start the program: `mvn exec:java -Dexec.mainClass=StreamsQuickstart.Pipe/LineSplit/WordCount`.   f. type data on console producer and observe data on console consumer.","closed","","guozhangwang","2017-08-04T23:28:07Z","2017-11-06T22:45:12Z"
"","4302","KAFKA-6326: when broker is unavailable, fast revover","- when broker is unavailable(such as broker's machine is down), controller will wait 30 sec timeout by dedault. it seems to be that the timeout waiting is not necessary. It will be increase the MTTR of dead broker . ![_f7d1d2b4-39ae-4e02-8519-99bcba849668](https://user-images.githubusercontent.com/231336/33720382-c38061d8-db9e-11e7-8a57-083d25abe1a8.png)","open","","leonhong","2017-12-07T14:34:35Z","2018-01-08T12:16:42Z"
"","4229","MINOR: Code cleanup","- Used arrays directly instead of constructing List while getting storeNames - Used proper Integer identifier in the Log.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2017-11-17T08:42:22Z","2018-02-19T11:35:59Z"
"","4393","KAFKA-4897: Add pause method to ShutdownableThread","- Use newly added pause method in LogCleaner and ControllerChannelManager classes  - Remove LogCleaner, Cleaner exclusions from findbugs-exclude.xml  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-01-05T07:38:01Z","2018-07-03T15:46:47Z"
"","3895","MINOR: Protocol schema refactor follow-up","- Use constants in a few places that were missed - Remove ProtoUtils by moving its methods to Schema - Merge SchemaVisitor and SchemaVisitorAdapter - Change SchemaVisitor package.","closed","","ijuma","2017-09-19T04:15:39Z","2017-12-22T18:26:23Z"
"","4007","MINOR: Java 9 version handling improvements","- Upgrade Gradle to 4.2.1, which handles Azul Zulu 9's version correctly. - Add tests to our Java version handling code - Refactor the code to make it possible to add tests - Rename `isIBMJdk` method to use consistent naming convention.","closed","","ijuma","2017-10-03T11:19:00Z","2017-12-22T18:24:25Z"
"","4231","MINOR: Small cleanups/refactoring in kafka.controller","- Updated logging to use string templates - Minor refactors - Fixed a few typos  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2017-11-18T20:43:06Z","2018-04-18T13:27:58Z"
"","4427","KAFKA-6447: Add Delegation Token Operations to KafkaAdminClient (KIP-249)","- Update unit and integration tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-01-16T18:34:32Z","2018-07-03T15:45:16Z"
"","4411","Fix AK streams doc nits","- Table formatting in /developer-guide/dsl-api - Missing /streams/developer-guide `include virtual` files - Improperly formatted Notes","closed","","joel-hamill","2018-01-11T00:33:05Z","2018-01-29T18:10:59Z"
"","4016","MINOR: Simplify log cleaner and fix compiler warnings","- Simplify LogCleaner.cleanSegments and add comment regarding thread unsafe usage of `LogSegment.append`. This was a result of investigating KAFKA-4972. - Fix compiler warnings (in some cases use the fully qualified name as a workaround for deprecation warnings in import statements).","closed","","ijuma","2017-10-04T14:25:44Z","2017-12-22T18:24:24Z"
"","4221","KAFKA-6215: KafkaStreamsTest fails in trunk","- set streams state.dir to test-dir (default /tmp is not reliable)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-15T16:13:51Z","2017-11-15T19:47:20Z"
"","4215","KAFKA-6121: Restore and global consumer should not use auto.offset.reset","- set auto.offset.reste to ""none"" for restore and global consumer - handle InvalidOffsetException for restore and global consumer - add corresponding tests - some minor cleanup  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-15T04:11:18Z","2017-12-11T19:02:05Z"
"","4040","KAFKA-6324: Change LogSegment.delete to deleteIfExists and harden log recovery","- Rename `delete()` to `deleteIfExists()` in `LogSegment`, `AbstractIndex` and `TxnIndex`. Throw exception in case of IO errors for more informative errors and to make it less likely that errors are ignored, `boolean` is used for the case where the file does not exist (like `Files.deleteIfExists()`). - Fix an instance of delete while open (should fix KAFKA-6322 and KAFKA-6075). - `LogSegment.deleteIfExists` no longer throws an exception if any of the files it tries to delete does not exist (fixes KAFKA-6194). - Remove unnecessary `FileChannel.force(true)` when deleting file. - Introduce `LogSegment.open()` and use it to improve encapsulation and reduce duplication. - Expand functionality of `LogSegment.onBecomeInactiveSegment()` to reduce duplication and improve encapsulation. - Use `AbstractIndex.deleteIfExists()` instead of deleting files manually. - Improve logging when deleting swap files. - Use CorruptIndexException instead of IllegalArgumentException. - Simplify `LogCleaner.cleanSegments()` to reduce duplication and improve encapsulation. - A few other clean-ups in Log, LogSegment, etc.","closed","","ijuma","2017-10-08T11:57:39Z","2021-04-05T21:51:05Z"
"","4129","KAFKA-6115: TaskManager should be type aware","- remove type specific methods from Task interface  - add generics to preserve task type  - add sub classes for different task types","closed","","mjsax","2017-10-24T23:42:58Z","2017-11-06T19:18:42Z"
"","3930","KAFKA-5856; AdminClient.createPartitions() follow-up (KIP-195)","- Remove DelayedCreatePartitions to reduce code duplication - Avoid unnecessary ZK calls (call it once per request instead of once per topic, if possible) - Simplify code - A few minor clean-ups","closed","","ijuma","2017-09-21T04:30:36Z","2017-12-22T18:22:33Z"
"","4067","MINOR: Merge script improvements","- Remove ""list commits"" since we never use it - Fix release branch detection to just look for branches that start with digits - Make script executable","closed","","ijuma","2017-10-12T14:39:15Z","2017-12-22T18:23:55Z"
"","3775","KAFKA-5818: KafkaStreams state transitions not correct","- need to check that state is CRATED at startup - some minor test cleanup","closed","","mjsax","2017-08-31T22:45:37Z","2017-09-04T15:39:12Z"
"","4112","MINOR: Rename and change package of async ZooKeeper classes","- kafka.controller.ZookeeperClient -> kafka.zookeeper.ZooKeeperClient - kafka.controller.ControllerZkUtils -> kafka.zk.KafkaZkClient - kafka.controller.ZkData -> kafka.zk.ZkData - Renamed various fields to match new names and for consistency - A few clean-ups in ZkData - Document intent","closed","","ijuma","2017-10-22T07:58:18Z","2017-12-22T18:23:39Z"
"","3503","KAFKA-5570: Join request's timeout should be slightly higher than the rebalance timeout","- Introduce per request timeouts in NetworkClient - Use rebalanceTimeout + 5s for join request timeout - A few minor clean-ups in NetworkClient and ConsumerNetworkClient  This should also fix the Streams issue that we set max.poll.interval.ms (aka rebalanceTimeout) to infinite without bumping the request timeout.  Still to do: - Tests","closed","","ijuma","2017-07-07T14:33:49Z","2018-06-11T21:59:20Z"
"","3542","KAFKA-5362: Follow up to Streams EOS system test","- improve tests to get rid of calls to `sleep` in Python  - fixed some flaky test conditions  - improve debugging","closed","","mjsax","2017-07-18T04:29:47Z","2017-10-09T18:30:54Z"
"","3859","KAFKA-5893: ResetIntegrationTest fails","- improve stderr output for better debugging","closed","","mjsax","2017-09-14T17:12:22Z","2017-09-18T17:01:53Z"
"","4300","KAFKA-6269: KTable restore fails after rebalance","- If more records are left when restoration is completed, return offset of next record - added integration test to verify behavior vs unit test as hard to mock all relevant behavior for all moving parts. - updated unit test   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2017-12-06T21:59:12Z","2018-01-02T17:42:52Z"
"","4003","MINOR: add suppress warnings annotations","- fixes examples with regard to new API  - fixes `Topology#addGlobalStore` parameters","closed","","mjsax","2017-10-02T19:21:24Z","2017-10-04T22:14:48Z"
"","4360","MINOR: Fixed clusterId reference in Metadata.","- Fixed clusterId reference in Metadata. - Fixed log message with respective error in KerberosLogin.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2017-12-27T06:17:46Z","2017-12-28T18:21:44Z"
"","4359","MINOR: Fix zk client session state metric names and various async zk clean-ups","- Fix zk session state and session change rate metric names: type should be SessionExpireListener instead of KafkaHealthCheck. Test verifying the fix was included. - Handle missing controller in controlled shutdown in the same way as if the broker is not registered (i.e. retry after backoff). - Restructure BrokerInfo to reduce duplication. It now contains a Broker instance and the JSON serde is done in BrokerIdZNode since `Broker` does not contain all the fields. - Remove dead code from `ZooKeeperClient.initialize` and remove redundant `close` calls. - Move ACL handling and persistent paths definition from ZkUtils to ZkData (and call ZkData from ZkUtils). - Remove ZooKeeperClientWrapper and ZooKeeperClientMetrics from ZkUtils (avoids metrics clash if third party users create a ZkUtils instance in the same process as the broker). - Introduce factory method in KafkaZkClient that creates ZooKeeperClient and remove metric name defaults from ZooKeeperClient. - Fix a few instances where ZooKeeperClient was not closed in tests. - Update a few TestUtils methods to use KafkaZkClient instead of ZkUtils. - Add test verifying SessionState metric. - Various clean-ups.  Testing: mostly relying on existing tests, but added a couple of new tests as mentioned above.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2017-12-26T19:33:54Z","2018-01-03T22:03:45Z"
"","3536","KAFKA-3856: Refactoring for KIP-120","- extract InternalTopologyBuilder from TopologyBuilder  - deprecate all ""leaking"" methods from public TopologyBuilder API  - changed TopologyDescription and all nested classed into interfaces","closed","","mjsax","2017-07-17T08:45:42Z","2018-06-05T23:50:45Z"
"","4283","KAFKA-6193: Only delete reassign_partitions znode after reassignment is complete","- Ensure that `partitionsBeingReassigned` is fully populated before `removePartitionFromReassignedPartitions` is invoked. This is necessary to avoid premature deletion of the `reassign_partitions` znode. - Modify and add tests to verify the fixes. - Add documentation. - Use `info` log message if assignedReplicas == newReplicas and remove control flow based on exceptions. - General logging improvements. - Simplify `initializePartitionAssignment` by relying on logic already present in `maybeTriggerPartitionReassignment`.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-12-01T15:26:54Z","2017-12-22T18:21:27Z"
"","4152","MINOR: Eliminate unnecessary Topic(And)Partition allocations in Controller","- Eliminated all the unnecessary allocations of `TopicPartition` and `TopicAndPartition` in the Controller. We now use the former in the Controller (bringing it inline with the rest of the non legacy code). - Fixed missed `Listener` -> `Handler` renames for companion objects. - More String.format -> String interpolation conversions (the former is roughly 5 times more expensive). - Some other minor clean-ups.","closed","","ijuma","2017-10-28T09:05:52Z","2017-12-22T18:21:54Z"
"","3845","KAFKA-4501: Fix EasyMock and disable PowerMock tests under Java 9","- EasyMock 3.5 supports Java 9.  - Fixed issues in `testFailedSendRetryLogic` and `testCreateConnectorAlreadyExists` exposed by new EasyMock version. The former was passing `anyObject` to `andReturn`, which doesn't make sense. This was leaving behind a global `any` matcher, which caused a few issues in the new version. Fixing this meant that the correlation ids had to be updated to actually match. The latter was missing a couple of expectations that the previous version of EasyMock didn't catch.  - Removed unnecessary PowerMock dependency from 3 tests.  - Disabled remaining PowerMock tests when running with Java 9 until https://github.com/powermock/powermock/issues/783 is in a release.  - Once we merge this PR, we can enable tests in the Java 9 builds in Jenkins.","closed","","ijuma","2017-09-13T12:17:40Z","2017-09-17T15:20:31Z"
"","3986","KAFKA-5949: Follow-up after latest KIP-161 changes","- compare KAFKA-5958","closed","","mjsax","2017-09-28T21:00:24Z","2017-09-29T17:02:21Z"
"","3623","HOTFIX: state transition cherry picking for 0.10.2","- Cherry picked from #3432  - Minor checkstyle fixes to get to build","closed","","enothereska","2017-08-04T10:49:40Z","2017-08-17T16:31:11Z"
"","3669","KAFKA-5726: KafkaConsumer.subscribe() overload that takes just Pattern","- changed the interface & implementations - updated tests to use the new method where applicable","closed","","attilakreiner","2017-08-15T09:20:52Z","2017-09-06T18:46:47Z"
"","4210","KAFKA-6167: Timestamp on streams directory contains a colon, which is an illegal character","- change segment delimiter to .  - added upgrade path  - added test for old and new upgrade path","closed","","mjsax","2017-11-14T21:17:28Z","2017-11-16T05:53:28Z"
"","3939","KAFKA-5949: User Callback Exceptions need to be handled properly","- catch user exception in user callback (TimestampExtractor, DeserializationHandler, StateRestoreListener) and wrap with StreamsException  Additional cleanup:  - rename globalRestoreListener to userRestoreListener  - remove unnecessary interface -> collapse SourceNodeRecordDeserializer and RecordDeserializer  - removed unused parameter loggingEnabled from ProcessorContext#register","closed","","mjsax","2017-09-21T21:18:19Z","2017-09-28T17:43:00Z"
"","4365","KAFKA-5368: Add test for skipped-records metric","- adding a missing test for KAFKA-5368","closed","","mjsax","2017-12-29T02:54:55Z","2018-01-02T20:25:15Z"
"","3544","KAFKA-5549 : Explain that 'client.id' is just used as a prefix within Streams","- Added new String CLIENT_ID_DOC in StreamsConfig for explanation","closed","","PranavManiar","2017-07-18T11:50:51Z","2017-07-27T15:02:05Z"
"","4005","MINOR: fix JavaDocs warnings","- add some missing annotations for deprecated methods","closed","","mjsax","2017-10-03T03:16:58Z","2017-10-03T16:18:02Z"
"","4402","KAFKA-3625: Add public test utils for Kafka Streams","- add new artifact test-utils  - add TopologyTestDriver  - add MockTime, TestRecord, add TestRecordFactory  This PR requires a KIP and is WIP. DO NOT MERGE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2018-01-08T21:21:57Z","2018-01-30T02:20:50Z"
"","4374","MINOR: Fix concurrency bug in MetadataCache and Metadata request when listeners inconsistent","- Add missing locking/volatile in MetadataCache.aliveEndPoint - Fix topic metadata not to throw BrokerNotAvailableException when listeners are inconsistent. Add test verifying the fix. As part of this fix, renamed Broker methods to follow Map convention where the `get` version returns `Option`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2018-01-02T11:55:56Z","2018-01-04T18:16:05Z"
"","3616","KAFKA-4541: Support for delegation token mechanism","- Add capability to create delegation token - Add authentication based on delegation token. - Add capability to renew/expire delegation tokens. - Add units tests and integration tests","closed","","omkreddy","2017-08-03T15:28:41Z","2018-07-03T15:46:42Z"
"","4077","(WIP) KAFKA-5142: Added support for record headers, reusing Kafka client's interfaces","*This is still a work in progress and should not be merged.*  This is a proposed PR that implements most of [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) but with some changes. The Kafka client library's `Headers` and `Header` interfaces are used directly so as to minimize the overhead of converting instances to a Connect-specific object. However, a new `ConnectHeaders` class is proposed to provide a fluent builder for easily constructing headers either in source connectors or SMTs that need to add/remove/modify headers, and a reader utility component for reading header values and converting to primitives.  Note that KIP-145 is still undergoing discussions, so this is provided merely as one possible approach.","closed","","rhauch","2017-10-16T23:15:05Z","2018-02-25T21:43:25Z"
"","4277","MINOR: schema equality check failing for complex objects","*The schema equality need to be done with the equals method to avoid unwanted side effects. When applied to complex objects with the same schema it was failing.*  *To test the faulty behaviour try calling the method on a complex object (at least 3 levels of nesting and array fields with a lot of objects).*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ntrp","2017-11-29T19:33:21Z","2020-02-11T17:38:03Z"
"","4269","KAFKA-5117: Add password masking for kafka connect REST endpoint","*More detailed description of your change, Mask all password type config parameter with ""*********"" instead of displaying the plain text in kafka connect REST endpoint.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","qiao-meng-zefr","2017-11-27T20:12:04Z","2018-11-14T19:53:24Z"
"","4314","KAFKA-6311: Expose Kafka cluster ID in Connect REST API (KIP-238)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Simple unit tests sufficiently exercise the behavior. In fact, this addition increases coverage since `RootResource` was not previously unit tested.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ewencp","2017-12-11T23:38:02Z","2020-10-16T06:05:14Z"
"","4420","MINOR: Adding Streams Use cases anchor","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  It is just adding link PR for easy sharing of streams use cases. I am not doing any test coverage or verifying documentation","closed","","manjuapu","2018-01-13T00:54:47Z","2018-01-18T17:59:33Z"
"","4476","MINOR: Fix typo in log","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dminkovsky","2018-01-25T22:12:16Z","2018-01-26T17:17:09Z"
"","4452","MINOR: Fixed a few typos in configs and streams docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2018-01-20T11:53:53Z","2018-04-18T13:28:06Z"
"","4425","Fix AK streams doc nits","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2018-01-16T17:03:48Z","2018-01-16T17:07:13Z"
"","4414","MINOR: Update TupleForwarder comment","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dminkovsky","2018-01-11T15:37:08Z","2018-02-08T01:41:44Z"
"","4405","Menu updates and navigation","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2018-01-09T18:19:29Z","2018-01-10T00:28:50Z"
"","4382","Kafka 6383: complete shut down for streams threads that have not started","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2018-01-03T18:22:05Z","2018-03-06T01:03:50Z"
"","4370","Lazy","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jaypearson2002","2018-01-01T10:22:05Z","2018-01-02T17:45:19Z"
"","4316","kafka-future-whencomplete","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cmccabe","2017-12-12T02:47:44Z","2019-05-20T18:56:39Z"
"","4236","testing...","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","samjawale","2017-11-19T10:29:23Z","2017-11-19T22:55:57Z"
"","4212","MINOR: Introduction fixup","*Clarify multi-tenant support, geo-replication, and some grammar fixes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","joel-hamill","2017-11-14T23:01:08Z","2017-11-15T22:35:05Z"
"","4431","KAFKA-2170: Updated Fixes For Windows Platform","**This is continuous development of [original pull request](https://github.com/apache/kafka/pull/3283) made by [nxmbriggs404](https://github.com/nxmbriggs404).**  I am using Kafka primary on Windows platform; but [KAFKA-1194](https://issues.apache.org/jira/browse/KAFKA-1194) affects all Kafka versions on Windows and makes it unusable without restarts. There are several pull requests available to fix this problem but most of them are outdated. [Nxmbriggs404 pull request](https://github.com/apache/kafka/pull/3283) is the most recent of them. Unfortunately, there is no further discussion and progress on forementioned pull request, so I created a new one.  Pull request contains the most recent merge of **apache/kafka** **trunk** branch into [original pull request](https://github.com/apache/kafka/pull/3283) made by [nxmbriggs404](https://github.com/nxmbriggs404) with all related tests and code part adaptations.  The only major change is in the `LogManager.scala` file within `asyncDelete`. I had to change operations order for `removedLog` from  1. Rename directory (atomic move)  2. Make checkpoint log for offsets and recovery 3. Mark for deletion  to  1. Make checkpoint log for offsets and recovery 2. Close `FileChannel` to avoid `AccessDeniedException` on Windows 3. Rename directory (atomic move) 4. Mark for deletion  New order allows `LogManagerTest.testFileReferencesAfterAsyncDelete` to succeed on Windows.  Currently I am running long live test with all of the changes mentioned above. It seems to function as expected: log retention works. Setup is very simple - one broker.  TODO: - `LogCleanerTest.testRecoveryAfterCrash` fails  There is potentially more broken parts on Windows... than the original bug  I think most Windows users would really like to see the old [KAFKA-1194](https://issues.apache.org/jira/browse/KAFKA-1194) issue being closed.","closed","","GeorgeCGV","2018-01-17T09:26:29Z","2019-01-28T18:10:06Z"
"","3631","KAFKA-5708: jackson upgrade (2.8.5 -> 2.9.1)","**_Prologue:_** PR #3116  **_Jackson release notes:_** - https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.8 - https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.9  **_Note:_** you may want to wait for [Jackson 2.9.1 release ](https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.9.1)  @ewencp: FYI","closed","","dejan2609","2017-08-06T17:11:10Z","2017-09-15T18:20:07Z"
"","4319","KAFKA-5142: Add Connect support for message headers (KIP-145)","**[KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) has been accepted, and this PR implements KIP-145 except without the SMTs.**  Changed the Connect API and runtime to support message headers as described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect).  The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.  The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.  A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.  Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation matches KIP-145 - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2017-12-13T01:31:51Z","2020-10-16T06:05:14Z"
"","3731","KAFKA-5720: Fix AdminClientIntegrationTest#testCallInFlightTimeouts","* When a call is aborted, that should count as a ""try"" in the failure log message. * FailureInjectingTimeoutProcessorFactory should fail the first request it is asked about. * testCallTimeouts should expect the first request it makes to fail because of the timeout we injected. * FailureInjectingTimeoutProcessorFactory should track how many failures it has injected, and the test should verify that one has been injected.","closed","","cmccabe","2017-08-24T23:25:59Z","2019-05-20T19:09:12Z"
"","3800","KAFKA-5764:  Add toLowerCase support to sasl.kerberos.principal.to.local rule","* Update the tests and docs ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-09-06T07:35:58Z","2018-07-03T15:43:05Z"
"","4367","MINOR: Update test classes to use KafkaZkClient methods","* Remove ZkUtils reference form ZooKeeperTestHarness  * minor cleanups  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-12-30T06:21:22Z","2018-07-03T15:44:44Z"
"","4351","KAFKA-6320: move ZK metrics in KafkaHealthCheck to ZookeeperClient","* Moved metrics in KafkaHealthCheck to ZookeeperClient. * Converted remaining ZkUtils usage in KafkaServer to ZookeeperClient and removed ZkUtils from KafkaServer. * Made the re-creation of ZooKeeper during ZK session expiration with infinite retries. * Added unit tests for all new methods in KafkaZkClient.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","junrao","2017-12-21T22:44:37Z","2017-12-22T16:13:37Z"
"","4323","KAFKA-5849: Add process stop, round trip workload, partitioned test","* Implement process stop faults via SIGSTOP / SIGCONT  * Implement RoundTripWorkload, which both sends messages, and confirms that they are received at least once.  * Allow Trogdor tasks to block until other Trogdor tasks are complete.  * Add CreateTopicsWorker, which can be a building block for a lot of tests.  * Simplify how TaskSpec subclasses in ducktape serialize themselves to JSON.  * Implement some fault injection tests in round_trip_workload_test.py","closed","","cmccabe","2017-12-14T01:35:26Z","2019-05-20T18:56:50Z"
"","4371","KAFKA-6363: Use MockAdminClient for any unit tests that depend on Adm…","* Implement MockAdminClient.deleteTopics * Use MockAdminClient instead of MockKafkaAdminClientEnv in StreamsResetterTest * Rename MockKafkaAdminClientEnv to AdminClientUnitTestEnv * Use MockAdminClient instead of MockKafkaAdminClientEnv in TopicAdminTest * Rename KafkaAdminClient to AdminClientUnitTestEnv in KafkaAdminClientTest.java * Migrate StreamThreadTest to MockAdminClient  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","h314to","2018-01-01T12:57:22Z","2018-01-09T05:09:25Z"
"","4434","KAFKA-6166: Streams configuration requires consumer. and producer. in order to be read","* Implement method to get custom properties * Add custom properties to getConsumerConfigs and getProducerConfigs * Add tests  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","h314to","2018-01-17T18:52:04Z","2018-01-31T01:38:54Z"
"","4088","MINOR: Controller and async ZookeeperClient improvements","* Fix issue in `retryRequestsUntilConnected` where the same response could appear multiple times (implies that we are lacking test coverage) * Introduce type member in AsyncRequest for the AsyncResponse type and refactor the code to eliminate most downcasts * Remove a number of unnecessary collection copies in `retryRequestsUntilConnected` * Move ControllerContext to its own file * Rename getACL/setACL to getAcl/setAcl to match Kafka naming convention * Replace tuple of 3 elements with case class in one place (we should do this in other places too) * Extract `send` and `shouldWatch` from `ZooKeeperClient.handleRequests` * Use pattern matching instead of if/else chains in a few places (we should do it in more places) * A couple of renames to avoid overloads and hence benefit from better type inference * Use Option and default arguments instead of passing null in some places * `Expired` is no longer a case class since it has no parameters, but it has state * Various minor clean-ups","closed","","ijuma","2017-10-18T17:28:50Z","2017-12-22T18:23:45Z"
"","4266","MINOR: Add maybeThrow method to ZooKeeperClient AsyncResponse","* Add maybeThrow method to ZooKeeperClient AsyncResponse * Update KafkaZkClient to use maybeThrow method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-11-27T16:11:58Z","2018-07-03T15:43:01Z"
"","4194","KAFKA-5646:  Use KafkaZkClient in DynamicConfigManager and AdminManager","* Add AdminZkClient class * Use KafkaZkClient, AdminZkClient  in ConfigCommand, TopicCommand * All the existing tests should work   ### Committer Checklist - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-11-08T18:20:23Z","2018-07-03T15:43:03Z"
"","4260","KAFKA-5647: Use KafkaZkClient in ReassignPartitionsCommand and PreferredReplicaLeaderElectionCommand","*  Use KafkaZkClient in ReassignPartitionsCommand *  Use KafkaZkClient in PreferredReplicaLeaderElectionCommand *  Updated test classes to use new methods *  All existing tests should pass  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2017-11-23T17:13:15Z","2018-07-03T15:42:57Z"
"","3990","KAFKA-5473: handle ZK session expiration properly when a new session can't be established","(WIP: this commit isn't ready to be reviewed yet. I was checking the travis-ci build with the configuration changes in my account and opened the PR prematurely against trunk. I will make it consistent with Contribution guidelines once it's well tested.)  https://issues.apache.org/jira/browse/KAFKA-5473  Design: `zookeeper.connection.retry.timeout.ms` => this determines how long to wait before triggering the shutdown. The default is 60000ms.   Currently the implementation only handles the `handleSessionEstablishmentError` by waiting for the sessionTimeout.","closed","","prasincs","2017-09-29T06:34:32Z","2017-12-16T00:14:59Z"
"","4290","KAFKA-6306: Auto-commit of offsets fail, and not recover forever...","###* Auto-commit of offsets fail, and not recover **forever**. at sendOffsetCommitRequest,  while ""generation equal NULL"",  ConsumerCoordinator request will fail always.  it maybe a bug. error log below:  has more and more warn log .... ""2017-12-01 22:08:39.112 WARN  pool-390-thread-1#1 (ConsumerCoordinator.java:626) - Auto-commit of offsets {drawing_gift_sent-1=OffsetAndMetadata{offset=32150359, metadata=''}} failed for group gift_rich_audience_write: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records."" #6306  https://issues.apache.org/jira/browse/KAFKA-6306","closed","","leonhong","2017-12-04T09:01:10Z","2018-02-17T02:55:57Z"
"","4373","MINOR: fixes a typo in a comment in config/server.properties","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","mulvenna","2018-01-02T06:57:19Z","2018-01-18T19:32:44Z"
"","4344","KAFKA-6321: Consolidate calls to KafkaConsumer's `beginningOffsets()` and `endOffsets()` in ConsumerGroupCommand","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2017-12-19T21:05:46Z","2018-01-26T04:54:18Z"
"","4437","MINOR: Improve internal topic integration test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-18T02:59:27Z","2020-04-24T23:54:59Z"
"","4408","MINOR: add reportCoverage to jenkins.sh","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2018-01-09T23:46:52Z","2020-04-24T23:47:13Z"
"","4404","KAFKA-5624: Add expiry check to sensor.add() methods","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-01-09T17:26:35Z","2018-07-03T15:44:42Z"
"","4394","KAFKA-4991: Resolve findbugs warnings in KerberosLogin","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2018-01-05T09:43:23Z","2018-07-03T15:46:50Z"
"","4333","MINOR: Use TopicPartition in ConsumerGroupCommand instead of TopicAndPartition where possible","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2017-12-15T23:36:25Z","2017-12-23T02:37:37Z"
"","4322","KAFKA-6126: Remove unnecessary topics created check","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-12-13T21:22:29Z","2017-12-21T02:11:52Z"
"","4307","KAFKA-6307 mBeanName should be removed before returning from JmxReporter#removeAttribute()","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2017-12-08T19:09:24Z","2018-01-01T10:17:40Z"
"","4294","MINOR: Include client-id in client authentication failure error messages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2017-12-05T12:26:10Z","2017-12-05T14:38:32Z"
"","4285","KAFKA-6296: Increase jitter to fix transient failure in NetworkClientTest.testConnectionDelayDisconnected","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2017-12-01T19:12:16Z","2017-12-01T21:05:31Z"
"","4255","KAFKA-6259: Make KafkaStreams.cleanup() clean global state directory","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-22T22:56:23Z","2017-11-29T19:29:59Z"
"","4253","MINOR: improve error message for Streams test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-22T22:05:17Z","2017-11-27T02:50:19Z"
"","4242","KAFKA-4857: Replace StreamsKafkaClient with AdminClient in Kafka Streams","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2017-11-21T02:19:17Z","2017-12-14T18:29:30Z"
"","4201","MINOR: Exclude PULL_REQUEST_TEMPLATE.md from rat checks","### Committer Checklist - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ijuma","2017-11-10T10:24:39Z","2017-12-22T18:21:44Z"
"","4461","KAFKA-3097: Add invalid principalType check to AclCommand","","closed","","omkreddy","2018-01-23T17:38:57Z","2018-07-03T15:46:45Z"
"","4458","MINOR: Optimize KTable-KTable join value getter supplier","","closed","","kamilszymanski","2018-01-22T22:22:12Z","2018-01-29T06:47:40Z"
"","4446","KAFKA-6462: fix unstable ResetIntegrationTest","","closed","","mjsax","2018-01-19T01:07:18Z","2018-01-24T21:25:46Z"
"","4445","MINOR: increase timeout for unstable KTableSourceTopicRestartIntegrationTest","","closed","","mjsax","2018-01-18T23:30:45Z","2018-01-23T00:08:27Z"
"","4444","MINOR: fix unstable FanoutIntegrationTest","","closed","","mjsax","2018-01-18T23:12:31Z","2018-01-23T23:40:25Z"
"","4439","MINOR: update upgrade notes with regard to KIP-149","","closed","","mjsax","2018-01-18T19:21:38Z","2018-01-29T21:15:52Z"
"","4422","MINOR: Fix typo in KafkaConsumer javadoc","","closed","","reftel","2018-01-15T11:40:08Z","2018-01-18T17:11:20Z"
"","4418","KAFKA-6254: Incremental fetch requests","","closed","","cmccabe","2018-01-12T19:44:11Z","2019-05-20T18:57:21Z"
"","4412","[KAFKA-6265] GlobalKTable missing #queryableStoreName()","","closed","","ConcurrencyPractitioner","2018-01-11T01:50:08Z","2018-01-11T02:05:17Z"
"","4391","MINOR: Introduced bnd to get kafka-clients OSGi compliant","","closed","","rlenferink","2018-01-04T19:15:56Z","2019-01-13T16:29:01Z"
"","4385","[KAFKA-4499] Adding documentation for querying WindowStores","","closed","","ConcurrencyPractitioner","2018-01-04T00:19:02Z","2018-01-13T01:29:21Z"
"","4380","MINOR: update docs with regard to improved resilience of Kafka Streams","","closed","","mjsax","2018-01-03T06:13:59Z","2018-01-23T00:07:42Z"
"","4355","MINOR: disable flaky Streams EOS system tests","","closed","","mjsax","2017-12-23T22:26:48Z","2017-12-29T18:52:39Z"
"","4353","MINOR: Update test classes to use KafkaZkClient/AdminZkClient methods","","closed","","omkreddy","2017-12-22T19:16:10Z","2018-07-03T15:42:56Z"
"","4342","KAFKA-4263: fix flaky test QueryableStateIntegrationTest.concurrentAccess","","closed","","mjsax","2017-12-19T07:57:32Z","2017-12-21T02:01:42Z"
"","4341","KAFKA-6382: Make ProducerConfig and ConsumerConfig constructors public","","closed","","mjsax","2017-12-18T22:40:35Z","2018-01-17T19:26:42Z"
"","4340","[KAFKA-6265] GlobalKTable missing #queryableStoreName()","","closed","","ConcurrencyPractitioner","2017-12-18T22:13:27Z","2018-01-11T02:29:26Z"
"","4339","[KAFKA-6328] Exclude node groups belonging to global stores in InternalTopologyBuilder#makeNodeGroups","","closed","","ConcurrencyPractitioner","2017-12-18T22:10:33Z","2018-01-17T00:58:46Z"
"","4337","MINOR: fix race condition in KafkaStreamsTest.shouldReturnThreadMetadata","","closed","","mjsax","2017-12-18T19:05:08Z","2017-12-19T01:52:14Z"
"","4335","MINOR: improve JavaDocs for KafkaStreams, KafkaProducer, KafkaConsumer","","closed","","mjsax","2017-12-15T23:59:11Z","2017-12-18T19:55:17Z"
"","4334","[KAFKA-6265] GlobalKTable missing #queryableStoreName()","","closed","","ConcurrencyPractitioner","2017-12-15T23:41:37Z","2017-12-19T06:18:13Z"
"","4332","KAFKA-6302: Improve AdmintClient JavaDocs","","closed","","mjsax","2017-12-15T23:21:13Z","2018-01-17T22:02:20Z"
"","4328","KAFKA-6371 Fix DelayedFetch toString","","closed","","mayt","2017-12-15T09:33:43Z","2018-02-26T07:36:47Z"
"","4284","MINOR: improve EOS docs","","closed","","mjsax","2017-12-01T19:00:27Z","2017-12-17T05:02:40Z"
"","4275","Close socketChannel in finally block","","closed","","cmccabe","2017-11-29T17:38:08Z","2019-05-20T18:57:00Z"
"","4273","KAFKA-4827: Porting fix for KAFKA-4827 to v0.10 and v0.11","","closed","","wicknicks","2017-11-29T03:25:49Z","2017-11-29T19:23:32Z"
"","4267","MINOR: fix version in tests/kafkatest/__init__.py","","closed","","cmccabe","2017-11-27T17:09:59Z","2019-05-20T19:05:29Z"
"","4264","MINOR: small fix for doc comment in NetworkClient.DefaultMetadataUpdater","","closed","","sweat123","2017-11-25T11:55:21Z","2017-11-25T12:16:47Z"
"","4249","MINOR: fix typo in ProducerConfig doc","","closed","","nick-zh","2017-11-22T15:06:32Z","2017-11-23T14:25:50Z"
"","4245","KAFKA-6255: Add ProduceBench to Trogdor","","closed","","cmccabe","2017-11-22T01:22:33Z","2019-05-20T18:57:05Z"
"","4203","MINOR: Remove note in docs that is no longer required","","closed","","rajinisivaram","2017-11-10T16:24:05Z","2017-11-15T13:01:04Z"
"","4199","KAFKA-6164; Shutdown quota managers if other components fail to start","","closed","","rajinisivaram","2017-11-09T16:14:30Z","2017-11-15T13:01:48Z"
"","4198","MINOR: make controller helper methods private","","closed","","onurkaraman","2017-11-09T02:01:49Z","2017-11-09T21:54:18Z"
"","4193","KAFKA-6185: Remove channels from explictlyMutedChannels set when closed","","closed","","rajinisivaram","2017-11-08T16:23:27Z","2017-11-09T16:18:26Z"
"","4192","MINOR: Remove unnecessary batch iteration in FileRecords.downConvert","","closed","","ijuma","2017-11-08T08:56:22Z","2017-12-22T18:21:49Z"
"","4187","MINOR: Handle error metrics removal during shutdown","","closed","","rajinisivaram","2017-11-07T10:08:37Z","2017-11-15T09:57:05Z"
"","4186","KAFKA-6179: Clear min timestamp tracker upon partition queue cleanup","","closed","","guozhangwang","2017-11-07T01:47:16Z","2020-04-24T23:56:26Z"
"","4183","Kafka 5692 elect preferred","","closed","","cmccabe","2017-11-06T22:12:20Z","2019-03-04T16:17:53Z"
"","4181","KAFKA-6164: Shutdown quota managers if other components fail to start","","closed","","rajinisivaram","2017-11-06T13:27:20Z","2017-11-09T15:25:21Z"
"","4178","0.10.1","","closed","","bbobbadi","2017-11-05T21:33:33Z","2017-12-22T01:43:58Z"
"","4177","KAFKA-6172; Cache lastEntry in TimeIndex to avoid unnecessary disk access","","closed","","lindong28","2017-11-05T07:21:35Z","2017-11-10T18:20:27Z"
"","4174","MINOR: Add pull request template","","closed","","ijuma","2017-11-03T13:06:19Z","2017-12-22T18:21:58Z"
"","4170","KAFKA-6157: Fix repeated words words in JavaDoc and comments.","","closed","","efeg","2017-11-02T00:03:50Z","2017-11-06T19:24:55Z"
"","4168","MINOR: update producer client request timeout in system test","","closed","","bbejeck","2017-11-01T21:38:32Z","2017-11-03T00:54:49Z"
"","4166","KAFKA-6074 Use ZookeeperClient in ReplicaManager and Partition","","closed","","tedyu","2017-11-01T03:11:13Z","2017-11-22T22:26:44Z"
"","4162","[KAFKA-4499]  Add all() and fetchAll() API for querying windowed KTable stores","","closed","","ConcurrencyPractitioner","2017-10-30T22:09:16Z","2018-01-06T21:19:28Z"
"","4158","KAFKA-2903: FileRecords.read doesn't handle size > sizeInBytes when start is not zero","","closed","","ijuma","2017-10-30T11:45:26Z","2017-12-22T18:22:03Z"
"","4155","KAFKA-5645: Use async ZookeeperClient in SimpleAclAuthorizer","","closed","","omkreddy","2017-10-28T18:57:14Z","2018-07-03T15:43:08Z"
"","4149","KAFKA-6126: Reduce rebalance time by not checking if created topics are available","","closed","","jeyhunkarimov","2017-10-27T23:20:07Z","2017-10-28T10:33:05Z"
"","4148","KAFKA-6120: RecordCollector should not retry sending","","closed","","mjsax","2017-10-27T19:25:45Z","2017-11-06T19:18:50Z"
"","4147","MINOR: Fix inconsistency in StopReplica/LeaderAndIsr error counts","","closed","","hachikuji","2017-10-27T19:06:05Z","2017-10-31T16:47:19Z"
"","4140","KAFKA-6131: Use atomic putIfAbsent to create txn marker queues","","closed","","rajinisivaram","2017-10-26T18:16:46Z","2017-10-27T02:56:04Z"
"","4139","MINOR: Indentation Fix","","closed","","vahidhashemian","2017-10-26T15:56:57Z","2017-10-27T13:12:30Z"
"","4136","KAFKA-6100: Down-grade RocksDB to 5.7.3","","closed","","guozhangwang","2017-10-26T00:16:46Z","2017-11-06T22:46:08Z"
"","4135","KAFKA-5848: Perform a complete topic name validation in KafkaConsumer's assign/subscribe","","closed","","vahidhashemian","2017-10-25T18:49:27Z","2018-01-19T22:55:14Z"
"","4130","HOTFIX: Remove sysout logging","","closed","","guozhangwang","2017-10-25T00:15:07Z","2017-11-06T22:46:06Z"
"","4128","MINOR: random cleanup and JavaDoc improvements for clients and Streams","","closed","","mjsax","2017-10-24T20:50:18Z","2017-10-30T23:30:40Z"
"","4126","KAFKA-6072: User ZookeeperClient in GroupCoordinator and TransactionCoordinator","","closed","","omkreddy","2017-10-24T16:41:35Z","2018-07-03T15:43:16Z"
"","4124","KAFKA-6074 Use ZookeeperClient in ReplicaManager and Partition","","closed","","tedyu","2017-10-24T04:54:15Z","2017-12-28T14:27:06Z"
"","4122","KAFKA-6096: Add multi-threaded tests for group coordinator, txn manager","","closed","","rajinisivaram","2017-10-23T22:06:24Z","2018-01-09T00:21:22Z"
"","4121","Kafka 6042: Use state write lock for delayed txn operations","","closed","","rajinisivaram","2017-10-23T21:56:19Z","2017-10-23T22:50:29Z"
"","4119","MINOR: added -1 value description as ""high watermark"" in the protocol delete records request","","closed","","ppatierno","2017-10-23T13:59:52Z","2017-10-27T13:41:59Z"
"","4118","KAFKA-6101 Reconnecting to broker does not exponentially backoff","","closed","","tedyu","2017-10-23T13:50:48Z","2017-10-23T15:16:26Z"
"","4110","[KAFKA-5212] Consumer ListOffsets request can starve group heartbeats","","closed","","ConcurrencyPractitioner","2017-10-21T18:40:06Z","2017-10-30T18:36:16Z"
"","4107","JDK 9 no longer contains JAXB API's as default","","closed","","narcisoguillen","2017-10-20T22:30:20Z","2017-12-22T01:44:31Z"
"","4106","KAFKA-5849: Add partitioned produce consume test","","closed","","cmccabe","2017-10-20T21:50:51Z","2017-12-14T01:32:04Z"
"","4104","MINOR: add hint for setting an uncaught exception handler to JavaDocs","","closed","","mjsax","2017-10-20T18:25:32Z","2017-10-23T18:00:32Z"
"","4103","KAFKA-6042: Avoid deadlock between two groups with delayed operations","","closed","","rajinisivaram","2017-10-20T12:43:00Z","2019-07-10T08:21:46Z"
"","4101","MINOR: Add missing semicolon to example jaas configuration","","closed","","omkreddy","2017-10-20T06:52:06Z","2018-07-03T15:43:14Z"
"","4100","KAFKA-6093; log.close() should not prevent log from being accessed","","closed","","lindong28","2017-10-20T02:45:40Z","2018-06-26T16:51:36Z"
"","4099","MINOR: Use ObjectName.quote instead of URL-encoding for JMX metric tags","","closed","","rajinisivaram","2017-10-19T15:48:41Z","2017-10-20T07:08:34Z"
"","4093","KAFKA-6083: The Fetcher should add the InvalidRecordException as a cause to the KafkaException when invalid record is found.","","open","","efeg","2017-10-19T00:24:20Z","2018-07-06T13:18:17Z"
"","4092","KAFKA-6087: Scanning plugin.path needs to support relative symlinks.","","closed","","kkonstantine","2017-10-18T23:50:02Z","2017-10-19T21:26:32Z"
"","4090","[KAFKA-6084] Propagate JSON parsing errors in ReassignPartitionsCommand","","closed","","viktorsomogyi","2017-10-18T21:43:21Z","2020-12-15T18:05:56Z"
"","4089","KAFKA-6071: Use ZookeeperClient in LogManager","","closed","","omkreddy","2017-10-18T18:33:46Z","2018-07-03T15:42:38Z"
"","4087","MINOR: Remove dead code","","closed","","ijuma","2017-10-18T15:55:20Z","2017-12-22T18:23:50Z"
"","4084","KAFKA-6070: add ipaddress and enum34 dependencies to docker image","","closed","","cmccabe","2017-10-17T21:57:00Z","2019-05-20T19:05:51Z"
"","4082","KAFKA-6225: Adds an option to consume continuously","","open","","astubbs","2017-10-17T15:46:26Z","2018-05-29T09:58:53Z"
"","4081","KAFKA-6069: Properly tag KafkaStreams metrics with the client id.","","closed","","twbecker","2017-10-17T13:59:06Z","2017-10-19T15:03:15Z"
"","4080","KAFKA-6226: Print units for the performance consumer","","open","","astubbs","2017-10-17T13:32:30Z","2018-03-02T19:30:51Z"
"","4079","MINOR: JavaDoc improvements for RangeAssignor","","closed","","astubbs","2017-10-17T11:11:58Z","2018-01-26T19:35:55Z"
"","4078","MINOR: update exception message for KIP-120","","closed","","mjsax","2017-10-17T00:13:10Z","2017-10-17T01:58:39Z"
"","4076","MINOR: A few javadoc fixes","","closed","","hachikuji","2017-10-16T22:44:35Z","2017-10-17T00:51:18Z"
"","4074","MINOR: add equals to SessionWindows","","closed","","dguy","2017-10-16T14:37:18Z","2017-10-17T01:03:11Z"
"","4072","MINOR: Add HttpMetricsReporter for system tests","","closed","","ewencp","2017-10-13T18:43:16Z","2017-11-09T17:44:36Z"
"","4070","MINOR: update comments in config/producer.properties","","closed","","omkreddy","2017-10-13T04:47:51Z","2018-07-03T15:42:34Z"
"","4068","MINOR: Update JavaDoc to use new API","","closed","","bbejeck","2017-10-12T14:54:58Z","2017-10-12T21:21:16Z"
"","4064","MINOR: add unit test for StateStoreSerdes","","closed","","mjsax","2017-10-11T23:30:35Z","2017-10-13T16:57:53Z"
"","4063","MINOR: improve Store parameter checks","","closed","","mjsax","2017-10-11T22:55:26Z","2017-10-12T20:45:04Z"
"","4062","KAFKA-6055: Fix a typo in JVM configuration of Windows tools","","closed","","vahidhashemian","2017-10-11T22:54:54Z","2017-10-12T09:10:52Z"
"","4060","MINOR: Add Kafka Streams upgrade workflow","","closed","docs,","bbejeck","2017-10-11T21:07:41Z","2018-03-23T19:34:35Z"
"","4057","KAFKA-6053: Fix NoSuchMethodError when creating ProducerRecords with older client versions","","closed","","apurvam","2017-10-11T17:00:08Z","2017-10-12T09:07:06Z"
"","4055","MINOR: Update `config/consumer.properties` to have new consumer properties","","closed","","omkreddy","2017-10-11T09:33:47Z","2018-07-03T15:42:33Z"
"","4054","HOTFIX: Updates on release.py before 1.0.0","","closed","","guozhangwang","2017-10-10T22:29:53Z","2020-04-24T23:46:58Z"
"","4053","KAFKA-6025: small fix for streams tutorial","","closed","","bbejeck","2017-10-10T21:47:45Z","2017-10-10T22:09:38Z"
"","4052","KAFKA-6046 DeleteRecordsRequest to a non-leader should give proper error","","closed","","tedyu","2017-10-10T17:17:43Z","2017-11-17T16:53:54Z"
"","4051","MINOR: KIP-182 follow up","","closed","","mjsax","2017-10-10T17:11:56Z","2017-10-11T01:55:07Z"
"","4048","MINOR: fix EOS test race condition","","closed","","mjsax","2017-10-09T21:27:16Z","2017-10-10T22:57:51Z"
"","4047","MINOR: Factor out some common group/transactional fields in request objects","","closed","","hachikuji","2017-10-09T19:11:00Z","2017-10-09T20:24:22Z"
"","4046","KAFKA-5541: minor follow-up","","closed","","mjsax","2017-10-09T18:46:22Z","2017-10-10T22:57:31Z"
"","4045","KAFKA-6027; Access to log should throw KafkaStorageException after the log has been marked offline","","closed","","lindong28","2017-10-09T17:47:54Z","2017-10-18T02:10:30Z"
"","4043","the rate calculation is wrong somehow, will make some heavy read and write log stop compaction","","closed","","lisa2lisa","2017-10-09T14:53:52Z","2017-10-10T13:19:16Z"
"","4037","KAFKA-5541: Streams should not re-throw if suspending/closing tasks fails","","closed","","mjsax","2017-10-07T01:06:30Z","2017-10-09T18:46:34Z"
"","4036","MINOR: KIP-161 upgrade docs change","","closed","","guozhangwang","2017-10-06T23:25:56Z","2017-11-06T22:45:49Z"
"","4035","MINOR: Avoid some unnecessary collection copies in KafkaApis","","closed","","hachikuji","2017-10-06T22:28:04Z","2017-10-10T22:05:27Z"
"","4032","MINOR: Fix typo","","closed","","jeffwidman","2017-10-05T23:56:24Z","2017-10-24T02:32:59Z"
"","4031","MINOR: log4j improvements on assigned tasks and store changelog reader","","closed","","guozhangwang","2017-10-05T23:50:44Z","2017-11-06T22:45:52Z"
"","4030","KAFKA-5953: Register all jdbc drivers available in plugin and class paths","","closed","connect,","kkonstantine","2017-10-05T21:25:25Z","2020-10-16T06:05:13Z"
"","4027","MINOR: fix inconsistance","","closed","","lisa2lisa","2017-10-05T19:45:30Z","2018-02-27T12:40:54Z"
"","4026","KAFKA-5746: Document new broker metrics added for health checks","","closed","","rajinisivaram","2017-10-05T19:19:37Z","2017-12-20T13:39:42Z"
"","4024","KAFKA-6012: Close request metrics only after closing request handlers","","closed","","rajinisivaram","2017-10-05T14:24:47Z","2017-10-05T16:27:53Z"
"","4023","KAFKA-5829: Only delete producer snapshots before the recovery point","","closed","","ijuma","2017-10-05T13:58:05Z","2017-12-22T18:24:03Z"
"","4015","KAFKA-6004: Allow authentication providers to override error message","","closed","","rajinisivaram","2017-10-04T13:53:43Z","2017-10-04T17:46:43Z"
"","4014","KAFKA-5738: Upgrade note for cumulative count metric (KIP-187)","","closed","","rajinisivaram","2017-10-04T13:00:19Z","2017-10-04T14:55:40Z"
"","4013","KAFKA-4764: Upgrade notes for authentication failure handling (KIP-152)","","closed","","rajinisivaram","2017-10-04T12:58:57Z","2017-10-04T14:27:05Z"
"","4012","KAFKA-6008: Sanitize the Kafka Connect workerId before passing it to AppInfoParser","","closed","connect,","scholzj","2017-10-04T11:27:20Z","2020-10-16T06:29:13Z"
"","4006","MINOR: JavaDoc improvements for new state store API","","closed","","mjsax","2017-10-03T07:50:50Z","2017-10-04T21:33:04Z"
"","3994","KAFKA-5985: update javadoc regarding closing iterators","","closed","","bbejeck","2017-09-29T16:40:50Z","2017-10-02T18:51:23Z"
"","3993","KAFKA-5995; Rename AlterReplicaDir to AlterReplicaDirs","","closed","","lindong28","2017-09-29T15:45:23Z","2017-10-18T02:10:09Z"
"","3984","MINOR: update streams quickstart for KIP-182","","closed","","dguy","2017-09-28T14:55:40Z","2017-10-04T19:21:17Z"
"","3981","HOTFIX: fix build compilation error","","closed","","dguy","2017-09-28T10:32:37Z","2017-09-28T11:55:10Z"
"","3980","MINOR: Push JMX metric name mangling into the JmxReporter (KIP-190 follow up)","","closed","","ewencp","2017-09-28T00:46:16Z","2017-10-11T21:34:39Z"
"","3979","KAFKA-5979: Use single AtomicCounter to generate internal names","","closed","","mjsax","2017-09-28T00:21:34Z","2017-09-28T17:42:43Z"
"","3978","KAFKA-5932: Avoid call to fetchPrevious in FlushListeners","","closed","","bbejeck","2017-09-27T20:28:41Z","2017-10-03T15:33:57Z"
"","3977","MINOR: Mark log cleaner integration tests as integration tests","","closed","","ewencp","2017-09-27T20:24:39Z","2018-10-17T16:19:07Z"
"","3976","MINOR: Follow-up cleanup of KAFKA-5960","","closed","","hachikuji","2017-09-27T18:07:42Z","2017-09-28T09:58:56Z"
"","3974","Kafka 5932[WIP] DON'T Merge  avoid fetch previous in flush listeners","","closed","","bbejeck","2017-09-27T17:06:43Z","2017-09-27T17:17:38Z"
"","3973","KAFKA-5958: Global stores access state restore listener","","closed","","bbejeck","2017-09-27T14:07:06Z","2017-09-28T09:56:16Z"
"","3971","MINOR: additional kip-182 doc updates","","closed","","dguy","2017-09-27T10:20:51Z","2017-10-02T20:22:31Z"
"","3970","KAFKA-5225: StreamsResetter doesn't allow custom Consumer properties","","closed","","mjsax","2017-09-27T06:08:51Z","2017-10-02T21:13:41Z"
"","3966","KAFKA-5980: FailOnInvalidTimestamp does not log error","","closed","","mjsax","2017-09-27T00:38:23Z","2017-10-04T22:14:53Z"
"","3965","KAFKA-5944: Unit tests for handling SASL authentication failures in clients","","closed","","vahidhashemian","2017-09-26T20:33:02Z","2018-02-12T22:29:45Z"
"","3964","MINOR: Update Jetty to 9.2.22.v20170606","","closed","","ewencp","2017-09-26T18:04:45Z","2017-09-26T21:59:20Z"
"","3958","Re-enable KafkaAdminClientTest#testHandleTimeout","","closed","","cmccabe","2017-09-25T19:18:21Z","2020-12-09T09:54:47Z"
"","3957","KAFKA-6005: Reject JoinGroup request from first member with empty protocol type/protocol list","","closed","","omkreddy","2017-09-25T13:06:36Z","2018-07-03T15:46:39Z"
"","3956","KAFKA-5970: Use ReentrantLock for delayed operation lock to avoid blocking","","closed","","rajinisivaram","2017-09-25T11:49:29Z","2017-10-04T18:37:20Z"
"","3953","KAFKA-5541: Streams should not re-throw if suspending/closing tasks fails","","closed","","mjsax","2017-09-25T01:29:22Z","2018-06-05T23:49:51Z"
"","3951","MINOR: update listener.security.protocol.map config description","","closed","","omkreddy","2017-09-23T17:55:58Z","2018-07-03T15:42:36Z"
"","3950","MINOR - Adding New York Times logo to streams page","","closed","","manjuapu","2017-09-23T01:45:11Z","2017-09-23T02:54:40Z"
"","3949","MINOR: Update Streams quickstart to create output topic with compaction enabled","","closed","","mjsax","2017-09-22T21:07:11Z","2017-09-25T18:49:54Z"
"","3948","KAFKA-4593: Don't throw IllegalStateException and die on task migration","","closed","","mjsax","2017-09-22T20:13:54Z","2017-09-29T16:58:56Z"
"","3947","KAFKA-5959: Fix NPE in Sender.canRetry when idempotence is not enabled","","closed","","apurvam","2017-09-22T19:01:17Z","2017-09-22T20:10:58Z"
"","3944","KAFKA-5960; Fix regression in produce version selection on old brokers","","closed","","hachikuji","2017-09-22T07:14:02Z","2017-09-25T23:14:51Z"
"","3943","MINOR: always set Serde.Long on count operations","","closed","","dguy","2017-09-22T06:32:38Z","2017-09-29T10:10:28Z"
"","3942","KAFKA-5957: Prevent second deallocate if response for aborted batch returns","","closed","","hachikuji","2017-09-22T00:19:53Z","2017-09-29T02:22:20Z"
"","3940","Adding LINE corp logo to streams page","","closed","","manjuapu","2017-09-21T22:33:54Z","2017-09-21T23:12:39Z"
"","3934","KAFKA-5954: Correct Connect REST API system test","","closed","","rhauch","2017-09-21T14:39:43Z","2017-09-21T15:48:16Z"
"","3932","KAFKA-5867: Log Kafka Connect worker info during startup","","closed","connect,","kkonstantine","2017-09-21T11:47:12Z","2020-10-16T06:05:12Z"
"","3929","Minor: tighten up the check of partition states in AbstractFetcherThread","","closed","","junrao","2017-09-21T01:22:03Z","2018-02-15T00:08:00Z"
"","3927","KAFKA-5862: Remove ZK dependency from Streams reset tool","","closed","","bbejeck","2017-09-20T22:09:53Z","2017-09-23T04:08:07Z"
"","3924","KAFKA-5547: Return TOPIC_AUTHORIZATION_FAILED error if no describe access for topics","","closed","","omkreddy","2017-09-20T17:11:15Z","2018-07-03T15:46:40Z"
"","3921","MINOR: add upgrade note for KIP-173 topic configs","","closed","","dguy","2017-09-20T12:57:52Z","2017-09-21T10:21:18Z"
"","3917","MINOR: Extend release.py with a subcommand for staging docs into the kafka-site repo","","closed","","ewencp","2017-09-20T04:18:11Z","2018-02-28T18:31:03Z"
"","3915","MINOR: add punctuate calls to test driver","","closed","","bbejeck","2017-09-20T02:14:51Z","2018-01-26T17:16:20Z"
"","3914","Adding See how Kafka Streams is being used section to Streams page","","closed","","manjuapu","2017-09-20T00:57:12Z","2017-09-20T16:26:33Z"
"","3913","KAFKA-5937: Improve ProcessorStateManager exception handling","","closed","","mjsax","2017-09-20T00:03:21Z","2017-09-22T05:03:38Z"
"","3912","KAFKA-5936: KafkaProducer.close should throw InterruptException","","closed","","mjsax","2017-09-19T22:50:04Z","2017-11-30T01:08:00Z"
"","3910","MINOR: Only include transactional id in LogContext if it's set","","closed","","ijuma","2017-09-19T21:04:05Z","2017-12-22T18:22:38Z"
"","3903","KAFKA-5931: deprecate KTable#through and KTable#to","","closed","","dguy","2017-09-19T11:37:54Z","2017-09-20T11:06:07Z"
"","3893","KAFKA-5893: Preserve original System.out in PrintedTest","","closed","","mjsax","2017-09-18T21:10:51Z","2017-09-19T04:33:36Z"
"","3892","MINOR: use StoreBuilder in KStreamImpl rather than StateStoreSupplier","","closed","","dguy","2017-09-18T15:20:25Z","2017-09-19T11:07:27Z"
"","3885","MINOR: Added "">"" prompt in examples where kafka-console-producer is used","","closed","","ppatierno","2017-09-18T07:55:53Z","2017-10-27T13:43:48Z"
"","3884","MINOR: various random minor fixes and improve KafkaConsumer JavaDocs","","closed","","mjsax","2017-09-18T02:39:44Z","2017-09-22T18:24:43Z"
"","3882","MINOR: Add metric templates for sender/fetcher rate totals","","closed","","rajinisivaram","2017-09-17T16:23:41Z","2017-09-18T09:15:18Z"
"","3874","KAFKA-5163; Support replicas movement between log directories (KIP-113)","","closed","","lindong28","2017-09-15T16:56:13Z","2017-10-18T02:11:06Z"
"","3873","MINOR: Add semicolon to 'SASL/SCRAM' doc","","closed","","makubi","2017-09-15T13:31:31Z","2018-10-18T15:17:10Z"
"","3862","MINOR: Dev guide fixup","","closed","","joel-hamill","2017-09-14T19:15:08Z","2017-10-04T18:30:43Z"
"","3858","MINOR: update docs to add note about removing SNAPSHOT from streams dependency","","closed","","dguy","2017-09-14T10:51:05Z","2017-09-18T08:49:38Z"
"","3857","MINOR: Bump version in streams quickstart archetype pom.xml","","closed","","dguy","2017-09-14T10:44:13Z","2017-09-14T11:01:37Z"
"","3854","MINOR: Fix LogContext message format in KafkaProducer","","closed","","vahidhashemian","2017-09-13T22:46:32Z","2017-09-14T10:19:47Z"
"","3852","MINOR: Update TransactionManager to use LogContext","","closed","","hachikuji","2017-09-13T21:05:37Z","2017-09-14T10:26:25Z"
"","3851","MINOR: Fix typo","","closed","","jeffwidman","2017-09-13T18:32:28Z","2017-09-13T20:22:49Z"
"","3850","MINOR: Un-hide the tutorial buttons on web docs","","closed","","guozhangwang","2017-09-13T17:52:48Z","2017-11-06T22:45:56Z"
"","3847","MINOR: update tutorial doc to match ak-site","","closed","","dguy","2017-09-13T12:55:24Z","2017-09-13T20:23:22Z"
"","3846","MINOR: update release script for streams quickstart","","closed","","dguy","2017-09-13T12:46:12Z","2017-09-20T09:58:45Z"
"","3841","KAFKA-5833: Reset thread interrupt state in case of InterruptedException","","closed","","mjsax","2017-09-13T01:35:03Z","2017-09-19T04:33:30Z"
"","3840","KAFKA-5879; Controller should read the latest IsrChangeNotification znodes when handling IsrChangeNotification event","","closed","","lindong28","2017-09-13T00:29:30Z","2017-10-18T02:09:52Z"
"","3839","KAFKA-5877; Controller should only update reassignment znode if there is change in the reassignment data","","closed","","lindong28","2017-09-12T21:25:42Z","2017-12-13T02:01:40Z"
"","3836","KAFKA-5872: Fix transient failure in SslSelectorTest.testMuteOnOOM","","closed","","rajinisivaram","2017-09-12T09:12:27Z","2017-09-14T02:03:29Z"
"","3835","MINOR: update operations doc on topic deletion","","closed","","omkreddy","2017-09-12T06:26:49Z","2018-07-03T15:42:36Z"
"","3834","MINOR: Tweak detection of kafka server start-up in system tests","","closed","","ijuma","2017-09-12T03:44:21Z","2017-09-17T15:20:23Z"
"","3833","MINOR: refactor build method to extract methods from if statements","","closed","","bbejeck","2017-09-12T00:33:32Z","2017-09-12T08:27:33Z"
"","3830","MINOR: Created convenience method to create ZkUtils","","closed","","bbaugher","2017-09-11T19:24:51Z","2017-12-22T01:46:56Z"
"","3828","MINOR: update processor topology test driver","","closed","","bbejeck","2017-09-11T15:35:43Z","2017-09-12T08:24:59Z"
"","3820","KAFKA-5864; ReplicaFetcherThread should not die due to replica in offline log directory","","closed","","lindong28","2017-09-08T21:31:53Z","2017-10-18T02:09:25Z"
"","3819","KAFKA-5576: RocksDB upgrade to 5.8, plus one bug fix on Bytes.wrap","","closed","","guozhangwang","2017-09-08T21:09:42Z","2020-04-24T23:47:04Z"
"","3817","MINOR: make Printed copy ctor protected","","closed","","dguy","2017-09-08T17:36:23Z","2017-09-08T21:13:29Z"
"","3816","MINOR: make Consumed copy ctor protected","","closed","","dguy","2017-09-08T17:18:46Z","2017-09-08T21:14:34Z"
"","3814","KAFKA-4504: update retention.bytes config description","","closed","","omkreddy","2017-09-08T08:31:27Z","2018-07-03T15:42:29Z"
"","3811","KAFKA-5839: Upgrade Guide doc changes for KIP-130","","closed","","fhussonnois","2017-09-07T19:22:57Z","2017-09-20T08:52:03Z"
"","3805","MINOR: Implement toString for NetworkClient#InFlightRequest","","closed","","bbaugher","2017-09-06T19:39:46Z","2017-09-11T22:49:33Z"
"","3804","MINOR: fixed typos","","closed","","mjsax","2017-09-06T19:06:10Z","2017-09-06T23:23:02Z"
"","3803","KAFKA-5845; KafkaController should send LeaderAndIsrRequest to broker which starts very soon after shutdown","","closed","","lindong28","2017-09-06T16:53:02Z","2017-12-13T02:01:07Z"
"","3797","KAFKA-5843; Mx4jLoader.maybeLoad should only be executed if kafka_mx4jenable is set to true","","closed","","lindong28","2017-09-06T05:12:25Z","2017-10-18T02:09:04Z"
"","3796","MINOR: KIP-138 renaming of string names","","closed","","guozhangwang","2017-09-06T00:06:01Z","2017-11-06T22:46:00Z"
"","3795","KAFKA-5783: Add KafkaPrincipalBuilder with support for SASL (KIP-189)","","closed","","hachikuji","2017-09-05T21:27:51Z","2017-09-14T09:17:57Z"
"","3793","MINOR: add mvn-pgp-plugin to sign streams quickstart jars","","closed","","dguy","2017-09-05T16:04:03Z","2017-09-05T17:52:40Z"
"","3792","KAFKA-5837: Set defaults for ReassignPartitionsCommand correctly","","closed","","rajinisivaram","2017-09-05T15:59:02Z","2017-09-05T23:34:37Z"
"","3790","MINOR: fix scalaVersion variable in templateData.js","","closed","","dguy","2017-09-05T10:41:58Z","2017-09-05T10:47:08Z"
"","3789","MINOR: Fix sftp_mkdir in release.py","","closed","","ijuma","2017-09-05T09:20:59Z","2017-09-05T09:54:01Z"
"","3788","MINOR: update security docs","","closed","","omkreddy","2017-09-05T09:05:17Z","2018-07-03T15:46:36Z"
"","3787","KAFKA-5823: Extend upgrade section for KIP-120","","closed","","mjsax","2017-09-05T00:49:26Z","2017-09-06T19:06:41Z"
"","3786","MINOR: preparing for bug fix release","","closed","","dguy","2017-09-04T16:49:14Z","2017-09-05T07:17:40Z"
"","3782","KAFKA-5829; Speedup broker startup after unclean shutdown by reducing unnecessary snapshot files deletion","","closed","","lindong28","2017-09-04T08:48:05Z","2017-10-18T02:08:45Z"
"","3781","MINOR: Improve documentation of AdminClient","","closed","","lindong28","2017-09-03T19:09:59Z","2017-12-13T02:02:06Z"
"","3779","KAFKA-5818: KafkaStreams state transitions not correct","","closed","","mjsax","2017-09-02T14:51:25Z","2017-09-04T15:39:10Z"
"","3778","KAFKA-5822: Consistent log formatting of topic partitions","","closed","","hachikuji","2017-09-02T00:45:31Z","2017-09-15T00:53:20Z"
"","3766","MINOR: Test SASL authorization id","","closed","","rajinisivaram","2017-08-31T12:54:01Z","2017-08-31T16:44:08Z"
"","3763","KAFKA-5812; Represent logDir with case class where absolute path is required","","closed","","lindong28","2017-08-30T22:12:08Z","2018-06-26T16:52:04Z"
"","3759","[WIP]: extract and make public KeySchema & HasNextCondition","","closed","","dguy","2017-08-30T14:53:06Z","2017-08-31T10:28:35Z"
"","3758","KAFKA-3131: enable error level for SSLException logs","","closed","","omkreddy","2017-08-30T12:00:25Z","2018-07-03T15:46:37Z"
"","3757","KAFKA-5379 follow up: reduce redundant mock processor context","","closed","","guozhangwang","2017-08-29T21:28:16Z","2017-11-06T22:45:24Z"
"","3752","Make ByteBufferInputStream.read(byte[], int, int) to follow the contract","","closed","","leventov","2017-08-28T20:36:49Z","2020-11-26T04:26:45Z"
"","3739","KAFKA-5789: Deleted topic is recreated when consumer subscribe the deleted one","","closed","","stakafum","2017-08-25T10:30:23Z","2019-05-11T08:30:21Z"
"","3735","KAFKA-4869: Update 0.10.2.0 upgrade notes","","closed","","omkreddy","2017-08-25T09:21:08Z","2018-07-03T15:46:32Z"
"","3734","KAFKA-5785; Always close connection if KafkaChannel.setSend throws exception","","closed","","ijuma","2017-08-25T09:07:23Z","2017-09-05T08:37:46Z"
"","3733","MINOR: KIP-160 docs","","closed","","guozhangwang","2017-08-25T00:17:32Z","2017-11-06T22:45:28Z"
"","3730","MINOR: stateful docs for aggregates","","closed","","enothereska","2017-08-24T17:02:38Z","2017-08-30T09:25:00Z"
"","3726","KAFKA-5777. Add ducktape integration for Trogdor","","closed","","cmccabe","2017-08-24T00:28:20Z","2019-05-20T19:08:44Z"
"","3723","KAFKA-5358: Consumer perf tool should count rebalance time.","","closed","","huxihx","2017-08-23T02:09:50Z","2017-09-06T02:47:45Z"
"","3722","KAFKA-5603: Don't abort TX for zombie tasks","","closed","","mjsax","2017-08-23T01:08:12Z","2017-08-24T16:32:29Z"
"","3721","Upgrade to ducktape 0.7.1","","closed","","cmccabe","2017-08-23T00:45:29Z","2019-05-20T19:09:06Z"
"","3719","KAFKA-5603: Don't abort TX for zombie tasks","","closed","","mjsax","2017-08-23T00:11:26Z","2017-09-05T23:35:28Z"
"","3718","KAFKA-5767; Kafka server should halt if IBP < 1.0.0 and there is log directory failure","","closed","","lindong28","2017-08-22T23:41:47Z","2017-10-18T02:08:22Z"
"","3710","[MINOR] Fix comment on how to consume __consumer_offsets","","closed","","Elyahou","2017-08-22T06:15:37Z","2018-01-26T19:29:57Z"
"","3709","KAFKA-5759; Allow user to specify relative path as log directory","","closed","","lindong28","2017-08-22T00:55:07Z","2017-09-01T16:51:30Z"
"","3707","MINOR: Verify startup of zookeeper service in system tests","","closed","","kkonstantine","2017-08-21T18:48:14Z","2017-08-23T16:53:57Z"
"","3706","KAFKA-5753: ShellTest.testRunProgramWithErrorReturn fails on macOS","","closed","","ijuma","2017-08-21T15:01:00Z","2017-08-22T06:36:51Z"
"","3704","KAFKA-5748 Fix console producer to set timestamp and partition","","open","","rancp","2017-08-21T06:01:48Z","2018-03-02T19:30:33Z"
"","3702","KAFKA-5756 Synchronization issue on flush","","closed","connect,","oleg-smith","2017-08-20T22:00:15Z","2020-10-16T06:29:11Z"
"","3700","KAFKA-5752: Update timeIndex, txnIndex file pointers to renamed (to be deleted) files","","closed","","omkreddy","2017-08-19T19:16:21Z","2018-07-03T15:46:31Z"
"","3699","Add the Trogdor fault injection daemon","","closed","","cmccabe","2017-08-19T18:08:51Z","2019-05-20T19:08:27Z"
"","3698","KAFKA-5750: Elevate log messages for denials to INFO in SimpleAclAuthorizer class","","closed","","omkreddy","2017-08-19T17:28:48Z","2018-07-03T15:46:39Z"
"","3697","KAFKA-2105: add topic null check to KafkaProducer.partitionsFor method","","closed","","omkreddy","2017-08-19T14:54:15Z","2018-07-03T15:46:34Z"
"","3696","KAFKA-4856: make unregisterAppInfo method to synchronized","","closed","","omkreddy","2017-08-19T08:00:41Z","2018-07-03T15:46:30Z"
"","3695","MINOR: Typographical error corrected in the StreamsBuilder Javadoc.","","closed","","kamalcph","2017-08-19T06:38:42Z","2017-08-25T12:48:09Z"
"","3694","MINOR: Improve help doc of ConsumerGroupCommand","","open","","vahidhashemian","2017-08-18T21:25:12Z","2018-06-01T19:31:07Z"
"","3693","KAFKA-2254: Use double quote to prevent globbing and word splitting in shell scripts","","closed","","omkreddy","2017-08-18T17:59:29Z","2018-07-03T15:43:00Z"
"","3689","KAFKA-5748 Fix console producer to set timestamp and partition","","closed","","rancp","2017-08-17T21:42:08Z","2017-08-21T06:00:06Z"
"","3688","KAFKA-5747: Producer snapshot loading should cover schema and other errors","","closed","","hachikuji","2017-08-17T21:33:32Z","2017-08-21T17:25:00Z"
"","3683","KAFKA-5686: update compression docs","","closed","","omkreddy","2017-08-17T09:57:23Z","2018-07-03T15:42:32Z"
"","3682","KAFKA-5745: makeLeader should invoke `convertHWToLocalOffsetMetadata` before marking it as leader","","closed","","huxihx","2017-08-17T03:48:56Z","2017-08-19T01:29:20Z"
"","3680","KAFKA-5743. Ducktape services should use subdirs of /mnt","","closed","","cmccabe","2017-08-16T23:55:50Z","2019-05-20T18:58:26Z"
"","3678","KAFKA-5233 follow up","","closed","streams,","mihbor","2017-08-16T22:26:45Z","2018-02-05T21:42:12Z"
"","3677","KAFKA-5742 support ZK chroot in system tests","","closed","","xvrl","2017-08-16T21:18:05Z","2017-08-17T22:25:55Z"
"","3674","KAFKA-5737. KafkaAdminClient thread should be daemon","","closed","","cmccabe","2017-08-15T23:21:29Z","2019-05-20T18:58:38Z"
"","3666","KAFKA-5730: Consumer should invoke async commit callback before sync commit returns","","closed","","hachikuji","2017-08-14T19:02:20Z","2017-08-17T22:07:22Z"
"","3665","HOTFIX: ConsoleConsumer using wrong old consumer config value for auto.offset.reset","","closed","","hachikuji","2017-08-14T16:33:10Z","2017-08-14T20:53:32Z"
"","3660","KAFKA-5727: Archetype project for Streams quickstart and tutorial web docs","","closed","","guozhangwang","2017-08-11T19:40:54Z","2017-11-06T22:45:13Z"
"","3656","KAFKA-5725: More failure testing","","closed","","enothereska","2017-08-11T08:35:54Z","2017-08-18T17:14:40Z"
"","3651","MINOR: Add missing deprecations on old request objects","","closed","","hachikuji","2017-08-09T19:53:23Z","2017-08-10T00:33:17Z"
"","3649","MINOR: Add one more instruction","","closed","","enothereska","2017-08-09T10:26:54Z","2017-08-11T06:23:02Z"
"","3644","KAFKA-5711: batch restore should handle deletes","","closed","","bbejeck","2017-08-08T18:13:20Z","2017-08-10T18:33:23Z"
"","3642","KafkaAdminClient should remove inflight call correctly after response is received","","closed","","lindong28","2017-08-08T08:21:55Z","2017-09-01T16:54:55Z"
"","3638","MINOR: Fix missing wait_until import from bad cherry-pick","","closed","","ewencp","2017-08-07T15:40:30Z","2017-08-08T01:30:06Z"
"","3635","KAFKA-3417: Wrap reporter calls in try/catch blocks","","closed","","mimaison","2017-08-07T11:53:16Z","2018-04-30T11:34:02Z"
"","3629","MINOR: First cut at porting State Store docs to AK","","closed","","bbejeck","2017-08-04T21:50:46Z","2017-08-11T08:37:02Z"
"","3628","KAFKA-5681; jarAll should build all scala versions.","","closed","","becketqin","2017-08-04T19:52:25Z","2017-08-07T21:31:40Z"
"","3627","MINOR: Fix error response handler for controlled shutdown v0","","closed","","hachikuji","2017-08-04T18:40:34Z","2017-08-04T20:09:49Z"
"","3626","KAFKA-5644: Fix Reset Consumer Group Offset tool to handle minute component of TimeZone","","closed","","omkreddy","2017-08-04T17:23:59Z","2018-07-03T15:46:32Z"
"","3625","HOTFIX: fix for standby tasks using batching restore","","closed","","bbejeck","2017-08-04T16:09:38Z","2017-08-07T17:14:09Z"
"","3621","KAFKA-5694; Add AlterReplicaDirRequest and DescribeReplicaDirRequest (KIP-113)","","closed","","lindong28","2017-08-04T05:21:57Z","2017-11-05T21:22:01Z"
"","3617","HOTFIX add the hyper link for storage","","closed","","guozhangwang","2017-08-03T21:54:02Z","2017-11-06T22:45:10Z"
"","3615","MINOR: Avoid duplicate processing of notifications in ZkNodeChangeNotificationListener","","closed","","omkreddy","2017-08-03T10:39:51Z","2017-08-04T11:20:47Z"
"","3614","KAFKA-5695; Test DeleteRecordsRequest in AuthorizerIntegrationTest","","closed","","lindong28","2017-08-03T04:59:58Z","2017-10-18T02:11:20Z"
"","3610","KAFKA-5674: max.connections.per.ip minimum should be 0","","closed","","viktorsomogyi","2017-08-02T13:59:24Z","2018-08-23T13:10:01Z"
"","3604","MINOR: add memory management section to streams docs","","closed","","dguy","2017-08-01T12:06:29Z","2017-08-16T13:23:00Z"
"","3602","KAFKA-5671: Add StreamsBuilder and Deprecate KStreamBuilder","","closed","","mjsax","2017-07-31T02:46:12Z","2018-06-05T23:50:04Z"
"","3597","KAFKA-5678: When the broker graceful shutdown occurs, the producer side sends timeout.","","open","","xiguantiaozhan","2017-07-30T05:38:18Z","2018-03-02T19:30:26Z"
"","3594","KAFKA-5663; Fix LogDirFailureTest system test","","closed","","lindong28","2017-07-28T21:56:26Z","2017-09-01T16:53:23Z"
"","3591","Throttled producer","","closed","","abhijitiitr","2017-07-28T03:52:37Z","2017-07-28T04:00:14Z"
"","3590","KAFKA-5670: (KIP-120) Add Topology and deprecate TopologyBuilder","","closed","","mjsax","2017-07-28T00:31:02Z","2018-06-05T23:50:44Z"
"","3583","KAFKA-5341; Add UnderMinIsrPartitionCount and per-partition UnderMinIsr metrics (KIP-164)","","closed","","lindong28","2017-07-26T17:33:20Z","2017-08-02T18:52:59Z"
"","3582","KAFKA-5656: Support bulk attributes request on KafkaMbean where some attributes do not exist","","closed","","ErikKringen","2017-07-26T17:13:41Z","2017-09-08T12:32:11Z"
"","3580","MINOR: Next release will be 1.0.0","","closed","","ijuma","2017-07-26T12:13:42Z","2017-08-22T06:37:28Z"
"","3579","Bump version to 0.11.0.1-SNAPSHOT","","closed","","ijuma","2017-07-26T11:55:20Z","2017-08-22T06:37:29Z"
"","3576","Fix typo in SMT doc : s/RegexpRouter/RegexRouter","","closed","","rmoff","2017-07-26T07:21:39Z","2017-07-26T16:19:00Z"
"","3571","KAFKA-5611; AbstractCoordinator should handle wakeup raised from onJoinComplete","","closed","","hachikuji","2017-07-25T05:44:52Z","2018-05-11T06:28:30Z"
"","3569","MINOR: enforce setting listeners in CREATE state.","","closed","","bbejeck","2017-07-24T22:40:55Z","2017-07-26T08:09:26Z"
"","3567","KAFKA-4711: fix docs onunclean.leader.election.enable default","","closed","","bobrik","2017-07-24T19:03:48Z","2018-01-10T21:32:17Z"
"","3565","KAFKA-5627; Reduce classes needed for LeaderAndIsrPartitionState and MetadataPartitionState","","closed","","lindong28","2017-07-23T02:47:49Z","2017-08-02T18:53:20Z"
"","3564","MINOR: Don't imply that ConsumerGroupCommand won't work with non-Java clients","","closed","","ewencp","2017-07-21T22:48:43Z","2017-07-22T02:37:50Z"
"","3563","Added safe deserialization impl","","closed","","rhauch","2017-07-21T21:38:26Z","2017-07-21T23:03:42Z"
"","3562","MINOR: Improve log warning to include the log name","","closed","","ijuma","2017-07-21T12:41:55Z","2017-08-22T06:38:31Z"
"","3561","MINOR: Give correct instructions for retaining previous unclear leader election behaviour","","closed","","ijuma","2017-07-21T12:40:58Z","2017-08-22T06:37:34Z"
"","3560","MINOR: Improve log warning to include the log name","","closed","","ijuma","2017-07-21T12:40:28Z","2017-08-22T06:38:33Z"
"","3559","KAFKA-5535: Handle null values in ExtractField","","closed","connect,","ewencp","2017-07-21T03:56:00Z","2020-10-16T06:08:16Z"
"","3554","KAFKA-5123 Refactor ZkUtils readData* methods","","closed","","baluchicken","2017-07-20T13:24:11Z","2017-12-14T10:48:23Z"
"","3553","KAFKA-5608: Follow-up to fix potential NPE and clarify method name","","closed","","ijuma","2017-07-20T11:15:17Z","2017-08-22T06:38:48Z"
"","3549","KAFKA-5461: Add metric to track global topic count and global parition count in a cluster","","closed","","abhishekmendhekar","2017-07-19T17:17:16Z","2017-08-04T10:30:46Z"
"","3547","KAFKA-5608: Add --wait option for JmxTool and use in system tests to avoid race between JmxTool and monitored services","","closed","","ewencp","2017-07-19T05:03:11Z","2017-07-20T11:17:58Z"
"","3546","MINOR: Apply extra serialized rsync step to both parallel and serial paths","","closed","","ewencp","2017-07-18T21:14:48Z","2017-07-20T17:50:12Z"
"","3545","KAFKA-5499: Minor cleanup of commit code","","closed","","enothereska","2017-07-18T13:24:59Z","2017-08-30T09:58:46Z"
"","3541","MINOR: Add another common error case for CorruptRecordException's error message","","closed","","ewencp","2017-07-18T01:18:48Z","2017-07-18T20:15:05Z"
"","3531","get new version","","closed","","devon-ye","2017-07-15T16:21:42Z","2017-12-23T05:44:55Z"
"","3529","MINOR: Use correct connectionId in error message","","closed","","rajinisivaram","2017-07-14T19:39:43Z","2017-07-17T08:52:37Z"
"","3523","MINOR: log4j for member failure from debug to info","","closed","","guozhangwang","2017-07-12T18:18:18Z","2017-07-15T22:06:39Z"
"","3520","MINOR: Typo error corrected in the KStream Javadoc.","","closed","","kamalcph","2017-07-11T10:18:09Z","2018-02-12T05:02:15Z"
"","3519","KAFKA-5576: increase the rocksDB version to 5.5.1 for Power support","","closed","","yussufsh","2017-07-11T05:01:01Z","2017-10-06T07:40:19Z"
"","3517","KAFKA-5579 check for null.","","closed","connect,","jcustenborder","2017-07-10T21:46:05Z","2020-10-16T06:29:10Z"
"","3510","HOTFIX: fix a few typos on streams quickstart","","closed","","guozhangwang","2017-07-10T01:44:36Z","2017-07-15T22:06:42Z"
"","3505","MINOR: Reduce logging level","","closed","","enothereska","2017-07-07T19:33:48Z","2017-07-07T21:28:18Z"
"","3504","KAFKA-5566: fixed race condition between flush and commit","","closed","","mjsax","2017-07-07T18:08:13Z","2017-07-10T22:39:29Z"
"","3502","HOTFIX: fix paths in streams index","","closed","","dguy","2017-07-07T13:56:39Z","2017-08-16T13:23:08Z"
"","3499","KAFKA-5567: Connect sink worker should commit offsets of original topic partitions","","closed","connect,","kkonstantine","2017-07-07T05:16:47Z","2020-10-16T06:08:16Z"
"","3497","HOTFIX: disable flaky system tests","","closed","","mjsax","2017-07-06T21:05:13Z","2017-07-23T18:13:39Z"
"","3496","KAFKA-5464: Follow up. Increase poll timeout","","closed","","mjsax","2017-07-06T17:05:37Z","2017-07-07T21:35:14Z"
"","3494","MINOR: Move quickstart under streams","","closed","","enothereska","2017-07-06T13:02:52Z","2017-07-10T08:13:51Z"
"","3493","HOTFIX: fix broken streams test","","closed","","dguy","2017-07-06T11:06:09Z","2017-08-16T13:23:09Z"
"","3487","MINOR: Fixed misleading reference to HTTPS instead of SSL support in the doc","","closed","","ppatierno","2017-07-04T14:40:28Z","2017-07-21T07:46:00Z"
"","3484","MINOR: add IQ docs to streams documentation","","closed","","dguy","2017-07-04T11:56:13Z","2017-08-16T13:23:11Z"
"","3483","MINOR: Wait for tasks to terminate to avoid exception in test teardown","","closed","","rajinisivaram","2017-07-04T11:25:12Z","2017-07-12T18:42:55Z"
"","3482","MINOR: Make topic config a section, with TOC entry.","","closed","","tombentley","2017-07-04T08:55:50Z","2017-07-05T08:42:46Z"
"","3481","MINOR: Configuration name corrected in the upgrade docs.","","closed","","kamalcph","2017-07-03T16:50:47Z","2018-02-12T05:02:45Z"
"","3480","MINOR: Define the term tombstone, since it's used elsewhere in the docs","","closed","","tombentley","2017-07-03T09:57:41Z","2020-07-09T10:57:53Z"