"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","2464","KAFKA-4662: adding test coverage for addSource methods with AutoOffse…","…tReset","closed","","bbejeck","2017-01-30T03:08:21Z","2017-02-03T16:51:16Z"
"","2311","KAFKA-4523; Fix crash at shutdown due to the group coordinator attemp…","…ting to write to a closed log.  Solution: shut down the group manager before shutting down the log manager to ensure that any delayed operations are done.","closed","","steveniemitz","2017-01-04T19:59:52Z","2017-01-05T11:07:56Z"
"","1590","KAFKA-3926: Transient test failures -  confirm all expected topics added/removed in createStream…","…Tasks call versus simply asserting createStreamTasks method called.","closed","","bbejeck","2016-07-06T04:40:24Z","2016-07-06T19:32:47Z"
"","1870","[KAFKA-4185] Abstract out password verifier in SaslServer as an injec…","…table dependency","open","","piyushvijay","2016-09-16T21:18:24Z","2022-06-14T12:52:12Z"
"","1849","KAFKA-4157: Transient system test failure in replica_verification_tes…","…t.test_replica_lags","closed","","granthenke","2016-09-13T14:28:14Z","2016-09-17T22:00:33Z"
"","1532","KAFKA-3842: consolidate utility methods to TestUtils, added StreamsTe…","…stUtils, added method for pausing tests to TestUtils  Changes made: 1. Added utility method for creating consumer configs. 2. Added methods for creating producer, consumer configs with default values for de/serializers. 3. Pulled out method for waiting for test state to TestUtils (not using Thread.sleep). 4. Added utility class for creating streams configs and methods providing default de/serializers.","closed","","bbejeck","2016-06-21T02:53:15Z","2016-06-25T03:07:14Z"
"","1772","MINOR: Update Kafka configuration documentation to use kafka-configs.…","…sh, instead of deprecated kafka-topics.sh --alter","closed","","SinghAsDev","2016-08-23T00:23:09Z","2016-08-26T02:16:32Z"
"","2100","KAFKA-4375: Reset thread interrupted state in a few places where Inte…","…rruptedException is caught in a thread Kafka didn't start.  See https://issues.apache.org/jira/browse/KAFKA-4375","closed","","srdo","2016-11-03T22:05:32Z","2016-12-09T22:15:12Z"
"","1770","WIP: KAFKA-2629: Enable getting passwords from an executable rathe…","…r than passing plaintext password","closed","","SinghAsDev","2016-08-22T22:24:44Z","2021-12-19T21:42:44Z"
"","2143","KAFKA-4400: Enabling configurable prefix for sink connectors' consume…","…r group names.  Author: Tianji Li   Reviewers: Ewen Cheslack-Postava","open","connect,","skyahead","2016-11-16T19:53:32Z","2018-03-02T19:29:47Z"
"","1642","[KAFKA-3979] Optimize memory used by replication process by using ada…","…ptive fetch message size","closed","","nepal","2016-07-20T13:37:59Z","2016-08-15T12:34:40Z"
"","1995","KAFKA-4254: Update producers metadata before failing on non-existent …","…partition","closed","","kkonstantine","2016-10-08T08:01:54Z","2016-10-14T04:28:34Z"
"","1554","MINOR: Follow-up from KAFKA-3842 with suggested fixes to creating tem…","…p directories, waitForCondition","closed","","bbejeck","2016-06-26T01:25:23Z","2016-06-29T22:17:37Z"
"","1844","MINOR: Adjusted session timeout intervals to stop triggering spuri…","…ous rebalances","closed","","enothereska","2016-09-12T10:10:58Z","2016-09-17T07:24:08Z"
"","1631","KAFKA-3934: Start scripts enable GC by default with no way t…","…o disable","closed","","granthenke","2016-07-18T15:51:20Z","2016-08-09T16:15:51Z"
"","1563","KAFKA-3904: File descriptor leaking (Too many open files) for long ru…","…nning stream process  I noticed when my application was running for more than one day, I will get 'Too many open files' error.  I used 'lsof' to list all the file descriptors used by the process, it's over 32K, but most of them belongs to the .lock file, e.g. a single lock file shows 2700 times.  I looked at the code, I think the problem is in:     FileChannel channel = new RandomAccessFile(lockFile, ""rw"").getChannel(); Each time new RandomAccessFile is called, a new fd will be created.  Fix this by caching the FileChannels we created so far.","closed","","HenryCaiHaiying","2016-06-28T05:34:34Z","2016-07-01T10:34:48Z"
"","1680","KAFKA-3946: Protocol guide should say that Produce request acks can o…","…nly be 0, 1, or -1  Rephrased the documentation string for the Produce request Updated the acks configuration docs to state that -1, 0, and 1 are the only allowed values","closed","","mimaison","2016-07-29T13:54:02Z","2018-04-18T13:32:04Z"
"","1909","MINOR: Allow for asynchronous start of producer consumer in validatio…","…n test","closed","","kkonstantine","2016-09-26T20:53:14Z","2016-09-29T20:55:21Z"
"","1577","KAFKA-3794: added stream / table names as prefix to functions that pr…","…int to the console.","closed","","bbejeck","2016-07-01T02:08:43Z","2016-07-06T17:47:18Z"
"","1678","KAFKA-3852: Clarify how to handle message format upgrade without kill…","…ing performance","closed","","gwenshap","2016-07-28T18:52:38Z","2016-07-28T20:48:33Z"
"","1966","KAFKA-4244: fixing formating issues in docs. missing headers and lots of paragrap…","…h misformatting","closed","","gwenshap","2016-10-04T21:45:47Z","2016-10-10T20:43:01Z"
"","1754","KAFKA-4057: Allow to specify the request version and replica ID in ka…","…fka.javaapi.FetchRequest - Added new arguments for versionId and replicaId in the constructor instead of using hardcoded values","closed","","mimaison","2016-08-17T14:56:22Z","2018-04-18T13:32:02Z"
"","1737","KAFKA-4038: Transient failure in DeleteTopicsRequestTest.testErrorDel…","…eteTopicRequests","closed","","granthenke","2016-08-15T02:15:00Z","2016-08-18T11:32:40Z"
"","1827","KAFKA-4081: Consumer API consumer new interface commitSync does not v…","…erify the validity of offset  Commit throws InvalidOffsetException if the offset is negative","closed","","mimaison","2016-09-06T16:57:32Z","2018-04-18T13:31:48Z"
"","2042","KAFKA-4312: If filePath is empty string writeAsText should have more m…","…eaningful error message","closed","","bbejeck","2016-10-19T01:24:03Z","2016-10-19T23:35:58Z"
"","2005","KAFKA-4269 extracted code updating topics when regex pattern specifie…","…d out of topicGroups method. The topicGroups method only called from StreamPartitionAssignor when KafkaStreams object  is the leader, needs to be executed for clients.","closed","","bbejeck","2016-10-10T23:42:26Z","2016-11-01T22:14:32Z"
"","2203","KAFKA-2857 ConsumerGroupCommand throws GroupCoordinatorNotAvailableEx…","…ception when describing a non-existent group before the offset topic is created (Old PR #1548)","closed","","imandhan","2016-12-02T09:41:55Z","2017-01-27T22:18:10Z"
"","1634","KAFKA-3924: Replacing halt with exit upon LEO mismatch to trigger gra…","…ceful shutdown  The patch is pretty simple and the justification is explained in https://issues.apache.org/jira/browse/KAFKA-3924  I could not find Andrew Olson, who seems to be the contributor of this part of the code, in github so I am not sure whom I should ask to review the patch.   the contribution is my original work and that i license the work to the project under the project's open source license.","closed","","maysamyabandeh","2016-07-18T20:52:13Z","2016-07-27T15:22:54Z"
"","1632","MINOR: MetadataCache brokerId is not set on first run with generated …","…broker id  This is because the id passed into the MetadataCache is the value from the config before the real broker id is generated.","closed","","granthenke","2016-07-18T17:00:08Z","2016-07-20T23:49:59Z"
"","2011","KAFKA-4291: TopicCommand --describe shows topics marked for deletion …","…as under-replicated and unavailable  TopicCommand --describe now shows if a topic is marked for deletion.  Developed with @edoardocomar","closed","","mimaison","2016-10-11T15:27:19Z","2018-04-18T13:31:39Z"
"","2270","KAFKA-4552. README.md has org.gradle.project.maxParallelForms instead…","… of maxParallelForks","closed","","cmccabe","2016-12-16T22:21:45Z","2019-05-20T18:33:04Z"
"","1626","[KAFKA-3968] Call fsync for parent directory first time when we flush…","… message set","open","","nepal","2016-07-15T15:39:33Z","2018-03-02T19:29:37Z"
"","2404","KAFKA-4060 and KAFKA-4476 follow up","ZK removed reveal a bug in `StreamPartitionAssigner` but did not fix it properly. This is a follow up bug fix.  Issue:  - If topic metadata is missing, `StreamPartitionAssigner` should not create any affected tasks that consume topics with missing metadata.  - Depending downstream tasks should not be create either.  - For tasks that are not created, no store changelog topics (if any) should get created  - For tasks that write output to not-yet existing internal repartitioning topics, those repartitioning topics should not get created","closed","","mjsax","2017-01-19T00:24:49Z","2017-01-21T21:19:28Z"
"","2189","KAFKA-4271: Fix the server start script for Windows 32-bit OS","Without this fix the new consumer fails to run on a 32-bit Windows OS.","closed","","vahidhashemian","2016-11-29T22:01:18Z","2016-12-01T23:40:34Z"
"","1645","HOTFIX: Adding init file so streams benchmark is autodiscovered","Without this file the benchmark does not run nightly.","closed","","enothereska","2016-07-21T02:35:04Z","2016-07-21T23:19:31Z"
"","2347","Initial commit of partition assignment check in console consumer.","With this patch, the consumer will considered initialized in the ProduceConsumeValidate tests only if it has partitions assigned.","closed","","apurvam","2017-01-12T00:15:42Z","2017-02-06T17:00:10Z"
"","1622","KAFKA-3859: Fix describe group command to report valid status when group is empty","With the new consumer, when all consumers of a consumer group are stopped (i.e. the group is empty), the describe consumer group command returns `Consumer group ... is rebalancing.` This PR fixes this issue by distinguishing between when the consumer group is actually rebalancing and when it has no active members.","closed","","vahidhashemian","2016-07-14T05:01:02Z","2016-10-22T20:30:45Z"
"","2232","HOTFIX: Fix HerderRequest.compareTo()","With KAFKA-3008 (#1788), the implementation does not respect the contract that 'sgn(x.compareTo(y)) == -sgn(y.compareTo(x))'  This fix addresses the hang with JDK8 in DistributedHerderTest.compareTo()","closed","connect,","shikhar","2016-12-08T23:23:56Z","2020-10-16T06:08:11Z"
"","2130","MINOR: Extract SCALA_BINARY_VERSION from SCALA_VERSION","Will allow users to set one fewer environment variable if they need to change scala version. Still, SCALA_BINARY_VERSION can be explicitly set.","closed","","kkonstantine","2016-11-14T22:11:31Z","2016-11-17T02:15:52Z"
"","1894","KAFKA-3438: Rack-aware replica assignment should warn of overloaded brokers","When using the `--generate` option of the replica reassignment tool the balance of replicas across brokers is checked (only if rack-aware is enabled) and in case an imbalance is detected, a proper warning message is appended to the output of the tool (which is the suggested assignment).  An example warning message:  ``` Warning: In the proposed assignment the most loaded broker (6) has 4.5x as many replicas as the least loaded broker (0). This is likely due to an uneven distribution of brokers across racks. You are advised to alter the rack configuration so there are approximately the same number of brokers per rack.  Stats for generated assignment:  - Number of brokers per rack:     rack0 -> 3 (0, 1, 2)     rack1 -> 3 (3, 4, 5)     rack2 -> 1 (6)  - Number of replicas per broker:     0 -> 2     1 -> 5     2 -> 3     3 -> 4     4 -> 4     5 -> 3     6 -> 9  - Number of replicas per rack:     rack0 -> 10     rack1 -> 11     rack2 -> 9 ```","open","","vahidhashemian","2016-09-20T23:56:39Z","2018-06-05T22:46:13Z"
"","2163","fix issue: stop kafka or zookeeper failed when full command length reach 4096 bytes","when user want to stop the zookeeper or kafka server, they will exec the script kafka-server-stop.sh or kafka-server-stop.sh.  In kafka-server-stop.sh and kafka-server-stop.sh, it get the pid with ps command, as below: PIDS=$(ps ax | grep -i 'kafka.Kafka' | grep java | grep -v grep | awk '{print $1}') PIDS=$(ps ax | grep java | grep -i QuorumPeerMain | grep -v grep | awk '{print $1}')  Since linux limit process command in 4096 bytes, the above command will fail when the user install kafka in a  folder with long path, just like ""/opt/bigdata/kafka_2.10-0.10.0.0"".  To fix this issue, we need to modify it as below: step1. modify kafka-run-class.sh, add -D parameter to the command; for kafka ,it is : java -Dproc_kafkaServer for zookeeper, it is : java -Dproc_zookeeper step2. modify kafka-server-stop.sh and kafka-server-stop.sh, get the PID with -D parameter information. if [ -z ""$PIDS"" ]; then PIDS=$(ps ax | grep java | grep -i proc_kafkaServer| grep -v grep | awk '{print $1}') fi if [ -z ""$PIDS"" ]; then PIDS=$(ps ax | grep java | grep -i proc_zookeeper| grep -v grep | awk '{print $1}') fi","open","","muliliao","2016-11-23T11:30:28Z","2018-03-02T19:29:49Z"
"","2162","Fixed kafka stop error","when user want to stop the zookeeper or kafka server, they will exec the script kafka-server-stop.sh or kafka-server-stop.sh.      In kafka-server-stop.sh and kafka-server-stop.sh, it get the pid with ps command, as below: PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '{print $1}') PIDS=$(ps ax | grep java | grep -i QuorumPeerMain | grep -v grep | awk '{print $1}')      Since linux limit process command in 4096 bytes, the above command will fail when the user install kafka in a  folder with long path, just like ""/opt/bigdata/kafka_2.10-0.10.0.0"".      To fix this issue, we need to modify it as below: step1. modify kafka-run-class.sh, add -D parameter to the command;  for kafka ,it is  :  java -Dproc_kafkaServer for zookeeper, it is : java -Dproc_zookeeper step2. modify kafka-server-stop.sh and kafka-server-stop.sh, get the PID with -D parameter information.  if [ -z ""$PIDS"" ]; then   PIDS=$(ps ax | grep java | grep -i proc_kafkaServer| grep -v grep | awk '{print $1}') fi  if [ -z ""$PIDS"" ]; then   PIDS=$(ps ax | grep java | grep -i proc_zookeeper| grep -v grep | awk '{print $1}') fi","closed","","muliliao","2016-11-23T09:43:53Z","2016-11-23T11:18:25Z"
"","2074","KAFKA-3853 (KIP-88): Report offsets for empty groups in consumer group describe command (new consumer)","When there are no active new consumers in a consumer group report the offsets within the group instead of reporting that the group has no active members.  This PR also implements the API change proposed by [KIP-88](https://cwiki.apache.org/confluence/display/KAFKA/KIP-88%3A+OffsetFetch+Protocol+Update).","closed","","vahidhashemian","2016-10-28T20:37:09Z","2017-01-14T03:28:43Z"
"","1839","KAFKA-4135: Consumer polls when not subscribed to any topic or assigned any partition should raise an exception","When the consumer is not subscribed to any topic or, in the case of manual assignment, is not assigned any partition, calling `poll()` should raise an exception.","closed","","vahidhashemian","2016-09-09T19:04:49Z","2016-09-19T23:43:49Z"
"","2087","MINOR: Fix NPE when Connect offset contains non-primitive type","When storing a non-primitive type in a Connect offset, the following NullPointerException will occur:  ``` 07:18:23.702 [pool-3-thread-1] ERROR o.a.k.c.storage.OffsetStorageWriter - CRITICAL: Failed to serialize offset data, making it impossible to commit offsets under namespace tenant-db-bootstrap-source. This likely won't recover unless the unserializable partition or offset information is overwritten. 07:18:23.702 [pool-3-thread-1] ERROR o.a.k.c.storage.OffsetStorageWriter - Cause of serialization failure: java.lang.NullPointerException: null 	at org.apache.kafka.connect.storage.OffsetUtils.validateFormat(OffsetUtils.java:51) 	at org.apache.kafka.connect.storage.OffsetStorageWriter.doFlush(OffsetStorageWriter.java:143) 	at org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets(WorkerSourceTask.java:319) ... snip ... ```  The attached patch fixes the specific case where OffsetUtils.validateFormat is attempting to provide a useful error message, but fails to because the schemaType method could return null.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","mfenniak","2016-11-01T13:38:22Z","2016-11-02T17:28:06Z"
"","1875","KAFKA-4190 kafka-reassign-partitions does not report syntax problem in json","When specifying invalid json file, kafka-reassign-partitions fails with error ""file is empty"" instead of reporting syntax error.","closed","","chemikadze","2016-09-18T08:25:36Z","2017-09-15T16:06:36Z"
"","1691","MINOR: lower logging severity for offset reset","When resetting the first dirty offset to the log start offset, we currently log an ERROR which makes users think the log cleaner has a problem and maybe has exited.  We should log a WARN instead to avoid alarming the users.","closed","","cotedm","2016-08-01T15:12:14Z","2016-08-01T16:08:54Z"
"","1555","MINOR: Fix ambiguous log message in RecordCollector","When producing fails in Kafka Streams, it gives an error like below:  ``` Error sending record: null ```  by this line: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java#L59  This isn't not making sense because of: - Practically metadata is always null when exception != null : https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordBatch.java#L107-L109 - It's quite misleading as we would interpret it like ""Kafka Streams attempted to send 'null' as a record"" which isn't in fact  As I find a PR #873  as the origin of the above line I changed it to instantiate callback on each send in order to log destination topic at least.","closed","","kawamuray","2016-06-26T08:00:26Z","2016-06-30T18:16:33Z"
"","2449","KAFKA-4557: Handle Producer.send correctly in expiry callbacks","When iterating deque for expiring record batches, delay the actual completion of the batch until iteration is complete since callbacks invoked during expiry may send more records, modifying the deque, resulting in ConcurrentModificationException in the iterator.","closed","","rajinisivaram","2017-01-26T16:30:46Z","2017-01-27T23:50:16Z"
"","2320","MINOR: Maybe decorate inner topics for SourceNode","When creating the source node in TopologyBuilder, we need to decorate its input topics if they are inner (i.e. repartition) topics with the prefix.  Also did some minor cleanup in the printing function for better visualization in debugging.","closed","","guozhangwang","2017-01-05T23:39:12Z","2017-07-15T22:09:11Z"
"","2062","Expose name of processor on KStream","When connecting a processor on a KStream to downstream processors or sinks you need to know the generated name of the processor. This change exposes that name in the call to `process`.","closed","","thijsc","2016-10-25T09:40:24Z","2016-11-01T21:40:51Z"
"","2250","KAFKA-4532: StateStores can be connected to the wrong source topic resulting in incorrect metadata returned from Interactive Queries","When building a topology with tables and StateStores, the StateStores are mapped to the source topic names. This map is retrieved via TopologyBuilder.stateStoreNameToSourceTopics() and is used in Interactive Queries to find the source topics and partitions when resolving the partitions that particular keys will be in. There is an issue where by this mapping for a table that is originally created with builder.table(""topic"", ""table"");, and then is subsequently used in a join, is changed to the internal repartition topic. This is because the mapping is updated during the call to topology.connectProcessorAndStateStores(..). In the case that the stateStoreNameToSourceTopics Map already has a value for the state store name it should not update the Map.","closed","","dguy","2016-12-13T18:13:06Z","2016-12-13T22:16:18Z"
"","2408","KAFKA-4493: Validate a plaintext client connection to a SSL broker","When a SSL Alert protocol is received from the broker by sending a request from a client, `InvalidTransportLayerException` is raised from `selector.poll`.","closed","","taku-k","2017-01-19T18:40:53Z","2018-03-12T09:50:45Z"
"","2223","Update `createFile` function of Uitls.scala","Well,Now I'm reading the source code and try to learn better Scala:-D.The question I find is sometimes obvious,If I try to create file with out certain permission,**the exception will be throwes like this:`java.io.IOException: Permission denied at java.io.UnixFileSystem.createFileExclusively(Native Method)`.** People should take more care when create file.ahh...I think this is not about Kafka logic? Thanks for your reply:-D","closed","","Allianzcortex","2016-12-07T07:28:54Z","2018-10-21T19:46:25Z"
"","2192","MyKafka","We suspect that the test suite hangs we have been seeing are due to PermGen exhaustion. It is a common reason for hard JVM lock-ups.  Author: Ismael Juma   Reviewers: Jason Gustafson   Closes #1926 from ijuma/test-jvm-params  (cherry picked from commit 67e99d0869dd49358d7ca549ac715b722fda89f5) Signed-off-by: Jason Gustafson","closed","","geeag","2016-11-30T09:24:35Z","2016-12-26T22:37:42Z"
"","1926","MINOR: Set JVM parameters for the Gradle Test executor processes","We suspect that the test suite hangs we have been seeing are due to PermGen exhaustion. It is a common reason for hard JVM lock-ups.","closed","","ijuma","2016-09-28T16:04:40Z","2016-09-29T02:15:44Z"
"","1890","MINOR: Use `hiResClockMs` in `testRequestExpiry` to fix transient test failure","We recently switched `SystemTimer` to use `hiResClockMs` (based on `nanoTime`), but we were still using `System.currentTimeMillis` in the test. That would sometimes mean that we would measure elapsed time as lower than expected.","closed","","ijuma","2016-09-20T10:18:57Z","2016-10-07T16:00:57Z"
"","1514","MINOR: Mention `log.message.format.version=0.10.0` in rolling upgrade section","We had mentioned this step in the performance impact section in the middle of a long paragraph, which made it easy to miss. I also tweaked the reason for setting `log.message.format.version` as it could be misinterpreted previously.","closed","","ijuma","2016-06-16T18:59:51Z","2016-06-19T23:04:06Z"
"","2066","MINOR: Increase zk connection timeout in tests for client created in `KafkaServer`","We had already made this change to the client created in `ZooKeeperTestHarness`. I last saw this failure when `SaslPlaintextTopicMetadataTest.testAliveBrokerListWithNoTopics` was executed in Jenkins.","closed","","ijuma","2016-10-26T10:37:50Z","2016-10-27T16:33:47Z"
"","1889","MINOR: Increase `zkConnectionTimeout` and timeout in `testReachableServer`","We had a number of failures recently due to these timeouts being too low. It's a particular problem if multiple forks are used while running the tests.","closed","","ijuma","2016-09-20T10:07:58Z","2016-09-22T21:54:19Z"
"","1701","MINOR: increase usability of shell-scripts","we do this by ensuring that the --zookeeper parameter consistently defaults to localhost:2181 — as it already does in some places.","closed","","igalic","2016-08-03T13:34:56Z","2017-11-07T20:04:13Z"
"","1692","MINOR: Fixed documentation for KStream left join KStream-KTable","We are not joining in a window here.","closed","","jpzk","2016-08-01T15:18:17Z","2016-08-03T21:02:30Z"
"","1878","MINOR: Add `KafkaServerStartable` constructor overload for compatibility","We added the `reporters` parameter as part of KIP-74: Cluster Id.","closed","","ijuma","2016-09-18T15:04:27Z","2016-09-19T20:08:54Z"
"","1551","MINOR: fix grammatical errors in DataException message","Was just reading kafka source code, my favourite Friday afternoon activity, when I found these small grammatical errors in some `DataException` messages.    Could someone please review? @ewencp @dguy","closed","","LaurierMantel","2016-06-24T19:10:01Z","2016-06-24T21:17:27Z"
"","2402","KAFKA-4672 fix source compatibility for lambda expressions","Variance changes introduced in KIP-100 cause compilation failures with lambda expression in Java 8. To my knowledge this only affects the following method  `KStreams.transform(TransformerSupplier<...>, String...)`  prior to the changes it was possible to write:  `streams.transform(MyTransformer::new)`  where `MyTransformer` extends `Transformer`  After the changes the Java compiler is unable to infer correct return types for the lambda expressions. This change fixed this by reverting to invariant return types for transformer suppliers.  please cherry-pick into 0.10.2.x","closed","","xvrl","2017-01-18T22:16:56Z","2017-04-17T22:00:33Z"
"","2356","KAFKA-4581: Fail early if multiple client login modules in sasl.jaas.config","Validate and fail client connection if multiple login modules are specified in sasl.jaas.config to avoid harder-to-debug authentication failures later on.","closed","","rajinisivaram","2017-01-12T15:20:13Z","2017-01-13T11:38:35Z"
"","1697","KAFKA-4014; Using Collections.singletonList instead of Arrays.asList for single arguments","Using Collections.singletonList instead of Arrays.asList for single arguments","closed","","rekhajoshm","2016-08-02T22:09:26Z","2016-08-06T18:23:15Z"
"","1768","KAFKA-4051: Use nanosecond clock for timers in broker","Use System.nanoseconds instead of System.currentTimeMillis in broker timer tasks to cope with changes to wall-clock time.","closed","","rajinisivaram","2016-08-22T09:59:21Z","2016-08-22T22:59:04Z"
"","1834","[WIP] KAFKA-4140: make system tests parallel friendly","Updates to take advantage of soon-to-be-released ducktape features.","closed","","granders","2016-09-07T19:20:11Z","2016-12-12T02:44:10Z"
"","2375","Kafka 4060 remove zk client dependency in kafka streams followup","Updated the KAFKA-4060 code based on the new Admin client API.  Also we won't delete internal topics anymore.","closed","","hjafarpour","2017-01-13T22:28:38Z","2017-01-23T17:41:35Z"
"","2377","Kafka 4060 docs update","Updated the docs with changes in KAFKA-4060.","closed","","hjafarpour","2017-01-13T23:12:42Z","2017-01-23T19:37:11Z"
"","2142","Merge pull request #1 from apache/trunk","update test","closed","","liuhang93","2016-11-16T09:27:55Z","2016-11-17T06:50:26Z"
"","2060","KAFKA-4211: Update system tests to use the new consumer by default","Update system test method signatures and method calls to use the new consumer by default.","closed","","vahidhashemian","2016-10-24T23:30:08Z","2016-11-17T00:34:01Z"
"","1493","KAFKA-3818: Change Mirror Maker default assignment strategy to round robin","Update Mirror Maker to use round robin assignment by default since it gives a better balance between the instances, in particular when the number of Mirror Maker instances exceeds the typical number of partitions per topic. There doesn't seem to be any need to keep range assignment since co-partitioning is not an issue.","closed","","vahidhashemian","2016-06-10T22:46:36Z","2022-02-10T10:30:48Z"
"","1492","KAFKA-3818: Change Mirror Maker default assignment strategy to round robin","Update Mirror Maker to to use round robin assignment by default since it gives a better balance between the instances, in particular when the number of Mirror Maker instances exceeds the typical number of partitions per topic. There doesn't seem to be any need to keep range assignment since co-partitioning is not an issue.","closed","","vahidhashemian","2016-06-10T22:34:57Z","2016-06-10T22:44:51Z"
"","1924","Merge pull request #1 from apache/trunk","update from origin","closed","","wangzzu","2016-09-28T03:18:42Z","2016-09-28T03:50:37Z"
"","1923","Merge pull request #1 from apache/trunk","update from origin","closed","","wangzzu","2016-09-28T03:13:35Z","2016-09-28T03:16:46Z"
"","1832","MINOR: Document that Connect topics should use compaction","Update documentation for Kafka Connect distributed’s config.storage.topic, offset.storage.topic, and status.storage.topic configuration values to indicate that all three should refer to compacted topics.","closed","","mfenniak","2016-09-07T12:55:28Z","2016-09-08T00:35:53Z"
"","1801","MINOR: Include TopicPartition in warning when log cleaner resets dirty offset","Typically this error condition is caused by topic-level configuration issues, so it is useful to include which topic partition was reset for operator use when debugging the root cause.","closed","","dpkp","2016-08-30T16:09:11Z","2016-09-07T00:16:30Z"
"","1941","HOTFIX: turn off auto topic creation in embedded kafka cluster","Turning off auto topic creation in the EmbeddedKafkaCluster used by Streams as it can cause race conditions that lead to build hangs. Fixed the couple of tests that needed to have some topics manually created","closed","","dguy","2016-09-30T09:36:52Z","2016-09-30T11:43:18Z"
"","1837","MINOR: Update the README.md to include a note about GRADLE_USER_HOME","Trying to build the source and publish it to internal Maven repo, I ran into an issue that I explain in the mailing list discussion here https://www.mail-archive.com/dev@kafka.apache.org/msg56359.html.   The commit here updates the README.md to make a note that the GRADLE_USER_HOME environment variable plays a role in deciding which file to add the maven configs to.","closed","","jaikiran","2016-09-09T06:55:29Z","2016-09-17T22:02:06Z"
"","2181","KAFKA-4453: add request prioritization","Today all requests (client requests, broker requests, controller requests) to a broker are put into the same queue. They all have the same priority. So a backlog of requests ahead of the controller request will delay the processing of controller requests. This causes requests infront of the controller request to get processed based on stale state.  Side effects may include giving clients stale metadata, rejecting ProduceRequests and FetchRequests, and data loss (for some unofficial definition of data loss in terms of messages beyond the high watermark).  We'd like to minimize the number of requests processed based on stale state. With request prioritization, controller requests get processed before regular queued up requests, so requests can get processed with up-to-date state.  Request prioritization can happen at the network layer with the RequestChannel. The RequestChannel can categorize the request as regular or prioritized based on the request id. If the incoming request id matches that of UpdateMetadataRequest, LeaderAndIsrRequest, and StopReplicaRequest, the request can get prioritized.  One solution is to simply add a prioritized request queue to supplement the existing request queue in the RequestChannel and add request prioritization-aware logic to both the sendRequest and receiveRequest operations of RequestChannel. sendRequest puts the request into the respective queue based on whether the request is prioritized or not. receiveRequest can optimistically check the prioritized request queue and otherwise fallback to the regular request queue. One subtlety here is whether to do a timed poll on just the regular request queue or on both the prioritized request queue and regular request queue sequentially. Only applying the timed poll to the regular request queue punishes a prioritized request that arrives before a regular request but moments after the prioritized request check. Applying the timed poll to both queues sequentially results in a guaranteed latency increase on a regular request.  An alternative is to replace RequestChannel’s existing request queue with a prioritization-aware blocking queue. This approach avoids the earlier stated subtlety by allowing the timed poll to apply to either prioritized or regular requests in low-throughput scenarios while still allowing queued prioritized requests to go ahead of queued regular requests.  This patch goes with the latter approach to avoid punishing late arriving prioritized requests.","closed","","onurkaraman","2016-11-28T18:57:05Z","2019-02-21T08:07:13Z"
"","2287","MINOR: Make SenderMetrics class private with private members","to reduce accessibility of the inner class.","closed","","rajatvig","2016-12-22T00:11:29Z","2018-11-10T06:07:56Z"
"","1982","KAFKA-4262: Increase data volume in replication test","To prevent test from completing without throttling before config change takes effect, produce more messages in the test.","closed","","rajinisivaram","2016-10-06T14:45:30Z","2016-10-07T14:03:41Z"
"","1858","HOTFIX: set sourceNodes to null for selectKey","To indicate its source topic is no longer guaranteed to be partitioned on key.","closed","","guozhangwang","2016-09-15T00:37:24Z","2016-09-15T15:42:49Z"
"","2240","MINOR: Update Log Compaction Details in design.html","tiny discrepancy in docs about Log compaction behaviour","closed","","curlup","2016-12-10T21:09:04Z","2019-10-16T15:34:17Z"
"","1951","MINOR: Rename property variables to match config","ThrottledLeaderReplicationRate\* => LeaderReplicationThrottledRate* ThrottledFollowerReplicationRate\* => FollowerReplicationThrottledRate\* LeaderThrottledReplicasList\* => LeaderReplicationThrottledReplicas\*  FollowerThrottledReplicasList\* => FollowerReplicationThrottledReplicas*","closed","","benstopford","2016-10-03T12:59:07Z","2016-10-03T20:22:26Z"
"","1682","HOTFIX: non-unique state.dirs in integration tests causing build to hang","Three Streams Integration tests were using the same directory for the state.dir config. This was causing the build to hang when run in parallel mode","closed","","dguy","2016-07-29T15:49:17Z","2016-07-30T12:34:57Z"
"","2063","KAFKA-4346: Add foreachValue method to KStream","This would be the value-only counterpart to foreach, similar to mapValues.  Adding this method would allow two things: 1. remove the need to de-serialize keys when operating directly on a source topic 2. enhance readability and allow for Java 8 syntactic sugar using method references without having to wrap existing methods that only operate on the value type.     For instance, let's imagine I have an app that handles  notifications, I can now write     ```    notifications.foreachValue(pushNotification::send)    ```     instead of     ```    notifications.foreach((key, notification) -> pushNotification.send(notification))    ```","closed","","xvrl","2016-10-25T22:04:44Z","2017-04-13T20:40:38Z"
"","2370","Cluster file generator should produce valid json","This way, if the ${KAFKA_NUM_CONTAINERS} is changed in docker/run_tests.sh, the json is still valid","closed","","0x0ece","2017-01-13T17:42:51Z","2017-01-13T18:37:37Z"
"","2453","MINOR: Escape '<' and '>' symbols in quickstart streams code snippet","This was missing from [an earlier PR](https://github.com/apache/kafka/pull/2247) that escaped these symbols in another section of the doc.","closed","","vahidhashemian","2017-01-26T20:47:05Z","2017-01-26T23:05:26Z"
"","2350","MINOR: Finished exposing the broker config","This was left over from KIP-104. Thanks to @ijuma for pointing out.","closed","","enothereska","2017-01-12T10:40:41Z","2017-01-23T15:31:28Z"
"","2280","KAFKA-4554: Fix ReplicaBuffer.verifyChecksum to use iterators instead of iterables","This was changed in b58b6a1bef0 and caused the `ReplicaVerificationToolTest.test_replica_lags` system test to start failing.  I also added a unit test and a couple of other minor clean-ups.","closed","","ijuma","2016-12-20T12:31:18Z","2016-12-20T19:18:42Z"
"","1957","MINOR: Add Replication Quotas Test Rig","This test rig lives in the other.kafka package so isn't part of our standard tests. It provides a convenient mechanism for measuring throttling performance over time. Measurements for each experiment are charted and presented to the user in an html file. The output looks like this:  **Experiment4** - BrokerCount: 25 - PartitionCount: 100 - Throttle: 4,000,000 B/s - MsgCount: 1,000 - MsgSize: 100,000 - TargetBytesPerBrokerMB: 400  ![image](https://cloud.githubusercontent.com/assets/1297498/19070450/3251bc52-8a23-11e6-88fe-94de6b9147c2.png) ![image](https://cloud.githubusercontent.com/assets/1297498/19070467/4c19f38e-8a23-11e6-986a-ba19d16819ca.png)","closed","","benstopford","2016-10-04T10:12:11Z","2017-01-10T08:43:40Z"
"","2073","MINOR: Fix issue in `AsyncProducerTest` where it expects the `port` config to be set","This test fails locally when I run it, but somehow Jenkins builds are passed. Not clear how.","closed","","ijuma","2016-10-28T09:43:15Z","2016-11-01T15:42:56Z"
"","1864","KAFKA-4177: Remove ThrottledReplicationRateLimit from Server Config","This small PR pulls ThrottledReplicationRateLimit out of KafkaConfig and puts it in a class that defines Dynamic Configs. Client configs are also placed in this class and validation added.","closed","","benstopford","2016-09-16T15:24:01Z","2016-09-29T08:38:06Z"
"","2150","KAFKA-4416: Add a `--group` option to console consumer","This simplifies running a console consumer as part of a custom group. Note that group id can be provided via only one of the three possible means: directly using `--group ` option (as part of this PR), as property via `--consumer-property` option,  or inside a config file via `--consumer.config` option.","closed","","vahidhashemian","2016-11-18T20:20:34Z","2017-10-04T15:52:50Z"
"","2187","Revert ""KAFKA-4345; Run decktape test for each pull request""","This reverts commit e035fc039598127e88f31739458f705290b1fdba for the following reasons:  1. License files are missing causing local builds to fail during the rat task (rat is not being run in Jenkins for some reason, filed KAFKA-4459 for that) 2. It renames a number of system test files when there's a better way to achieve the goal of running a subset of system tests to stay under the Travis limit. 3. It adds the gradle wrapper binary even though this was removed intentionally a while back.  A new PR will be submitted for KAFKA-4345 without the undesired changes.","closed","","ijuma","2016-11-29T09:26:25Z","2016-11-29T19:14:02Z"
"","1655","KAFKA-3982: Fix processing order of some of the consumer properties","This PR updates processing of console consumer's input properties.  For both old and new consumer, the value provided for `auto.offset.reset` indirectly through `consumer.config` or `consumer.property` arguments will now take effect. For new consumer and for `key.deserializer` and `value.deserializer` properties, the precedence order is fixed to first the value directly provided as an argument, then the value provided indirectly via `consumer.property` and then `consumer.config`, and finally a default value.","closed","","vahidhashemian","2016-07-22T20:56:08Z","2017-06-02T19:48:57Z"
"","2104","[KAFKA-4380] Remove cleanshutdownfile","This PR removes the cleanshutdownfile as suggested in the code comments as a TODO.  Use of this seems to be well covered by existing tests (some of which needed to be updated).  The gradlew unit tests pass locally on my machine, but since this is my first (small) PR to Kafka I may have left something out.","closed","","holdenk","2016-11-04T20:59:03Z","2017-08-25T18:07:32Z"
"","1757","KAFKA-2170: Fixes for Windows","This PR is to revive the older PR https://github.com/apache/kafka/pull/154 made by @mpoindexter (Mike Poindexter). I have replicated his work over the new code base. This lets 6 LogTest unit tests to pass on Windows.","open","","soumyajit-sahu","2016-08-17T19:21:30Z","2022-06-14T12:38:56Z"
"","1812","KIP-74: Add fetch response size limit and implement round-robin on client side","This PR is implementation of [KIP-74](https://cwiki.apache.org/confluence/display/KAFKA/KIP-74%3A+Add+Fetch+Response+Size+Limit+in+Bytes) which is originally motivated by [KAFKA-2063](https://issues.apache.org/jira/browse/KAFKA-2063).  Your comments are greatly appreciated.","closed","","nepal","2016-09-01T17:01:52Z","2016-09-18T16:16:35Z"
"","2338","MINOR: remove unnecessary store info from TopologyBuilder","This PR is extracted from https://github.com/apache/kafka/pull/2333 as an incremental fix to ease the reviewing:  1. Removed `storeToProcessorNodeMap` from ProcessorTopology since it was previously used to set the context current record, and can now be replaced with the dirty entry in the named cache.  2. Replaced `sourceStoreToSourceTopic` from ProcessorTopology with `storeToChangelogTopic` map, which includes the corresponding changelog topic name for all stores that are changelog enabled.  3. Modified `ProcessorStateManager` to rely on `sourceStoreToSourceTopic` when retrieving the changelog topic; this makes the second parameter `loggingEnabled` in `register` not needed any more, and we can deprecate the old API with a new one.  4. Also fixed a minor issue in `KStreamBuilder`: if the storeName is not provided in the `table(..)` function, do not create the underlying materialized store. Modified the unit tests to cover this case.  5. Fixed a bunch of other unit tests failures that are exposed by this refactoring, in which we are not setting the applicationId correctly when constructing the mocking processor topology.","closed","","guozhangwang","2017-01-10T06:25:17Z","2017-07-15T22:07:31Z"
"","1937","KAFKA-4108: Improve DumpLogSegments offsets-decoder output format","This PR improves the output format of DumpLogSegments when the `--offset-decoder` option is used for consuming `__consumer_offsets`, especially when it comes to group metadata.  An example of the partial output with existing formatting:  ``` key: metadata::console-consumer-40190 payload: consumer:range:1:{consumer-1-20240b92-fbf4-44d5-bf8c-66b6d70c9948=[foo-0]} ```  An example of the same output with suggested formatting:  ``` key: {""metadata"":""console-consumer-40190""} payload: {""protocolType"":""consumer"",""protocol"":""range"",""generationId"":1,""assignment"":""{consumer-1-20240b92-fbf4-44d5-bf8c-66b6d70c9948=[foo-0]}""} ```","closed","","vahidhashemian","2016-09-29T23:00:37Z","2017-10-03T17:46:15Z"
"","1523","KAFKA-3878: Support exponential backoff policy via reconnect.backoff.max (KIP-144)","This PR implements the exponential backoff policy described in KIP-144.  Summary: - add `reconnect.backoff.max.ms` common client configuration parameter - if `reconnect.backoff.max.ms` > `reconnect.backoff.ms`, apply an exponential backoff policy - apply +/- 20% random jitter to smooth cluster reconnects","closed","","dpkp","2016-06-19T18:16:18Z","2017-05-19T13:11:09Z"
"","1830","KAFKA-4093 : Cluster Identifiers (KIP-78)","This PR implements  KIP-78:Cluster Identifiers [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-78%3A+Cluster+Id#KIP-78:ClusterId-Overview) and includes the following changes: 1. Changes to broker code    - generate cluster id and store it in Zookeeper    - update protocol to add cluster id to metadata request and response    - add ClusterResourceListener interface, ClusterResource class and ClusterMetadataListeners utility class    - send ClusterResource events to the metric reporters 2. Changes to client code    - update Cluster and Metadata code to support cluster id    - update clients for sending ClusterResource events to interceptors, (de)serializers and metric reporters 3. Integration tests for interceptors, (de)serializers and metric reporters for clients and for protocol changes and metric reporters for broker. 4. System tests for upgrading from previous versions.","closed","","arrawatia","2016-09-07T07:45:21Z","2016-09-17T07:17:07Z"
"","1726","KAFKA-4033: KIP-70: Revise Partition Assignment Semantics on New Consumer's Subscription Change","This PR changes topic subscription semantics so a change in subscription does not immediately cause a rebalance. Instead, the next poll or the next scheduled metadata refresh will update the assigned partitions.","closed","","vahidhashemian","2016-08-12T18:21:01Z","2016-09-09T02:57:52Z"
"","1989","Kafka 4180 - Shared authentification with multiple actives Kafka producers/consumers","This PR builds on top of @rajinisivaram https://github.com/apache/kafka/pull/1979 codeveloped with @mimaison   KAFKA-4180 : Authentication with multiple actives Kafka producers/consumers  Changed caching in LoginManager to allow one LoginManager per client JAAS configuration. Added test to End2EndAuthorization for SASL Plain and Gssapi with two consumers with different credentials.","closed","","edoardocomar","2016-10-07T15:31:21Z","2016-12-27T11:26:44Z"
"","1572","KAFKA-3854: Fix issues with new consumer's subsequent regex (pattern) subscriptions","This patch fixes two issues: 1. Subsequent regex subscriptions fail with the new consumer. 2. Subsequent regex subscriptions would not immediately refresh metadata to change the subscription of the new consumer and trigger a rebalance.  The final note on the JIRA stating that a later created topic that matches a consumer's subscription pattern would not be assigned to the consumer upon creation seems to be as designed. A repeat  `subscribe()` to the same pattern or some wait time until the next automatic metadata refresh would handle that case.  An integration test was also added to verify these issues are fixed with this PR.","closed","","vahidhashemian","2016-06-29T23:18:26Z","2016-07-05T21:52:54Z"
"","1499","KAFKA-3831: Prepare for updating new-consumer-based Mirror Maker's default partition assignment strategy to round robin","This patch adds proper warning message and necessary doc updates for updating the default partition assignment strategy of Mirror Maker from range to round robin. The actual switch would occur as part of a major release cycle (to be scheduled).","closed","","vahidhashemian","2016-06-13T17:14:12Z","2016-09-27T02:08:08Z"
"","2412","MINOR: reduce verbosity of cache flushes","This log message tends to be extremely verbose when state stores are being restored","closed","","xvrl","2017-01-20T18:43:31Z","2017-01-20T20:08:30Z"
"","1681","KAFKA-3590: KafkaConsumer fails with ""Messages are rejected since there are fewer in-sync replicas than required."" when polling","This just improves the error message since it's confusing that a consumer throws a message you'd normally see for a producer.","closed","","cotedm","2016-07-29T15:03:10Z","2016-09-08T13:05:17Z"
"","1921","KAFKA-3697: Update quick start documentation to use the new consumer","This is to imply that the Java consumer/producer are the recommended consumer/producer now.","closed","","vahidhashemian","2016-09-27T23:09:57Z","2016-09-30T02:38:21Z"
"","1553","KAFKA-3740: Add configs for RocksDBStore","This is the part I of the work to add the StreamsConfig to ProcessorContext.  We need to access StreamsConfig in the ProcessorContext so other components (e.g. RocksDBWindowStore or LRUCache can retrieve config parameter from application)","closed","","HenryCaiHaiying","2016-06-25T20:24:43Z","2016-06-30T18:10:47Z"
"","2330","KAFKA-4602 - KIP-72 - Allow putting a bound on memory consumed by Incoming requests","this is the initial implementation.","closed","","radai-rosenblatt","2017-01-06T22:40:08Z","2017-07-26T06:29:02Z"
"","2398","HOTFIX: KIP-104 for code freeze","This is the HOTFIX PR for any issues detected with KIP-104 until code freeze. Note: do not merge until close to code freeze.  The name changes reflect feedback received while writing the documentation.","closed","","enothereska","2017-01-18T17:01:48Z","2017-01-25T19:41:03Z"
"","1712","KAFKA-3989: initial support for adding a JMH benchmarking module.","This is related to KAFKA-3973, as the benchmarks were re-written in JMH as part of this PR. Results are posted in the KAFKA-3973 jira ticket.","closed","","bbejeck","2016-08-08T18:27:48Z","2017-03-06T13:40:07Z"
"","1752","KAFKA-3776: Unify store and downstream caching in streams","This is joint work between @dguy and @enothereska. The work implements KIP-63. Overview of main changes: - New byte-based cache that acts as a buffer for any persistent store and for forwarding changes downstream. - Forwarding record path changes: previously a record in a task completed end-to-end. Now it may be buffered in a processor node while other records complete in the task. - Cleanup and state stores and decoupling of cache from state store and forwarding. - More than 80 new unit and integration tests.","closed","","enothereska","2016-08-17T10:22:51Z","2016-09-16T17:14:43Z"
"","1735","MINOR: Add application id prefix for copartitionGroups in TopologyBuilder","This is bugfix that is already in trunk but not backported to 0.10.0.","closed","","guozhangwang","2016-08-14T07:00:50Z","2016-08-16T16:54:31Z"
"","1704","KAFKA-4018: Streams causing older slf4j-log4j library to be packaged along with newer version","This is a regression caused by 0bb1d3ae.  After that commit, Streams no longer has a direct dependency on slf4j-log4j12, but zkclient has a dependency on an older version of slf4j-log4j12, so we get a transitive dependency on the older version.  The fix is to simply exclude the undesired dependencies from the zkclient dependency.","closed","","ijuma","2016-08-03T23:59:38Z","2016-08-04T01:05:14Z"
"","2333","KAFKA-3452 Follow-up: Optimize ByteStore Scenarios","This is a refactoring follow-up of https://github.com/apache/kafka/pull/2166. Main refactoring changes:  1. Extract `InMemoryKeyValueStore` out of `InMemoryKeyValueStoreSupplier` and remove its duplicates in test package.  2. Add two abstract classes `AbstractKeyValueIterator` and `AbstractKeyValueStore` to collapse common functional logics.  3. Added specialized `BytesXXStore` to accommodate cases where key value types are Bytes / byte[] so that we can save calling the dummy serdes.  4. Make the key type in `ThreadCache` from byte[] to Bytes, as SessionStore / WindowStore's result serialized bytes are in the form of Bytes anyways, so that we can save unnecessary `Bytes.get()` and `Bytes.wrap(bytes)`.  Each of these should arguably be a separate PR and I apologize for the mess, this is because this branch was extracted from a rather large diff that has multiple refactoring mingled together and @dguy and myself have already put lots of efforts to break it down to a few separate PRs, and this is the only left-over work. Such PR won't happen in the future.  Ping @dguy @enothereska @mjsax for reviews","closed","","guozhangwang","2017-01-09T00:50:04Z","2017-07-15T22:07:22Z"
"","1579","KAFKA-3923: Make KafkaMetric not final, update JmxReporter and unit tests","This is a patch for the previously highlighted issue: https://issues.apache.org/jira/browse/KAFKA-3923  I've minimised the scope of changes, so that now Metrics class is not affected and generics are introduced on the method level instead of class level.","closed","","stepio","2016-07-01T16:15:33Z","2017-12-22T20:25:07Z"
"","1971","MINOR: Clarify 0.10.1.0 upgrade docs","This is a minor change to fix the most glaring issues. We have another JIRA to revamp the upgrade docs.","closed","","ijuma","2016-10-05T13:32:14Z","2016-10-06T16:40:05Z"
"","2360","KAFKA-3452 Follow-up: Refactoring StateStore hierarchies","This is a follow up of https://github.com/apache/kafka/pull/2166 - refactoring the store hierarchies as requested","closed","","dguy","2017-01-12T20:29:46Z","2017-01-17T22:14:46Z"
"","1539","KAFKA-3663 : Implements KIP-59 for a kafka-brokers.sh command","This implements KIP-59: Proposal for a kafka broker command. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-59%3A+Proposal+for+a+kafka+broker+command for details and sample output.","open","","JThakrar","2016-06-22T15:15:07Z","2018-06-13T08:42:36Z"
"","2252","HOTFIX: fix state transition stuck on rebalance","This fixes a problem where the Kafka instance state transition gets stuck on rebalance (Thanks to @dguy for pointing out). Also adjusts the test in QueryableStateIntegration test.","closed","","enothereska","2016-12-13T20:52:27Z","2016-12-15T17:45:02Z"
"","1582","KAFKA-3910 [WIP]: Cyclic schema support in ConnectSchema and SchemaBuilder","This feature uses a FutureSchema as a placeholder to be resolved later. Resolution is attempted whenever a ConnectSchema is constructed, it attempts to resolve all its children (fields, keySchema, or valueSchema) and recurses until the end of the tree.   A FutureSchema is resolved when it finds a parent schema that matches its name, and optional flag. If a FutureSchema is accessed before being resolved, it will throw a DataException.  The SchemaBuilder constructs a FutureSchema if a field is added with only a type name.","open","connect,","johnhofman","2016-07-04T11:50:43Z","2018-03-02T19:29:35Z"
"","1547","KAFKA-3594; After calling MemoryRecords.close() method, hasRoomFor() method should return false","This exception is occurring when producer is trying to append a record to a Re-enqueued record batch in the accumulator. We should not allow to add a record to Re-enqueued record batch. This is due a bug in MemoryRecords.java/hasRoomFor() method. After calling MemoryRecords.close() method, hasRoomFor() method should return false.  This is a backport to the 0.9.0 branch.","closed","","ijuma","2016-06-23T23:02:01Z","2016-06-24T08:57:56Z"
"","1630","MINOR: Fix typo in Operations section","This contribution is my original work, and I license the work to the project under the project's open source license.","closed","","ssaamm","2016-07-18T14:46:48Z","2016-07-19T09:20:53Z"
"","1748","KAFKA-3940 Log should check the return value of dir.mkdirs()","This commit changes all the occurrences of dir.mkdirs() with Files.createDirectory(dir.toPath())","closed","","imandhan","2016-08-17T00:30:41Z","2016-08-25T21:37:11Z"
"","2397","HOTFIX: ChangeLoggingKeyValueStore.name() returns null","This class doesn't need to override this method as it is handled appropriately by the super class","closed","","dguy","2017-01-18T15:14:14Z","2017-01-18T18:29:56Z"
"","2440","KAFKA-4699: Invoke producer callbacks before completing the future","This behaviour was changed in 8b3c6c0, but it caused interceptor test failures (which rely on callbacks) and since we’re so close to code freeze, it’s better to be conservative.","closed","","ijuma","2017-01-26T09:16:57Z","2017-09-05T09:31:01Z"
"","1776","KIP-73 - Replication Quotas","This applies to Replication Quotas  based on KIP-73 [(link)](https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas) originally motivated by KAFKA-1464.  System Tests Run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/544/  **This first PR demonstrates the approach**.   **_Overview of Change_** The guts of this change are relatively small. Throttling occurs on both leader and follower sides. A single class tracks the throttled throughput in and out of each broker (**_ReplicationQuotaManager_**).    On the follower side, the Follower Throttled Rate is calculated as fetch responses arrive. Then, before the next fetch request is sent, we check to see if the quota is violated, removing throttled partitions from the request if it is. This is all encapsulated in a few lines of code in the **_ReplicaFetcherThread_**. There is existing code to handle temporal back off, if the request ends up being empty.   On the leader side it's a little more complex. When a fetch request arrives in the leader, it is built, partition by partition, in **_ReplicaManager.readFromLocalLog_**. As we put each partition into the fetch response, we check if the total size fits in the current quota. If the quota is exceeded, the partition will not be added to the fetch response. Importantly, we don't increase the quota at this point, we just check to see if the bytes will fit.   Now, if there aren't enough bytes to send the response immediately, which is common if we're catching up and throttled, then the request will be put in purgatory. I've added some simple code to **_DelayedFetch_** to handle throttled partitions (throttled partitions are checked against the quota, rather than the messages available in the log).  When the delayed fetch completes, and exits purgatory, _**ReplicaManager.readFromLocalLog**_ will be called again. This is why _**ReplicaManager.readFromLocalLog**_ does not actually increase the quota, it just checks whether enough bytes are available for a partition.   Finally, when there are enough bytes to be sent, or the delayed fetch times out, the response will be sent. Before it is sent the throttled-outbound-rate is increased, based on the size of throttled partitions being sent. This is at the end of _**KafkaApis.handleFetchRequest**_, exactly where client quotas are recorded.   There is an acceptance test which asserts the whole throttling process stabilises on the desired value. This covers a number of use cases including many-to-many replication. See **_ReplicationQuotaTest_**.  Note:  It should be noted that this protocol can over-request. The request is built, based on the quota at time t1 (_ReplicaManager.readFromLocalLog_). The bytes in the response are recorded at time t2 (end of _KafkaApis.handleFetchRequest_), where t2 > t1. For this reason I originally included an OverRequestedRate as a JMX metric, but testing has not seen revealed any obvious issue. Over-requesting is quickly compensated by subsequent requests, stabilising close to the quota value.   _**Main stuff left to do:**_ - The fetch size is currently unbounded. This will be addressed in KIP-74, but we need to ensure this ensures requests don’t go beyond the throttle window. - There are two failures showing up in the system tests on this branch:  StreamsSmokeTest.test_streams (which looks like it fails regularly) and OffsetValidationTest.test_broker_rolling_bounce (which I need to look into)  _**Stuff left to do that could be deferred:**_  - Add the extra metrics specified in the KIP.  - There are no system tests.  - There is no validation for the cluster size / throttle combination that could lead to ISR dropouts","closed","","benstopford","2016-08-23T21:35:20Z","2016-09-16T05:29:02Z"
"","2048","Pluggable verifiable clients","This adds support for pluggable VerifiableConsumer and VerifiableProducers test client implementations allowing third-party clients to be used in-place of the Java client in kafkatests.  A new VerifiableClientMixin class is added and the standard Java Verifiable{Producer,Consumer} classes have been changed to use it.  While third-party client drivers may be implemented with a complete class based on the Mixin, a simpler alternative which requries no kafkatest class implementation is available through the VerifiableClientApp class that uses ducktape's global param to specify the client app to use (passed to ducktape through the `--globals ` command line argument).  Some existing kafkatest clients for reference: Go: https://github.com/confluentinc/confluent-kafka-go/tree/master/kafkatest Python: https://github.com/confluentinc/confluent-kafka-python/tree/master/confluent_kafka/kafkatest C++: https://github.com/edenhill/librdkafka/blob/0.9.2.x/examples/kafkatest_verifiable_client.cpp C#/.NET: https://github.com/confluentinc/confluent-kafka-dotnet/tree/master/test/Confluent.Kafka.VerifiableClient  This PR also contains documentation for the simplex JSON-based verifiable\* client protocol.  There are also some minor improvements that makes troubleshooting failing client tests easier.","closed","","edenhill","2016-10-20T20:55:19Z","2017-03-22T07:05:04Z"
"","2105","KAFKA-4322 StateRestoreCallback begin and end indication","This adds a begin and end callback to StateRestoreCallback.  The contribution is my original work and I license the work to Apache Kafka under the Kafka's open source license.","closed","","markcshelton","2016-11-04T21:01:22Z","2017-12-22T20:26:11Z"
"","1775","MINOR: Move a few methods from the `ZKUtils` class to the companion object","They don't require access to `ZkClient`.  Also include a few obvious clean-ups in `ZKUtils`: - Remove redundant rethrows and braces - Use named arguments for booleans","closed","","ijuma","2016-08-23T16:04:18Z","2016-08-26T01:39:12Z"
"","1665","MINOR: Increase default timeout for other `wait` methods in `TestUtils`","They are now consistent with `waitUntilTrue`.","closed","","ijuma","2016-07-26T07:50:56Z","2016-07-26T23:57:04Z"
"","1782","MINOR: Fix ProcessorTopologyTestDriver to support multiple source topics","There's a minor bug in ProcessorTopologyTestDriver that prevents it from working with a topology that contains multiple sources.  The bug is that `consumer.assign()` is called while looping through all the source topics, but, consumer.assign resets the state of the MockConsumer to only consume from the topics passed in.  This patch fixes the issue by calling consumer.assign once with all the TopicPartition instances.  Unit test (testDrivingSimpleMultiSourceTopology) included.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","mfenniak","2016-08-24T13:04:26Z","2016-08-26T19:52:41Z"
"","2216","MINOR: Upgrade Gradle to 3.2.1 and Scala to 2.12.1","There were a couple of important issues fixed in Gradle 3.2.1: * [GRADLE-3582] - Gradle wrapper fails to escape arguments with nested quotes * [GRADLE-3583] - Newlines in JAVA_OPTS breaks application plugin shell script in Gradle 3.2  And a lot of important issues fixed in Scala 2.12.1: * http://www.scala-lang.org/news/2.12.1","closed","","ijuma","2016-12-06T15:01:48Z","2016-12-08T23:47:08Z"
"","1985","MINOR: A bunch of clean-ups related to usage of unused variables","There should be only one cases where these clean-ups have a functional impact: replaced repeated identical logs with a single log for the stale controller epoch case.  The rest should just make the code easier to read and make it a bit less wasteful. I did this exercise because unused variables sometimes mask bugs.","closed","","ijuma","2016-10-07T02:16:06Z","2016-10-25T02:18:18Z"
"","1497","KAFKA-3802 log mtimes reset on broker restart / shutdown","There seems to be a bug in the JDK that on some versions the mtime of the file is modified on FileChannel.truncate() even if the javadoc states `If the given size is greater than or equal to the file's current size then  the file is not modified.`.  This causes problems with log retention, as all the files then look like they contain recent data to Kafka. Therefore this is only done if the channel size is different to the target size.","closed","","msiuts","2016-06-11T20:34:56Z","2016-07-06T15:13:39Z"
"","2242","KAFKA-4497: Fix the ByteBufferMessageSet.filterInto","There seems a few bugs in `ByteBufferMessageSet.filterInto`, including incorrect offsets, messages read and retained. This patch fixes them and added unit test.","closed","","becketqin","2016-12-12T01:15:05Z","2016-12-13T20:58:08Z"
"","2318","KAFKA-4597; RecordMetadata should return log append time when appropriate","There is a slight change of behaviour: we now complete the `Future` returned from `send` before the callbacks are invoked. This seems OK and perhaps a little better as the `Future` can make progress sooner (as it would typically be blocked on a different thread than the I/O thread that invokes the callbacks).  Edit: https://github.com/apache/kafka/pull/2440 reverts the slight change in behaviour for reasons explained there.","closed","","ijuma","2017-01-05T15:37:28Z","2017-09-05T09:33:56Z"
"","1910","KAFKA-4214:kafka-reassign-partitions fails all the time when brokers are bounced during reassignment","There is a corner case bug, where during partition reassignment, if the controller and a broker receiving a new replica are bounced at the same time, the partition reassignment is failed.  The cause of this bug is a block of code in the KafkaController which fails the reassignment if the aliveNewReplicas != newReplicas, ie. if some of the new replicas are offline at the time a controller fails over.  The fix is to have the controller listen for ISR change events even for new replicas which are not alive when the controller boots up. Once the said replicas come online, they will be in the ISR set, and the new controller will detect this, and then mark the reassignment as successful.  Interestingly, the block of code in question was introduced in KAFKA-990, where a concern about this exact scenario was raised :)  This bug was revealed in the system tests in https://github.com/apache/kafka/pull/1904.  The relevant tests will be enabled in either this or a followup PR when PR-1904 is merged.  Thanks to @junrao identifying the issue and providing the patch.","closed","","apurvam","2016-09-26T21:25:59Z","2016-09-27T00:18:53Z"
"","2036","MINOR: Update rocksDB dependency to 4.11.2","There are 32 failing tests on both trunk and my branch.","closed","","jozanek","2016-10-18T12:29:42Z","2016-10-20T06:44:29Z"
"","1949","fix a wrong word","the wrong word in this sentence "" allows us to bring machies up in parallel on AWS."" . “machies” change to ""machines"".","closed","","yangwei71","2016-10-03T12:32:12Z","2016-10-03T16:55:03Z"
"","1833","MINOR: fix transient QueryableStateIntegration test failure","The verification in verifyGreaterOrEqual was incorrect. It was failing when a new key was found. Set the TimeWindow to a large value so all windowed results fall in a single window","closed","","dguy","2016-09-07T15:58:55Z","2016-09-08T17:54:24Z"
"","1583","MINOR: fix generics in KStream.groupBy(...)","The type param `V1` in the `KStream.groupBy(...)` methods is not needed.  @guozhangwang","closed","","dguy","2016-07-04T14:36:51Z","2016-07-04T17:41:06Z"
"","2444","KAFKA-4645: Improve test coverage of ProcessorTopology","the toString method prints the topology, but had no tests making sure it works and/or doesn't cause exceptions","closed","","dguy","2017-01-26T14:33:38Z","2017-02-17T00:58:27Z"
"","1688","MINOR: Remove unnecessary synchronized block in org.apache.kafka.streams.processor.internals.StreamTask","The StreamTask is owned by a specific thread, so it doesn't seem necessary to synchronized the processing of the records as discussed with @guozhangwang  on the dev mailing list","closed","","PierreCoquentin","2016-07-30T09:16:47Z","2016-08-02T21:31:53Z"
"","2405","KAFKA-3896: Fix KStreamRepartitionJoinTest","The root cause of this issue is that in InternalTopicManager we are creating topics one-at-a-time, and for this test, there are 31 topics to be created, as a result it is possible that the consumer could time out during the assignment in rebalance, and the next leader has to do the same again because of ""makeReady"" calls are one-at-a-time.  This patch batches the topics into a single create request and also use the StreamsKafkaClient directly to fetch metadata for validating the created topics. Also optimized a bunch of inefficient code in InternalTopicManager and StreamsKafkaClient.  Minor cleanup: make the exception message more informative in integration tests.","closed","","guozhangwang","2017-01-19T04:43:22Z","2017-07-15T22:07:28Z"
"","2359","KAFKA-3452: follow up - fix state store restoration for session and window stores","The refactoring of the stores in https://github.com/apache/kafka/pull/2166 introduced `ChangeLoggingSegmentedBytesStore`, thus removing the change logging from the `RocksDBWindowStore` etc. However, the inner most store still needs to know whether or not logging is enabled such that it can register correctly with `ProcessorContext` and enable StateStore restoration","closed","","dguy","2017-01-12T18:04:42Z","2017-05-16T14:05:28Z"
"","1892","KAFKA-4197: Make ReassignPartitionsTest System Test move data","The ReassignPartitionsTest system tests doesn't reassign any replicas (i.e. move data).   This is a simple issue. It uses a 3 node cluster with replication factor of 3, so whilst the replicas are jumbled around, nothing actually is moved from machine to machine when the assignment is executed.   This fix just ups the number of nodes to 4 so things move.   Tests pass locally.  There are runs pending on the two branch builders  Passes: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/551/ https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/94/ https://jenkins.confluent.io/job/system-test-kafka-branch-builder/553/ https://jenkins.confluent.io/job/system-test-kafka-branch-builder/554/ https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/95  Failures: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/552 => _RuntimeError: There aren't enough available nodes to satisfy the resource request. Total cluster size: 1, Requested: 4, Already allocated: 1, Available: 0._ Which I assume to do with the test env.","closed","","benstopford","2016-09-20T12:50:57Z","2016-09-20T14:41:57Z"
"","2290","MINOR: Rephrase Javadoc summary for ConsumerRecord","The original Javadoc description for `ConsumerRecord` is slightly confusing in that it can be read in a way such that an object is a key value pair received from Kafka, but (only) consists of the metadata associated with the record. This PR makes it clearer that the metadata is _included_ with the record, and moves the comma so that the phrase ""topic name and partition number"" in the sentence is more closely associated with the phrase ""from which the record is being received"".","closed","","LoneRifle","2016-12-23T01:34:47Z","2016-12-30T05:13:21Z"
"","1899","HOTFIX: Decrease commit interval","The original commit interval of 30 seconds might be too large in some cases, e.g., when the verifier finishes before those 30 seconds have elapsed.","closed","","enothereska","2016-09-22T09:02:45Z","2016-09-22T19:46:25Z"
"","2226","KAFKA-4492: java.lang.IllegalStateException: Attempting to put a clean entry for key... into NamedCache","The NamedCache wasn't correctly dealing with its re-entrant nature. This would result in the LRU becoming corrupted, and the above exception occurring during eviction. For example: Cache A: dirty key 1 eviction runs on Cache A Node for key 1 gets marked as clean Entry for key 1 gets flushed downstream Downstream there is a processor that also refers to the table fronted by Cache A Downstream processor puts key 2 into Cache A This triggers an eviction of key 1 again ( it is still the oldest node as hasn't been removed from the LRU) As the Node for key 1 is clean flush doesn't run and it is immediately removed from the cache.  So now we have dirtyKey set with key =1, but the value doesn't exist in the cache. Downstream processor tries to put key = 1 into the cache, it fails as key =1 is in the dirtyKeySet.","closed","","dguy","2016-12-07T17:29:50Z","2016-12-07T20:15:19Z"
"","1481","MINOR: Pass absolute directory path to RocksDB.open","The method `RocksDB.open` assumes an absolute file path. If a relative path is configured, it leads to an exception like the following:  ``` org.apache.kafka.streams.errors.ProcessorStateException: Error opening store CustomerIdToUserIdLookup at location ./tmp/rocksdb/CustomerIdToUserIdLookup     at org.rocksdb.RocksDB.open(Native Method)     at org.rocksdb.RocksDB.open(RocksDB.java:183)     at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:214)     at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:165)     at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:170)     at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init(MeteredKeyValueStore.java:85)     at org.apache.kafka.test.KStreamTestDriver.(KStreamTestDriver.java:64)     at org.apache.kafka.test.KStreamTestDriver.(KStreamTestDriver.java:50)     at com.simple.estuary.transform.streaming.CartesianTransactionEnrichmentJobTest.testBuilder(CartesianTransactionEnrichmentJobTest.java:41) ```  Is there any risk to always fetching the absolute path as proposed here?  Let me know if you think this requires a JIRA issue or a unit test. I started working on a unit test, but don't know of a great solution for writing out a file to a relative directory.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","jklukas","2016-06-08T19:51:25Z","2016-06-23T21:37:09Z"
"","1781","KAFKA-4082; Support usage of Gradle 3.0 for bootstrapping gradlew in 0.10.0","The main requirement is to remove the usage of `useAnt` and we need to upgrade scoverage because the older version refers to `useAnt`.","closed","","ijuma","2016-08-24T10:40:18Z","2016-08-24T20:35:13Z"
"","1879","HOTFIX: logic in QuerybaleStateIntegrationTest.shouldBeAbleToQueryState incorrect","The logic in `verifyCanGetByKey` was incorrect. It was   ``` windowState.size() < keys.length && countState.size() < keys.length && System.currentTimeMillis() < timeout ```  but should be:  ``` (windowState.size() < keys.length || countState.size() < keys.length) && System.currentTimeMillis() < timeout ```","closed","","dguy","2016-09-19T08:56:04Z","2016-09-19T17:29:39Z"
"","2261","MINOR: Replace deepIterator/shallowIterator with deepEntries/shallowEntries","The latter return `Iterable` instead of `Iterator` so that enhanced foreach can be used in Java.","closed","","ijuma","2016-12-15T16:24:28Z","2016-12-16T18:57:31Z"
"","1690","MINOR: Use `close()` instead of `dispose()` in various RocksDB classes","The latter has been deprecated.","closed","","ijuma","2016-07-31T22:16:15Z","2016-08-02T21:34:26Z"
"","1664","KAFKA-1911: Async delete topic","The last patch submitted by @MayureshGharat (back in Dec 15) has been rebased to the latest trunk. I took care of a couple of test failures (MetricsTest) along the way. @jjkoshy , @granders , @avianey , you may be interested in this PR.","closed","","sutambe","2016-07-26T00:23:04Z","2016-11-30T18:41:34Z"
"","1693","HOTFIX: start embedded kafka in KafkaStreamsTest to avoid hanging","The KafkaStreamsTest can occasionally hang if the test doesn't run fast enough. This is due to there being no brokers available on the broker.urls provided to the StreamsConfig. The KafkaConsumer does a poll and blocks causing the test to never complete.","closed","","dguy","2016-08-02T11:40:18Z","2016-08-02T19:49:40Z"
"","1930","KAFKA-4228 - make producer close on sender thread death, make consumer shutdown on failure to rebalance, and make MM die on any of the above.","the JIRA issue (https://issues.apache.org/jira/browse/KAFKA-4228) details a cascade of failures that resulted in an entire mirror maker cluster stalling due to an OOM death on one mm instance.  this patch makes producers and consumers close themselves on the errors encountered, and mm to shut down if anything happens to producers or consumers.  Signed-off-by: radai-rosenblatt radai.rosenblatt@gmail.com","closed","","radai-rosenblatt","2016-09-29T00:27:40Z","2018-04-18T01:50:55Z"
"","1716","KAFKA-3123: Make abortAndPauseCleaning() a no-op when state is already LogCleaningPaused","The function of LogCleanerManager.abortAndPauseCleaning() is to mark a TopicAndPartition as LogCleaningPaused. Hence, it should be a no-op when the TopicAndPartition is already in that state.","closed","","soumyajit-sahu","2016-08-10T00:52:30Z","2017-06-14T18:42:54Z"
"","1544","MINOR: Clean `Metrics.defaultRegistry` to avoid transient failures in `testSessionExpireListenerMetrics`","The failure could manifest itself if the default metrics registry had some entries from other tests:  `java.lang.AssertionError: Unexpected meter count expected:<0> but was:<3>`  I also removed an unused variable and improved the error message to include the metric name.","closed","","ijuma","2016-06-23T08:15:14Z","2016-06-23T22:05:41Z"
"","2070","KAFKA-4349: Handle 'PreparingRebalance' and 'AwaitingSync' states in consumer group describe","The edge case where consumer group state is `PreparingRebalance` or `AwaitingSync` will be separately handled as the group assignment is not yet determined.","closed","","vahidhashemian","2016-10-26T20:19:12Z","2016-10-28T01:58:49Z"
"","2344","MINOR:  add file append to connect-log4j.properties","The default log4j for kafka connect should include a log to file. (It would be nice)","closed","connect,","berryma4","2017-01-11T14:26:42Z","2020-10-16T06:08:12Z"
"","1868","MINOR: Improve output format of `kafka_reassign_partitions.sh` tool","The current output for the `--generate` option looks like this  ``` Current partition replica assignment  {""version"":1,""partitions"":[{""topic"":""t1"",""partition"":0,""replicas"":[0]}]} Proposed partition reassignment configuration  {""version"":1,""partitions"":[{""topic"":""t1"",""partition"":0,""replicas"":[1]}]} ```  This PR simply changes it to  ``` Current partition replica assignment {""version"":1,""partitions"":[{""topic"":""t1"",""partition"":0,""replicas"":[0]}]}  Proposed partition reassignment configuration {""version"":1,""partitions"":[{""topic"":""t1"",""partition"":0,""replicas"":[1]}]} ```  to make it more readable.","closed","","vahidhashemian","2016-09-16T20:25:11Z","2016-12-22T00:26:15Z"
"","1643","KAFKA-3915; Don't convert messages from v0 to v1 during log compaction","The conversion is unsafe as the converted message size may be greater than the message size limit. Updated `LogCleanerIntegrationTest` to test the max message size case for both V0 and the current version.  Also include a few minor clean-ups: - Remove unused code branch in `LogCleaner.compressMessages` - Avoid unintentional usage of `scala.collection.immutable.Stream` (`toSeq` on an `Iterator`) - Add explicit result type in `FileMessageSet.iterator`","closed","","ijuma","2016-07-20T13:38:57Z","2016-07-20T20:00:37Z"
"","1556","KAFKA-3902","The contribution is my original work and that I license the work to the project under the project's open source license.  Contributors: Guozhang Wang, Phil Derome @guozhangwang   Added checkEmpty to validate processor does nothing  and added a inhibit check for filter to fix issue.","closed","","phderome","2016-06-26T12:33:09Z","2016-07-02T00:25:35Z"
"","1846","KAFKA-4153: Fix incorrect KStream-KStream join behavior with asymmetric time window","The contribution is my original work and I license the work to the project under the project's open source license.  @guozhangwang","closed","","eliaslevy","2016-09-12T22:45:22Z","2016-09-20T00:14:31Z"
"","2129","KAFKA-4409; Fix deadlock between topic event handling and shutdown in…","The consumer can deadlock on shutdown if a topic event fires during shutdown. The shutdown acquires the rebalance lock and then the topic-event-watcher lock. The topic event watcher acquires these in the reverse order. Shutdown should not need to acquire the topic-event-watcher’s lock - all it does is unsubscribes from topic events.","closed","","jjkoshy","2016-11-14T17:50:49Z","2016-11-15T00:47:08Z"
"","1825","MINOR: Reduce the log level when the peer isn't authenticated but is using SSL","The commit here changes the log level of a log message from WARN to DEBUG. As noted in the mail discussion here https://www.mail-archive.com/dev@kafka.apache.org/msg56035.html, in a pretty straightforward/typical and valid setup, the broker logs get flooded with the following message:  ```            [2016-09-02 08:07:13,773] WARN SSL peer is not authenticated, returning ANONYMOUS instead (org.apache.kafka.common.network.SslTransportLayer)     [2016-09-02 08:07:15,710] WARN SSL peer is not authenticated, returning ANONYMOUS instead (org.apache.kafka.common.network.SslTransportLayer)     [2016-09-02 08:07:15,711] WARN SSL peer is not authenticated, returning ANONYMOUS instead (org.apache.kafka.common.network.SslTransportLayer)     [2016-09-02 08:07:15,711] WARN SSL peer is not authenticated, returning ANONYMOUS instead (org.apache.kafka.common.network.SslTransportLayer)     [2016-09-02 08:07:15,712] WARN SSL peer is not authenticated, returning ANONYMOUS instead (org.apache.kafka.common.network.SslTransportLayer)      .... ```  and it goes on forever.   Personally, I would like to remove this log message altogether for two reasons: - It's a valid case for the peer to be not authenticated but still using SSL and the code rightly handles it to return anonymous principal - The fact that this method gets called way too frequently and irrespective of what log level it gets logged at, it will end up flooding the log if that level is enabled.  Having said that I don't know if there will be an agreement on removing this log message altogether, hence just lowering the level for now.","closed","","jaikiran","2016-09-06T05:31:13Z","2016-09-07T08:09:32Z"
"","2243","KAFKA-4497: LogCleaner appended the wrong offset to time index. Backport the fix to 0.10.1.","The code in trunk has been changed, so we need to provide a different fix for this problem in 0.10.1.   Some explanations. I was able to reproduce the initial issue with a unit test. I did not include the unit test here because the tests requires a lot of hard coded constants to precisely control the message size after compression, read buffer and which message is filtered out. In trunk I have added a unit test to `ByteBufferMessageSet.fliterInto()` which holds the current buggy logic. Given no one will be developing on 0.10.1 branch anymore except bug fixes, I think it is fine to leave the ugly unit test out of this patch.","closed","","becketqin","2016-12-12T01:32:55Z","2016-12-13T17:47:21Z"
"","2103","KAFKA-4379: Remove caching of dirty and removed keys from StoreChangeLogger","The `StoreChangeLogger` currently keeps a cache of dirty and removed keys and will batch the changelog records such that we don't send a record for each update. However, with KIP-63 this is unnecessary as the batching and de-duping is done by the caching layer. Further, the `StoreChangeLogger` relies on `context.timestamp()` which is likely to be incorrect when caching is enabled","closed","","dguy","2016-11-04T12:49:29Z","2016-11-11T18:21:40Z"
"","2249","KAFKA-4473: RecordCollector should handle retriable exceptions more strictly","The `RecordCollectorImpl` currently drops messages on the floor if an exception is non-null in the producer callback. This will result in message loss and violates at-least-once processing. Rather than just log an error in the callback, save the exception in a field. On subsequent calls to `send`, `flush`, `close`, first check for the existence of an exception and throw a `StreamsException` if it is non-null. Also, in the callback, if an exception has already occurred, the `offsets` map should not be updated.","closed","","dguy","2016-12-13T10:10:35Z","2016-12-20T18:36:47Z"
"","1495","MINOR: Respect the default value of partition argument in SimpleConsumerShell","The `partition` argument is not marked as required, and has a default of `0`, according to the tool's help message. However, if `partition` is not provided the command returns with `Missing required argument ""[partition]""`. This patch is to fix the required arguments of the tool by removing `partition` from them.","closed","","vahidhashemian","2016-06-10T23:57:30Z","2016-06-12T00:51:08Z"
"","1584","MINOR: fix generics in KStream.groupBy(...)","The `KStream.groupBy(..)` calls don't change the value, only the key, so they don't need the type param `V1` as the new stream will always be of type `KStream`. The `Serde` in the overloaded `groupBy` should have a type param of  `V` to match the returned `KStream`","closed","","dguy","2016-07-04T16:57:58Z","2016-07-05T16:36:47Z"
"","1871","KAFKA-4183 Corrected Kafka Connect's JSON Converter to properly convert from null to logical values","The `JsonConverter` class has `LogicalTypeConverter` implementations for Date, Time, Timestamp, and Decimal, but these implementations fail when the input literal value (deserialized from the message) is null.   Test cases were added to check for these cases, and these failed before the `LogicalTypeConverter` implementations were fixed to consider whether the schema has a default value or is optional, similarly to how the `JsonToConnectTypeConverter` implementations do this. Once the fixes were made, the new tests pass.","closed","","rhauch","2016-09-16T22:43:42Z","2018-02-25T21:23:40Z"
"","1867","KAFKA-4183 Corrected Kafka Connect's JSON Converter to properly convert from null to logical values","The `JsonConverter` class has `LogicalTypeConverter` implementations for Date, Time, Timestamp, and Decimal, but these implementations fail when the input literal value (deserialized from the message) is null.   Test cases were added to check for these cases, and these failed before the `LogicalTypeConverter` implementations were fixed to consider whether the schema has a default value or is optional, similarly to how the `JsonToConnectTypeConverter` implementations do this. Once the fixes were made, the new tests pass.","closed","connect,","rhauch","2016-09-16T19:14:10Z","2020-10-16T06:29:08Z"
"","1618","KAFKA-3945: Change the type of 'acks' config in console producer to String","The `acks` config that is provided to console producer with `request-required-acks` comes with `all`, `-1`, `0`, `1` as valid options (`all` and `-1` being interchangeable). Currently, the console producer expects an integer for this input and that makes `all` to become an invalid input. This PR fixes this issue by changing the input type to String.","closed","","vahidhashemian","2016-07-12T23:15:13Z","2016-08-03T09:56:48Z"
"","1749","KAFKA-4052: Allow passing properties file to ProducerPerformance","Tested by running on a kerberos enabled Kafka cluster.","closed","","SinghAsDev","2016-08-17T01:27:49Z","2016-08-26T02:20:27Z"
"","1991","KAFKA-4265: Run replication quotas test with producer acks=1","Test expects all records to be published successfully, which cannot be guaranteed with acks=0 since failures are not retried.","closed","","rajinisivaram","2016-10-07T17:59:30Z","2016-10-08T15:11:45Z"
"","1927","KAFKA-4227: Shutdown AdminManager when KafkaServer is shutdown","Terminate topic purgatory thread in AdminManager during server shutdown to avoid threads being left around in unit tests.","closed","","rajinisivaram","2016-09-28T18:15:49Z","2016-09-28T19:55:32Z"
"","2343","[WIP] Client Compatibility","Temporary PR to run tests in Jenkins, this will be merged to #2264 once @cmccabe comes online.","closed","","ijuma","2017-01-11T13:28:57Z","2017-09-05T09:45:26Z"
"","1944","KAFKA-4234: Temporarily disable committing offsets in consumer's `unsubscribe()`","Temporarily disable the offset commit (when auto commit is enabled) in the new consumer's `unsubscribe()` method towards a workaround for the issue reported in [KAFKA-3491](https://issues.apache.org/jira/browse/KAFKA-3491). For now, a call to `unsubscribe()` can be made to reset the offsets in case processing the batch received from the most recent `poll()` is interrupted (due to some exception).","closed","","vahidhashemian","2016-09-30T18:44:52Z","2016-09-30T20:29:43Z"
"","1856","KAFKA-3719: Allow underscores in hostname","Technically this does not strictly adhere to RFC-952 however it is valid for domain names, urls and uris so we should loosen the requirements a tad.","closed","","rnpridgeon","2016-09-14T19:54:54Z","2016-09-23T21:25:00Z"
"","2255","KAFKA-4539: StreamThread is not correctly creating  StandbyTasks","Tasks that don't have any `StateStore`s wont have a `StandbyTask`, so `createStandbyTask` can return `null`. We need to check for this in `StandbyTaskCreator.createTask(...)`  Also, the checkpointed offsets for `StandbyTask`s are never loaded.","closed","","dguy","2016-12-14T13:10:46Z","2016-12-16T01:24:42Z"
"","2323","KAFKA-4580: Use sasl.jaas.config for some system tests","Switched console_consumer, verifiable_consumer and verifiable_producer to use new sasl.jaas_config property instead of static JAAS configuration file when used with SASL_PLAINTEXT.","closed","","rajinisivaram","2017-01-06T10:14:55Z","2017-01-17T19:03:00Z"
"","2072","Kafka-4351: Topic regex behavioral change with MirrorMaker new consumer","Support CSV values in the regexp for MirrorMaker new consumer as OldConsumer does.","closed","","huxihx","2016-10-28T06:02:47Z","2017-01-03T11:09:24Z"
"","1862","KAFKA-4175: Can't have StandbyTasks in KafkaStreams where NUM_STREAM_THREADS_CONFIG > 1","standby tasks should be assigned per consumer not per process","closed","","dguy","2016-09-15T16:22:01Z","2016-09-19T17:31:40Z"
"","1906","KAFKA-4216: Control Leader & Follower Throttled Replicas Separately","Splits the throttled replica configuration (the list of which replicas should be throttled for each topic) into two. One for the leader throttle, one for the follower throttle.  So:  quota.replication.throttled.replicas =>  quota.leader.replication.throttled.replicas & quota.follower.replication.throttled.replicas","closed","","benstopford","2016-09-26T15:47:44Z","2016-09-29T07:59:10Z"
"","2238","MINOR: Sync up 'kafka-run-class.bat' with 'kafka-run-class.sh'","Some of the recent changes to `kafka-run-class.sh` have not been applied to `kafka-run-class.bat`. These recent changes include setting proper streams or connect classpaths. So any streams or connect use case that leverages `kafka-run-class.bat` would fail with an error like ``` Error: Could not find or load main class org.apache.kafka.streams.??? ```","closed","","vahidhashemian","2016-12-09T22:06:19Z","2016-12-29T17:02:32Z"
"","2448","KAFKA-4644: Improve test coverage of StreamsPartitionAssignor","Some exception paths not previously covered. Extracted `ensureCopartitioning` into a static class.","closed","","dguy","2017-01-26T16:28:58Z","2017-02-17T00:58:19Z"
"","2015","Small refactoring to improve readability and reduce method complexity","Small method extraction to reduce complexity and improve readability of the assign method.   Private methods were created for: - Get the assignment suppliers based on the subscriptions (decoding of subscriptions info is also done at this point, where it is used) - Calculate the number of partitions for internal topic - Add tasks to state change log topic subscribers  These methods are being called inside the assign method.","closed","","picadoh","2016-10-12T22:23:47Z","2016-10-24T21:21:46Z"
"","2159","KAFKA-4273 - Add TTL support for RocksDB","Since Streams DSL doesn't support fine grained configurations of state stores (it uses only RocksDB) - I have added  new StreamsConfig called `rocksdb.ttl.sec` - which allows you to set TTL for all state stores used by the topology. To make it short, if you set property to a value `>=1`, it will use TtlDB instead of RocksDB and this will lead to records getting expired after this defined period.  This should help users to bound their disk usage and provide a configuration for use cases where your data has natural TTL/retention. For example, when you process data only for one hour, and after that you don't need the data in state stores anymore.  I have added a [test](https://github.com/apache/kafka/compare/trunk...dpoldrugo:KAFKA-4273-ttl-support?expand=1#diff-d908a80c770d196ac823752da3b3a864R117) to check if TtlDB is expiring records, but I can't make TtlDB expire records within a reasonable window (1 minute). So I have **@Ignore**d the test. Do you have any suggestions how to force TtlDB to expire records more quickly?  Since I'm using Kafka and Kafka Streams 0.10.1.0, I have also added this code to the [0.10.1](https://github.com/dpoldrugo/kafka/tree/0.10.1-KAFKA-4273-ttl-support) branch, and if the review goes well I hope it can be added to the 0.10.1.1 release. The patch is here: [KAFKA_4273_Add_TTL_support_for_RocksDB_v1.patch.txt](https://github.com/apache/kafka/files/607665/KAFKA_4273_Add_TTL_support_for_RocksDB_v1.patch.txt)   **Suggestion for future work** Since this config/feature applies to all state stores, it would be nice to provide an API for users to configure TTL for every state store individually, for example during toplogy building with KStreamBuilder.  Now: KStreamBuilder#table(String topic, final String storeName) Suggestion: KStreamBuilder#table(String topic, final String storeName, **_StoreOptions_ storeOptions**) Where **_StoreOptions_** would be something like this: `{ ttlSeconds: int }`  More details: [KAFKA-4273](https://issues.apache.org/jira/browse/KAFKA-4273)  @guozhangwang @dguy @mjsax @norwood @enothereska @ijuma    - could you check this out?","open","kip,","dpoldrugo","2016-11-22T21:52:00Z","2020-06-12T23:04:36Z"
"","1722","remove incomplete gradle wrapper infrastructure","Since jar files (including gradle-wrapper.jar) have to be excluded from source releases current gradle wrapper infrastructure was incomplete rendering it unusable and confusing (I expected gradlew script to work without having to run default gradle task to download gradle-wrapper.jar).","closed","","kamilszymanski","2016-08-12T07:42:21Z","2016-12-20T10:55:12Z"
"","1608","MINOR: Set additivity false for kafkaAppender (log4j.properties)","Since current additivity setting of `kafkaAppender` is `true` (default value), This causes duplicated logs in the `kafkaAppender` and the `root` appender.   It would be better default log4j configuration for **production env** if the additivity value of `kafkaAppender` is `false`","open","","1ambda","2016-07-11T14:55:19Z","2018-03-02T19:29:36Z"
"","2045","Fix for kafka-4295: kafka-console-consumer.sh does not delete the temporary group in zookeeper","Since consumer stop logic and zk node removal code are in separate threads, so when two threads execute in an interleaving manner, persistent node '/consumers/' might not be removed for those console consumer groups which do not specify ""group.id"". This will pollute Zookeeper with lots of inactive console consumer offset information.","closed","","huxihx","2016-10-20T09:08:22Z","2016-10-22T02:10:30Z"
"","2054","kafka-4295: ConsoleConsumer does not delete the temporary group in zookeeper","Since consumer stop logic and zk node removal code are in separate threads, so when two threads execute in an interleaving manner, persistent node '/consumers/' might not be removed for those console consumer groups which do not specify ""group.id"". This will pollute Zookeeper with lots of inactive console consumer offset information.","closed","","huxihx","2016-10-22T12:44:45Z","2017-08-04T12:57:54Z"
"","1592","Update design.html","Since 0.9.0.1 Configuration parameter log.cleaner.enable is now true by default.","closed","","nihed","2016-07-06T15:50:07Z","2016-07-09T01:08:10Z"
"","2131","Remove failing ConnectDistributedTest.test_bad_connector_class","Since #1911 was merged it is hard to externally test a connector transitioning to FAILED state due to an initialization failure, which is what this test was attempting to verify.  The unit test added in #1778 already exercises exception-handling around Connector instantiation.","closed","","shikhar","2016-11-15T00:59:39Z","2016-11-15T01:10:50Z"
"","1829","KAFKA-4134: log ConnectException at WARN","Simply log the connection refused instance.  If we're worried about spamming users, I can add a flag to make sure we only log this exception once, but the initial change is to simply log what we're given.  @ijuma looks like you were last to touch this code, would you mind having a look?","closed","connect,","cotedm","2016-09-06T19:18:25Z","2019-12-20T19:40:21Z"
"","1896","KAFKA-4200: Fix throttle argument in kafka-reassign-partitions.sh","Simple jira which alters two things:  1. kafka-reassign-partitions --verify prints Throttle was removed regardless of whether a throttle was applied. It should only print this if the value was actually changed. 2. --verify should exception if the —throttle argument. (check generate too)  To test this I extracted all validation logic into a separate method and added a test which covers the majority of combinations. The validation logic was retained as is, other than implementing (2) and adding validation to the --broker-list option which you can currently apply to any of hte main actions (where it is ignored). Requirement 1 was tested manually (as it's just println).   Testing: - Build passes locally.  - System test reassign_partitions_test.py also passes.","closed","","benstopford","2016-09-21T12:37:28Z","2016-09-27T13:34:36Z"
"","2006","KAFKA-4289 - moved short-lived loggers to companion objects","Signed-off-by: radai-rosenblatt radai.rosenblatt@gmail.com","closed","","radai-rosenblatt","2016-10-11T00:21:50Z","2016-10-12T12:11:05Z"
"","1961","KAFKA-4250: make ProducerRecord and ConsumerRecord extensible","Signed-off-by: radai-rosenblatt radai.rosenblatt@gmail.com","closed","","radai-rosenblatt","2016-10-04T17:37:43Z","2016-10-19T23:35:59Z"
"","1861","HOTFIX: fix KafkaStreams SmokeTest","Set the NUM_STREAM_THREADS_CONFIG = 1 in SmokeTestClient as we get locking issues when we have NUM_STREAM_THREADS_CONFIG > 1 and we have Standby Tasks, i.e., replicas. This is because the Standby Tasks can be assigned to the same KafkaStreams instance as the active task, hence the directory is locked","closed","","dguy","2016-09-15T14:29:01Z","2016-09-15T15:58:54Z"
"","1797","KAFKA-4062: Require --print-data-log if --offsets-decoder is enabled for DumpLogOffsets","set print-data-log option when offset-decoder is set.  @hachikuji we had talked about this one before, does this change look ok to you?","closed","","cotedm","2016-08-29T18:11:45Z","2016-08-31T04:35:48Z"
"","1654","MINOR: Update MirrorMaker docs to remove multiple --consumer.config options","See: - https://issues.apache.org/jira/browse/KAFKA-1650 - https://mail-archives.apache.org/mod_mbox/kafka-users/201512.mbox/%3CCAHwHRrUeTq_-EHXiUXdrbgHcRt-0E_t0+5kOYaF9Qy4aNVqYkA@mail.gmail.com%3E","closed","","ottomata","2016-07-22T19:07:43Z","2016-08-26T03:32:43Z"
"","1486","KAFKA-3753: Add approximateNumEntries() method to KeyValueStore interface","See https://issues.apache.org/jira/browse/KAFKA-3753  This contribution is my original work and I license the work to the project under the project's open source license.  cc @guozhangwang @kichristensen @ijuma","closed","","jklukas","2016-06-09T15:12:55Z","2016-06-16T00:09:10Z"
"","1928","KAFKA-4225: Replication Quotas: Control Leader & Follower Limit Separately","See final commit.   (depends on https://github.com/apache/kafka/pull/1906)","closed","","benstopford","2016-09-28T20:02:10Z","2016-09-29T08:25:38Z"
"","1766","KAFKA-4058: Failure in org.apache.kafka.streams.integration.ResetIntegrationTest.testReprocessingFromScratchAfterReset","see #1765","closed","","mjsax","2016-08-19T16:11:13Z","2016-08-20T19:08:22Z"
"","2355","KAFKA-4590: SASL/SCRAM system tests","Runs sanity test and one replication test using SASL/SCRAM.","closed","","rajinisivaram","2017-01-12T14:14:25Z","2017-01-17T12:57:13Z"
"","1902","KAFKA-4209: Reduce run time for quota integration tests","Run quota tests which expect throttling only until the first produce/consume request is throttled.","closed","","rajinisivaram","2016-09-23T11:09:10Z","2016-09-29T00:19:33Z"
"","1783","MINOR: Update rocksDB dependency to 4.9.0","rocksdbjni version 4.9.0 now includes support for running on Windows; this PR updates Kafka Stream's dependency to that version.  Tests pass locally, except for a timeout in testReprocessingFromScratchAfterReset that doesn't seem related; it happens with and without this change.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","mfenniak","2016-08-24T22:28:03Z","2016-08-25T03:25:48Z"
"","1986","HOTFIX: move restoreConsumer.assign() to shutdownTasksAndState","restoreConsumer.assign(..) in removeStandbyTasks was logging an (ignorable) exception due to the restoreConsumer being closed. Moved the restoreConsumer.assign(..) to shutdownTasksAndState as this is done prior to the closing of consumers.","closed","","dguy","2016-10-07T08:01:11Z","2016-10-07T18:30:39Z"
"","2201","KAFKA-4306: Shutdown distributed herder with a timeout.","Resolves  KAFKA-4306: Connect workers won't shut down if brokers are not available KAFKA-4154: Kafka Connect fails to shutdown if it has not completed startup","closed","connect,","kkonstantine","2016-12-01T23:35:01Z","2020-10-16T06:08:11Z"
"","1708","KAFKA-4025 - make sure file.encoding system property is set to UTF-8 when invoking the rat task","reset back to previous value after.","closed","","radai-rosenblatt","2016-08-07T02:23:24Z","2016-08-07T22:22:35Z"
"","1893","KAFKA-4178: Windows in Rate Calculation","ReplicationQuotas included a new Rate class which changed the way the window is calculated. @junrao asked that we look to consolidate this. On balance I believe there is a good case for both, so this PR pulls out two policies for calculating the window, and attempts to explain what they are and why we might need them.   They are: **Elapsed**: Set the window to the difference between the oldest and newest measurement time. Handle NaN. _(Replication Quotas use this)_ **Fixed**: Fix the window to the full duration (10s by default). _(Client Quotas use this)_  Replication Quotas uses the **Elapsed** policy rather than **Fixed**, as fixed provides an underestimate during the first window. Underestimates are best avoided in rebalancing tasks, as they tend to create an immediate load spike when replication starts. So it's preferable to overestimate the rate, and hence throttle immediately in response to a rate change. This approach is also a bit easier to test.   I may not fully understand the background for Client Quotas moving to this **Fixed** approach (Fixed is my nomenclature by the way - but the change was made in response to KAFKA-2443 & KAFKA-2567). There was was an issue with returning NaN, but should really be tangential (it is handled in both policies). However it seems sensible to slowly throttle on a client's connection down to the desired rate when you initialise or change quotas, so there is logic to it.   So I think there is a requirement for both types of rate, and hence I've included both in this PR. **Elapsed** suits throttled replication, where the general use case is to initiate some rebalance and immediately apply a throttle. **Fixed** suits client quotas where we want to gently throttle a client down to the desired rate over a period of seconds.","open","","benstopford","2016-09-20T17:45:03Z","2018-03-02T19:29:42Z"
"","1883","Use pre-calculated value in trace message","Replaced the math.min(size, sizeInBytes) call with the count variable that has already been calculated within the writeTo method. Replaced string concatenation with interpolation.  Replaces borked PR #1877","open","","lukezaparaniuk","2016-09-19T19:43:56Z","2022-06-14T12:38:09Z"
"","1877","Use pre-calculated value in trace message","Replaced the math.min(size, sizeInBytes) call with the count variable that has already been calculated within the writeTo method","closed","","lukezaparaniuk","2016-09-18T11:17:18Z","2016-09-19T19:33:47Z"
"","1811","KAFKA-4112: Remove alpha quality label from Kafka Streams in docs","Rephrase 'alpha quality' wording in Streams section of api.html. Couple of other minor fixes in streams.html","closed","","dguy","2016-09-01T07:35:26Z","2016-09-01T19:26:18Z"
"","1908","KAFKA-3396 : Unauthorized topics are returned to the user","Reopening of https://github.com/apache/kafka/pull/1428","closed","","edoardocomar","2016-09-26T18:15:52Z","2017-06-22T12:11:39Z"
"","2374","KAFKA-3209: KIP-66: more single message transforms","Renames `HoistToStruct` SMT to `HoistField`.  Adds the following SMTs: `ExtractField` `MaskField` `RegexRouter` `ReplaceField` `SetSchemaMetadata` `ValueToKey`  Adds HTML doc generation and updates to `connect.html`.","closed","connect,","shikhar","2017-01-13T22:03:16Z","2020-10-16T06:37:50Z"
"","1699","MINOR: rename StateStoreProvider.getStores() -> StateStoreProvider.stores()","Rename StateStoreProvider.getStores(...) to StateStoreProvider.stores(...) as this is consistent with the naming of other 'getters' in the public API.","closed","","dguy","2016-08-03T08:21:56Z","2016-08-03T22:25:07Z"
"","2339","MINOR: rename SessionStore.findSessionsToMerge to findSessions","Rename `SessionStore.findSessionsToMerge` to `findSessions`","closed","","dguy","2017-01-10T16:38:19Z","2017-01-10T18:39:51Z"
"","1799","Kafka 4060 remove zk client dependency in kafka streams","Removed Zookeeper client from the Kafka Streams. The internal topic manager now will use Kafka Client to create/delete internal topics.","closed","","hjafarpour","2016-08-29T21:25:22Z","2016-09-20T00:23:17Z"
"","2194","KAFKA-4443: Minor comment clean-up","Removed stale comment left behind, minor fixes (UpdateMetadataRequest instead of MetadataUpdateRequest) and remove redundant comments.","closed","","ijuma","2016-11-30T11:12:29Z","2016-12-01T15:40:53Z"
"","2004","MINOR: Fix table of content and section numbers in the documentation","Removed a non-existing reference in table of contents and fixed some section numbers.","closed","","vahidhashemian","2016-10-10T21:47:32Z","2016-10-11T00:10:02Z"
"","2089","MINOR: Fix documentation of compaction","Removed a duplicate line and also cleaned up some of the language around compaction guarantees.","closed","","apurvam","2016-11-01T22:28:39Z","2016-11-02T05:26:53Z"
"","2373","KAFKA-4568: Simplify test code for multiple SASL mechanisms","Remove workaround for testing multiple SASL mechanisms using sasl.jaas.config and the new support for multiple client modules within a JVM.","closed","","rajinisivaram","2017-01-13T21:19:43Z","2017-01-18T11:42:45Z"
"","2319","KAFKA-4551: StreamsSmokeTest.test_streams intermittent failure","Remove use of TestTimestampExtractor as it causes the logs to roll and segments get deleted. Remove the wcnt example as it is dependent on the TestTimestampExtractor - windowed counting is covered elsewhere. Change all aggregate operations to use TimeWindow as use of UnlimitedWindow was causing logs to roll and segments being deleted.","closed","","dguy","2017-01-05T19:26:48Z","2017-01-09T19:13:58Z"
"","2236","KAFKA-4517: Remove deprecated shell script","Remove the kafka-consumer-offset-checker.sh script completely since it was already deprecated in Kafka9  Currently it's quite confusing to new Kafka operators that this script exists because it seems to do exactly what they want for checking offsets, only to later realize they should instead use kafka-consumer-groups.sh script","closed","","jeffwidman","2016-12-09T19:44:28Z","2017-05-12T18:05:44Z"
"","1948","KAFKA-4245: Don't swallow IOExceptions","Remove swallowing of exceptions from BlockingChannel#connect.  This commit also slightly reworks the handling of a BlockingChannel in KafkaServer to correctly handle the possibility of an IOException being thrown by BlockingChannel#connect().  Note that I'm totally new to Scala, so any advice on anything I might be doing here that is unconventional or just plain wrong is  certainly welcome.","closed","","gabrielreid","2016-10-02T14:00:38Z","2018-02-25T21:23:04Z"
"","1888","MINOR: remove unused code from InternalTopicManager","Remove isValidCleanupPolicy and related fields as they are never used.","closed","","dguy","2016-09-20T08:41:28Z","2016-09-21T18:14:39Z"
"","2092","MINOR: remove commented out code and System.out.println","Remove commented out code and System.out.println from KTableKTableJoinIntegrationTest","closed","","dguy","2016-11-02T10:51:56Z","2016-11-08T20:10:51Z"
"","2385","MINOR: remove unused constructor param from ProcessorStateManager","Remove applicationId parameter as it is no longer used.","closed","","dguy","2017-01-16T11:56:25Z","2017-01-16T19:41:27Z"
"","2119","MINOR: remove unused fields from KTableImpl","Remove `keySerde`, `valSerde`, `OUTERTHIS_NAME`, `OUTEROTHER_NAME`, `LEFTTHIS_NAME`, `LEFTOTHER_NAME` from `KTableImpl` as they are all unused fields","closed","","dguy","2016-11-09T14:51:42Z","2016-11-09T19:18:18Z"
"","2028","Replace default X509TrustManager with ReloadableX509TrustManager.","ReloadableX509TrustManager will allow us to replace broker tuststore at any time without restart the broker.","closed","","allenxiang","2016-10-14T13:45:17Z","2016-10-14T17:56:41Z"
"","1494","KAFKA-1981 Make log compaction point configurable","Reduced scope to controlling only minimum time before compaction. Changed to using message time when available.","closed","","ewasserman","2016-06-10T22:56:25Z","2016-08-26T20:56:37Z"
"","2001","KAFKA-4283: records deleted from CachingKeyValueStore still appear in range and all queries","Records that are deleted/removed from the CachingKeyValueStore shouldn't appear in range and all queries. Modified the iterator such that it skips over the deleted records.","closed","","dguy","2016-10-10T13:43:16Z","2016-10-11T20:58:16Z"
"","1913","KAFKA-3175 (Rebased) : topic not accessible after deletion even when delete.topic.enable is disabled","Rebased the patch with current trunk.","closed","","MayureshGharat","2016-09-27T03:23:20Z","2016-09-27T16:36:30Z"
"","1542","KAFKA-3892 prune metadata response to subscribed topics","Rebased from PR #1541","closed","","iamnoah","2016-06-22T21:05:01Z","2017-10-19T14:34:10Z"
"","2389","Kafka 4060 remove zk client dependency in kafka streams followup re-branched from trunk","Re-branched the trunk and applied the changes to the new branch to simplify commit log.","closed","","hjafarpour","2017-01-17T22:27:14Z","2017-01-18T23:55:11Z"
"","1625","MINOR: fix extra broker setup","Quickstart setup for brokers 1 and 2 need ""zookeeper.connect"" setting.  Otherwise you are greeted with: org.apache.kafka.common.config.ConfigException: Missing required configuration ""zookeeper.connect"" which has no default value.","closed","","mweirauch","2016-07-15T11:43:50Z","2016-08-12T08:33:38Z"
"","2425","KAFKA-4687: Fix InvalidTopicException due to topic creation race condition","Pull request for https://issues.apache.org/jira/browse/KAFKA-4687, complete details can be found in the Jira.  With this change, instead of incorrectly throwing an InvalidTopicException, a TopicExistsException would be thrown [here](https://github.com/apache/kafka/blob/0.10.1.1/core/src/main/scala/kafka/admin/AdminUtils.scala#L474).  Note that an alternative (or possibly additional) change could be to proactively check if the retrieved topic inventory [here](https://github.com/apache/kafka/blob/0.10.1.1/core/src/main/scala/kafka/admin/AdminUtils.scala#L432) contains the topic being created, and throw a TopicExistsException at that point. Let me know if that would be preferred, and I'll update the code accordingly.  **Edit**: Updated PR to revert `hasCollision` changes and add the alternative change in `AdminUtils`.","closed","","noslowerdna","2017-01-23T22:47:54Z","2017-01-24T20:27:22Z"
"","2451","KAFKA-4648: Improve test coverage StreamTask","Provide test coverage for exception paths in: `schedule()`, `closeTopology()`, and `punctuate()`","closed","","dguy","2017-01-26T18:05:22Z","2017-02-17T00:58:06Z"
"","2293","KAFKA-4180 : Authentication with multiple actives Kafka","producers/consumers  Changed caching in LoginManager to allow one LoginManager per client JAAS configuration. Added test to End2EndAuthorization for SASL Plain and Gssapi with two consumers with different credentials.  developed with @mimaison","closed","","edoardocomar","2016-12-27T11:24:34Z","2017-03-16T11:29:05Z"
"","2307","kafka-4434: KafkaProducer configuration is logged twice","ProducerConfig calls AbstractConfig.init where does the logging. KafkaProducer init will inovoke ProducerConfig.init twice that leads to logging twice.","closed","","huxihx","2017-01-04T07:33:12Z","2017-01-04T12:00:08Z"
"","1836","KAFKA-3703: Graceful close for consumers and producer with acks=0","Process requests received from channels before they were closed. For consumers, wait for coordinator requests to complete before returning from close.","closed","","rajinisivaram","2016-09-08T20:40:44Z","2017-02-01T11:58:50Z"
"","2327","MINOR: Fix small error in javadoc for persistent Stores","Probably copy/pasted from the factory for in-memory stores above it :)","closed","","nixsticks","2017-01-06T20:39:16Z","2017-01-09T21:01:27Z"
"","2102","KAFKA-2284: corrects value type in beforeReleasingPartitions","Previously the values in the map were scala Set values, now they are correctly java.util.Set as advertised by API.","closed","","leachbj","2016-11-04T01:10:21Z","2016-11-20T21:56:54Z"
"","1822","Fix javadocs of Windowed","Previous Javadoc was referring to a 0.10.0.x method that was since removed from trunk.","closed","","miguno","2016-09-05T07:46:39Z","2016-09-05T07:49:42Z"
"","2269","fixup typo","pretty boring docfix, ""no"" -> ""not""","closed","","smferguson","2016-12-16T21:10:43Z","2016-12-21T19:38:19Z"
"","1571","MINOR: Fix few documentation errors in streams quickstart","Plus a minor enhancement","closed","","glikson","2016-06-29T20:43:35Z","2016-07-05T05:56:14Z"
"","1715","WIP: Add a consumer offset migration tool","Please see the dev mailing list for context.","closed","","granthenke","2016-08-09T17:01:25Z","2022-02-10T16:16:34Z"
"","1666","[KAFKA-3991]: allow MirrorMaker to have custom producer","Please see jira: https://issues.apache.org/jira/browse/KAFKA-3991","closed","","ooasis","2016-07-26T16:55:01Z","2022-02-10T16:15:49Z"
"","2247","MINOR: Fix Streams examples in documentation","Performed minor cleanup and escaped `<` and `>` so code examples are shown correctly in the browser.","closed","","vahidhashemian","2016-12-12T22:32:45Z","2016-12-13T17:45:04Z"
"","1988","Commits the Gradle wrapper jar/properties file","Per the Gradle user guide, https://docs.gradle.org/current/userguide/gradle_wrapper.html, it's recommended that these are committed into version control.   This allows for out-of-the-box running of `gradlew` or `gradlew.bat`.","closed","","valdisrigdon","2016-10-07T14:10:48Z","2018-02-28T05:01:53Z"
"","1943","KAFKA-4241: StreamsConfig doesn't pass through custom consumer and producer properties to ConsumerConfig and ProducerConfig","pass through user-defined consumer and producer properties from StreamsConfig to ConsumerConfig and ProducerConfig. For example, consumer interceptor support, i.e, consumer.interceptor.classes=SomeClass","closed","","dguy","2016-09-30T15:18:33Z","2016-10-02T04:58:25Z"
"","2256","KAFKA-4534: StreamPartitionAssignor only ever updates the partitionsByHostState and metadataWithInternalTopics on first assignment","partitionsByHostState and metadataWithInternalTopics need to be updated on each call to onAssignment() otherwise they contain invalid/stale metadata.","closed","","dguy","2016-12-14T13:51:52Z","2016-12-19T20:23:30Z"
"","2183","Removed obsolete parameter form example config in docs.","Parameter controller.message.queue.size was removed in 0.9 (KAFKA-2122) but is still listed in an example broker configuration in the documentation.","closed","","soenkeliebau","2016-11-28T22:56:24Z","2017-02-07T17:21:50Z"
"","2313","KAFKA-4575: ensure topic created before starting sink for ConnectDistributedTest.test_pause_resume_sink","Otherwise in this test the sink task goes through the pause/resume cycle with 0 assigned partitions, since the default metadata refresh interval is quite long","closed","","shikhar","2017-01-05T00:28:47Z","2017-01-05T23:23:16Z"
"","1498","KAFKA-3830; getTGT() debug logging exposes confidential information","Only log the client and server principals, which is what ZooKeeper does after ZOOKEEPER-2405.","closed","","ijuma","2016-06-13T13:24:16Z","2016-06-15T16:34:42Z"
"","1740","MINOR: Remove # from .bat start script","On Windows, the following output is seen when starting Zookeeper and Kafka servers:  ``` '#' is not recognized as an internal or external command, operable program or batch file. ```  This pull request makes a minor correction to the Windows `kafka-run-class.bat` script to replace the use of `#` with `rem`.","closed","","p-thorpe","2016-08-15T16:08:35Z","2016-08-19T03:50:53Z"
"","2325","KAFKA-4441 Monitoring incorrect during topic creation and deletion","OfflinePartitionsCount PreferredReplicaImbalanceCount metrics check for topic being deleted  Added integration test which polls the metrics while topics are being created and deleted  Developed with @mimaison","closed","","edoardocomar","2017-01-06T15:16:06Z","2017-03-16T11:29:02Z"
"","2204","KAFKA-4205; KafkaApis: fix NPE caused by conversion to array","NPE was caused by `log.logSegments.toArray` resulting in array containing `null` values. The exact reason still remains somewhat a mystery to me, but it seems that the culprit is `JavaConverters` in combination with concurrent data structure access.  Here's a simple code example to prove that: ```scala import java.util.concurrent.ConcurrentSkipListMap // Same as `JavaConversions`, but allows explicit conversions via `asScala`/`asJava` methods. import scala.collection.JavaConverters._  case object Value val m = new ConcurrentSkipListMap[Int, Value.type] new Thread { override def run() = { while (true) m.put(9000, Value) } }.start() new Thread { override def run() = { while (true) m.remove(9000) } }.start() new Thread { override def run() = { while (true) { println(m.values.asScala.toArray.headOption) } } }.start() ```  Running the example will occasionally print `Some(null)` indicating that there's something shady going on during `toArray` conversion.  `null`s magically disappear by making the following change: ```diff - println(m.values.asScala.toArray.headOption) + println(m.values.asScala.toSeq.headOption) ```","closed","","ataraxer","2016-12-02T14:55:32Z","2016-12-04T19:22:22Z"
"","1794","KAFKA-1981 Make log compaction point configurable","Now uses LogSegment.largestTimestamp to determine age of segment's messages.","closed","","ewasserman","2016-08-26T20:55:54Z","2016-09-12T01:46:15Z"
"","2144","KAFKA-4417: Update build dependencies for 0.10.2 cycle","Notes on the updates: * Gradle to 3.2: better incremental build and faster IDE import times (https://docs.gradle.org/3.2/release-notes) * zkclient to 0.10: it now uses slf4j-api instead of log4j * zookeeper to 3.4.9: a few important bug fixes (http://zookeeper.apache.org/doc/r3.4.9/releasenotes.html) * jackson to 2.8.5: lots of updates (https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.6, https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.7, https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.8) * jersey to 2.24: bug fixes (https://jersey.java.net/release-notes/2.23.html, https://jersey.java.net/release-notes/2.24.html) * jopt to 5.0.3: minor improvements, the major version bump is due to requiring Java 7 instead of 6 (https://pholser.github.io/jopt-simple/changes.html) * argparse4j to 0.7.0: minor tweaks and improvements (https://github.com/tatsuhiro-t/argparse4j/blob/argparse4j-0.6.0/NEWS, https://github.com/tatsuhiro-t/argparse4j/blob/argparse4j-0.7.0/NEWS) * Gradle plugins * bcpkix to 1.55: quite a few additions, but nothing that matters to us (http://www.bouncycastle.org/releasenotes.html)","closed","","ijuma","2016-11-17T10:50:43Z","2016-11-17T22:43:48Z"
"","1677","KAFKA-4004: NetworkReceive.complete() should not throw NullPointerException after partially reading the size","NetworkReceive.readFrom can partially read the size and not allocate a buffer. A following NetworkReceive.complete() should not throw a NullPointerException and should instead just return false.","closed","","onurkaraman","2016-07-28T18:20:22Z","2016-07-28T18:52:12Z"
"","1999","2.11-0.10.0.1 can't see /consumers/group node in zookeeper","My producers and consumers all connected to kafka borkers and can exchange messages quite well, I can see ""/consumers"" node in zookeeper but there's nothing under ""/consumers"" node.","closed","","cyfonly","2016-10-10T10:17:26Z","2018-04-28T00:02:11Z"
"","2295","MINOR: Mx4jLoader always returns false even if mx4j is loaded & started","Mx4jLoader.scala should explicitly `return true` if the class is successfully loaded and started, otherwise it will return false even if the class is loaded.","closed","","eribeiro","2016-12-29T21:12:33Z","2016-12-30T05:04:13Z"
"","1685","KAFKA-4008: Module ""tools"" should not be dependent on ""core""","moved streams application reset tool from tools to core","closed","","mjsax","2016-07-29T22:40:23Z","2016-08-02T03:12:45Z"
"","1628","KAFKA-3812 State store locking is incorrect","Move all state directory creation/locking/unlocking/cleaning to a single class. Don't release the channel until the lock is released. Refactor code to make use of new class","closed","","dguy","2016-07-17T16:31:54Z","2016-07-19T23:01:23Z"
"","2442","KAFKA-4642: Improve test coverage of ProcessorStateManager","Most of the exception paths weren't covered. Now they are.","closed","","dguy","2017-01-26T12:36:33Z","2017-02-17T00:58:28Z"
"","1805","MINOR: onCompletion: metadata valid only if no exception","Modifies example in doc change","closed","","ybyzek","2016-08-31T13:52:34Z","2016-08-31T19:09:33Z"
"","1891","MINOR: add javadoc comment to PersistenKeyValueFactory.enableCaching","missing javadoc on public API method PersistenKeyValueFactory.enableCaching","closed","","dguy","2016-09-20T10:45:36Z","2016-09-21T18:12:13Z"
"","2393","MINOR: Use new-consumer config in MirrorMaker doc","Mirrormaker was updated to default to the new consumer in 3db752a565071c78e4b11eaafa739844fa785b04  Old consumer calls the param `auto.commit.enable`, new consumer calls it `enable.auto.commit`. So I updated the docstring to use the new consumer name for the param.","closed","","jeffwidman","2017-01-18T01:49:42Z","2017-05-18T21:46:19Z"
"","2394","KAFKA-4668: Replicate mirrormaker topics from earliest","MirrorMaker currently inherits the default value for `auto.offset.reset`, which is `latest`.  While for most consumers this is a sensible default, MirrorMakers are specifically designed for replication, so they should default to replicating topics from the beginning.  A specific scenario where this really matters is when a MirrorMaker is subscribed to a regex pattern. If auto-topic creation is enabled on the cluster, and you start producing to a non-existent topic that matches the regex, then there will be a period of time where the producer is producing before the new topic's partitions have been picked up by the MirrorMaker. Those messages will never be consumed by the MirrorMaker because it will start from latest, ignoring those just-produced messages.","closed","tools,","jeffwidman","2017-01-18T01:57:43Z","2022-02-10T16:18:51Z"
"","1885","Fix comments in KStreamKStreamJoinTest","Minor comment fixes.","closed","","eliaslevy","2016-09-20T00:36:49Z","2016-09-22T17:34:01Z"
"","2288","MINOR: Create Hyperlinks in protocol api docs","Minor changes to generate hyperlinks in the protocol api documentation to ease lookup.","closed","","imandhan","2016-12-22T03:20:14Z","2017-01-27T22:17:58Z"
"","1591","KAFKA-3836: KStreamReduce and KTableReduce should not pass nulls to Deserializers","Minor changes to check null changes.","closed","","jeyhunkarimov","2016-07-06T09:07:19Z","2016-07-06T19:27:48Z"
"","1641","HOTFIX: fix compilation error due to merge of KAFKA-3812","Merge of KAFKA-3812 caused a compilation error in StreamThreadStateStoreProviderTest","closed","","dguy","2016-07-19T23:50:15Z","2016-07-19T23:55:24Z"
"","1802","wrong property was mentioned in doc","max.fetch.wait is mentioned in document where it should have been fetch.wait.max.ms","closed","","yourspraveen","2016-08-30T16:51:24Z","2018-01-02T18:08:33Z"
"","1824","KAFKA-4123: Queryable State returning null for key before all stores in instance have been initialized","Mark the store as open after the DB has been restored from the changelog. Only add the store to the map in ProcessorStateManager post restore. Make RocksDBWindowStore.Segment override openDB(..) as it needs to mark the Segment as open. Throw InvalidStateStoreException if any stores in a KafkaStreams instance are not available.","closed","","dguy","2016-09-05T13:10:36Z","2016-09-08T00:43:08Z"
"","1633","KAFKA-3855: Guard race conditions in TopologyBuilder","Mark all public `TopologyBuilder` methods as synchronized as they can modify data-structures and these methods could be called from multiple threads","closed","","dguy","2016-07-18T18:09:55Z","2016-07-19T15:53:30Z"
"","2429","KAFKA-4677: Avoid unnecessary task movement across threads during rebalance","Makes task assignment more sticky by preferring to assign tasks to clients that had previously had the task as active task. If there are no clients with the task previously active, then search for a standby. Finally falling back to the least loaded client.","closed","","dguy","2017-01-24T17:04:05Z","2017-02-17T00:58:16Z"
"","2003","make documentation follow latest template","Make the latest version of our docs follow the latest site template structure.","closed","","derrickdoo","2016-10-10T21:21:04Z","2016-10-10T21:47:48Z"
"","2205","KAFKA-4481: relax streams api type contraints","Make appropriate methods contravariant in key and value types.","closed","","xvrl","2016-12-02T20:12:57Z","2017-01-11T17:15:53Z"
"","2115","KAFKA-4364: Remove secrets from DEBUG logging","leverage fix from KAFKA-2690 to remove secrets from task logging","closed","","rnpridgeon","2016-11-08T23:39:33Z","2016-11-09T22:59:32Z"
"","2065","KAFKA-4302: Simplify KTableSource","KTableSource is always materialized since IQ: - removed flag KTableSource#materialized - removed MaterializedKTableSourceProcessor","closed","","mjsax","2016-10-26T06:08:48Z","2016-10-30T18:39:21Z"
"","2080","KAFKA-4302: Simplify KTableSource","KTableSource is always materialized since IQ:   - removed flag KTableSource#materialized   - removed MaterializedKTableSourceProcessor","closed","","mjsax","2016-10-31T22:09:30Z","2016-11-01T21:26:37Z"
"","1917","KAFKA-4223: RocksDBStore should close all open iterators on close","Keep track of open Rocks DB iterators. When a store is closed, close all open iterators.","closed","","dguy","2016-09-27T10:18:30Z","2016-09-30T08:42:17Z"
"","1819","KAFKA-3708: rethink exception handling in Kafka Streams","KafkaExceptions currently thrown from within StreamThread/StreamTask currently bubble up without any additional context. This makes it hard to figure out where something went wrong, i.e, which topic had the serialization exception etc","closed","","dguy","2016-09-02T15:33:01Z","2016-09-28T18:08:28Z"
"","1487","MINOR:  kafka-run-class.sh now runs under Cygwin.","kafka-run-class.sh now runs under Cygwin; paths and classpath are set up properly.  **WARNING:**  The script was not tested on a Linux machine, only under Cygwin.  Prior to merge it into trunk, if accepted, please run a quick test to ensure nothing broke.  From my own code review, there should not be any problem, but we can never be too sure.  I do not have the environment to test it under Linux at this moment.","closed","","deragon","2016-06-09T15:14:22Z","2016-08-09T17:39:52Z"
"","2078","kafka-run-class.sh can now handle paths containing spaces used in ${CLASSPATH} and in ${JAVA} (useful for Macs users) and now runs under Cygwin (useful for Windows users)","kafka-run-class.sh can now handle paths containing spaces used in ${CLASSPATH} and in ${JAVA}.  This was required on Mac OSX since many library paths have spaces within them.  This was resolved by surrounding ${CLASSPATH} and ${JAVA} with double quotes ("").  The change is very minor.","open","","deragon","2016-10-31T16:58:53Z","2018-10-15T19:23:25Z"
"","2312","Kafka 960 metrics upgrade","KAFKA-960 Replaced outdated yammer.metrics with codahale.metrics 3.1.0.","open","","andreinechaev","2017-01-04T22:50:05Z","2020-07-15T22:26:55Z"
"","2470","Kafka-4711 :Change Default unclean.leader.election.enabled from True to False (KIP-106)","Kafka-4711 :Change Default unclean.leader.election.enabled from True to False (KIP-106) KAFKA-4623 :Change Default unclean.leader.election.enabled from True to False","closed","","sharad-develop","2017-01-31T16:39:20Z","2017-03-03T02:07:45Z"
"","2349","KAFKA-4603 Command parsed error, change all OptionParser constructor","KAFKA-4603 the command parsed error Using ""new OptionParser"" might result in parse error  Change all the OptionParser constructor in Kafka into ""new OptionParser(false)""","closed","","auroraxlh","2017-01-12T10:25:35Z","2017-08-04T01:34:09Z"
"","2322","KAFKA-4603 the argument of shell in doc wrong and command parsed error","KAFKA-4603 the argument of shell in doc wrong and command parsed error     In ""7.6.2 Migrating clusters"" of document security.html, the argument ""--zookeeper.connection"" of shell ""zookeeper-security-migrat.sh""   is wrong  and the using of OptionParser is not correct     This patch corrected the doc and changed the OptionParser constructor","closed","","auroraxlh","2017-01-06T09:48:35Z","2017-01-13T16:35:43Z"
"","2263","KAFKA-4508. Create system tests that run newer versions of the client…","KAFKA-4508. Create system tests that run newer versions of the client against older versions of the broker","closed","","cmccabe","2016-12-15T23:25:18Z","2019-05-20T18:33:18Z"
"","2264","Kafka 4507: The client should send older versions of requests to the broker if necessary","KAFKA-4507  The client should send older versions of requests to the broker if necessary.  Note: This builds on top of KAFKA-3600, which has not yet been committed yet.","closed","","cmccabe","2016-12-16T01:52:20Z","2019-05-20T18:33:27Z"
"","2156","kafka-4428: Kafka does not exit when it receives ""Address already in use"" error during startup","kafka-4428: Kafka does not exit when it receives ""Address already in use"" error during startup  During Acceptor initialization, if ""Address already in use"" error is encountered, the countdown latches for all Processors have no chance to be counted down, hence Kafka server fails to exit, pending when invoking Processor.shutdown","closed","","huxihx","2016-11-22T03:42:09Z","2016-12-28T18:28:35Z"
"","1601","KAFKA-3905; Handle invalid collection of topics, patterns on subscription for list of topics, with patterns, and with assignments","KAFKA-3905: Handling null/empty topics and collections, patterns when subscription with list of topics or with patterns, and with assignments. - Added validity checks for input parameters on subscribe, assign to avoid NPE, and provide an argument exception instead - Updated behavior for subscription with null collection to be same as when subscription with emptyList.i.e., unsubscribes. - Added tests on subscription, assign","closed","","rekhajoshm","2016-07-09T17:21:14Z","2016-07-14T19:46:26Z"
"","1561","KAFKA-3905; Handle invalid collection of topics, patterns on subscription for list of topics, with patterns, and with assignments","KAFKA-3905: Handling null/empty topics and collections, patterns when subscription with list of topics or with patterns, and with assignments. - Added validity checks for input parameters on subscribe, assign to avoid NPE, and provide an argument exception instead - Updated behavior for subscription with null collection to be same as when subscription with emptyList.i.e., unsubscribes. - Added tests on subscription, assign","closed","","rekhajoshm","2016-06-27T20:47:40Z","2016-07-09T17:22:33Z"
"","1559","KAFKA-3844; Sort configuration items in log","KAFKA-3844; Sort configuration items in log","closed","","rekhajoshm","2016-06-27T18:00:38Z","2016-07-09T01:05:23Z"
"","2289","KAFKA-3355","KAFKA-3355 GetOffsetShell command doesn't work with SASL enabled Kafka","closed","","mgarmes","2016-12-22T11:11:27Z","2017-04-21T21:37:03Z"
"","1491","remove size and message count constraints","KAFKA-1981Make log compaction point configurable  Reduced scope to controlling only minimum time before compaction. Changed to using message time when available.","closed","","ewasserman","2016-06-10T21:37:27Z","2016-06-10T22:54:11Z"
"","2096","KAFKA-4372: Kafka Connect REST API does not handle DELETE of connector with slashes in their names","Kafka Connect REST API does not handle in many places connectors with slashes in their names because it expects PathParams, this PR intends to :  * Reject as bad requests API calls trying to create connectors with slashes in their names * Add support for connector with slashes in their names in the DELETE part of the API to allow users to cleanup their connectors without dropping everything.  This PR adds as well the Unit Test needed for the creation part and was tested manually for the DELETE part.","closed","connect,","ogirardot","2016-11-03T13:58:56Z","2020-10-16T06:38:32Z"
"","2177","KAFKA-3959: enforce offsets.topic.replication.factor upon __consumer_offsets auto topic creation (KIP-115)","Kafka brokers have a config called ""offsets.topic.replication.factor"" that specify the replication factor for the ""__consumer_offsets"" topic. The problem is that this config isn't being enforced. If an attempt to create the internal topic is made when there are fewer brokers than ""offsets.topic.replication.factor"", the topic ends up getting created anyway with the current number of live brokers. The current behavior is pretty surprising when you have clients or tooling running as the cluster is getting setup. Even if your cluster ends up being huge, you'll find out much later that __consumer_offsets was setup with no replication.  The cluster not meeting the ""offsets.topic.replication.factor"" requirement on the internal topic is another way of saying the cluster isn't fully setup yet.  The right behavior should be for ""offsets.topic.replication.factor"" to be enforced. Topic creation of the internal topic should fail with GROUP_COORDINATOR_NOT_AVAILABLE until the ""offsets.topic.replication.factor"" requirement is met. This closely resembles the behavior of regular topic creation when the requested replication factor exceeds the current size of the cluster, as the request fails with error INVALID_REPLICATION_FACTOR.","closed","","onurkaraman","2016-11-28T06:09:04Z","2017-02-02T06:31:19Z"
"","2304","kafka-4576: Log segments close to max size break on fetch","JVM spec does not make a guarantee that the buffer will be filled up when invoking FileChannel.read, which is called in searchOffsetWithSize to fail the follower replicas' reading data from leader when large log segment is set.","closed","","huxihx","2017-01-04T03:39:38Z","2017-01-24T11:03:26Z"
"","2101","KAFKA-4377: remove deprecated scala.collection.JavaConversions calls","JavaConversions are deprecated in 2.12 in favour of JavaConverters.","closed","","leachbj","2016-11-03T22:06:17Z","2016-11-18T13:55:39Z"
"","2271","MINOR: Improvements to Record related classes (part 1)","Jason recently cleaned things up significantly by consolidating the Message/Record classes into the common Java code in the clients module. While reviewing that, I noticed a few things that could be improved a little more. To make reviewing easier, there will be multiple PRs.","closed","","ijuma","2016-12-17T15:02:33Z","2017-09-05T09:47:12Z"
"","1503","MINOR: Small enhancement to Deserializer Javadoc","I’ve implemented my own custom Deserializer and been using it with `KStream.reduceByKey`; I observed that `reduceByKey` was passing null to my implementation, but it wasn’t clear to me what my implementation was expected to do in this case. So this attempts to clarify it.  This is my original work and I license this work to the Kafka project under Kafka’s open source license (the Apache License 2.0).","closed","","aviflax","2016-06-14T14:32:24Z","2016-07-25T18:04:31Z"
"","1600","KAFKA-3942: Change IntegrationTestUtils.purgeLocalStreamsState to use java.io.tmpdir","It was previously only deleting files/folders where the path started with /tmp. Changed it to delete from the value of the System Property `java.io.tmpdir`. Also changed the tests that were creating State dirs under /tmp to just use `TestUtils.tempDirectory(..)`","closed","","dguy","2016-07-09T09:04:40Z","2016-07-11T08:51:01Z"
"","1925","Remove duplicate paragraph in Log Compaction docs","It seems that the last guarantee in the Log Compaction guarantees list is duplicated, so I removed the one with less formatting.  In case I'm missing something and these two paragraphs state two separate guarantees, I think the difference between them should be explained explicitly.","closed","","IvanVergiliev","2016-09-28T13:07:17Z","2018-01-26T19:09:18Z"
"","1613","KAFKA-3954; Consumer should use internal topics information returned by the broker","It previously hardcoded it.","closed","","ijuma","2016-07-12T09:01:22Z","2016-08-09T03:33:34Z"
"","2465","KAFKA-4710: Interpolate log4j's logging source interpretation to correct location info of logs written through trait methods","Issue: https://issues.apache.org/jira/browse/KAFKA-4710  This PR fixes location information of log4j `LoggingEvent` which is currently given wrongly due to indirect logger method invocation through Logging trait method. By introducing custom `LoggingEvent` which manually traverses stack trace to find a correct location where the logs are originally written, an appender can now obtain the correct location instead of somewhere inside the `Logging.scala`.","closed","","kawamuray","2017-01-30T08:20:37Z","2017-03-03T13:54:09Z"
"","2352","KAFKA-4614 Forcefully unmap mmap of OffsetIndex to prevent long GC pause","Issue: https://issues.apache.org/jira/browse/KAFKA-4614  Fixes the problem that the broker threads suffered by long GC pause. When GC thread collects mmap objects which were created for index files, it unmaps memory mapping so kernel turns to delete a file physically. This work may transparently read file's metadata from physical disk if it's not available on cache. This seems to happen typically when we're using G1GC, due to it's strategy to left a garbage for a long time if other objects in the same region are still alive. See the link for the details.","closed","","kawamuray","2017-01-12T12:45:48Z","2017-01-19T17:49:09Z"
"","1816","KAFKA-4116: Handle 0.0.0.0 as a special case when using advertised.listeners","Issue: https://issues.apache.org/jira/browse/KAFKA-4116","closed","","kawamuray","2016-09-02T07:17:49Z","2018-02-25T21:25:38Z"
"","1707","KAFKA-4024: Fix metadata update to not apply backoff until it experiences failure","Issue: https://issues.apache.org/jira/browse/KAFKA-4024  Fixes a bug that inappropriately applies backoff as interval between metadata updates even though the current one is outdated.","closed","","kawamuray","2016-08-06T09:27:36Z","2016-11-04T06:24:58Z"
"","1606","KAFKA-3947: Add dumping current assignment capability to kafka-reassign-partitions.sh","Issue: https://issues.apache.org/jira/browse/KAFKA-3947","open","","kawamuray","2016-07-11T10:14:32Z","2018-03-02T19:29:36Z"
"","1778","KAFKA-4042: Contain connector & task start/stop failures within the Worker","Invoke the statusListener.onFailure() callback on start failures so that the statusBackingStore is updated. This involved a fix to the putSafe() functionality which prevented any update that was not preceded by a (non-safe) put() from completing, so here when a connector or task is transitioning directly to FAILED.  Worker start methods can still throw if the same connector name or task ID is already registered with the worker, as this condition should not happen.","closed","","shikhar","2016-08-24T07:44:45Z","2016-08-26T21:04:25Z"
"","2396","MINOR: Fix javadoc typos in KStream#process","interface for `Processor` in comments incorrectly had `transform` rather than `process`.","closed","","dguy","2017-01-18T14:13:52Z","2017-01-18T16:12:01Z"
"","2212","KAFKA-4488: UnsupportedOperationException during initialization of StandbyTask","Instead of throwing `UnsupportedOperationException` from `StandbyTask.recordCollector()` return a No-op implementation of `RecordCollector`. Refactored `RecordCollector` to have an interface and impl.","closed","","dguy","2016-12-05T17:13:46Z","2016-12-06T21:09:16Z"
"","1560","KAFKA-3906 - Connect logical types do not support nulls.","Initial commit with failing unit tests for proposed functionality.","closed","","jcustenborder","2016-06-27T19:05:06Z","2016-07-28T05:11:16Z"
"","2057","KAFKA-2089: Increase metadata wait time in unit test","Increase timeout in test to avoid transient failures due to long GC or slow machine.","closed","","rajinisivaram","2016-10-24T14:08:41Z","2016-10-24T23:19:05Z"
"","1997","KAFKA-4266: ReassignPartitionsClusterTest - Ensure ZK publication completed before start","Increase the reliability of the one temporal comparison in ReassignPartitionsClusterTest by imposing a delay after ZK is updated. This should be more reliable than just increasing the amount of data.  This relates to a previous PR: https://github.com/apache/kafka/pull/1982","closed","","benstopford","2016-10-08T12:17:38Z","2017-03-06T13:37:32Z"
"","1506","KAFKA-3755 Tightening the offset check in ReplicaFetcherThread","Including an additional check to make sure that the first offset in the message set to be appended to the log is >= than the log end offset.","closed","","imandhan","2016-06-14T22:29:47Z","2016-06-16T22:58:58Z"
"","1880","KAFKA-3283: Remove beta from new consumer documentation","Include a few clean-ups (also in producer section), mention deprecation plans and reorder so that the new consumer documentation is before the old consumers.","closed","","ijuma","2016-09-19T09:38:22Z","2016-09-19T21:27:05Z"
"","1904","KAFKA-4213: First set of system tests for replication throttling, KIP-73.","In this patch, we test `kafka-reassign-partitions` when throttling is active.   This patch also fixes the following: 1. KafkaService.verify_reassign_partitions did not check whether    partition reassignment actually completed successfully (KAFKA-4204).    This patch works around those shortcomings so that we get the right    signal from this method. 2. ProduceConsumeValidateTest.annotate_missing_messages would call    `pop' on the list of missing messages, causing downstream methods to get    incomplete data. We fix that in this patch as well.","closed","","apurvam","2016-09-23T23:46:58Z","2016-09-30T18:59:39Z"
"","2435","Replaced for with foreach and replace if ,else if, else  condition with match","In some Places for the loop was used but it can be replaced by the for each. In One file if else if else was used so I replaced the same with match.","closed","","akashsethi24","2017-01-26T04:52:39Z","2017-03-07T04:29:18Z"
"","2381","KAFKA-3502: move RocksDB options construction to init()","In RocksDBStore, options / wOptions / fOptions are constructed in the constructor, which needs to be dismissed in the close() call; however in some tests, the generated topology is not initialized at all, and hence the corresponding state stores are supposed to not be able to be closed as well since their `init` function is not called. This could cause the above option objects to be not released.  This is fixed in this patch to move the logic out of constructor and inside `init` functions, so that no RocksDB objects will be created in the constructor only. Also some minor cleanups:  1. In KStreamTestDriver.close(), we lost the logic to close the state stores but only call `flush`; it is now changed back to call both. 2. Moved the forwarding logic from KStreamTestDriver to MockProcessorContext to remove the mutual dependency: these functions should really be in ProcessorContext, not the test driver.","closed","","guozhangwang","2017-01-15T06:29:00Z","2017-07-15T22:07:30Z"
"","1900","[DOCS]: fix ambiguity - consuming by multiple consumers, but by exact…","In doc it stays:  _""Our topic is divided into a set of totally ordered partitions, each of which is consumed by one consumer at any given time.""_  And consumer is described as:   _""We'll call **processes** that subscribe to topics and process the feed of published messages **consumers**.""_  Which might lead to a wrong conclusion - that each partition can be read by one process at any given time.  I think this statements misses information about **consumer groups**, so i propose:  _""Our topic is divided into a set of totally ordered partitions, each of which is consumed by exactly one consumer (from each subscribed consumer groups) at any given time""_  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","pilloPl","2016-09-22T12:40:33Z","2016-10-04T02:37:51Z"
"","1759","KAFKA-4056: Kafka logs values of sensitive configs like passwords","In case of unknown configs, only list the name without the value","closed","","mimaison","2016-08-18T15:07:56Z","2018-04-18T13:32:02Z"
"","2275","Fix exception handling in case of file record truncation during write","In case of file record truncation during write due to improper types usage (`AtomicInteger` in place of `int`) `IllegalFormatConversionException` would be thrown instead of `KafkaException`","closed","","kamilszymanski","2016-12-19T12:47:35Z","2016-12-20T00:40:29Z"
"","2281","KAFKA-4561: Ordering of operations in StreamThread.shutdownTasksAndState may void at-least-once guarantees","In `shutdownTasksAndState` and `suspendTasksAndState` we commit offsets BEFORE we flush any state. This is wrong as if an exception occurs during a flush, we may violate the at-least-once guarantees, that is we would have committed some offsets but NOT sent the processed data on to other Sinks. Also during suspend and shutdown, we should try and complete all tasks even when exceptions occur. We should just keep track of the exception and rethrow it at the end if necessary. This helps with ensuring that StateStores etc are closed.","closed","","dguy","2016-12-20T19:04:59Z","2017-01-03T16:34:58Z"
"","1684","KAFKA-4000: Collect and record per-topic consumer metrics","Improve consumer metric collection by collecting and recording metrics per topic.","closed","","vahidhashemian","2016-07-29T22:27:02Z","2016-12-10T00:42:59Z"
"","2152","KAFKA-4424: make serializer classes final","implementations of simple serializers / deserializers should be final to prevent JVM method call overhead. See also: https://wiki.openjdk.java.net/display/HotSpot/VirtualCalls  edit: fixed test, this seems to break the API slightly (maybe include in 11.x though?)","closed","","MatthiasBechtold","2016-11-20T12:49:23Z","2016-11-24T01:48:33Z"
"","2022","KAFKA-4292: Configurable SASL callback handlers","Implementation of KIP-86: https://cwiki.apache.org/confluence/display/KAFKA/KIP-86%3A+Configurable+SASL+callback+handlers","closed","","rajinisivaram","2016-10-13T12:19:39Z","2018-04-05T08:41:43Z"
"","1979","KAFKA-4259: Dynamic JAAS configuration for Kafka clients","Implementation of KIP-85: https://cwiki.apache.org/confluence/display/KAFKA/KIP-85%3A+Dynamic+JAAS+configuration+for+Kafka+clients","closed","","rajinisivaram","2016-10-06T10:55:48Z","2017-09-29T13:01:12Z"
"","2086","KAFKA-3751: SASL/SCRAM implementation","Implementation of KIP-84: https://cwiki.apache.org/confluence/display/KAFKA/KIP-84%3A+Support+SASL+SCRAM+mechanisms","closed","","rajinisivaram","2016-11-01T13:28:30Z","2017-01-10T13:05:51Z"
"","1753","KAFKA-3492: Secure quotas for authenticated users","Implementation and tests for secure quotas at  and  levels as described in KIP-55. Also adds dynamic default quotas for ,  and . For each client connection, the most specific quota matching the connection is used, with user quota taking precedence over client-id quota.","closed","","rajinisivaram","2016-08-17T12:43:49Z","2016-09-17T17:06:46Z"
"","2351","MINOR: add Eclipse generated files to toplevel .gitignore","Ignoring sibfolders' .gitignore except tests Ignoring core/cache-main and core/cache-tests","closed","","edoardocomar","2017-01-12T10:51:46Z","2017-03-30T11:45:29Z"
"","2310","KAFKA-4497 - Ignore offset -1 in TimeIndex avoiding exception issue","Ignore offset -1 in TimeIndex avoiding exception issue Killing compaction thread.  As it seems issue remains, in 0.10.1.1 that during compaction -1 is being sent to TimeIndex, as anyhow this means a messages with no messages, we should ignore -1 offset, to avoid exception being thrown and killing the compaction thread.","closed","","ig-michaelpearce","2017-01-04T11:23:59Z","2017-01-06T15:20:33Z"
"","1564","KAFKA-3905:check collection elements in KafkaConsumer#subscribe.","If topic collection has null or """" as its element, throw an IllegalArgumentException.","closed","","iBuddha","2016-06-28T08:04:22Z","2016-06-29T00:19:28Z"
"","1596","KAFKA-1543: Change replication factor during partition map generation","If the topic-to-move-json-file contains a new replication-factor for a topic, it is used when assigning partitions to brokers.  If missing, the existing replication-factor of the topic is maintained.","open","","llamahunter","2016-07-06T23:46:51Z","2020-07-21T19:06:21Z"
"","1804","KAFKA-4104: Queryable state metadata is sometimes invalid","If the thread or process is not the coordinator the Cluster instance in StreamPartitionAssignor will always be null. This builds an instance of the Cluster with the metadata associated with the Assignment","closed","","dguy","2016-08-31T11:54:09Z","2016-09-02T17:09:30Z"
"","1920","HOTFIX: fix npe in StreamsMetadataState when onChange has not been called","If some StreamsMetadataState methods are called before the onChange method is called a NullPointerException was being thrown. Added null check for cluster in isInitialized method","closed","","dguy","2016-09-27T19:11:13Z","2016-09-28T00:36:00Z"
"","2018","KAFKA-4297: fix possiblity that didn't stop with shell (solution 2)","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I modified 'kafka.Kafka' to 'kafkaServer-gc' in 'kafka-server-stop.sh'","open","","Mabin-J","2016-10-13T04:20:03Z","2018-03-02T19:29:45Z"
"","1978","HOTFIX: Cannot Stop Kafka with Shell Script (Solution 2)","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I modified 'kafka.Kafka' to 'kafkaServer-gc' in 'kafka-server-stop.sh'","closed","","Mabin-J","2016-10-06T08:45:32Z","2016-10-13T04:23:28Z"
"","1977","HOTFIX: Cannot Stop Kafka with Shell Script (Solution 2)","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I modified 'kafka.Kafka' to 'kafkaServer-gc' in 'kafka-server-stop.sh'","closed","","Mabin-J","2016-10-06T04:21:02Z","2016-10-06T08:44:56Z"
"","1975","HOTFIX: Cannot Stop with 'kafka-server-stop.sh'","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I added 'w' option in 'ps' command.","closed","","Mabin-J","2016-10-06T01:42:11Z","2016-10-06T02:44:28Z"
"","2017","KAFKA-4297: fix possiblity that didn't stop with shell (solution 1)","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I added '-Dkafka.server' option in running command and modified stop script.","open","","Mabin-J","2016-10-13T04:18:43Z","2018-03-02T19:29:45Z"
"","1976","HOTFIX: Cannot Stop Kafka with Shell Script (Solution 1)","If Kafka's homepath is long, kafka cannot stop with 'kafka-server-stop.sh'.  That command showed this message:  ``` No kafka server to stop ```  This bug is caused that command line is too long like this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka ```  but that is not all command line. Full command line is this.  ``` /home/bbdev/Amasser/etc/alternatives/jre/bin/java -Xms1G -Xmx5G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true -Xloggc:/home/bbdev/Amasser/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/home/bbdev/Amasser/var/log/kafka -Dlog4j.configuration=file:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../config/log4j.properties -cp :/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/aopalliance-repackaged-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/argparse4j-0.5.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-api-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-file-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-json-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/connect-runtime-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/guava-18.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-api-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-locator-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/hk2-utils-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-annotations-2.6.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-core-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-databind-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-base-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-jaxrs-json-provider-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jackson-module-jaxb-annotations-2.6.3.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javassist-3.18.2-GA.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.annotation-api-1.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.inject-2.4.0-b34.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/javax.ws.rs-api-2.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-client-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-common-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-container-servlet-core-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-guava-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-media-jaxb-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jersey-server-2.22.2.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-continuation-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-http-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-io-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-security-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-server-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlet-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-servlets-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jetty-util-9.2.15.v20160210.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/jopt-simple-4.9.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka_2.11-0.10.0.1-test-sources.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-clients-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-log4j-appender-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-streams-examples-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/kafka-tools-0.10.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/log4j-1.2.17.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/lz4-1.3.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/metrics-core-2.2.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/reflections-0.9.10.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/rocksdbjni-4.8.0.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-library-2.11.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/scala-parser-combinators_2.11-1.0.4.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-api-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/slf4j-log4j12-1.7.21.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/snappy-java-1.1.2.6.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zkclient-0.8.jar:/home/bbdev/Amasser/etc/alternatives/kafka/bin/../libs/zookeeper-3.4.6.jar kafka.Kafka /home/bbdev/Amasser/etc/kafka/server.properties ```  So, I added '-Dkafka.server' option in running command and modified stop script.","closed","","Mabin-J","2016-10-06T04:20:35Z","2016-10-13T04:23:22Z"
"","2024","KAFKA-4300: NamedCache throws an NPE when evict is called and the cache is empty","If evict is called on a NamedCache and the cache is empty an NPE is thrown. This was reported on the user list from a developer running 0.10.1.","closed","","dguy","2016-10-13T16:57:48Z","2016-10-13T20:18:14Z"
"","1918","[WIP] allow Processors to write to State Stores during Processor.init()","If a Processor writes to a State Store in the init and 0 records are processed before `commit` is called an IllegalStateException is thrown. This is because the `RecordContext` has not been set on `ProcessorContext` and `RecordCollector` calls `context.timestamp()` when it is flushing the records to the changelog. This sets a dummy `RecordContext` with the timestamp set to `System.currentTimeMillis` prior to Processor init.","closed","","dguy","2016-09-27T14:13:27Z","2016-09-30T16:28:32Z"
"","2254","KAFKA-4537: StreamPartitionAssignor incorrectly adds standby partitions to the partitionsByHostState map","If a KafkaStreams app is using Standby Tasks then `StreamPartitionAssignor` will add the standby partitions to the partitionsByHostState map for each host. This is incorrect as the partitionHostState map is used to resolve which host is hosting a particular store for a key.  The result is that doing metadata lookups for interactive queries can return an incorrect host","closed","","dguy","2016-12-14T11:55:21Z","2016-12-16T00:50:35Z"
"","1605","KAFKA-3725: Update documentation with regards to XFS","I've updated the ops documentation with information on using the XFS filesystem, based on LinkedIn's testing (and subsequent switch from EXT4).  I've also added some information to clarify the potential risk to the suggested EXT4 options (again, based on my experience with a multiple broker failure situation).","closed","","toddpalino","2016-07-11T00:28:11Z","2016-07-11T08:04:17Z"
"","2298","KAFKA-4528: Fix failure in ProducerTest.testAsyncSendCanCorrectlyFailWithTimeout","I was able to reproduce the failure in less than 10 runs before the change. With the change, the test passed 70 times consecutively.","closed","","ijuma","2017-01-03T11:05:55Z","2017-09-05T09:47:39Z"
"","2132","add a space to separate two words","I think we should add a space here, otherwise the two words will join together. And replace the host string with a constant, otherwise when I need to modify the host, I need to modify several files.","closed","","ZhengQian1","2016-11-15T06:13:06Z","2016-11-16T19:36:46Z"
"","2093","MINOR: Add description of how consumer wakeup acts if no threads are awakened","I think the Javadoc should describe what happens if wakeup is called and no other thread is currently blocking. This may be important in some cases, e.g. trying to shut down a poll thread, followed by manually committing offsets.","closed","","srdo","2016-11-02T17:23:05Z","2016-11-04T07:38:56Z"
"","2257","MINOR: Reenable streams smoke test","I ran it 3 times and it works again.","closed","","enothereska","2016-12-14T20:50:26Z","2016-12-14T22:47:32Z"
"","1741","MINOR: Add more verbose logging for offset map building","I propose to make the offset map building a bit more verbose to make log cleaning messages a bit easier to follow, for people not much familiar with the code like myself.  Specifically, it's confusing when we get messages like these in the log:  ``` [2016-08-15 16:22:42,861] INFO Cleaner 0: Growing cleaner I/O buffers from 65536bytes to 131072 bytes. (kafka.log.LogCleaner) [2016-08-15 16:22:46,002] INFO Cleaner 0: Growing cleaner I/O buffers from 65536bytes to 131072 bytes. (kafka.log.LogCleaner) [2016-08-15 16:22:48,844] INFO Cleaner 0: Growing cleaner I/O buffers from 65536bytes to 131072 bytes. (kafka.log.LogCleaner) ```  When stumbling upon this the first time, these looked like a bug (why are buffers trying to grow into the same sizes, for the same thread?). Then, after checking the code, I realized these are for different segments.  This PR aims to make that a bit more clear.  Does this look good?","open","","eliasdorneles","2016-08-15T18:10:06Z","2018-03-02T19:29:39Z"
"","1557","Improved warning to be clear on first read.","I hope it will make searching for reason it is actually shown a bit cleaner.  Changed just text. Also fixed typo (server parameter is not called max.message.bytes, but it is message.max.bytes) Consult http://kafka.apache.org/documentation.html#configuration if needed.","closed","","LuboVarga","2016-06-27T06:24:16Z","2018-02-26T18:18:31Z"
"","2237","support scala 2.12 build","I have a scenario where we have a 0.8 Kafka Broker managed by a different org but I would like to upgrade my application to Scala 2.12. I know the 0.8 branch is no longer maintained and all I am looking for is a one-off publish of a scala 2.12 compatible jar from the head of the 0.8 branch.","closed","","pjfanning","2016-12-09T20:15:44Z","2017-04-13T12:15:18Z"
"","1729","Assign ConfigDef.NO_DEFAULT_VALUE as a literal, use equals() for comp…","I doubt there is a good reason for using `new String("""")` and `==` for equality comparison.","closed","","shikhar","2016-08-12T22:37:34Z","2016-08-13T06:38:54Z"
"","2331","KAFKA-4607: Validate the names of auto-generated internal topics","I considered catching errors to add further information about naming internal state stores. However, Topic.validate() will throw an error that prints the offending name, so I decided not to add too much complexity.","closed","","nixsticks","2017-01-06T23:27:22Z","2017-03-17T19:03:56Z"
"","1541","KAFKA-3892 prune metadata response to subscribed topics","I believe this will cause clients to defensively prune their cluster metadata in all cases. It doesn't address why a client without a Pattern subscription would receive a response containing all topics and partitions for the cluster (which is still undesirable, but I am guessing would require a fix for the broker.)  In my own testing, this restored the amount of heap required to 0.8 consumer levels.  I am concerned that I do not 100% understand all the uses of this class. My assumption is that only topics that have been added are expected in the response and that the two unit test modifications I needed to make were oversights.  I am also assuming that this behavior was only applied to the pattern matching case to avoid a small amount of (presumed) unnecessary work and not for correctness reasons.","closed","","iamnoah","2016-06-22T20:24:51Z","2016-06-22T20:49:52Z"
"","1558","org.apache.kafka.streams.errors.StreamsException: Failed to rebalance","I am running the below simple example in kafka streams and i got a weird exception which i cannot handle   ```       [ Properties props = new Properties();         props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""streams-pipe"");         props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""192.168.1.3:9092"");         props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());         props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());         // setting offset reset to earliest so that we can re-run the demo code with the same pre-loaded data         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");          KStreamBuilder builder = new KStreamBuilder();          builder.stream(""streams-file-input"").to(""streams-pipe-output"");          KafkaStreams streams = new KafkaStreams(builder, props);         streams.start();          // usually the stream application would be running forever,         // in this example we just let it run for some time and stop since the input data is finite.         Thread.sleep(5000L);          streams.close();](url)](url) ```  `Exception in thread ""StreamThread-1"" org.apache.kafka.streams.errors.StreamsException: Failed to rebalance at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:299)             at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)         Caused by: org.apache.kafka.streams.errors.ProcessorStateException: Error while creating the state manager`","closed","","l16h7n1n6s","2016-06-27T13:43:22Z","2016-07-23T22:13:45Z"
"","1705","KAFKA-3817: KTableRepartitionMap publish old Change first, for non-count aggregates","I affirm that the contribution is my original work and that I license the work to the project under the project's open source license.   This cleans up misbehaviour that was introduce while fixing KAFKA-3817. It is impossible for a non-count aggregate to be build, when the addition happens before the removal. IMHO making sure that these details are correct is very important.   This PR has local test errors. It somehow fails the ResetIntegrationTest. It doesn't quite appear to me why but it looks like this PR breaks it, especially because the error appears with the ordering of the events. Still I am unable to find where I could have broken it. Maybe not seems to fail on trunk aswell.","closed","","Kaiserchen","2016-08-04T08:39:19Z","2016-08-05T17:32:21Z"
"","1998","KAFKA-4281: Should be able to forward aggregation values immediately","https://issues.apache.org/jira/browse/KAFKA-4281","closed","streams,","gfodor","2016-10-09T22:48:50Z","2018-01-30T04:47:23Z"
"","1710","KAFKA-4025 - make sure file.encoding system property is set to UTF-8 when calling rat","https://issues.apache.org/jira/browse/KAFKA-4025 this patch sets the file.encoding system property to UTF-8 before invoking rat during the build process and resets it to the original value afterwards","closed","","radai-rosenblatt","2016-08-07T22:25:39Z","2016-10-12T23:29:24Z"
"","1581","KAFKA-3922: add constructor to AbstractStream class","https://issues.apache.org/jira/browse/KAFKA-3922  KAFKA-3922 add copy-constructor to AbstractStream class  This copy-constructor allow to access protected variables from subclasses.   It should be used to extend KStreamImpl and KTableImpl classes by implementing a decorator pattern.","closed","","fhussonnois","2016-07-03T16:34:56Z","2016-07-19T17:56:58Z"
"","1537","KAFKA-3846: KIP-65: include timestamp in Connect record types","https://cwiki.apache.org/confluence/display/KAFKA/KIP-65%3A+Expose+timestamps+to+Connect","closed","","shikhar","2016-06-22T00:44:44Z","2016-06-30T21:00:03Z"
"","1679","MINOR: replace reference to HoppingWindows in streams.html","HoppingWindows was removed prior to the 0.10.0 release. I've updated the doc to refer to the correct TimeWindows","closed","","dguy","2016-07-29T09:50:21Z","2016-07-30T12:35:11Z"
"","1482","MINOR: Fix a variable name semantically correct.","Hi all, This is my first commit to Kafka.  ""msec / 1000"" turns into sec, isn't it? I just have fixed a variable name. @granders","closed","","uchan-nos","2016-06-09T06:59:55Z","2016-06-12T00:17:36Z"
"","1835","[DOCS] KAFKA-4130: Link to Varnish architect notes is broken","Hi all,  The PR is related to   https://issues.apache.org/jira/browse/KAFKA-4130  Thanks, Andrea","closed","","oscerd","2016-09-08T10:15:22Z","2016-10-11T16:33:36Z"
"","1566","MINOR: bug fixes to ducktape services","Here's a (mostly successful) run with these changes:  http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2016-06-27--001.1467080884--alexlod--ducktape-fixes--ad85493/  At least one of the failed tests is failing in trunk, too:  http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2016-06-28--001.1467090978--alexlod--ducktape-fixes--ad85493/  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","alexlod","2016-06-28T16:35:36Z","2016-07-05T17:19:25Z"
"","1637","KAFKA-2311: Consumer's ensureNotClosed method not thread safe","Here is the patch on github @ijuma.  Acquiring the consumer lock (the single thread access controls) requires that the consumer be open. I changed the closed variable to be volatile so that another thread's writes will visible to the reading thread.  Additionally, there was an additional check if the consumer was closed after the lock was acquired. This check is no longer necessary.  This is my original work and I license it to the project under the project's open source license.","closed","","tbrooks8","2016-07-19T17:15:19Z","2016-09-13T03:31:37Z"
"","2267","KAFKA-4514: Add Codec for ZStandard Compression","Hello. This PR resolves [KAFKA-4514: Add Codec for ZStandard Compression](https://issues.apache.org/jira/browse/KAFKA-4514). Please have a look when you are free. Since I am a total newbie of Apache Kafka, feel free to point out the deficiencies.  Add to the feature itself, I have a question: **Should we support an option for ZStandard compression level?**  According to [ZStandard official documentation](https://github.com/facebook/zstd/blob/dev/lib/zstd.h), it supports compression level of 1 ~ 22. Because of that, [Hadoop added a new configuration option named ""io.compression.codec.zstd.level"", whose default value is 3](https://issues.apache.org/jira/browse/HADOOP-13578). In this PR, I configured the compression level to 1 as a temporary one but wondering following problems:  - Should we provide a configurable option? - Would it better to change the default value, from 1 to another one?  I am looking forward to your advice. Thanks.","closed","","dongjinleekr","2016-12-16T09:52:13Z","2018-11-19T09:11:19Z"
"","1739","KAFKA-4032: Uncaught exceptions when autocreating topics","handled by adding a catch all for any unhandled exception. Because the jira specifically mentions the InvalidReplicationFactor exception, a test was added for that specific case.","closed","","granthenke","2016-08-15T03:18:45Z","2016-08-22T23:15:39Z"
"","2433","Update kafka-run-class.bat","handle existing classpath with spaces list each lib dependency individually","closed","","jonfreedman","2017-01-25T12:46:29Z","2017-01-26T22:17:53Z"
"","2277","KAFKA-4527: task status was being updated before actual pause/resume","h/t @ewencp for pointing out the issue","closed","connect,","shikhar","2016-12-19T21:34:22Z","2020-10-16T06:37:34Z"
"","1758","KAFKA-4019: Update log cleaner to handle max message size of topics","Grow read and write buffers of cleaner up to the maximum message size of the log being cleaned if the topic has larger max message size than the default config of the broker.","closed","","rajinisivaram","2016-08-18T08:27:01Z","2016-10-04T03:24:37Z"
"","2051","KAFKA-4311: Multi layer cache eviction causes forwarding to incorrect ProcessorNode","Given a topology like the one below. If a record arriving in `tableOne` causes a cache eviction, it will trigger the `leftJoin` that will do a `get` from `reducer-store`. If the key is not currently cached in `reducer-store`, but is in the backing store, it will be put into the cache, and it may also trigger an eviction. If it does trigger an eviction and the eldest entry is dirty it will flush the dirty keys. It is at this point that a ClassCastException is thrown. This occurs because the ProcessorContext is still set to the context of the `leftJoin` and the next child in the topology is `mapValues`. We need to set the correct `ProcessorNode`, on the context, in the `ForwardingCacheFlushListener` prior to calling `context.forward`. We also need to  remember to reset the `ProcessorNode` to the previous node once `context.forward` has completed.  ```        final KTable one = builder.table(Serdes.String(), Serdes.String(), tableOne, tableOne);         final KTable two = builder.table(Serdes.Long(), Serdes.String(), tableTwo, tableTwo);         final KTable reduce = two.groupBy(new KeyValueMapper>() {             @Override             public KeyValue apply(final Long key, final String value) {                 return new KeyValue<>(value, key);             }         }, Serdes.String(), Serdes.Long())                 .reduce(new Reducer() {..}, new Reducer() {..}, ""reducer-store"");      one.leftJoin(reduce, new ValueJoiner() {..})         .mapValues(new ValueMapper() {..});  ```","closed","","dguy","2016-10-21T15:19:47Z","2016-11-09T18:44:05Z"
"","1826","KAFKA-4129: Processor throw exception when getting channel remote address after closing the channel","Get channel remote address before calling `channel.close`","closed","","xiaotao183","2016-09-06T13:19:36Z","2016-09-07T00:18:10Z"
"","2382","KAFKA-4617: Generate core project with correct source folders.","Generate core project with correct source folders. In addition set output folders same as command line build. Don't generate unnecessary projects (https://issues.apache.org/jira/browse/KAFKA-4604)","closed","","dhwanikatagade","2017-01-15T17:17:26Z","2017-02-10T06:00:27Z"
"","1483","KAFKA-3799: Enable SSL endpoint validation in system tests","Generate certificates with hostname in SubjectAlternativeName and enable hostname validation.","closed","","rajinisivaram","2016-06-09T07:02:11Z","2016-09-01T07:36:30Z"
"","1594","KAFKA-3931: Fix transient failures in pattern subscription tests","Full credit for figuring out the cause of these failures goes to @hachikuji.","closed","","vahidhashemian","2016-07-06T21:27:01Z","2016-07-12T18:32:15Z"
"","1484","KAFKA-3810: replication of internal topics should not be limited by replica.fetch.max.bytes","From the kafka-dev mailing list discussion: [[DISCUSS] scalability limits in the coordinator](http://mail-archives.apache.org/mod_mbox/kafka-dev/201605.mbox/%3CCAMQuQBZDdtAdhcgL6h4SmTgO83UQt4s72gc03B3VFghnME3FTA@mail.gmail.com%3E)  There's a scalability limit on the new consumer / coordinator regarding the amount of group metadata we can fit into one message. This restricts a combination of consumer group size, topic subscription sizes, topic assignment sizes, and any remaining member metadata.  Under more strenuous use cases like mirroring clusters with thousands of topics, this limitation can be reached even after applying gzip to the __consumer_offsets topic.  Various options were proposed in the discussion: 1. Config change: reduce the number of consumers in the group. This isn't always a realistic answer in more strenuous use cases like MirrorMaker clusters or for auditing. 2. Config change: split the group into smaller groups which together will get full coverage of the topics. This gives each group member a smaller subscription.(ex: g1 has topics starting with a-m while g2 has topics starting with n-z). This would be operationally painful to manage. 3. Config change: split the topics among members of the group. Again this gives each group member a smaller subscription. This would also be operationally painful to manage. 4. Config change: bump up KafkaConfig.messageMaxBytes (a topic-level config) and KafkaConfig.replicaFetchMaxBytes (a broker-level config). Applying messageMaxBytes to just the __consumer_offsets topic seems relatively harmless, but bumping up the broker-level replicaFetchMaxBytes would probably need more attention. 5. Config change: try different compression codecs. Based on 2 minutes of googling, it seems like lz4 and snappy are faster than gzip but have worse compression, so this probably won't help. 6. Implementation change: support sending the regex over the wire instead of the fully expanded topic subscriptions. I think people said in the past that different languages have subtle differences in regex, so this doesn't play nicely with cross-language groups. 7. Implementation change: maybe we can reverse the mapping? Instead of mapping from member to subscriptions, we can map a subscription to a list of members. 8. Implementation change: maybe we can try to break apart the subscription and assignments from the same SyncGroupRequest into multiple records? They can still go to the same message set and get appended together. This way the limit become the segment size, which shouldn't be a problem. This can be tricky to get right because we're currently keying these messages on the group, so I think records from the same rebalance might accidentally compact one another, but my understanding of compaction isn't that great. 9. Implementation change: try to apply some tricks on the assignment serialization to make it smaller. 10. Config and Implementation change: bump up the __consumer_offsets topic messageMaxBytes and (from Jun Rao) fix how we deal with the case when a message is larger than the fetch size. Today, if the fetch size is smaller than the fetch size, the consumer will get stuck. Instead, we can simply return the full message if it's larger than the fetch size w/o requiring the consumer to manually adjust the fetch size. 11. Config and Implementation change: same as above but only apply the special fetch logic when fetching from internal topics  This PR provides an implementation of option 11.  That being said, I'm not very happy with this approach as it essentially doesn't honor the ""replica.fetch.max.bytes"" config. Better alternatives are definitely welcome!","closed","","onurkaraman","2016-06-09T09:51:41Z","2016-06-19T18:42:42Z"
"","2432","KAFKA-3502: KStreamTestDriver needs to be closed after the test case","Found a few recently added unit tests did not close KStreamTestDriver after the test itself is closed; this can cause RocksDB virtual function called if the contained topology has some persistent store since they will be initialized but not closed in time.  MINOR fix: found that when closing KStreamTestDriver, we need to first flushing all stores before closing any of them; this is triggered from the `KTableKTableLeftJoin.shouldNotThrowIllegalStateExceptionWhenMultiCacheEvictions`.  MINOR fix: in CachingXXXStore, the `name` field is actually used as the cache's namespace, not really the store name or its corresponding topic name. Fixed it by renaming it to `cacheName` and use `this.name()` elsewhere which will call the underlying store's name.","closed","","guozhangwang","2017-01-25T00:14:46Z","2017-07-15T22:07:27Z"
"","1974","AN-75711 Spike for Kafka server side changes to prevent a broker from being leader","For basic proof of concept,  1. Added Kafka configuration to blacklist a specific Kafka broker from becoming a leader for any partition. 2. The blacklisted broker is passed down the chain to all different PartitionLeaderSelectors and the selectors prevent that broker from being elected as leader during the partition state changes.  Notes from JIRA below (added by Jason) -  Potential Issues: - Leader value for the partition in Zookeeper does not null out (i.e. it remains 1 even if the hot broker with id 1 goes down).  May not be a problem. - When creating a topic, the initial leader may be the prohibited one.  Need to figure out how this works, but shouldn't matter for Komodo as we guarantee that the hot node is the leader before we do anything. - Not 100% positive that there are no ripple effects, but relatively sure.   What's needed before we propose it to the community: - Mark a broker as ineligible and have it set state in Zookeeper - LeaderSelector should use that state in Zookeeper to filter leader selection instead of the configuration","closed","","silpamittapalli","2016-10-05T20:47:47Z","2016-10-06T13:23:42Z"
"","2365","MINOR: avoid closing over both pre & post-transform record in WorkerSourceTask","Followup to #2299 for KAFKA-3209","closed","","shikhar","2017-01-13T00:36:26Z","2017-01-13T02:45:32Z"
"","1964","KAFKA-4010: add ConfigDef toEnrichedRst() for additional fields in output","followup on https://github.com/apache/kafka/pull/1696  cc @rekhajoshm","closed","","shikhar","2016-10-04T21:27:17Z","2016-10-11T03:02:39Z"
"","1854","MINOR: Give a name to the coordinator heartbeat thread","Followed the same naming pattern as the producer sender thread.","closed","","ijuma","2016-09-14T15:57:57Z","2016-09-14T16:17:19Z"
"","1580","Hotfix: auto-repartitioning for merge() and code simplifications","follow-up to auto-through feature: - add sourceNode to transform() - enable auto-repartitioning in merge() - null check not required anymore (always join-able due to auto-through)","closed","","mjsax","2016-07-01T19:57:01Z","2016-07-05T17:22:34Z"
"","1507","KAFKA-3840; Allow clients default OS buffer sizes","Follow up on KAFKA-724 (#1469) to allow OS socket buffer sizes auto tuning for both the broker and the clients.","closed","","slaunay","2016-06-14T22:33:04Z","2016-06-16T12:39:08Z"
"","1967","cherry-picking KAFKA-4244 to 0.10.1.0 branch","fixing few minor conflicts","closed","","gwenshap","2016-10-04T22:13:55Z","2016-10-13T22:59:38Z"
"","2120","KAFKA-4395: Break static initialization order dependency between KafkaConfig and Logconfig","Fixes static initialization order dependency between KafkaConfig and LogConfig. @jjkoshy please take a look.","closed","","sutambe","2016-11-09T18:29:01Z","2016-11-23T11:12:35Z"
"","1593","KAFKA-3857 Additional log cleaner metrics","Fixes KAFKA-3857  Changes proposed in this pull request:  The following additional log cleaner metrics have been added. 1. num-runs: Cumulative number of successful log cleaner runs since last broker restart. 2. last-run-time: Time of last log cleaner run. 3. num-filthy-logs: Number of filthy logs. A non zero value for an extended period of time indicates that the cleaner has not been successful in cleaning the logs.  A note on num-filthy-logs: It is incremented whenever a filthy topic partition is added to inProgress HashMap. And it is decremented once the cleaning is successful, or if the cleaning is aborted. Note that the existing LogCleaner code does not provide a metric to check if the clean operation is successful or not. There is an inProgress HashMap with topicPartition  => LogCleaningInProgress entries in it, but the entries are removed from the HashMap even when clean operation throws an exception. So, added an additional metric num-filthy-logs, to differentiate between a successful log clean case and an exception case.  The code is ready. I have tested and verified JMX metrics. There is one case I couldn't test though. It's the case where numFilthyLogs is decremented in 'resumeCleaning(...)' in LogCleanerManager.scala Line 188. It seems to be a part of the workflow that aborts the cleaning of a particular partition. Any ideas on how to test this scenario?","closed","","kiranptivo","2016-07-06T18:35:37Z","2017-01-14T00:43:42Z"
"","2378","KAFKA-3857 Additional log cleaner metrics","Fixes KAFKA-3857  Changes proposed in this pull request:  An additional log cleaner metric has been added: time-since-last-run-ms: Time since the last log cleaner run, in milliseconds.  This metric would be reset to 0 every time log cleaner thread runs. If this metric keeps constantly increasing, it indicates that the log cleaner thread is not alive.  If you are creating alerts around log cleaner, you could monitor this metric. A high ""time-since-last-run-ms"" value (eg: 600000) indicates that the log cleaner hasn't been running since the last 10 minutes.  The code has been tested. JMX metric has been verified.  Note: This pull request is a continuation of the following pull request.  PR#1593 was quite old and I had some trouble rebasing it. Decided to start a fresh PR.  https://github.com/apache/kafka/pull/1593/files/927b28cf41275874945beb7377f7f36c462f27c8#diff-ca1c127eee4b3c748ae73028f6abeab8","closed","","kiranptivo","2017-01-14T00:35:27Z","2017-01-14T21:34:31Z"
"","1984","mods grep target to find kafka process, fixes KAFKA-4264","Fixes issue described here: https://issues.apache.org/jira/browse/KAFKA-4264","open","","davemcphee","2016-10-06T17:44:39Z","2018-03-06T12:16:22Z"
"","2399","KAFKA-4596: Throttled Replica Reassignment Error","Fixes a logic error in the Reassignment process which throws an exception if you don't rebalance all partitions.","closed","","benstopford","2017-01-18T17:14:34Z","2017-01-25T13:48:52Z"
"","1663","KAFKA-3935: Fix test_restart_failed_task system test for SinkTasks","Fix the test by using a more liberal timeout and forcing more frequent SinkTask.put() calls. Also add some logging to aid future debugging.","closed","","ewencp","2016-07-26T00:02:37Z","2016-07-26T02:02:36Z"
"","2421","HOTFIX: Fix cmd line argument order in streams system tests","Fix the ordering of cmd line arguments passed to the system tests.","closed","","dguy","2017-01-23T12:49:52Z","2017-02-17T00:58:33Z"
"","1496","MINOR: Fix connect development guide.","Fix some trivial misses. @ewencp","closed","","uchan-nos","2016-06-11T08:12:40Z","2016-06-12T00:20:49Z"
"","2200","KAFKA-4472: offsetRetentionMs miscalculated in GroupCoordinator","Fix possible integer overflow","closed","","kichristensen","2016-12-01T23:15:09Z","2016-12-06T11:55:55Z"
"","2210","KAFKA-4451: Fix OffsetIndex overflow when replicating a highly compacted topic. OffsetIndex provides checks for overflow, used when deciding to roll a LogSegment","Fix OffsetIndex overflow when replicating a highly compacted topic. https://issues.apache.org/jira/browse/KAFKA-4451","closed","","michaelschiff","2016-12-04T22:29:12Z","2016-12-15T23:47:15Z"
"","2109","MINOR: fix incorrect logging in StreamThread","Fix incorrect logging when unable to create an active task. The output was: Failed to create an active task %s:  It should have the taskId.","closed","","dguy","2016-11-07T15:18:47Z","2016-11-09T19:23:08Z"
"","2324","KAFKA-4604: Fix gradle build to avoid generating unnecessary projects","Fix gradle build to avoid generating unnecessary projects for non project sub folders. The unnecessary projects cause unwanted file deletes in git when these projects are cleaned in eclipse.","closed","","dhwanikatagade","2017-01-06T10:19:41Z","2017-01-15T17:19:41Z"
"","1843","KAFKA-4131 :Multiple Regex KStream-Consumers cause Null pointer exception","Fix for bug outlined in KAFKA-4131","closed","","bbejeck","2016-09-10T01:42:02Z","2016-09-16T00:08:36Z"
"","1860","KAFKA-4055: System tests for secure quotas","Fix existing client-id quota test which currently don't configure quota overrides correctly. Add new tests for user and (user, client-id) quota overrides and default quotas.","closed","","rajinisivaram","2016-09-15T10:55:48Z","2016-09-26T00:02:23Z"
"","2467","MINOR: Use API hyperlinks in 'Kafka Protocol Guide' to facilitate navigation","Finding the protocol associated with an API key can be a challenge in the lengthy [web page](http://kafka.apache.org/protocol.html#protocol_api_keys). Adding hyperlinks would definitely help with that.  Co-authored with @imandhan.","closed","","vahidhashemian","2017-01-30T18:34:08Z","2017-02-26T02:45:52Z"
"","2047","Fix vagrant rsync exclude argument","Existing VMs will need to be re-provisioned or re-created to pick up this change.  Reference docs: https://www.vagrantup.com/docs/synced-folders/rsync.html","closed","","edenhill","2016-10-20T20:25:33Z","2016-10-20T23:19:05Z"
"","2447","KAFKA-4646: Improve test coverage AbstractProcessorContext","Exception paths in `register()`, `topic()`, `partition()`, `offset()`, and `timestamp()`, were not covered by any existing tests","closed","","dguy","2017-01-26T15:10:33Z","2017-02-17T00:58:08Z"
"","2059","KAFKA-4333: Report coordinator id of consumer groups with consumer group '--list' option","Enhance the output of the consumer group command and report the coordinator id of each group (for Java API based consumers) when `--list` option is used.","closed","","vahidhashemian","2016-10-24T23:15:51Z","2017-12-22T22:58:43Z"
"","1983","client fails trying to bind to jmx port already in use by server","Enabling JMX_PORT causes Kafka clients to attempt to bind to a port already bound by server. Modified so as to distinguish if being called to launch server or client and choose accordingly whether to add JMX port option or not.","open","","edi-bice","2016-10-06T15:33:45Z","2019-12-25T14:35:46Z"
"","2084","KAFKA-4361: Streams does not respect user configs for ""default"" params","Enable user provided consumer and producer configs to override the streams default configs.","closed","","dguy","2016-11-01T10:20:19Z","2016-11-02T17:22:40Z"
"","1845","KAFKA-4163: NPE in StreamsMetadataState during re-balance operations","During rebalance operations the Cluster object gets set to Cluster.empty(). This can result in NPEs when doing certain operation on StreamsMetadataState. This should throw a StreamsException if the Cluster is empty as it is not yet (re-)initialized","closed","","dguy","2016-09-12T13:17:17Z","2016-09-19T18:01:35Z"
"","2266","KAFKA-4540: Suspended tasks that are not assigned to the StreamThread need to be closed before new active and standby tasks are created","During `onPartitionsAssigned` first close, and remove, any suspended `StandbyTasks` that are no longer assigned to this consumer.","closed","","dguy","2016-12-16T08:56:23Z","2016-12-20T18:36:39Z"
"","1847","KAFKA-4079: Documentation for secure quotas","Details in KIP-55.","closed","","rajinisivaram","2016-09-13T09:00:11Z","2016-09-19T19:17:22Z"
"","2188","KAFKA-4455; ensure tasks are closed after CommitFailedException","Details can be found in the task: [KAFKA-4455](https://issues.apache.org/jira/browse/KAFKA-4455)  Some discussion was also here: [stream shut down due to no locks for state store](https://groups.google.com/forum/#!topic/confluent-platform/i5cwYhpUtx4)  @dguy @enothereska chek it out.","closed","","dpoldrugo","2016-11-29T12:42:10Z","2018-02-25T21:19:23Z"
"","1683","KAFKA-2063: Add possibility to bound fetch response size","Details are: 1. Protocol is extended with new version of FetchRequest with extra parameter limit_bytes. 2. New config setting fetch.limit.bytes is added. It is set to zero by default (which means ""no limit""), so we preserve old behaviour by default 3. When broker receives FetchRequest with limit_bytes != 0, it performs random shuffle of partitions before reading is started. This way we can ensure that no starvation can happen for ""slow"" partitions 4. For each partition we read up to max_bytes requested for this partition, even if current value of limit_bytes is greater than zero but less than max_bytes. This way we can properly handle case when next message in given partition is larger than current value of limit_bytes.","closed","","nepal","2016-07-29T16:11:45Z","2016-08-31T08:04:08Z"
"","1934","KAFKA-4206 Improve handling of invalid credentials to mitigate DOS issue","Delay closing channels for connections where a SALException has been thrown.  This PR is a proof of concept and would like to stimulate feedback. This same approach has been used successfully in IBM MessageHub and **proved** capable of reducing dramatically the impact of SSL connections with wrong SASL credentials. Without this patch, a lot of cpu time is dedicated to SSL handshakes, many network threads are busy and the overall latencies suffer for already authenticated clients.  this PR has been codeveloped with @mimaison","closed","","edoardocomar","2016-09-29T16:21:43Z","2021-11-04T10:48:29Z"
"","2317","kafka-4592: Kafka Producer Metrics Invalid Value","Default constructor of Max assigns negative infinity as the init value. For the record size and like, value of zero is more reasonable.","closed","","huxihx","2017-01-05T14:22:06Z","2017-05-31T01:07:08Z"
"","1962","KAFKA-4251: fix test driver not launching in Vagrant 1.8.6","custom ip resolver in test driver makes incorrect assumption when calling vm.communicate.execute, causing driver to fail launching with Vagrant 1.8.6, due to https://github.com/mitchellh/vagrant/pull/7676","closed","","xvrl","2016-10-04T18:56:36Z","2016-10-04T21:45:50Z"
"","2173","KAFKA-4449: Add Serializer/Deserializer for POJO","Currently, there are only build-in serializer/deserializer for basic data type (String, Long, etc). It's better to have serializer/deserializer for POJO.  If we had this, user can serialize/deserialize all of their POJO with it. Otherwise, user may need to create e pair of serializer and deserializer for each kind of POJO, just like the implementation in the stream example PageViewTypeDemo https://github.com/apache/kafka/blob/trunk/streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java  Let's take above streams-example as an example, Serde was created for PageView as below           final Serializer pageViewSerializer = new JsonPOJOSerializer<>();         serdeProps.put(""JsonPOJOClass"", PageView.class);         pageViewSerializer.configure(serdeProps, false);          final Deserializer pageViewDeserializer = new JsonPOJODeserializer<>();         serdeProps.put(""JsonPOJOClass"", PageView.class);         pageViewDeserializer.configure(serdeProps, false);          final Serde pageViewSerde = Serdes.serdeFrom(pageViewSerializer, pageViewDeserializer);   If we use this POJO serializer/deserializer, the Serde can be created with only one line          Serdes.serdeFrom(PageView.class)","closed","streams,","habren","2016-11-26T14:34:19Z","2019-09-24T18:45:43Z"
"","1698","KAFKA-3999: Record raw size of fetch responses as part of consumer metrics","Currently, only the decompressed size of fetch responses is recorded. This PR adds a sensor to keep track of the raw size as well.","closed","","vahidhashemian","2016-08-02T23:12:02Z","2017-09-21T23:58:06Z"
"","1609","Minor: Improve PartitionState logging and remove duplication of code","Currently, logs involving PartitionState are not very helpful.  ```     Broker 449 cached leader info org.apache.kafka.common.requests.UpdateMetadataRequest$PartitionState@3285d64a for partition - in response to UpdateMetadata request sent by controller 356 epoch 138 with correlation id 0      TRACE state.change.logger: Broker 449 received LeaderAndIsr request org.apache.kafka.common.requests.LeaderAndIsrRequest$PartitionState@66d6a8eb correlation id 3 from controller 356 epoch 138 for partition [,] ```","closed","","SinghAsDev","2016-07-11T20:34:48Z","2016-07-21T00:01:12Z"
"","2471","KAFKA-4317: Regularly checkpoint StateStore changelog offsets","Currently the checkpoint file is deleted at state store initialization and it is only ever written again during a clean shutdown. This can result in significant delays during restarts as the entire store needs to be loaded from the changelog.  We can mitigate against this by frequently checkpointing the offsets. The checkpointing happens only during the commit phase, i.e, after we have manually flushed the store and the producer. So we guarantee that the checkpointed offsets are never greater than what has been flushed.  In the event of hard failure we can recover by reading the checkpoints and consuming from the stored offsets.","closed","","dguy","2017-01-31T17:34:39Z","2017-03-30T11:10:50Z"
"","1543","KAFKA-3890 Kafka Streams: task assignment is not maintained on cluster restart or rolling restart","Current task assignment in TaskAssignor is not deterministic.  During cluster restart or rolling restart, we have the same set of participating worker nodes.  But the current TaskAssignor is not able to maintain a deterministic mapping, so about 20% partitions will be reassigned which would cause state repopulation. When the topology of work nodes (# of worker nodes, the TaskIds they are carrying with) is not changed, we really just want to keep the old task assignment.  Add the code to check whether the node topology is changing or not: - when the prevAssignedTasks from the old clientStates is the same as the new task list - when there is no new node joining (its prevAssignTasks would be either empty or conflict with some other nodes) - when there is no node dropping out (the total of prevAssignedTasks from other nodes would not be equal to the new task list)  When the topology is not changing, we would just use the old mapping.  I also added the code to check whether the previous assignment is balanced (whether each node's task list is within [1/2 average -- 2 \* average]), if it's not balanced, we will still start the a new task assignment.","closed","","HenryCaiHaiying","2016-06-22T23:14:25Z","2016-07-11T11:42:27Z"
"","1538","KAFKA-3890 Kafka Streams: task assignment is not maintained on cluster restart or rolling restart","Current task assignment in TaskAssignor is not deterministic.  During cluster restart or rolling restart, we have the same set of participating worker nodes.  But the current TaskAssignor is not able to maintain a deterministic mapping, so about 20% partitions will be reassigned which would cause state repopulation. When the topology of work nodes (# of worker nodes, the TaskIds they are carrying with) is not changed, we really just want to keep the old task assignment.  Add the code to check whether the node topology is changing or not: - when the prevAssignedTasks from the old clientStates is the same as the new task list - when there is no new node joining (its prevAssignTasks would be either empty or conflict with some other nodes) - when there is no node dropping out (the total of prevAssignedTasks from other nodes would not be equal to the new task list)  When the topology is not changing, we would just use the old mapping.  I also added the code to check whether the previous assignment is balanced (whether each node's task list is within [1/2 average -- 2 \* average]), if it's not balanced, we will still start the a new task assignment.","closed","","HenryCaiHaiying","2016-06-22T02:24:13Z","2016-06-22T23:12:25Z"
"","2158","KAFKA-4432: Added support to supply custom message payloads to perf-producer script.","Current implementation of ProducerPerformance creates static payload. This is not very useful in testing compression or when you want to test with production/custom payloads. So, we decided to add support for providing payload file as an input to producer perf test script.  We made the following changes: 1. Added support to provide a payload file which can have the list of payloads that you actually want to send. 2. Moved payload generation inside the send loop for cases when payload file is provided.  Following are the changes to how the producer-performance is evoked: 1. You must provide ""--record-size"" or ""--payload-file"" but not both. This is because, record size cannot be guaranteed when you are using custom events.   e.g. ./kafka-producer-perf-test.sh --topic test_topic --num-records 100000 --producer-props bootstrap.servers=127.0.0.1:9092 acks=0 buffer.memory=33554432 compression.type=gzip batch.size=10240 linger.ms=10 --throughput -1 --payload-file ./test_payloads --payload-delimiter , 2. Earlier ""--record-size"" was a required config, now you must provide exactly one of ""--record-size"" or ""--payload-file"". Providing both will result in an error. 3. Support for an additional parameter ""--payload-delimiter"" has been added which defaults to ""\n""","closed","","SandeshKarkera","2016-11-22T15:58:35Z","2017-01-20T03:02:47Z"
"","2472","KAFKA-3265: Create Java Admin Client","Create a Java AdminClient as described in KAFKA-3265 and KIP-4.  Currently it supports getAllGroups, getAllBrokerVersions, deleteTopics, and createTopics.  For each API, it uses the associated Kafka requests rather than going through ZooKeeper.  It is called AdministrativeClient to avoid confusing with the scala.kafka.admin.AdminClient class.","closed","","cmccabe","2017-01-31T18:47:53Z","2019-05-20T18:42:09Z"
"","2294","KAFKA-4535: http://kafka.apache.org/quickstart Step 8  missing  argument","consumer missing argument This patch corrects the error. Signed-off-by: aurora777","closed","","auroraxlh","2016-12-28T09:05:44Z","2017-01-05T01:30:36Z"
"","2058","kafka-4295: ConsoleConsumer does not delete the temporary group in zookeeper","ConsoleConsumer does not delete the temporary group in zookeeper Author: huxi_2b@hotmail.com Since consumer stop logic and zk node removal code are in separate threads, so when two threads execute in an interleaving manner, persistent node '/consumers/' might not be removed for those console consumer groups which do not specify ""group.id"". This will pollute Zookeeper with lots of inactive console consumer offset information.","closed","","huxihx","2016-10-24T23:09:50Z","2016-10-28T13:10:22Z"
"","2221","MINOR: fix metric collection NPE during shutdown","collecting socket server metrics during shutdown may throw NullPointerException","closed","","xvrl","2016-12-07T01:17:49Z","2016-12-09T00:27:09Z"
"","2285","KAFKA-4426: Add close with timeout for KafkaConsumer","Code corresponding to KIP-102.","closed","","rajinisivaram","2016-12-21T12:30:48Z","2017-01-12T02:31:25Z"
"","1536","MINOR: KAFKA-3176 follow-up to fix minor issues","Co-authored with @ijuma.","closed","","vahidhashemian","2016-06-21T17:53:40Z","2016-06-22T08:22:46Z"
"","2336","MINOR: Fix small error in javadoc for persistent Stores","Closed the last PR I made for this because I accidentally borked it with my other PR. Small error; I figure this is from copy-pasting the above doc","closed","","nixsticks","2017-01-09T19:30:55Z","2017-01-10T06:54:33Z"
"","2430","MINOR: close state store in CachingSessionStoreTest","close the sessions store in `@After` to release rocksdb resources.","closed","","dguy","2017-01-24T18:14:57Z","2017-02-17T00:58:31Z"
"","1817","KAFKA-3703: Flush outgoing writes before closing client selector","Close client connections only after outgoing writes complete or timeout.","closed","","rajinisivaram","2016-09-02T08:15:09Z","2016-09-07T23:34:41Z"
"","2235","KAFKA-4516: When a CachingStateStore is closed it should clear its associated NamedCache","Clear and remove the NamedCache from the ThreadCache when a CachingKeyValueStore or CachingWindowStore is closed. Validate that the store is open when doing any queries or writes to Caching State Stores.","closed","","dguy","2016-12-09T17:33:39Z","2016-12-13T02:00:48Z"
"","1872","KAFKA-4183: centralize checking for optional and default values to avoid bugs","Cleaner to just check once for optional & default value from the `convertToConnect()` function.  It also helps address an issue with conversions for logical type schemas that have default values and null as the included value. That test case is _probably_ not an issue in practice, since when using the `JsonConverter` to serialize a missing field with a default value, it will serialize the default value for the field. But in the face of JSON data streaming in from a topic being [generous on input, strict on output](http://tedwise.com/2009/05/27/generous-on-input-strict-on-output) seems best.","closed","connect,","shikhar","2016-09-16T23:25:51Z","2020-10-16T06:29:08Z"
"","2046","MINOR: Improve the help doc of consumer group command","Clarify the consumer group command help message around `zookeeper`, `bootstrap-server`, and `new-consumer` options.","closed","","vahidhashemian","2016-10-20T18:45:30Z","2017-05-24T16:15:53Z"
"","1792","KAFKA-3595: window stores use compact,delete config for changelogs","changelogs of window stores now configure cleanup.policy=compact,delete with retention.ms set to window maintainMs + StreamsConfig.WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG StoreChangeLogger produces messages with context.timestamp().","closed","","dguy","2016-08-26T08:43:02Z","2016-09-08T01:02:57Z"
"","1970","KAFKA-4253: Fix Kafka Stream thread shutting down process ordering","Changed the ordering in `StreamThread.shutdown` 1. commitAll (we need to commit so that any cached data is flushed through the topology) 2. close all tasks 3. producer.flush() - so any records produced during close are flushed and we have offsets for them 4. close all state managers 5. close producers/consumers 6. remove the tasks  Also in `onPartitionsRevoked` 1. commitAll 2. close all tasks 3. producer.flush 4. close all state managers","closed","","dguy","2016-10-05T10:08:23Z","2016-10-06T16:51:51Z"
"","1876","Matched method comment argument name to actual argument name","Changed the lowerBound argument reference in the summary comment of the translateOffset method to match the actual argument name: startingFilePosition.","closed","","lukezaparaniuk","2016-09-18T10:09:22Z","2016-09-18T22:02:33Z"
"","1702","Kafka 3940: Log should check the return value of dir.mkdirs()","Changed occurrences of dir.mkdirs() to Files.createDirectory and similarly for File.delete to Files.delete","closed","","imandhan","2016-08-03T21:31:13Z","2016-08-10T22:19:51Z"
"","1942","KAFKA-4233: StateDirectory fails to create directory if any parent directory does not exist","Change the creation of the directories, in the StateDirectory constructor, to use mkdirs so any parents get created. Throw an exception if the directory doesn't exist and couldn't be created","closed","","dguy","2016-09-30T10:24:08Z","2016-09-30T18:56:51Z"
"","1795","KAFKA-3129: Console producer issue when request-required-acks=0","change console producer default acks to 1, update acks docs.  Also added the -1 config to the acks docs since that question comes up often.  @ijuma and @vahidhashemian, does this look reasonable to you?","closed","","cotedm","2016-08-29T12:47:22Z","2016-09-07T16:09:34Z"
"","1742","KAFKA-4015: Change cleanup.policy config to accept a list of valid policies","Change cleanup.policy to accept a comma separated list of valid policies. Updated LogCleaner.CleanerThread to also run deletion for any topics configured with compact,delete. Ensure Log.deleteSegments only runs when delete is enabled. Additional Integration and unit tests to cover new option","closed","","dguy","2016-08-16T08:53:11Z","2016-08-25T15:16:40Z"
"","2260","KAFKA-4529; LogCleaner should not delete the tombstone too early.","cc @junrao","closed","","becketqin","2016-12-15T05:55:49Z","2016-12-15T19:15:34Z"
"","1873","KAFKA-4184:  Intermitant failures in ReplicationQuotasTest.shouldBootstrapTwoBrokersWithFollowerThrottle","Build is unstable, so it's hard to validate this change. Of the various builds up until 11am BST the test ran twice and passed twice.","closed","","benstopford","2016-09-17T10:12:35Z","2016-09-20T14:40:06Z"
"","1644","MINOR: Update to Gradle 2.14.1","Better performance is always welcome:  ""The Gradle build itself has seen a 50% reduction in configuration time. You'll see the biggest impact on multi-project builds""","closed","","ijuma","2016-07-20T23:42:46Z","2016-07-22T04:33:25Z"
"","2299","KAFKA-3209: KIP-66: single message transforms","Besides API and runtime changes, this PR also includes 2 data transformations (`InsertField`, `HoistToStruct`) and 1 routing transformation (`TimestampRouter`).  There is some gnarliness in `ConnectorConfig` / `ConfigDef` around creating, parsing and validating a dynamic `ConfigDef`.","closed","connect,","shikhar","2017-01-03T19:21:56Z","2020-10-16T06:37:34Z"
"","2044","Changes for supporting unsgined tinyint values","Based upon the theory provided in the official jdbc documentation, I have added a condition to include both Short and Byte as INT8 types in ConnectSchema. (https://docs.oracle.com/javase/6/docs/technotes/guides/jdbc/getstart/mapping.html) Here's the doc:  > 8.3.5 SMALLINT >  > The JDBC type SMALLINT represents a 16-bit signed integer value between -32768 and 32767. >  > The corresponding SQL type, SMALLINT, is defined in SQL-92 and is supported by all the major databases. The SQL-92 standard leaves the precision of SMALLINT up to the implementation, but in practice, all the major databases support at least 16 bits. >  > The recommended Java mapping for the JDBC SMALLINT type is as a Java short.  This would allow kafka-connect-jdbc to allow both signed and unsigned values. Currently it fails for unsigned values(i.e values > 127). This is specifically needed when fetching unsigned tinyint values from dbs using kafka-connect-jdbc. I have sent the PR to them and schema-registry as well to address the issue.","closed","connect,","vamossagar12","2016-10-19T18:08:05Z","2020-03-29T02:06:49Z"
"","2077","MINOR: Fix document header/footer links","Based on:  https://github.com/apache/kafka-site/pull/27  I recommend also merging into 10.1.0.0 branch to avoid mishaps when re-publishing docs.","closed","","gwenshap","2016-10-30T19:08:36Z","2016-12-21T22:36:56Z"
"","1660","KAFKA-3933: always fully read deepIterator","Avoids leaking native memory and hence crashing brokers on bootup due to running out of memory.  Seeeing as `messageFormat > 0` always reads the full compressed message set and is the default going forwards, we can use that behaviour to always close the compressor when calling `deepIterator`","closed","","tcrayford","2016-07-25T13:36:14Z","2016-07-26T11:38:34Z"
"","1614","KAFKA-3933: close deepIterator during log recovery","Avoids leaking native memory and hence crashing brokers on bootup due to running out of memory.  Introduces `kafka.common.ClosableIterator`, which is an iterator that can be closed, and changes the signature of `ByteBufferMessageSet.deepIterator` to return it, then changes the callers to always close the iterator.  This is a followup from https://github.com/apache/kafka/pull/1598 with more native memory leaks in the broker code found and fixed.","closed","","tcrayford","2016-07-12T13:33:46Z","2016-07-25T13:36:20Z"
"","1598","KAFKA-3933: close deepIterator during log recovery","Avoids leaking native memory and hence crashing brokers on bootup due to running out of memory.  Introduces `kafka.common.ClosableIterator`, which is an iterator that can be closed, and changes the signature of `ByteBufferMessageSet.deepIterator` to return it, then changes the caller `LogSegment` to always close the iterator.  https://issues.apache.org/jira/browse/KAFKA-3933","closed","","tcrayford","2016-07-08T12:47:30Z","2016-07-11T19:08:16Z"
"","1603","KAFKA-1429: Yet another deadlock in controller shutdown","Author: pengwei pengwei.li@huawei.com   Reviewers: NA","closed","","pengwei-li","2016-07-10T01:32:56Z","2016-11-24T00:59:37Z"
"","2175","KAFKA-4229:Controller can't start after several zk expired event","Author: pengwei   Reviewers: wangguoz.gmail.com","closed","","pengwei-li","2016-11-27T03:54:40Z","2017-01-25T00:08:58Z"
"","2278","KAFKA-4526 - Disable throttling test until it can be fixed correctly.","At present, the test is fragile in the sense that the console consumer has to start and be initialized before the verifiable producer begins producing in the produce-consume-validate loop.  If this doesn't happen, the consumer will miss messages at the head of the log and the test will fail.  At present, the consumer is considered inited once it has a PID. This is a weak assumption. The plan is to poll appropriate metrics (like partition assignment), and use those as a proxy for consumer initialization. That work will be tracked in a separate ticket. For now, we will disable the tests so that we can get the builds healthy again.","closed","","apurvam","2016-12-19T22:28:01Z","2016-12-19T23:42:40Z"
"","2064","KAFKA-4345: Run decktape test for each pull request","As of now the ducktape tests that we have for kafka are not run for pull request. We can run these test using travis-ci. Here is a sample run: https://travis-ci.org/raghavgautam/kafka/builds/170574293","closed","","raghavgautam","2016-10-25T22:42:37Z","2016-11-28T17:27:34Z"
"","1639","MINOR: Remove slf4j-log4j from kafka-streams compile dependencies","As kafka-streams is intended to be used by applications that may or may not wish to use log4j, kafka-streams itself should not have a dependency on a concrete log framework.  This change adapts the dependencies to be API-only for compile, and framework-specific for the test runtime only.  I read through the [Contributing Code Guidelines](https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes) and interpreted this as a trivial change that doesn't require a Jira ticket.  Please let me know if I've interpreted that wrongly.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","mfenniak","2016-07-19T22:12:59Z","2016-07-19T22:45:43Z"
"","1714","KAFKA-4011 - fix issues and beef up tests around ByteBoundedBlockingQueue","as discussed under KIP-72","closed","","radai-rosenblatt","2016-08-08T22:20:37Z","2016-11-14T21:49:01Z"
"","1725","KAFKA-3894: log cleaner can partially clean a segment","As discussed in https://issues.apache.org/jira/browse/KAFKA-3894, this PR makes the log cleaner do a ""partial"" clean on a segment, whereby it builds a partial offset map up to a particular offset in a segment. Once cleaning resumes again, we will continue from the next dirty offset, which can now be located in the middle of a segment.  Prior to this PR, segments with overly numerous keys could crash the log cleaner thread, as it was required that the log cleaner had to fit at least a single segment in the offset map.","closed","","tcrayford","2016-08-12T17:31:15Z","2016-08-22T18:08:44Z"
"","1517","[Kafka Streams] Clean-up script [WIP]","Anyof: @guozhangwang @enothereska @dguy @miguno   Two variants: 1. python client + bash script (seems not to be the right way, due to external dependencies) 2. java only (bash script to call the java program not included yet)","closed","","mjsax","2016-06-17T09:37:16Z","2016-07-19T12:46:13Z"
"","2363","MINOR: make methods introduced in KAFKA-4490 consistent with KIP-100","and remove some unnecessary @SuppressWarnings annotations","closed","","xvrl","2017-01-12T22:38:20Z","2017-01-13T01:39:06Z"
"","1528","KAFKA-3864: make field.get return field's default value when needed","And not the containing struct's default value.  The contribution is my original work and that I license the work to the project under the project's open source license.  @ewencp","closed","","rollulus","2016-06-20T17:13:22Z","2016-06-20T19:33:23Z"
"","2002","MINOR: Fix typos in documentation","And improve readability by adding proper punctuations.","closed","","vahidhashemian","2016-10-10T21:20:06Z","2016-10-10T22:58:56Z"
"","2334","subset of KAFKA-4353: Add uuid","Alternatively I am open to using Avro's 'Fixed' type with a 16 byte size. However most requests I have  received wish to see UUID represented as type String so  I went with that to start.   Also I just realized that Intellij's 'optimized imports' squashed the import list. Follow-up commit soon to follow with expanded import list","closed","","rnpridgeon","2017-01-09T11:57:57Z","2018-05-09T13:47:41Z"
"","2025","KAFKA-4293 - improve ByteBufferMessageSet.deepIterator() performance by relying on underlying stream's available() implementation","also: provided better available() for ByteBufferInputStream provided better available() for KafkaLZ4BlockInputStream added KafkaGZIPInputStream with a better available() fixed KafkaLZ4BlockOutputStream.close() to properly flush  Signed-off-by: radai-rosenblatt radai.rosenblatt@gmail.com","closed","","radai-rosenblatt","2016-10-13T19:50:03Z","2017-05-06T23:18:15Z"
"","1669","KAFKA-3996: ByteBufferMessageSet.writeTo() should be non-blocking","Also: - Introduce a blocking variant to be used by `FileMessageSet.append` - Add tests - Minor clean-ups","closed","","ijuma","2016-07-27T05:16:57Z","2016-07-27T14:52:20Z"
"","2095","KAFKA-2247: Merge kafka.utils.Time and kafka.common.utils.Time","Also: * Make all implementations of `Time` thread-safe as they are accessed from multiple threads in some cases. * Change default implementation of `MockTime` to use two separate variables for `nanoTime` and `currentTimeMillis` as they have different `origins`.","closed","","ijuma","2016-11-03T10:55:08Z","2016-12-02T14:14:22Z"
"","1774","KAFKA-4082: Upgrade to Gradle 3.0","Also upgrade scoverage (required for compatibility) and remove usage of `useAnt` which doesn't exist in Gradle 3.0  It turns out that one cannot even run `gradle` to download the project Gradle version if `useAnt` is used in the build. This is unfortunate (the SBT launcher has much saner behaviour).  Release notes: https://docs.gradle.org/3.0/release-notes","closed","","ijuma","2016-08-23T14:22:27Z","2016-08-24T01:00:38Z"
"","1610","KAFKA-3941: Delay eviction listener in InMemoryKeyValueLoggedStore after restoration","Also move the initialization that restores from changelog to inner stores.","closed","","guozhangwang","2016-07-11T22:37:12Z","2017-07-15T22:08:44Z"
"","1604","KAFKA-3887 Follow-up: add unit test for null checking in KTable aggregates","Also made a pass over the streams unit tests, with the following changes: 1. Removed three integration tests as they are already covered by other integration tests. 2. Merged `KGroupedTableImplTest` into `KTableAggregateTest`. 3. Use mocks whenever possible to reduce code duplicates.","closed","","guozhangwang","2016-07-10T05:07:44Z","2016-07-11T20:57:31Z"
"","1623","MINOR: Remove redundant clause in secureAclsEnabled check","Also include a few minor clean-ups.","closed","","ijuma","2016-07-14T08:38:15Z","2016-08-09T02:45:43Z"
"","1597","KAFKA-3817 Follow-up: Avoid forwarding old value if it is null in KTableRepartition","Also handle Null value in SmokeTestUtil.","closed","","guozhangwang","2016-07-07T18:51:40Z","2016-07-08T15:52:10Z"
"","1672","MINOR: More graceful handling of buffers that are too small in Record's `isValid` and `ensureValid`","Also add tests and make `Crc32.update` perform the same argument checks as `java.util.zip.CRC32`.","closed","","ijuma","2016-07-28T11:51:45Z","2016-09-07T00:35:18Z"
"","2446","KAFKA-4701: Add dynamic truststore manager.","Allow kafka brokers to dynamically reload truststore without restarting.","closed","","allenxiang","2017-01-26T14:49:48Z","2018-08-01T21:31:42Z"
"","1661","KAFKA-3987: Allow config of the hash algorithm used by the log cleaner","Allow configuration of the hash algorithm used by the Log Cleaner's offset map","open","","luafran","2016-07-25T14:49:54Z","2018-03-02T19:29:37Z"
"","2371","KAFKA-4588: Wait for topics to be created in QueryableStateIntegrationTest.shouldNotMakeStoreAvailableUntilAllStoresAvailable","After debugging this i can see the times that it fails there is a race between when the topic is actually created/ready on the broker and when the assignment happens. When it fails `StreamPartitionAssignor.assign(..)` gets called with a `Cluster` with no topics. Hence the test hangs as no tasks get assigned. To fix this I added a `waitForTopics` method to `EmbeddedKafkaCluster`. This will wait until the topics have been created.","closed","","dguy","2017-01-13T18:11:14Z","2017-01-17T20:56:45Z"
"","1950","MINOR: Add unit tests to the ReassignPartitionsCommand","Adds a bunch of tests to unit tests to the assignment command.  Moves the Rack aware test into its own class as it makes use of ZooKeeperTestHarness and slows everything else down.","closed","","benstopford","2016-10-03T12:43:24Z","2017-01-06T11:59:40Z"
"","1916","MINOR: add test to make sure ProcessorStateManager can handle State Stores with logging disabled","Adding the test so we know that the State Stores with logging disabled or without a topic don't throw any exceptions.","closed","","dguy","2016-09-27T08:52:44Z","2016-09-28T00:44:12Z"
"","1480","KAFKA-3762 Log.loadSegments() should log the message in exception","Adding an error logging message in Log.loadSegments() in the case when an index file corresponding to a log file exists but an exception is thrown.  Signed-off-by: Ishita Mandhan imandha@us.ibm.com","closed","","imandhan","2016-06-07T21:34:56Z","2016-06-14T18:55:02Z"
"","1796","KAFKA-4092: retention.bytes should not be allowed to be less than segment.bytes","adding a LogConfig value validator.  @gwenshap or @junrao would you mind taking a look?","closed","","cotedm","2016-08-29T18:03:54Z","2017-03-28T17:57:11Z"
"","2106","Added unit tests for DefaultDecoder and StringDecoder","Added Unit tests for Default Decoder and String Decoder classes","open","","ntrphanikumar","2016-11-05T16:01:33Z","2018-03-02T19:29:46Z"
"","1903","KAFKA-4213: First set of system tests for replication throttling","Added the first set of system tests for replication quotas. These tests validate throttling behavior during partition reassigment.  Along with this patch are fixes to the test framework which include: 1. KakfaService.verify_replica_reassignment: this method was a no-op and would always return success, as explained in KAFKA-4204. This patch adds a workaround to the problems mentioned there, by grepping correctly for success, failure, and 'in progress' states of partition reassignment.    2.ProduceConsumeValidateTest.annotate_missing_messages would call missing.pop() to enumerate the first 20 missing messages. This meant that all future counts of what is actually missing would be off by 20, leading to the impression of data loss.","closed","","apurvam","2016-09-23T23:32:18Z","2016-09-23T23:34:31Z"
"","2206","KAFKA-4454 : Authorizer should also include the Principal generated by the PrincipalBuilder.","Added support to include the Principal generated by the PrincipalBuilder in KafkaChannel","closed","","MayureshGharat","2016-12-02T23:24:46Z","2018-02-25T21:17:14Z"
"","1711","KAFKA-3936: Validate parameters as early as possible","Added non null checks to parameters supplied via the DSL and `TopologyBuilder`","closed","","dguy","2016-08-08T14:23:34Z","2016-08-09T18:31:36Z"
"","1602","KAFKA-3943 - ConfigDef with Builder pattern","Added Builder class and define() method with no arguments. Added testcase validating the ConfigDef using the current implementation against the new builder implementation.","closed","","jcustenborder","2016-07-09T23:34:50Z","2018-05-16T17:15:10Z"
"","1535","KAFKA-3727 - ConsumerListener for UnknownTopicOrPartitionException","Added a ConsumerListener to KafkaConsumer  Modified Fetcher and Cluster to account for Unknown Topics  Added unit test","closed","","edoardocomar","2016-06-21T17:45:24Z","2017-06-22T12:17:04Z"
"","1545","KAFKA-3727 - ClientListener for UnknownTopicOrPartitionException","Added a ClientListener to KafkaConsumer, Fetcher and Client  Modified Fetcher and NetworkClient to notify listeners for Unknown Topic or Partition  Added unit test","closed","","edoardocomar","2016-06-23T15:58:34Z","2017-06-22T12:16:38Z"
"","1548","KAFKA-2857 ConsumerGroupCommand throws GroupCoordinatorNotAvailableException when describing a non-existent group before the offset topic is created","Added a check to make sure different cases when offset topic hasn't been created and consumer group describe command is run, are handled appropriately.","closed","","imandhan","2016-06-23T23:47:27Z","2016-12-02T18:59:31Z"
"","2097","KAFKA-4366: KafkaStreams.close() blocks indefinitely","Added `timeout` and `timeUnit` to `KafkaStreams.close(..)`. Now do close on a thread and `join` that thread with the provided `timeout`. Changed `state` in `KafkaStreams` to use an enum. Added system test to ensure we don't deadlock on close when an uncaught exception handler that calls `System.exit(..)` is used and there is also a shutdown hook that calls `KafkaStreams.close(...)`","closed","","dguy","2016-11-03T18:51:14Z","2017-09-08T21:45:07Z"
"","2166","KAFKA-3452: Support session windows","Add support for SessionWindows based on design detailed in https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows. This includes refactoring of the RocksDBWindowStore such that functionality common with the RocksDBSessionStore isn't duplicated.","closed","","dguy","2016-11-24T16:48:23Z","2017-02-21T22:37:45Z"
"","1649","KAFKA-3929: Add prefix for underlying clients configs in StreamConfig","Add prefixes for consumer and producer configs to StreamsConfig, but be backward compatible.","closed","","dguy","2016-07-21T23:15:30Z","2016-08-02T21:15:30Z"
"","1657","KAFKA-3986 Add null check to completedReceives","Add null check to guard against the unlikely event where a channel has been closed but not yet removed from completedReceives","closed","","rnpridgeon","2016-07-23T13:17:59Z","2018-02-25T07:32:21Z"
"","1640","KAFKA-3740: Enable configuration of RocksDBStores","Add new config StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG to enable advanced RocksDB users to override default RocksDB configuration","closed","","dguy","2016-07-19T23:28:22Z","2016-07-21T18:04:48Z"
"","2419","kafka-4684: Kafka does not offer kafka-configs.bat on Windows box","Add kafka-configs.bat script for Windows.","closed","","huxihx","2017-01-22T06:28:01Z","2017-01-26T01:54:31Z"
"","2244","KAFKA-4490: Add Global Table support to Kafka Streams","Add Global Tables to KafkaStreams. Global Tables are fully replicated once-per instance of KafkaStreams. A single thread is used to update them. They can be used to join with KStreams, KTables, and other GlobalKTables. When participating in a join a GlobalKTable is only ever used to perform a lookup, i.e., it will never cause data to be forwarded to downstream processor nodes.","closed","","dguy","2016-12-12T11:43:49Z","2017-01-12T19:46:50Z"
"","2037","add Windows script 'kafka-consumer-groups.bat' as issue Kafka-4310 mentions","add file 'kafka-consumer-groups.bat' for Windows platform","closed","","huxihx","2016-10-18T13:06:39Z","2016-10-18T23:13:03Z"
"","2308","kafka-3739:Add no-arg constructor for library provided serdes","Add default constructor for library provided serdes","closed","","huxihx","2017-01-04T08:16:52Z","2017-01-13T23:47:23Z"
"","2452","KAFKA-4649: Improve test coverage GlobalStateManagerImpl","Add coverage for exception paths in `initialize()`","closed","","dguy","2017-01-26T18:21:23Z","2017-02-17T00:58:11Z"
"","1747","KAFKA-4050: Allow configuration of the PRNG used for SSL","Add an optional configuration for the SecureRandom PRNG implementation, with the default behavior being the same (use the default implementation in the JDK/JRE).","closed","","toddpalino","2016-08-16T21:16:25Z","2018-05-11T04:58:16Z"
"","1648","KAFKA-3983 - Add additional information to debug","Add additional information to Acceptor debug message upon connection acceptance","closed","","rnpridgeon","2016-07-21T22:12:40Z","2016-07-22T17:10:31Z"
"","2450","KAFKA-4647: Improve test coverage of GlobalStreamThread","Add a test to ensure a `StreamsException` is thrown when an exception other than `StreamsException` is caught","closed","","dguy","2017-01-26T17:06:48Z","2017-02-17T00:58:17Z"
"","2107","KAFKA-4385-avoid-unnecessary-meta-request-overhead","add a producer config: ""metadata.fetch.max.count"" default to Integer.MAX_VALUE, then everything is the same as before; when specified, the metadata request will be limited by both ""metadata.fetch.timeout.ms"" (time limit) and ""metadata.fetch.max.count"" (request count)  when ""auto.create.topics.enable=false"", ""metadata.fetch.max.count"" can be set as 1, so that it could avoid lots of unncessary duplicate metadata","open","","yaojuncn","2016-11-06T05:15:21Z","2018-03-02T19:29:47Z"
"","1689","HOTFIX: Fix punctuation timestamp in PunctuationQueue.java","Actually, there are two problems:  1) Processor's ""punctuate"" method is not calling if there are NO new messages in ""source"" topic 2) When message comes after let's say 20 seconds delay (punctuation timeout was set to 2 seconds) - method punctuate is calling 10 times in a loop for every ""missed"" call within delayed period with the SAME timestamp  Actually suggested change only fixes second item (every missed call will be using it's timestamp). Can anyone, please, comment if first item's statement is intended behavior or a bug?","closed","","antonnazaruk","2016-07-30T21:35:38Z","2017-03-14T21:32:42Z"
"","2477","MINOR: Change rocksdb logging to error level","According to the java-doc: https://github.com/facebook/rocksdb/blob/master/java/src/main/java/org/rocksdb/Logger.java#L31 the rocksdb logging level should be set Error or Fatal for production usage.","closed","","dguy","2017-02-01T09:37:52Z","2017-02-17T00:58:13Z"
"","1718","KAFKA-4031: Check if buffer cleaner is null before using it","A small fix to check null before using the reference","closed","","soumyajit-sahu","2016-08-10T20:37:19Z","2016-08-12T11:08:17Z"
"","1624","KAFKA-1194: New config to specify memory mapped file update capability for underlying OS","A new configuration value that can be used to indicate if the underlying OS (and environment) supports updates to memory mapped files.   On windows systems, metadata updates, and rename operations fail if the file is still memory mapped.  This fix will close the file if updates are not supported, and then proceeds to execute the update operations.","open","","raviperi","2016-07-14T19:48:02Z","2018-03-02T19:29:36Z"
"","1840","MINOR: catch InvalidStateStoreException in QueryableStateIntegrationTest","A couple of the tests may transiently fail in QueryableStateIntegrationTest as they are not catching InvalidStateStoreException. This exception is expected during rebalance.","closed","","dguy","2016-09-09T19:26:27Z","2016-09-12T04:01:04Z"
"","1912","KAFKA-4201: Add an `assignment.strategy` option to new-consumer-based Mirror Maker","A command line option `assignment.strategy` is added to new-consumer-based Mirror Maker that overwrites any assignment strategy specified in the new-consumer config.","closed","","vahidhashemian","2016-09-27T00:42:01Z","2022-02-10T16:30:47Z"
"","1587","MINOR: fix generics in Windows.segments and Windows.until","`Windows.segments(...)` and `Windows.until(...)` currently aren't returning the `Window` with its type param `W`. This causes the generic type to be lost and therefore methods using this can't infer the correct return types.","closed","","dguy","2016-07-05T10:13:24Z","2016-07-05T18:38:44Z"
"","2253","KAFKA-4534: StreamPartitionAssignor only ever updates the partitionsByHostState and metadataWithInternalTopics on first assignment.","`partitionsByHostState` and `metadataWithInternalTopics` need to be updated on each call to `onAssignment()` otherwise they contain invalid/stale metadata.","closed","","dguy","2016-12-14T10:40:46Z","2016-12-14T13:49:48Z"
"","2410","MINOR: Fix typo in WordCountProcessorDemo","`bin-kafka-console-producer.sh` should be `bin/kafka-console-producer.sh`.","closed","","wmarshall484","2017-01-20T08:06:39Z","2017-01-26T02:02:15Z"
"","2000","KAFKA-4284: Make Partitioner a Closeable and close it when closing the producer","[KAFKA-4284](https://issues.apache.org/jira/browse/KAFKA-4284)  Even though Partitioner has a close method it is not closed when the producer is closed. Serializers, interceptors and metrics are all closed, so partitioners should be closed to.  To be able to use the same mechanism to close the partitioner as the serializers, etc. I had to make the `Partitioner` interface extend `Closeable`. Since this doesn't change the interface that feels ok and should be backwards compatible.  Looking at [KAFKA-2091](https://issues.apache.org/jira/browse/KAFKA-2091) (d6c45c70fb9773043766446e88370db9709e7995) that introduced the `Partitioner` interface it looks like the intention was that the producer should close the partitioner.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","iconara","2016-10-10T11:41:09Z","2016-11-08T17:54:22Z"
"","1755","KAFKA-4039: delay invocation of System.exit via FatalExitException","@resetius would be great if you can confirm that the deadlock no longer manifests with the path. Thanks","closed","","maysamyabandeh","2016-08-17T18:31:36Z","2017-08-04T14:35:09Z"
"","1534","MINOR: update streams.html with KStream API changes","@mjsax @guozhangwang","closed","","dguy","2016-06-21T11:46:58Z","2016-06-21T19:17:25Z"
"","2007","KAFKA-4114: allow different offset reset strategies","@mjsax   Here's my first pass at finer grained auto offset reset strategies.  I've left TODO comments about whether we want to consider adding this to `KGroupedTable.aggregate` and `KStreamImpl` when re-partitioning a source.","closed","","bbejeck","2016-10-11T00:52:31Z","2017-01-12T06:22:45Z"
"","1769","Minor follow up for KAFKA-3163 (KIP-33)","@junrao Could you take a look when get a chance? Thanks.","closed","","becketqin","2016-08-22T16:40:48Z","2016-08-22T17:34:43Z"
"","1728","MINOR: add describe to valid group acl operations","@junrao","closed","","norwood","2016-08-12T21:55:46Z","2016-08-12T22:03:00Z"
"","1667","KAFKA-3993 Console Producer Drops Data","@ijuma Would you mind taking a look?  This fixes the problem for me using this test script.  ``` export BOOTSTRAP_SERVERS=localhost:9092 export TOPIC=bar export MESSAGES=10000 ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --partitions 1 --replication-factor 1 --topic ""$TOPIC"" \ && echo ""acks=all"" > /tmp/producer.config \ && echo ""linger.ms=0"" >> /tmp/producer.config \ && seq ""$MESSAGES"" | ./bin/kafka-console-producer.sh --broker-list ""$BOOTSTRAP_SERVERS"" --topic ""$TOPIC"" \ && ./bin/kafka-console-consumer.sh --bootstrap-server ""$BOOTSTRAP_SERVERS"" --new-consumer --from-beginning --max-messages ""${MESSAGES}"" --topic ""$TOPIC"" ```","closed","","theduderog","2016-07-26T18:13:20Z","2016-10-06T17:40:01Z"
"","1527","KAFKA-3809: Auto-generate documentation for topic-level configuration","@ijuma said that it would make sense to split out this work from KAFKA-3234, since KAFKA-3234 had both a mechanical change (generating docs) as well as a change requiring discussion (deprecating/renaming config options).  @jjkoshy, I hope you don't mind that I took over this work. It's been 3 months since the last activity on KAFKA-3234, so I thought it would be okay to take over.  This work is essentially is the first 5-6 commits from Joel's https://github.com/apache/kafka/pull/907. However, since I'm not very experienced with git, I didn't do a direct merge/rebase, but instead largely hand-merged it. I did some minor cleanup. All credit goes to Joel, all blame goes to me. :)  For reference, I attached the auto-generated configuration.html file (as a PDF, because github won't let me attache html). [configuration.pdf](https://github.com/apache/kafka/files/323901/configuration.pdf)  This is my first time writing Scala, so let me know if there are any changes needed.  I don't know who is the right person to review this. @ijuma, can you help me redirect this to the appropriate person? Thanks.","closed","","wushujames","2016-06-20T17:03:11Z","2016-08-07T01:14:42Z"
"","1549","KAFKA-3896: Unstable test KStreamRepartitionJoinTest.shouldCorrectlyRepartitionOnJoinOperations","@ijuma i checked the cases where this test has failed and it seems to always be on the verification of the left join. I've ran this test plenty of times and i can't get it to fail. However in the interest of having stable builds, i've removed just the part of the test that is failing (which happens to be the last verification). Thanks, Damian","closed","","dguy","2016-06-24T07:16:36Z","2016-06-27T18:51:33Z"
"","1658","KAFKA-3689 - process only distinct connectionIds within processDisconnected()","@ijuma discovered a possible scenario in which connectionQuotas may be [doubly-decremented](https://issues.apache.org/jira/browse/KAFKA-3689?focusedCommentId=15385962&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15385962)  This PR intends to remove that possibility by first filtering out any duplicate connectionIds. This way each","closed","","rnpridgeon","2016-07-24T13:06:34Z","2018-05-09T14:20:30Z"
"","2376","KAFKA-4467: Run tests on travis-ci using docker","@ijuma @ewencp @cmccabe @harshach Please review. Here is a sample run: https://travis-ci.org/raghavgautam/kafka/builds/191714520  In this run 214 tests were run and 144 tests passed.  I will open separate jiras for fixing failures.","closed","","raghavgautam","2017-01-13T22:52:21Z","2017-03-10T00:25:28Z"
"","1760","KAFKA-3937: Kafka Clients Leak Native Memory For Longer Than Needed With Compressed Messages","@ijuma - Making the change against trunk based on your suggestions to have the stream closing handled in the private RecordIterator constructor which I understand is only to be used only if the block of message(s) are compressed.","closed","","wiyu","2016-08-18T17:29:21Z","2016-08-23T22:48:04Z"
"","1743","KAFKA-3937: Kafka Clients Leak Native Memory For Longer Than Needed With Compressed Messages","@ijuma - Creating this PR against 0.9.0 as this is what we're using in prod. I can modify this for trunk if the logic solution looks good.   I modified the record iterator to inherit from closable and implemented the method to call streams.close(). The stream will now be closed when we are done traversing the inner iterator. I had to move the `try {} catch{}` out of the `if` portion to deal with IOException coming from `innerDone()`.","closed","","wiyu","2016-08-16T14:35:32Z","2016-08-17T22:37:25Z"
"","1659","MINOR: cleanup apache license in python files","@ijuma  As discussed in https://github.com/apache/kafka/pull/1645, this patch removes an extraneous line from several **init**.py files, and a few others as well","closed","","granders","2016-07-25T00:46:58Z","2016-07-26T02:04:46Z"
"","1713","MINOR: Add security protocol option for fetch","@ijuma","closed","","norwood","2016-08-08T18:43:57Z","2016-08-09T00:53:01Z"
"","2161","KAFKA-4307 Inconsistent parameters between console producer and consumer","@gwenshap please review","closed","","baluchicken","2016-11-23T08:48:39Z","2022-02-10T16:26:40Z"
"","1573","hotfix: check join window boundaries","@guozhangwang Might need to rebase after https://github.com/apache/kafka/commit/b669b2786c69d2dfa719033eb7c02ac444f115f4 got dropped...","closed","","mjsax","2016-06-29T23:59:51Z","2016-06-30T18:07:41Z"
"","1501","MINOR: Expose window store sequence number","@guozhangwang @mjsax @enothereska   Currently, Kafka Streams does not have a util to get access to the sequence number added to the key of windows state store changelogs.  I'm interested in exposing it so the the contents of a changelog topic can be 1) inspected for debugging purposes and 2) saved to text file and loaded from text file","closed","","theduderog","2016-06-13T22:57:59Z","2016-06-15T21:02:50Z"
"","1765","MINOR: improve Streams application reset tool to make sure application is down","@guozhangwang @miguno @dguy @enothereska @hjafarpour See #1764","closed","","mjsax","2016-08-19T15:57:08Z","2016-08-20T19:08:51Z"
"","1570","KAFKA-3914: Global discovery of state stores","@guozhangwang @enothereska please take a look. A few things that need to be clarified 1. I've added `StreamsConfig.USER_ENDPOINT_CONFIG`, but should we have separate configs for host and port or is this one config ok? 2. `HostState` in the KIP has a `byte[]` field - not sure why and what it would be populated with 3. The API calls in the KIP all return `Map>`, however i don't see why the  `Set` is required so I've changed it to `Map`","closed","","dguy","2016-06-29T15:14:50Z","2016-06-30T09:16:12Z"
"","1565","KAFKA-3912: Query local state stores","@guozhangwang @enothereska please review","closed","","dguy","2016-06-28T10:45:24Z","2016-07-19T21:02:54Z"
"","1576","KAFKA-3914: Global discovery of state stores","@guozhangwang @enothereska @mjsax @miguno  please take a look. A few things that need to be clarified 1. I've added StreamsConfig.USER_ENDPOINT_CONFIG, but should we have separate configs for host and port or is this one config ok? 2. `HostState` in the KIP has a byte[] field - not sure why and what it would be populated with 3. I've changed the API to return `List` as opposed to `Map>` as i find this far more intuitive to work with.","closed","","dguy","2016-06-30T09:15:51Z","2016-08-10T21:26:21Z"
"","2122","failing to close this iterator causes leaks in rocksdb","@guozhangwang @dguy","closed","","norwood","2016-11-11T04:21:14Z","2016-11-11T14:37:10Z"
"","1761","KAFKA-4064 Add support for infinite endpoints for range queries in Kafka Streams KV stores","@guozhangwang   I had to fix the bug with in-memory ranges being exclusive and RocksDB being inclusive to get meaningful tests to work.","closed","kip,","theduderog","2016-08-18T23:45:07Z","2021-07-30T02:59:10Z"
"","1828","MINOR: allow creation of statestore without loggingenabled or explicit sourcetopic","@guozhangwang","closed","","norwood","2016-09-06T17:06:44Z","2016-09-19T19:37:56Z"
"","1575","hotfix: check join window boundaries","@guozhangwang","closed","","mjsax","2016-06-30T00:07:09Z","2016-06-30T17:51:41Z"
"","2176","KAFKA-4403 Update KafkaBasedLog to use new endOffsets consumer API","@ewencp plz review","closed","","baluchicken","2016-11-27T16:38:50Z","2016-11-30T03:22:25Z"
"","2423","KAFKA-4679 Remove unstable markers from Connect APIs","@ewencp ignore this PR if you are already started to work on this ticket.","closed","","baluchicken","2017-01-23T13:11:59Z","2017-01-28T04:00:26Z"
"","2422","KAFKA-4679 Remove unstable markers from Connect APIs","@ewencp ignore this PR if you are already started to work on this ticket.","closed","","baluchicken","2017-01-23T13:03:00Z","2017-01-23T13:06:33Z"
"","1653","KAFKA-2932: Adjust importance level of Kafka Connect configs","@ewencp I went down the list of connect configs and it looks like only the internal converter configs are mismarked.  It looks like the `cluster` config that is present in the current docs is already gone.  The only other values I can see arguing to change importance on are the ssl configs (marked high) but they are consistent with the producer/consumer config docs so that's at least consistent.  Everything else marked high looks either mandatory or requires consideration in a production deployment to me.","closed","","cotedm","2016-07-22T18:11:14Z","2016-08-07T04:59:06Z"
"","1668","KAFKA-3973: feasibility caching bytes vs records","@enothereska   Added the benchmark test per comments.  Thanks  EDIT: Results from the test and additional comments are in the Jira ticket for KAFKA-3973","open","streams,","bbejeck","2016-07-26T23:25:36Z","2018-11-18T07:04:01Z"
"","1884","KAFKA-4060: Remove zk client dependency in kafka streams","@dguy @guozhangwang This is a new PR for KAFKA-4060.","closed","","hjafarpour","2016-09-20T00:22:57Z","2017-01-11T17:16:25Z"
"","2127","KAFKA-4384: ReplicaFetcherThread stopped after ReplicaFetcherThread received a corrupted message","@becketqin   Here is the patch for KAFKA-4384 (ReplicaFetcherThread stopped after ReplicaFetcherThread received a corrupted message).","closed","","jun-he","2016-11-12T06:58:17Z","2016-11-25T02:28:13Z"
"","1589","MINOR: Increase default `waitTime` in `waitUntilTrue` to 15 seconds","5 seconds is probably enough when running tests locally, but doesn't seem to be so for Jenkins when it is overloaded.","closed","","ijuma","2016-07-05T23:30:45Z","2016-07-22T16:59:56Z"
"","2463","KAFKA-4706","4706 - Unify StreamsKafkaClient instances","closed","","sharad-develop","2017-01-29T20:49:25Z","2017-01-31T15:19:19Z"
"","2286","KAFKA-3284: Remove beta label from security documentation","4 release cycles (0.9.0.0, 0.10.0.0, 0.10.1.0, 0.10.2.0) should be enough to remove the beta label.","closed","","ijuma","2016-12-21T22:30:27Z","2016-12-22T16:16:19Z"
"","1703","KAFKA-3875: Transient test failure: kafka.api.SslProducerSendTest.testSendNonCompressedMessageWithCreateTime","1. The IllegalStateException is actually thrown from testCloseWithZeroTimeoutFromSenderThread() due to a bug. We call producer.close() in the callback. Once the first callback is called, producing records in the callback will hit the IllegalStateException. This only pollutes the output, but doesn't fail the test. I fixed this by only calling producer.send() in the first callback. 2. It's not clear which test throws TimeoutException and it's not reproducible locally. One thing is that the error message in TimeoutException is mis-leading since the timeout is not necessarily due to metadata. Improved this by making the error message in TimeoutException clearer. 3. It's not clear what actually failed testSendNonCompressedMessageWithCreateTime(). One thing I found is that since we set the linger time to MAX_LONG and are sending small messages, those produced messages won't be drained until we call producer.close(10000L, TimeUnit.MILLISECONDS). Normally, 10 secs should be enough for the records to be sent. My only hypothesis is that since SSL is more expensive, occasionally, 10 secs is still not enough. So, I bumped up the timeout from 10 secs to 20 secs.","closed","","junrao","2016-08-03T22:33:28Z","2016-08-04T11:31:11Z"
"","2379","KAFKA-4633: Always using regex pattern subscription in StreamThread","1. In StreamThread, always use subscribe(Pattern, ..) function in order to avoid sending MetadataRequest with specific topic names and cause brokers to possibly auto-create subscribed topics; the pattern is generated as ""topic-1|topic-2..|topic-n"".  2. In ConsumerCoordinator, let the leader to refresh its metadata if the generated assignment contains some topics that is not contained in the subscribed topics; also in SubscriptionState, modified the verification for regex subscription to against the regex pattern instead of the matched topics since the returned assignment may contain some topics not yet created when joining the group but existed after the rebalance; also modified some unit tests in `KafkaConsumerTest` to accommodate the above changes.   3. Minor cleanup: changed String[] to List to avoid overloaded functions.  4. Minor cleanup: enforced strong typing in SinkNodeFactory and removed unnecessary unchecked tags.  5. Minor cleanup: augmented unit test error message and fixed a potential transient failure in KafkaStreamTest.","closed","","guozhangwang","2017-01-14T15:44:22Z","2017-07-15T22:07:29Z"
"","1651","MINOR: Fix typos in security section","1. I think the instructions in step 2 of the security section which describe adding the CA to server/client truststores are swapped. That is, the instruction that says to add the CA to the server truststore adds it to the client truststore (and vice versa). 2. ""clients keys"" should be possessive (""clients' keys"").  This contribution is my original work, and I license the work to the project under the project's open source license.","closed","","ssaamm","2016-07-22T13:13:49Z","2016-08-22T22:17:50Z"
"","2386","[WIP] Upgrade Kafka and apply windows fix","1. Created branch off of https://github.com/fluetm/kafka/tree/kafka-upgrade. 2. Cherry-picked windows fix for https://issues.apache.org/jira/browse/KAFKA-1194 from https://github.com/fluetm/kafka/tree/1194-fix 3. Resolved merge conflicts and customized the windows fix so that it fits in the released version of Kafka we upgraded to, rather than the unreleased trunk  Note: 34 out of 1018 Kafka tests failed on my local Mac, while the rest passed. The failed tests are mostly from LogDeletionIntegrationTest","closed","","silpamittapalli","2017-01-17T18:50:26Z","2017-03-14T11:51:13Z"
"","2012","KAFKA-4117: Stream partitionassignro cleanup","1. Create a new `ClientMetadata` to collapse `Set consumerMemberIds`, `ClientState state`, and `HostInfo hostInfo`. 2. Stop reusing `stateChangelogTopicToTaskIds` and `internalSourceTopicToTaskIds` to access the (sub-)topology's internal repartition and changelog topics for clarity; also use the source topics num.partitions to set the num.partitions for repartition topics, and clarify to NOT have cycles since otherwise the while loop will fail. 3. `ensure-copartition` at the end to modify the number of partitions for repartition topics if necessary to be equal to other co-partition topics. 4. Refactor `ClientState` as well and update the logic of `TaskAssignor` for clarity as well. 5. Change default `clientId` from `applicationId-suffix` to `applicationId-processId` where `processId` is an UUID to avoid conflicts of clientIds that are from different JVMs, and hence conflicts in metrics. 6. Enforce `assignment` partitions to have the same size, and hence 1-1 mapping to `activeTask` taskIds. 7. Remove the `AssignmentSupplier` class by always construct the `partitionsByHostState` before assigning tasks to consumers within a client. 8. Remove all unnecessary member variables in `StreamPartitionAssignor`. 9. Some other minor fixes on unit tests, e.g. remove `test only` functions with java field reflection.","closed","","guozhangwang","2016-10-11T15:48:49Z","2017-07-15T22:09:06Z"
"","2388","KAFKA-4591: Create Topic Policy follow-up","1. Added javadoc to public classes 2. Removed `s` from config name for consistency with interface name 3. The policy interface now implements Configurable and AutoCloseable as per the KIP 4. Use `null` instead of `-1` in `RequestMetadata` 5. Perform all broker validation before invoking the policy 6. Add tests","closed","","ijuma","2017-01-17T21:14:47Z","2017-09-05T09:42:03Z"
"","2110","KAFKA-4387: Fix KafkaConsumer not responding correctly to interrupts,…","... throw InterruptException from blocking methods when interrupted  See https://issues.apache.org/jira/browse/KAFKA-4387","closed","","srdo","2016-11-07T19:14:03Z","2016-11-30T04:22:12Z"
"","1525","MINOR: speed up streams integration tests","... by  a) merging some for startup/shutdown efficiency. b) use independent state dirs.  c) remove some tests that are covered elsewhere  @guozhangwang @ewencp - tests are running much quicker now, i.e, down to about 1 minute on my laptop (from about 2 - 3 minutes). There were some issues with state-dirs in some of the integration tests that was causing the shutdown of the streams apps to take a long time.","closed","","dguy","2016-06-20T09:33:47Z","2016-06-20T21:38:15Z"
"","2340","MINOR: ConfigDef `parseType` exception message updated.","- When Kafka configurations are provided programmatically, for the below configuration incorrect error message gets printed.     * configuration: kafkaProps.put(""zookeeper.session.timeout.ms"", 60_000L); - expects INTEGER value.     * Error Msg: ""Invalid value 60000 for configuration zookeeper.session.timeout.ms : Expected value to be an number.""     * Long is also a number which misleading the error message. - Minor code cleanup and unwanted local variables are removed. - Java doc dangling imports are updated.","closed","","kamalcph","2017-01-10T18:04:33Z","2017-01-11T17:51:29Z"
"","2345","MINOR: ConfigDef `parseType` exception message updated.","- When Kafka configurations are provided programmatically, for the below configuration incorrect error message gets printed.         * configuration: kafkaProps.put(""zookeeper.session.timeout.ms"", 60_000L); - expects Integer value         * Error Msg: ""Invalid value 60000 for configuration zookeeper.session.timeout.ms : Expected value to be an number.""         * Long is also a number which misleads the error message.","closed","","kamalcph","2017-01-11T17:14:02Z","2017-01-27T12:24:12Z"
"","1767","KAFKA-4058: Failure in org.apache.kafka.streams.integration.ResetIntegrationTest.testReprocessingFromScratchAfterReset","- use AdminTool to check for active consumer group","closed","","mjsax","2016-08-20T19:07:12Z","2016-09-07T06:03:13Z"
"","1756","KAFKA-4058: Failure in org.apache.kafka.streams.integration.ResetIntegrationTest.testReprocessingFromScratchAfterReset","- use AdminTool to check for active consumer group","closed","","mjsax","2016-08-17T19:19:12Z","2016-08-30T20:29:14Z"
"","1895","[DOCS] KAFKA-4151; Update public docs for KIP-78","- Updated implementation docs with details on Cluster Id generation and ZK data structure. - Mention cluster id in ""noteworthy changes for 0.10.1.0"" in upgrade docs. - Add cluster id to metrics table in ops docs.","closed","","arrawatia","2016-09-21T07:12:34Z","2016-09-24T09:28:07Z"
"","2342","KAFKA-3452: follow-up -- introduce SesssionWindows","- TimeWindows represent half-open time intervals while SessionWindows represent closed time intervals","closed","","mjsax","2017-01-11T01:10:05Z","2017-01-12T04:34:19Z"
"","2245","Separate Streams documentation and setup docs with easy to set variables","- Seperate Streams documentation out to a standalone page.  - Setup templates to use handlebars.js - Create template variables to swap in frequently updated values like version number from a single file templateData.js","closed","","derrickdoo","2016-12-12T18:42:26Z","2016-12-14T02:03:57Z"
"","2049","KAFKA-4331: Kafka Streams resetter is slow because it joins the same group for each topic","- reworked to use a sinlge KafkaConsumer and subscribe only once","closed","","mjsax","2016-10-21T00:01:46Z","2016-10-24T20:45:12Z"
"","2326","MINOR: Various small scala cleanups","- Removed unnecessary semicolons","closed","","mimaison","2017-01-06T18:30:28Z","2018-04-18T13:31:46Z"
"","2306","MINOR: PartitionInfo toString corrected","- Removed the extra ',' character while printing the replicas / in-sync replicas identifier","closed","","kamalcph","2017-01-04T06:12:36Z","2017-01-05T10:26:54Z"
"","2222","KAFKA-4500: code improvements","- Removed redundant modifiers, not needed String.format() - Removed unnecessary semicolon, additional assignment, inlined return  - Added few param, return tags - Using StringBuilder for consistency across codebase - Using try-with-resources","closed","","rekhajoshm","2016-12-07T07:17:12Z","2016-12-20T13:31:21Z"
"","1675","MINOR: Fixed a few warnings in clients","- Removed an unnecessary annotation - Parameterized a couple of raw types","closed","","mimaison","2016-07-28T16:59:18Z","2016-07-31T22:07:04Z"
"","2239","Docs templates","- Move Streams documentation out to it's own page - Render docs with Handlebars.js so we quickly set repeated items like doc version numbers","closed","","derrickdoo","2016-12-10T01:14:40Z","2016-12-12T20:06:08Z"
"","2082","KAFKA-4352: instable ResetTool integration test","- increased timeout to stabilize test","closed","","mjsax","2016-11-01T00:51:14Z","2016-11-01T17:31:37Z"
"","1522","KAFKA-3866; KerberosLogin refresh time bug and other improvements","- Handle correctly the case where the refresh is supposed to happen now - Replace `now + minTimeBeforeLogin` with `lastLogin + minTimeBeforeLogin` - Break from loop if there are no retries left after `reLogin` fails - `reLogin` no longer checks if sufficient time has elapsed as the check   is done elsewhere when appropriate and it would incorrectly not relogin   even if that meant that the ticket would expire - Removed duplicate logic in `getRefreshTime` regarding a refresh time   that was greater than the expiry time - Comment and logging clean-ups - Add validators for a few SASL configs","open","","ijuma","2016-06-18T23:43:44Z","2018-03-02T19:29:34Z"
"","1777","KAFKA-4001: Improve Kafka Streams Join Semantics (KIP-77)","- fixed leftJoin -> outerJoin test bug - simplified to only use values - fixed inner KTable-KTable join - fixed left KTable-KTable join - fixed outer KTable-KTable join - fixed inner, left, and outer left KStream-KStream joins - added inner KStream-KTable join - fixed left KStream-KTable join","closed","","mjsax","2016-08-23T22:37:38Z","2016-10-20T20:07:08Z"
"","2056","KAFAK-4058: Failure in org.apache.kafka.streams.integration.ResetIntegrationTest.testReprocessingFromScratchAfterReset","- fixed consumer group dead condition - disabled state store cache","closed","","mjsax","2016-10-23T00:55:16Z","2016-10-24T04:32:44Z"
"","1485","KAFKA-3805: Check if DB is null.","- Check if DB is null before flushing or closing. In some cases, a state store is closed twice. This happens in `StreamTask.close()` where both `node.close()` and `super.close` (in `ProcessorManager`) are called in a sequence. If the user's processor defines a `close` that closes the underlying state store, then the second close will be redundant.","closed","","enothereska","2016-06-09T14:47:00Z","2016-06-16T23:18:51Z"
"","2138","KAFKA-4331: Kafka Streams resetter is slow because it joins the same group for each topic","- bug-fix follow up   - Resetter fails if no intermediate topic is used because seekToEnd() commit ALL partitions to EOL","closed","","mjsax","2016-11-16T00:42:54Z","2016-11-24T01:36:36Z"
"","2209","KAFKA-4476: Kafka Streams gets stuck if metadata is missing","- break loop in StreamPartitionAssigner.assign() in case partition metadata is missing  - fit state transition issue (follow up to KAFKA-3637: Add method that checks if streams are initialised)  - some test improvements","closed","","mjsax","2016-12-03T09:24:24Z","2016-12-11T05:54:49Z"
"","2337","MINOR: rework JavaDoc for windowing related public API","- also some code refactoring and bug fixes","closed","","mjsax","2017-01-09T22:25:11Z","2017-01-24T19:23:24Z"
"","2459","MINOR: update JavaDocs for Kafka Streams DSL helpers","- also deprecate ZK config for Streams","closed","","mjsax","2017-01-27T12:22:27Z","2017-01-28T00:49:26Z"
"","1972","KAFKA-3224: New log deletion policy based on timestamp","- adds a new topic-level broker configuration, `log.retention.min.timestamp`   - if unset, this setting is ignored   - setting this value to a Unix timestamp will allow the log cleaner to delete any segments for a given topic whose last timestamp is earlier than the set timestamp ##  ### [KIP-47](https://cwiki.apache.org/confluence/display/KAFKA/KIP-47+-+Add+timestamp-based+log+deletion+policy) ### [JIRA](https://issues.apache.org/jira/browse/KAFKA-3224)","closed","","bill-warshaw","2016-10-05T13:55:00Z","2017-10-16T16:17:08Z"
"","2273","KAFKA-4555: Using Hamcrest for expressive intent in tests","- Adding hamcrest in gradle files - Using hamcrest in couple of tests - SourceTaskOffsetCommitterTest, MetadataTest","closed","","rekhajoshm","2016-12-18T21:12:03Z","2017-02-06T18:32:37Z"
"","1696","KAFKA-4010; ConfigDef.toRst() to have grouped sections with dependents info","- Added sort method with group order - Added dependents info","closed","","rekhajoshm","2016-08-02T21:15:21Z","2018-02-25T21:27:47Z"
"","1636","KAFKA-3185: Allow users to cleanup internal Kafka Streams data","- added Kafka Stream Application Reset Tool","closed","","mjsax","2016-07-19T12:14:12Z","2016-07-27T21:12:21Z"
"","1745","KAFKA-4042: prevent DistributedHerder thread from dying from connector/task lifecycle exceptions","- `worker.startConnector()` and `worker.startTask()` can throw (e.g. `ClassNotFoundException`, `ConnectException`, or any other exception arising from the constructor of the connector or task class when we `newInstance()`), so add catch blocks around those calls from the `DistributedHerder` and handle by invoking `onFailure()` which updates the `StatusBackingStore`. - `worker.stopConnector()` throws `ConnectException` if start failed causing the connector to not be registered with the worker, so guard with `worker.ownsConnector()` - `worker.stopTasks()` and `worker.awaitStopTasks()` throw `ConnectException` if any of them failed to start and are hence not registered with the worker, so guard those calls by filtering the task IDs with `worker.ownsTask()`","closed","","shikhar","2016-08-16T19:08:44Z","2016-08-24T07:47:41Z"
"","2164","KAFKA-4438; Cross compile to Scala 2.12.0","(cherry picked from commit f3aad3b54b7cbc5109d8398829a31100fd82b3e0)","closed","","leachbj","2016-11-24T00:34:52Z","2016-12-05T14:23:46Z"
"","1717","KAFKA-3742: (FIX) Can't run bin/connect-*.sh with -daemon flag","## Problem  Current connect scripts (`connect-distributed.sh`, `connect-standalone.sh`) do not support `-daemon` flag even if users specify the flag since `kafka-run-class.sh` requires that the`-daemon` flag should precede other arguments (e.g. class name) ## Solution  Do the same thing like in `kafka-server-start.sh` - Parse a command - Add `-daemon` to `$EXTRA_ARGS` if exists","closed","","1ambda","2016-08-10T11:00:54Z","2016-08-26T03:23:37Z"
"","2479","KAFKA-4719: Consumption timeout should take into account producer request timeout","","closed","","hachikuji","2017-02-01T18:10:35Z","2017-02-02T18:49:57Z"
"","2478","KAFKA-4702: Parametrize streams benchmarks to run at scale","","closed","","enothereska","2017-02-01T12:47:58Z","2017-02-08T22:03:13Z"
"","2476","KAFKA-4586; Add purgeDataBefore() API (KIP-107)","","closed","","lindong28","2017-02-01T05:11:37Z","2017-06-29T03:56:58Z"
"","2475","MINOR: Use an explicit `Errors` object when possible instead of a numeric error code","","closed","","vahidhashemian","2017-01-31T22:25:43Z","2017-02-10T05:22:25Z"
"","2474","KAFKA-4039: Fix deadlock during shutdown due to log truncation not allowed","","closed","","ijuma","2017-01-31T21:09:56Z","2017-09-05T09:30:29Z"
"","2473","KAFKA-4717: Use absolute paths to files in root directory so all jars include LICENSE and NOTICE files","","closed","","ewencp","2017-01-31T20:54:18Z","2017-02-01T00:07:46Z"
"","2469","MINOR: Logging improvements in consumer internals","","closed","","hachikuji","2017-01-31T06:20:35Z","2017-01-31T21:58:24Z"
"","2468","KAFKA-4613: Follow-up to fix JavaDocs","","closed","","mjsax","2017-01-30T23:36:00Z","2017-01-31T00:55:08Z"
"","2466","KAFKA-4144: Allow per stream/table timestamp extractor","","closed","","jeyhunkarimov","2017-01-30T18:12:23Z","2017-05-13T04:40:04Z"
"","2462","MINOR: JavaDoc markup cleanup","","closed","","mjsax","2017-01-28T01:21:20Z","2017-01-29T22:06:21Z"
"","2461","MINOR: added upgrade and API changes to docs","","closed","","mjsax","2017-01-27T23:38:48Z","2017-02-03T05:37:11Z"
"","2460","MINOR: Update copyright year in the NOTICE file.","","closed","","ewencp","2017-01-27T21:10:23Z","2017-01-27T22:49:56Z"
"","2458","KAFKA-4714: Flatten and Cast single message transforms (KIP-66)","","closed","connect,","ewencp","2017-01-27T07:27:04Z","2020-10-16T06:08:12Z"
"","2457","KAFKA-4450: Add upgrade tests for 0.10.1 releases and rename TRUNK to CURRENT_BRANCH to reduce confusion.","","closed","","ewencp","2017-01-27T03:25:53Z","2017-01-28T01:41:47Z"
"","2456","KAFKA-4705: ConfigDef should support deprecated keys as synonyms","","open","","cmccabe","2017-01-27T01:10:25Z","2018-03-02T19:29:53Z"
"","2455","KAFKA-4704: Coordinator cache loading fails if groupId is reused for offset storage after group is removed","","closed","","hachikuji","2017-01-26T23:09:36Z","2017-01-27T20:31:02Z"
"","2454","MINOR: Change loggin for ignored maybeAddMetric from debug to trace","","closed","","guozhangwang","2017-01-26T22:10:49Z","2017-07-15T22:07:25Z"
"","2445","KAFKA-4578: Upgrade notes for 0.10.2.0","","closed","","ijuma","2017-01-26T14:49:29Z","2017-09-05T09:30:35Z"
"","2443","MINOR: Close create topics policy during shutdown and more tests","","closed","","ijuma","2017-01-26T13:37:12Z","2017-09-05T09:30:47Z"
"","2441","KAFKA-4700: Don't drop security configs in `StreamsKafkaClient`","","closed","","ijuma","2017-01-26T12:11:42Z","2017-09-05T09:30:53Z"
"","2439","KAFKA-2700: Delete topic should remove the corresponding ACLs","","closed","","omkreddy","2017-01-26T09:00:36Z","2018-07-03T15:46:18Z"
"","2438","MINOR: update KTable JavaDoc","","closed","","mjsax","2017-01-26T08:24:58Z","2017-01-27T16:41:38Z"
"","2437","MINOR: Streams API JavaDoc improvements","","closed","","mjsax","2017-01-26T08:22:37Z","2017-01-27T05:51:26Z"
"","2436","HOTFIX: Consumer offsets not properly loaded on coordinator failover","","closed","","hachikuji","2017-01-26T06:49:13Z","2017-01-26T20:53:54Z"
"","2434","KAFKA-4698: VerifyError in kafka/client/ClientUtils due to -target:jvm-1.7","","closed","","ijuma","2017-01-26T00:40:35Z","2017-09-05T09:33:43Z"
"","2431","KAFKA-4547 (0.10.1 hotfix): Avoid unnecessary offset commit that could lead to an invalid offset position if partition is paused","","closed","","vahidhashemian","2017-01-24T20:39:42Z","2017-01-26T04:57:45Z"
"","2428","HOTIFX: streams system test do not start up correctly","","closed","","mjsax","2017-01-24T01:13:44Z","2017-01-25T03:54:19Z"
"","2427","KAFKA-4673: Fix thread-safety of Python VerifiableConsumer class","","closed","","ewencp","2017-01-24T00:05:05Z","2017-01-24T21:20:00Z"
"","2426","MINOR: Add offset information to consumer_test error messages","","closed","","hachikuji","2017-01-23T23:50:23Z","2017-01-24T00:44:59Z"
"","2424","KAFKA-4688: The Docker image should contain version 0.10.1.0 of Kafka","","closed","","cmccabe","2017-01-23T21:01:00Z","2019-05-20T18:33:55Z"
"","2420","KAFKA-4613: Treat null-key records the same way for joins and aggreations","","closed","","jeyhunkarimov","2017-01-22T16:37:26Z","2017-01-30T23:15:48Z"
"","2418","HOTFIX: KAFKA-4060 and KAFKA-4476 follow up","","closed","","mjsax","2017-01-22T04:28:55Z","2017-01-23T17:35:55Z"
"","2417","KAFKA-3835: Streams is creating two ProducerRecords for each send via RecordCollector","","closed","","jeyhunkarimov","2017-01-22T01:31:23Z","2017-01-25T00:37:13Z"
"","2416","MINOR: Refactor partition lag metric for cleaner encapsulation","","closed","","hachikuji","2017-01-21T05:16:27Z","2017-01-26T02:54:52Z"
"","2415","KAFKA-4547 (0.10.1 hotfix): Avoid unnecessary offset commit that could lead to an invalid offset position if partition is paused","","closed","","vahidhashemian","2017-01-21T04:11:13Z","2017-01-21T04:55:01Z"
"","2414","KAFKA-4635: Client Compatibility follow-ups","","closed","","cmccabe","2017-01-20T23:31:40Z","2019-05-20T18:34:20Z"
"","2413","MINOR: update JavaDoc for DSL PAPI-API","","closed","","mjsax","2017-01-20T19:25:14Z","2017-04-21T17:34:48Z"
"","2411","[WIP] KAFKA-4677: Avoid unnecessary task movement across threads of the same process during rebalance","","closed","","dguy","2017-01-20T16:51:05Z","2017-02-17T00:58:35Z"
"","2409","MINOR: Update consumer group describe output in the documentation","","closed","","vahidhashemian","2017-01-20T00:02:05Z","2017-01-25T00:25:02Z"
"","2407","logDirs empty error added","","closed","","vogetihrsh","2017-01-19T13:35:19Z","2017-01-22T23:14:15Z"
"","2406","KAFKA-4636; Per listener security settings overrides (KIP-103)","","closed","","ijuma","2017-01-19T13:16:37Z","2017-09-05T09:30:50Z"
"","2403","MINOR: add Streams system test for broker backwards compatibility","","closed","","mjsax","2017-01-18T22:43:52Z","2017-01-27T07:20:11Z"
"","2401","KAFKA-4671: Fix Streams window retention policy","","closed","","mjsax","2017-01-18T20:42:10Z","2017-01-24T01:01:26Z"
"","2400","MINOR: Clarify misleading comment in WordCount example","","closed","","gwenshap","2017-01-18T19:45:33Z","2017-01-26T02:00:53Z"
"","2395","KAFKA-4615: Use a timeout when polling the request in AdminClient","","closed","","Mogztter","2017-01-18T13:31:29Z","2018-11-29T15:51:27Z"
"","2392","MINOR: refactor streams system test class hierachy","","closed","","mjsax","2017-01-18T01:05:04Z","2017-01-19T18:33:38Z"
"","2391","MINOR: remove ZK from system tests","","closed","","mjsax","2017-01-18T00:57:01Z","2017-01-18T02:15:26Z"
"","2390","KAFKA-4630: Implement RecordTooLargeException when communicating with pre-KIP-74 brokers","","closed","","cmccabe","2017-01-17T23:26:50Z","2019-05-20T18:34:12Z"
"","2387","KAFKA-4664: Update docs/protocol.html with KIP-97 information","","closed","","cmccabe","2017-01-17T19:12:59Z","2019-05-20T18:33:45Z"
"","2384","KAFKA-4622: Consumer should handle group authorization errors in offset fetch","","closed","","hachikuji","2017-01-16T02:08:53Z","2017-01-16T10:57:34Z"
"","2383","MINOR: Some cleanups and additional testing for KIP-88","","closed","","hachikuji","2017-01-16T01:01:21Z","2017-01-17T21:23:00Z"
"","2380","MINOR: replace remove() with delete() after 5.0.1 RocksDB upgrade","","closed","","guozhangwang","2017-01-14T18:49:53Z","2017-07-15T22:07:30Z"
"","2372","MINOR: Remove unneeded client used API lists","","closed","","hachikuji","2017-01-13T18:47:40Z","2017-01-13T19:36:00Z"
"","2369","KAFKA-4589: SASL/SCRAM documentation","","closed","","rajinisivaram","2017-01-13T17:02:53Z","2017-01-19T16:36:33Z"
"","2368","MINOR: Remove unused CONSUMER_APIS and PRODUCER_APIS","","closed","","ijuma","2017-01-13T12:39:47Z","2017-09-05T09:44:14Z"
"","2367","KAFKA-4627: Fix timing issue in consumer close tests","","closed","","rajinisivaram","2017-01-13T12:29:19Z","2017-01-13T19:10:13Z"
"","2366","KAFKA-4626: Add KafkaConsumer#close change to upgrade notes","","closed","","rajinisivaram","2017-01-13T10:26:05Z","2017-01-13T17:33:40Z"
"","2364","MINOR: fix JavaDoc","","closed","","mjsax","2017-01-12T22:44:23Z","2017-01-12T23:45:10Z"
"","2362","HOTFIX: Added another broker to smoke test","","closed","","enothereska","2017-01-12T22:22:58Z","2017-01-13T19:12:03Z"
"","2361","KAFKA-4591: Create Topic Policy (KIP-108)","","closed","","ijuma","2017-01-12T21:15:34Z","2017-09-05T09:42:05Z"
"","2358","WIP -- DO NOT MERGE -- KAFKA-4222: Transient failure in QueryableStateIntegrationTest.queryOnRebalance","","closed","","mjsax","2017-01-12T17:57:17Z","2017-04-04T20:51:42Z"
"","2357","Fix error in design docs","","closed","","dasl-","2017-01-12T17:15:34Z","2017-01-12T19:29:32Z"
"","2354","KAFKA-4565: Separation of Internal and External traffic (KIP-103)","","closed","","ijuma","2017-01-12T14:13:38Z","2017-09-05T09:42:06Z"
"","2353","MINOR: Remove unnecessary options in SCRAM test jaas config","","closed","","rajinisivaram","2017-01-12T12:46:24Z","2017-01-13T02:21:40Z"
"","2348","MINOR: Minor improvements in consumer close timeout handling","","closed","","hachikuji","2017-01-12T01:25:29Z","2017-01-13T02:32:00Z"
"","2346","KAFKA-4619: Dissallow to output records with unknown keys in TransformValues","","closed","","mjsax","2017-01-11T21:13:11Z","2017-01-16T19:25:03Z"
"","2341","KAFKA-4547: Avoid unnecessary offset commit that could lead to an invalid offset position if partition is paused","","closed","","vahidhashemian","2017-01-10T21:46:07Z","2017-01-20T06:00:45Z"
"","2335","TRIVIAL: Fix spelling of log message","","closed","","reftel","2017-01-09T12:41:40Z","2017-01-10T06:12:45Z"
"","2332","MINOR: some public JavaDoc cleanup","","closed","","mjsax","2017-01-07T18:48:32Z","2017-01-09T23:18:06Z"
"","2329","KAFKA-4381: Add per partition lag metrics to the consumer","","closed","","becketqin","2017-01-06T22:36:58Z","2017-01-13T19:39:08Z"
"","2328","KAFKA-3264: Deprecate the old Scala consumer (KIP-109)","","closed","","vahidhashemian","2017-01-06T21:45:57Z","2017-06-02T15:08:31Z"
"","2321","MINOR: update JavaDoc for simple helper interfaces of KStream and KTable operators","","closed","","mjsax","2017-01-06T01:08:21Z","2017-01-06T16:24:50Z"
"","2316","KAFKA-4363: Documentation for sasl.jaas.config property","","closed","","rajinisivaram","2017-01-05T14:03:28Z","2017-01-17T12:57:44Z"
"","2315","MINOR: Update JavaDoc for KTable helper interfaces","","closed","","mjsax","2017-01-05T05:33:20Z","2017-01-05T20:05:50Z"
"","2314","MINOR: cleanup Kafka Streams exception classes","","closed","","mjsax","2017-01-05T02:09:25Z","2017-01-06T19:48:48Z"
"","2309","KAFKA-4583: Fix KafkaConsumerTest.testGracefulClose transient failure","","closed","","rajinisivaram","2017-01-04T11:03:48Z","2017-01-04T12:58:46Z"
"","2305","MINOR: Replacing for with foreach loop in stream test classes","","closed","","PKOfficial","2017-01-04T05:42:07Z","2017-01-10T19:53:46Z"
"","2303","MINOR: improve license header check by providing head file instead of (prefix) header regex","","closed","","mjsax","2017-01-04T01:02:58Z","2017-03-02T21:52:52Z"
"","2302","KAFKA-4531 (WIP): Centralize validation of configs for consumer and producer","","open","","vahidhashemian","2017-01-04T00:14:31Z","2018-03-02T19:29:51Z"
"","2301","KAFKA-3856: Cleanup Kafka Stream builder API (KIP-120)","","closed","","mjsax","2017-01-03T22:16:52Z","2017-07-23T18:08:30Z"
"","2300","KAFKA-4584: Fail the 'kafka-configs' command if the config to be removed does not exist","","closed","","vahidhashemian","2017-01-03T20:31:18Z","2017-01-04T05:41:56Z"
"","2297","MINOR: Code refactoring in scala classes.","","closed","","himani1","2017-01-03T09:31:43Z","2017-01-03T11:19:39Z"
"","2296","KAFKA-4404: Add javadocs to document core Connect types, especially that integer types are signed","","closed","connect,","ewencp","2016-12-29T21:59:19Z","2020-10-16T06:08:12Z"
"","2292","MINOR: Update rocksDB dependency to 5.0.1","","closed","","jozanek","2016-12-27T08:33:14Z","2017-01-11T17:37:47Z"
"","2291","MINOR: Fix typo","","closed","","jeffwidman","2016-12-23T23:41:21Z","2016-12-28T11:57:46Z"
"","2284","MINOR: KStream JavaDoc fix","","closed","","mjsax","2016-12-21T10:17:54Z","2016-12-21T17:39:47Z"
"","2283","HOTFIX: Convert exception to warning since clearly it is happening.","","closed","","enothereska","2016-12-20T22:30:57Z","2017-01-10T09:55:35Z"
"","2282","MINOR: Support auto-incrementing offsets in MemoryRecordsBuilder","","closed","","hachikuji","2016-12-20T20:03:07Z","2016-12-21T01:10:30Z"
"","2279","KAFKA-4166: Fix transient MM failure caused by slow old consumer shutdown","","closed","","hachikuji","2016-12-19T22:47:10Z","2016-12-20T00:41:21Z"
"","2276","MINOR: Add more exception information in ProcessorStateManager","","closed","","guozhangwang","2016-12-19T21:30:52Z","2017-07-15T22:09:10Z"
"","2274","Implement topic config for internal topics","","closed","","sjmittal","2016-12-19T06:18:27Z","2017-02-15T03:08:11Z"
"","2272","KAFKA-4553: Improve round robin assignment in Connect to avoid uneven distributions of connectors and tasks","","closed","connect,","ewencp","2016-12-18T00:14:19Z","2020-10-16T06:08:12Z"
"","2268","MINOR: Replace TopicAndPartition with TopicPartition in `Log` and `ReplicaManager`","","closed","","ijuma","2016-12-16T17:32:08Z","2016-12-21T01:12:34Z"
"","2265","KAFKA-4549: Change to call flush method before writeEndMark method in close method of KafkaLZ4BlockOutputStream","","closed","","fossamagna","2016-12-16T05:15:55Z","2016-12-28T12:52:39Z"
"","2262","KAFKA-4548: Add CompatibilityTest to verify that individual features …","","closed","","cmccabe","2016-12-15T22:36:47Z","2019-05-20T18:34:03Z"
"","2259","MINOR: Fix typo on introduction page","","closed","","ashishg-qburst","2016-12-15T05:20:27Z","2016-12-16T01:27:25Z"
"","2258","MINOR: update KStream JavaDocs","","closed","","mjsax","2016-12-15T00:44:37Z","2016-12-15T23:55:10Z"
"","2251","KAFKA-4529; Fix the issue that tombstone can be deleted too early.","","closed","","becketqin","2016-12-13T19:03:47Z","2016-12-26T02:55:52Z"
"","2248","add instrumentation to follower local time","","closed","","rnpridgeon","2016-12-12T22:35:52Z","2016-12-12T22:46:20Z"
"","2246","KAFKA-4525: Kafka should not require SSL trust store password","","closed","","granthenke","2016-12-12T20:27:32Z","2017-02-10T03:02:27Z"
"","2241","KAFKA-4521; MirrorMaker should flush all messages before releasing partition ownership during rebalance","","closed","","lindong28","2016-12-11T07:04:50Z","2016-12-15T22:23:04Z"
"","2234","KAFKA-4431: Make consumer heartbeat thread a daemon thread","","closed","","rajinisivaram","2016-12-09T13:58:58Z","2016-12-12T03:08:55Z"
"","2233","KAFKA-4509: Task reusage on rebalance fails for threads on same host","","closed","","mjsax","2016-12-09T00:33:48Z","2016-12-13T20:12:04Z"
"","2231","NOMERGE: Test new Jenkins PR plugin v4","","closed","","ijuma","2016-12-08T12:21:09Z","2016-12-21T00:08:04Z"
"","2230","NOMERGE: Test new Jenkins PR plugin v3","","closed","","ijuma","2016-12-08T12:17:38Z","2016-12-08T12:32:42Z"
"","2229","NOMERGE: Test new Jenkins PR plugin v2","","closed","","ijuma","2016-12-08T12:04:57Z","2016-12-08T12:15:38Z"
"","2228","NOMERGE: Test new Jenkins PR plugin","","closed","","ijuma","2016-12-08T11:47:47Z","2016-12-08T12:16:00Z"
"","2227","KAFKA-4510: StreamThread must finish rebalance in state PENDING_SHUTDOWN","","closed","","mjsax","2016-12-07T23:57:05Z","2016-12-11T05:54:21Z"
"","2225","KAFKA-4486: Don't commit offsets on exception","","closed","","enothereska","2016-12-07T16:10:01Z","2016-12-10T01:18:34Z"
"","2224","KAFKA-4503: Expose the log dir for a partition as a metric","","closed","","ijuma","2016-12-07T13:17:10Z","2016-12-07T16:19:49Z"
"","2220","MINOR: Update ducktape version to 0.5.3","","closed","","ewencp","2016-12-06T21:50:46Z","2016-12-06T21:58:11Z"
"","2219","HOTFIX: Disable this test until it's fixed (0.10.1)","","closed","","enothereska","2016-12-06T20:55:29Z","2016-12-07T09:27:26Z"
"","2218","KAFKA-4480: Report an error in 'kafka-configs' command if the config to be removed does not exist","","closed","","vahidhashemian","2016-12-06T20:37:58Z","2017-01-03T18:27:09Z"
"","2217","HOTFIX: Disable test until fixed","","closed","","enothereska","2016-12-06T20:30:04Z","2016-12-07T00:24:45Z"
"","2215","KAFKA-3537: Expose metrics registry","","closed","","enothereska","2016-12-06T14:14:24Z","2017-04-09T12:12:22Z"
"","2214","KAFKA-1595; remove global lock from json parser","","closed","","resetius","2016-12-06T11:45:45Z","2017-07-26T14:19:30Z"
"","2213","KAFKA-3038; Future'based pseudo-async controller","","closed","","resetius","2016-12-06T11:26:00Z","2017-07-26T14:19:36Z"
"","2211","HOTFIX: Fix bug in readToLogEnd in KafkaBasedLog.","","closed","connect,","kkonstantine","2016-12-05T08:08:32Z","2020-10-16T06:08:11Z"
"","2208","KAFKA-4485; Follower should be in the isr if its FetchRequest has fetched up to the logEndOffset of leader","","closed","","lindong28","2016-12-03T03:34:24Z","2016-12-21T13:22:56Z"
"","2207","KAFKA-4483; Fix NPE in `Log` constructor if log level is INFO or finer","","closed","","ijuma","2016-12-03T01:42:29Z","2016-12-03T04:12:19Z"
"","2202","MINOR: Improvements in group metadata cleanup and test coverage","","closed","","hachikuji","2016-12-02T04:41:37Z","2016-12-02T19:02:49Z"
"","2199","HOTFIX: Temporary suspension of 2 tests","","closed","","enothereska","2016-12-01T18:24:03Z","2016-12-06T22:12:14Z"
"","2198","HOTFIX: Forgot one transition","","closed","","enothereska","2016-12-01T11:52:37Z","2016-12-02T18:03:34Z"
"","2197","KAFKA-4465: Create docker image and scripts for running tests locally","","closed","","raghavgautam","2016-12-01T02:36:35Z","2017-01-09T00:13:32Z"
"","2196","KAFKA-3910: prototype of another approach to cyclic schemas","","open","","shikhar","2016-12-01T01:40:57Z","2018-03-02T19:29:50Z"
"","2195","KAFKA-3994: Fix deadlock in Watchers by calling tryComplete without any locks","","closed","","hachikuji","2016-11-30T22:09:47Z","2016-12-05T19:42:31Z"
"","2193","KAFKA-4405: avoid calling pollNoWakeup unnecessarily","","closed","","enothereska","2016-11-30T09:30:49Z","2016-12-13T13:29:20Z"
"","2191","KAFKA-4447: Controller resigned but it also acts as a controller for a long time","","closed","","xiguantiaozhan","2016-11-30T05:47:18Z","2016-12-20T02:49:03Z"
"","2190","KAFKA-4469: Fix consumer performance regression from list removal and copy","","closed","","hachikuji","2016-11-30T02:32:03Z","2016-11-30T21:34:26Z"
"","2186","KAFKA-4458 add per partition in-sync and assigned replica count","","closed","","xvrl","2016-11-29T05:43:22Z","2017-04-17T21:57:40Z"
"","2185","MINOR: added logging to debug test","","closed","","mjsax","2016-11-29T00:46:14Z","2016-12-02T01:02:33Z"
"","2184","KAFKA-4457. Add BrokerVersionCommand","","closed","","cmccabe","2016-11-28T23:13:01Z","2019-05-20T18:33:35Z"
"","2182","ConfigDef experimentation - support List and Map","","closed","","shikhar","2016-11-28T19:32:39Z","2017-01-11T19:27:11Z"
"","2180","KAFKA-4439: NetworkClient: create a builder class to encapsulate ctor arguments","","closed","","cmccabe","2016-11-28T17:51:48Z","2019-05-20T18:32:43Z"
"","2179","MINOR: Fix typos in KafkaConsumer docs","","closed","","jeffwidman","2016-11-28T16:31:41Z","2016-12-02T02:01:38Z"
"","2178","Minor: Fix typos in KafkaConsumer docs","","closed","","jeffwidman","2016-11-28T16:22:34Z","2016-12-09T19:13:31Z"
"","2174","MINOR: Make release notes script check resolutions to avoid spurious inclusion of non-fix 'fixes' in release notes.","","closed","","ewencp","2016-11-27T00:55:12Z","2016-11-29T19:37:12Z"
"","2172","HOTFIX: KAFKA-4245 follow up: remove gradle wrapper, fix file headers so Rat checks pass, and move Travis cluster file into travis directory.","","closed","","ewencp","2016-11-25T19:33:24Z","2016-11-29T17:15:01Z"
"","2171","KAFKA-4427: Skip topic groups with no tasks","","closed","","enothereska","2016-11-25T15:53:52Z","2016-11-29T19:08:39Z"
"","2170","KAFKA-4445; PreferredLeaderElectionCommand should query zookeeper only once per topic","","closed","","lindong28","2016-11-25T06:09:54Z","2016-12-07T18:10:25Z"
"","2169","KAFKA-4415; Reduce time to create and send MetadataUpdateRequest","","closed","","lindong28","2016-11-25T04:33:38Z","2016-11-30T13:19:04Z"
"","2168","KAFKA-4443; Controller should send UpdateMetadataRequest prior to LeaderAndIsrRequest during failover","","closed","","lindong28","2016-11-25T04:33:10Z","2016-11-30T10:59:04Z"
"","2167","KAFKA-4442; Controller should grab lock when it is being initialized to avoid race condition","","closed","","lindong28","2016-11-25T04:08:55Z","2016-12-06T22:49:45Z"
"","2165","KAFKA-4440: Make producer RecordMetadata non-final","","closed","","rajinisivaram","2016-11-24T10:15:33Z","2017-09-25T08:29:57Z"
"","2160","KAFKA-4095: Remove topic offsets and owners from ZK consumer groups upon topic deletion","","closed","","vahidhashemian","2016-11-22T23:29:34Z","2018-07-08T02:15:28Z"
"","2157","Typo in Javadoc","","closed","","astubbs","2016-11-22T10:10:34Z","2016-11-22T11:30:10Z"
"","2155","KAFKA-4429; records-lag should be zero if FetchResponse is empty","","closed","","lindong28","2016-11-22T00:39:35Z","2017-01-03T23:56:27Z"
"","2154","HOTFIX: Increased wait time","","closed","","enothereska","2016-11-21T09:47:01Z","2016-11-22T09:22:30Z"
"","2153","MINOR: Update JavaDoc of KStream interface","","closed","","mjsax","2016-11-21T05:52:14Z","2016-12-08T20:04:19Z"
"","2151","Feature/authorizer name reference","","closed","","reftel","2016-11-19T13:48:14Z","2017-01-23T16:25:54Z"
"","2149","HOTFIX: Hotfix streams smoke test","","closed","","enothereska","2016-11-18T14:17:42Z","2016-11-21T09:47:24Z"
"","2148","KAFKA-4420; Group StopReplicaRequests for partitions on the same broker into one StopReplicaRequest","","closed","","lindong28","2016-11-18T00:55:10Z","2016-11-19T02:17:29Z"
"","2147","revert corrupted commit 10cfc1628df024f7596d3af5c168fa90f59035ca","","closed","","mjsax","2016-11-17T23:08:58Z","2016-11-18T20:36:34Z"
"","2146","KAFKA-4272: Add missing 'connect' Windows batch scripts","","closed","","vahidhashemian","2016-11-17T22:02:43Z","2016-12-09T20:21:55Z"
"","2145","MINOR: Remove unused code in `LeaderAndIsr`, `ApiUtils` and `TopicMetadataRequest`","","closed","","ijuma","2016-11-17T16:36:48Z","2016-11-29T18:34:09Z"
"","2141","KAFKA-4444; Aggregate requests sent from controller to broker during controlled shutdown","","closed","","lindong28","2016-11-16T05:32:43Z","2016-11-29T01:09:20Z"
"","2140","KAFKA-4390: Replace MessageSet usage with client-side alternatives","","closed","","hachikuji","2016-11-16T04:24:03Z","2021-01-26T02:25:09Z"
"","2139","KAFKA-4161: KIP-89: Allow sink connectors to decouple flush and offset commit","","closed","","shikhar","2016-11-16T01:00:17Z","2016-12-01T23:01:59Z"
"","2137","KAFKA-1548 Refactor the ""replica_id"" in requests","","closed","","baluchicken","2016-11-15T13:34:13Z","2017-12-22T20:26:24Z"
"","2136","MINOR: Remove unused `ByteBoundedBlockingQueue` class and `zkSessionTimeout` parameter","","closed","","ijuma","2016-11-15T11:54:40Z","2016-11-16T17:52:16Z"
"","2135","KAFKA-3637: Added initial states","","closed","","enothereska","2016-11-15T11:43:17Z","2016-12-01T07:56:07Z"
"","2134","KAFKA-4406: add ssl.provider.classes config option","","closed","","reftel","2016-11-15T11:25:22Z","2019-12-19T15:23:20Z"
"","2133","KAFKA-4355: Skip topics that have no partitions","","closed","","enothereska","2016-11-15T10:04:01Z","2016-11-22T17:55:48Z"
"","2128","KAFKA-4402: make the KafkaProducer true round robin per topic","","closed","","yaojuncn","2016-11-13T21:47:27Z","2019-09-05T07:53:22Z"
"","2126","MINOR: Fix export command for additional env vars in connect system tests","","closed","","kkonstantine","2016-11-11T21:41:48Z","2016-11-11T23:01:24Z"
"","2125","KAFKA-4399; deadlock cleanupGroupMetadata and offset commit fixed","","closed","","resetius","2016-11-11T19:59:44Z","2016-12-02T03:57:44Z"
"","2124","KAFKA-4359: Removed commit interval","","closed","","enothereska","2016-11-11T12:52:17Z","2016-11-16T20:47:49Z"
"","2123","KAFKA-4397: Refactor Connect backing stores for thread safety","","closed","connect,","kkonstantine","2016-11-11T07:40:29Z","2020-10-16T06:09:17Z"
"","2121","KAFKA-4392: Handle NoSuchFileException gracefully in StateDirectory","","closed","","guozhangwang","2016-11-10T21:40:06Z","2017-07-15T22:09:09Z"
"","2118","MINOR: improve exception message for incompatible Serdes to actual key/value data types","","closed","","mjsax","2016-11-09T03:05:30Z","2016-11-11T01:07:21Z"
"","2117","KAFKA-4393: Improve invalid/negative TS handling","","closed","","mjsax","2016-11-09T02:38:36Z","2016-12-10T00:18:25Z"
"","2116","KAFKA-4362 : Consumer can fail after reassignment of the offsets topic partition","","closed","","MayureshGharat","2016-11-09T01:46:00Z","2016-11-23T04:37:00Z"
"","2114","MINOR: add upgrade guide for Kafka Streams API","","closed","","mjsax","2016-11-08T22:46:52Z","2016-11-09T23:31:52Z"
"","2113","KAFKA-4376: Cross compile to Scala 2.12.0","","closed","","leachbj","2016-11-08T22:37:07Z","2016-11-24T21:22:32Z"
"","2112","MINOR: fix typos and incorrect docs","","closed","","xvrl","2016-11-08T19:14:15Z","2017-04-17T21:57:09Z"
"","2111","throw exception when the connection is error.","","closed","","huyanping","2016-11-08T04:23:50Z","2017-01-05T01:19:57Z"
"","2108","MINOR: Fix regex on connector path param in ConnectorsResource","","closed","","ewencp","2016-11-07T01:05:40Z","2016-11-07T16:49:15Z"
"","2099","MINOR: Fix re-raise of python error in system tests","","closed","","ewencp","2016-11-03T21:54:54Z","2016-11-04T22:57:54Z"
"","2098","KAFA-4378: resolve eta-expansion of zero-argument method warnings","","closed","","leachbj","2016-11-03T21:45:49Z","2017-05-03T09:21:09Z"
"","2094","KAFKA-4360：Controller may deadLock when autoLeaderRebalance encounter zk expired","","closed","","xiguantiaozhan","2016-11-03T03:58:07Z","2016-11-09T18:38:47Z"
"","2091","MINOR: Bug fixed","","closed","","himani1","2016-11-02T10:15:27Z","2016-11-04T04:35:47Z"
"","2090","KAFKA-4269: Follow up for 0.10.1 branch -update topic subscriptions for regex","","closed","","bbejeck","2016-11-02T00:16:39Z","2016-12-13T18:18:18Z"
"","2088","Cross compile to Scala 2.12.0-RC2","","closed","","leachbj","2016-11-01T22:08:12Z","2016-11-17T02:39:03Z"
"","2085","KAFKA-4360：Controller may deadLock when autoLeaderRebalance encounter zk expired","","closed","","xiguantiaozhan","2016-11-01T12:35:55Z","2016-11-03T03:51:34Z"
"","2083","KAFKA-4318 Migrate ProducerSendTest to the new consumer","","closed","","baluchicken","2016-11-01T09:40:31Z","2017-01-03T16:08:42Z"
"","2081","MINOR: Extend mirror maker test to include interceptors","","closed","connect,","kkonstantine","2016-11-01T00:05:57Z","2020-10-16T06:09:16Z"
"","2079","HOTFIX: improve error message on invalid input record timestamp","","closed","","mjsax","2016-10-31T21:57:52Z","2016-11-01T21:27:02Z"
"","2076","HOTFIX: improve error message on invalid input record timestamp","","closed","","mjsax","2016-10-28T22:13:48Z","2016-10-30T18:38:21Z"
"","2075","KAFKA-4357: Fix consumer group describe output when there is no active member (old consumer)","","closed","","vahidhashemian","2016-10-28T21:38:42Z","2016-11-01T19:31:59Z"
"","2071","KAFKA-4340: Change default message.timestamp.difference.max.ms to the same as log.retention.ms","","closed","","becketqin","2016-10-27T03:29:11Z","2017-02-12T12:56:48Z"
"","2069","KAFKA-2066: Use client-side FetchRequest/FetchResponse on server","","closed","","hachikuji","2016-10-26T19:18:30Z","2016-11-15T00:47:08Z"
"","2068","MINOR: improve JavaDoc for Streams window retention time","","closed","","mjsax","2016-10-26T18:05:36Z","2016-10-26T19:58:46Z"
"","2067","Add missing zookeeper parameter","","closed","","thbar","2016-10-26T16:19:28Z","2016-10-26T17:59:32Z"
"","2061","KAFKA-4339: Update system tests to accommodate the new consumer group describe output","","closed","","vahidhashemian","2016-10-25T03:19:33Z","2016-10-25T12:08:38Z"
"","2055","check for null timestamp rather than value in hashcode","","closed","","andrewstevenson","2016-10-22T13:29:55Z","2016-10-25T18:11:36Z"
"","2053","KAFKA-4326: Refactor LogCleaner for better reuse of common copy/compress logic","","closed","","hachikuji","2016-10-21T22:53:45Z","2016-10-28T10:32:25Z"
"","2052","MINOR: add list_topics command to help debug tests","","closed","","xvrl","2016-10-21T21:39:15Z","2017-04-17T21:56:49Z"
"","2050","Replaced unnecessary isDefined and get on option values with fold","","closed","","himani1","2016-10-21T11:04:38Z","2016-11-02T05:43:03Z"
"","2043","KAFKA-4313: ISRs may thrash when replication quota is enabled","","closed","","junrao","2016-10-19T02:09:56Z","2016-10-20T10:13:44Z"
"","2041","MINOR: Clarify how to fix conversion issues when plain JSON data is used with schemas.enable=true","","closed","","ewencp","2016-10-18T21:19:19Z","2016-11-15T14:24:29Z"
"","2040","KAFKA-4161: prototype for exploring API change","","closed","","shikhar","2016-10-18T19:51:52Z","2016-10-26T07:15:22Z"
"","2039","HOTFIX: follow up on KAFKA-4275","","closed","","mjsax","2016-10-18T19:20:17Z","2016-11-01T17:55:51Z"
"","2038","HOTFIX: Fix put logic","","closed","","enothereska","2016-10-18T16:05:46Z","2016-11-01T17:24:23Z"
"","2035","Replaced unnecessary map and getOrElse with exists","","closed","","himani1","2016-10-18T08:40:57Z","2016-10-19T05:12:51Z"
"","2034","KAFKA-4309: Allow ""pluggable"" properties in KafkaService in System Tests","","closed","","benstopford","2016-10-17T21:26:14Z","2016-10-21T09:12:59Z"
"","2033","Documentation for Throttled Replication","","closed","","benstopford","2016-10-17T21:04:45Z","2016-10-18T21:25:31Z"
"","2032","KAFKA-3559: Recycle old tasks when possible","","closed","","enothereska","2016-10-17T10:48:28Z","2016-10-30T18:39:31Z"
"","2031","KAFKA-4303: Ensure commitSync does not block unnecessarily in poll without in-flight requests","","closed","","hachikuji","2016-10-14T20:35:03Z","2016-10-14T22:37:35Z"
"","2030","MINOR: Added more basic concepts to the documentation","","closed","","enothereska","2016-10-14T17:07:10Z","2016-10-19T21:22:36Z"
"","2029","MINOR: Some images should be centered in the documentation","","closed","","hachikuji","2016-10-14T16:58:09Z","2016-10-14T17:05:01Z"
"","2027","KAFKA-4301: Add more trace for SSL handshake","","closed","","rajinisivaram","2016-10-14T09:25:02Z","2016-10-25T13:21:29Z"
"","2026","MINOR: Improve on Streams log4j","","closed","","guozhangwang","2016-10-13T21:56:54Z","2017-07-15T22:09:05Z"
"","2023","KAFKA-4319; AbstractFetcherManager: shutdown speedup","","closed","","resetius","2016-10-13T14:17:22Z","2016-10-19T16:52:41Z"
"","2021","MINOR: remove duplicate doc headers","","closed","","omkreddy","2016-10-13T09:49:48Z","2018-07-03T15:46:16Z"
"","2020","Removed unnecessary if/else clause.","","closed","","himani1","2016-10-13T07:27:39Z","2016-10-19T00:08:25Z"
"","2019","KAFKA-4298: Ensure compressed message sets are not converted when log cleaning","","closed","","hachikuji","2016-10-13T05:45:14Z","2016-10-14T04:25:47Z"
"","2016","KAFKA-4296: Fix LogCleaner statistics rolling","","closed","","hachikuji","2016-10-13T00:33:31Z","2016-10-21T21:41:55Z"
"","2014","HOTFIX: Increase number of retries in smoke test","","closed","","enothereska","2016-10-12T08:46:53Z","2016-10-12T19:45:48Z"
"","2013","MINOR: Code refactor - cleaning up some boolean assignments","","closed","","imandhan","2016-10-12T01:01:50Z","2016-10-18T00:16:24Z"
"","2010","MINOR: Fixed broken links in the documentation","","closed","","vahidhashemian","2016-10-11T04:58:56Z","2016-10-12T03:26:31Z"
"","2009","KAFKA-4290: Fix timeout overflow in WorkerCoordinator.poll","","closed","","hachikuji","2016-10-11T04:30:12Z","2016-10-11T06:23:29Z"
"","2008","MINOR: Add images missing from documentation","","closed","","hachikuji","2016-10-11T02:53:33Z","2016-10-11T02:59:06Z"
"","1996","MINOR: Fixed introduction doc - wrong streams api link","","closed","","JakubDziworski","2016-10-08T08:32:39Z","2016-10-10T17:16:22Z"
"","1994","MINOR: Introduction Doc: Fixed incomplete sentence","","closed","","JakubDziworski","2016-10-08T07:36:03Z","2016-10-10T16:27:00Z"
"","1993","KAFKA-4274: offsetsForTimes() hang on empty map.","","closed","","becketqin","2016-10-07T23:17:26Z","2016-10-08T01:45:17Z"
"","1992","KAFKA-4275: Check of State-Store-assignment to Processor-Nodes is not enabled","","closed","","mjsax","2016-10-07T22:31:10Z","2016-10-18T04:49:19Z"
"","1990","MINOR: Update Quickstart in documentation to account for Windows platforms","","closed","","vahidhashemian","2016-10-07T17:51:45Z","2016-10-09T17:33:06Z"
"","1987","KAFKA-4267: Fix and test quota config path used for initialization","","closed","","rajinisivaram","2016-10-07T10:17:58Z","2016-10-07T11:29:21Z"
"","1981","KAFKA-4261: Provide debug option in vagrant-up.sh","","closed","","fpj","2016-10-06T13:05:31Z","2017-01-03T22:50:35Z"
"","1980","KAFKA-4252: Fix metric name in documentation","","closed","","rajinisivaram","2016-10-06T11:47:06Z","2016-10-06T18:21:02Z"
"","1973","KAFKA-3985: Transient system test failure ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol","","closed","","fpj","2016-10-05T16:10:24Z","2016-10-05T22:05:00Z"
"","1969","MINOR: missing fullstop in doc for `max.partition.fetch.bytes`","","closed","","shikhar","2016-10-04T22:43:26Z","2016-11-04T20:30:10Z"
"","1968","MINOR: missing whitespace in doc for `ssl.cipher.suites`","","closed","","shikhar","2016-10-04T22:42:18Z","2016-11-04T21:02:26Z"
"","1965","KAFKA-4176: Only call printStream.flush for System.out","","closed","","guozhangwang","2016-10-04T21:38:55Z","2017-07-15T22:09:04Z"
"","1963","MINOR: Update documentation for 0.10.1 release","","closed","","hachikuji","2016-10-04T20:57:37Z","2016-12-26T22:37:49Z"
"","1960","MINOR: Tweak upgrade note on KIP-62 to include request.timeout.ms","","closed","","hachikuji","2016-10-04T17:34:33Z","2016-10-04T17:39:22Z"
"","1959","KAFKA-4165: Add 0.10.0.1 as a source for compatibility tests","","closed","","hachikuji","2016-10-04T17:12:54Z","2016-10-04T18:13:26Z"
"","1958","MINOR: trivial doc cleanup","","closed","","omkreddy","2016-10-04T10:55:36Z","2018-07-03T15:46:15Z"
"","1956","MINOR: Add upgrade notes for KIP-62","","closed","","hachikuji","2016-10-04T02:12:13Z","2016-10-04T04:17:58Z"
"","1955","MINOR: Tweak implementation of `FetchRequest.shuffle` and upgrade.html improvements","","closed","","ijuma","2016-10-04T00:04:38Z","2016-10-04T01:19:05Z"
"","1954","KAFKA-4248: Consumer should rematch regex immediately in subscribe","","closed","","hachikuji","2016-10-03T21:11:07Z","2016-10-03T23:20:08Z"
"","1953","KAFKA-4247: remove root from traversal path","","closed","","rnpridgeon","2016-10-03T20:34:42Z","2018-01-09T18:46:08Z"
"","1952","MINOR: Only require re-join if the metadata snapshot's topics info has changed","","closed","","guozhangwang","2016-10-03T15:40:04Z","2016-10-05T03:03:53Z"
"","1947","HOTFIX: ProducerPerformanceTest should not use mutable default arguments","","closed","","hachikuji","2016-10-01T22:58:57Z","2016-10-02T00:22:09Z"
"","1946","MINOR: Follow-up minor improvements/cleanup for KAFKA-3396","","closed","","hachikuji","2016-10-01T06:52:44Z","2016-10-04T00:24:19Z"
"","1945","HOTFIX: Revert async change in ProduceConsumeValidateTest","","closed","","hachikuji","2016-09-30T23:43:17Z","2016-10-01T00:16:54Z"
"","1940","HOTFIX: recreate state.dir after cleanup","","closed","","mjsax","2016-09-30T05:56:08Z","2016-10-07T20:14:07Z"
"","1939","KAFKA-4235: Fix the race condition in Sender.initiateClose() that may result in missing some callbacks.","","closed","","becketqin","2016-09-30T00:24:31Z","2016-09-30T18:51:15Z"
"","1938","KAFKA-4058: improved test condition evaluation","","closed","","mjsax","2016-09-29T23:27:23Z","2016-09-30T19:35:28Z"
"","1936","KAFKA-3824: Clarify autocommit delivery semantics for consumer","","closed","","hachikuji","2016-09-29T22:52:20Z","2016-10-01T00:26:30Z"
"","1935","KAFKA-4232: make MemoryRecords.writable volatile.","","closed","","becketqin","2016-09-29T18:13:15Z","2016-09-30T00:20:19Z"
"","1933","KAFKA-4230: HistogramSample needs override Sample's reset method","","open","","iBuddha","2016-09-29T08:38:29Z","2018-03-02T19:29:43Z"
"","1932","KAFKA-4225: Replication Quotas: Control Leader & Follower Limit Separately","","closed","","benstopford","2016-09-29T08:28:54Z","2016-09-30T10:23:47Z"
"","1931","HOTFIX: Tools for releases prior to 0.10.1 need --new-consumer flag","","closed","","hachikuji","2016-09-29T06:22:36Z","2016-09-29T06:50:31Z"
"","1929","HOTFIX: do not call partitioner if num partitions is non-positive","","closed","","guozhangwang","2016-09-28T21:34:37Z","2016-09-30T16:37:40Z"
"","1922","HOTFIX: remove git conflicts","","closed","","guozhangwang","2016-09-28T00:55:53Z","2016-09-28T01:41:03Z"
"","1919","MINOR: fixes a few error logging formats","","closed","","guozhangwang","2016-09-27T18:14:46Z","2016-09-28T00:37:46Z"
"","1915","KAFKA-3965 mirror maker should not commit offset when exception is thrown from producer.send()","","closed","","becketqin","2016-09-27T05:08:17Z","2016-09-29T23:43:39Z"
"","1914","MINOR: Make new consumer default for Mirror Maker","","closed","","hachikuji","2016-09-27T03:38:46Z","2016-09-27T21:10:53Z"
"","1911","KAFKA-3829: Ensure valid configuration prior to creating connector","","closed","","hachikuji","2016-09-26T23:51:52Z","2016-11-13T00:13:52Z"
"","1907","MINOR: Wakeups propagated from commitOffsets in WorkerSinkTask should be caught","","closed","","hachikuji","2016-09-26T17:36:16Z","2016-10-25T17:51:06Z"
"","1905","MINOR: Remove no longer required --new-consumer switch in docs","","closed","","ijuma","2016-09-25T08:45:46Z","2016-09-27T23:50:30Z"
"","1901","MINOR: Fix Javadoc for KafkaConsumer.poll","","closed","","reftel","2016-09-23T07:05:49Z","2016-09-27T04:16:05Z"
"","1898","KAFKA-3782: Ensure heartbeat thread restarted after rebalance woken up","","closed","","hachikuji","2016-09-22T05:26:33Z","2016-09-22T17:08:45Z"
"","1897","KAFKA-4194 follow up patch for KIP-79","","closed","","becketqin","2016-09-21T17:48:16Z","2016-09-29T02:45:35Z"
"","1887","HOTFIX: Added check for metadata unavailable","","closed","","enothereska","2016-09-20T07:10:36Z","2016-09-20T10:39:36Z"
"","1886","MINOR: Bump to version 0.10.2","","closed","","hachikuji","2016-09-20T03:44:49Z","2016-09-20T04:20:15Z"
"","1882","MINOR: some trace logging for streams debugging","","closed","","norwood","2016-09-19T17:16:51Z","2016-11-08T22:50:15Z"
"","1881","KAFKA-4193: Fix for Intermittent failure in FetcherTest","","closed","","benstopford","2016-09-19T17:04:56Z","2016-09-20T06:18:04Z"
"","1874","HOTFIX: Increase timeout for bounce test","","closed","","enothereska","2016-09-17T19:58:44Z","2016-09-17T21:44:48Z"
"","1869","HOTFIX: changed quickstart donwload from 0.10.0.0 to 0.10.0.1","","closed","","mjsax","2016-09-16T20:28:57Z","2016-09-17T21:46:06Z"
"","1866","MINOR: Add test cases for delays in consumer rebalance listener","","closed","","hachikuji","2016-09-16T19:09:25Z","2016-09-22T22:04:29Z"
"","1865","KAFKA-4173: SchemaProjector should successfully project missing Struct field when target field is optional","","closed","connect,","shikhar","2016-09-16T18:18:44Z","2020-10-16T06:09:16Z"
"","1863","KAFKA-4126: Log a server-side warning when a record is sent to a missing topic with auto create disabled","","open","","vahidhashemian","2016-09-15T23:42:44Z","2022-06-14T12:51:28Z"
"","1859","KAFKA-3590: Handle not-enough-replicas errors when writing to offsets topic","","closed","","hachikuji","2016-09-15T03:28:09Z","2016-09-23T20:15:10Z"
"","1857","KAFKA-4172: Ensure fetch responses contain the requested partitions","","closed","","hachikuji","2016-09-15T00:26:25Z","2016-09-15T01:05:40Z"
"","1855","KAFKA-4160: Ensure rebalance listener not called with coordinator lock","","closed","","hachikuji","2016-09-14T18:34:21Z","2016-09-15T05:32:29Z"
"","1853","KAFKA-4162: Fixed typo ""rebalance""","","closed","","mvj3","2016-09-14T09:21:25Z","2016-09-14T17:39:16Z"
"","1852","KAFKA-4148: KIP-79 ListOffsetRequest v1 and add search offsets by timestamp in consumer","","closed","","becketqin","2016-09-14T00:13:57Z","2016-09-20T01:39:28Z"
"","1851","KAFKA-4158; Reset quota to default value if quota override is deleted","","closed","","lindong28","2016-09-13T18:26:40Z","2016-09-14T00:34:46Z"
"","1850","KAFKA-2000: Delete topic should also delete consumer offsets.","","closed","","omkreddy","2016-09-13T17:00:25Z","2018-07-03T15:46:17Z"
"","1848","KAFKA-3930: Replace colon in ipv6 addresses used in metrics tags","","closed","","rajinisivaram","2016-09-13T13:53:12Z","2016-09-30T19:40:34Z"
"","1842","KAFKA-4145: Avoid redundant integration testing in ProducerSendTests","","closed","","vahidhashemian","2016-09-09T23:16:55Z","2016-09-10T07:17:12Z"
"","1841","KAFKA-4147: Fix transient failure in ConsumerCoordinatorTest.testAutoCommitDynamicAssignment","","closed","","hachikuji","2016-09-09T22:03:48Z","2016-09-11T07:47:07Z"
"","1838","HOTFIX: Temporarily ignoring this test until fixed","","closed","","enothereska","2016-09-09T12:06:30Z","2016-09-09T14:23:17Z"
"","1831","KAFKA-4139: Reset findCoordinatorFuture when brokers are unavailable","","closed","","rajinisivaram","2016-09-07T09:50:07Z","2016-09-08T16:43:49Z"
"","1823","Fixes javadoc of Windows, fixes typo in parameter name of KGroupedTable","","closed","","miguno","2016-09-05T08:02:48Z","2021-02-09T10:53:40Z"
"","1821","KAFKA-3807: Fix transient test failure caused by race on future completion","","closed","","hachikuji","2016-09-02T20:26:25Z","2016-09-09T21:09:15Z"
"","1820","MINOR: Add timeout to testRenegotiation.","","closed","","SinghAsDev","2016-09-02T18:40:28Z","2020-12-09T10:05:14Z"
"","1818","Backport KAFKA-3587","","closed","","id","2016-09-02T12:46:36Z","2017-12-22T20:18:16Z"
"","1815","KAFKA-4115: grow default heap size for connect-distributed.sh to 1G","","closed","","shikhar","2016-09-02T04:02:03Z","2018-02-25T21:26:48Z"
"","1814","MINOR: doc fix related to monitoring consumer lag.","","closed","","alexlod","2016-09-02T02:09:53Z","2016-11-29T17:58:24Z"
"","1813","make mirror maker threads daemons and make sure any uncaught exceptions are logged","","closed","","radai-rosenblatt","2016-09-02T00:34:46Z","2018-06-14T13:21:19Z"
"","1810","KAFKA-4077: Backdate system test certificates to cope with clock skew","","closed","","rajinisivaram","2016-09-01T07:21:13Z","2016-09-01T17:06:25Z"
"","1809","KAFKA-4099: Fix the potential frequent log rolling","","closed","","becketqin","2016-09-01T03:44:49Z","2016-09-02T19:50:18Z"
"","1808","MINOR: changes embedded broker time to MockTime","","closed","","mjsax","2016-08-31T22:40:25Z","2016-09-06T22:35:46Z"
"","1807","KAFKA-4103: Fix regression in DumpLogSegments offsets decoder","","closed","","hachikuji","2016-08-31T17:47:37Z","2016-08-31T20:40:37Z"
"","1806","KAFKA-4105: Queryable state tests","","closed","","enothereska","2016-08-31T15:48:27Z","2016-09-05T04:50:36Z"
"","1803","KAFKA-4023: add thread/task id for logging prefix","","closed","","bbejeck","2016-08-31T01:47:46Z","2016-09-06T18:39:38Z"
"","1800","KAFKA-4100: ensure 'fields' and 'fieldsByName' are not null for Struct schemas","","closed","","shikhar","2016-08-29T21:57:05Z","2016-08-30T02:09:26Z"
"","1798","KAFKA-4098: NetworkClient should not intercept user metdata requests on disconnect","","closed","","hachikuji","2016-08-29T18:58:46Z","2016-08-29T20:13:51Z"
"","1793","MINOR: Include request header in exception when correlation of request/response fails","","closed","","ijuma","2016-08-26T14:21:33Z","2016-08-26T18:13:04Z"
"","1791","KAFKA-4089: KafkaProducer expires batch when metadata is stale","","closed","","sutambe","2016-08-26T01:33:51Z","2016-11-16T19:22:58Z"
"","1790","KAFKA-4070: implement Connect Struct.toString()","","closed","","shikhar","2016-08-25T23:39:15Z","2016-08-26T02:26:08Z"
"","1789","MINOR: Improve log message in `ReplicaManager.becomeLeaderOrFollower`","","closed","","ijuma","2016-08-25T23:00:48Z","2016-08-26T02:28:26Z"
"","1788","KAFKA-3008: Parallel start and stop of connectors and tasks in Connect","","closed","connect,","kkonstantine","2016-08-25T21:49:05Z","2020-10-16T06:09:16Z"
"","1787","KAFKA-3940 Log should check the return value of dir.mkdirs()","","closed","","imandhan","2016-08-25T21:36:11Z","2017-01-27T22:18:19Z"
"","1786","HOTFIX: Fix verbose logging in ControllerChannelManager.brokerReady","","closed","","hachikuji","2016-08-25T20:34:38Z","2016-08-25T23:27:51Z"
"","1785","HOTFIX: disabled application-reset-tool integration test","","closed","","mjsax","2016-08-25T16:50:00Z","2016-08-25T23:03:45Z"
"","1784","KAFKA-4074: Deleting a topic can make it unavailable even if delete.topic.enable is false","","closed","","omkreddy","2016-08-25T09:35:49Z","2018-07-03T15:46:13Z"
"","1780","Kafka 4077: Backdate system test certificates to cope with clock skew","","closed","","rajinisivaram","2016-08-24T10:16:57Z","2016-09-01T07:19:25Z"
"","1779","KAFKA-4083: fix different replication factor during replica reassignment","","closed","","wanwenli","2016-08-24T09:46:31Z","2020-05-15T03:00:23Z"
"","1773","KAFKA-4073: MirrorMaker should handle messages without timestamp correctly","","closed","","ijuma","2016-08-23T02:27:00Z","2016-08-23T04:50:15Z"
"","1771","KAFKA-2894: WorkerSinkTask should rewind offsets on rebalance","","closed","","kkonstantine","2016-08-22T23:53:49Z","2016-08-23T22:48:04Z"
"","1764","MINOR: improve Streams application reset tool to make sure application is down","","closed","","mjsax","2016-08-19T15:54:11Z","2016-08-22T22:46:19Z"
"","1763","KAFKA-4066: Fix NPE in consumer due to multi-threaded updates","","closed","","rajinisivaram","2016-08-19T10:47:28Z","2016-08-22T21:25:16Z"
"","1762","KAFKA-3949: Fix race condition when metadata update arrives during rebalance","","closed","","hachikuji","2016-08-19T04:28:09Z","2016-08-20T05:00:30Z"
"","1751","KAFKA-4053: remove redundant if/else statements in TopicCommand","","closed","","sh-z","2016-08-17T09:31:26Z","2016-08-19T18:59:49Z"
"","1750","KAFKA-4044: log actual socket send/receive buffer size after connecting in Selector","","closed","","omkreddy","2016-08-17T08:51:24Z","2018-07-03T15:42:23Z"
"","1746","KAFKA-4049: Fix transient failure in RegexSourceIntegrationTest","","closed","","guozhangwang","2016-08-16T20:09:14Z","2017-07-15T22:08:41Z"
"","1744","HOTFIX: Re-inserted system out","","closed","","enothereska","2016-08-16T17:59:35Z","2016-08-16T18:34:16Z"
"","1738","KAFKA-3994: Fix deadlock in Watchers by calling tryComplete without the lock","","closed","","hachikuji","2016-08-15T03:08:40Z","2016-12-01T05:30:37Z"
"","1736","MINOR: Refactor TopologyBuilder with ApplicationID Prefix","","closed","","guozhangwang","2016-08-14T07:10:28Z","2016-08-23T04:32:32Z"
"","1734","KAFKA-3916: Check for disconnects properly before sending from the controller","","closed","","hachikuji","2016-08-14T01:22:47Z","2019-04-08T17:10:28Z"
"","1733","KAFKA-4037: Make Connect REST API retries aware of 409 CONFLICT errors","","closed","","ewencp","2016-08-13T23:27:52Z","2016-08-18T22:28:58Z"
"","1732","MINOR: Clarification in producer config documentation","","closed","","vahidhashemian","2016-08-13T00:16:36Z","2016-08-19T03:40:31Z"
"","1731","MINOR: add slf4jlog4j to streams example","","closed","","guozhangwang","2016-08-12T22:52:24Z","2016-08-22T22:53:26Z"
"","1730","MINOR: KAFKA-2946 Follow up. Add Delete to AclCommandTest","","closed","","granthenke","2016-08-12T22:37:37Z","2016-08-13T01:12:13Z"
"","1727","KAFKA-3847: Use a separate producer per source task","","closed","","ewencp","2016-08-12T18:26:01Z","2016-08-12T21:07:15Z"
"","1724","KAFKA-3997: log partition name on truncation","","closed","","resetius","2016-08-12T15:42:49Z","2016-08-12T22:25:24Z"
"","1723","KAFKA-4035: AclCommand should allow Describe operation on groups","","closed","","omkreddy","2016-08-12T14:24:35Z","2018-07-03T15:42:22Z"
"","1721","KAFKA-3845: KIP-75: Add per-connector converters","","closed","","ewencp","2016-08-12T04:27:39Z","2016-08-19T03:57:04Z"
"","1720","KAFKA-4034: Avoid unnecessary consumer coordinator lookup","","closed","","hachikuji","2016-08-12T03:17:37Z","2016-08-12T23:39:55Z"
"","1719","Sentences are not fluent","","closed","","sven0726","2016-08-11T09:50:39Z","2016-08-25T03:31:47Z"
"","1709","MINOR: Doc individual partition must fit on the server that host it","","closed","","b1tfury","2016-08-07T10:45:53Z","2016-08-09T18:29:03Z"
"","1706","MINOR: doc changes for QueueTimeMs JMX metrics.","","closed","","alexlod","2016-08-05T22:11:38Z","2016-08-26T14:37:32Z"
"","1700","KAFKA-4016: Added join benchmarks","","closed","","enothereska","2016-08-03T13:10:31Z","2016-08-19T22:24:01Z"
"","1695","KAFKA-4013: Included exception cause in SaslServerCallbackHandler","","closed","","bbaugher","2016-08-02T20:43:29Z","2016-08-09T01:12:28Z"
"","1694","KAFKA-4012: Added #toString() to KerberosShortNamer","","closed","","bbaugher","2016-08-02T20:38:22Z","2016-08-12T21:58:09Z"
"","1687","KAFKA-3950: kafka mirror maker tool is not respecting whitelist option","","closed","","omkreddy","2016-07-30T03:54:56Z","2018-07-03T15:42:21Z"
"","1686","KAFKA-4002: task.open() should be invoked in case that 0 partitions is assigned to task","","closed","","Ishiihara","2016-07-29T23:15:58Z","2016-08-07T04:12:08Z"
"","1676","MINOR: Consumer should throw KafkaException on invalid checksum","","closed","","hachikuji","2016-07-28T18:02:51Z","2016-07-28T20:38:44Z"
"","1674","HOTFIX: Fixes to javadoc and to state store name for link joins","","closed","","enothereska","2016-07-28T16:51:26Z","2016-08-02T21:41:51Z"
"","1673","Hotfix: fixed instable Streams application reset integration test","","closed","","mjsax","2016-07-28T16:22:42Z","2016-07-28T23:24:57Z"
"","1671","KAFKA-3185: [Streams] Added Kafka Streams Application Reset Tool","","closed","","mjsax","2016-07-27T22:35:20Z","2016-07-27T23:11:46Z"
"","1670","KAFKA-3851: Automate release notes and include links to upgrade notes for release and most recent docs to forward users of older releases to newest docs.","","closed","","ewencp","2016-07-27T05:36:15Z","2016-07-27T23:00:53Z"
"","1662","KAFKA-3500: Handle null keys and values in KafkaOffsetBackingStore.","","closed","","ewencp","2016-07-25T21:25:51Z","2016-07-27T02:44:05Z"
"","1656","KAFKA-3977: Defer fetch parsing for space efficiency and to ensure exceptions are raised to the user","","closed","","hachikuji","2016-07-22T21:27:24Z","2016-07-28T10:19:43Z"
"","1652","KAFKA-2394: move to RollingFileAppender by default for log4j","","closed","","cotedm","2016-07-22T14:39:22Z","2017-12-12T19:31:06Z"
"","1650","KAFKA-3782: Fix transient failure in connect distributed bounce test","","closed","","hachikuji","2016-07-22T00:54:53Z","2016-07-22T03:09:41Z"
"","1647","MINOR: Upgrade RocksDB to 4.8.0","","closed","","ijuma","2016-07-21T21:17:20Z","2016-07-21T23:57:05Z"
"","1646","The order of the parameters of zkSessionTimeOutMs and zkConnectionTimeoutMs for creating the ZkUtils object is reversed.","","closed","","wangzzu","2016-07-21T08:10:30Z","2016-10-25T10:36:16Z"
"","1638","KAFKA-3911: KTable source materialization","","closed","","enothereska","2016-07-19T19:41:54Z","2016-07-21T21:45:30Z"
"","1635","MINOR: Remove unused parameter in `checkIfPartitionReassignmentSucceeded` and clean-ups","","closed","","ijuma","2016-07-19T09:03:01Z","2016-07-20T02:05:08Z"
"","1629","KAFKA-3960 - Committed offset not set after first assign","","closed","","13h3r","2016-07-18T08:28:25Z","2016-07-23T08:53:16Z"
"","1627","KAFKA-3888: send consumer heartbeats from a background thread (KIP-62)","","closed","","hachikuji","2016-07-15T22:54:47Z","2016-08-17T18:50:40Z"
"","1621","MINOR: Added simple streams benchmark to system tests","","closed","","enothereska","2016-07-13T22:34:16Z","2016-07-19T16:32:34Z"
"","1620","KAFKA-3940: Log should check the return value of dir.mkdirs()","","closed","","jimjag","2016-07-13T15:40:38Z","2016-07-14T19:12:28Z"
"","1619","KAFKA-3858: Add functions to print stream topologies","","closed","","enothereska","2016-07-13T14:47:33Z","2016-07-21T20:12:32Z"
"","1617","syncing mine with upstream","","closed","","prabcs","2016-07-12T20:27:22Z","2016-07-12T20:27:36Z"
"","1616","KAFKA-2946: DeleteTopic - protocol and server side implementation","","closed","","granthenke","2016-07-12T18:32:25Z","2016-08-12T22:38:18Z"
"","1615","KAFKA-3950: kafka mirror maker tool is not respecting whitelist option","","closed","","omkreddy","2016-07-12T17:35:38Z","2018-07-03T15:42:21Z"
"","1612","KAFKA-3952: Consumer rebalance verifier never succeed due to type mismatch","","closed","","wanwenli","2016-07-12T03:19:57Z","2016-07-15T17:12:21Z"
"","1611","MINOR: Check null in SmokeTestDriver to avoid NPE","","closed","","guozhangwang","2016-07-11T22:43:27Z","2016-07-12T19:15:47Z"
"","1607","MINOR: Doc of 'retries' config should mention about max.in.flight.requests.per.connection to avoid confusion","","closed","","kawamuray","2016-07-11T14:02:44Z","2016-07-19T09:11:32Z"
"","1599","KAFKA-2941: Clarify docs for key and value Converters","","closed","","ewencp","2016-07-08T22:05:09Z","2016-07-11T04:23:52Z"
"","1595","MINOR: Typo fix in comments","","closed","","naferx","2016-07-06T23:14:04Z","2016-07-09T01:09:46Z"
"","1588","KAFKA-3825: Allow users to specify different types of state stores in Streams DSL","","closed","","jeyhunkarimov","2016-07-05T13:08:47Z","2016-11-18T08:14:10Z"
"","1586","Kafka-3836: KStreamReduce and KTableReduce should not pass nulls to Deserializers","","closed","","jeyhunkarimov","2016-07-05T09:15:18Z","2016-07-06T07:47:35Z"
"","1585","KAFKA-3836: KStreamReduce and KTableReduce should not pass nulls to Deserializers","","closed","","jeyhunkarimov","2016-07-04T22:08:46Z","2016-07-06T08:04:13Z"
"","1578","MINOR: remove ""auto.commit.interval.ms"" from ""Manual Offset Control"" example","","closed","","xuwei-k","2016-07-01T06:08:35Z","2016-07-02T01:09:23Z"
"","1574","KAFKA-3920: Add Schema source connector to Kafka Connect","","closed","","Ishiihara","2016-06-30T00:05:10Z","2016-07-08T17:57:30Z"
"","1569","Kafka 3836: KStreamReduce and KTableReduce should not pass nulls to Deserializers","","closed","","jeyhunkarimov","2016-06-29T11:58:30Z","2016-07-06T08:04:37Z"
"","1568","MINOR: Fix consumer constructor doc string","","closed","","granthenke","2016-06-28T20:47:21Z","2016-07-02T01:12:01Z"
"","1567","MINOR: fix Bash shebang on vagrant/ scripts","","closed","","shikhar","2016-06-28T18:09:07Z","2016-07-15T18:25:33Z"
"","1562","KAFKA-3908; Set ReceiveBufferSize for socket used by Processor","","closed","","lindong28","2016-06-27T22:38:29Z","2017-03-15T03:15:07Z"
"","1552","MINOR: Typo fix in docs ops","","closed","","thanasis00","2016-06-24T20:42:33Z","2016-07-02T01:21:21Z"
"","1550","MINOR: Improve doc string in PartitionGrouper","","closed","","guozhangwang","2016-06-24T18:46:06Z","2016-06-24T21:47:12Z"
"","1546","HOTFIX: Remove Java verion 1.6 in quick-start docs","","closed","","guozhangwang","2016-06-23T21:41:52Z","2016-06-23T21:45:43Z"
"","1540","MINOR: Verify acls for group resource on all servers of test cluster.","","closed","","SinghAsDev","2016-06-22T20:19:49Z","2016-06-22T22:27:44Z"
"","1533","KAFKA-3872: Reduce log cleaner buffer size to 2 MB","","closed","","enothereska","2016-06-21T06:09:11Z","2016-06-21T20:52:43Z"
"","1531","KAFKA-3865: Fix transient failure in WorkerSourceTaskTest.testSlowTaskStart","","closed","","hachikuji","2016-06-21T00:20:37Z","2016-06-21T03:32:41Z"
"","1530","KAFKA-3769: Create new sensors per-thread in KafkaStreams","","closed","","guozhangwang","2016-06-20T23:30:33Z","2016-08-17T21:37:23Z"
"","1529","Kafka-3880: Disallow Join Window with size zero","","closed","","mjsax","2016-06-20T21:20:49Z","2016-06-23T21:31:16Z"
"","1526","KAFKA-3870: Expose state store names in DSL","","closed","","enothereska","2016-06-20T11:00:20Z","2016-07-18T19:13:28Z"
"","1524","KAFKA-3869 Fix streams example","","closed","","ghost","2016-06-20T07:58:40Z","2016-12-14T01:03:24Z"
"","1521","MINOR: Check null keys in KTableSource","","closed","","guozhangwang","2016-06-17T23:34:57Z","2016-06-18T19:02:50Z"
"","1520","HOTFIX: Check hasNext in KStreamWindowReduce","","closed","","guozhangwang","2016-06-17T23:32:10Z","2016-06-18T18:57:29Z"
"","1519","KAFKA-3863: System tests covering connector/task failure and restart","","closed","","hachikuji","2016-06-17T21:43:52Z","2016-06-23T00:07:20Z"
"","1518","KAFKA-3632; remove fetcher metrics on shutdown and leader migration","","closed","","hachikuji","2016-06-17T20:18:55Z","2016-06-20T00:18:58Z"
"","1516","KAFKA-3837: Report the thread name of the blocking thread when throwing ConcurrentModificationException","","open","","bharatviswa504","2016-06-16T21:54:56Z","2018-03-02T19:29:33Z"
"","1515","KAFKA-3849: Add explanation on why polling every second in MirrorMaker is required.","","closed","","SinghAsDev","2016-06-16T19:44:48Z","2016-07-09T01:52:44Z"
"","1513","MINOR: Follow-up from KAFKA-2720 with comment/style fixes","","closed","","hachikuji","2016-06-16T17:03:44Z","2016-06-16T22:03:42Z"
"","1512","KAFKA-3838 zkClient and Zookeeper version bump","","closed","","mangas","2016-06-16T07:58:18Z","2016-06-18T00:03:09Z"
"","1511","KAFKA-3850: WorkerSinkTask commit prior to rebalance should be retried on wakeup","","closed","","hachikuji","2016-06-16T00:43:09Z","2016-06-19T22:31:32Z"
"","1510","Minor: warn if the TGT cannot be renewed.","","closed","","harshach","2016-06-15T16:05:05Z","2016-07-02T01:36:46Z"
"","1509","KAFKA-3691: Confusing logging during metadata update timeout","","closed","","granthenke","2016-06-15T15:19:58Z","2016-06-16T22:30:43Z"
"","1508","MINOR: Fix quota violation exception message","","closed","","rajinisivaram","2016-06-15T14:41:43Z","2016-06-16T07:11:31Z"
"","1505","KAFKA-3837: Report the thread name of the blocking thread when throwing ConcurrentModificationException","","closed","","bharatviswa504","2016-06-14T22:16:42Z","2016-06-20T16:48:44Z"
"","1504","KAFKA-3838 zkClient and Zookeeper version bump","","closed","","mangas","2016-06-14T15:50:31Z","2016-06-16T07:59:45Z"
"","1502","KAFKA-3824: Clarify the at least once delivery with auto commit enabled.","","closed","","Ishiihara","2016-06-13T23:50:54Z","2018-02-25T07:38:08Z"
"","1500","MINOR: Fix javadoc typos in ConsumerRebalanceListener","","closed","","vahidhashemian","2016-06-13T20:05:25Z","2016-06-19T08:43:06Z"
"","1490","KAFKA-3769: Optimize metrics recording overhead","","closed","","guozhangwang","2016-06-10T19:39:01Z","2017-07-15T22:08:46Z"
"","1489","KAFKA-2945: CreateTopic - protocol and server side implementation","","closed","","granthenke","2016-06-10T16:20:48Z","2016-07-12T15:21:57Z"
"","1488","KAFKA-3817: handle null keys in KTableRepartitionMap","","closed","","guozhangwang","2016-06-09T22:41:53Z","2017-07-15T22:08:47Z"