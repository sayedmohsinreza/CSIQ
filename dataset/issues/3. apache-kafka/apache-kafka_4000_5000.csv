"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","7714","Merge pull request #1 from apache/trunk","同步源地址数据  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cshuig","2019-11-19T14:48:15Z","2019-11-19T14:49:55Z"
"","7713","Merge pull request #1 from apache/trunk","同步源地址数据  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cshuig","2019-11-19T14:35:58Z","2019-11-19T14:41:38Z"
"","7635","KAFKA-9131: Remove dead code for handling timeout exception","…ns in catch clause and move it to the callback  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","gleb-kom","2019-11-02T18:36:04Z","2019-12-05T20:44:27Z"
"","7755","KAFKA-9233: Fix IllegalStateException in Fetcher retrieval of beginni…","…ng or end offsets for duplicate TopicPartition values  Minor bug fix. The issue was introduced in Kafka 2.3.0, likely by [KAFKA-7831](https://issues.apache.org/jira/browse/KAFKA-7831).  Tested by, `./gradlew clients:test --tests FetcherTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","noslowerdna","2019-11-27T19:30:14Z","2020-04-01T16:45:46Z"
"","8336","KAFKA-9749: TransactionMarkerRequestCompletionHandler should treat KAFKA_STORAGE_…","…ERROR as retriable  When handling a WriteTxnResponse, the TransactionMarkerRequestCompletionHandler throws an IllegalStateException when the remote broker responds with a KAFKA_STORAGE_ERROR and does not retry the request. This leaves the transaction state stuck in PendingAbort or PendingCommit, with no way to change that state other than restarting the broker, because both EndTxnRequest and InitProducerIdRequest return CONCURRENT_TRANSACTIONS if the state is PendingAbort or PendingCommit. This patch changes the error handling behavior in TransactionMarkerRequestCompletionHandler to retry KAFKA_STORAGE_ERRORs. This matches the existing client behavior, and makes sense because KAFKA_STORAGE_ERROR causes the host broker to shut down, meaning that the partition being written to will move its leadership to a new, healthy broker.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2020-03-24T01:21:55Z","2020-03-25T04:20:37Z"
"","7958","KAFKA-9425: InFlightRequests Class Uses Thread-Safe Counter Non-Threa…","…d-Safe Collection  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","belugabehr","2020-01-14T15:07:22Z","2020-04-16T14:08:28Z"
"","8288","KAFKA-9713: Remove dead BufferExhaustedException, which was deprecate…","…d in 0.9.0.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-03-12T20:11:50Z","2020-03-30T14:09:37Z"
"","8136","KAFKA-6266: Repeated occurrence of WARN Resetting first dirty offset …","…(#8089)  Previously, checkpointed offsets for a log were only updated if the log was chosen for cleaning once the cleaning job completes. This caused issues in cases where logs with invalid checkpointed offsets would repeatedly emit warnings if the log with an invalid cleaning checkpoint wasn't chosen for cleaning.  Proposed fix is to update the checkpointed offset for logs with invalid checkpoints regardless of whether it gets chosen for cleaning.  Reviewers: Anna Povzner , Jun Rao   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-02-18T23:56:33Z","2020-02-21T02:14:27Z"
"","7489","KAFKA-7273 Clarification on mutability of headers passed to Converter…","…#fromConnectData()","closed","connect,","gunnarmorling","2019-10-10T18:38:49Z","2020-10-16T06:17:32Z"
"","7876","KAFKA-9344:Logged consumer config does not always match actual config…","… values  https://issues.apache.org/jira/browse/KAFKA-9344  Similar to KAFKA-8928, during consumer construction, some configs might be overridden (client.id for instance), but the actual values will not be reflected in the info log. It'd better display the overridden values for those configs.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-12-30T09:50:15Z","2020-03-10T21:19:40Z"
"","7739","KAFKA-7373: Add a new command line option --comand.config, to support…","… passing SSL properties  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ekanthi","2019-11-23T00:37:10Z","2019-11-23T00:37:10Z"
"","8059","KAFKA-7052 Avoiding NPE in ExtractField SMT in case of non-existent fields","… in case of non-existent fields  https://issues.apache.org/jira/browse/KAFKA-7052  *More detailed description of your change n/a  *Summary of testing strategy (including rationale) Added JUnit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  @rhauch, hey, that's a quick first attempt for making the `ExtractField` SMT more flexible when it comes to encountering a record that doesn't contain the specified field. It's a common situation for connectors like Debezium which produce different ""kinds"" of records/topics; in our case e.g. actual change event topics and meta-topics such as TX data or schema history. One might want to apply the `ExtractField` SMT to the CDC records but not to those others; as that's currently not possible, a strategy is needed for more gracefully handling records without the specified field. Also see [KAFKA-7052](https://issues.apache.org/jira/browse/KAFKA-7052) for some backgrounds.  The proposal is to add a new option `behavior.on.non.existent.field` to the SMT which makes the behavior configurable. Its supported values are:  * fail: raise an exception (default for records with schema) * return-null: return null (default for records without schema) * pass-on: pass on the unmodified original record  The defaults are so to keep backwards compatibility with the current behavior. The ""pass-on"" value will address the original use case reported in KAFKA-7052.  I did a quick implementation of that proposal to foster feedback. Happy to adjust and expand as needed, e.g. to adjust with existing naming patterns for the option and/or its values as well as docs (not sure where that'd go). Thanks!  CC @rmoff, @big-andy-coates.","closed","connect,","gunnarmorling","2020-02-07T11:55:39Z","2020-10-16T06:15:45Z"
"","7771","KAFKA-9184: Redundant task creation and periodic rebalances after zombie Connect worker rejoins the group","Zombie workers, defined as workers that lose connectivity with the Kafka broker coordinator and get kicked out of the group but don't experience a jvm restart, have been keeping their tasks running. This side-effect is more disrupting with the new Incremental Cooperative rebalance protocol. When such workers return:  a) they join the group with existing assignment and this leads to redundant tasks running in the Connect cluster, and b) they interfere with the computation of lost tasks, which before this fix would lead to the scheduled rebalance delay not being reset correctly back to 0. This results in periodic rebalances.   This fix focuses on resolving the above side-effects as follows:  * Each worker now tracks its connectivity with the broker coordinator in an unblocking manner. This allows the worker to detect that the broker coordinator is unreachable. The timeout is set to be equal to the heartbeat interval. If during this time the connection remains inactive, the worker will proactively stop all its connectors and tasks and will keep attempting to connect to the coordinator.  * The incremental cooperative assignor will keep the delay to a positive value as long as it can detect lost tasks. If the set of tasks that are computed as lost becomes empty, the delay will be set to zero and no additional rebalancing will be scheduled.   Besides the test included in this PR, the improvements are being tested with a framework that deploys a Connect cluster on docker images and introduces network partitions between all or selected workers and the Kafka brokers.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-12-03T21:36:13Z","2020-10-16T05:51:00Z"
"","7625","MINOR: Update docs about ZooKeeper upgrade issue from 3.4.X to 3.5.6","ZK upgrade from 3.4.X to 3.5.6 fails with ""java.io.IOException: No snapshot found"" if there are no snapshot files. This was discussed in https://issues.apache.org/jira/browse/ZOOKEEPER-3056  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-10-31T16:39:54Z","2019-11-09T09:00:56Z"
"","7613","KAFKA-8677: Simplify the best-effort network client poll to never throw exception","Within KafkaConsumer.poll, we have an optimization to try to send the next fetch request before returning the data in order to pipelining the fetch requests; however, this pollNoWakeup should NOT throw any exceptions, since at this point the fetch position has been updated. If an exception is thrown and the callers decide to capture and continue, those records would never be returned again, causing data loss.  Also fix the flaky test itself.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-30T04:24:29Z","2020-04-24T23:58:32Z"
"","7911","KAFKA-9386; Apply delete ACL filters to resources from filter even if not in cache","With the old SimpleAclAuthorizer, we were handling delete filters that matched a single resource by looking up that resource directly, even if it wasn't in the cache. AclAuthorizerTest.testHighConcurrencyDeletionOfResourceAcls relies on this behaviour and fails intermittently when the cache is not up-to-date. This PR includes the resource from non-matching filters even if it is not in the cache to retain the old behaviour.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-08T18:45:27Z","2020-01-09T14:48:03Z"
"","8031","KAFKA-9447: Add new customized EOS model example","With the improvement of 447, we are now offering developers a better experience on writing their customized EOS apps with group subscription, instead of manual assignments. With the demo, user should be able to get started more quickly on writing their own EOS app, and understand the processing logic much better.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-01T17:17:11Z","2020-02-06T18:13:34Z"
"","8218","KAFKA-9441: Unify committing within TaskManager","With KIP-447 we need to commit all tasks at once using eos-beta as all tasks share the same producer. Right now, each task is committed individually (by making multiple calls to `consumer.commitSync` or individual calls to `producer.commitTransaction` for the corresponding task producer).  To allow for a unified commit logic, we move the commit logic into `TaskManager`. For non-eos, we collect all offset to be committed and do a single call to `consumer.commitSync` -- for eos-alpha we still commit each transaction individually (but now triggered by the `TaskManager` to have all code in one place).  To allow for a unified commit logic, we need to split existing method on `Task` interface in pre/post parts to allow committing in between, in particular:   - `commit()` -> `prepareCommit()` and `postCommit()`  - `suspend()` -> `prepareSuspend()` and `suspend()` (we keep the name as it still does suspend the task)  - `closeClean()` -> `prepareCloseClean()` and `closeClean(Map checkpoint)` (we keep the name as it still does close the task)  - `closeDirty()` -> `prepareCloseDirty()` and `closeDirty()` (we keep the name as it still does close the task)  `prepareCloseClean()` returns checkpoint information to allow checkpointing after the `TaskManager` did the commit within `closeClean()`.  In a follow up PR, we will introduce eso-beta and commit a single transaction over all task using the shared producer.  Call for review @guozhangwang @abbccdda","closed","kip,","mjsax","2020-03-04T03:10:26Z","2020-06-12T23:20:18Z"
"","7610","KAFKA-9077: Fix reading of metrics of Streams' SimpleBenchmark","With KIP-444 the metrics definitions are refactored. Thus, Streams' SimpleBenchmark needs to be updated to correctly access the refactored metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2019-10-29T18:05:49Z","2019-10-29T20:02:32Z"
"","8453","KAFKA-9841: Revoke duplicate connectors and tasks when zombie workers return with an outdated assignment","With Incremental Cooperative Rebalancing, if a worker returns after it's been out of the group for sometime (essentially as a zombie worker) and hasn't voluntarily revoked its own connectors and tasks in the meantime, there's the possibility that these assignments have been distributed to other workers and redundant connectors and tasks might be running now in the Connect cluster.   This PR complements previous fixes such as KAFKA-9184, KAFKA-9849 and KAFKA-9851 providing a last line of defense against zombie tasks: if at any rebalance round the leader worker detects that there are duplicate assignments in the group, it revokes them completely and resolves duplication with a correct assignment in the rebalancing round that will follow task revocation.","closed","connect,","Lucent-Wong","2020-04-09T07:28:22Z","2020-07-18T18:03:22Z"
"","7881","WIP MINOR: move ZK ACL lookup outside of inWriteLock in AclAuthorizer","WIP:  The inWriteLock taken out in AclChangedNotificationHandler could cause unnecessary contention and block authorize calls/request handler threads when newly updated ACLs are loaded from ZooKeeper. I imagine this call could take >1s on occasion. This change moves the `getAclsFromZk(resource)` call to outside of the write lock where it can't interfere with inReadLock users. I believe this is safe as the update does not depend on locking for the ZK lookup.","closed","","lbradstreet","2020-01-01T03:28:27Z","2020-01-01T03:44:17Z"
"","7697","[DO NOT MERGE] 2.4 with all Streams-side changes reverted","WIP -- version probing is almost certainly still broken, may need to ""un-revert"" some other bugfixes as well","closed","streams,","ableegoldman","2019-11-16T03:30:32Z","2019-12-13T01:34:32Z"
"","8034","KAFKA-9494: Include data type of the config in ConfigEntry","Why this change? To enable better validation. Including the data type can significantly improve client's(web or cli or any other client) ability to validate the values before even making a request to Kafka. In the absence of `type` the only way to know if the user specified value is correct is to make an `alter` call and check if there is no error.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Updated and added new unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","srpanwar-confluent","2020-02-02T19:45:08Z","2020-05-08T19:59:08Z"
"","7971","MINOR: Suppress DescribeConfigs Denied log during CreateTopics","While processing CreateTopics requests, we check `DescribeConfigs` access to decide whether topic configs should be returned in the response. Since the user didn't explicitly ask for configs, we should suppress Denied log at INFO level for this case, similar to other authorize checks used for filtering out authorized resources.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-16T11:40:03Z","2020-01-17T10:01:10Z"
"","7700","KAFKA-9180: Introduce BrokerMetadataCheckpointTest","While investigating KAFKA-9180, I noticed that we had no unit test coverage. It turns out that the behavior was correct, so we just fix the test coverage issue.  Also updated .gitignore with jmh-benchmarks/generated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-16T17:35:45Z","2019-11-18T16:43:56Z"
"","8297","MINOR: enforce non-negative invariant for checkpointed offsets","While discussing KIP-441 we realize we don't strictly enforce that all checkpointed offset sums are positive (or 0, though there's not much point to checkingpoint a 0 offset is there)?  Rather than awkwardly try handle this within every user/reader of the checkpoint file, we should just make a guarantee that all returned checkpointed offsets are positive.","closed","","ableegoldman","2020-03-13T20:36:21Z","2020-06-26T22:37:32Z"
"","7695","KAFKA-9196; Update high watermark metadata after segment roll","When we roll a new segment, the log offset metadata tied to the high watermark may need to be updated. This is needed when the high watermark is equal to the log end offset at the time of the roll. Otherwise, we risk exposing uncommitted data early.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-15T23:16:34Z","2019-11-18T18:17:46Z"
"","8290","KAFKA-9677: Fix consumer fetch with small consume bandwidth quotas","When we changed quota communication with KIP-219, fetch requests get throttled by returning empty response with the delay in `throttle_time_ms` and Kafka consumer retries again after the delay. With default configs, the maximum fetch size could be as big as 50MB (or 10MB per partition). The default broker config (1-second window, 10 full windows of tracked bandwidth/thread utilization usage) means that < 5MB/s consumer quota (per broker) may block consumers from being able to fetch any data.  This PR ensures that consumers cannot get blocked by quota by capping `fetchMaxBytes` in KafkaApis.handleFetchRequest() to quota window * consume bandwidth quota. In the example of default configs (10-second quota window) and 1MB/s consumer bandwidth quota, fetchMaxBytes would be capped to 10MB.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2020-03-12T23:36:13Z","2020-03-25T12:46:51Z"
"","8131","MINOR: Allow test driver to forward headers to Deserializer","When using Kafka Streams test utils to read records from an output topic with a custom deserializer that uses headers, we should forward record headers to this serializer by calling the overloaded method that support this.","open","","orevial","2020-02-18T11:04:04Z","2020-02-18T11:04:04Z"
"","7598","KAFKA-9098: When users name repartition topic, use the name for the repartition filter, source and sink node.","When users specify a name for a repartition topic, we should use the same name for the repartition filter, source, and sink nodes. With the addition of KIP-307 if users go to the effort of naming every node in the topology having processor nodes with generated names is inconsistent behavior.   Updated tests in the streams test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-10-25T16:16:49Z","2019-11-08T21:21:38Z"
"","7745","KAFKA-9218: MirrorMaker 2 can fail to create topics","When the scheduled refreshTopicPartitions runs, check existing topics in both source and target clusters in order to compute topic partitions to be created on target.  If a temporary failure to create the target topic is encountered (e.g. insufficient number of brokers), on the next refresh the target topic creation will be re-attempted.  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2019-11-25T15:35:57Z","2020-01-17T13:52:25Z"
"","7554","KAFKA-9065; Loading offsets and group metadata loops forever","When the metadata manager loads the groups and the offsets of a partition of the `__consumer-offsets` topic, `GroupMetadataManager.doLoadGroupsAndOffsets` could loop forever if the start offset of the partition is smaller than the end offset and no records are effectively read from the partition.  While the conditions leading to this issue are not clear, I've got the case where a partition was having two segments which were both empty in a cluster. This could theoretically happen when all the tombstones in the first are expired and the second is truncated or when the partition is accidentally corrupted.  As a side effect, the `doLoadGroupsAndOffsets` spins forever, blocks the single thread of the scheduler, blocks the loading of all the groups and offsets which are after in the queue, and blocks the expiration of the offsets.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-10-18T08:57:15Z","2020-01-08T08:27:02Z"
"","8466","MINOR: cleaner resume log message is misleading","When the LogManager resumes cleaning it states that compaction is resumed, however the topic in question is not necessarily a compacted one.","closed","","lbradstreet","2020-04-11T18:52:08Z","2020-04-15T05:09:00Z"
"","7894","KAFKA-9364: Fix misleading consumer logs on throttling","When the consumer's fetch request is throttled by the KIP-219 mechanism, it receives an empty fetch response.  The consumer should not log this as an error.","closed","","cmccabe","2020-01-03T19:46:11Z","2020-01-06T18:30:43Z"
"","7946","MINOR: MiniKdc JVM shutdown hook fix","When started as a separate process (which happens only in system tests), the shutdown hook for `MiniKdc` was running immediately, which causes tests that depend on it for Kerberos to fail.  The system tests in `zookeeper_security_upgrade_test.py`, for example, fail because ZooKeeper is unable to contact the KDC.  I ran the above system test locally and confirmed it failed prior to the fix and succeeded after the fix:  WITHOUT FIX: ``` $ ducktape tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py [INFO  - 2020-01-13 09:19:04,249 - runner_client - log - lineno:240]: RunnerClient: kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=PLAINTEXT: Summary: Zookeeper node failed to start Traceback (most recent call last): ... test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=PLAINTEXT status:     FAIL ```  WITH FIX: ``` $ ducktape tests/kafkatest/tests/core/zookeeper_security_upgrade_test.py [INFO:2020-01-13 09:24:08,072]: RunnerClient: kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=PLAINTEXT: PASS ... test_id:    kafkatest.tests.core.zookeeper_security_upgrade_test.ZooKeeperSecurityUpgradeTest.test_zk_security_upgrade.security_protocol=PLAINTEXT status:     PASS ```","closed","","rondagostino","2020-01-13T14:32:27Z","2020-01-24T22:21:13Z"
"","7555","MINOR: Added missed curly brackets to the log.error","When some particular connector throws exception, `WorkerSinkTask` doesn't show that message in log. For example, when Snowflake connector for Kafka Connect throws some Exception related with Insufficient privileges, Kafka Connect prints such message in log:     `ERROR WorkerSinkTask{id=SnowflakeConnector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. (org.apache.kafka.connect.runtime.WorkerSinkTask:558)`  Which is not informative and it doesn't show the root message of the Exception. So, I've added missed curly brackets to the log.error(...) in `WorkerSinkTask`.","closed","connect,","NarekDW","2019-10-18T10:23:06Z","2020-10-16T06:15:41Z"
"","8478","HOTFIX: don't close or wipe out someone else's state","When it comes to actually closing a task we now treat all states exactly the same, and call `StateManagerUtil#closeStateManager` regardless of whether it's in `CREATED` or `RESTORING` or `RUNNING`  Unfortunately `StateManagerUtil` doesn't actually check to make sure that we actually own the lock for this task's state. During a dirty close with eos enabled, we wipe the state -- but in some cases, this means deleting the state out from under another StreamThread who is still in the process of revoking this task","closed","streams,","ableegoldman","2020-04-14T02:06:20Z","2020-04-20T22:25:42Z"
"","8239","KAFKA-9666: Don't increase transactional epoch when trying to fence if the log append fails","When fencing producers, we currently blindly bump the epoch by 1 and write an abort marker to the transaction log. If the log is unavailable (for example, because the number of in-sync replicas is less than min.in.sync.replicas), we will roll back the attempted write of the abort marker, but still increment the epoch in the transaction metadata cache. During periods of prolonged log unavailability, producer retires of InitProducerId calls can cause the epoch to be increased to the point of exhaustion, at which point further InitProducerId calls fail because the producer can no longer be fenced. With this patch, we track whenever we have failed to write the bumped epoch, and when that has happened, we don't bump the epoch any further when attempting to fence. This is safe because the in-memory epoch is still causes old producers to be fenced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2020-03-06T16:55:16Z","2020-07-15T21:16:07Z"
"","7498","KAFKA-9023: Log request destination when the Producer gets disconnected","When diagnosing network issues, it is useful to have a clear picture of which client disconnected from which broker at what time. Previously, when the producer received a NETWORK_EXCEPTION in its responses, it would not log the node which disconnected.","closed","","stanislavkozlovski","2019-10-11T09:07:12Z","2020-11-19T15:42:39Z"
"","7496","KAFKA-9018: Throw clearer exceptions on serialisation errors","When Connect fails on a deserialisation error, it doesn't show if that's the key or value that's thrown the error, nor does it give the user any indication of the topic/partition/offset of the message.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mmolimar","2019-10-11T03:27:12Z","2021-06-30T15:28:49Z"
"","7814","KAFKA-9398: Interrupt StreamThread when close timeout reached and all threads aren't stopped","When calling `KafkaStreams.close(Duration.ofXXX)` when the timeout expires, it's possible the main thread won't exit due to a `StreamThread` that is still working or otherwise hung-up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-12-11T00:10:14Z","2020-01-15T00:17:19Z"
"","7893","KAFKA-9363 Confirms successful topic creation for JAdminClient","When admin client script is used with --zookeeper, it's printing a confirmation message. The same doesn't occur when using JAdminClient. Since KIP 500 is going to replace ZK in the future, it would be nice to have this confirmation message retained.  No unit testing is requires as this message is simply a confirmation on the console upon a successful return from the AdminClient API call.","closed","","mmanna-sapfgl","2020-01-03T17:09:42Z","2020-01-12T06:27:24Z"
"","7486","MINOR: ListPartitionReassignmentsResponse should not be entirely failed when a topic-partition does not exist","When a ListPartitionReassignmentsRequest containing a non-existing topic-partition is sent to the controller, the controller returns a ListPartitionReassignmentsResponse with the top level error code set to UNKNOWN_TOPIC_OR_PARTITION. If multiple topic-partitions are passed in the same request, the caller has no way to know which ones do not exist any more. It has to use another admin call to figure it out and then retries the former.  Instead, the controller should ignore any non-existing topic-partitions. It is fine because they don't have any reassignments by definition so the contract of the API remains valid.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-10-10T14:36:02Z","2019-10-11T10:22:18Z"
"","8037","KAFKA-9491; Increment high watermark after full log truncation","When a follower's fetch offset is behind the leader's log start offset, the follower will do a full log truncation. When it does so, it must update both its log start offset and high watermark. The previous code did the former, but not the latter. Failure to update the high watermark in this case can lead to out of range errors if the follower becomes leader before getting the latest high watermark from the previous leader. The out of range errors occur when we attempt to resolve the log position of the high watermark in `DelayedFetch` in order to determine if a fetch is satisfied.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-02-04T06:19:51Z","2020-02-04T19:39:07Z"
"","8368","KAFKA-9770: Close underlying state store also when flush throws","When a caching state store is closed it calls its flush() method. If flush() throws an exception the underlying state store is not closed.  This commit ensures that state stores underlying a caching state store are closed even when flush() throws.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-03-26T21:53:15Z","2020-04-06T19:45:53Z"
"","8392","MINOR: Fix MockAdminClient to not throw IndexOutOfBoundsException when brokerId is above the known one.","When `brokerId` is above the known one, an `IndexOutOfBoundsException` is thrown. The current handling logic likely assumed that the collection was a Map.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-03-31T12:22:42Z","2020-10-06T20:10:24Z"
"","8469","[KAFKA-9826] Handle an unaligned first dirty offset during log cleaning.","What ==== In KAFKA-9826, a log whose first dirty offset was past the start of the active segment and past the last cleaned point resulted in an endless cycle of picking the segment to clean and discarding it. Though this didn't interfere with cleaning other log segments, it kept the log cleaner thread continuously busy (potentially wasting CPU and impacting other running threads) and filled the logs with lots of extraneous messages.  This was determined to be because the active segment was getting mistakenly picked for cleaning, and because the `logSegments` code handles (start == end) cases only for (start, end) on a segment boundary: the intent is to return a null list, but if they're not on a segment boundary, the routine returns that segment.  This fix has two parts: 1. It changes logSegments to handle start==end by returning an empty List always. 2. It changes the definition of calculateCleanableBytes to not consider anything past the UncleanableOffset; previously, it would potentially shift the UncleanableOffset to match the firstDirtyOffset even if the firstDirtyOffset was past the firstUncleanableOffset. This has no real effect now in the context of the fix for (1) but it makes the code read more like the model that the code is attempting to follow.  These changes require modifications to a few test cases that handled this particular test case; they were introduced in the context of KAFKA-8764. Those situations are now handled elsewhere in code, but the tests themselves allowed a DirtyOffset in the active segment, and expected an active segment to be selected for cleaning.  An additional unit test for the logSegments call is added.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","steverod","2020-04-12T01:44:51Z","2020-04-15T05:27:51Z"
"","7574","MINOR: Check against empty replicas in AlterPartitionReassignments","We would previously allow an empty replica set being passed to the API, resulting in us removing all assignments from a partition.","closed","","stanislavkozlovski","2019-10-22T08:42:11Z","2019-10-22T22:31:12Z"
"","7624","MINOR: Correctly mark offset expiry in GroupMetadataManager's OffsetExpired metric","We would mistakenly increment the `OffsetCommits` metric instead","closed","","stanislavkozlovski","2019-10-31T15:51:59Z","2019-11-04T18:13:11Z"
"","8323","MINOR: allow retries for unitTest and integrationTest runs","We will currently retry if you run gradle test, but not unitTest or integrationTest, which are used directly in Jenkins. This means that we have not been achieving the expected retry behavior.","closed","","lbradstreet","2020-03-21T00:25:58Z","2020-03-21T16:09:30Z"
"","8463","HOTFIX: need to cleanup any tasks closed in TaskManager","We were hitting an `IllegalStateException: There is already a changelog registered for ...` in trunk-eos due to failing to call `TaskManager#cleanup` on unrevoekd tasks that we end up closing in `handleAssignment` after failing to batch commit","closed","","ableegoldman","2020-04-10T23:25:25Z","2020-06-26T22:39:45Z"
"","7809","KAFKA-9288: Do not allow the same object to be inserted multiple times into ImplicitLinkedHashCollection","We should not allow the same object to be inserted multiple times into ImplicitLinkedHashCollection. It causes corruption because there is only one set of previous and next pointers in the node.","closed","","cmccabe","2019-12-10T00:49:20Z","2019-12-11T00:27:17Z"
"","8173","KAFKA-9614: Not initialize topology twice in StreamTask","We only initialize topology when transiting from restoring -> running.  Also tighten some unit tests for this fix: a. restoring -> suspended should just write checkpoint file without committing. b. suspended -> restoring should not need any inner updates. c. restoring -> running should always try to fetch committed offsets, and forward timeout exceptions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-26T18:24:38Z","2020-02-26T22:03:20Z"
"","8258","HOTFIX: Task#dirtyClose should not throw","We need to swallow exceptions from `StateManagerUtil#close` in `dirtyClose` for both active and standby tasks","closed","streams,","ableegoldman","2020-03-09T20:40:48Z","2020-03-11T21:47:37Z"
"","8452","HOTFIX: fix flaky shouldEnforceRebalance test in StreamThreadTest","We need to make sure the StreamThread actually makes it through the `runOnce` method in the main run loop at least once before shutting it down.  There's no particularly clean or direct way to do so at the moment, so we leverage the StreamThread's `now` variable as a proxy for whether the thread has entered the `runOnce` method.","closed","","ableegoldman","2020-04-09T04:25:19Z","2020-04-10T03:31:34Z"
"","7589","KAFKA-8972: need to flush state even on unclean close","We need to make sure all stores in a task have been flushed before trying to close any of them, as stores are flushed again on close. Otherwise in the case of a stream-stream join with two attached stores, when the second store is closed it will be flushed and cause a lookup on the first (already-closed) store, which will then throw in `validateStoreOpen`.  During a normal (clean) close, we will always flush first in `StreamTask#suspend` but when closing tasks as zombies (as in the new `onPartitionsLost`) we were not flushing all stores first before closing them one-by-one.","closed","streams,","ableegoldman","2019-10-24T00:19:32Z","2019-11-14T00:53:32Z"
"","8141","MINOR: Add missing @Test annotation to MetadataTest#testMetadataMerge","We missed adding the @Test annotation to said method which meant it wouldn't run on unit test execution.","closed","","stanislavkozlovski","2020-02-20T08:30:14Z","2020-02-20T17:59:04Z"
"","7748","KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled","We missed a branch in which we might catch a ProducerFencedException. It should always be converted to a TaskMigratedException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-11-26T16:05:39Z","2019-12-16T06:59:19Z"
"","8298","KAFKA-9533: Fix JavaDocs of KStream.transformValues","We might want to cherry-pick this back to 2.3.  Call for review @bbejeck","closed","streams,","mjsax","2020-03-13T21:30:47Z","2020-03-14T18:57:23Z"
"","7661","added new cached authorizer:change the dim of cache","We met the same performance issue which is descripted in the pr #3756 in our production environment,hence, we make a revision for the mechamisum of authorization, our revision have such optimizations  1、Build a cache for authorization, which can avoid recomputation of authorization result. The authorization result will fetch on the result catch if the same result has been computed rather than compute it again 2、Differ from the pr 3756, when we build the result cache of the authorization, we take the resource into first consideration. In this way, the authorization is recomputed only when the authorization are change of specific resource. Compared to the the frequency of recomputation can be reduced obviously.","open","","StevenLuMT","2019-11-07T06:30:41Z","2020-01-18T09:22:09Z"
"","8179","HOTFIX: testInitTransactionTimeout should use prepareResponse instead of respond","We have seen a flaky behavior due to using #respond instead of #prepareResponse call for the txn test.   Reproduced on local as:  ``` [2020-02-26 15:08:49,194] ERROR [Producer clientId=producer-bad-transaction, transactionalId=bad-transaction] Uncaught error in kafka producer I/O thread:  (org.apache.kafka.clients.producer.internals.Sender:241) java.lang.ClassCastException: org.apache.kafka.common.requests.InitProducerIdResponse cannot be cast to org.apache.kafka.common.requests.FindCoordinatorResponse 	at org.apache.kafka.clients.producer.internals.TransactionManager$FindCoordinatorHandler.handleResponse(TransactionManager.java:1494) 	at org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1260) 	at org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109) 	at org.apache.kafka.clients.MockClient.poll(MockClient.java:294) 	at org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:458) 	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:312) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:239) ```  and  ``` java.lang.IllegalStateException: No requests pending for inbound response FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='NONE', nodeId=0, host='host1', port=1000)  	at org.apache.kafka.clients.MockClient.respond(MockClient.java:345) 	at org.apache.kafka.clients.MockClient.respond(MockClient.java:319) 	at org.apache.kafka.clients.producer.KafkaProducerTest.testInitTransactionTimeout(KafkaProducerTest.java:770) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-26T23:11:28Z","2020-02-27T17:05:56Z"
"","7988","KAFKA-8532:controller-event-thread deadlock with zk-session-expiry-handler0","We have observed a serious deadlock between controller-event-thead and zk-session-expirey-handle thread. When this issue occurred, it's only one way to recovery the kafka cluster is restart kafka server.  After detailed analysis， I found that that ControllerEvent(RegisterBrokerAndReelect, and Expired) are invoked ZookeeperClient.reinitialize method in same time when zookeeper session expired, this will lead to a controller deadlock. (RegisterBrokerAndReelect need a established zookeeper session to process finish by controller-event-thread, Expired need be process by controller-event-thread to established zookeeper session, but controller-event-thread is busy to handing RegisterBrokerAndReelect ).","open","","leiboo","2020-01-21T03:36:21Z","2020-01-23T17:58:07Z"
"","8376","KAFKA-9724 Newer clients not always sending fetch request to older brokers","We had a similar case previously with KAFKA-8422 (#6806) where we would skip the validation step if the broker was on a version older than 2.3.   ~This PR makes a similar change on the `prepareFetchRequest` side. If the broker is older than 2.3, we will skip the transition to AWAITING_VALIDATION but also we will clear that state if it had been set by a call to `seekUnvalidated`.~  This PR adds a check for an appropriate version of OFFSETS_FOR_LEADER_EPOCH before entering the validation state in the consumer. If the broker does not support version 3+ of this API, we skip validation.  Also cleaned up the logic behind updating last-seen leader epochs. Now, if the leader epoch is `null`, we will not update it unless we are processing a metadata response. This prevents the client from learning about an epoch before it knows anything about the leader.","closed","","mumrah","2020-03-27T20:25:25Z","2020-05-27T20:37:23Z"
"","8397","MINOR: more logs for empty assignment","We find that brokers may send empty assignment for some members unexpectedly, and would need more logs investigating this issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-31T20:50:25Z","2020-04-01T00:00:20Z"
"","8177","KAFKA-9605: Do not attempt to abort batches when txn manager is in fatal error","We detected a bug in soak where the producer batches shall be failed in sender loop before the produce response callback. This shall trigger an illegal state exception on the producer batch as it is already aborted.   The impact is not severe since sender is on its own thread but should be fixed to avoid unnecessary critical exception.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-26T21:47:32Z","2020-03-11T22:24:00Z"
"","8269","KAFKA-9659: Add more log4j when updating static member mappings","We could update the mappings when:  1) known static member joins with empty member.id: we will generate a new member.id and put into the mapping, and if there's already an old member id it will be replaced.  2) unknown static member joins with empty member.id: we will generation a new member.id and blindly put into the mapping.  3) new leader loading __consumer_offsets and read consumer group generation messages.  I suspect that there's a bug on broker side such that upon leader migration, the new leader did not load all the way up to the latest entry and hence in 3) above we mistakenly bootstrap the coordinator's cache with an old generation's mapping.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-11T00:58:22Z","2020-03-12T23:41:54Z"
"","7642","HOTFIX: fix bug in VP test where it greps for the wrong log message","We added more version info to the log messages but must not have cleanly backported the test update -- looking back it seems like older versions might also need a fix (separate PR as the version numbers are different in 2.1-2.3)  PR for 2.1-2.3: https://github.com/apache/kafka/pull/7643","closed","tests,","ableegoldman","2019-11-04T22:27:23Z","2019-11-05T00:14:26Z"
"","7502","MINOR: move ""Added/Removed sensor"" log messages to TRACE","Very spam.  (left the ""Removing expired sensor"" logs as debug, but open to moving them as well)  Can we cherry-pick this back to branches we still do bugfix releases for?","closed","","ableegoldman","2019-10-11T22:18:39Z","2020-01-18T02:10:03Z"
"","8105","KAFKA-9441: Add internal StreamsProducer","Upfront refactoring for KIP-447.  Introduces `StreamsProducer` that allows to share a producer over multiple tasks and track the TX status.  Call for review @guozhangwang @abbccdda","closed","kip,","mjsax","2020-02-13T03:04:44Z","2020-06-12T23:20:09Z"
"","7792","KAFKA-6049: Add auto-repartitioning for cogroup","updating the docs and implementing auto repartitioning need to make an int test for the repartitioning still ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2019-12-06T18:52:46Z","2020-08-18T17:17:42Z"
"","7990","MINOR: Update AclCommand help message to match implementation","Updates the authorizer option message in the tool to refer to the new AclAuthorizer classname instead of the deprecated SimpleAclAuthorizer since we changed the implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-21T11:12:37Z","2020-01-21T14:39:23Z"
"","8160","Merge pull request #1 from apache/trunk","update to newest  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nackinggit","2020-02-24T11:14:18Z","2020-02-24T11:21:05Z"
"","8148","MINOR: Document endpoints for connector topic tracking (KIP-558)","Update the site documentation to include the endpoints introduced with KIP-558 and a short paragraph on how this feature is used in Connect.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2020-02-21T02:03:05Z","2020-10-16T05:53:50Z"
"","8114","KAFKA-9290: Update IQ related JavaDocs","Update IQ related JavaDocs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","highluck","2020-02-13T22:47:04Z","2020-05-08T06:34:29Z"
"","7541","KAFKA-9058: Lift queriable and materialized restrictions on FK Join","Update foreign key joins not to require materialization or queriability to function.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-17T04:42:06Z","2019-10-17T19:05:13Z"
"","8404","KAFKA-10787: Introduce an import order in Java sources","Until now, Kafka has not enforced any import order policy. However, the lack of import order policy has been introduced many meaningless import order changes in the commits.  This PR adds an import order policy in `checkstyle.xml` that has been most generally accepted - that is, it organizes the imports to the following 4 groups in alphabetical order:  1. `org.apach.kafka.*` 2. `javax.*` 3. `java.*` 4. `*`  As an example, I reorganized 5 files in `org.apache.kafka.streams.state.internals` package here. Please pay attention to that all java package imports are grouped together and the imports on `org.apache.kafka` always go to the top of the file.  (This PR is WIP yet; As soon as this change is accpeted, I will update the PR with reorganizing all imports.)","closed","","dongjinleekr","2020-04-01T15:34:50Z","2021-03-29T11:52:02Z"
"","7975","MINOR: Unnecessary code remove","unnecessary code remove  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-16T16:51:42Z","2020-01-18T15:22:10Z"
"","7670","KAFKA-7016: Not hide the stack trace for ApiException","Unit tests passed locally, do not think it has any compatibility breakage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2019-11-08T21:41:29Z","2022-08-03T18:25:20Z"
"","8051","Hotfix: spotsbug failure for EOS example","Unfortunately, the spotsbug check fails the new EOS example. This patch aims to unblock trunk ASAP.  Ran local `./gradlew checkstyleMain checkstyleTest spotbugsMain spotbugsTest spotbugsScoverage` and passed already   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-06T18:24:29Z","2020-02-06T18:58:06Z"
"","8006","MINOR: Typo correction","Typo correction  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-26T09:11:54Z","2020-03-02T00:54:34Z"
"","8253","KAFKA-9656: Return COORDINATOR_NOT_AVAILABLE for older producer clients","Txn commit has a bug for lower versions where it would treat `COORDINATOR_LOAD_IN_PROGRESS` as fatal. This PR fixes the handling of older client to return  `COORDINATOR_NOT_AVAILABLE` so that it won't crash upon doing txn commit.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-08T05:57:53Z","2020-03-18T20:07:46Z"
"","7600","[KAFKA-8522] Implementing proposal as outlined in KIP-534","Trying to resolve long running issues we have with transactional marker and tombstone retention.","closed","","ConcurrencyPractitioner","2019-10-27T17:22:57Z","2020-01-02T04:36:42Z"
"","8447","HOTFIX: exclude ConsumerCoordinator from NPathComplexity check","Try to simplify the fix so this PR does not refactor ```ConsumerCoordinator#onJoinComplete```. By contrast, this PR exclude ConsumerCoordinator from NPathComplexity check directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-04-08T09:35:27Z","2020-04-08T12:00:04Z"
"","7521","MINOR: remove unused import in QueryableStateIntegrationTest","Trunk is currently broken due checkstyleTest failure in streams.","closed","","lbradstreet","2019-10-15T19:12:39Z","2019-10-15T19:18:58Z"
"","7767","KAFKA-9071 transactional.id.expiration.ms config change from Int to Long","transactional.id.expiration.ms config value should be implemented as a Long  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","despondency","2019-12-02T20:42:47Z","2022-04-17T23:57:26Z"
"","8438","KAFKA-9828 / Add partition field in TestRecord","TopologyTestDriver creates `TestRecord` for consumed events. In order to test partitioning, when one uses custom partitioner, would be useful if `TestRecord` had `partition` field as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lkokhreidze","2020-04-07T09:09:44Z","2020-06-12T23:15:13Z"
"","8088","KAFKA-9535: Update metadata upon retrying partitions for ListOffset","Today if we attempt to list offsets with a fenced leader epoch, consumer will infinitely retry without updating the metadata.   The fix is to trigger the metadata update call whenever we see following retriable exceptions before a second attempt:  NOT_LEADER_FOR_PARTITION REPLICA_NOT_AVAILABLE   KAFKA_STORAGE_ERROR OFFSET_NOT_AVAILABLE LEADER_NOT_AVAILABLE FENCED_LEADER_EPOCH UNKNOWN_LEADER_EPOCH   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-11T06:51:28Z","2020-02-17T04:04:30Z"
"","8380","MINOR: Refactor StreamsProducer","To get better encapsulation, it seems to make sense to move the ownership (ie, creation and closing) of `KafkaProducer` inside of `StreamsProducer` instead of letting the `ActiveTaskCreator` handle it.  This will also simplify `ActiveTaskCreator#reInitializeThreadProducer()`  as introduced in #8331 (ie, there is a PR overlap and which ever PR is merged later, needs a rebase).  Call for review @guozhangwang @abbccdda","closed","kip,","mjsax","2020-03-28T07:39:42Z","2020-06-12T23:14:20Z"
"","8261","KAFKA-9685: Solve Set concatenation perf issue in AclAuthorizer","To dismiss the usage of operation ++ against Set which is slow when Set has many entries. This pr introduces a new class 'AclSets' which takes multiple Sets as parameters and do 'find' against them one by one. For more details about perf and benchmark, refer to [KAFKA-9685](https://issues.apache.org/jira/browse/KAFKA-9685)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiao-zhangS","2020-03-10T05:32:30Z","2020-03-13T15:23:43Z"
"","7659","MINOR: Return null in key mapping of committed","To be consistent with other grouping APIs, and also modified callers accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-11-06T18:16:38Z","2020-04-24T23:52:14Z"
"","7897","KAFKA-9365: Add server side change  to include consumer group information within transaction commit","To be able to correctly fence zombie producer txn commit, we propose to add (member.id, group.instance.id, generation) into the transaction commit protocol to raise the same level of correctness guarantee as consumer commit.  Major changes involve:  1. Upgrade transaction commit protocol with  (member.id, group.instance.id, generation). The client will fail if the broker is not supporting the new protocol. 2. Refactor group coordinator logic to handle new txn commit errors such as FENCED_INSTANCE_ID, UNKNOWN_MEMBER_ID and ILLEGAL_GENERATION. We loose the check on transaction commit when the member.id is set to empty. This is because the member.id check is an add-on safety for producer commit, and we also need to consider backward compatibility for old producer clients without member.id information. And if producer equips with group.instance.id, then it must provide a valid member.id (not empty definitely), the same as a consumer commit.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2020-01-05T05:58:01Z","2020-01-15T03:42:42Z"
"","7639","KAFKA-9133: add extra bounds check for dirty offset before cleaning","To avoid the issue described in KAFKA-9133, I added an extra check to ensure the dirty offset is not larger than the active segment base offset. If it is, we reset to the log start offset.","closed","","timvlaer","2019-11-04T14:23:03Z","2019-11-08T07:46:55Z"
"","7568","KAFKA-9074: Correct Connect’s `Values.parseString` to properly parse a time and timestamp literal","Time and timestamp literal strings contain a `:` character, but the internal parser used in the `Values.parseString(String)` method tokenizes on the colon character to tokenize and parse map entries. The colon could be escaped, but then the backslash character used to escape the colon is not removed and the parser fails to match the literal as a time or timestamp value.  This fix corrects the parsing logic to properly parse timestamp and time literal strings whose colon characters are either escaped or unescaped. Additional unit tests were added to first verify the incorrect behavior and then to validate the correction.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-10-21T18:08:14Z","2020-10-16T06:15:41Z"
"","8266","KAFKA-9695; Handle null config values for createTopics, alterConfigs","Throw InvalidRequestException if null configs are specified for CreateTopics, AlterConfigs or IncrementalAlterConfigs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-10T22:14:21Z","2020-03-12T10:44:16Z"
"","7819","KAFKA-9024: Better error message when field specified does not exist","Throw a `DataException` with informative error message when a field does not exist. This is better than current behavior of throwing NPE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ncliang","2019-12-11T19:37:02Z","2020-10-16T06:15:44Z"
"","7776","MINOR: Actually run the delete topic command in kafka.py","This was missed in #7621","closed","","mumrah","2019-12-04T15:05:43Z","2019-12-04T22:54:38Z"
"","7791","KAFKA-7489: Backport fix ConnectDistributedTest system test to use KafkaVersion","This was fixed in `2.4` and `trunk` per #7023, but the use of `KafkaVersion` should have been backported. This commit makes this change on `2.3`, and should be backported to `2.2` and `2.1`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-12-06T16:40:55Z","2019-12-06T19:43:33Z"
"","8004","HOTFIX: Fix broken connect transform test case due to older junit","This was broken by https://github.com/apache/kafka/commit/28f013708ffe8e48e46f408c7f570bf2cd5c54b2. The older branches have an older version of junit.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-25T00:44:02Z","2020-01-31T11:58:17Z"
"","7662","KAFKA-9133; Cleaner should handle log start offset larger than active segment base offset","This was a regression in 2.3.1. In the case of a DeleteRecords call, the log start offset may be higher than the active segment base offset. The cleaner should allow for this case gracefully.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-07T21:59:00Z","2019-11-09T03:35:23Z"
"","7928","KAFKA-9152; Improve Sensor Retrieval","This ticket shall improve two aspects of the retrieval of sensors: https://issues.apache.org/jira/browse/KAFKA-9152  1. Currently, when a sensor is retrieved with *Metrics.*Sensor() (e.g. ThreadMetrics.createTaskSensor()) after it was created with the same method *Metrics.*Sensor(), the sensor is added again to the corresponding queue in *Sensors (e.g. threadLevelSensors) in StreamsMetricsImpl. Those queues are used to remove the sensors when removeAll*LevelSensors() is called. Having multiple times the same sensors in this queue is not an issue from a correctness point of view. However, it would reduce the footprint to only store a sensor once in those queues.  2. When a sensor is retrieved, the current code attempts to create a new sensor and to add to it again the corresponding metrics. This could be avoided.  Both aspects could be improved by checking whether a sensor already exists by calling getSensor() on the Metrics object and checking the return value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-10T16:13:44Z","2020-01-24T21:56:27Z"
"","7728","MINOR: Fix producer timeouts in log divergence test","This test was taking more than 5 minutes because the producer writes were timing out. The problem was that broker 100 was being shutdown before broker 101, which meant that the partition was still offline after broker 100 was restarted. The producer timeouts were not detected because the produce future was not checked. After the fix, test time drops to about 15s.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-21T17:20:39Z","2019-11-22T19:37:23Z"
"","7974","MINOR: fix flaky StreamsUpgradeTestIntegrationTest","This test failed on trunk recently. It looks like a race condition in which `kafkaStreams4` hadn't completed the rebalance by the time we tested the final version number. Rather than trying to synchronize on state transitions, it's more foolproof (although potentially more opaque) to just wait a while for the versions to converge.  See https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk11/detail/kafka-trunk-jdk11/1085/tests for the failure  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-01-16T16:22:07Z","2020-01-16T22:31:05Z"
"","7504","MINOR: remove multiple_rolling_upgrade_test","This system test was marked `@Ignore` around a year and a half ago pending the version probing work, but never turned on again.  These days, it is made redundant by the suite of system tests in `streams_upgrade_test`, which cover rolling upgrades (including version probing and metadata change).","closed","","ableegoldman","2019-10-12T00:02:59Z","2019-10-14T21:05:57Z"
"","8154","KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout`","This should fix the flakiness with this test case. With a short timeout, the call may timeout and the client might disconnect. Currently this can be exposed to the user as either a TimeoutException or a DisconnectException. To be consistent, rather than exposing the underlying retriable error, we handle both cases with a TimeoutException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-02-21T21:59:13Z","2020-02-24T01:37:46Z"
"","8271","MINOR: Update Streams IQ JavaDocs to not point to a deprecated method","This should be cherry-picked to `2.5` branch.  Call for review @vvcephei  Btw: I am wondering if we could fail the build if we point to deprecated methods in JavaDocs? This way, updates like this would not slip.","closed","streams,","mjsax","2020-03-11T02:36:33Z","2020-03-11T22:30:14Z"
"","8167","KAFKA-9533: Revert  ValueTransform forwards `null`","This reverts commit a41d3d86c13a55a75e67b1635bc74361ebe6d7af.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2020-02-25T16:16:24Z","2020-02-25T18:25:20Z"
"","7769","KAFKA-9203: Revert ""MINOR: Remove workarounds for lz4-java bug affecting byte buffers (#6679)""","This reverts commit 90043d5f as it caused a regression in some cases:  > Caused by: java.io.IOException: Stream frame descriptor corrupted >         at org.apache.kafka.common.record.KafkaLZ4BlockInputStream.readHeader(KafkaLZ4BlockInputStream.java:132) >         at org.apache.kafka.common.record.KafkaLZ4BlockInputStream.(KafkaLZ4BlockInputStream.java:78) >         at org.apache.kafka.common.record.CompressionType$4.wrapForInput(CompressionType.java:110)  I will investigate why after, but I want to get the safe fix into 2.4.0. The reporter of KAFKA-9203 has verified that reverting this change makes the problem go away.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-12-03T14:02:29Z","2019-12-03T16:26:21Z"
"","7721","Revert ""KAFKA-9165: Fix jersey warnings in Trogdor (#7669)""","This reverts commit 69a63304de53ea10858f842b8db03932a05cc671, which seems to be causing unexpected 404s for Trogdor endpoints.","closed","","cmccabe","2019-11-20T17:22:49Z","2019-11-20T20:49:26Z"
"","8259","KAFKA-7421: Ensure Connect's PluginClassLoader is truly parallel capable and resolve deadlock occurrences","This PR:  * Adds SynchronizationTest class for concurrency testing of the classloading isolation mechanism * Adds a test which deterministically reproduced a deadlock between simultaneous upward (Plugin -> Delegating) & downward (Delegating -> Plugin) class loading operations. * Makes PluginClassLoader parallel capable, resolving the above deadlock by allowing multiple threads to concurrently use the PluginClassLoader. * Makes DelegatingClassLoader parallel capable to allow parallel loading of classes from the parent loader (usually the system class loader)  Signed-off-by: Greg Harris   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-03-09T20:58:21Z","2021-07-14T17:38:16Z"
"","8186","KAFKA-9618: Directory deletion failure leading to error task RocksDB open","This PR tries to reorder the closing behavior by doing the state store cleanup first before releasing the lock.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-27T19:10:29Z","2020-03-04T05:45:39Z"
"","8327","KAFKA-9743: Catch commit offset exception to eventually close dirty tasks","This PR tries to close all the dirty tasks during `HandleAssignment` in case the commit call failed. The previous outcome was that all the lost tasks are not properly closed which leads to the RocksDB metric stats not cleared, and eventually blows the application away, since we have an illegal state check for re-adding an existing metrics: ```  public void addMetricsRecorder(final RocksDBMetricsRecorder metricsRecorder) {         final String metricsRecorderName = metricsRecorderName(metricsRecorder);         if (metricsRecordersToTrigger.containsKey(metricsRecorderName)) {             throw new IllegalStateException(""RocksDB metrics recorder for store \"""" + metricsRecorder.storeName() +                 ""\"" of task "" + metricsRecorder.taskId().toString() + "" has already been added. ""                 + ""This is a bug in Kafka Streams."");         }         metricsRecordersToTrigger.put(metricsRecorderName, metricsRecorder);     } ``` Unit test will be added shortly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-22T07:02:29Z","2020-03-24T16:26:15Z"
"","8265","KAFKA-9657: Throw upon offset fetch unsupported stable flag protocol","This PR tries to add an internal flag to throw if we hit an unexpected protocol version for offset fetch. It could be used together with `EOS_BETA` flag so that if server side downgrades unexpectedly, we shall fail the application ASAP.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-10T20:37:12Z","2020-03-13T05:28:59Z"
"","8053","KAFKA-9499; Improve deletion process by batching more aggressively","This PR speeds up the deletion process by doing the following: - Batch whenever possible to minimize the number of requests sent out to other brokers; - Refactor `onPartitionDeletion` to remove the usage of `allLiveReplicas`.  The tests covers the code which has been updated thus I haven't extended the tests in this area.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-02-06T20:55:31Z","2020-02-12T20:11:41Z"
"","8434","KAFKA-7599: Don't throttle in Trogdor when targetMessagesPerSec is 0","This PR solves the feature request [KAFKA-7599](https://issues.apache.org/jira/browse/KAFKA-7599) Instead of disabling throttling when targetMessagesPerSec is absent, it will disable when the value is set to 0. This commit also allows an unrestricted number of messages when maxMessages is set to 0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Atharva747","2020-04-06T19:18:11Z","2020-05-07T14:55:59Z"
"","8236","KAFKA-9670: Reduce allocations in Metadata Response preparation","This PR removes  intermediate  conversions between `MetadataResponse.TopicMetadata` => `MetadataResponseTopic` and `MetadataResponse.PartitionMetadata` => `MetadataResponsePartition` objects.  There is 15-20% reduction in object allocations and 5-10% improvement in metadata request performance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-03-06T07:46:38Z","2020-03-16T16:30:49Z"
"","7978","KAFKA-8532: Properly release countDownLatch in ZooKeeperClient#handleRequests","This PR releases countDownLatch in ZooKeeperClient#handleRequests in the scenario where exception happens. This prevents the deadlock shown below: ``` ""zk-session-expiry-handler0"" #163089 daemon prio=5 os_prio=0 tid=0x00007fcc9c010000 nid=0xfb22 waiting on condition [0x00007fcbb01f8000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method)  parking to wait for <0x00000005ee3f7000> (a java.util.concurrent.CountDownLatch$Sync) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) ... ""controller-event-thread"" #51 prio=5 os_prio=0 tid=0x00007fceaeec4000 nid=0x310 waiting on condition [0x00007fccb55c8000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method)  parking to wait for <0x00000005d1be5a00> (a java.util.concurrent.CountDownLatch$Sync) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304) at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231) ```  See https://issues.apache.org/jira/browse/KAFKA-8532 for background.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2020-01-17T16:37:56Z","2020-01-21T03:50:32Z"
"","8149","KAFKA-9586: Fix errored json filename in ops documentation","This PR is the counterpart of apache/kafka-site#253.  cc/ @omkreddy  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-02-21T04:22:18Z","2020-02-21T13:19:30Z"
"","8020","KAFKA-9146 [WIP]: Add option to force delete active members in StreamsResetter","This PR is mainly to enhance https://issues.apache.org/jira/browse/KAFKA-9146. 1. `org.apache.kafka.clients.admin.Admin#removeMembersFromConsumerGroup` has been changed to support both static or dynamic members~ 2. New cmdline option: --force for `StreamsResetter` is introduced, if --force specified when using the StreamsResetter, then all the active static/dynamic members will be removed.  Related KIP: KIP-571: https://cwiki.apache.org/confluence/display/KAFKA/KIP-571%3A+Add+option+to+force+remove+members+in+StreamsResetter  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","feyman2016","2020-01-30T15:14:40Z","2020-06-12T23:22:42Z"
"","8005","MINOR: Fix topology builder debug log message","This PR is follow up to the comment from #7969:  https://github.com/apache/kafka/pull/7969#discussion_r370902327  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2020-01-25T01:50:58Z","2020-01-28T00:19:27Z"
"","7997","KAFKA-9113: Clean up task management and state management","This PR is collaborated by Guozhang Wang and John Roesler. It is a significant tech debt cleanup on task management and state management, and is broken down by several sub-tasks listed below:  1. Extract embedded clients (producer and consumer) into RecordCollector from StreamTask. https://github.com/guozhangwang/kafka/pull/2 https://github.com/guozhangwang/kafka/pull/5  2. Consolidate the standby updating and active restoring logic into ChangelogReader and extract out of StreamThread. https://github.com/guozhangwang/kafka/pull/3 https://github.com/guozhangwang/kafka/pull/4  3. Introduce Task state life cycle (created, restoring, running, suspended, closing), and refactor the task operations based on the current state. https://github.com/guozhangwang/kafka/pull/6 https://github.com/guozhangwang/kafka/pull/7  4. Consolidate AssignedTasks into TaskManager and simplify the logic of changelog management and task management (since they are already moved in step 2) and 3)). https://github.com/guozhangwang/kafka/pull/8 https://github.com/guozhangwang/kafka/pull/9  Also simplified the StreamThread logic a bit as the embedded clients / changelog restoration logic has been moved into step 1) and 2). https://github.com/guozhangwang/kafka/pull/10   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-01-22T05:46:31Z","2020-04-24T23:59:58Z"
"","8260","KAFKA-9625: Fixing IncrementalAlterConfigs with respect to Broker Configs","This PR introduces a bug fix for `IncrementalAlterConfigs` with respect to altering broker configs. Previously broker throttles were incorrectly marked as sensitive configurations. In addition, when trying to delete configs, `DescribeConfigs` would always return the initially deleted configs. This PR fixes both of those issues.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","skaundinya15","2020-03-10T01:42:46Z","2020-05-05T21:19:00Z"
"","7994","KAFKA-9437: KIP-559: Make the Kafka Protocol Friendlier with L7 Proxies","This PR implements the KIP-559: https://cwiki.apache.org/confluence/display/KAFKA/KIP-559%3A+Make+the+Kafka+Protocol+Friendlier+with+L7+Proxies - it adds the Protocol Type and the Protocol Name fields in JoinGroup and SyncGroup API; - it validates that the fields are provided by the client when the new version of the API is used and ensure that they are consistent. it errors out otherwise; - it validates that the fields are consistent in the client and errors out otherwise; - it adds many tests related to the API changes but also extends the testing coverage of the requests/responses themselves. - it standardises the naming in the coordinator. now, `ProtocolType` and `ProtocolName` are used across the board in the coordinator instead of having a mix of protocol type, protocol name, subprotocol, protocol, etc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-01-21T19:30:49Z","2020-08-11T06:50:22Z"
"","7609","KAFKA-9059: Implement ReassignmentMaxLag","This PR implements the kafka.server:type=ReplicaManager,name=ReassignmentMaxLag,clientId=Leader metric by keeping track of the reassignment lag in the follower.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","viktorsomogyi","2019-10-29T16:04:31Z","2020-05-06T16:15:45Z"
"","8257","KAFKA-9539; Add leader epoch in StopReplicaRequest (KIP-570)","This PR implements KIP-570.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-03-09T14:20:17Z","2020-08-11T06:50:36Z"
"","8011","KAFKA-8503; Add default api timeout to AdminClient (KIP-533)","This PR implements `default.api.timeout.ms` as documented by KIP-533. This is a rebased version of #6913 with some additional test cases and small cleanups.  Co-authored-by: huxi   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-28T17:24:01Z","2020-01-31T06:48:52Z"
"","8248","KAFKA-9501: convert between active and standby without closing stores","This PR has gone through several significant transitions of its own, but here's the latest:  1. TaskManager just collects the tasks to transition and refers to the active/standby task creator to handle closing & recycling the old task and creating the new one. If we ever hit an exception during the close, we bail and close all the remaining tasks as dirty. 2. The task creators tell the task to ""close but recycle state"". If this is successful, it tells the recycled processor context and state manager that they should transition to the new type. 3. During ""close and recycle"" the task just does a normal clean close, but instead of closing the state manager it informs it to recycle itself:  maintain all of its store information (most importantly the current store offsets) but unregister the changelogs from the changelog reader 4. The new task will (re-)register its changelogs during initialization, but skip re-registering any stores. It will still read the checkpoint file, but only use the written offsets if the store offsets are not already initialized from pre-transition 5. To ensure we don't end up with manual compaction disabled for standbys, we have to call the state restore listener's `onRestoreEnd` for any active restoring stores that are switching to standbys","closed","","ableegoldman","2020-03-07T05:47:44Z","2020-06-26T22:37:14Z"
"","8109","KAFKA-9545: Fix subscription bugs from Stream refactoring","This PR fixes two bugs related to stream refactoring:  1. The subscribed topics are not updated correctly when topic gets removed from broker.  2. The `remainingPartitions` computation doesn't account the case when one task has a pattern subscription of multiple topics. Then the input partition change will not be assumed as `containsAll`  The bugs are exposed from integration test `testRegexMatchesTopicsAWhenDeleted` and could be used to verify the fix works.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2020-02-13T18:10:29Z","2020-02-16T23:55:00Z"
"","8307","KAFKA-9727: cleanup the state store for standby task dirty close and check null for changelogs","This PR fixes three things:  1.  the state should be closed when standby task is restoring as well 2. the EOS standby task should also wipe out state under dirty close 3. the changelog reader should check for null as well  The sequence to reproduce the system test failure:  1. Stream job close uncleanly, leaving active task 0_0 no committed offset 2. The task 0_0 switch from active to standby task, which never logs anything in checkpoint under EOS 3. Task 0_0 gets illegal state for not finding checkpoints, throwing task corrupted exception 4. Exception were caught and the task was closed, however the state store was already registered, and not released. 5. Next iteration we shall hit lock not available as it never gets released. 6. We shall also hit a NPE in the changelog removal as well since it gets removed in the first time handling corruption of the standby task.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-17T04:20:08Z","2020-03-20T22:43:03Z"
"","8305","HOTFIX: StateDirectoryTest should use Set instead of List","This PR fixes the undefined order of list directory result, to make the test result consistent, otherwise we would hit out of order from time to time:  ``` java.lang.AssertionError: expected:<[/tmp/kafka-k0Nn0/applicationId/0_0, /tmp/kafka-k0Nn0/applicationId/0_1]> but was:<[/tmp/kafka-k0Nn0/applicationId/0_1, /tmp/kafka-k0Nn0/applicationId/0_0]> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:120) 	at org.junit.Assert.assertEquals(Assert.java:146) 	at org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldOnlyListNonEmptyTaskDirectories(StateDirectoryTest.java:318) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","abbccdda","2020-03-16T23:20:29Z","2020-03-17T04:37:47Z"
"","7904","KAFKA-9335: Fix StreamPartitionAssignor regression in repartition topics counts","This PR fixes the regression introduced in 2.4 from 2 refactoring PRs: https://github.com/apache/kafka/pull/7249/ https://github.com/apache/kafka/pull/7419/  The bug was introduced by having a logical path leading `numPartitionsCandidate` to be 0, which is assigned to `numPartitions` and later being checked by `setNumPartitions`. In the subsequent check we will throw illegal argument if the `numPartitions` is 0.  This bug is both impacting new 2.4 application and upgrades to 2.4 in certain types of topology. The example in original JIRA was imported as a new integration test to guard against such regression. We also verify that without the bug fix application will still fail by running this integration test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-01-07T17:16:40Z","2020-01-07T23:00:54Z"
"","8168","KAFKA-9607: Do not clear partition queues during close","This PR fixes the illegal state bug where a task gets revived but has no input partition assigned anymore.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-25T21:14:52Z","2020-03-07T08:12:25Z"
"","7932","KAFKA-8764: LogCleanerManager endless loop while compacting/clea","This PR fixes LogCleaner's endless loop while clearing LogSegemnts with holes.   In rare cases, when clearing LogSegments with missing records, LogCleaner was unable to progress resulting with high CPU usage, high disk read/writes and excessive cleaner logs (if enabled). This PR addresses such situation by skipping missing record(s) and, as result, avoiding endless loop while clearing such Logs.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","trajakovic","2020-01-10T20:14:07Z","2020-01-31T16:24:26Z"
"","8226","KAFKA-9651: Fix ArithmeticException (÷ by 0) in Partitioner impls","This PR fixes KAKFA-9651 by throwing UnknownTopicOrPartitionException instead of ArithmeticException.","closed","","tombentley","2020-03-05T11:42:51Z","2020-03-23T17:07:55Z"
"","7794","KAFKA-7588: Pass custom configs to the ChannelBuilder implementations","This PR ensures we provide custom configs to ChannelBuilder implementations apart from  `config.values()` we pass currently. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-12-06T19:58:37Z","2019-12-12T15:03:48Z"
"","8164","KAFKA-9600: EndTxn should enforce strict epoch checking if from client","This PR enhances the epoch checking logic for endTransaction call in TransactionCoordinator. Previously it relaxes the checking by allowing a producer epoch bump, which is error-prone since there is no reason to see a producer epoch bump from client.  Since this is purely a server side bug which requires no client side change, we haven't added any integration test to verify this behavior yet.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-25T05:42:59Z","2020-03-28T17:55:14Z"
"","7898","KAFKA-9366: Change log4j dependency into log4j2","This PR changes log4j dependency into log4j2.  log4j migrated into log4j2 after its last release of 1.2.17 (May 2012), which is affected by this problem. So, the only way to fix it is by moving log4j dependency into log4j2.  The problem is: the API for setting log level dynamically is different between log4j and log4j2. So, this PR also updates how `Log4jController` works.  This PR also fixes a potential problem in `Log4jController#getLogLevel` - what if the root logger's level is null? It may result in `NullPointerException`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2020-01-06T15:16:41Z","2022-04-30T11:50:22Z"
"","7998","KAFKA-9460: Enable TLSv1.2 by default and disable all others protocol versions","This PR by default disable all SSL protocols except TLSv1.2. Changes discussed in KIP-553.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2020-01-22T17:30:45Z","2020-01-28T20:15:35Z"
"","8016","update example reassign file to include log_dirs configuration","This PR brings the docs up to date with KIP-113 and provides an example of how to configure a specified log_dir for a partition.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ x] Verify documentation (including upgrade notes)","open","","mitchell-h","2020-01-29T00:12:40Z","2020-01-29T00:12:40Z"
"","8062","KAFKA-9498; Topic validation during the topic creation triggers unnecessary TopicChange events","This PR avoids generating unnecessary TopicChange events during the topic validation. It does so by adding a `registerWatch` field in the `GetChildrenRequest` request. This allows to not register the watch when topics are queried from the topic validation logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-02-07T20:55:03Z","2020-02-26T08:11:17Z"
"","8199","Kafka 9626: Improve ACLAuthorizer.acls() performance","This PR avoids creation of unnecessary sets in AclAuthorizer.acls() method implementation.  Perf results: **Old** ``` Benchmark                                (aclCount)  (resourceCount)  Mode  Cnt    Score   Error  Units AclAuthorizerBenchmark.testAclsIterator           5             5000  avgt   15    5.821 ? 0.309  ms/op AclAuthorizerBenchmark.testAclsIterator           5            10000  avgt   15   15.303 ? 0.107  ms/op AclAuthorizerBenchmark.testAclsIterator           5            50000  avgt   15   74.976 ? 0.543  ms/op AclAuthorizerBenchmark.testAclsIterator          10             5000  avgt   15   15.366 ? 0.184  ms/op AclAuthorizerBenchmark.testAclsIterator          10            10000  avgt   15   29.899 ? 0.129  ms/op AclAuthorizerBenchmark.testAclsIterator          10            50000  avgt   15  167.301 ? 1.723  ms/op AclAuthorizerBenchmark.testAclsIterator          15             5000  avgt   15   21.980 ? 0.114  ms/op AclAuthorizerBenchmark.testAclsIterator          15            10000  avgt   15   44.385 ? 0.255  ms/op AclAuthorizerBenchmark.testAclsIterator          15            50000  avgt   15  241.919 ? 3.955  ms/op ``` **New**  ``` Benchmark                                (aclCount)  (resourceCount)  Mode  Cnt   Score   Error  Units AclAuthorizerBenchmark.testAclsIterator           5             5000  avgt   15   0.666 ? 0.004  ms/op AclAuthorizerBenchmark.testAclsIterator           5            10000  avgt   15   1.427 ? 0.015  ms/op AclAuthorizerBenchmark.testAclsIterator           5            50000  avgt   15  21.410 ? 0.225  ms/op AclAuthorizerBenchmark.testAclsIterator          10             5000  avgt   15   1.230 ? 0.018  ms/op AclAuthorizerBenchmark.testAclsIterator          10            10000  avgt   15   4.303 ? 0.744  ms/op AclAuthorizerBenchmark.testAclsIterator          10            50000  avgt   15  36.724 ? 0.409  ms/op AclAuthorizerBenchmark.testAclsIterator          15             5000  avgt   15   2.433 ? 0.379  ms/op AclAuthorizerBenchmark.testAclsIterator          15            10000  avgt   15   9.818 ? 0.214  ms/op AclAuthorizerBenchmark.testAclsIterator          15            50000  avgt   15  52.886 ? 0.525  ms/op ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-03-01T16:51:35Z","2020-03-02T20:25:13Z"
"","8174","KAFKA-9439: add KafkaProducer API unit tests","This PR adds unit tests for `KafkaProducer.close()`, `KafkaProducer.abortTransaction()`, and `KafkaProducer.flush()`.  Increase KafkaProducer JUnit code coverage from 82% methods, 82% lines to 87% methods, 85% lines.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2020-02-26T20:23:33Z","2020-06-23T21:07:15Z"
"","7621","KAFKA-9123 Test a large number of replicas","This PR adds two new system tests to exercise the system with a large number of replicas.   The first test creates 500 topics with 34 partitions and x3 replication for a total of 51,000 replicas. This is done across 8 brokers which is 6375 replicas per broker. Once the topics have been created and verified, a controlled shutdown of each broker is performed. Finally, each of the topics is deleted.  The other test runs produce and consume benchmark utilities against a small number of topics.","closed","","mumrah","2019-10-31T13:35:08Z","2019-11-23T00:32:09Z"
"","8103","KAFKA-7061: KIP-280 Enhanced log compaction","This PR adds support for the enhanced log compaction and the design detailed in the [KIP-280: Enhanced Log Compaction](https://cwiki.apache.org/confluence/display/KAFKA/KIP-280%3A+Enhanced+log+compaction)  ### Committer Checklist (excluded from commit message) - [ ] Yet to handle the special case to retain LEO (log-end-offset) record / create an empty message batch for non-offset based compaction strategy as detailed in the KIP - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  Call committers for review @mjsax @junrao @guozhangwang","open","","senthilm-ms","2020-02-13T00:50:05Z","2022-07-05T18:27:59Z"
"","7528","KAFKA-7061: KIP-280 Enhanced log compaction","This PR adds support for the enhanced log compaction and the design detailed in the [KIP-280: Enhanced Log Compaction](https://cwiki.apache.org/confluence/display/KAFKA/KIP-280%3A+Enhanced+log+compaction)  ### Committer Checklist (excluded from commit message) - [ ] Yet to handle the special case to retain LEO (log-end-offset) record / create an empty message batch for non-offset based compaction strategy as detailed in the KIP - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  Call committers for review @mjsax @junrao @guozhangwang","closed","","senthilm-ms","2019-10-16T04:24:22Z","2020-02-13T00:52:38Z"
"","8153","KAFKA-9594: Add a separate lock to pause the follower log append while checking if the log dir could be replaced.","This PR adds new lock is used to prevent the follower replica from being updated while ReplicaAlterDirThread is executing maybeReplaceCurrentWithFutureReplica() to replace follower replica with the future replica.  Now doAppendRecordsToFollowerOrFutureReplica() doesn't need to hold the lock on leaderIsrUpdateLock for local replica updation and ongoing log appends on the follower will not delay the makeFollower() call.  **Benchmark results for Partition.makeFollower() ** Old: ``` Benchmark                                        Mode  Cnt     Score    Error  Units PartitionMakeFollowerBenchmark.testMakeFollower  avgt    15  2046.967 ? 22.842  ns/op ```  New: ``` Benchmark                                        Mode  Cnt     Score   Error  Units PartitionMakeFollowerBenchmark.testMakeFollower  avgt    15  1278.525 ? 5.354  ns/op ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-02-21T21:13:19Z","2020-02-26T05:26:17Z"
"","7526","KAFKA-8047 remove KafkaMbean when network close","This PR adds logic to remove sensors for a particular node connection when node disconnects.  In order to be more familiar with `JMXReporter`, `Metrics` and `Sensor`s functionality I did some tests refactoring to make them more communicative (at least from my point of view)  When a connection is established a KafkaMBean is registered, and each metric is added as MBean attribute. MBean is removed only when all metrics are.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","alex-dukhno","2019-10-16T00:53:25Z","2019-10-16T00:53:25Z"
"","8394","KAFKA-8107; Flaky Test kafka.api.ClientIdQuotaTest.testQuotaOverrideDelete","This PR adds a `waitForQuotaUpdate` after the quotas are removed. It also changes the default request quota to Long.MaxValue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-03-31T15:19:59Z","2020-04-08T06:43:42Z"
"","8123","KAFKA-8507: Unify bootstrap-server flag for command line tools (KIP-499) part-2","This patch updates ReplicaVerificationTool, TransactionalMessageCopier, GetOffsetShell and VerifiableLog4jAppender to add and prefer the --bootstrap-server flag for defining the connection point of the Kafka cluster. This change is part of KIP-499: https://cwiki.apache.org/confluence/display/KAFKA/KIP-499+-+Unify+connection+name+flag+for+command+line+tool.","open","","stanislavkozlovski","2020-02-15T10:34:48Z","2020-12-01T23:09:31Z"
"","8448","KAFKA-9796; Broker shutdown could be stuck forever under certain conditions","This patch reworks the SocketServer to always start the acceptor threads after the processor threads and to always stop the acceptor threads before the processor threads. It ensures that the acceptor shutdown is not blocked waiting on the processors to be fully shutdown by decoupling the shutdown signal and the awaiting. It also ensure that the processor threads drain its newConnection queue to unblock acceptors that may be waiting. However, the acceptors still bind during the startup, only the processing of new connections and requests is further delayed.  The flow looks like this now:  ``` val socketServer = ...  socketServer.startup(startProcessingRequests = false) // Acceptors are bound.  socketServer.startProcessingRequests(authorizerFutures) // Acceptors and Processors process new connections and requests  socketServer.stopProcessingRequests() // Acceptors and Processors are stopped  socketServer.shutdown() // SocketServer is shutdown (metrics, etc.) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-04-08T12:24:27Z","2020-04-16T16:16:49Z"
"","8320","KAFKA-8470: State change logs should not be in TRACE level","This patch revises the log level and content of many of the state change logger logs and changes its default log level to INFO.","closed","","stanislavkozlovski","2020-03-20T14:56:43Z","2020-03-26T21:53:41Z"
"","7638","MINOR: Rework NewPartitionReassignment public API","This patch removes the NewPartitionReassignment#of() method in favor of a simple constructor. Said method was confusing due to breaking two conventions - always returning a non-empty Optional and thus not being used as a static factory method.","closed","","stanislavkozlovski","2019-11-04T12:18:33Z","2019-11-05T18:34:12Z"
"","7708","MINOR: Remove explicit version checks in getErrorResponse methods","This patch removes the explicit version check pattern we used in `getErrorResponse`, which is a pain to maintain (as seen by KAFKA-9200). We already check that requests have a valid version range in the `AbstractRequest` constructor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-18T19:22:00Z","2019-11-19T01:32:24Z"
"","7879","MINOR: Remove spammy, unhelpful log message about preferred leaders","This patch removes a spammy log message in the controller which is printed every time the leader imbalance ratio is checked. It is unhelpful because preferred leaders are generally deterministic and is spammy because it includes _every_ partition in the cluster.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-30T22:12:53Z","2020-01-02T21:45:25Z"
"","7641","MINOR: Add a unit test for GroupMetadataManager#offsetExpiredSensor","This patch is a continuation of b7cab2d66f017464d3242525643a2afe2c28e924 which fixed a bug in the recording of the sensor but did not follow the policy of adding a test for said bugfix","open","","stanislavkozlovski","2019-11-04T18:12:52Z","2020-12-09T09:39:32Z"
"","7596","KAFKA-9102; Increase default zk session timeout and replica max lag [KIP-537]","This patch increases the default value of `zookeeper.session.timeout` from 6s to 18s and `replica.lag.time.max.ms` from 10s to 30s. This change was documented in KIP-537: https://cwiki.apache.org/confluence/display/KAFKA/KIP-537%3A+Increase+default+zookeeper+session+timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-25T01:25:34Z","2019-10-26T05:10:02Z"
"","7663","MINOR: Fix version range check in MessageTest","This patch fixes the test utility `testAllMessageRoundTripsFromVersion` in `MessageTest` which was unintentionally excluding the highest version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-08T00:18:40Z","2019-11-08T06:15:40Z"
"","7682","KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure","This patch fixes an NPE in `DefaultMetadataUpdater` due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-12T18:01:49Z","2019-12-04T21:23:44Z"
"","8454","KAFKA-9844; Maximum number of members within a group is not always enforced due to a race condition in join group","This patch fixes a race condition in the join group request handling which sometimes results in not enforcing the maximum number of members allowed in a group. The JIRA provides an example: https://issues.apache.org/jira/browse/KAFKA-9844  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-04-09T12:31:27Z","2020-10-06T20:10:07Z"
"","7895","MINOR: Fix failing test case in TransactionLogTest","This patch fixes a brittle expectation on the `toString` implementation coming from `Set`. This was failing on jenkins with the following error: ``` java.lang.AssertionError: expected: but was: ``` Instead we convert the collection to a string directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-04T01:13:51Z","2020-01-06T17:24:54Z"
"","7735","MINOR: Cleanup redundancies in BaseRequestTest","This patch eliminates a lot of redundancy and general messiness around the usage of `BaseRequestTest` and specifically response deserialization.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-22T02:25:22Z","2019-12-05T06:54:47Z"
"","7690","KAFKA-9183; Remove redundant admin client integration testing","This patch creates a `BaseAdminIntegrationTest` to be the root for all integration test extensions. Most of the existing tests will only be tested in `PlaintextAdminIntegrationTest`, which extends from `BaseAdminIntegrationTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-14T00:07:10Z","2019-11-16T00:06:16Z"
"","7931","KAFKA-9420: Add flexible version support for converted protocols","This patch bumps some APIs that have recently been converted to use the generated protocols in order to support flexible versions. This change was documented and approved in KIP-482: https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-10T19:32:39Z","2020-01-14T19:36:35Z"
"","7737","KAFKA-9224 (EOS improvement): Flush state store after transaction commit","This patch attempts to enforce a strict order to flush the state store only after ongoing transaction gets committed under EOS. Major changes include:  - Rewrite `StreamTask.commit` to separate EOS and non EOS commit scenario - Under EOS,  we make the atomic operation in the following order: commit ongoing transaction, flush the store, begin another transaction. This means there is no intermediate data left within the cache and the ongoing modification to the state store only gets visible after the transaction gets committed. - Add a `bounded` config on the stream thread cache to reject further writes. This feature is important as under EOS the thread cache is the only container we are going to leverage. When the cache is almost full, it will begin throwing a `CacheFullException` to the caller. - In ProcessorContext, handle `CacheFullException` by requesting a task commit in next round. This should only be needed on ReadWrite access to state stores for general put operation, and avoiding further interruption to the application.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","abbccdda","2019-11-22T04:08:38Z","2019-11-22T08:19:51Z"
"","7591","KAFKA-9094: Add server-side replica validation for ZK triggered reassignments","This patch adds the same server-side validation we use for API triggered reassignments to the ZK path.","open","","stanislavkozlovski","2019-10-24T10:29:10Z","2019-10-24T10:29:36Z"
"","7499","KAFKA-5682: Include partitions in exceptions raised during consumer record deserialization/validation","This patch adds a new RecordDeserializationException which is raised when the user is expected to seek past an offset whose record is corrupted in some way","closed","","stanislavkozlovski","2019-10-11T17:11:56Z","2021-06-04T10:05:55Z"
"","7724","KAFKA-8509; Add downgrade system test","This patch adds a basic downgrade system test. It verifies that producing and consuming continues to work before and after the downgrade.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-21T02:08:16Z","2019-11-22T18:09:14Z"
"","7579","MINOR: Add toString to PartitionReassignment","This patch adds a `toString()` implementation to `PartitionReassignment`. It also makes the `ListPartitionReassignmentsResult` constructor use default access, which is the standard for the admin client *Result classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-22T20:21:08Z","2019-10-23T07:53:44Z"
"","8389","KAFKA-9777; Remove txn purgatory to fix race condition on txn completion","This patch addresses a locking issue with DelayTxnMarker completion. Because of the reliance on the shared read lock in TransactionStateManager and the deadlock avoidance algorithm in `DelayedOperation`, we cannot guarantee that a call to checkAndComplete will offer an opportunity to complete the job. This patch removes the reliance on this lock in two ways:  1. We replace the transaction marker purgatory with a map of transaction with pending markers. We were not using purgatory expiration anyway, so this avoids the locking issue and simplifies usage. 2. We were also relying on the read lock for the `DelayedProduce` completion when calling `ReplicaManager.appendRecords`. As far as I can tell, this was not necessary. The lock order is always 1) state read/write lock, 2) txn metadata locks. Since we only call `appendRecords` while holding the read lock, a deadlock does not seem possible.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-03-30T17:45:24Z","2020-03-31T01:16:49Z"
"","8377","KAFKA-9777; Use asynchronous write to log after txn marker completion","This patch addresses a locking issue with `DelayTxnMarker` completion. Because of the reliance on the read lock in `TransactionStateManager`, we cannot guarantee that a call to `checkAndComplete` will offer an opportunity to complete the job. This patch removes the reliance on this lock. Instead when the operation is completed, we write the completion message to the log asynchronously.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-03-27T22:07:31Z","2020-03-30T17:44:18Z"
"","8409","KAFKA-6145: KIP-441 Pt. 6 Trigger probing rebalances until group is stable","This KIP is fairly straightforward, and does as the title describes by enforcing a rebalance once the configured `probing.rebalance.interval` has elapsed. However, we have had to modify the original plan in the KIP slightly to handle an edge case with static membership enabled:  Since the group leader can crash and restart without triggering a rebalance, we can't rely on a purely in-memory flag/counter to keep track of these probing rebalances. We can instead rely on the assignment, encoding the upcoming probing rebalance in the `AssignmentInfo`. This is encoded as the time in ms of the next scheduled rebalance, ie it is set to `currentTimeMs + probingRebalanceIntervalMs` when the assignment is being generated. This anchors the probing rebalances to wall clock time, and ensures a pathologically failing member will not prevent the group from ever rebalancing. We leave it up to a single member to be responsible for triggering the probing rebalances, and encode this for a single consumer on the leader's client (chosen arbitrarily).   This PR also adds tracking of whether a followup rebalance is required to `StreamsPartitionAssignor#assign` in order to log when the group is actually stable. This applies not only to KIP-441, but to version probing and cooperative rebalances as well.","closed","","ableegoldman","2020-04-03T01:33:39Z","2020-06-26T22:37:50Z"
"","8464","KAFKA-9852: Change the max duration that calls to the buffer pool can block from 2000ms to 10ms","This is to reduce overall test runtime, as this is wallclock time.  Adjusted one assert condition on a testcase as the success was dependant on thread runtimes and the much lower tolerances due to the reduced time broke this test.  Ran a couple thousand iterations of the test class on my machine without failed test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2020-04-11T00:02:03Z","2020-04-23T07:19:09Z"
"","8009","KAFKA-9308: Reworded the ssl part of the security documentation","This is to fix various issues (mainly as noted by this jira, the problem that SAN extension values are not copied to certificates) and add some recommendations.  Build the page and reviewed it, used Intellij HTML syntax checker to ensure valid HTML syntax.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2020-01-28T13:06:24Z","2020-02-26T11:34:47Z"
"","8254","KIP-557: Add Emit On Change Support","This is the initial draft PR for adding emit on change. For reference of the design document, please see here.   https://cwiki.apache.org/confluence/display/KAFKA/KIP-557%3A+Add+emit+on+change+support+for+Kafka+Streams  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","ConcurrencyPractitioner","2020-03-08T18:29:07Z","2020-06-12T23:19:03Z"
"","7984","KAFKA-9445: Allow adding changes to allow serving from a specific partition","This is the implementation of KIP-562: https://cwiki.apache.org/confluence/display/KAFKA/KIP-562%3A+Allow+fetching+a+key+from+a+single+partition+rather+than+iterating+over+all+the+stores+on+an+instance     ### Committer Checklist (excluded from commit message)  * [ ]  Verify design and implementation  * [ ]  Verify test coverage and CI build status  * [ ]  Verify documentation (including upgrade notes)","closed","","brary","2020-01-19T10:39:14Z","2020-02-06T08:53:57Z"
"","7883","MINOR: fix omitted 'next.' in interactive queries documentation","This is the counterpart of [kafka-site PR](https://github.com/apache/kafka-site/pull/247).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-01-01T19:36:38Z","2020-02-19T18:24:40Z"
"","7833","KAFKA-9113: Extract clients from tasks to record collectors","This is part1 of a series of PRs for task management cleanup:  1. Primarily cleanup MockRecordCollectors: remove unnecessary anonymous inheritance but just consolidate on the NoOpRecordCollector -> renamed to MockRecordCollector. Most relevant changes are unit tests that would be relying on this MockRecordCollector.  2. Let StandbyContextImpl#recordCollector() to return null instead of returning a no-op collector, since in standby tasks we should ALWAYS bypass the logging logic and only use the inner store for restoreBatch. Returning null helps us to realize this assertion failed as NPE as early as possible whereas a no-op collector just hides the bug.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-12-14T04:24:20Z","2020-04-24T23:44:30Z"
"","7548","KAFKA-8700: Flaky Test QueryableStateIntegrationTest#queryOnRebalance","This is not guaranteed to actually fix `queryOnRebalance`, since the failure could never be reproduced locally. I did not bump timeouts because it looks like that has been done in the past for this test without success. Instead this change makes the following improvements:  1. It waits for the application to be in a RUNNING state before proceeding with the test.  2. It waits for the remaining instance to return to RUNNING state within a timeout after rebalance. I observed once that we were able to do the KV queries but the instance was still in REBALANCING, so this should reduce some opportunity for flakiness.  3. The meat of this change: we now iterate over all keys in one shot (vs. one at a time with a timeout) and collect various failures, all of which are reported at the end. This should help us to narrow down the cause of flakiness if it shows up again.  Before:  ``` java.lang.AssertionError: Condition not met within timeout 120000. waiting for metadata, store and value to be non null     at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:376)     ... ```  After:  ``` java.lang.AssertionError: Not all keys are available for store word-count-store-stream-three-0 in 120000 ms     * No value is available for these keys: [goodbye, foo, bar]     * Exceptions were raised for the following keys:         Sam:             java.lang.IllegalArgumentException: Ooops             	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.lambda$verifyAllKVKeys$6(QueryableStateIntegrationTest.java:289)             	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417)             	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385)             	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.verifyAllKVKeys(QueryableStateIntegrationTest.java:265)             	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.queryOnRebalance(QueryableStateIntegrationTest.java:431)  	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:24)         ... ```  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cpettitt-confluent","2019-10-17T20:41:15Z","2019-10-23T19:39:32Z"
"","8237","MINOR: Add clarifying ""additionally"" to streams aggregation docs","This is just to emphasize the difference between aggregating a stream and a table, as the text currently reads, it could suggest that for grouping a table one does not need to provide an adder but just a subtractor in its place.  No testing performed, as only a text change in html docs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2020-03-06T13:30:39Z","2020-03-23T20:17:18Z"
"","7962","KAFKA-6144: option to query restoring and standby","This is based on a temporary branch, which is mirrored from https://github.com/apache/kafka/pull/7960.  I will delete the temporary branch once #7960 is merged and re-target this PR to trunk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-01-14T23:17:55Z","2020-01-17T04:10:51Z"
"","7926","MINOR: Improve AuthorizerIntegrationTest","This is another round in my effort to clean up some of the integration tests and improve build time. This patch improves the authorizer integration tests in the following ways:  1. We use a separate principal for inter-broker communications. This ensures that ACLs set in the test cases do not interfere with inter-broker communication. We had two test cases (`testCreateTopicAuthorizationWithClusterCreate` and `testAuthorizationWithTopicExisting`) which depend on topic creation and were timing out because of inter-broker metadata propagation failures. The timeouts were treated as successfully satisfying the expectation of authorization. So the tests passed, but not because of the intended reason. 2. Previously `GroupAuthorizerIntegrationTest` was inheriting _all_ of the tests from `AuthorizerIntegrationTest`. This seemed like overkill since the ACL evaluation logic is essentially the same.   Totally this should take about 5-10 minutes off the total build time and make the authorizer integration tests a little more resilient to problems with inter-broker communication.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-10T00:24:49Z","2020-02-24T20:12:34Z"
"","7617","KAFKA-8972 (2.4 blocker): bug fix for restoring task","This is a typo bug which is due to calling a wrong map.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-10-31T00:58:23Z","2019-11-01T00:14:29Z"
"","7772","KAFKA-9261; Client should handle inconsistent leader metadata","This is a reduced scope fix for KAFKA-9261. The purpose of this patch is to ensure that partition leader state is kept in sync with broker metadata in `MetadataCache` and consequently in `Cluster`. Due to the possibility of metadata event reordering, it was possible for this state to be inconsistent which could lead to an NPE in some cases. The test case here provides a specific scenario where this could happen.  Also see #7770 for additional detail.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-03T21:54:30Z","2019-12-04T04:52:27Z"
"","8049","MINOR: Added missing default serdes to the streams.scala.Serdes","This is a minor improvement. I noticed that `org.apache.kafka.streams.scala.Serdes` don't have all serdes from the `org.apache.kafka.common.serialization.Serdes`. So, I added missing serdes.","closed","","LMnet","2020-02-06T06:31:00Z","2020-06-30T09:53:23Z"
"","8146","MINOR: Standby task commit needed when offsets updated","This is a minor fix of a regression introduced in the refactoring PR: in current trunk `standbyTask#commitNeeded` always return false, which would cause standby tasks to never be committed until closed. To go back to the old behavior we would return true when new data has been applied and offsets being updated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-21T01:15:11Z","2020-02-21T20:08:07Z"
"","7685","KAFKA-9011: Removed multiple calls to supplier.get()","This is a followup PR for #7520 to address issue of multiple calls to get() as it was pointed out by @bbejeck  in https://github.com/apache/kafka/pull/7520#discussion_r345374820  @vvcephei   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","kokachev","2019-11-13T02:15:21Z","2019-11-14T21:12:47Z"
"","8457","KAFKA-9842; Add test case for OffsetsForLeaderEpoch grouping in Fetcher","This is a follow-up to #8077. The bug exposed a testing gap in how we group partitions. This patch adds a test case which reproduces the reported problem.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-09T21:01:17Z","2020-04-14T00:20:02Z"
"","7916","KAFKA-9389 - Document how to use kafka-reassign-partitions.sh to change log dirs for a partition","This is a docs only PR.  Added documentation about specifying log_dirs inline with KIP-113 and updated example json schemas to include new ""log_dirs"" field examples.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ x] Verify documentation (including upgrade notes)","closed","","mitchell-h","2020-01-09T07:06:37Z","2020-01-28T18:39:47Z"
"","8117","KAFKA-8403: Suppress needs a Materialized variant","This is a completely new implemetation from the previous PR.  - [KAFKA-8403: Suppress needs a Materialized variant](https://issues.apache.org/jira/browse/KAFKA-8403) - [KIP-508: Make Suppression State Queriable](https://cwiki.apache.org/confluence/display/KAFKA/KIP-508%3A+Make+Suppression+State+Queriable)  Here are some guidelines:  - 9bd279f ~ 37060b6: Refactor the `TimeOrderedKeyValueStore` into new interfaces, with separating `Serdes` from BytesStore. (see `MeteredSuppressionBuffer`) - 798af5c ~ 0a684e0: Implement the `KTable#suppress` variant and make it queriable. - 8f2e2c8  ~ 3387e27: Make `SuppressionBuffer` accessable by the users. - 821014f: Implement a testing functionality.  Please have a look when you are free. Thanks in advance! :smile:  cc/ @mjsax @bbejeck @vvcephei @guozhangwang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","dongjinleekr","2020-02-14T14:02:59Z","2021-03-17T05:40:58Z"
"","8439","KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase","This is a cherry-pick PR from #8405 to trunk (due to the large divergence we cannot do that vie git cherry-pick).  * Fix the direct cause of the observed issue on the client side: when heartbeat getting errors and resetting generation, we only need to set it to UNJOINED when it was not already in REBALANCING; otherwise, the join-group handler would throw the retriable UnjoinedGroupException to force the consumer to re-send join group unnecessarily.  * Fix the root cause of the issue on the broker side: we should still trigger rebalance when static member joins in CompletingRebalance phase; otherwise the member.ids would be changed when the assignment is received from the leader, hence causing the new member.id's assignment to be empty.  * Added log4j entries as a by-product of my investigation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-04-07T18:59:03Z","2020-04-25T00:01:56Z"
"","7630","KAFKA-9073: check assignment in requestFailed to avoid NPE","This is a cherry-pick of the bug-fix included in https://github.com/apache/kafka/pull/6884 to 2.3 and older branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-11-01T00:54:59Z","2019-12-04T02:33:49Z"
"","7805","KAFKA-9212; Ensure LeaderAndIsr state updated in controller context during reassignment","This is a cherry-pick of https://github.com/apache/kafka/commit/5d0cb1419cd1f1cdfb7bc04ed4760d5a0eae0aa1. The main differences are 1) leader epoch validation is unconditionally disabled, and 2) the test case has been refactored due to the absence of the reassignment admin APIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-09T18:15:23Z","2019-12-10T02:15:33Z"
"","8023","KAFKA-8507 kip 499 Unify connection name flag for command line tool","This is a change updates ConsoleProducer, ConsumerPerformance, VerifiableProducer, and VerifiableConsumer classes to add and prefer the --bootstrap-server flag for defining the connection point of the Kafka cluster.   ### Committer Checklist (excluded from commit message) - [x ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mitchell-h","2020-01-31T03:56:40Z","2020-02-13T21:44:52Z"
"","7580","KAFKA-9057: Backport KAFKA-8819 and KAFKA-8340 to 1.0","This includes the fix from #7315 , but backported to be compatible with 1.0  Changes to this PR since #7549 are mostly in Plugins and PluginsTest.  * Added PluginsTest file which did not exist before * Removed Converter::configure test case * Removed HeaderConverter test plugin and test cases * Removed Converter Types test cases * Removed classloader control test cases  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2019-10-22T20:24:05Z","2020-10-16T06:15:42Z"
"","8140","HOTFIX: don't try to remove uninitialized changelogs from assignment & don't prematurely mark task closed","This fixes two issues which together caused the soak to crash/some test to fail occasionally.  What happened was: In the main `StreamThread` loop we initialized a new task in `TaskManager#checkForCompletedRestoration` which includes registering, but not initializing, its changelogs. We then complete the loop and call poll, which resulted in a rebalance that revoked the newly-initialized task. In `TaskManager#handleAssignment` we then closed the task cleanly and go to remove the changelogs from the `StoreChangelogReader` only to get an `IllegalStateException` because the changelog partitions were not in the restore consumer's assignment (due to being uninitialized).  This by itself should^ be a recoverable error, as we catch exceptions here and retry closing the task as unclean. Of course the task actually was successfully closed (clean) so we now get an unexpected exception `Illegal state CLOSED while closing active task`  The fix(es) I'd propose are: 1) ~~Keep the restore consumer's assignment in sync with the registered changelogs, ie the set `ChangelogReader#changelogs` but pause them until they are initialized~~ edit: since the consumer does still perform some actions (gg fetches) on paused partitions, we should avoid adding uninitialized changelogs to the restore consumer's assignment. Instead, we should just skip them when removing. 2) Move the `StoreChangelogReader#remove` call to _before_ the `task.closeClean` so that the task is only marked as closed if everything was successful. We should do so regardless, as we should (attempt to) remove the changelogs even if the clean close failed and we must do unclean.","closed","","ableegoldman","2020-02-20T02:57:14Z","2020-06-26T22:37:58Z"
"","8017","KAFKA-9422: Track the set of topics a connector is using (KIP-558)","This feature corresponds to KIP-558 and extends how the status.storage.topic is used to include information that allows Kafka Connect to keep track which topics a connector is using.   The set of topics a connector is actively using, is exposed via a new endpoint that is added to the REST API of Connect workers. * A GET /connectors/{name}/topics request will return the set of topics that have been recorded as active since a connector started or since the set of topics was reset for this connector.   An additional endpoints allows users to reset the set of active topics for a connector via the second endpoint that this feature is adding:  * A PUT /connectors/{name}/topics/reset request clears the set of active topics.   An operator may enable or disable this feature by setting:  topic.tracking.enable (true by default).   Additionally, an operator may disable only reset requests via the Connect REST API by setting:  topic.tracking.allow.reset to false (true by default).   In the current commit the code for KIP-558 is feature complete.  It's tested by extending the current unit tests. Because changes are required in the integration testing framework itself in order to add integration testing for topic tracking (KIP-558), these tests will be submitted in a follow up pull request. Same, for Connect system tests that will extend coverage beyond integration testing. System tests will be added in a separate pull request, if additional coverage is required beyond integration testing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2020-01-29T03:29:11Z","2020-10-16T05:51:00Z"
"","7730","MINOR: don't log entire task string when setting partition time","This ends up spamming the logs and doesn't seem to be providing much useful information, rather than logging the full task (which includes the entire topology description) we should just log the task id.  Cherry-pick to 2.4","closed","streams,","ableegoldman","2019-11-21T22:25:31Z","2019-12-03T23:46:08Z"
"","7487","KAFKA-9010: Add PartitionGrower, Support partition requery in ProduceBench test.","This creates a test in trogdor to grow a topic at a specified interval by a specified number of partitions.  This test also introduces the ability for the ProduceBench test workload to requery the partitions of the active topics at an interval, if so desired.  If no query interval is specified, the default behaviour of ProduceBenchWorker remains unchanged.","open","","scott-hendricks","2019-10-10T17:34:38Z","2020-03-06T21:02:39Z"
"","7828","MINOR: add UPGRADE_FROM to 2.0 config docs","This config was introduced in 2.0, so we should merge this PR first and cherry-pick back to 2.0 before merging the [2.4 update](https://github.com/apache/kafka/pull/7825)","closed","","ableegoldman","2019-12-13T02:08:18Z","2019-12-13T19:29:27Z"
"","7825","MINOR: add UPGRADE_FROM to 2.4 config docs","This config is among those missing from the ""Configuring a Streams Application"" docs, and since it is once again required for safe upgrade in 2.4 we should really make sure it's included.  This should only be cherry-picked to 2.4 since it references the new version (2.4)  Separate PR for the 2.0 - 2.3 docs: https://github.com/apache/kafka/pull/7831","closed","","ableegoldman","2019-12-12T03:48:29Z","2019-12-16T21:55:31Z"
"","8289","KAFKA-9712: Catch and handle exception thrown by reflections scanner","This commit works around a bug in v0.9.12 in upstream `reflections` library by catching and handling the exception thrown.  The reflections issue is tracked by: https://github.com/ronmamo/reflections/issues/273  New unit tests were introduced to test the behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ncliang","2020-03-12T21:48:59Z","2020-10-16T05:53:51Z"
"","7648","KAFKA-9143: Log task reconfiguration error only when it happened","This commit makes `DistributedHerder` log that some error has happened during task reconfiguration only when it actually has happened.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ivanyu","2019-11-05T16:25:29Z","2020-10-16T06:15:43Z"
"","7857","KAFKA-9314: Apply RetryWithToleranceOperator to push() and poll()","This commit applies RetryWithToleranceOperator added as part of KIP-298 to be used when retrying RetriableExceptions thrown from SinkTask::put() and SourceTask::poll().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ncliang","2019-12-19T20:07:40Z","2020-06-11T06:10:16Z"
"","7877","KAFKA-9312: Wait for splitted batches to be processed after a KafkaProducer#flush()","This commit adds the logic to wait for `splitted` batches to be processed after a Message Too Large Exception has been received. Also, add a new test class to cover `IncompleteBatches` class.  This code adds a new constructor to the classes `ProducerBatch` and `RecordAccumulator` visible to the package, this decouples the dependency between `ProduceRequestResult` and `IncompleteBatches` respectively and allow to test the method `RecordAccumulator#awaitFlushCompletion()`  The [Jira ticket](https://issues.apache.org/jira/browse/KAFKA-9312) provides a [test here](https://github.com/lbradstreet/kafka/commit/733a683273c31823df354d0a785cb2c24365735a#diff-0b8da0c7ceecaa1f00486dadb53208b1R2339) that prove the error without these changes. This PR does not include that specific test since it involves sleeping the thread and could lead to indeterminate behavior.","open","","jonathansantilli","2019-12-30T16:02:11Z","2020-09-21T07:48:44Z"
"","7890","KAFKA-9327: GroupMetadata metrics are not documented","This commit adds documentation on following `GroupMetadata` gauges:  - `NumOffsets` - `NumGroups` - `NumGroupsPreparingRebalance` - `NumGroupsCompletingRebalance` - `NumGroupsEmpty` - `NumGroupsStable` - `NumGroupsDead`  cc/ @gwenshap  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-01-03T05:36:13Z","2020-03-02T06:45:08Z"
"","8240","KAFKA-7787: protocol enums","This changes message generator to support generating classes containing constants with integer values (enums, in effect), and support for validation that message fields use only supported values from those domains (with support for different allowed values in different versions).","open","","tombentley","2020-03-06T18:40:09Z","2020-03-06T18:40:09Z"
"","7493","KAFKA-8988. Replace CreatePartitions Request/Response with automated protocol","This change update the CreatePartitions request and response api objects to use automated protocol. The change involved updating all client code to use the newly updated CreatePartitionsRequest and CreatePartitionsResponse classes.  Updated relevant tests. All tests pass.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-10-11T01:26:10Z","2020-01-08T23:47:28Z"
"","8000","KAFKA-9417: New Integration Test for KIP-447","This change mainly have 2 components:  1. extend the existing `transactions_test.py` to also try out new `sendTxnOffsets(groupMetadata)` API to make sure we are not introducing any regression or compatibility issue   a. We shrink the time window to 10 seconds for the txn timeout scheduler on broker so that we could trigger expiration earlier than later 2. create a completely new system test class called `group_mode_transactions_test` which is more complicated than the existing system test, as we are taking rebalance into consideration and using multiple partitions instead of one. For further breakdown:   a. The message count was done on partition level, instead of global as we need to visualize the per partition order throughout the test. For this sake, we extend ConsoleConsumer to print out the data partition as well to help message copier interpret the per partition data.   b. The progress count includes the time for completing the pending txn offset expiration   c. More visibility and feature improvements on `TransactionMessageCopier` to better work under either standalone or group mode. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-01-22T21:55:14Z","2020-02-12T20:46:45Z"
"","7488","Provide better messages when waiting for a condition in test","This change lifts `retryOnExceptionWithTimeout` from `OptimizedKTableIntegrationTest` into `TestUtils` and uses it for some of the pre-canned condition wait functions. `retryOnExceptionWithTimeout` differs from `waitForCondition` in that it allows you to use regular assertions, with more descriptive messages and more details context when a test fails.  Some examples:  * `IntegrationTestUtils.waitUntilMinValuesRecordsReceived`  Before:  ``` java.lang.AssertionError: Condition not met within timeout 1000. Did not receive all 51 records from topic output-three-0  	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:377) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:354) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:682) 	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1000) 	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.queryOnRebalance(QueryableStateIntegrationTest.java:379) ```  After:  ``` java.lang.AssertionError: Did not receive all 51 records from topic output-three-0 within 1000 ms Expected: is a value equal to or greater than <51>      but: <38> was less than <51>  	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinValuesRecordsReceived$6(IntegrationTestUtils.java:674) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinValuesRecordsReceived(IntegrationTestUtils.java:670) 	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.waitUntilAtLeastNumRecordProcessed(QueryableStateIntegrationTest.java:1000) 	at org.apache.kafka.streams.integration.QueryableStateIntegrationTest.queryOnRebalance(QueryableStateIntegrationTest.java:379) ```  * `IntegrationTestUtils.waitUntilMinRecordsReceived`  Before:  ``` java.lang.AssertionError: Condition not met within timeout 60000. Did not receive all 1 records from topic outputTopic  	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:377) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:354) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinRecordsReceived(IntegrationTestUtils.java:460) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinRecordsReceived(IntegrationTestUtils.java:432) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.verifyKeyValueTimestamps(IntegrationTestUtils.java:729) 	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.checkResult(AbstractJoinIntegrationTest.java:170) 	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.runTest(AbstractJoinIntegrationTest.java:218) 	at org.apache.kafka.streams.integration.TableTableJoinIntegrationTest.testInner(TableTableJoinIntegrationTest.java:121) ```  After:  ``` java.lang.AssertionError: Did not receive all 51 records from topic outputTopic within 60000 ms Expected: is a value equal to or greater than <51>      but: <1> was less than <51>  	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilMinRecordsReceived$0(IntegrationTestUtils.java:458) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinRecordsReceived(IntegrationTestUtils.java:454) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinRecordsReceived(IntegrationTestUtils.java:432) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.verifyKeyValueTimestamps(IntegrationTestUtils.java:734) 	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.checkResult(AbstractJoinIntegrationTest.java:170) 	at org.apache.kafka.streams.integration.AbstractJoinIntegrationTest.runTest(AbstractJoinIntegrationTest.java:218) 	at org.apache.kafka.streams.integration.TableTableJoinIntegrationTest.testInner(TableTableJoinIntegrationTest.java:121) ```  * And my favorite: `IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived`  Before:  ``` java.lang.AssertionError: Condition not met within timeout 60000. Did not receive all [KeyValueTimestamp{key=A, value=A:A, timestamp=1570731458595}, KeyValueTimestamp{key=B, value=B:B, timestamp=1570731458595}, KeyValueTimestamp{key=C, value=C:C, timestamp=1570731458595}, KeyValueTimestamp{key=D, value=D:D, timestamp=1570731458595}, KeyValueTimestamp{key=F, value=F:F, timestamp=1570731458595}, KeyValueTimestamp{key=E, value=E:E, timestamp=1570731458595}] records from topic output-0  	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:377) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:354) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(IntegrationTestUtils.java:638) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilFinalKeyValueTimestampRecordsReceived(IntegrationTestUtils.java:569) 	at org.apache.kafka.streams.integration.KStreamAggregationDedupIntegrationTest.validateReceivedMessages(KStreamAggregationDedupIntegrationTest.java:246) 	at org.apache.kafka.streams.integration.KStreamAggregationDedupIntegrationTest.shouldReduce(KStreamAggregationDedupIntegrationTest.java:125) ```  After:  ``` java.lang.AssertionError: Received records from topic output-0 did not match expected within 60000 ms Expected: is <{A=[KeyValueTimestamp{key=A, value=A:A, timestamp=1570731302494}], B=[KeyValueTimestamp{key=B, value=B:B, timestamp=1570731302494}], C=[KeyValueTimestamp{key=C, value=C:C, timestamp=1570731302494}], D=[KeyValueTimestamp{key=D, value=D:D, timestamp=1570731302494}], E=[KeyValueTimestamp{key=E, value=E:E, timestamp=1570731302494}], F=[KeyValueTimestamp{key=F, value=F:F, timestamp=1570731302494}]}>      but: was <{A=[KeyValueTimestamp{key=A, value=A:A, timestamp=1570731302494}], B=[KeyValueTimestamp{key=B, value=B:B, timestamp=1570731302494}], C=[KeyValueTimestamp{key=C, value=C:C, timestamp=1570731302494}], D=[KeyValueTimestamp{key=D, value=D:D, timestamp=1570731302494}], E=[KeyValueTimestamp{key=E, value=E:E, timestamp=1570731302494}]}>  	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.lambda$waitUntilFinalKeyValueRecordsReceived$5(IntegrationTestUtils.java:631) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:417) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:385) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(IntegrationTestUtils.java:603) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilFinalKeyValueTimestampRecordsReceived(IntegrationTestUtils.java:566) 	at org.apache.kafka.streams.integration.KStreamAggregationDedupIntegrationTest.validateReceivedMessages(KStreamAggregationDedupIntegrationTest.java:246) 	at org.apache.kafka.streams.integration.KStreamAggregationDedupIntegrationTest.shouldReduce(KStreamAggregationDedupIntegrationTest.java:125) ```  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cpettitt-confluent","2019-10-10T18:19:26Z","2019-10-16T05:21:56Z"
"","7674","KAFKA-9106 make metrics exposed via jmx configurable","This change implements [KIP-544](https://cwiki.apache.org/confluence/display/KAFKA/KIP-544%3A+Make+metrics+exposed+via+JMX+configurable)","closed","","xvrl","2019-11-09T02:51:07Z","2020-02-13T18:21:15Z"
"","7671","MINOR: refactor replica last sent HW updates due to performance regression","This change fixes a performance regression due to follower last seen highwatermark handling introduced in 23beeea34bf9e5e7a33dc0566d54a17b2b417c0f. maybeUpdateHwAndSendResponse is expensive for brokers with high partition counts, as it requires a partition and a replica lookup for every partition being fetched. This refactor moves the last seen watermark update into the follower fetch state update where we have already looked up the partition and replica.  I've seen cases where maybeUpdateHwAndSendResponse is responsible 8% of CPU usage, not including the responseCallback call that is part of it.  I have benchmarked this change with org.apache.kafka.jmh.partition.UpdateFollowerFetchStateBenchmark and it adds 5ns of overhead to Partition.updateFollowerFetchState, which is a rounding error compared to the current overhead of maybeUpdateHwAndSendResponse.","closed","","lbradstreet","2019-11-08T21:45:05Z","2019-11-13T05:21:19Z"
"","8396","KAFKA-9754: Add an option to ignore ProduceBench errors","This change adds the following features to the Trogdor ProduceBenchWorker:  Ability to ignore errors caused by SendRecords. Tracking of the number of errors ignored. Additional tracking of the number of callbacks with exceptions. Ability to extend ProduceBenchWorker's verify phase to deal with large topics.  In order to maintain backward compatibility, ignoring errors is disabled by default.  All build-time integration tests passed:  BUILD SUCCESSFUL in 42m 8s 334 actionable tasks: 255 executed, 79 up-to-date","open","","scott-hendricks","2020-03-31T18:04:00Z","2020-04-25T19:53:43Z"
"","7858","Run pylint as a ducktape test","This change adds pylint as a dependency for the ducktape test as well adding a ducktape test that runs pylint. The rationale is that not every environment that runs our build will have pylint installed, but it's easy to add as a python dependency for the ducktape tests.  If desired for local development, pylint can be installed as a system package (apt, brew, etc) or installed with pip into a virtualenv. To run pylint on our tests, simply run:      pylint -E kafkatest  inside our ""tests"" directory. This will check for pylint errors in the kafkatest module. Run without the `-E` to see all the warnings.  The ducktape tests added here will fail an assertion if any pylint errors are present.  This PR also fixes all the pylint errors that were present.","open","","mumrah","2019-12-20T20:16:31Z","2020-01-28T14:54:23Z"
"","8324","KAFKA-9701 (fix): Only check protocol name when generation is valid","This bug was incurred by https://github.com/apache/kafka/pull/7994 with a too-strong consistency check. It is because a reset generation operation could be called in between the `joinGroupRequest` -> `joinGroupResponse` -> `SyncGroupRequest` -> `SyncGroupResponse` sequence of events, if user calls `unsubscribe` in the middle of consumer#poll().  Proper fix is to avoid the protocol name check when the generation is invalid.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2020-03-21T00:31:13Z","2022-05-11T13:09:54Z"
"","8166","KAFKA-9602: Close the stream internal producer only in EOS","This bug reproduces through the trunk stream test, the producer was closed unexpectedly when EOS is not turned on.  Will work on adding unit test to guard this logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-25T06:56:41Z","2020-02-26T04:12:12Z"
"","8190","KAFKA-9623: Keep polling until the task manager is no longer rebalancing in progress","This bug is found via the flaky SmokeTestDriverIntegrationTest. Without this PR the test fails every 3-4 times, after this issue is fixed we've run the test 20+ locally without error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-28T01:32:39Z","2020-04-24T23:52:02Z"
"","7592","MINOR: Bring back the NewPartitionReassignment#of() public API","This API was removed previously in an unrelated change, but seems worth keeping","closed","","stanislavkozlovski","2019-10-24T16:25:48Z","2019-11-04T18:30:38Z"
"","8481","KAFKA-9034 Tolerate spaces in $JAVA_HOME","This allows kafka tools to work on Cygwin where $JAVA_HOME typically contains a space (e.g. ""C:\Program Files\Java\jdkXXX"")","closed","","sebwills","2020-04-14T08:53:40Z","2020-04-30T08:21:50Z"
"","8334","KAFKA-6145: Add balanced assignment algorithm","This algorithm assigns tasks to clients and tries to - balance the distribution of the  partitions of the   same input topic over stream threads and clients,   i.e., data parallel workload balance - balance the distribution of work over stream threads. The algorithm does not take into account potentially existing states on the client.  The assignment is considered balanced when the difference in assigned tasks between the stream thread with the most tasks and the stream thread with the least tasks does not exceed a given balance factor.  The algorithm prioritizes balance over stream threads higher than balance over clients.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-03-23T14:54:29Z","2020-03-24T22:49:13Z"
"","7608","KAFKA-8972 (2.4 blocker): clear all state for zombie task on TaskMigratedException","Third bugfix for the failing broker bounce system test with cooperative rebalancing:  **tl;dr** We need to remove everything associated with a task when it is closed, but in some cases (eg `AssignedTasks#commit`) on a `TaskMigratedException` we would close it as a zombie and then (only) remove the taskId from the `running` map. This left  its partitions, restorers, state stores, etc around and in an undefined state, causing exceptions when closing and/or opening the stores again.  **Longer explanation:** In AssignedTasks (the abstract class from which the standby and active task variations extend) a commit failure (even due to broker down/unavailable) is treated as a TaskMigratedException after which the failed task is closed as a zombie and removed from running -- the remaining tasks (ie those still in running are then also closed as zombies in the subsequent onPartitionsLost  However we do not remove the closed task from runningByPartition nor do we remove the corresponding changelogs, if restoring, from the StoreChangelogReader since that applies only to active tasks, and AssignedTasks is generic/abstract. The changelog reader then retains a mapping from the closed task's changelog partition to its CompositeRestoreListener (and does not replace this when the new one comes along after the rebalance). The restore listener has a reference to a specific RocksDBStore instance, one which was closed when the task was closed as a zombie, so it accidentally tries to restore to the ""old"" RocksDBStore instance rather than the new one that was just opened.  Although technically this bug existed before KIP-429, it was only uncovered now that we remove tasks and clear their state/partitions/etc one at a time. We don't technically need to cherrypick the fix back earlier as before we just blindly clear all data structures entirely during an eager rebalance.","closed","streams,","ableegoldman","2019-10-29T02:20:09Z","2019-11-14T01:00:29Z"
"","7506","KAFKA-9035: Improve logging on potential loop forwaring in Kafka Connect REST","These two commits make Kafka Connect warn the user when there's a possibility of redirecting REST requests from a Connect worker to itself.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","ivanyu","2019-10-13T15:00:25Z","2020-03-22T00:50:38Z"
"","8387","MINOR: Update dependencies.gradle, Dockerfile, version.py, and bash for 2.4.1 upgrade","These files were missed in the `2.4.1` release  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-03-30T14:34:08Z","2020-03-30T16:56:04Z"
"","8197","KAFKA-9573: Fix VerifiableProducer and VerifiableConsumer to work with older Kafka versions","These classes are used by `upgrade_test.py` with old Kafka versions so they can only use functionality that exists in all Kafka versions. This change fixes the test for Kafka versions older than 0.11.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2020-03-01T06:36:03Z","2020-03-02T00:31:16Z"
"","8157","KAFKA-9088: Consolidate InternalMockProcessorContext and MockInternalProcessorContext","These changes migrate usages of `InternalMockProcessorContext` to use a new version of `MockInternalProcessorContext`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","pierDipi","2020-02-22T10:31:55Z","2021-09-24T23:32:51Z"
"","8132","KAFKA-9567: Docs, system tests for ZooKeeper 3.5.7","These changes depend on [KIP-515: Enable ZK client to use the new TLS supported authentication](https://cwiki.apache.org/confluence/display/KAFKA/KIP-515%3A+Enable+ZK+client+to+use+the+new+TLS+supported+authentication), which was only added to 2.5.0. The upgrade to ZooKeeper 3.5.7 was merged to both 2.5.0 and 2.4.1 via https://issues.apache.org/jira/browse/KAFKA-9515, but this change must only be merged to 2.5.0 (it will break the system tests if merged to 2.4.1).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2020-02-18T16:13:44Z","2020-02-25T14:30:22Z"
"","8078","Minor: move test cases from TimeConversionTest to ConsumerGroupCommandServerlessTest","There is no other scala files having multiples test classes so I guess the test class should be moved to an individual scala file. It was introduced by #4407.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-10T10:22:22Z","2020-06-19T06:39:45Z"
"","8412","KAFKA-9750; Fix race condition with log dir reassign completion","There is a race on receiving a LeaderAndIsr request for a replica with an active log dir reassignment. If the reassignment completes just before the LeaderAndIsr handler updates epoch information, it can lead to an illegal state error since no future log dir exists. This patch fixes the problem by ensuring that the future log dir exists when the fetcher is started. Removal cannot happen concurrently because it requires access the same partition state lock.  Co-authored-by: Chia-Ping Tsai   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-03T03:33:11Z","2020-04-03T18:51:05Z"
"","8101","KAFKA-9544; Fix flaky test `AdminClientTest.testDefaultApiTimeoutOverride`","There is a race condition with the backoff sleep in the test case and setting the next allowed send time in the AdminClient. To fix it, we allow the test case to do the backoff sleep multiple times if needed.  The test failure was fairly easy to reproduce prior to this fix. With the fix, I could not reproduce the problem after 500 runs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-02-12T22:47:42Z","2020-10-15T07:47:42Z"
"","8071","Minor: the configured transformations used by SourceTask/SinkTask sho…","There are many closable objects used by building SourceTask/SinkTask but we don't close them actually when exception is thrown. A obvious issue is the configured transformations are never closed when the build of task fails. This patch introduces a collection to keep all closeable objects to make sure they are closed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2020-02-08T08:27:40Z","2020-09-26T07:54:32Z"
"","8285","KAFKA-9700:Fix negative estimatedCompressionRatio issue","There are cases that currentEstimation is smaller than COMPRESSION_RATIO_IMPROVING_STEP and it will get negative estimatedCompressionRatio,which leads to misjudgment about if there is no room and MESSAGE_TOO_LARGE might occur.  Change-Id: I0932a2a6ca669f673ab5d862d3fe7b2bb6d96ff6 Signed-off-by: Jiamei Xie   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiameixie","2020-03-12T07:12:08Z","2020-04-03T01:21:18Z"
"","7995","KAFKA-9462: Correct exception message in DistributedHerder","There are a few exception messages in DistributedHerder which were copied from other exception message.  This PR corrects the messages to reflect actual condition  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","tedyu","2020-01-22T00:10:55Z","2020-01-24T19:56:09Z"
"","7753","KAFKA-9232: Coordinator new member heartbeat completion does not work for JoinGroup v3","The v3 JoinGroup logic does not properly complete the initial heartbeat for new members, which then expires after the static 5 minute timeout. The core issue is in the `shouldKeepAlive` method, which returns false when it should return true.","closed","","ableegoldman","2019-11-27T02:46:17Z","2020-06-26T22:38:04Z"
"","8435","MINOR: Fix log cleaner offset range log message","The upper limit offset is displayed incorrectly in the log cleaner summary message. For example: ``` Log cleaner thread 0 cleaned log __consumer_offsets-47 (dirty section = [358800359, 358800359]) ``` We should be using the first uncleanable offset as the upper limit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-06T20:01:18Z","2020-04-07T00:11:13Z"
"","7827","MINOR: upgrade system test should check for ISR rejoin on each roll","The upgrade system test correctly rolls by upgrading the broker and  leaving the IBP, and then rolling again with the latest IBP version. Unfortunately, this is not sufficient to pick up many problems in our IBP gating as we charge through the rolls and after the second roll all of the brokers will rejoin the ISR and the test will be treated as a success.  I have tested this theory by testing the upgrade test with single roll IBP bumps. ""java.lang.IllegalArgumentException: Invalid version"" exceptions were encountered on brokers, but the test still passed.  This test adds two new checks: 1. We wait for the ISR to stabilize for all partitions. This is best practice during rolls, and is enough to tell us if a broker hasn't rejoined after each roll. 2. We check the broker logs for some common protocol errors. This is a fail safe as it's possible for the test to be successful even if some protocols are incompatible and the ISR is rejoined.","closed","","lbradstreet","2019-12-12T23:39:03Z","2019-12-30T19:02:30Z"
"","7803","MINOR: Fix javadoc of `flatTransform()`","The two missing } in the javadoc of `flatTransform()` lead to the following javadoc compile warnings:  `.../KStream.java:1156: warning - End Delimiter } missing for possible See Tag in comment string`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-12-09T12:22:15Z","2019-12-09T16:20:44Z"
"","7599","KAFKA-9105: Add back truncateHead method to ProducerStateManager","The truncateHead method was removed from ProducerStateManager by github.com/apache/kafka/commit/c49775b. This meant that snapshots were no longer removed when the log start offset increased, even though the intent of that change was to remove snapshots but preserve the in-memory mapping. This patch adds the required functionality back.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-10-25T17:38:59Z","2019-10-25T21:44:30Z"
"","8124","[KAFKA-9563] Fix Kafka connect consumer and producer override docs","The true parameters for overriding producer config or consumer config in **Kafka-Connect** are   `producer.override.[PRODUCER-CONFIG] ` or `consumer.override.[CONSUMER-CONFIG]`   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","blcksrx","2020-02-15T13:29:12Z","2020-03-23T03:07:22Z"
"","8188","MINOR: Throttle consumer timeout increase","The test_throttled_reassignment test fails because the consumer that is used to validate reassignment does not start on time to consume all messages. This does not seem like an issue with the throttling of the reassignment, since increasing the timeout allowed the test to pass multiple consecutive runs locally.  This test seemed to rely on the default JmxTool for the console consumer that was removed in this commit: 179d0d7 The console consumer would check to see if it had partitions assigned to it before beginning to consume. Although the test occasionally failed with the JmxTool, it began to fail much more after the removal.  Error messages of failures followed the below format with varying numbers of missed messages. They are the first messages by the producer.  535 acked message did not make it to the Consumer. They are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19...plus 515 more. Total Acked: 192792, Total Consumed: 192259. We validated that the first 535 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer. In the scope of the test, this error suggests that the test is falling into the race condition described in produce_consume_validate.py, which has the timeout to prevent the consumer from missing initial messages.  This can serve as a temporary fix until the logic of consumer startup is addressed further.","closed","","mattwong949","2020-02-27T22:30:27Z","2020-02-27T22:50:41Z"
"","8302","MINOR: Fix kafka.server.RequestQuotaTest missing recently added ApiKeys.","The test was broken by commit 227a7322b77840e08924b9486e4bda2f3dfc1f1a.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-03-15T19:06:02Z","2020-03-16T23:21:45Z"
"","7754","MINOR: Adjust `testClientDisconnectionUpdatesRequestMetrics` to also test small response case","The test also passes with small responses now in both macOS Catalina and Linux. Remove outdated comment and exercise both paths.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-27T07:49:36Z","2019-12-16T22:52:35Z"
"","8242","KAFKA-9674: corruption should also cleanup producer and recreate","The task producer cleanup doesn't involve handling of task corruption. Adding recreation of task producer to avoid reusing a fatal state producer in next cycle.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2020-03-06T18:46:29Z","2020-03-07T18:37:16Z"
"","8431","MINOR: Rename description of flatMapValues transformation","The table of (stateless) transformations uses the transformation name in the first column and a description in the second column. I adjusted the transformation name for FlatMapValues accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","docs,","maseiler","2020-04-06T13:38:04Z","2020-06-30T20:44:52Z"
"","8361","HOTFIX: faile to compile HighwatermarkCheckpointBench by scala 2.13","The source compatibility offered by ```scala-collection-compat``` is based on scala implicit conversions. It means the ""explicit"" reference in java code ```HighwatermarkCheckpointBench``` can't join the party (```scala.jdk.CollectionConverters``` is linked to scala 2.13 code and the symbol used by ```HighwatermarkCheckpointBench``` is gone)  It seems to me we should have a collection of helper methods to offer the conversion like ```scala-collection-compat``` and it servers for jmh java code only.  This PR replaces ```scala.jdk.CollectionConverters``` by ```scala.collection.JavaConverters``` since ```scala.collection.JavaConverters``` methods required by ```HighwatermarkCheckpointBench``` exists on both scala 2.12 and scala 2.13.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-26T08:52:47Z","2020-03-26T11:27:59Z"
"","8170","HOTFIX: compile error in EmbeddedKafkaCluster","the signature of EmbeddedKafkaCluster#getAllTopicsInCluster was changed by #8062 but we did not change the usage introduced by #8029 before merging #8062  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-26T04:18:23Z","2020-02-27T07:10:31Z"
"","8471","KAFKA-9854 Re-authenticating causes mismatched parse of response","the schema of LIST_OFFSETS consists of   1. throttle_time_ms:INT32 and  1. responses:ARRAY  If throttle_time is zero and size of responses is small enough, its binary is compatible to schema of SASL_HANDSHAKE which is composed of   1. error_code:INT16 and  1. mechanisms:ARRAY(STRING)  Hence, there is no Schema error when SASL_HANDSHAKE tries to parse resoponse of LIST_OFFSETS but the check of correction id still throw IllegalStateException due to mismatched parse. The IllegalStateException is NOT caught and it is not sent back to Selector so the cascading error happens that all following responses are parsed by incorrect Schema.  https://issues.apache.org/jira/browse/KAFKA-9854  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-04-12T16:53:00Z","2020-04-16T12:26:31Z"
"","8142","KAFKA-9577: SaslClientAuthenticator incorrectly negotiates SASL_HANDSHAKE version","The SaslClientAuthenticator incorrectly negotiates supported SaslHandshakeRequest version and  uses the maximum version supported by the broker whether or not the client supports it.   This bug was exposed by a recent version bump in https://github.com/apache/kafka/commit/0a2569e2b9907a1217dd50ccbc320f8ad0b42fd0.  This PR rolls back the recent SaslHandshake[Request,Response] bump, fixes the version negotiation, and adds a test to prevent anyone from accidentally bumping the version without a workaround such as a new ApiKey. The existing key will be difficult to support for clients < 2.5 due to the incorrect negotiation.  Tests: - Prevent SASL_HANDSHAKE schema version bump - Add test to return ApiVersions unsupported by client","closed","","lbradstreet","2020-02-20T19:13:23Z","2020-02-22T05:49:12Z"
"","8373","KAFKA-9775: Fix IllegalFormatConversionException in ToolsUtils","The runtime type of Metric.metricValue() needn't always be a Double, for example, if it's a gauge from IntGaugeSuite. Since it's impossible to format non-double values with 3 point precision IllegalFormatConversionException resulted.","closed","","tombentley","2020-03-27T11:24:45Z","2020-04-03T17:05:14Z"
"","8256","KAFKA-9675: Fix bug that prevents RocksDB metrics to be updated","The root cause of the bug is that the statistics object is passed to the RocksDB options after the database is opened. Apparently, the options object is copied during the opening process. The solution is to pass the statistics object before the database is opened.  Additionally, I added some unit tests to check if the Kafka Streams' metrics are updated when the measurements in RocksDB's statistics object change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-03-09T13:18:42Z","2020-04-15T20:52:36Z"
"","8032","KAFKA-8245 Flaky Test DeleteConsumerGroupsTest#testDeleteCmdAllGroups","the root cause is the “shutdown” may obstruct consumer to complete the group join. If the consumer wakes up before completing the partition assignments, the consumer is in a state of UNJOINED (so it won’t send LeaveGroupRequest when closing) and the broker is in a state of CompletingRebalance until timeout.   To fix the flaky, all groups should be in Stable state to make sure all Consumers have completed the group joining (so they are able to send LeaveGroupRequest when closing).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2020-02-01T17:42:59Z","2020-02-17T20:44:43Z"
"","8202","Minor: stabilize flaky ReassignPartitionsClusterTest#znodeReassignmen…","The result of first alter can make the ""waiting"" complete so we should not only check the ""empty"" but also the expected broker id.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-02T15:31:58Z","2020-06-19T06:40:37Z"
"","7822","MINOR: Fix throttle usage in reassignment test case","The replication throttle in `testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress` was not setting the quota on the partition correctly, so the test case was not working as expected. This patch fixes the problem and also fixes a bug in `waitForTopicCreated` which caused the function to always wait for the full timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-11T23:41:11Z","2019-12-17T16:20:26Z"
"","8145","KAFKA-9581: Remove rebalance exception withholding","The rebalance exception withholding is no longer necessary as we have better mechanism for catching and wrapping these exceptions. Throw them directly should be fine and simplify our current error handling.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-21T00:58:30Z","2020-02-24T16:50:46Z"
"","7562","KAFKA-9089; Reassignment should be resilient to unexpected errors","The purpose of this patch is to make the reassignment algorithm simpler and more resilient to unexpected errors. Specifically, it has the following improvements:  1. Remove `ReassignedPartitionContext`. We no longer need to track the previous reassignment through the context and we now use the assignment state as the single source of truth for the target replicas in a reassignment. 2. Remove the intermediate assignment state when overriding a previous reassignment. Instead, an overriding reassignment directly updates the assignment state and shuts down any unneeded replicas. Reassignments are _always_ persisted in Zookeeper before being updated in the controller context. 3. To fix race conditions with concurrent submissions, reassignment completion for a partition always checks for a zk partition reassignment to be removed. This means the controller no longer needs to track the source of the reassignment. 4. Api reassignments explicitly remove reassignment state from zk prior to beginning the new reassignment. This fixes an inconsistency in precedence. Upon controller failover, zookeeper reassignments always take precedence over any active reassignment. So if we do not have the logic to remove the zk reassignment when an api reassignment is triggered, then we can revert to the older zk reassignment.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-19T18:28:38Z","2019-10-25T02:33:17Z"
"","7505","KAFKA-7204: Avoid clearing records for paused partitions on poll of MockConsumer","The previous version of MockConsumer does not allow the clients to test consecutive calls to poll while consuming only from a partial set of partitions due to the fact that it clears all the records after each call. This change makes MockConsumer clearing the records only for the partitions that are not paused (whose records are actually returned by the poll). The remaining paused partitions will retain the records.  Unit test added accordingly.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","consumer,","efgpinto","2019-10-12T11:48:47Z","2020-01-21T19:46:14Z"
"","8474","KAFKA-9853: Improve performance of Log.fetchOffsetByTimestamp","The previous code did not use the collection produced by `takeWhile()`.  It only used the length of that collection to select the next element. Perhaps some people would consider this a MINOR change.  I created the JIRA ticket because this is part of the core library & probably warrants some discussion.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","ebolinger","2020-04-13T02:42:05Z","2020-04-14T13:50:14Z"
"","7582","KAFKA-9038: Allow creating partitions while partition reassignment is in progress","The PR check reassignment in progress at topic level and remove zk node assignment check, note that the check cannot be removed completely to prevent further exposure to race condition between in progress reassignments and the createPartitions update.","closed","","rite2nikhil","2019-10-23T05:53:30Z","2019-10-25T06:40:03Z"
"","8476","KAFKA-9838; Add log concurrency test and fix minor race condition","The patch adds a new test case for validating concurrent read/write behavior in the `Log` implementation. In the process of verifying this, we found a race condition in `read`. The previous logic checks whether the start offset is equal to the end offset before collecting the high watermark. It is possible that the log is truncated in between these two conditions which could cause the high watermark to be equal to the log end offset. When this happens, `LogSegment.read` fails because it is unable to find the starting position to read from.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-14T00:09:56Z","2020-04-16T00:18:31Z"
"","8137","KAFKA-8967 Flaky test kafka.api.SaslSslAdminIntegrationTest.testCreat…","The other brokers sync ACLs from zk notification so the sync may be slower than the Assert. The fix is to wait all brokers to sync the ACLs.   https://issues.apache.org/jira/browse/KAFKA-8967  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2020-02-19T13:53:06Z","2020-07-25T16:18:45Z"
"","8441","KAFKA-9748: Extend Streams integration tests for EOS beta","The original PR was only considering `EosIntegrationTest`. However, there are actually a few more integration test with eos-alpha enabled that we missed to update.  Call for review @abbccdda @guozhangwang","closed","kip,","mjsax","2020-04-07T22:33:25Z","2020-06-12T23:10:54Z"
"","7645","HOTFIX: remove reference to unused Assignment error code","The original draft of KIP-429 made extensive use of an error code to communicate rebalance updates to all members, however this was removed after some discussion. We should take it out of the log messages as well to avoid confusion  Should be cherry-picked to 2.4","closed","","ableegoldman","2019-11-04T23:26:38Z","2019-11-08T03:17:26Z"
"","7796","MINOR: alter annotations to be more reasonable","the original ""should only have one"" means ""have more than one"" which didn't include ""don't have"", it's better to change to ""should have and only have one"".","closed","","dengziming","2019-12-07T03:23:27Z","2020-01-03T02:06:11Z"
"","7884","[KAFKA-8522] Streamline tombstone and transaction marker removal","The objective of this PR is to prevent tombstones from persisting in logs under low throughput conditions.","closed","","ConcurrencyPractitioner","2020-01-02T04:36:01Z","2021-01-15T22:06:21Z"
"","7539","KAFKA-6968: Adds calls to listener on rebalance of MockConsumer","The MockConsumer has a rebalance operation which receives a new set of partitions to simulate the rebalance of a consumer group. However, the current version is not calling the listener provided on subscription to notify the client when partitions are revoked and assigned.  This MR calls both listeners on rebalance and adds a unit test to cover that.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","efgpinto","2019-10-16T23:54:38Z","2021-05-28T12:56:49Z"
"","7540","Eagerly unmap during resize","The mmap field gets re-initialized here, so the old object will become garbage-collectible. We should be eagerly unmapping here too, not just for Windows support.","open","","shikhar","2019-10-17T02:44:59Z","2021-07-17T21:00:55Z"
"","7924","MINOR: Remove updatePartitionReplicaAssignment from ControllerContext","The method updatePartitionReplicaAssignment allows the user to direcly modify the replicas assigned to a partition without also modifying the addingReplicas and removingReplicas fields. This is not a safe operation for all inputs.  Clients should instead use updatePartitionFullReplicaAssignment.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-01-09T23:21:40Z","2020-01-13T17:51:30Z"
"","7491","KAFKA-9014: Fix AssertionError when SourceTask.poll returns an empty list","The method contract for SourceTask#poll mentions that in the absence of records the method should return null. But the task or the metrics reporter should also not fail if a connector chooses to return an empty list of records.   Tested with unit tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-10-10T20:45:48Z","2020-10-16T06:17:32Z"
"","8093","MINOR: Reduce log level to Trace for fetch offset downgrade","The log for fallback isn't very helpful for debugging purpose, reducing to a trace log. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-12T01:11:17Z","2020-02-19T00:01:20Z"
"","7867","MINOR: Propagate LogContext to channel builders and SASL authenticator","The log context is useful when debugging applications which have multiple clients. This patch propagates the context to the channel builders and the SASL authenticator.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-24T01:36:09Z","2020-01-06T22:27:31Z"
"","7705","MINOR: Use IntegrationTestHarness properly in BaseAdminIntegrationTest","The latter was previously hardcoding logDirCount instead of using the method defined in the superclass since it was unnecessarily duplicating logic.  Also tweak IntegrationTestHarness and remove unnecessary method override from SaslPlainPlaintextConsumerTest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-18T11:48:16Z","2019-11-20T15:19:26Z"
"","8424","HOTFIX: fix StreamsConfig build failure","The last merged code caused some unexpected failure: https://github.com/apache/kafka/commit/ab5e4f52ecb072df55c7f5cd8941122a135cdf79  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2020-04-04T03:56:36Z","2020-04-04T18:12:34Z"
"","8097","MINOR: Do not configure infinite retries for idempotent producers","The KafkaProducer code would set infinite retries (MAX_INT) if the producer was configured with idempotence and no retries were configured by the user. This is superfluous because KIP-91 changed the retry functionality to both be time-based and the default retries config to be MAX_INT.","closed","","stanislavkozlovski","2020-02-12T17:30:07Z","2020-02-13T07:42:03Z"
"","7851","KAFKA-9315: The Kafka Metrics class should clear the mbeans map when closing","The JmxReporter should clear the mbeans map when closing. Otherwise, metrics may be incorrectly re-registered if the JmxReporter class is used after it is closed.  For example, calling JmxReporter#close followed by JmxReporter#unregister could result in some of the mbeans that were removed in the close operation being re-registered.","closed","","cmccabe","2019-12-18T20:34:34Z","2020-01-28T14:39:12Z"
"","8196","KAFKA-9478: Remove zombie partition reassignments from ZooKeeper","The JIRA ticket explains the issue with reassignment commands concurrently issued using `/admin/reassign_partitions` ZooKeeper node.  This commit fixes this issue by deleting from `/admin/reassign_partitions` partition reassignments that otherwise would 1) not be acted upon any way; and 2) remain there forever preventing the controller from deleting the Zk node and restoring the watch on it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ivanyu","2020-02-29T12:38:32Z","2020-06-29T05:19:31Z"
"","7668","MINOR: clarify statefulness in transform/process variations","The javadocs for all `#transform` and `#process` variations claim that these are stateful operations. This has been the source of some confusion as this is not necessarily true, unless of course you explicitly attach a state store.  The assumption in the docs is that for any stateless processing of this kind, the corresponding `#map` or `#forEach` variation would be used. But these do not provide access to metadata so there is in fact a slight distinction between the two family of operators besides statefulness, and a reason to use transformers or processors besides needing state.","closed","streams,","ableegoldman","2019-11-08T19:18:41Z","2019-11-10T06:43:43Z"
"","7605","MINOR: clarify KafkaStreams.close javadoc","The javadoc on the `KafkaStreams#close(Duration)` overload incorrectly stated that a zero-duration timeout would wait forever. In fact, it will not wait at all and makes the execution asynchronous.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-28T22:10:37Z","2019-11-10T07:11:40Z"
"","8313","KAFKA-5604: Remove the redundant TODO marker on the Streams side","The issue itself has been fixed a while ago on the producer side, so we can just remove this TODO marker now (we've removed the isZombie flag already anyways).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-18T19:19:18Z","2020-03-18T21:23:04Z"
"","8013","Kip 113 docs updatev1","The is a docs update only.  It updates the docs to reflect KIP-113 information about specifying a log_dir when reassigning partitions.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ x] Verify documentation (including upgrade notes)","closed","","mitchell-h","2020-01-28T19:44:13Z","2020-01-28T19:44:31Z"
"","8098","KAFKA-8073 Transient failure in kafka.api.UserQuotaTest.testThrottled…","the input data in testThrottledProducerConsumer are 1000 messages and each message carries only a string bytes which is converted from int value. The quota is 8000 bytes/second so it fails to start the throttle if the machine is too slow to complete all request. It is a timer-based test case. The workaround is to increase the size of input data.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-12T18:36:18Z","2020-06-19T06:40:00Z"
"","8468","MINOR: reduce impact of trace logging in replica hot path","The impact of trace logging is normally small, on the order of 40ns per getEffectiveLevel check, however this adds up with trace is called multiple times per partition in the replica fetch hot path.  This PR removes some trace logs that are not very useful and reduces cases where the level is checked over and over for one fetch request.  Back of envelope savings calculation ----- Assume 1500 leader replicas on a broker, 8 followers fetching 200 times per second.  Partitions per fetch request = 1500 * 2 (replication) / 8 (followers) = 375 Partition reads per second = 375 * 8 * 200 = 6000000 CPU time in ms used per second for single trace call saved = 40 * 6000000 / 1000000 = 24ms  Assuming 4 cores each trace call we save saves approximately 24/1000/4 = 0.6%.","closed","","lbradstreet","2020-04-12T01:10:28Z","2020-04-17T21:22:32Z"
"","7686","KAFKA-9178: restoredPartitions is not cleared until the last restoring task completes","The ideal fix for this PR is blocked by https://issues.apache.org/jira/browse/KAFKA-9177, since any partitions we remove from `restoredPartitions` will just be added back to the set on the next loop unless we remove/pause completed partitions in the restore consumer  For now, we can just remove from `restoredPartitions` whenever we suspend or close a running task.  This PR is for trunk only -- the fix for 2.4 is being included in https://github.com/apache/kafka/pull/7691","closed","streams,","ableegoldman","2019-11-13T03:35:10Z","2019-11-19T21:51:01Z"
"","7611","MINOR: Fix documentation for updateCurrentReassignment","The function KafkaController.updateCurrentReassignment doesn't return any value. Fix the documentation to reflect that.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-10-29T21:16:51Z","2019-10-30T16:00:26Z"
"","8064","MINOR. Fix scala 2.11 compilation error","The fix was cherry-picked from trunk where scala version is 2.12. So it failed in 2.4. Couple of fixes:  1. Either doesn't have toOption method, so used match directly 2. SAM can't be used, fixed that to mention interface  Compilation with `-PscalaVersion=2.11` passes.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-02-07T23:06:45Z","2020-02-08T03:08:01Z"
"","8351","KAFKA-9763: Correct InsertField to not skip keys of tombstone records","The fix for [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) attempted to skip (immediately returns) tombstone records when InsertField$Value is used. However, it also inadvertently also skips tombstone records when InsertField$Key is used, despite the possibility that the tombstone record’s key is non null.  This commit corrects the behavior so that the InsertField$Value continues to skip tombstone records, but InsertField$Key only skips when the record’s *key* is null.  Added two unit tests, and verified that these tests fail without the proposed fix and pass with the proposed fix. The other unit tests were added with [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) continue to pass.  This should be backported all the way back to the 1.0 branch, since that's how far back [KAFKA-8523](https://issues.apache.org/jira/browse/KAFKA-8523) was backported.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2020-03-25T19:12:17Z","2020-03-25T19:31:49Z"
"","7832","Add jmx exporter to kafka start up script","The file location is to be changed by user.  My file structure is:  ``` - lekafgo-hackday/    - jmx-exporter.yaml    - jmx_prometheus_javaagent-0.12.0.jar    - kafka/ ```","closed","","yifeili3","2019-12-13T20:13:31Z","2019-12-13T20:17:33Z"
"","8423","KAFKA-9818: improve error message to debug test","The error message on the ticket is not helpful. To figure out why the test fails, we need to improve the error message first and wait until it fails again.  \cc @ableegoldman @vvcephei","closed","tests,","mjsax","2020-04-04T02:03:13Z","2020-04-07T04:22:38Z"
"","7614","MINOR: fix docs around leader rebalancing","The documentation around leader rebalancing haven't been updated since Kafka 0.8.3, and are now incorrect.  This change updates the documentation to reflect the fact that the auto.leader.rebalance.enable default is true, as well as improving the grammar.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","wheresalice","2019-10-30T08:32:46Z","2019-11-10T18:17:41Z"
"","7597","MINOR: improve logging of tasks on shutdown","The current logging during a close/shutdown claims to list all active tasks but in fact lists only the running and suspended ones. Likewise it lists only the running standby tasks.  Also changed the name of `AssignedTasks#close` to `shutdown` as that more accurately reflects its current (and only) usage","closed","streams,","ableegoldman","2019-10-25T02:37:39Z","2019-10-29T17:29:34Z"
"","8052","MINOR: Improve EOS example exception handling","The current EOS example is over-complicating the exception handling by mixing non fatal and fatal ones. This cleanup is trying to make the code readability better. Will rebase after #8051 is merged  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-06T18:58:12Z","2020-02-21T19:06:16Z"
"","8112","KAFKA-9557: correct thread process-rate sensor to measure throughput","The current definition of the thread-level process-rate/total metric seems not correct by any reasonable standard.  It claims to measure the number/rate of ""calls to process"", but doesn't define what a ""call to process"" is. I think a reasonable definition would be a ""call to process a record"", which in Streams is equal to a call to `task.process()`. This is exactly what the _task-level_ process-rate/total metric does.  However, the thread-level metric measures the number/rate of calls to `taskManager.process()`, in which the method processes _at least one_ record. This doesn't seem like a useful quantity for operators to measure, and it certainly doesn't seem to honor the spirit of the metric.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-02-13T22:10:46Z","2020-02-14T21:54:46Z"
"","7829","KAFKA-9297; CreateTopic API do not work with older version of the request/response","The create topic api do not work with older version of the api. It can be reproduced by trying to create a topic with `kafka-topics.sh` from 2.3. It timeouts.  https://github.com/apache/kafka/commit/b94c7f479b917d4ec602c31a24f11390627c479b has added a check which raises an exception if a field has been set to a non-default value unless the field is marked as ""ignorable"".  The fields added in the version 5 of the response are always set regardless of the version used by the client. If an older version is used, an exception is thrown during the serialization because the fields have non-default values. We should either not set the fields for older versions in the api layer or mark them as ignorable. I have chosen the later in this case because it looks cleaner.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-12-13T09:17:00Z","2019-12-16T18:34:57Z"
"","7653","KAFKA-9079: Fix reset logic in transactional message copier","The consumer's `committed` API does not return an entry in the response map for a requested partition if there is no committed offset. The transactional message copier, which is used in the transaction system test, did not account for this. If the first transaction attempted by the copier was randomly aborted, then we would not seek to the beginning as expected, which means we would fail to copy some of the records.  This patch fixes the problem by iterating over the assignment rather than the result of `committed` when resetting offsets. It also adds enables additional logging in the transaction message copier service to make finding problems easier in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-06T06:57:48Z","2019-11-06T19:06:48Z"
"","7964","MINOR: change the log level from ERROR to DEBUG when failing to get plugin class loader","The connectors located at classpath are loaded by AppClassLoader so Worker fails to find the PluginClassLoader for them. It should be normal case but the log level is ERROR.  there is similar case in DelegatingClassLoader#loadClass, and the log level is TRACE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2020-01-15T04:41:14Z","2020-08-03T17:39:59Z"
"","7866","KAFKA-9330: Skip `join` when `AdminClient.close` is called in callback thread","The close method calls `Thread.join` to wait for AdminClient thread to die, but if the close is called in the api completion callback, `join` waits forever, as the thread calling `join` is same as the thread it wants to wait to die.  This change checks for this condition and skips the join. The thread will then return to main loop, where it will check for this condition and exit.  Added a new unit test to invoke this condition. The test fails with timeout if the thread is joined in callback, passes otherwise.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-12-23T23:55:05Z","2020-01-07T19:34:24Z"
"","7770","KAFKA-9261; Client should handle unavailable leader metadata","The client caches metadata fetched from Metadata requests. Previously, each metadata response overwrote all of the metadata from the previous one, so we could rely on the expectation that the broker only returned the leaderId for a partition if it had connection information available. This behavior changed with KIP-320 since having the leader epoch allows the client to filter out partition metadata which is known to be stale. However, because of this, we can no longer rely on the request-level guarantee of leader availability. There is no mechanism similar to the leader epoch to track the staleness of broker metadata, so we still overwrite all of the broker metadata from each response, which means that the partition metadata can get out of sync with the broker metadata in the client's cache. Hence it is no longer safe to validate inside the `Cluster` constructor that each leader has an associated `Node`  Fixing this issue was unfortunately not straightforward because the cache was built to maintain references to broker metadata through the `Node` object at the partition level. In order to keep the state consistent, each `Node` reference would need to be updated based on the new broker metadata. Instead of doing that, this patch changes the cache so that it is structured more closely with the Metadata response schema. Broker node information is maintained at the top level in a single collection and cached partition metadata only references the id of the broker. To accommodate this, we have removed `PartitionInfoAndEpoch` and we have altered `MetadataResponse.PartitionMetadata` to eliminate its `Node` references.  Note that one of the side benefits of the refactor here is that we virtually eliminate one of the hotspots in Metadata request handling in `MetadataCache.getEndpoints` (which was renamed to `maybeFilterAliveReplicas`). The only reason this was expensive was because we had to build a new collection for the `Node` representations of each of the replica lists. This information was doomed to just get discarded on serialization, so the whole effort was wasteful. Now, we work with the lower level id lists and no copy of the replicas is needed (at least for all versions other than 0).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-03T19:41:53Z","2020-02-05T17:13:12Z"
"","7942","KAFKA-9409: Supplement immutability of ClusterConfigState class in Connect","The class claims to be immutable, but there are some mutable features of this class. Increase the immutability of it and add a little cleanup:  * Pre-initialize size of ArrayList * Remove superfluous syntax * Use ArrayList instead of LinkedList since the list is created once  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","belugabehr","2020-01-12T17:05:12Z","2020-05-21T03:05:29Z"
"","7856","KAFKA-9318: added SMT to extract map values","The change adds a SMT to extract map entries.  The SMT is counterpart to the existing ExtractField SMT.  The SMT is isolated from other components and simple unit tests seem to be sufficient.","open","kip,","piotrsmolinski","2019-12-19T13:31:48Z","2020-06-11T01:39:15Z"
"","7817","KAFKA-9291:revert the code of KAFKA-9288 which bring fatal bug","the bug details can be seen at https://issues.apache.org/jira/browse/KAFKA-9291","closed","","dengziming","2019-12-11T12:22:11Z","2019-12-12T02:13:31Z"
"","7816","Kafka-9291: remove the code of KAFKA-9288 which bring fatal bug","the bug details can be seen at https://issues.apache.org/jira/browse/KAFKA-9291","closed","","dengziming","2019-12-11T11:40:25Z","2019-12-11T12:22:54Z"
"","7808","MINOR: add paragraph about max parallelism","The AK Streams architecture docs should explain how the maximum parallelism is determined","closed","docs,","ableegoldman","2019-12-09T21:26:17Z","2019-12-16T17:24:34Z"
"","8185","MINOR: Throttle consumer timeout increase","The `test_throttled_reassignment` test fails because the consumer that is used to validate reassignment does not start on time to consume all messages. This does not seem like an issue with the throttling of the reassignment, since increasing the timeout allowed the test to pass multiple consecutive runs locally.  This test seemed to rely on the default JmxTool for the console consumer that was removed in this commit: https://github.com/apache/kafka/commit/179d0d73d65ab2c3eb8bc79c70b9893f07038447#diff-70247099943a55f682e6e6e4f400334eL69 The console consumer would check to see if it had partitions assigned to it before beginning to consume. Although the test occasionally failed with the JmxTool, it began to fail much more after the removal.  Error messages of failures followed the below format with varying numbers of missed messages. They are the first messages by the producer. ``` 535 acked message did not make it to the Consumer. They are: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19...plus 515 more. Total Acked: 192792, Total Consumed: 192259. We validated that the first 535 of these missing messages correctly made it into Kafka's data files. This suggests they were lost on their way to the consumer. ``` In the scope of the test, this error suggests that the test is falling into the race condition described in `produce_consume_validate.py`, which has the timeout to prevent the consumer from missing initial messages.  This can serve as a temporary fix until the logic of consumer startup is addressed further.","closed","","mattwong949","2020-02-27T17:54:42Z","2020-02-27T22:36:42Z"
"","7722","MINOR: Updated StreamTableJoinIntegrationTest to use TTD","The `StreamTableJoinIntegrationTest` is a known flaky test. This PR converts the `StreamTableJoinIntegrationTest` to use the `ToplogyTestDriver` which will eliminate the flakiness.  Also, note that the `StreamTableJoinIntegrationTest` extends the `AbstractJoinIntegrationTest` class.  There are two other tests (`StreamStreamJoinIntegrationTest` and `TableTableJoinIntegrationTest` that also extend `AbstractJoinIntegrationTest`.  These tests will also get coverted to use the TTD in a follow-up PR soon.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-11-20T18:36:06Z","2019-11-25T16:17:36Z"
"","8329","KAFKA-9742: Fix StandbyTaskEOSIntegrationTest End offset","The `StandbyTaskEOSIntegrationTest` was broken due to the incorrect offset setting in the checkpoint file enforced by https://github.com/apache/kafka/commit/6cf27c9c771900baf43cc47f9b010dbf7a86fa22. The fix is to set the offset to a legitimate value even if the topic doesn't exist.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","abbccdda","2020-03-22T22:53:46Z","2020-03-23T20:55:14Z"
"","7865","KAFKA-9329. KafkaController::replicasAreValid should return error","The `KafkaController::replicasAreValid` method currently returns a boolean indicating if replicas are valid or not. But the failure condition loses any context on why replicas are not valid. This change updates the metod to return the error conition if validation fails. This allows caller to report the error to the client.  The change also renames the `replicasAreValid` method to `validateReplicas` to reflect updated semantics.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-12-23T21:36:39Z","2020-01-17T22:34:01Z"
"","7673","MINOR. Update Epoch field descrition in FetchRequest api doc","The `Epoch` field description was copy of the `SessionId` field. This change updates it to describe `Epoch` instead.  No code change, only description changes. Code compiles.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-11-09T01:58:26Z","2019-11-09T06:38:08Z"
"","7950","KAFKA-9419: Integer Overflow Possible with CircularIterator","The `CircularIterator` class uses a wrapping index-based approach to iterate over a list.  This can be a performance problem O(n^2) for a LinkedList.  Also, the index counter itself is never reset, a modulo is applied to it for every list access.  At some point, it may be possible that the index counter overflows to a negative value and therefore may cause a negative index read and an ArrayIndexOutOfBoundsException.  I propose changing this implementation to avoid these two scenarios.  Use the `Collection` `Iterator` classes to avoid using an index counter and it avoids having to seek to the correct index every time, this avoiding the LinkedList performance issue.  I have added unit tests to validate the new implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","belugabehr","2020-01-13T21:16:38Z","2020-05-06T23:02:27Z"
"","8026","Bump trunk to 2.6.0-SNAPSHOT","The 2.5 branch has been created, so we need to bump trunk to the next snapshot version, 2.6.0-SNAPSHOT","closed","","mumrah","2020-01-31T22:02:17Z","2020-02-03T18:04:57Z"
"","7743","MINOR: Fix --enable-autocommit flag in verifiable consumer","The --enable-autocommit argument is a flag. It does not take a parameter. This was broken in #7724.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-23T18:37:10Z","2019-11-23T22:18:43Z"
"","8159","KAFKA-9599 create unique sensor to record group rebalance","the ""offset deletion"" and ""group rebalance"" should not be recorded by the same sensor since they are totally different.  the code is introduced by #7276  jira link: https://issues.apache.org/jira/browse/KAFKA-9599  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-24T07:08:12Z","2020-02-24T19:39:15Z"
"","7955","KAFKA-9423: Refine layout of configuration options on website and make individual settings directly linkable","Test strategy: Built the site and served from an Apache httpd docker container. Used w3c validator to ensure generated HTML is valid.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2020-01-14T12:10:14Z","2020-02-12T17:46:37Z"
"","7989","KAFKA-9457; Fix flaky test org.apache.kafka.common.network.SelectorTest.testGracefulClose","Test currently assumes that exactly one receive is completed within 1 second. We cannot guarantee that. Updated to increase timeout to 5 seconds and allow one or more pending receives to complete.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-21T11:02:44Z","2020-01-21T15:40:34Z"
"","7917","KAFKA-9379; Fix flaky test TopicCommandWithAdminClientTest.testCreateAlterTopicWithRackAware","Test creates a topic, waits for `adminClient.listTopics` to return the topic and then performs `alterTopic()`, which also uses `adminClient.listTopics` to verify that the topic exists. This may fail if the topic hadn't yet been propagated to the broker to which the second `listTopics` is sent. This PR updates the test to wait for topic to be in the metadata cache of all brokers before calling `alterTopic()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-09T11:27:07Z","2020-01-09T14:17:42Z"
"","8113","HOTFIX: Fix breakage in `ConsumerPerformanceTest`","Test cases in `ConsumerPerformanceTest` were failing and causing a system exit rather than throwing the expected exception following #8023. We didn't catch this because the tests were marked as skipped and not failed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-02-13T22:13:49Z","2020-02-14T02:20:16Z"
"","7789","MINOR: Increase the timeout in one of Connect's distributed system tests","System test fails sometimes because Connect restarts around 30 seconds, so increasing timeout to 70 seconds (similar to other tests in same class).  Can be backported to `2.4`, `2.3`; see #7790 for similar change for `2.2` and `2.1`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-12-06T16:20:28Z","2019-12-06T21:33:44Z"
"","7790","MINOR: Increase the timeout in one of Connect's distributed system tests","System test fails sometimes because Connect restarts around 30 seconds, so increasing timeout to 70 seconds (similar to other tests in same class).  Can be backported to `2.1`; see #7789 for similar fix on `trunk`, `2.4`, and `2.3`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-12-06T16:23:14Z","2019-12-06T21:33:06Z"
"","7744","MINOR: Remove logic conditional on exception messages from LogValidator","Such logic is very brittle. Take the chance to simplify the code a bit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-24T02:05:23Z","2019-12-04T03:27:04Z"
"","7495","KAFKA-9020: Streams sub-topologies should be sorted by sink -> source relationship","Subtopologies are currently ordered alphabetically by source node, which prior to KIP-307 happened to always result in the ""correct"" (ie topological) order. Now that users may name their nodes anything they want, we must explicitly order them so that upstream node groups/subtopologies come first and the downstream ones come after.","closed","streams,","ableegoldman","2019-10-11T02:43:30Z","2020-06-26T22:38:16Z"
"","7788","KAFKA-9278: Remove static exception instance","Static exception instances are showing irrelevant stacktrace which makes troubleshooting difficult.","open","","qinghui-xu","2019-12-06T12:29:27Z","2019-12-06T13:06:03Z"
"","8138","KAFKA-9573: Fix JVM options to run early versions of Kafka on the latest JVMs","Startup scripts for the early version of Kafka contain removed JVM options like `-XX:+PrintGCDateStamps` or `-XX:UseParNewGC`.  When system tests run on JVM that doesn't support these options we should set up environment variables with correct options.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2020-02-19T15:00:06Z","2020-03-25T19:24:56Z"
"","7756","KAFKA-7016: Don't fill in stack traces","Stack traces are useful.  I'm pretty sure this exists in all versions of Kafka, but we should consider cherry-picking it back to at least a few older versions we may still do a bugfix for.","closed","","ableegoldman","2019-11-27T20:43:26Z","2019-12-03T23:46:17Z"
"","7993","MINOR: Add explicit result type in public defs/vals","specifying the type will reduce mistakes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-21T15:28:20Z","2020-01-31T07:10:44Z"
"","7692","MINOR: Add method `hasMetrics()` to class `Sensor`","Sometimes to be backwards compatible regarding metrics the simplest solution is to create an empty sensor. Recording an empty sensor on the hot path may negatively impact performance. With `hasMetrics()` recordings of empty sensors on the hot path can be avoided without being to invasive.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-11-14T12:36:40Z","2019-11-14T16:25:32Z"
"","8433","MINOR: Should cleanup the tasks after dirty close","Some tasks get closed inside HandleAssignment and did not remove from the task manager bookkeep list. The next time they would be re-closed which is illegal state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-04-06T16:43:40Z","2020-04-07T03:44:15Z"
"","8100","MINOR: Small Connect integration test fixes","Small improvements and fixes in Connect's integration tests and utilities.  Follows up on https://github.com/apache/kafka/pull/8055  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2020-02-12T22:34:41Z","2020-02-14T00:33:17Z"
"","7910","KAFKA-9387: Use non JNI LZ4 Hashing for header checksums,","since they are just a few bytes and the Java Version outperforms the native version  see: https://lz4.github.io/lz4-java/1.3.0/xxhash-benchmark/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","CodingFabian","2020-01-08T17:09:51Z","2020-01-31T13:02:48Z"
"","7970","KAFKA-9338; Fetch session should cache request leader epoch","Since the leader epoch was not maintained in the fetch session cache, no validation would be done except for the initial (full) fetch request. This patch adds the leader epoch to the session cache and addresses the testing gaps.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-16T06:58:06Z","2020-01-17T22:36:05Z"
"","8299","KAFKA-9568: enforce rebalance if client endpoint has changed","Since the assignment info includes a map with all member's host info, we can just check the received map to make sure our endpoint is contained. If not, we need to force the group to rebalance and get our updated endpoint info.","closed","","ableegoldman","2020-03-14T00:18:33Z","2020-03-24T18:29:08Z"
"","8379","KAFKA-9780: Deprecate commit records without record metadata","Since KIP-382 (MirrorMaker 2.0) a new method ``commitRecord`` was included in ``SourceTask`` class to be called by the worker adding a new parameter with the record metadata. The old ``commitRecord`` method is called from the new one and it's preserved just for backwards compatibility.  The idea is to deprecate this method so that we could remove it in a future release.  [KIP-586](https://cwiki.apache.org/confluence/display/KAFKA/KIP-586%3A+Deprecate+commit+records+without+record+metadata).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mmolimar","2020-03-27T23:27:42Z","2020-05-21T19:18:10Z"
"","7679","MINOR: Fixing null handilg in ValueAndTimestampSerializer","Since `ValueAndTimestampSerializer` wraps an unknown `Serializer`, the output of that `Serializer` can be `null`. In which case the line ```java .allocate(rawTimestamp.length + rawValue.length) ``` will throw a `NullPointerException`.  This pull request returns `null` instead.  Not sure where to place tests for this, any suggestions would be appreciated.  Thanks","closed","","ncreep","2019-11-11T18:59:23Z","2020-02-05T23:23:32Z"
"","7864","MINOR: Fix assertion in testCreatePermissionMetadataRequestAutoCreate","Since `retry` expects a `by name` parameter, passing a nullary function causes assertion failures to be ignored. I verified this by introducing a failing assertion before/after the fix.  I checked other `retry` usages and they looked correct.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-12-23T20:23:22Z","2020-01-08T11:11:20Z"
"","7509","MINOR: Update authorization primitives in security.html","Since 2.3, there are 5 new APIs: ElectPreferredLeaders, IncrementalAlterConfigs, AlterPartitionReassignments, DescribePartitionReassignments and OffsetDelete  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-14T18:09:59Z","2020-01-06T17:52:55Z"
"","7507","MINOR: Update authorization primitives in security.html","Since 2.3, there are 2 new APIs: ElectPreferredLeaders and IncrementalAlterConfigs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-14T17:27:04Z","2019-10-14T17:31:44Z"
"","7787","Kafka-9277: Move the group state transition rules into their states","Similar to KAFKA-5258 which move all partition and replica state transition rules into their states, we move the group state transition rules into their states.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2019-12-06T07:51:00Z","2019-12-23T23:36:33Z"
"","8003","KAFKA-8843: KIP-515: Zookeeper TLS support","Signed-off-by: Ron Dagostino     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2020-01-24T22:11:38Z","2020-02-10T16:28:24Z"
"","8472","KAFKA-9855 - return cached Structs for Schemas with no fields","Signed-off-by: radai-rosenblatt   at the time of this writing there are 6 schemas in kafka APIs with no fields - 3 versions each of LIST_GROUPS and API_VERSIONS.  when reading instances of these schemas off the wire there's little point in returning a unique Struct object (or a unique values array inside that Struct) since there is no payload.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","radai-rosenblatt","2020-04-12T19:51:09Z","2020-05-21T04:24:01Z"
"","8193","be helpful when throwing ConcurrentModificationException out of consumers","Signed-off-by: Radai Rosenblatt   instead of capturing the threadId of the thread currently accessing the consumer, capture the thread itself. this means any exceptions thrown on concurrent use can be a lot more helpful by pointing out the actual threads involved, instead of sending users to go fish  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","radai-rosenblatt","2020-02-29T01:10:49Z","2020-05-21T04:05:39Z"
"","7572","MINOR: Fix Java 8 dependency from KAFKA-6290 backport","Signed-off-by: Greg Harris   This was inadvertently added in #7371, and breaks the build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2019-10-21T22:01:54Z","2020-10-16T06:15:41Z"
"","8426","Catch ZooKeeper Exceptions and remove non-updated partitions from partitionsToMakeFollower","Signed-off-by: Andrew Choi   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrewchoi5","2020-04-04T19:34:15Z","2020-04-04T19:34:33Z"
"","7654","MINOR: Log both unused key and unused value","Showing both key and value is useful in tracing the incorrect settings.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2019-11-06T07:23:01Z","2020-03-17T08:59:58Z"
"","7759","MINOR: Correct the wrong comment which confuse me.","sessionTimeout mean we mark the coordinator unknown, i dont know what the comment there means.","closed","","maobaolong","2019-11-28T10:36:34Z","2019-12-05T11:47:52Z"
"","7751","KAFKA-7987: Reinitialize ZookeeperClient after auth failures","Schedule a client reinitialization after encountering a Zookeeper auth failure. This allows for reconnections when transient network errors are encountered during connection establishment.  The Zookeeper client doesn't expose details of the auth failure so we can't determine whether an error is retriable or not, so all auth failures are retried.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  Tested in a local docker environment by using iptables to block communication with ZK and Kerberos nodes. With this change, after auth failure due to Kerberos timeout, client continued retrying until communication was reopened and connection successfully re-established.","closed","","parafiend","2019-11-26T23:31:29Z","2020-11-04T19:54:27Z"
"","7545","KAFKA-9053: AssignmentInfo#encode hardcodes the LATEST_SUPPORTED_VERSION","Same as https://github.com/apache/kafka/pull/7537 but targeted at 2.3 for cherry-pick","closed","","ableegoldman","2019-10-17T16:12:29Z","2019-10-18T02:10:22Z"
"","8147","MINOR: don't assign standby tasks with no logged state","Right now the task assignor just blindly assigns N standby tasks per active task (where N = num.standbys) and attempts to distribute them evenly across all instances/threads. But only standby tasks that are stateful, and whose stores are changelog-enabled, will ever actually be created.  This can result in a less-balanced assignment, and should be cleaned up in particular before implementing KIP-441 to remove the noise of ghost standbys","closed","","ableegoldman","2020-02-21T01:49:49Z","2020-06-26T22:37:55Z"
"","8244","KAFKA-8820: kafka-reassign-partitions.sh should support the KIP-455 API","Rewrite ReassignPartitionsCommand to use the KIP-455 API when possible, rather than direct communication with ZooKeeper.  Direct ZK access is still supported, but deprecated, as described in KIP-555.  As specified in KIP-455, the tool has several new flags.  --cancel stops an assignment which is in progress.  --preserve-throttle causes the --verify and --cancel commands to leave the throttles alone. --additional allows users to execute another partition assignment even if there is already one in progress.  Finally, --show displays all of the current partition reassignments.  Reorganize the reassignment code and tests somewhat to rely more on unit testing using the MockAdminClient and less on integration testing.  Each integration test where we bring up a cluster seems to take about 5 seconds, so it's good when we can get similar coverage from unit tests.  To enable this, MockAdminClient now supports incrementalAlterConfigs, alterReplicaLogDirs, describeReplicaLogDirs, and some other APIs.  MockAdminClient is also now thread-safe, to match the real AdminClient implementation.  In DeleteTopicTest, use the KIP-455 API rather than invoking the reassignment command.","closed","","cmccabe","2020-03-06T19:18:06Z","2020-03-20T03:44:35Z"
"","8084","KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState (#7941)","Reviewers: Jason Gustafson , Guozhang Wang  (cherry picked from commit a565d1a182cc69c9994c4512b5e9877e97f06cdf)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-02-11T00:19:53Z","2020-02-11T18:11:32Z"
"","8375","KAFKA-9776: Downgrade TxnCommit API v3 when broker doesn't support","Revert the decision for the `sendOffsetsToTransaction(groupMetadata)` API to fail with old version of brokers for the sake of making the application easier to adapt between versions. This PR silently downgrade the TxnOffsetCommit API when the build version is small than 3.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","abbccdda","2020-03-27T20:10:31Z","2020-04-03T04:48:44Z"
"","8296","MINOR: reuse pseudo-topic in FKJoin","Reuse the same pseudo-topic for serializing the LHS value in the foreign-key join resolver as we originally used to serialize it before sending the subscription request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-03-13T15:13:16Z","2020-03-14T04:04:20Z"
"","7733","KAFKA-9192: fix NPE when for converting optional json schema in structs","resolves the bug https://issues.apache.org/jira/browse/KAFKA-9192  line #701 will throw a `NPE` if `jsonValue` is `null`, if the schema was optional and the field never existed  Signed-off-by: Lev Zemlyanov   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","levzem","2019-11-22T00:07:14Z","2020-10-16T06:15:43Z"
"","8158","HOTFIX: fix NPE in Kafka Streams IQ","Reported on the mailing list. Would be good to get this into 2.5.0, too. If not, also ok. (\cc @arvindth)  Call for review @vvcephei @guozhangwang @brary @vitojeng @vinothchandar @vpapavas","closed","streams,","mjsax","2020-02-22T12:10:14Z","2020-02-23T12:18:21Z"
"","8223","KAFKA-9654 ReplicaAlterLogDirsThread can't be created again if the pr…","ReplicaManager does create ReplicaAlterLogDirsThread only if an new future log is created. If the previous ReplicaAlterLogDirsThread encounters error when moving data, the target partition is moved to ""failedPartitions"" and ReplicaAlterLogDirsThread get idle due to empty partitions. The future log is still existent so we CAN'T either create another ReplicaAlterLogDirsThread to handle the parition or update the paritions of the idler ReplicaAlterLogDirsThread.  ReplicaManager should call ReplicaAlterLogDirsManager#addFetcherForPartitions even if there is already a future log since we can create an new ReplicaAlterLogDirsThread to handle the new partitions or update the partitions of existent ReplicaAlterLogDirsThread to make it busy again.  https://issues.apache.org/jira/browse/KAFKA-9654  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-04T21:11:55Z","2020-03-25T17:51:27Z"
"","7553","MINOR: fix typo in TestInputTopic.getTimestampAndAdvance","Renamed TestInputTopic's`getTimestampAndAdvanced` to `getTimestampAndAdvance`","closed","streams,","ableegoldman","2019-10-18T01:57:55Z","2019-10-22T19:34:13Z"
"","8398","MINOR: Rename TopicCommandTest","Rename the test suite to later add unittests that don't depend on ZK or the AdminClient TopiCommand types.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-03-31T22:31:22Z","2020-04-01T19:45:32Z"
"","7966","KAFKA-8847; Deprecate and remove usage of supporting classes in kafka.security.auth","Removes references to the old scala Acl classes from `kafka.security.auth` (Acl, Operation, ResourceType, Resource etc.) and replaces these with the Java API. Only the old `SimpleAclAuthorizer`, `AuthorizerWrapper` used to wrap legacy authorizer instances and tests using `SimpleAclAuthorizer` continue to use the old API. Deprecated the old scala API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-15T15:37:23Z","2020-01-17T13:20:35Z"
"","7885","MINOR: Remove compilation warnings","Removed warnings:  1. `kafka/core/src/main/scala/kafka/zk/ZkData.scala:278: local val removingReplicasJsonOpt in value $anonfun is never used` 2. `kafka/core/src/main/scala/kafka/server/AdminManager.scala:447: match may not be exhaustive.` 3. `kafka/core/src/test/scala/integration/kafka/api/AbstractConsumerTest.scala:337: private var partitionAssignment in class ConsumerAssignmentPoller is never updated: consider using immutable val` 4. `kafka/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala:263: local var brokerConfigs in method testUpdatesUsingConfigProvider is never updated: consider using immutable val`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","daehokimm","2020-01-02T07:59:53Z","2020-01-03T00:12:38Z"
"","7992","MINOR: Remove unused fields in StreamsMetricsImpl","remove unnecessary code  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-21T14:46:28Z","2020-01-29T05:41:07Z"
"","8107","MINOR: Remove Diamond and code code Alignment","Remove Diamond and code code Alignment  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","highluck","2020-02-13T16:02:44Z","2020-06-26T06:22:59Z"
"","8025","KAFKA-9489 Remove @InterfaceStability.Evolving from AdminClient","Remove `@InterfaceStability.Evolving` from AdminClient and related entities in `clients.admin`  Doc-only change.  https://issues.apache.org/jira/browse/KAFKA-9489  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","raymondng","2020-01-31T19:23:00Z","2020-02-03T22:29:41Z"
"","8274","KAFKA-9699: remove -E option from sed in kafka-run-class.sh","remove -E option from sed in kafka-run-class.sh because old version of sed not support -E more detail please look at  https://issues.apache.org/jira/browse/KAFKA-9699","open","","iamgd67","2020-03-11T04:41:25Z","2020-07-08T01:44:59Z"
"","8081","KAFKA-9523: Migrate BranchedMultiLevelRepartitionConnectedTopologyTest into a unit test","Relying on integration test to catch an algorithm bug introduces more flakiness, reduce the test into a unit test to reduce the flakiness until we upgrade Java/Scala libs.  Checked the test shall fail with older [version](https://github.com/apache/kafka/pull/7904/files) of StreamsPartitionAssignor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-10T20:09:36Z","2020-02-10T21:09:27Z"
"","8330","KAFKA-9742: Fix broken StandbyTaskEOSIntegrationTest","Relax the requirement that tasks' reported offsetSum is less than the endOffsetSum for those tasks. This was surfaced by a test for corrupted tasks, but it can happen with real corrupted tasks. Rather than throw an exception on the leader, we now de-prioritize the corrupted task. Ideally, that instance will not get assigned the task and the stateDirCleaner will make  the problem ""go away"". If it does get assigned the task, then it will detect the corruption and delete the task directory before recovering the entire changelog. Thus, the estimate we provide accurately reflects the amount of lag such a corrupted task would have to recover (the whole log).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","vvcephei","2020-03-22T23:21:49Z","2020-06-12T23:13:02Z"
"","8249","MINOR: Refactor StreamsProducer usage","Related to KIP-447 (extracted from #8218 to keep the other PR smaller):  We want to use `StreamsProducer` instead of `KafkaProducer` within `ActiveTaskCreator`. Furthermore, `StreamsProducer` should expose metrics and close the internal `KafkaProducer`.  The lion's share of the PR is additional code cleanup, mostly related to generics.  Call for review @guozhangwang @abbccdda @vvcephei","closed","kip,","mjsax","2020-03-07T06:22:42Z","2020-06-12T23:19:10Z"
"","8301","HOTFIX: build error caused by DescribeClientQuotasRequest","related to #8083  fix the build error and add some tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-15T14:13:16Z","2020-03-16T15:56:48Z"
"","8421","KAFKA-9800: [KIP-580] Admin Client Exponential Backoff Implementation","Refactored Call Class Split retry() and fail() Hardcoded the retryBackoffMaxMs for now to pass the existing tests  *More detailed description of your change,  *Summary of testing strategy (including rationale)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2020-04-03T21:14:27Z","2020-06-10T20:05:58Z"
"","7693","MINOR: Fix Streams EOS system tests by adding clean-up of state dir","Recently, system tests `test_rebalance_[simple|complex]` failed repeatedly with a verfication error. The cause was most probably the missing clean-up of a state directory of one of the processors.  A node is cleaned up when a service on that node is started and when  a test is torn down.  If the clean-up flag `clean_node_enabled` of a EOS Streams service is  unset, the clean-up of the node is skipped.  The clean-up flag of `processor1` in the EOS tests should stay set before its first start, so that the node is cleaned before the service is started. Afterwards for the multiple restarts of `processor1` the cleans-up flag should be unset to re-use the local state.  After the multiple restarts are done, the clean-up flag of processor1 should again be set to trigger node clean-up during the test teardown.  A dirty node can lead to test failures when tests from Streams EOS tests are scheduled on the same node, because the state store would not start empty since it reads the local state that was not cleaned up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2019-11-15T09:02:11Z","2020-06-01T20:48:50Z"
"","7869","KAFKA-9332 change how to get joinGroupTimeoutMs","rebalanceTimeoutMs + 5000 is always greater than rebalanceTimeoutMs，   joinGroupTimeoutMs = rebalanceTimeoutMs + 5000，May reduce calculations","closed","","sasukerui","2019-12-24T12:09:55Z","2019-12-24T12:14:21Z"
"","8276","KAFKA-9458: Kafka broker crashes in windows platform","Reason: I found that in LogSegment, files are being renamed as per the status flag(deleted, cleaned, swap). In windows renaming or deleting an opened file is not allowed.  Existing behavior (LogSegment Management): While deleting a segment or creating a clean segment, we are trying to change the status of the segment. Segment status is maintained by the file suffix.   Possible solutions: 1. Close the existing files before renaming or deleting 2. Maintain a separate status file to store the status of the segment.  I've implemented the second solution and it's successfully running on my windows environment. Please review the changes and let me know your comments.","open","","hirik","2020-03-11T10:09:19Z","2020-06-05T08:29:23Z"
"","8126","KAFKA-8025: Fix flaky RocksDB test","Reading the stack trace on the Jira, I think the issue might be related to the shared mock object we use.  The test testup is a little bit upside down to allow the usage of reflection. The test verifies that a method is called, by _not_ registering an expected call, and catching the corresponding exception from `verify()` (because we use reflections, we cannot easily register an expected call and let `verify()` throw if the call was not made).  Thus, the expected message from `verify()` is something like: ``` Unexpected method call DBOptions.setEnableThreadTracking(true): ```  When the test fails the stack trace says: ``` Unexpected method call DBOptions.setEnableThreadTracking(true):\n    DBOptions.close(): expected: 1, actual: 0 ```  It indicates that an expected call was registered, however, the test actually never does this. Thus, I suspect that the mocked object is not isolated and as the tests are executed in parallel some race-condition applies (eg, one thread calls a method in the actual test to eventually trigger the exception in verify, while the other thread is just calling `replay()` accidentally register the call of the other thread as expected method call).   This fix tries to address the issue, by moving the mocked objects as local variables into the test methods.  Call for review @vvcephei @bbejeck @cadonna","closed","streams,","mjsax","2020-02-16T21:14:49Z","2020-02-18T20:28:40Z"
"","7587","MINOR: Do not hold on to request buffers unless the request is using zero-copy","Rather than just assuming that any request containing a field of type ""bytes"" uses zero copy,  do not hold on to request buffers unless the byte buffer in question is zero-copy. ProduceRequest and SaslAuthenticateRequest are the only ones which do.  Make RenewDelegationTokenRequest and RenewDelegationTokenResponse flexible.  Make ExpireDelegationTokenRequest and ExpireDelegationTokenResponse flexible.  SimpleExampleMessage.json: fix formatting, etc.","open","","cmccabe","2019-10-23T21:53:30Z","2020-12-20T16:25:14Z"
"","8477","KAFKA-9864: avoid expensive QuotaViolationException usage","QuotaViolationException generates an exception message via String.format which uses regexes, and it does not use it as it is only used for control flow, e.g. https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ClientQuotaManager.scala#L258. It also generates an unnecessary stack trace, which is now avoided using the same pattern as in ApiException.  I have also avoided use of QuotaViolationException for control flow in ReplicationQuotaManager which is another hotspot that we have seen in practice.  Example profile showing a small overall cost, measuring 0.5%. This could be large in cases where quotas are exceeded frequently. ![image](https://user-images.githubusercontent.com/252189/79177311-68dca800-7db7-11ea-8937-8bee1ad3cc92.png)","closed","","lbradstreet","2020-04-14T01:49:30Z","2020-04-15T16:30:45Z"
"","7855","KAFKA-9317. Provide a group admin command to get group information or do some admin operation.","Provide a group admin command to get group information or do some admin operation.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","maobaolong","2019-12-19T08:32:15Z","2019-12-19T08:32:26Z"
"","8216","[KAFKA-9644] Non-existent configs in incrementalAlterConfigs APPEND/SUBTRACT","Problem ---- The `incrementalAlterConfigs` API supports OpType.APPEND and OpType.SUBTRACT for configuration properties of LIST type. If an APPEND or SUBTRACT OpType is submitted for a config property which currently has no value, then the operation fails with a NullPointerException on the broker side (conveyed as an ""unknown server error"" to the client).  This is because the alter code does a `getProperty` of the existing configuration value with no concern as to whether or not the property actually exists.  This change handles the case of existing null properties.   Testing ----- This change includes 2 test cases in the unit test that demonstrate the issue for OpType.SUBTRACT and OpType.APPEND.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","steverod","2020-03-04T01:47:47Z","2020-03-13T01:20:34Z"
"","8348","Minor: Don't swallow errors when altering log dirs in ReplicaManager","Previously, when trying to change the log dirs for a offline replica, the reassignment tool would only give a generic error message like  > Caused by: org.apache.kafka.common.errors.KafkaStorageException: Disk error when trying to access log file on the disk.  And the broker would not have any error in its log. This was due to a few exceptions getting swallowed in `ReplicaManager#alterReplicaLogDirs`. After this change, the broker will produce error logs like:  > [2020-03-25 11:46:10,382] ERROR [ReplicaManager broker=0] Unable to alter log dirs for test-1 (kafka.server.ReplicaManager) org.apache.kafka.common.errors.KafkaStorageException: Partition test-1 is offline","closed","","mumrah","2020-03-25T15:49:29Z","2020-03-26T18:07:12Z"
"","7709","MINOR: Generated classes should have standard equals","Previously, generates classes that were used as keys got non-standard equals and hashCode functions which only checked the fields that were part of their keys.  This was confusing for people using those functions in different contexts.  Instead, we should give ImplicitLinkedHashCollection and subclasses the ability to use a separate function to compare and hash the key fields, and generate standard equals and hashCode functions.","closed","","cmccabe","2019-11-18T19:31:36Z","2020-04-26T04:49:05Z"
"","7712","MINOR: Add validation in MockAdminClient for replication factor","Previously, creating a MockAdminClient with a number of brokers and trying to create a topic with a larger replication factor through it would result in an IndexOutOfBoundsException","closed","","stanislavkozlovski","2019-11-19T12:01:01Z","2019-11-22T14:32:25Z"
"","8089","KAFKA-6266: Repeated occurrence of WARN Resetting first dirty offset","Previously, checkpointed offsets for a log were only updated if the log was chosen for cleaning once the cleaning job completes. This caused issues in cases where logs with invalid checkpointed offsets would repeatedly emit warnings if the log with an invalid cleaning checkpoint wasn't chosen for cleaning.  Proposed fix is to update the checkpointed offset for logs with invalid checkpoints regardless of whether it gets chosen for cleaning.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-02-11T18:34:41Z","2020-02-18T22:35:33Z"
"","7727","MINOR: Do not log local address on failed Selector polls","Previously whenever we would print out a socket description on a socket which was not yet connected, Kafka would log the local IP address in a log similar to: ``` [SocketServer brokerId=6] Failed authentication with  () ``` Needless to say, this can be confusing. This patch changes the log message to something more explicit: ``` [SocketServer brokerId=6] Failed authentication with (Remote Address: [non-connected socket], Local Address: localhost) () ```","open","","stanislavkozlovski","2019-11-21T14:02:41Z","2019-11-22T14:31:44Z"
"","8235","KAFKA-9176: Do not update limit offset if we are in RESTORE_ACTIVE mode","Previously we may be updating the standby's limit offset as committed offsets to those source changelogs, and then inside the inner method we check if the state is in RESTORE_ACTIVE or not, which is a bug.  We should, instead, just check on the caller that we can skip restoring if: 1) we are in RESTORE_ACTIVE mode. 2) there're no source changelog partitions. 3) those partitions do not have any buffered records.  Also updated the unit test for this coverage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-03-05T22:58:01Z","2020-03-10T00:48:51Z"
"","7586","HOTFIX: don't discard ownedPartitions in VP upgrade system test","Previously was flaky roughly 1/8 times, kicked off 15 runs here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3252/","closed","","ableegoldman","2019-10-23T21:01:41Z","2019-10-24T23:53:05Z"
"","7920","KAFKA-7737; Use single path in producer for initializing the producerId","Previously the idempotent producer and transactional producer use separate logic when initializing the producerId. This patch consolidates the two paths. We also do some cleanup in `TransactionManagerTest` to eliminate brittle expectations on `Sender`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-09T17:31:35Z","2020-01-23T16:14:32Z"
"","8352","KAFKA-8319: Make KafkaStreamsTest a non-integration test class","Previous KafkaStreamsTest takes 2min20s on my local laptop, because lots of its integration test which is producing / consuming records, and checking state directory file system takes lots of time. On the other hand, these tests should be well simplified with mocks.  This test reduces the test from a clumsy integration test class into a unit tests with mocks of its internal modules. And some other test functions should not be in KafkaStreamsTest actually and have been moved to other modular test classes. Now it takes 2s.  Also it helps removing the potential flakiness of the following (some of them are claimed resolved only because we have not seen them recently, but after looking at the test code I can verify they are still flaky):  * KAFKA-5818 (the original JIRA ticket indeed exposed a real issue that has been fixed, but the test itself remains flaky) * KAFKA-6215 * KAFKA-7921 * KAFKA-7990 * KAFKA-8319 * KAFKA-8427  Reviewers: Bill Bejeck , John Roesler , Bruno Cadonna   This commit was cherry-picked from trunk and adapted.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-03-25T19:17:56Z","2020-03-26T10:25:57Z"
"","8181","KAFKA-9584 Headers ConcurrentModificationException","Prevent exceptions when modifying headers from within punctuate by creating a new Headers before punctuate is performed.   Unit test added  This contribution is my original work and I license it to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","MicahRam","2020-02-27T06:12:55Z","2020-09-29T15:01:42Z"
"","8203","MINOR: Port streams broker compatibility fix","Ports system test fix from #7997   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-03-02T17:12:49Z","2020-03-02T17:31:04Z"
"","8091","KAFKA-6607: Commit correct offsets for transactional input data","Port of #8040 to `trunk` -- the only real difference is the missing check of we are in pending shutdown and a task is not found -- in `trunk` we always create all new task hence this check is not necessary.  \cc @guozhangwang","closed","streams,","mjsax","2020-02-11T21:57:42Z","2020-02-12T20:19:38Z"
"","8461","Minor: Add a note to Drop older staging artifacts during RC process","Per the generated instructions in the release script, we just 'Close' the newly uploaded release candidate artifacts on the staging repository. However, if we don't also 'Drop' any older artifacts, the new artifacts won't be published to https://repository.apache.org/content/groups/staging/org/apache/kafka/  Thanks to @omkreddy for catching this!","closed","","mumrah","2020-04-10T14:46:53Z","2020-04-16T13:00:13Z"
"","8326","KAFKA-8639: Replace AddPartitionsToTxn with Automated Protocol","Part of the protocol automation effort.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-21T16:32:28Z","2020-04-24T04:39:12Z"
"","8215","KAFKA-9451: Update MockConsumer to support ConsumerGroupMetadata","Part of KIP-447","closed","streams,","mjsax","2020-03-03T23:43:34Z","2020-03-11T22:22:51Z"
"","7927","KAFKA-9294: Add tests for Named parameter","Part (2) -- stateful KStream operators  Follow up to #7874    KStream interface contains some method reordering (I also renamed some parameters as highlighted)  Call for review @bbejeck @vvcephei","closed","streams,","mjsax","2020-01-10T01:16:33Z","2020-03-11T08:05:37Z"
"","7647","KAFKA-9140: Also reset join future when generation was reset in order to re-join","Otherwise the join-group would not be resend and we'd just fall into the endless loop.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-11-05T03:46:13Z","2020-04-25T00:02:12Z"
"","8443","KAFKA-9832: Extend Streams system tests for EOS-beta","Other system tests that need an update.  Call for review @abbccdda @guozhangwang   System test for both tests passed:  - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3890/  - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3888/","closed","kip,","mjsax","2020-04-08T01:05:59Z","2020-06-12T23:10:47Z"
"","7880","KAFKA-9352: use 'roundrobin' to assign topic-partition to mirroring tasks","originally, when mirrormaker replicates a group of topics, the assignment between topic-partition and tasks are pretty static. E.g. partitions from the same topic tend to be grouped together as much as possible on the same task. For example, 3 tasks to mirror 3 topics with 8, 2 and 2 partitions respectively. 't1' denotes 'task 1', 't0p5' denotes 'topic 0, partition 5'  The original assignment will look like:  t1 -> [t0p0, t0p1, t0p2, t0p3] t2 -> [t0p4, t0p5, t0p6, t0p7] t3 -> [t1p0, t1p2, t2p0, t2p1]  The potential issue of above assignment is: if topic 0 has more traffic than other topics (topic 1, topic 2), t1 and t2 will be loaded more traffic than t3. When the tasks are mapped to the mirrormaker instances (workers) and launched, it will create unbalanced load on the workers. Please see the picture below as an unbalanced example of 2 mirrormaker instances:    Given each mirrored topic has different traffic and number of partitions, to balance the load across all mirrormaker instances (workers), 'roundrobin' helps to evenly assign all topic-partition to the tasks, then the tasks are further distributed to workers by calling 'ConnectorUtils.groupPartitions()'. For example, 3 tasks to mirror 3 topics with 8, 2 and 2 partitions respectively. 't1' denotes 'task 1', 't0p5' denotes 'topic 0, partition 5' t1 -> [t0p0, t0p3, t0p6, t1p1] t2 -> [t0p1, t0p4, t0p7, t2p0] t3 -> [t0p2, t0p5, t1p0, t2p1]  The improvement of this new above assignment over the original assignment is: the partitions of topic 0, topic 1 and topic 2 are all spread over all tasks, which creates a relatively even load on all workers, after the tasks are mapped to the workers and launched. Please see the picture below as a balanced example of 4 mirrormaker instances:","closed","","ning2008wisc","2019-12-31T22:21:00Z","2020-01-13T21:26:51Z"
"","7811","MINOR: Adjust MaxInlineLevel from 9 to 15","OpenJDK has changed the default from 9 to 15 in JDK 14:  https://bugs.openjdk.java.net/browse/JDK-8234863 https://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2019-December/036332.html  Scala applications tend to see the biggest benefit (one of the Scala benchmarks improved by 3x).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-12-10T16:28:32Z","2020-01-08T11:58:01Z"
"","8187","KAFKA-9620: Do not throw in the middle of consumer user callbacks","One way of fixing it forward.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-27T20:43:52Z","2020-03-02T17:16:08Z"
"","7503","KAFKA-9029: Flaky Test CooperativeStickyAssignorTest.testReassignmentWithRandomSubscriptionsAndChanges","One of the sticky assignor tests involves a random change in subscriptions that the current assignor algorithm struggles to react to and in cooperative mode ends up requiring more than one followup rebalance.   Apparently, in rare cases it can also require more than 2. Bumping the ""allowed subsequent rebalances"" to 4 (increase of 2) to allow some breathing room and reduce flakiness (technically any number is ""correct"", but if it turns out to ever require more than 4 we should revisit and improve the algorithm because that would be excessive (see [KAFKA-8767](https://issues.apache.org/jira/browse/KAFKA-8767))","closed","","ableegoldman","2019-10-11T22:36:42Z","2019-10-14T21:06:04Z"
"","8195","Minor: fix the unrelated types comparsion in MetadataRequestTest","one line change. the type of leaderId was changed to java.util.Optional.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-29T08:46:14Z","2020-02-29T22:13:21Z"
"","8252","KAFKA-6145: Pt 2.5 Compute overall task lag per client","Once we have encoded the offset sums per task for each client, we can compute the overall lag during `assign` by fetching the end offsets for all changelog and subtracting.  If the `listOffsets` request fails, we simply return a ""completely sticky"" assignment, ie all active tasks are given to previous owners regardless of balance.  Builds (but does not yet use) the `statefulTasksToRankedCandidates` map with the ranking:     Rank -1:    active running task     Rank 0:     standby or restoring task whose overall lag is within `acceptableRecoveryLag`     Rank 1:      tasks whose lag is unknown (eg during version probing)     Rank 1+:    all other tasks are ranked according to their actual total lag","closed","","ableegoldman","2020-03-07T21:08:09Z","2020-06-26T22:37:44Z"
"","8390","MINOR: Fix Scala 2.13 compiler warnings","Once Scala 2.13.2 is officially released, I will submit a follow up PR that enables `-Xfatal-warnings` with the necessary warning exclusions. Compiler warning exclusions were only introduced in 2.13.2 and hence why we have to wait for that. I used a snapshot build to test it in the meantime.  Changes: * Remove Deprecated annotation from internal request classes * Class.newInstance is deprecated in favor of Class.getConstructor().newInstance * Replace deprecated JavaConversions with CollectionConverters * Remove unused kafka.cluster.Cluster * Don't use Map and Set methods deprecated in 2.13:     - collection.Map +, ++, -, --, mapValues, filterKeys, retain     - collection.Set +, ++, -, -- * Add scala-collection-compat dependency to streams-scala and update version to 2.1.4. * Replace usages of deprecated Either.get and Either.right * Replace usage of deprecated Integer(String) constructor * `import scala.language.implicitConversions` is not needed in Scala 2.13 * Replace usage of deprecated `toIterator`, `Traversable`, `seq`, `reverseMap`, `hasDefiniteSize` * Replace usage of deprecated alterConfigs with incrementalAlterConfigs where possible * Fix implicit widening conversions from Long/Int to Double/Float * Avoid implicit conversions to String * Eliminate usage of deprecated procedure syntax * Remove `println`in `LogValidatorTest` instead of fixing the compiler warning since tests should not `println`. * Eliminate implicit conversion from Array to Seq * Remove unnecessary usage of 3 argument assertEquals * Replace `toStream` with `iterator` * Do not use deprecated SaslConfigs.DEFAULT_SASL_ENABLED_MECHANISMS * Replace StringBuilder.newBuilder with new StringBuilder * Rename AclBuffers to AclSeqs and remove usage of `filterKeys` * More consistent usage of Set/Map in Controller classes: this also fixes deprecated warnings with Scala 2.13 * Add spotBugs exclusion for inliner artifact in KafkaApis with Scala 2.12.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-03-30T19:31:01Z","2020-04-01T13:20:49Z"
"","7649","MINOR: Clean up PartitionAssignor for KIP-441","On-the-side cleanups extracted from the PR for KAFKA-9103, so that the actual PR can be as small as possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2019-11-05T20:12:18Z","2020-06-12T23:27:09Z"
"","8420","KAFKA-9815; Ensure consumer always re-joins if JoinGroup fails","On metadata change for assigned topics, we trigger rebalance, revoke partitions and send JoinGroup. If metadata reverts to the original value and JoinGroup fails, we don't resend JoinGroup because we don't set `rejoinNeeded`. This PR sets `rejoinNeeded=true` when rebalance is triggered due to metadata change to ensure that we retry on failure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-04-03T19:57:37Z","2020-04-07T08:19:52Z"
"","8021","Add retries to release.py script","On a spotty internet connection, the SFTP connections to the Apache servers can timeout or fail occasionally. These errors are usually recoverable by re-running `release.py`, but that's inconvenient.   This adds a simple retry mechanism to the `cmd` method which for now is only utilized by `sftp_mkdir`.","closed","","mumrah","2020-01-30T15:36:20Z","2020-01-30T19:29:15Z"
"","8354","KAFKA-9752; New member timeout can leave group rebalance stuck (#8339)","Older versions of the JoinGroup rely on a new member timeout to keep the group from growing indefinitely in the case of client disconnects and retrying. The logic for resetting the heartbeat expiration task following completion of the rebalance failed to account for an implicit expectation that shouldKeepAlive would return false the first time it is invoked when a heartbeat expiration is scheduled. This patch fixes the issue by making heartbeat satisfaction logic explicit.  Reviewers:  Chia-Ping Tsai , Guozhang Wang , Rajini Sivaram   Conflicts:   - core/src/main/scala/kafka/coordinator/group/MemberMetadata.scala   - core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-25T20:08:10Z","2020-03-26T13:29:52Z"
"","8339","KAFKA-9752; New member timeout can leave group rebalance stuck","Older versions of the JoinGroup rely on a new member timeout to keep the group from growing indefinitely in the case of client disconnects and retrying. The logic for resetting the heartbeat expiration task following completion of the rebalance failed to account for an implicit expectation that `shouldKeepAlive` would return false the first time it is invoked when a heartbeat expiration is scheduled. This patch fixes the issue by making heartbeat satisfaction logic explicit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-03-24T07:21:31Z","2020-03-25T05:16:50Z"
"","8070","MINOR:add missing quote for malformed line content","Noticed weird extra single quote in logs but actually not in the checkpoint file:  java.io.IOException: Malformed line in checkpoint file (/var/local/kafka/data_new1/recovery-point-offset-checkpoint): aa'  Realized it should be:  java.io.IOException: Malformed line in checkpoint file (/var/local/kafka/data_new1/recovery-point-offset-checkpoint): 'aa'  *Summary of testing strategy (including rationale) Minor logging change, not test needed.","closed","","cnZach","2020-02-08T04:12:42Z","2020-02-08T14:17:08Z"
"","7757","[MINOR] add console-producer and basic test","Noticed that we didn't have a way of producing arbitrary messages so I added a wrapper service around the `kafka_console_producer.sh` script.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","brianbushree","2019-11-27T22:11:52Z","2020-03-27T17:53:53Z"
"","7973","MINOR: Handle expandIsr in PartitionLockTest and ensure read threads not blocked on write","Noticed a PR build failure in the new PartitionLockTest:  ``` ERROR Exception during updateFollowerFetchState (kafka.cluster.PartitionLockTest:76) scala.MatchError: null 	at kafka.cluster.Partition.maybeUpdateIsrAndVersion(Partition.scala:1193) 	at kafka.cluster.Partition.expandIsr(Partition.scala:1183) 	at kafka.cluster.Partition.$anonfun$maybeExpandIsr$2(Partition.scala:690) 	at kafka.cluster.Partition.maybeExpandIsr(Partition.scala:686) 	at kafka.cluster.Partition.updateFollowerFetchState(Partition.scala:609) 	at kafka.cluster.PartitionLockTest.$anonfun$updateFollowerFetchState$1(PartitionLockTest.scala:267) 	at kafka.cluster.PartitionLockTest.updateFollowerFetchState(PartitionLockTest.scala:257) ```  The mocked instance was not handling `expandIsr`, so updated the test to handle this. Also updated the test to use the offset from the batch to trigger expandIsr more often. With this change, the test may try to acquire write lock while appends are blocked on read lock, so separated out the test using write lock to make it safer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-16T15:29:42Z","2020-01-17T19:21:02Z"
"","7909","MINOR: Update dependencies for Kafka 2.5","Noteworthy: * zstd decompression speed improvement of ~10%: https://github.com/facebook/zstd/releases/tag/v1.4.4 * EasyMock, PowerMock and Mockito: improved support for Java 13. * Replace usage of method deprecated by Mockito. * Gradle plugins updated to versions that require Gradle 5.x, this is fine since we no longer depend on the installed Gradle version. * Fixed build not to depend on methods deprecated in Gradle 5.x (fixes KAFKA-8786). * Reflections 0.9.12 no longer depends on Guava (fixes KAFKA-3061). * Updated `OptimizedKTableIntegrationTest` to pass with new version of Hamcrest. * Several Jetty improvements and bug fixes:    - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.21.v20190926    - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.22.v20191022    - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.23.v20191118    - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.24.v20191120    - https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.25.v20191220  Note that I did not upgrade lz4 due to https://github.com/lz4/lz4-java/issues/156.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-01-08T14:18:56Z","2020-01-09T00:25:34Z"
"","8306","MINOR: clean up required setup for StreamsPartitionAssignorTest","No logical or behavioral changes, just a bit of cleanup in this class before we have to write and fix a lot of these tests for KIP-441:  - Moved creation of `streamsMetadata` mock to `setUp` (in exactly one test it will be overwritten with a strict mock) - Tried to clean up the use of helper methods for configuring the assignor","closed","","ableegoldman","2020-03-17T01:56:36Z","2020-06-26T22:37:41Z"
"","7742","MINOR: Remove unnecessary license generation code in wrapper.gradle","Newer versions of Gradle handle this automatically.  Credit to @granthenke for the tip.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-23T18:08:50Z","2019-11-24T18:51:53Z"
"","7965","KAFKA-9436: New Kafka Connect SMT for plainText => Struct(or Map)","New SMT - plain text => struct(map) - regex group condition with ordered key name - compatible with single plain text input and struct field input plain text   ### sample1  ~~~ ""111.61.73.113 - - [08/Aug/2019:18:15:29 +0900] \""OPTIONS /api/v1/service_config HTTP/1.1\"" 200 - 101989 \""http://local.test.com/\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"""" SMT connect config with regular expression below can easily transform a plain text to struct (or map) data. ~~~   ~~~ ""transforms"": ""TimestampTopic, RegexTransform"", ""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ToStructByRegexTransform$Value"",  ""transforms.RegexTransform.struct.field"": ""message"", ""transforms.RegexTransform.regex"": ""^([\\d.]+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \""(GET|POST|OPTIONS|HEAD|PUT|DELETE|PATCH) (.+?) (.+?)\"" (\\d{3}) ([0-9|-]+) ([0-9|-]+) \""([^\""]+)\"" \""([^\""]+)\""""  ""transforms.RegexTransform.mapping"": ""IP,RemoteUser,AuthedRemoteUser,DateTime,Method,Request,Protocol,Response,BytesSent,Ms:NUMBER,Referrer,UserAgent"" ~~~   ### sample2  ~~~ {    ""code"" : ""dev_kafka_pc001_1580372261372""    ,""recode1"" : ""a""    ,""recode2"" : ""b""  } ~~~  ~~~ ""transforms"": ""RegexTransform"", ""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ToStructByRegexTransform$Value"",  ""transforms.RegexTransform.struct.field"": ""message"", ""transforms.RegexTransform.regex"": ""^(.{3,4})_(.*)_(pc|mw|ios|and)([0-9]{3})_([0-9]{13})"" ""transforms.RegexTransform.mapping"": ""env,serviceId,device,sequence,datetime:TIMEMILLIS""  ~~~","open","kip,","whsoul","2020-01-15T10:34:02Z","2022-01-25T21:41:47Z"
"","8328","MINOR: move some client methods to new ClientUtils","Moves three ""types"" of methods:  1) `getXXXClientId` 2) `XXXClientMetrics` 3) `fetchEndOffsets`","closed","streams,","ableegoldman","2020-03-22T20:42:06Z","2020-03-27T21:59:29Z"
"","8104","Changes to migrate to Artifactory","Move kafka build to Artifactory  Use Gradle Artifactory Credentials.  Kafka Team, please help cherry pick this into other branches once PR build passes and merged into master","closed","","mohnishbasha","2020-02-13T01:54:18Z","2020-02-13T01:54:34Z"
"","8353","KAFKA-9764: Remove stream simple benchmark suite","Motivation indicated in the JIRA. This suite is no longer useful to provide accurate results.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","abbccdda","2020-03-25T19:54:10Z","2020-10-05T20:37:31Z"
"","7896","KAFKA-9362: Missing GC logs when running Kafka with Java 11 with OpenJ9 VM","Modified `bin/kafka-run-class.sh` to use Java 8-like way of enabling GC logs when using a Java version based on OpenJ9 JVM, because this JVM does not support JEP 158 new options for GC logging.  To test the change, I ran the Quickstart with various types of Java: * ORACLE Java 8 * AdoptOpenJDK8 with HotSpot * AdoptOpenJDK11 with HotSpot * AdoptOpenJDK8 with OpenJ9 * AdoptOpenJDK11 with OpenJ9  Without this PR, running Quickstart with AdoptOpenJDK11 with OpenJ9 led to `JVMJ9VM085W`  errors in Zookeeper and in Kafka server output, and no GC logs were generated. With this PR, there are no more OpenJ9 errors and GC logs are produced as expected.  Also ran the graddle tests just for non-reg.","open","","avermeer","2020-01-04T22:13:05Z","2020-01-11T15:54:41Z"
"","8209","KAFKA-9632; Fix MockScheduler synchronization for safe use in Log/Partition tests","MockScheduler deadlocks if these two operations happen concurrently:  a) Thread1 executes a task which attempts to acquire LockA  b) Thread2 holding LockA attempts to schedule a new task  This  makes the use of MockScheduler unsafe in Log/Partition tests like PartitionLockTest which perform Log operations that schedule tasks while holding the Log lock. This PR reduces synchronization in MockScheduler to avoid blocking or deadlocking when scheduling tasks while holding a lock.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-03T12:13:25Z","2020-03-04T09:36:07Z"
"","7718","KAFKA-9088: Consolidate InternalMockProcessorContext and MockInternalProcessorContext","Mock `InternalProcessorContext` with a wrapper around `EasyMock`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","pierDipi","2019-11-20T09:43:09Z","2020-02-16T21:42:25Z"
"","7578","KAFKA-9078: Fix Connect system test after adding MM2 connector classes","MM2 added a few connector classes in Connect's classpath and given that the assertion in the Connect REST system tests need to be adjusted to account for these additions.   This fix makes sure that the loaded Connect plugins are a superset of the expected by the test connectors.   Testing: The change is straightforward. The fix was tested with local system test runs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-10-22T19:55:30Z","2019-10-23T15:35:53Z"
"","7954","MINOR: Use Math.min for StreamsPartitionAssignor#updateMinReceivedVersion method","Minor refactoring - changed `StreamsPartitionAssignor#updateMinReceivedVersion` method to use Math.min.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2020-01-14T09:00:01Z","2020-01-19T18:07:19Z"
"","7675","MINOR: Redundant copy in RecordHeaders constructor","minor improvement  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shikhar","2019-11-09T18:22:14Z","2019-11-14T03:03:02Z"
"","7837","HOTFIX: fix even/odd rows for upgrade.from config","Minor followup to https://github.com/apache/kafka/pull/7825, should be cherry-picked to 2.4","closed","","ableegoldman","2019-12-16T21:50:38Z","2019-12-31T00:26:24Z"
"","7631","HOTFIX: remove from restoringByPartition once restored","Minor follow up to https://github.com/apache/kafka/pull/7608: For some reason the `AssignedStreamTasks#updateRestored` method only updates the `restoring` and `restoredPartitions` data structures, but there is a third map holding restored tasks & partitions: `restoringByPartitions` -- this was causing failures during `onPartitionsLost` due to the new strict emptiness checks introduced in https://github.com/apache/kafka/pull/7608)  Also fixes a strict emptiness check based on a faulty assumption (introduced in https://github.com/apache/kafka/pull/7608) , as the `partitionInfo` in `StoreChangelogReader` is just the latest metadata for all topics and does not need to be, and shouldn't be, empty.  Also improves the `TaskManager#closeLostTasks` logging, by separating by case and logging the specific failure before throwing.  Needs to be cherry-picked to 2.4 as well","closed","streams,","ableegoldman","2019-11-01T01:49:00Z","2019-11-14T01:08:19Z"
"","8250","MINOR: Reset `streamTime` in clear()","Minor follow up to #8168, in particular https://github.com/apache/kafka/pull/8168/files#r389236659  Call for review @guozhangwang @abbccdda","closed","streams,","mjsax","2020-03-07T08:11:51Z","2020-03-11T05:30:36Z"
"","8391","MINOR: reduce garbage in operation and resource java conversions","Minor change to reduce unnecessary garbage generation on Operation and ResourceType java/scala conversions.  The round-trip tests in https://github.com/apache/kafka/blob/dbeaba5d9e45846d89835c1d30c138147528d0f4/core/src/test/scala/unit/kafka/security/auth/ResourceTypeTest.scala#L48 and https://github.com/apache/kafka/blob/dbeaba5d9e45846d89835c1d30c138147528d0f4/core/src/test/scala/unit/kafka/security/auth/OperationTest.scala#L33 test this code path well so this change is very small risk.","closed","","lbradstreet","2020-03-31T05:27:56Z","2020-04-01T03:16:07Z"
"","7517","KAFKA-9000: fix flaky FK join test by using TTD","Migrate this integration test to use TopologyTestDriver instead of running 3 Streams instances.  Dropped one test that was attempting to produce specific interleavings. If anything, these should be verified deterministically by unit testing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","vvcephei","2019-10-15T02:59:09Z","2019-10-17T14:32:04Z"
"","8286","KAFKA-9703:Free up resources when splitting huge batches","Method split takes up too many resources and might cause outOfMemory error when the bigBatch is huge. Call closeForRecordAppends() to free up resources like compression buffers.  Change-Id: Iac6519fcc2e432330b8af2d9f68a8d4d4a07646b Signed-off-by: Jiamei Xie   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiameixie","2020-03-12T07:12:59Z","2020-06-11T01:52:35Z"
"","8342","Kafka-9621: AdminClient listOffsets operation does not respect retries and backoff","MERGE AFTER KAFKA-9047  *More detailed description,  Similar to https://issues.apache.org/jira/browse/KAFKA-9047, currently the listOffsets operation doesn't respect the configured retries and backoff for a given call.  For example, the code path could go like so:  Make a metadata request and schedule subsequent list offsets calls  Metadata error comes back with InvalidMetadataException  Go back to 1  The problem here is that the state is not preserved across calls. We lose the information regarding how many tries the call has been tried and how far out we should schedule the call to try again. This could lead to a tight retry loop and put pressure on the brokers.  *Changes Added an overloaded getListOffsetsCalls() to take in the failed call metadata and build call instances' retry number and backoff variables upon the existing getListOffsetsCalls(). The retry calls can then recursively have their retry numbers + 1 and backoffs + x.  *Summary of testing strategy (including rationale)  Unit tests:  Test if listOffsets can get the correct offsets for each partition after maxAllowedNumTries times of failed tries Test if listOffsets will timeout after maxAllowedNumTries + 1 times of failed tries Test if listOffsets will respect the retry backoff   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2020-03-24T21:17:14Z","2020-04-10T04:09:21Z"
"","8217","Kafka-9621: AdminClient listOffsets operation does not respect retries and backoff","MERGE AFTER KAFKA-9047  *More detailed description of your change,  Similar to https://issues.apache.org/jira/browse/KAFKA-9047, currently the listOffsets operation doesn't respect the configured retries and backoff for a given call.   For example, the code path could go like so: 1) Make a metadata request and schedule subsequent list offsets calls 2) Metadata error comes back with InvalidMetadataException  3) Go back to 1  The problem here is that the state is not preserved across calls. We loose the information regarding how many tries the call has been tried and how far out we should schedule the call to try again. This could lead to a tight retry loop and put pressure on the brokers.  *Summary of testing strategy (including rationale)  Unit tests: 1. Test if listOffsets can get the correct offsets for each partition after maxAllowedNumTries times of failed tries 2. Test if listOffsets will timeout after maxAllowedNumTries + 1 times of failed tries 3. Test if listOffsets will respect the retry backoff  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2020-03-04T02:40:29Z","2020-03-24T21:17:22Z"
"","8370","KAFKA-9753: Add active tasks process ratio","Measure the percentage ratio the stream thread spent on processing each task among all assigned active tasks (KIP-444).  Also add unit tests to cover the added metrics in this PR and the previous https://github.com/apache/kafka/pull/8358  Also trying to fix the flaky test reported in KAFKA-5842  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","guozhangwang","2020-03-27T00:30:02Z","2020-06-12T23:14:11Z"
"","8372","MINOR: Using lambdas in producer package","Maybe using lambda will be more concise. I replaced old codes with lambdas in `org/apache/kafka/clients/producer` and `org/apache/kafka/clients/producer/internals` packages.","closed","","17hao","2020-03-27T07:49:53Z","2020-03-29T11:19:33Z"
"","8363","MINOR: Add missing @Override annotation and simplify Thread declarations with lambdas","Maybe it will improve the readability if `@Override` annotation is applied over the `run()` method since `Sender` class implements `Runnable`.","closed","","17hao","2020-03-26T10:00:01Z","2020-04-01T15:39:15Z"
"","7980","MINOR. Use `transform` instead of `mapValues`","mapValues is a view and doesn't create a new collection (so is lazy). So its good to use it if not all values in the Map is going to be processed. However if we are going to process all the values multiple times, its wasteful as it runs transformation function each time. If the transformation function is random, then the resulting collection each time we map isn't same either. On the other hand `transform` is strict and creates a new collection with updated values.  In `KafkaApis::filterAuthorized` we are using mapValues and then using 'map' to go over the resulting collection couple of times. This change updates the code to use `transform` instead of `mapValues`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-01-17T20:03:39Z","2020-01-17T23:12:18Z"
"","7982","MINOR: Improve producer test BufferPoolTest#testCloseNotifyWaiters.","Makes the main thread wait for workers to be ready to test the desired functionality before proceeding.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-01-18T17:24:25Z","2020-04-28T00:16:54Z"
"","7665","KAFKA-9159: The caller of sendListOffsetRequest need to handle retriable InvalidMetadata","loop call Consumer.endOffsets Throw TimeoutException: Failed to get offsets by times in 30000ms after a leader change","closed","","zzccctv","2019-11-08T08:19:10Z","2020-01-14T03:17:14Z"
"","7660","Consumer.endOffsets Throw TimeoutException: Failed to get offsets by times in 30000ms after a leader change","loop call Consumer.endOffsets Throw TimeoutException: Failed to get offsets by times in 30000ms after a leader change","closed","","zzccctv","2019-11-07T03:09:49Z","2019-11-08T08:13:58Z"
"","7699","KAFKA-9194: Missing documentation for replicaMaxWaitTimeMs config value","Looks it is a typo, the actual key supposed to be this #replicaFetchWaitMaxTimeMs(replica.fetch.wait.max.ms) instead of that the docs have this #replicaMaxWaitTimeMs  ### Committer Checklist (excluded from commit message) - [*] Verify design and implementation  - [*] Verify test coverage and CI build status - [*] Verify documentation (including upgrade notes)","closed","","satishbellapu","2019-11-16T10:53:06Z","2020-06-18T03:31:02Z"
"","8316","MINOR: Include partition directory for logging generated from Log","Logging from the Log layer could be confusing at times, when we have quick topic recreations, as we include the parent log directory in log messages. This PR changes this so that we include the partition directory instead.","open","","dhruvilshah3","2020-03-19T15:42:17Z","2020-06-08T03:55:36Z"
"","7556","MINOR Fixed LogCleanerIntegrationTest.testIsThreadFailed by removing the metric if it exists.","LogCleanerIntegrationTest.testIsThreadFailed is failed if it is run as the first test or it is run alone as the metric may not exist already and causes java.util.NoSuchElementException. It is fixed by removing only if it exists.  ``` kafka.log.LogCleanerIntegrationTest > testIsThreadFailed FAILED     java.util.NoSuchElementException: None.get         at scala.None$.get(Option.scala:529)         at scala.None$.get(Option.scala:527)         at kafka.log.LogCleanerIntegrationTest.removeMetric(LogCleanerIntegrationTest.scala:217)         at kafka.log.LogCleanerIntegrationTest.testIsThreadFailed(LogCleanerIntegrationTest.scala:199) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2019-10-18T11:38:53Z","2019-10-19T17:25:57Z"
"","7534","MINOR: log reason for fatal error in locking state dir","Log the reason for fatal errors while locking the state directory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-16T20:44:54Z","2019-10-17T14:33:17Z"
"","8144","KAFKA-9591: Log clearer error messages when there is an offset out of range (Client Change)","Log the partition start offset and last stable offset when the subscriptions have the default offset reset policy and the fetch offset is out of index.","closed","","ctan888","2020-02-20T22:48:09Z","2020-03-10T22:07:38Z"
"","7704","KAFKA-9200: ListOffsetRequest missing error response for v5","ListOffsetResponse getErrorResponse is missing a a case for version 5, introduced by 152292994e45d5bedda0673c142f9406e70d5d3e and released in 2.3.0.  ``` java.lang.IllegalArgumentException: Version 5 is not valid. Valid versions for ListOffsetRequest are 0 to 5                                                                                                                                                                                                                                                                        at org.apache.kafka.common.requests.ListOffsetRequest.getErrorResponse(ListOffsetRequest.java:282)                                                                                                                                                                                                                                                                         at kafka.server.KafkaApis.sendErrorOrCloseConnection(KafkaApis.scala:3062)                                                                                                                                                                                                                                                                                                 at kafka.server.KafkaApis.sendErrorResponseMaybeThrottle(KafkaApis.scala:3045)                                                                                                                                                                                                                                                                                             at kafka.server.KafkaApis.handleError(KafkaApis.scala:3027)                                                                                                                                                                                                                                                                                                                at kafka.server.KafkaApis.handle(KafkaApis.scala:209)                                                                                                                                                                                                                                                                                                                      at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:78)                                                                                                                                                                                                                                                                                                      at java.lang.Thread.run(Thread.java:748) ```","closed","","lbradstreet","2019-11-17T22:05:56Z","2019-11-18T03:22:59Z"
"","7878","KAFKA-9346: Consumer back-off logic when fetching pending offsets","Let consumer back-off and retry offset fetch when the specific offset topic has pending commits. The major change lies in the broker side offset fetch logic, where a request configured with flag `WaitTransaction` to true will be required to back-off when some pending transactional commit is ongoing. This prevents any ongoing transaction being modified by third party, thus guaranteeing the correctness with input partition writer shuffling.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2019-12-30T20:04:51Z","2020-01-18T23:15:40Z"
"","7802","[WIP]KAFKA-8733 Initial sketch of the solution mentioned in KIP-501.","Leader partition maintains follower replica state which is added with pending fetch requests that are to be processed for fetch offsets >= log-end-offset in the earlier fetch request. Follower replica is considered to be insync if there are any such pending fetch requests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","satishd","2019-12-09T11:55:05Z","2021-06-23T05:15:25Z"
"","7868","KAFKA-6144: Allow state stores to serve stale reads during rebalance","KIP-535 Implementation  *Summary of testing strategy* - [x] Unit tests for standby metadata - [x] Integration test for querying standbys, during rebalance - [x] Local testing - [x] Unit tests around APIs used for allLocalOffsetLags() API - [x] Integration test for lag APIs; LagInfo class - [x] Final test coverage good; all new methods; even some love for deprecated APIs - [ ] Streams System tests passing (1 test failing; currently triaging) - [ ] Special cases like source topic optimization;  - [ ] Update KIP with latest   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vinothchandar","2019-12-24T02:23:35Z","2020-01-18T18:30:15Z"
"","8246","KAFKA-6145: Pt 2. Include offset sums in subscription","KIP-441 Pt. 2: Compute sum of offsets across all stores/changelogs in a task and include them in the subscription.  Previously each thread would just encode every task on disk, but we now need to read the changelog file which is unsafe to do without a lock on the task directory. So, each thread now encodes only its assigned active and standby tasks, and ignores any already-locked tasks.  In some cases there may be unowned and unlocked tasks on disk that were reassigned to another instance and haven't been cleaned up yet by the background thread. Each StreamThread makes a weak effort to lock any such task directories it finds, and if successful is then responsible for computing and reporting that task's offset sum (based on reading the checkpoint file)  This PR therefore also addresses two orthogonal issues: 1) Prevent background cleaner thread from deleting unowned stores during a rebalance 2) Deduplicate standby tasks in subscription: each thread used to include every (non-active) task found on disk in its ""standby task"" set, which meant every active, standby, and unowned task was encoded by _every_ thread.","closed","kip,","ableegoldman","2020-03-06T23:55:44Z","2020-06-26T22:37:42Z"
"","7795","KAFKA-9212; Ensure LeaderAndIsr state updated in controller context during reassignment","KIP-320 improved fetch semantics by adding leader epoch validation. This relies on reliable propagation of leader epoch information from the controller. Unfortunately, we have encountered a bug during partition reassignment in which the leader epoch in the controller context does not get properly updated. This causes UpdateMetadata requests to be sent with stale epoch information which results in the metadata caches on the brokers falling out of sync.   This bug has existed for a long time, but it is only a problem due to the new epoch validation done by the client. Because the client includes the stale leader epoch in its requests, the leader rejects them, yet the stale metadata cache on the brokers prevents the consumer from getting the latest epoch. Hence the consumer cannot make progress while a reassignment is ongoing.   Although it is straightforward to fix this problem in the controller for the new releases (which this patch does), it is not so easy to fix older brokers which means new clients could still encounter brokers with this bug. To address this problem, this patch also modifies the client to treat the leader epoch returned from the Metadata response as ""unreliable"" if it comes from an older version of the protocol. The client in this case will discard the returned epoch and it won't be included in any requests.  Also, note that the correct epoch is still forwarded to replicas correctly in the LeaderAndIsr request, so this bug does not affect replication.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-07T02:28:49Z","2019-12-09T02:25:40Z"
"","7800","KAFKA-9212; Ensure LeaderAndIsr state updated in controller context during reassignment (#7795)","KIP-320 improved fetch semantics by adding leader epoch validation. This relies on reliable propagation of leader epoch information from the controller. Unfortunately, we have encountered a bug during partition reassignment in which the leader epoch in the controller context does not get properly updated. This causes UpdateMetadata requests to be sent with stale epoch information which results in the metadata caches on the brokers falling out of sync.  This bug has existed for a long time, but it is only a problem due to the new epoch validation done by the client. Because the client includes the stale leader epoch in its requests, the leader rejects them, yet the stale metadata cache on the brokers prevents the consumer from getting the latest epoch. Hence the consumer cannot make progress while a reassignment is ongoing.  Although it is straightforward to fix this problem in the controller for the new releases (which this patch does), it is not so easy to fix older brokers which means new clients could still encounter brokers with this bug. To address this problem, this patch also modifies the client to treat the leader epoch returned from the Metadata response as ""unreliable"" if it comes from an older version of the protocol. The client in this case will discard the returned epoch and it won't be included in any requests.  Also, note that the correct epoch is still forwarded to replicas correctly in the LeaderAndIsr request, so this bug does not affect replication.  This is a cherry-pick of 5d0cb1419 to the 2.4 branch with the changes necessary to make it work. Since the changes were not trivial, I submitted a pull request. Jason remains the author of the change.","closed","","ijuma","2019-12-09T02:21:11Z","2019-12-09T16:12:31Z"
"","8456","MINOR: Annotate KafkaAdminClientTest.testAlterClientQuotas() with @Test","KafkaAdminClientTest.testAlterClientQuotas() is uncalled. It is clearly intended to be a test method, but lacks `@Test`.","closed","","tombentley","2020-04-09T16:37:36Z","2020-05-03T21:03:11Z"
"","8428","MINOR: fix inaccurate RecordBatchIterationBenchmark.measureValidation benchmark","KAFKA-9820 (https://github.com/apache/kafka/pull/8422) added a benchmark of LogValidator.validateMessagesAndAssignOffsetsCompressed. Unfortunately it instantiated BrokerTopicStats within the benchmark itself, and it is expensive. The fixed benchmark does not change the outcome of the improvement in KAFKA-9820, and actually increases the magnitude of the improvement in percentage terms.  ``` Updated benchmark before KAFKA-9820: Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score      Error   Units RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15  164173.236 ± 2927.701   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   20440.980 ±  364.411  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15  137120.002 ±    0.002    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   20708.378 ±  372.041  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15  138913.935 ±  398.960    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15       0.547 ±    0.107  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15       3.664 ±    0.689    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15    2713.000             counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15    1398.000                 ms RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15  164305.533 ± 5143.457   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   20490.828 ±  641.408  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15  137328.002 ±    0.002    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   20767.922 ±  648.843  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15  139185.616 ±  325.790    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15       0.681 ±    0.053  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15       4.560 ±    0.292    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15    3101.000             counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15    1538.000                 ms RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15  169572.635 ±  595.613   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   21129.934 ±   74.618  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15  137216.002 ±    0.002    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   21410.416 ±   70.458  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15  139037.806 ±  309.278    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15       0.312 ±    0.420  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15       2.026 ±    2.725    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15    3398.000             counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15    1701.000                 ms JMH benchmarks done   Updated benchmark after KAFKA-9820: Benchmark                                                                     (bufferSupplierStr)  (bytes)  (compressionType)  (maxBatchSize)  (messageSize)  (messageVersion)   Mode  Cnt       Score     Error   Units RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15  322678.586 ± 254.126   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   20376.474 ±  15.326  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   69544.001 ±   0.001    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   20485.394 ±  44.087  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15   69915.744 ± 143.372    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15       0.027 ±   0.002  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15       0.091 ±   0.008    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15    3652.000            counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4               1           1000                 2  thrpt   15    1773.000                ms RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15  321332.070 ± 869.841   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   20303.259 ±  55.609  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   69600.001 ±   0.001    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   20394.052 ±  72.842  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15   69911.238 ± 160.177    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15       0.028 ±   0.003  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15       0.096 ±   0.010    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15    3637.000            counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4               2           1000                 2  thrpt   15    1790.000                ms RecordBatchIterationBenchmark.measureValidation                                        NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15  315490.355 ± 271.921   ops/s RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate                         NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   19943.166 ±  21.235  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm                    NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   69640.001 ±   0.001    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space                NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   20020.263 ±  43.144  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Eden_Space.norm           NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15   69909.228 ± 136.413    B/op RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen                   NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15       0.026 ±   0.002  MB/sec RecordBatchIterationBenchmark.measureValidation:·gc.churn.G1_Old_Gen.norm              NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15       0.090 ±   0.008    B/op RecordBatchIterationBenchmark.measureValidation:·gc.count                              NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15    3571.000            counts RecordBatchIterationBenchmark.measureValidation:·gc.time                               NO_CACHING   RANDOM                LZ4              10           1000                 2  thrpt   15    1764.000                ms ```","closed","","lbradstreet","2020-04-05T00:27:49Z","2020-04-05T21:24:56Z"
"","8479","KAFKA-9769: Finish operations for leaderEpoch-updated partitions up to point ZK Exception","KAFKA-9769: Finish operations for leaderEpoch-updated partitions up to point ZK Exception occurs. https://issues.apache.org/jira/browse/KAFKA-9769  For example, in such case, we will have the following mechanism : 1 - P1 and P2 succeeds. leaderEpoch for them are incremented because no ZkException occurs 2 - while making follower for P3, ZkException occurs and the leaderEpoch is not updated and thus thepartitionsToMakeFollower += partition isn’t executed. We catch this ZkException in line 1498 and log it as an error. No Exception is thrown. 3 - After catching the exception, makeFollower for P4 is then not executed. 4 - so the partitionsToMakeFollower only contains P1, P2. And fetchers are added to these partitionsToMakeFollower  Signed-off-by: Andrew Choi   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrewchoi5","2020-04-14T02:38:36Z","2020-07-06T18:25:40Z"
"","8096","KAFKA-9479 Describe consumer groups with --all-groups now prints head…","KAFKA-9479 Describe consumer groups with --all-groups now prints header only once  Describing the consumer groups with --all-groups will now only print the header once, instead of once per entry. The maximum coordinator length is computed first, by looping through the GroupStates in advance, to ensure correct cormatting.","closed","","vetler","2020-02-12T07:32:59Z","2020-03-25T11:56:23Z"
"","7913","KAFKA-9343: Add ps command for Kafka and zookeeper process on z/OS.","KAFKA-9343: Make Kafka shell runnable on z/OS  Finished both unit and integration test, and check  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","zshuo","2020-01-09T01:39:41Z","2020-03-12T08:18:59Z"
"","7640","KAFKA-9137: FETCH_SESSION_ID_NOT_FOUND caused by incorrect FetchSessionCache eviction","KAFKA-9137 describes a case where fetch sessions appear to be prematurely evicted from the FetchSessionCache causing frequent `FETCH_SESSION_ID_NOT_FOUND` errors and fetch session restarts.  I believe the handling in `kafka.server.FetchSession` is incorrect in two ways. Firstly it only touches the cache entry if the number of partitions changed between fetches. This means that a fetch session that is in steady state will not have its last used time updated. The second problem is that it supplies the current lastUsedMs as the current time when updating lastUsedMs for the session. As a result, even when the cache entry is touched, the time is not updated. Overall I believe this means that every entry's lastUsedMs will be the time it was created. When the fetch session becomes full, the oldest entry will be evicted, not the least recently used entry. Note that this also breaks the privileged session semantics as recently used ReplicaFetcher sessions will be considered to have been lost as they will not be considered used for longer than `MIN_INCREMENTAL_FETCH_SESSION_EVICTION_MS`.  The cache itself is currently tested reasonably well, but the interaction between FetchSession and FetchSessionCache is not currently tested. I have added a new test that tests the integration point between FetchSession and FetchSessionCache by adding new sessions and incrementally fetching them to check whether the right session is evicted.","closed","","lbradstreet","2019-11-04T16:45:38Z","2019-11-06T01:27:51Z"
"","7594","KAFKA-9088: Consolidate InternalMockProcessorContext and MockInternalProcessorContext","KAFKA-9088: Consolidate InternalMockProcessorContext and MockInternalProcessorContext  Merge `InternalMockProcessorContext` into `MockInternalProcessorContext` and replace all `InternalMockProcessorContext` usages by `MockInternalProcessorContext`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","pierDipi","2019-10-24T20:05:34Z","2019-11-22T19:56:59Z"
"","7607","KAFKA-9040 Add --all that includes dynamic and static config","KAFKA-9040 Implement --all option for describing configs  Added unit test: kafka.admin.ConfigCommandTest.testDescribeAllBrokerConfig  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","raymondng","2019-10-29T00:40:44Z","2020-01-30T03:32:04Z"
"","7886","KAFKA-9353: Add groupInstanceId to DescribeGroup for better visibility","Kafka-8538(#6957) has already added `group.instance.id` to `MemberDescription` but didn't print it, so we just print it. before the change, the describe command print as follows: ``` GROUP           CONSUMER-ID                                                  HOST            CLIENT-ID               #PARTITIONS      DemoConsumer    consumer-DemoConsumer-2-89251f12-f0ae-4dc1-a118-bda49f2a6e86 /127.0.0.1      consumer-DemoConsumer-2 0                DemoConsumer    consumer-DemoConsumer-1-72221c6b-f3d9-4c68-96db-ffffa12ddf93 /127.0.0.1      consumer-DemoConsumer-1 1                ```  after the change, the describe command print as follows: ``` GROUP           CONSUMER-ID                                    GROUP-INSTANCE-ID HOST            CLIENT-ID                       #PARTITIONS      DemoConsumer    groupIns2-f050379c-9c0d-433c-bbe0-44de6177b60d groupIns2         /127.0.0.1      consumer-DemoConsumer-groupIns2 0                DemoConsumer    groupIns1-44805ba9-ae6f-49d3-89af-44a4b95aff8d groupIns1         /127.0.0.1      consumer-DemoConsumer-groupIns1 1                         ```  if all the `GROUP-INSTANCE-ID` is null, just as the previous: ``` GROUP           CONSUMER-ID                                                  HOST            CLIENT-ID               #PARTITIONS      DemoConsumer    consumer-DemoConsumer-2-89251f12-f0ae-4dc1-a118-bda49f2a6e86 /127.0.0.1      consumer-DemoConsumer-2 0                DemoConsumer    consumer-DemoConsumer-1-72221c6b-f3d9-4c68-96db-ffffa12ddf93 /127.0.0.1      consumer-DemoConsumer-1 1                ```","closed","","dengziming","2020-01-02T10:20:21Z","2020-05-27T16:06:01Z"
"","7773","KAFKA-9265: Fix kafka.log.Log instance leak on log deletion","KAFKA-8448 fixes problem with similar leak. The Log objects are being held in ScheduledExecutor PeriodicProducerExpirationCheck callback. The fix in KAFKA-8448 was to change the policy of ScheduledExecutor to remove the scheduled task when it gets canceled (by calling setRemoveOnCancelPolicy(true)).  This works when a log is closed using close() method. But when a log is deleted either when the topic gets deleted or when the rebalancing operation moves the replica away from broker, the delete() operation is invoked. Log.delete() doesn't close the pending scheduled task and that leaks Log instance.  Fix is to close the scheduled task in the Log.delete() method too.  Tested with and without this fix and Log instances are no longer leaked and scheduled tasks are gone once log is deleted.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-12-03T22:04:13Z","2019-12-04T18:43:32Z"
"","7900","KAFKA-9373: Improve shutdown performance via lazy accessing the offset and time indices","KAFKA-7283 enabled lazy mmap on index files by initializing indices on-demand rather than performing costly disk/memory operations when creating all indices on broker startup. This helped reducing the startup time of brokers. However, segment indices are still created on closing segments, regardless of whether they need to be closed or not.  This patch: * Improves shutdown performance via lazy accessing the offset and time indices. * Eliminates redundant disk accesses and memory mapped operations while deleting or renaming files that back segment indices. * Prevents illegal accesses to underlying indices of a closed segment, which would lead to memory leaks due to recreation of the underlying memory mapped objects.  In our evaluations in a cluster with 31 brokers, where each broker has 13K to 20K segments, we observed up to 2 orders of magnitude faster LogManager shutdown times with this patch -- i.e. dropping the LogManager shutdown time of each broker from 10s of seconds to 100s of milliseconds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","efeg","2020-01-07T02:48:02Z","2020-03-26T16:40:48Z"
"","8346","KAFKA-9373: Reduce shutdown time by avoiding unnecessary loading of indexes","KAFKA-7283 enabled lazy mmap on index files by initializing indices on-demand rather than performing costly disk/memory operations when creating all indices on broker startup. This helped reducing the startup time of brokers. However, segment indices are still created on closing segments, regardless of whether they need to be closed or not.  This is a cleaned up version of #7900, which was submitted by @efeg. It eliminates unnecessary disk accesses and memory map operations while deleting, renaming or closing offset and time indexes.  In a cluster with 31 brokers, where each broker has 13K to 20K segments, @efeg and team observed up to 2 orders of magnitude faster LogManager shutdown times - i.e. dropping the LogManager shutdown time of each broker from 10s of seconds to 100s of milliseconds.  To avoid confusion between `renameTo` and `setFile`, I replaced the latter with the more restricted updateParentDir` (it turns out that's all we need).  Co-authored-by: Adem Efe Gencer  Co-authored-by: Ismael Juma   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-03-25T14:48:47Z","2020-03-26T16:41:08Z"
"","7483","KAFKA-8897: Warn about no guaranteed backwards compatibility in RocksDBConfigSetter","Kafka Streams cannot guarantee backwards compatibility of code in a user defined RocksDBConfigSetter since it does not control backwards compatibility of RocksDB. RocksDB may remove methods without deprecating them as it has already done in the past with CompactionOptionsFIFO#setTtl().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-10-10T11:13:08Z","2019-10-18T14:10:57Z"
"","7559","KAFKA-9067: added support for changing the provided BigDecimal scale","Kafka Connect schema framework failed whenever the scale of the set big decimal value was different from the one declared in the field definition. This change allows to gracefully expand or reduce scale using defined rounding rules.","open","kip,","piotrsmolinski","2019-10-19T12:15:13Z","2020-06-11T01:39:47Z"
"","8374","[MINOR] use the the proper error code for log","just a small change for the log content","closed","","lisa2lisa","2020-03-27T13:05:03Z","2020-03-27T13:57:57Z"
"","8245","MINOR: break up StreamsPartitionAssignor's gargantuan #assign","Just a minor refactoring of `StreamsPartitionAssignor`'s endless `assign` method into logical chunks to hopefully improve readability. No logical changes, literally just moving code around and adding docs.  The hope is to make it easier to write and review KIP-441 PRs that dig into the assignment logic","closed","kip,","ableegoldman","2020-03-06T22:51:32Z","2020-06-26T22:37:32Z"
"","8048","KAFKA-9509: Fixing flakiness of MirrorConnectorsIntegrationTest.testReplication","JIRA: https://issues.apache.org/jira/browse/KAFKA-9509  As the JIRA indicates, `org.apache.kafka.connect.mirror.MirrorConnectorsIntegrationTest.testReplication` has shown to be an increasingly flaky test recently. This PR aims to make this test more deterministic. Specifically, the flakiness was due to a timing issue between the tasks not starting up in time for the test to start running. This PR remediates that by introducing a status check after every connector is started up. These status checks include that the connector is found on the connect cluster as well as there are tasks created and up and running for that connector. These checks are introduced before the test starts running so that there is a confidence that the connectors and tasks are started up correctly before the test runs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","skaundinya15","2020-02-05T21:59:24Z","2020-10-16T05:53:49Z"
"","8057","KAFKA-9507 AdminClient should check for missing committed offsets","JIRA: https://issues.apache.org/jira/browse/KAFKA-9507  Addresses exception being thrown by AdminClient when listConsumerGroupOffsets returns a negative offset.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2020-02-06T23:48:02Z","2020-02-08T00:43:52Z"
"","8007","KAFKA-9477 Document RoundRobinAssignor as an option for partition.assignment.strategy","JIRA: https://issues.apache.org/jira/browse/KAFKA-9477  Summary: Add RoundRobinAssignor as an option to consumer configs  @joel-hamill","closed","consumer,","arodoni","2020-01-28T00:14:39Z","2020-02-05T23:31:42Z"
"","7484","MINOR: Improve assert in testCreateTopicsResponseMetadataAndConfig","It's much easier to debug when one can see the config names than one can only see a number.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-10T12:53:47Z","2019-10-11T04:37:41Z"
"","8198","MINOR: fix linking errors in javadoc","It seems to me we should block PR for javadoc error since the doc error is too noisy to the build log.  The Javadoc error about inaccessible link is NOT included by this PR.  related to #8291  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-01T08:11:07Z","2020-03-22T17:15:37Z"
"","8317","KAFKA-9691: Fix NPE by waiting for reassignment request","It seems likely the NPE reported in KAFKA-9691 was due to the call to `alterPartitionReassignments()` returning but the reassignment request not being completed yet, so try to fix it by calling `get()` on the returned `KafkaFuture`.","closed","","tombentley","2020-03-19T16:49:00Z","2020-04-09T14:24:45Z"
"","8151","MINOR: Add missing @Test annotations to miscellaneous tests","It seems like we missed adding @Test annotations to some test methods and therefore did not execute them on unit test suites.","open","","stanislavkozlovski","2020-02-21T09:39:50Z","2020-04-30T14:10:06Z"
"","8079","MINOR: revert change in log level for skipped record","It looks like it was a mistake during #6521  Specifically, while addressing code review comments to change other logs from debugs to warnings, this one seems to have been included by accident.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-02-10T17:59:17Z","2020-06-11T09:52:13Z"
"","7799","KAFKA-9287; Fix unneeded delay before failing pending transaction commit","It is possible for the user to call `commitTransaction` before a pending `AddPartitionsToTxn` call has returned. If the `AddPartitionsToTxn` call returns an abortable error, we need to cancel any pending batches in the accumulator and we need to fail the pending commit. The problem in this case is that `Sender.runOnce`, after failing the batches, enters `NetworkClient.poll` before it has a chance to cancel the commit. Since there are no pending requests at this time, this will block unnecessarily and prevent completion of the doomed commit. This patch fixes the problem by returning from `runOnce` if any batches have been aborted, which allows the commit to also fail without the delay.  Note that this was the cause of the delay executing `AuthorizerIntegrationTest.testTransactionalProducerTopicAuthorizationExceptionInCommit` compared to other tests in this class. After fixing the bug, the delay is gone and the test time is similar to other test cases in the class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-08T07:33:00Z","2020-01-09T19:11:23Z"
"","7544","MINOR: Upgrade zk to 3.5.6","It includes an important fix for people running on k8s:  * ZOOKEEPER-3320: Leader election port stop listen when hostname unresolvable for some time  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-17T13:05:44Z","2019-10-17T16:59:26Z"
"","8066","KAFKA-4090: Validate SSL connection in client","Issue has been around for a while.  Will post update on JIRA.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","belugabehr","2020-02-07T23:37:40Z","2020-02-07T23:37:40Z"
"","7581","KAFKA-9048 [WIP]: Keep track of state at FetchState when it needs to be updated","Introducing new state `FetchingWithNewState`, and only add to the session when that state is set, indicating some fields has changed since the last time. Scenarios include:  1. fetch offset has changed. 2. log start offset has changed. 3. leader epoch has changed. 4. when the partition is excluded from the previous fetch since it is delayed / truncated, indicating that when it has transited to Fetching we need to update the state as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-23T03:06:40Z","2020-04-24T23:44:33Z"
"","7968","KAFKA-9358 Explicitly unregister brokers and controller from ZK","Instead of closing the zkClient immediately after controller shutdown, the controller now resigns on shutdown rather than relying on the zkClient shutdown to implicitly do so.","open","","mumrah","2020-01-15T21:14:48Z","2020-01-17T19:25:14Z"
"","7681","KAFKA-9169: fix standby checkpoint initialization","Instead of caching the checkpoint map during StandbyTask initialization, use the latest checkpoints (which would have been updated during suspend).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-11-11T22:50:21Z","2019-11-14T04:04:37Z"
"","8047","MINOR: Add timer for update limit offsets","Instead of always try to update committed offset limits as long as there are buffered records for standby tasks, we leverage on the commit interval to reduce our `consumer.committed` frequency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-05T20:46:56Z","2020-04-24T23:52:12Z"
"","8371","KAFKA-9753: A few more metrics to add","Instance-level:  * number of alive stream threads  Thread-level:   * avg / max number of records polled from the consumer per runOnce, INFO * avg / max number of records processed by the task manager (i.e. across all tasks) per runOnce, INFO  Task-level:   * number of current buffered records at the moment (i.e. it is just a dynamic gauge), DEBUG.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-27T00:38:33Z","2020-04-25T00:00:51Z"
"","8055","MINOR: Start using Response and replace IOException in EmbeddedConnectCluster for failures","Inspecting the Response status code and entity is useful under testing. Therefore Connect's integration tests will benefit to transition from handling IOExceptions to inspecting the status code of the REST calls.   Furthermore, catching a non-retriable IOException and wrapping it within ConnectException simplifies how lambdas are written for tests as well how failed REST responses are handled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2020-02-06T21:56:51Z","2020-10-16T05:53:50Z"
"","7843","(DO NOT MERGE) KAFKA-9284: Zookeeper TLS docs and system tests","Initial commit, just a working system test for now, no docs.  I discovered that mutual authentication is required, so client needs a key store in addition to a trust store.  Also, client key store password must be the same as any password associated with the key in that key store -- if the passwords don't match (or if the key store is not specified at all) then the client wil be unable to connect to Zookeeper on the TLS clientSecurePort.  Specifying the required parameters (e.g. key/trust store locations and password, etc.) currently must be done via Java system properties, and it is possible to define system properties via ""-D"" flags at the command line.  This is less than desireable because command line arguments are insecure.  A better approach would be to specify them in a .properties file and have the JVM read them from there.  This might be doable by alowing the bin/kafka-run-class.sh script to accept a pair of parameters (e.g. -sysprops /path/to/system.properties) and invoke a wrapper class when loading system properties from a file is requested; the wrapper class could read and apply the system property definitions before forwarding the invocation to the intended class.  Adding actual Kafka configs is another option, but I think that would only apply to the brokers -- command line tools (Zookeeper security migration tool, etc.) would still need a separate solution.  If/how client certificate authentication can work with the Zookeeper security migration tool (which adds ACLs to lock down znodes) is an open question and requires more investigation.  For example, can SASL authentication work on top of certificate authentication?  If so then it is perhaps an acceptable first pass to leave things as they are -- only support applying Zookeeper ACLs on the SASL identity.  But if not, or if we wish to support ACLs based on the certificate identity, then we have to investigate how this can be done (maybe no change is needed) and also if we can support adding ACLs multiple certificate identities. If we can't support ading ACLs for multiple certificate identities then we would be constrained to all brokers authenticating with the same certificate.  So there is investigation to be done here.  KIP 515 was opened to address some of the above issues but it has been dormant for a while (and the number 515 was actually already taken by another KIP); this KIP might need to be revived (with a new KIP number).","closed","","rondagostino","2019-12-17T16:37:04Z","2020-02-10T02:23:07Z"
"","7719","Increase default Xmx for console commands.","Increase default Xmx for console commands from 256M to 1024M.","open","","allenxiang","2019-11-20T13:57:48Z","2019-11-20T16:39:30Z"
"","8293","KAFKA-9715: Eliminate reference to interBrokerProtocolVersion in TransactionStateManager","In this change, I'm eliminating the reference to interBrokerProtocolVersion in TransactionStateManager. The reason is that it is an unused attribute in TransactionStateManager. The attribute is not being used in any of the existing logic inside the class (and the class has no sub-classes). Therefore, it felt like a good candidate for elimination.  **Test plan:**  Ran the following unit tests: `./gradlew core:unitTest`","closed","","kowshik","2020-03-13T04:26:54Z","2020-03-13T20:16:47Z"
"","8292","TransactionStateManager: Eliminate referene to interBrokerProtocolVersion","In this change, I'm eliminating the reference to `interBrokerProtocolVersion` in `TransactionStateManager`. The reason is that it is an unused attribute in `TransactionStateManager`. The attribute is not being used in any of the existing logic inside the class (and the class has no sub-classes). Therefore, it felt like a good candidate for elimination.","closed","","kowshik","2020-03-13T04:16:39Z","2020-03-13T04:21:34Z"
"","8189","MINOR: Double Quote CLASSPATH to prevent shell glob expansion.","In the event that the CLASSPATH does not have an ending "":"", the shell can expand the CLASSPATH globs to be space-separated list of paths/jars, which is not how the JVM CLI accepts arguments to `-cp` switch. So double quote the variable to prevent pattern expansion, and pass the glob pattern directly to the JVM.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrewegel","2020-02-28T01:31:58Z","2020-02-28T01:33:04Z"
"","8191","MINOR: Double Quote CLASSPATH to prevent shell glob expansion.","In the event that the CLASSPATH does not have an ending "":"", the shell can expand the CLASSPATH globs to be space-separated list of paths/jars, which is not how the JVM CLI accepts arguments to -cp switch. So double quote the variable to prevent pattern expansion, and pass the glob pattern directly to the JVM.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andrewegel","2020-02-28T01:35:40Z","2020-02-29T23:13:01Z"
"","7706","Using AclCommand,avoid call the global method loadcache in SimpleAclAuthorizer","In the class Named AclCommand,configure SimpleAclAuthorizer,but no need call loadCache. If there are a lot of topics, loadCache will be very slow. now we have 20,000 topics in a new cluster,all these topics need to be authed for each project group.No matter how to run AclCommand, it will be very slow. this updating can improve the running time from minutes to less than one second","open","","StevenLuMT","2019-11-18T13:53:09Z","2020-02-16T11:00:28Z"
"","7999","KAFKA-9464: Close the producer in completeShutdown","In StreamThread#completeShutdown, the producer (if not null) should be closed.  Also removed some dead code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tedyu","2020-01-22T21:50:13Z","2020-01-23T19:19:55Z"
"","7836","HOTFIX: fix system test race condition","In some system tests a Streams app is started and then prints a message to stdout, which the system test waits for to confirm the node has successfully been brought up. It then greps for certain log messages in a retriable loop.   But waiting on the Streams app to start/print to stdout does not mean the log file has been created yet, so the `grep` may return an error. Although this occurs in a retriable loop it is assumed that grep will not fail, and the result is piped to `wc` and then blindly converted to an int in the python function, which fails since the error message is a string (throws `ValueError`)  We should catch the `ValueError` and return a 0 so it can try again rather than immediately crash","closed","tests,","ableegoldman","2019-12-16T21:44:11Z","2020-01-07T02:35:01Z"
"","8403","KAFKA-9797; Fix TestSecurityRollingUpgrade.test_enable_separate_interbroker_listener","In order to perform non-disruptive update of inter-broker listener to a different security protocol, we need to first ensure that the listener is enabled on all brokers. Once the listener is available on all brokers, inter-broker listener can be updated using rolling update.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-04-01T11:08:24Z","2020-04-15T12:04:12Z"
"","7577","KAFKA-9076: support consumer offset sync across clusters in MM 2.0","In order to make the Kafka consumer and stream application migrate from source to target cluster transparently and conveniently, e.g. in event of source cluster failure, a background task is proposed to periodically sync the consumer offsets from the source to target cluster, so that when the consumer and stream applications switch to the target cluster, they will resume to consume from where they left off at source cluster.  https://cwiki.apache.org/confluence/display/KAFKA/KIP-545%3A+support+automated+consumer+offset+sync+across+clusters+in+MM+2.0","closed","connect,","ning2008wisc","2019-10-22T19:14:31Z","2020-06-26T15:39:02Z"
"","8143","KAFKA-9582: Do not abort transaction in unclean close","In order to avoid hitting the fatal exception during unclean close, we should avoid calling the abortTransaction() call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-20T22:44:01Z","2020-02-21T18:28:18Z"
"","7824","MINOR: flush only the evicted dirty entry","In NamedCache we currently flush _all_ dirty entries in the cache any time an eviction occurs. Since we don't batch the writes, there's not much benefit to flushing everything. Modifying this behavior to only flush the LRU entry would improve the cache in two ways  1) Makes it more effective at what it does, namely: reducing downstream traffic and writes to the underlying store 2) Helps reduce a source of overcounting with _at_least_once_. Without a cache, all records will immediately go into the changelog. If you process N records and then crash before the next commit, on recovery all N records will be loaded into the state store but the task will be resumed from the previous commit, causing these records to be processed twice/overcounted. Buffering  entries in the cache for as long as possible before the next commit means fewer records at risk of being overcounted","open","streams,","ableegoldman","2019-12-12T01:37:46Z","2019-12-17T23:13:08Z"
"","8001","KAFKA-9465: Enclose consumer call with catching InvalidOffsetException","In maybeUpdateStandbyTasks, the try block encloses restoreConsumer.poll and record handling. Since InvalidOffsetException is thrown by restoreConsumer.poll, we should enclose this call in the try block.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tedyu","2020-01-22T22:29:55Z","2020-05-14T00:32:09Z"
"","7606","MINOR: Preserve backwards-compatibility by renaming the AlterPartitionReassignment metric to PartitionReassignment","In https://github.com/apache/kafka/commit/18d4e57f6e8c67ffa7937fc855707d3a03cc165a#diff-394389922df5210adf43a8b7064cc4ffL61 we unintentionally renamed the metric with the previous changes to reassignments","closed","","stanislavkozlovski","2019-10-28T23:13:51Z","2019-10-30T09:47:32Z"
"","7651","Using AclCommand,avoid call the global method loadcache in SimpleAclAuthorizer","in AclCommand,configure SimpleAclAuthorizer,but no need call loadCache. If there are a lot of topics, loadCache will be very slow. now we have 20,000 topics in a new cluster,all these topics need to be authed for each project group.No matter how to run AclCommand, it will be very slow. this updating can improve the running time from minutes to less than one second","closed","","StevenLuMT","2019-11-06T03:01:42Z","2019-11-18T13:53:27Z"
"","7518","KAFKA-9032: Bypass serdes for tombstones","In a KTable context, null record values have a special ""tombstone"" significance. We should always bypass the serdes for such tombstones, since otherwise the serde could violate Streams' table semantics.  Added test coverage for this case and fixed the code accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-15T03:18:35Z","2019-10-16T16:57:14Z"
"","8077","KAFKA-9583: use topic-partitions grouped by node to send OffsetsForLeaderEpoch requests","In `validateOffsetsAsync`, we group the requests by leader node for efficiency. The list of topic-partitions are grouped from `partitionsToValidate` (all partitions) to `node` => `fetchPostitions` (partitions by node). However, when actually sending the request with `OffsetsForLeaderEpochClient`, we use `partitionsToValidate`, which is the list of all topic-partitions passed into `validateOffsetsAsync`. This results in extra partitions being included in the request sent to brokers that are potentially not the leader for those partitions.  This PR fixes the issue by using `fetchPositions`, which is the proper list of partitions that we should send in the request. Additionally, a small typo of API name in `OffsetsForLeaderEpochClient` is corrected (it originally referenced `LisfOffsets` as the API name).  ## Tests  Tested with the following configuration:  topic-partitions: ``` ❯ kafkacat -b localhost:8080 -L Metadata for all topics (from broker -1: localhost:8080/bootstrap):  3 brokers:   broker 1 at localhost:9092 (controller)   broker 2 at localhost:9093   broker 3 at localhost:9094  2 topics:   topic ""broker-a.only"" with 1 partitions:     partition 0, leader 1, replicas: 1, isrs: 1   topic ""broker-b.only"" with 5 partitions:     partition 0, leader 2, replicas: 2, isrs: 2     partition 1, leader 2, replicas: 2, isrs: 2     partition 2, leader 2, replicas: 2, isrs: 2     partition 3, leader 2, replicas: 2, isrs: 2     partition 4, leader 2, replicas: 2, isrs: 2 ```  before the change: `broker-a.only-1` was sent to broker 2, which is not the leader for this partition.  after the change: `broker-a.only-1` was not sent to broker 2.  Client tests are run with `./gradlew :clients:test`. Ideas for additional tests are welcome!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andyfangdz","2020-02-10T05:30:31Z","2021-03-04T03:47:11Z"
"","8278","KAFKA-8803: Remove timestamp check in completeTransitionTo","In `prepareAddPartitions` the txnStartTimestamp could be updated as updateTimestamp, which is assumed to be always larger then the original startTimestamp. However, due to ntp time shift the timer may go backwards and hence the newStartTimestamp be smaller than the original one. Then later in completeTransitionTo the time check would fail with an IllegalStateException, and the txn would not transit to Ongoing.  An indirect result of this, is that this txn would NEVER be expired anymore because only Ongoing ones would be checked for expiration.  We should do the same as in https://github.com/apache/kafka/pull/3286 to remove this check.  Also added test coverage for both KAFKA-5415 and KAFKA-8803.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-11T17:56:22Z","2020-04-25T00:00:35Z"
"","7650","MINOR: Fetch only from leader should be respected in purgatory","In #7361, we inadvertently reverted a change to enforce leader only fetching for old versions of the protocol. This patch fixes the problem and adds a new test case to cover fetches which hit purgatory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-06T01:12:19Z","2019-11-06T17:24:43Z"
"","7781","KAFKA-8904: Improve producer's topic metadata fetching.","Improves the producer's topic metadata fetching logic by maintaining a per-topic last refresh time, which is used to discriminate which topics' metadata should be fetched.  The improvement can be witnessed during producer startup, where many topics may be fetched serially. Previously, where N topics were being fetched serially, O(N^2) total topic metadata would be processed, whereas this improvement makes it O(N).  A simple test can show the differences fairly easily. Starting a producer on 500 existing topics with 64 partitions each, single-threaded:  Old: 1000 records sent, 32.446463 records/sec (0.00 MB/sec), 65.52 ms avg latency, 459.00 ms max latency, 23 ms 50th, 213 ms 95th, 360 ms 99th, 459 ms 99.9th.  New: 1000 records sent, 206.143063 records/sec (0.00 MB/sec), 10.55 ms avg latency, 187.00 ms max latency, 7 ms 50th, 29 ms 95th, 43 ms 99th, 187 ms 99.9th.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-12-04T22:12:43Z","2020-02-11T14:09:46Z"
"","7523","KIP-500: Update kafka-configs.sh to support non-ZK topics operations.","Improves the broker-side functionality of kafka-config.sh to include topic operations, as well as support for listing of brokers and topics when provided an empty entity name.  Minor changes to functionality to make it equivalent to the ZK-version. Because listing of entities was not supported for the broker, the command was not distinguishing between empty and default entities, where the former is a request to list the entities, the latter is for the default configuration.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-10-15T21:36:08Z","2019-12-04T20:13:53Z"
"","7780","KIP-543: Support broker topic actions in ConfigCommand.","Improves the broker-side functionality of kafka-config.sh to include topic operations, as well as support for listing of brokers and topics when provided an empty entity name.  Minor changes to functionality to make it equivalent to the ZK-version. Because listing of entities was not supported for the broker, the command was not distinguishing between empty and default entities, where the former is a request to list the entities, the latter is for the default configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-12-04T20:14:04Z","2019-12-20T01:05:20Z"
"","7801","[WIP] Add KIP-145 HeaderTo and HeaderFrom transforms for Connect","Implements the SMTs HeaderTo and HeaderFrom described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect). Extends #4319 and complements #7284.  Since we're not using schemas in our project that code is untested. Will try to add some tests unless somebody feels like contributing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","flojon","2019-12-09T08:46:25Z","2020-03-22T00:51:07Z"
"","8083","KIP-546 (1/2): Implements describeClientQuotas() and alterClientQuotas().","Implements describeClientQuotas() and alterClientQuotas() to match current functionality, i.e. extensible entity types and other new quotas features like resolving quotas is not yet supported.  This is the minimal functionality necessary for KIP-500, and converts the ConfigCommand to use the client quotas APIs. resolveClientQuotas() and the corresponding client quotas command will be in a future PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-02-10T22:26:13Z","2020-03-15T06:03:14Z"
"","7821","DOCS - clarify transactionalID and idempotent behavior","If transactional.id is set without setting enable.idempotence, the producer will set enable.idempotence to true implicitly. The docs should reflect this.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2019-12-11T23:04:12Z","2020-02-07T21:08:26Z"
"","7622","HOTFIX: Try to complete Send even if no bytes were written","If there are pending bytes in the transport layer, we may complete a send even if no bytes were recorded as written. We assume bytes are written when they are in the netWriteBuffer, but we only consider the send as completed when it's in the socket channel buffer.  This fixes a regression introduced via 0971f66ff546. The impact is that we would sometimes throw the following exception in `MultiRecordsSend.writeTo`:  ```java if (completed())     throw new KafkaException(""This operation cannot be invoked on a complete request.""); ```  Added unit test verifying the bug fix. While in the area, I simplified one of the `SslSelectorTest` methods.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-31T14:09:04Z","2019-11-02T15:42:35Z"
"","8418","KAFKA-9807; Protect LSO reads from concurrent high-watermark updates","If the high-watermark is updated in the middle of a read with the `read_committed` isolation level, it is possible to return data above the LSO. In the worst case, this can lead to the read of an aborted transaction. The root cause is that the logic depends on reading the high-watermark twice. We fix the problem by reading it once and caching the value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-03T17:23:33Z","2020-04-03T20:56:43Z"
"","8411","KAFKA-9812: fix infinite loop in test code","If the EosIntegrationTest fails an assertion after setting `gcInjected:=true` but before it gets to set `doGC=false`, then it would never set the flag, and the transformer would become an infinite loop.  There are a couple of ways to tackle this, but I opted to just check inside the loop that we're not currently shutting down.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","vvcephei","2020-04-03T02:54:13Z","2020-04-03T17:26:38Z"
"","7726","[MINOR] fix requested array size exceeds VM limit","if the dedup buffer size per thread is bigger or equal than 2G (aka Int.MaxValue), kafka won't be able to start with exception of: ``` 2019-11-20 16:06:07,530] FATAL [Kafka Server ], Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer) java.lang.OutOfMemoryError: Requested array size exceeds VM limit         at java.nio.HeapByteBuffer.(HeapByteBuffer.java:57)         at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)         at kafka.log.SkimpyOffsetMap.(OffsetMap.scala:45)         at kafka.log.LogCleaner$CleanerThread.(LogCleaner.scala:214)         at kafka.log.LogCleaner$$anonfun$3.apply(LogCleaner.scala:105)         at kafka.log.LogCleaner$$anonfun$3.apply(LogCleaner.scala:105)         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)         at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)         at scala.collection.immutable.Range.foreach(Range.scala:160)         at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)         at scala.collection.AbstractTraversable.map(Traversable.scala:104)         at kafka.log.LogCleaner.(LogCleaner.scala:105)         at kafka.log.LogManager.(LogManager.scala:78)         at kafka.log.LogManager$.apply(LogManager.scala:598)         at kafka.server.KafkaServer.startup(KafkaServer.scala:215)         at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)         at kafka.Kafka$.main(Kafka.scala:65)         at kafka.Kafka.main(Kafka.scala) ```  **For more detailed reference**: https://plumbr.io/outofmemoryerror/requested-array-size-exceeds-vm-limit","open","","lisa2lisa","2019-11-21T12:58:56Z","2020-01-06T16:30:07Z"
"","7557","MINOR: don't require key serde in join materialized","If the `Materialized` is missing a key serde, set it from the current scope.   This was the intent of the prior code, but it didn't actually update the `MaterializedInternal`, which is needed to correctly construct the store builder later on.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-18T21:17:57Z","2019-10-21T22:29:15Z"
"","8325","KAFKA-9741: Update ConsumerGroupMetadata before calling onPartitionsRevoked()","If partitions are revoked, an application may want to commit the current offsets.  Using transactions, committing offsets would be done via the producer passing in the current `ConsumerGroupMetadata`. If the metadata is not updates before the callback, the call to `commitTransaction(...)` fails as and old generationId would be used.  Call for review @ableegoldman @abbccdda @guozhangwang @hachikuji   \cc @mumrah (not sure if this is a blocker for 2.5 or not)","closed","consumer,","mjsax","2020-03-21T01:29:42Z","2020-06-11T01:12:44Z"
"","8425","KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away","If a stateful task is intended to be moved to a client which is already caught-up, we should not create a warmup task for it. Instead, just reassign that task right away.","closed","","ableegoldman","2020-04-04T04:04:28Z","2020-06-26T22:37:55Z"
"","7710","MINOR: Add ignorable field check to `toStruct`","If a field is not marked as ignorable, we should raise an exception if it has been set to a non-default value. This check already exists in `Message.write`, so this patch adds it to `Message.toStruct`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-19T01:09:21Z","2019-11-23T06:05:03Z"
"","7738","KAFKA-9226: Updated documentation section on log deletion policies","I've updated the documentation section on log retention.  - Removed mention of pluggable deletion policies - Changed deletion of segment by file evaluation from access time to record timestamp - Added details of size based cleanup policies   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-11-22T14:18:25Z","2019-11-27T12:08:07Z"
"","7543","MINOR: add missed curly brackets to the log.error","I've added missed curly brackets to the log.error(...) in `WorkerTask` as it doesn't show the root cause of the exception if some appears in Kafka Connector.","closed","","NarekDW","2019-10-17T12:54:44Z","2019-10-18T09:56:47Z"
"","7987","KAFKA-9456: Add support for printing all the latencies","I've added a new optional argument called 'latencies_csv'. The allowed values is ""0"" (disabled) or ""1"" (enabled).  If enabled, after execution, it will print to stdout all the latencies stored instead of a brief report. I've also changed the partial latency prints during the test to 'stderr'.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","ffosilva","2020-01-20T16:05:58Z","2020-06-13T00:29:11Z"
"","8347","[MINOR] backport kafkatest per-broker overrides & extra JVM args","I would like this to get cherry-picked back to 2.0, 2.1, 2.2 as well so here are the corresponding system test runs:  [2.0](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3853/) - [2 failures](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-03-25--001.1585169593--brianbushree--2.0-backport--fe46c05/report.html)  [2.1](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3850/) - [ALL PASSING](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-03-25--001.1585166718--brianbushree--2.1-backport--b7b25f5/report.html)  [2.2](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3851/ ) - [ALL PASSING](http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-03-25--001.1585169218--brianbushree--2.2-backport--8f76409/report.html)  [2.3](https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3852/) - [ALL PASSING](http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2020-03-25--001.1585170359--brianbushree--2.3-backport--da99900/)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2020-03-25T15:21:35Z","2020-03-26T16:23:00Z"
"","8095","MINOR: Fix unnecessary metadata fetch before group assignment","I tracked the recent increase in the flakiness of one of the offset reset tests (KAFKA-9538) back to https://github.com/apache/kafka/pull/7941. After investigation, I found that following this patch, the consumer was sending an additional metadata request prior to performing the group assignment. This slight timing difference was enough to trigger the test failures. The problem turned out to be due to a bug in `SubscriptionState.groupSubscribe`, which no longer counted the local subscription when determining if there were new topics to fetch metadata for. Hence the extra metadata update.  Without the fix, I saw 30-50% test failures locally. With it, I could no longer reproduce the failure. However, #6561 is probably still needed to improve the resilience of this test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-02-12T06:30:10Z","2020-02-12T19:45:07Z"
"","8208","KAFKA-8895: delete all topics before recreating","I think the root cause of KAFKA-8893, KAFKA-8894, KAFKA-8895 and KSTREAMS-3779 are the same: some intermediate topics are not deleted in the `setup` logic before recreating the user topics, which could cause the waitForDeletion (that check exact match of all existing topics) to fail, and also could cause more records to be returned because of the intermediate topics that are not deleted from the previous test case.  Also inspired by https://github.com/apache/kafka/pull/5418/files I used a longer timeout (120 secs) for deleting all topics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-03T00:55:03Z","2020-04-25T00:04:46Z"
"","7786","KAFKA-9179; Fix flaky test due to race condition when fetching reassignment state","I see `TopicCommandWithAdminClientTest.testDescribeUnderReplicatedPartitionsWhenReassignmentIsInProgress` failing locally quite often, probably 30% of the time or more. After investigating, the failures are due to a race condition on reassignment completion. The previous code fetched metadata first and then fetched the reassignment state. It is possible in between those times for the reassignment to complete, which leads to spurious URPs being reported. The fix is to change the order of these checks and to explicitly check for reassignment completion. This is still not a 100% reliable approach because it is also possible for another reassignment to be submitted, but it catches the most likely case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-05T19:03:40Z","2019-12-06T00:57:24Z"
"","8106","KAFKA-9319: Fix generation of CA certificate for system tests.","I perform system tests check to ensure that we can enable only `TLSv1.3` by default.  I've found two issues:    1. CA certificate that is generated in `security_config.py` can't be validated by the openjdk11, therefore, tests with SSL enabled failed. (Error message is ""TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate"")   2. The actual stack trace of the fail is hidden when the `ConfigException` stack trace printed.  This PR fixes those 2 issues:    * ` --ext bc=ca:true` param for `keytool` added.    * SSL Validation exception printed in error log.  [Keytool documentation](https://docs.oracle.com/en/java/javase/11/tools/keytool.html)  >Supported Named Extensions > The keytool command supports these named extensions. The names aren't case-sensitive. > BC or BasicContraints > Values: > The full form is ca:{true|false}[,pathlen:len] or len, which is short for ca:true,pathlen:len. > When len is omitted, the resulting value is ca:true.   Command to run tests(openjdk11 used): ``` export tests=""tests/kafkatest/tests/connect/connect_distributed_test.py"" TC_PATHS=""$tests"" bash tests/docker/run_tests.sh ```  java version in a docker container: ``` [nizhikov@sbt-qa-01 kafka]$ docker exec -it ducker04 bash ducker@ducker04:/$ java -version openjdk version ""11.0.6"" 2020-01-14 OpenJDK Runtime Environment 18.9 (build 11.0.6+10) OpenJDK 64-Bit Server VM 18.9 (build 11.0.6+10, mixed mode) ```  Exception in tests *without* new `ext` parameter:  ``` [2020-02-13 10:17:46,244] DEBUG Created SSL context with keystore SecurityStore(path=/mnt/security/test.keystore.jks, modificationTime=Thu Feb 13 10:17:43 UTC 2020), truststore SecurityStore(path=/mnt/security/test.truststore.jks, modificationTime=Thu Feb 13 10:17:41 UTC 2020), provider SunJSSE. (org.apache.kafka.common.security.ssl.SslEngineBuilder) javax.net.ssl.SSLHandshakeException: PKIX path validation failed: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate         at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)         at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:320)         at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263)         at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:258)         at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1332)         at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1207)         at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1150)         at java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392)         at java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:443)         at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1061)         at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1048)         at java.base/java.security.AccessController.doPrivileged(Native Method)         at java.base/sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:995)         at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.handshake(SslFactory.java:360)         at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.validate(SslFactory.java:301)         at org.apache.kafka.common.security.ssl.SslFactory$SslEngineValidator.validate(SslFactory.java:282)         at org.apache.kafka.common.security.ssl.SslFactory.configure(SslFactory.java:98)         at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:168)         at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:157)         at org.apache.kafka.common.network.ChannelBuilders.serverChannelBuilder(ChannelBuilders.java:97)         at kafka.network.Processor.(SocketServer.scala:724)         at kafka.network.SocketServer.newProcessor(SocketServer.scala:367)         at kafka.network.SocketServer.$anonfun$addDataPlaneProcessors$1(SocketServer.scala:252)         at kafka.network.SocketServer.addDataPlaneProcessors(SocketServer.scala:251)         at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1(SocketServer.scala:214)         at kafka.network.SocketServer.$anonfun$createDataPlaneAcceptorsAndProcessors$1$adapted(SocketServer.scala:211)         at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)         at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)         at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)         at kafka.network.SocketServer.createDataPlaneAcceptorsAndProcessors(SocketServer.scala:211)         at kafka.network.SocketServer.startup(SocketServer.scala:122)         at kafka.server.KafkaServer.startup(KafkaServer.scala:242)         at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44)         at kafka.Kafka$.main(Kafka.scala:82)         at kafka.Kafka.main(Kafka.scala) Caused by: sun.security.validator.ValidatorException: PKIX path validation failed: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate         at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:369)         at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:263)         at java.base/sun.security.validator.Validator.validate(Validator.java:264)         at java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313)         at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:276)         at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:141)         at java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1310)         ... 30 more Caused by: sun.security.validator.ValidatorException: TrustAnchor with subject ""CN=SystemTestCA"" is not a CA certificate         at java.base/sun.security.validator.PKIXValidator.verifyTrustAnchor(PKIXValidator.java:393)         at java.base/sun.security.validator.PKIXValidator.toArray(PKIXValidator.java:333)         at java.base/sun.security.validator.PKIXValidator.doValidate(PKIXValidator.java:366)         ... 36 more ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2020-02-13T15:16:47Z","2020-02-17T10:01:44Z"
"","7862","KAFKA-9246:Update Heartbeat timeout when ConsumerCoordinator commit offset","I misoperated my git and need to reopen a PR(the original one is #7761).  the `GroupCoordinator` will renew HeartbeatExpiration after receiving `CommitOffsetReq`, so `ConsumerCoordinator` can also renew Heartbeat after sending `CommitOffsetReq`.  However, the `ConsumerCoordinator` depends on heartbeats to detect `PreparingRebalance` and rejoin but the `commitOffsetReq` will generally not return an `Errors.ReblanceInProgress`  when `PreparingRebalance`. So we just add a `ReblanceInProgress` field to the `CommitOffsetResponse` to indicate whether the `GroupCoordinator` is `PreparingRebalance`.  below is my implementation steps: 1. bump the protocol version of `CommitOffsetReq` and `CommitOffsetResp` and add a field to `ReblanceInProgress` field to the `CommitOffsetResp`. 2. add parameter to `GroupCoordinator.doCommitOffsets` to identify whether should we respond `ReblanceInProgress`; 3. when in `PreparingRebalance`, `GroupCoordinator` just act as before but add a `ReblanceInProgress` to the response; 4. client will interpret it intelligently.  and the interaction is as follows: 1. if an OLD client commits offset, it just acts as previous, and the broker will also act as previous; 2. if a new client commits offset, it will update heartbeat, the broker may respond a  `ReblanceInProgress`, the client also has the logic to handle it.  If this works well, we can also add `update Heartbeat logic` to `FetchOffsetReq` and `TxnOffsetCommitReq`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2019-12-23T09:50:15Z","2020-10-23T01:49:12Z"
"","7933","KAFKA-9088: InternalProcessorContext mock builder [partial]","I made a simpler version of a builder for mocking `InternalProcessorContext`.   ref: [7718](https://github.com/apache/kafka/pull/7718)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pierDipi","2020-01-11T20:42:36Z","2020-02-16T23:44:05Z"
"","7519","KAFKA-4222 / KAFKA-8700 / KAFKA-5566: Wait for state to transit to RUNNING upon start","I looked into the logs of the above tickets, and I think for a couple fo them it is due to the fact that the threads takes time to restore, or just stabilize the rebalance since there are multi-threads. Adding the hook to wait for state to transit to RUNNING upon starting.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-15T06:24:03Z","2020-04-24T23:58:25Z"
"","8264","MINOR: double -Xss setting from 2m to 4m in build.gradle","I have seen an increased incidence in StackOverflowError(s) when compiling scala. This change doubles the max stack size to 4m.  ``` > Task :core:compileScala FAILED FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':core:compileScala'. > java.lang.StackOverflowError (no error message) ```","closed","","lbradstreet","2020-03-10T16:41:39Z","2020-03-17T16:12:41Z"
"","8437","KAFKA-7965 (part-1): Fix one case which makes ConsumerBounceTest#testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup flaky","I have been investigating `ConsumerBounceTest#testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup` during the last week. I have identified two cases that makes it fail from times to times, especially under high resource constraints. This PR explains and propose a fix for the first case.  In a nutshell, two consumers are kicked out of the group because of the preferred leader election: - When the group is loaded by Group Coordinator 0 which has the limit in place, it triggers a rebalance to kick out a consumer from the group. - A first member is kicked out while it re-joins the group. - Before the rebalance completes, the controller moves the leader from 0 to 1. Therefore, the group is loaded by Group Coordinator 1 and unloaded by Group Coordinator 0. The loaded group has still all members because the rebalance did not complete, therefore it triggers another rebalance to kick out a member from the group.   - A second member is kicked out while it re-joins the group.  The `ConsumerAssignmentPoller` stop themselves when an exception is raised and they report the exception. Therefore the test fails because two consumers have been kicked out from the group where it expects only one to be kicked out.  ``` assertEquals(1, raisedExceptions.size) assertTrue(raisedExceptions.head.isInstanceOf[GroupMaxSizeReachedException]) ```  To mitigate this, I propose to disable the `AutoLeaderRebalanceEnableProp` for all the tests in `ConsumerBounceTest`. It makes things unpredictable and therefore increase the ricks of flakiness.  **I haven't been able to get this failure again with this fix. I have run the single test for 24+ hours in a while loop within a docker contain with limited resources to verify.**  Bellow, you can find the relevant traces captured when the test failed. ``` // Group is loaded in GroupCoordinator 0 // A rebalance is triggered because the group is over capacity [2020-04-02 11:14:33,393] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:33,406] INFO [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Discovered group coordinator localhost:40071 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:794) [2020-04-02 11:14:33,409] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,410] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,412] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:33,413] INFO [GroupCoordinator 0]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:33,431] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 28 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)  // A first consumer is kicked out of the group while trying to re-join [2020-04-02 11:14:33,449] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627) [2020-04-02 11:14:33,451] ERROR [daemon-consumer-assignment-2]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76) org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members. [2020-04-02 11:14:33,451] INFO [daemon-consumer-assignment-2]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66)  // Before the rebalance is completed, a preferred replica leader election kicks in and move the leader from 0 to 1 [2020-04-02 11:14:34,155] INFO [Controller id=0] Processing automatic preferred replica leader election (kafka.controller.KafkaController:66) [2020-04-02 11:14:34,169] INFO [Controller id=0] Starting replica leader election (PREFERRED) for partitions group-max-size-test-0,group-max-size-test-3,__consumer_offsets-0 triggered by AutoTriggered (kafka.controller.KafkaController:66)  // The group is loaded in GroupCoordinator 1 before completing the rebalance // Another rebalance is triggered because the group is still over capacity [2020-04-02 11:14:34,194] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-ceaab707-69af-4a65-8275-cb7db7fb66b3 at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-07077ca2-30e9-45cd-b363-30672281bacb at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,199] INFO Static member MemberMetadata(memberId=ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d, groupInstanceId=Some(null), clientId=ConsumerTestConsumer, clientHost=/127.0.0.1, sessionTimeoutMs=10000, rebalanceTimeoutMs=60000, supportedProtocols=List(range), ).groupInstanceId of group group-max-size-test loaded with member id ConsumerTestConsumer-5d359e65-1f11-43ce-874e-fddf55c0b49d at generation 1. (kafka.coordinator.group.GroupMetadata$:126) [2020-04-02 11:14:34,201] INFO [GroupCoordinator 1]: Loading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,202] INFO [GroupCoordinator 1]: Preparing to rebalance group group-max-size-test in state PreparingRebalance with old generation 1 (__consumer_offsets-0) (reason: Freshly-loaded group is over capacity (GroupConfig(10,1800000,2,0).groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets) (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,203] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager:66)  // Prefered leader election is completed [2020-04-02 11:14:34,235] INFO [Controller id=0] Partition __consumer_offsets-0 completed preferred replica leader election. New leader is 1 (kafka.controller.KafkaController:66)  // Group is unloaded from GroupCoordinator 0 [2020-04-02 11:14:34,237] INFO [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager:66) [2020-04-02 11:14:34,237] INFO [GroupCoordinator 0]: Unloading group metadata for group-max-size-test with generation 1 (kafka.coordinator.group.GroupCoordinator:66) [2020-04-02 11:14:34,238] INFO [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-0. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager:66)  // A second consumer is kicked out of the group while trying to re-join [2020-04-02 11:14:34,252] ERROR [Consumer clientId=ConsumerTestConsumer, groupId=group-max-size-test] Attempt to join group failed due to fatal error: The consumer group has reached its max size. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:627) [2020-04-02 11:14:34,254] ERROR [daemon-consumer-assignment-1]: Error due to (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:76) org.apache.kafka.common.errors.GroupMaxSizeReachedException: Consumer group group-max-size-test already has the configured maximum number of members. [2020-04-02 11:14:34,254] INFO [daemon-consumer-assignment-1]: Stopped (kafka.api.AbstractConsumerTest$ConsumerAssignmentPoller:66) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-04-07T08:08:51Z","2020-04-28T07:38:10Z"
"","8150","KAFKA-9587: Producer configs are omitted in the documentation","I found this glitch while working on another issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-02-21T04:54:38Z","2020-10-14T05:30:02Z"
"","7891","MINOR: Group KafkaController, ReplicaManager metrics in documentation","I found these glitches while working on [KAFKA-9327](https://github.com/apache/kafka/pull/7890)  As of now, `KafkaController` and `ReplicaManager` metrics in documentation are partitioned; this commit merges them by group.  cc/ @gwenshap  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-01-03T05:50:25Z","2021-04-30T19:50:37Z"
"","7834","MINOR: Removed accidental double negation in error message.","I believe the error message changed in this commit is displayed when tasks or connectors are revoked while an unresolved rebalance is in progress.  As during such a time the connectors and tasks are stopped anyway that step is skipped and this info message displayed, which in that case due to the double negation is incorrect.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-12-16T17:36:43Z","2020-01-16T08:00:24Z"
"","8385","KAFKA-9786 fix flaky MetricsTest.testGeneralBrokerTopicMetricsA…","https://issues.apache.org/jira/browse/KAFKA-9786  ``` java.lang.AssertionError: expected:<18> but was:<23> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at kafka.metrics.MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered(MetricsTest.scala:108) ```  As gradle may use same JVM to run multiples test (see https://docs.gradle.org/current/dsl/org.gradle.api.tasks.testing.Test.html#org.gradle.api.tasks.testing.Test:forkEvery), the metrics from other tests can break ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered```.   ```   private def topicMetrics(topic: Option[String]): Set[String] = {     val metricNames = KafkaYammerMetrics.defaultRegistry.allMetrics().keySet.asScala.map(_.getMBeanName)     filterByTopicMetricRegex(metricNames, topic)   } ```  ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered``` is not captured by QA since the test which leaves orphan metrics in JVM is ```ReplicaManagerTest``` and it belongs to ```integrationTest```. By contrast, ```MetricsTest``` is a part of ```unitTest```. Hence, they are NOT executed by same JVM (since they are NOT in the same task). ```MetricsTest.testGeneralBrokerTopicMetricsAreGreedilyRegistered``` fails frequently on my jenkins because my jenkins verify kafka by running ""./gradlew clean core:test"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-30T09:30:26Z","2020-06-19T06:40:11Z"
"","8287","KAFKA-9711 The authentication failure caused by SSLEngine#beginHandsh…","https://issues.apache.org/jira/browse/KAFKA-9711  ```java  @Override     public void handshake() throws IOException {         if (state == State.NOT_INITALIZED)             startHandshake(); // this line         if (ready())             throw renegotiationException();         if (state == State.CLOSING)             throw closingException(); ```  SSLEngine#beginHandshake is possible to throw authentication failures (for example, no suitable cipher suites) so we ought to catch SSLException and then convert it to SslAuthenticationException so as to process authentication failures correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-12T14:24:37Z","2020-03-25T01:45:06Z"
"","8224","KAFKA-9704: Fix the issue z/OS won't let us resize file when mmap.","https://issues.apache.org/jira/browse/KAFKA-9704  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","zshuo","2020-03-04T22:49:27Z","2020-04-24T14:24:15Z"
"","8212","KAFKA-9642: Change ""BigDecimal(double)"" constructor to ""BigDecimal.valueOf(double)""","https://issues.apache.org/jira/browse/KAFKA-9642 for this reason, I modified the code that is using the ""BigDecimal(double)"" constructor.","closed","connect,","SoontaekLim","2020-03-03T22:06:14Z","2020-04-09T17:22:24Z"
"","8119","KAFKA-9558: Fixing retry logic for getListOffsetsCalls","https://issues.apache.org/jira/browse/KAFKA-9558  This PR is to fix the retry logic for `getListOffsetsCalls`. Previously, if there were partitions with errors, it would only pass in the current call object to retry after a metadata refresh. However this is incorrect as if there's a leader change, the call object never gets updated with the correct leader node to query. This PR fixes this by making another call to `getListOffsetsCalls` with only the error topic partitions as the next calls to be made after the metadata refresh. In addition there is an additional test to test the scenario where a leader change occurs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","skaundinya15","2020-02-14T22:10:45Z","2020-02-19T17:57:37Z"
"","8085","KAFKA-9556: Fix two issues with KIP-558 and expand testing coverage","https://issues.apache.org/jira/browse/KAFKA-9556  Correct the Connect worker logic to properly disable the new topic status (KIP-558) feature when `topic.tracking.enable=false`, and fix automatic topic status reset after a connector is deleted.  Also adds new `ConnectorTopicsIntegrationTest` and expanded unit tests.  This should be backported only to the `2.5` branch, which is when the feature was introduced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2020-02-11T01:06:21Z","2020-10-16T05:53:50Z"
"","8094","KAFKA-9541:Flaky Test DescribeConsumerGroupTest#testDescribeGroupMembersWithShortInitializationTimeout","https://issues.apache.org/jira/browse/KAFKA-9541  Occasionally the captured exception is DisconnectedException instead of TimeoutException. That might be due to an unexpected long pause that caused the node disconnection.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2020-02-12T04:30:30Z","2020-02-23T18:40:22Z"
"","8163","KAFKA-9455; Consider using TreeMap for in-memory stores of Streams","https://issues.apache.org/jira/browse/KAFKA-9455  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","highluck","2020-02-25T05:29:16Z","2020-05-29T19:43:39Z"
"","7929","KAFKA-9393: DeleteRecords may cause extreme lock contention for large partition directories","https://issues.apache.org/jira/browse/KAFKA-9393 This PR avoids a performance issue with `DeleteRecords` when a partition directory contains high numbers of files. Previously, `DeleteRecords` would iterate the partition directory searching for producer state snapshot files. With this change, the iteration is removed in favor of keeping a 1:1 mapping between producer state snapshot file and segment file. A segment files corresponding producer state snapshot file is now deleted when the segment file is deleted.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2020-01-10T19:03:54Z","2020-10-09T21:25:02Z"
"","7860","KAFKA-9322: Add `tail -n` feature for ConsoleConsumer","https://issues.apache.org/jira/browse/KAFKA-9322  When debugging, it will be convenient to quickly check the last N messages for a partition using ConsoleConsumer. Currently `offset` could not be negative except -1 and -2. However, we could simply break this rule to support `tail -n` feature.  Slightly different, with this patch applied, -1 means the last one message instead of the latest offset, -2 means the last two messages intead of the earliest offset. You could use `latest` and `earliest` explicitly if you really start reading from those positions.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2019-12-23T03:02:30Z","2020-01-21T03:29:45Z"
"","7797","KAFKA-9286: Optimize RecordAccumulator","https://issues.apache.org/jira/browse/KAFKA-9286  * For producer, the `Sender` [call](https://github.com/apache/kafka/blob/ba365bbb8deb16ec133e0e7983c1c56cef0152aa/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L342) `RecordAccumulator#ready` to collect all ready nodes. Then it [call](https://github.com/apache/kafka/blob/ba365bbb8deb16ec133e0e7983c1c56cef0152aa/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L369) `RecordAccumulator#drain` to iterate all ready nodes's topic partition to check if there is data to send.   This process is redundancy. When we collect all ready nodes, its ready topic partition can also be collected.  In our case, the producer only produce data to part(about 20) of all partitions(1024), with this patch, the performance can be increased ~3.4%.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","louxiu","2019-12-07T09:34:25Z","2020-05-18T02:52:16Z"
"","7826","Kafka-9275: Update ""reason"" string for rebalance for faster investigation when rebalance","https://issues.apache.org/jira/browse/KAFKA-9275 update ""reason"" string for rebalance of: 1. error when storing group assignment during SyncGroup 2. on member LeaveGroup 3. Updating metadata for member 4. Adding new member 5. on heartbeat expiration  Add clientId and clientHost information","closed","","dengziming","2019-12-12T04:28:01Z","2020-11-06T09:19:26Z"
"","7798","Kafka-9275: Update ""reason"" string for rebalance for faster investigation when rebalance","https://issues.apache.org/jira/browse/KAFKA-9275 update ""reason"" string for rebalance of: 1. error when storing group assignment during SyncGroup 2. on member LeaveGroup 3. Updating metadata for member 4. Adding new member 5. on heartbeat expiration  Add clientId and clientHost information","closed","","dengziming","2019-12-07T09:48:43Z","2019-12-12T04:28:07Z"
"","7870","KAFKA-9254: Overridden topic configs are reset after dynamic default change","https://issues.apache.org/jira/browse/KAFKA-9254  Currently, when dynamic broker config is updated, the log config will be recreated with an empty overridden configs. In such case, when updating dynamic broker configs a second round, the topic-level configs will lost.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-12-25T08:19:56Z","2020-01-24T22:47:24Z"
"","7761","KAFKA-9246: Update Heartbeat timeout when ConsumerCoordinator commit offset","https://issues.apache.org/jira/browse/KAFKA-9246 As proposed in KAFKA-3470, GroupCoordinator will invoke completeAndScheduleNextHeartbeatExpiration to update the expires when handleCommitOffsets, so the consumer can also update the heartbeat timeout of ConsumerCoordinator when sending OffsetCommitRequest to reduce Heartbeat Request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2019-11-28T13:07:41Z","2019-12-23T09:51:48Z"
"","7736","KAFKA-9202: serde in ConsoleConsumer with access to headers","https://issues.apache.org/jira/browse/KAFKA-9202  The Deserializer interface has two methods, one that gives access to the headers and one that does not. ConsoleConsumer.scala only calls the latter method. It would be nice if it were to call the default method that provides header access, so that custom serde that depends on headers becomes possible.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-11-22T03:08:55Z","2019-12-28T23:43:06Z"
"","7711","KAFKA-9157: Avoid generating empty segments if all records are deleted after cleaning","https://issues.apache.org/jira/browse/KAFKA-9157  If all records are deleted after cleaning, we should avoid generating empty log segments.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2019-11-19T09:39:39Z","2020-10-12T09:13:02Z"
"","7655","KAFKA-9150: DescribeGroup uses member assignment as metadata","https://issues.apache.org/jira/browse/KAFKA-9150  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-11-06T09:16:39Z","2019-11-06T09:31:18Z"
"","7590","KAFKA-9093: NullPointerException in KafkaConsumer with group.instance.id","https://issues.apache.org/jira/browse/KAFKA-9093  `log` in KafkaConsumer does not get initialized if an invalid value for `group.intance.id` is given during consumer construction.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-10-24T09:45:38Z","2019-11-01T06:59:22Z"
"","7619","KAFKA-9069: Flaky Test AdminClientIntegrationTest.testCreatePartitions","https://issues.apache.org/jira/browse/KAFKA-9069  Make `getTopicMetadata` in AdminClientIntegrationTest always read metadata from controller to get a consistent view.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-10-31T03:02:36Z","2019-11-29T15:37:54Z"
"","8161","KAFKA-9047: Making AdminClient group operations respect retries and backoff","https://issues.apache.org/jira/browse/KAFKA-9047  Previously, `AdminClient` group operations did not respect a `Call`'s number of configured tries and retry backoff. This could lead to tight retry loops that put a lot of pressure on the broker. This PR introduces fixes that ensures for all group operations the `AdminClient` respects the number of tries and the backoff a given `Call` has.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","skaundinya15","2020-02-24T21:10:22Z","2020-03-18T19:21:12Z"
"","7948","KAFKA-9042: Auto infer external topic partitions in stream reset tool","https://issues.apache.org/jira/browse/KAFKA-9042  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","highluck","2020-01-13T16:43:13Z","2020-06-12T23:01:12Z"
"","7947","KAFKA-9042: Auto infer external topic partitions in stream reset tool","https://issues.apache.org/jira/browse/KAFKA-9042  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-13T16:30:58Z","2020-01-13T16:57:42Z"
"","7618","KAFKA-9025: ZkSecurityMigrator not working with zookeeper chroot","https://issues.apache.org/jira/browse/KAFKA-9025  If a chroot is configured, ZkSecurityMigrator should prompt a confirm to user to ensure whether chroot is specified correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-10-31T02:25:32Z","2019-12-10T08:25:07Z"
"","8410","MINOR: Remove redundant braces from log message in FetchSessionHandler","https://issues.apache.org/jira/browse/KAFKA-8889 attempted to fill in the missing stacktrace in the log message when handling errors in `FetchSessionHandler#handleError`  But the fix is not effective without KAFKA-7016  The current fix removes the redundant pair of braces {} at the end of the log message. If and when the Throwable that is passed as argument to this method has a stacktrace, the log message will include it. Currently it doesn't because the Throwable argument does not have a stacktrace.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2020-04-03T02:51:46Z","2020-04-04T20:47:30Z"
"","8022","KAFKA 8382: Add TimestampedSessionStore","https://issues.apache.org/jira/browse/KAFKA-8382  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-30T16:59:52Z","2020-07-07T18:31:00Z"
"","8382","KAFKA-9685: PT2, avoid unnecessary set creation in ACL matching","https://github.com/apache/kafka/pull/8261 went a long way to solving some of the ACL performance issues. I don't think we need to create sets at all for the `find` and `isEmpty` calls. ` testAuthorizer` is 22% to 62% of the cost after this change:  ``` Before: Benchmark                                (aclCount)  (resourceCount)  Mode  Cnt   Score    Error  Units AclAuthorizerBenchmark.testAclsIterator           5             5000  avgt   15   0.430 ±  0.004  ms/op AclAuthorizerBenchmark.testAclsIterator           5            10000  avgt   15   0.980 ±  0.007  ms/op AclAuthorizerBenchmark.testAclsIterator           5            50000  avgt   15  11.191 ±  0.032  ms/op AclAuthorizerBenchmark.testAclsIterator          10             5000  avgt   15   0.880 ±  0.007  ms/op AclAuthorizerBenchmark.testAclsIterator          10            10000  avgt   15   2.642 ±  0.029  ms/op AclAuthorizerBenchmark.testAclsIterator          10            50000  avgt   15  26.361 ±  0.242  ms/op AclAuthorizerBenchmark.testAclsIterator          15             5000  avgt   15   1.655 ±  0.024  ms/op AclAuthorizerBenchmark.testAclsIterator          15            10000  avgt   15   5.276 ±  0.041  ms/op AclAuthorizerBenchmark.testAclsIterator          15            50000  avgt   15  40.702 ±  0.574  ms/op AclAuthorizerBenchmark.testAuthorizer             5             5000  avgt   15   0.202 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer             5            10000  avgt   15   0.233 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer             5            50000  avgt   15   0.424 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10             5000  avgt   15   0.202 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10            10000  avgt   15   0.253 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10            50000  avgt   15   0.423 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15             5000  avgt   15   0.198 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15            10000  avgt   15   0.242 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15            50000  avgt   15   0.391 ±  0.002  ms/op JMH benchmarks done  After: Benchmark                                (aclCount)  (resourceCount)  Mode  Cnt   Score    Error  Units AclAuthorizerBenchmark.testAclsIterator           5             5000  avgt   15   0.504 ±  0.164  ms/op AclAuthorizerBenchmark.testAclsIterator           5            10000  avgt   15   1.038 ±  0.271  ms/op AclAuthorizerBenchmark.testAclsIterator           5            50000  avgt   15  11.767 ±  0.028  ms/op AclAuthorizerBenchmark.testAclsIterator          10             5000  avgt   15   0.827 ±  0.016  ms/op AclAuthorizerBenchmark.testAclsIterator          10            10000  avgt   15   2.801 ±  0.027  ms/op AclAuthorizerBenchmark.testAclsIterator          10            50000  avgt   15  26.157 ±  0.191  ms/op AclAuthorizerBenchmark.testAclsIterator          15             5000  avgt   15   1.814 ±  0.053  ms/op AclAuthorizerBenchmark.testAclsIterator          15            10000  avgt   15   5.420 ±  0.065  ms/op AclAuthorizerBenchmark.testAclsIterator          15            50000  avgt   15  41.372 ±  0.659  ms/op AclAuthorizerBenchmark.testAuthorizer             5             5000  avgt   15   0.064 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer             5            10000  avgt   15   0.070 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer             5            50000  avgt   15   0.240 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10             5000  avgt   15   0.055 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10            10000  avgt   15   0.084 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            10            50000  avgt   15   0.249 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15             5000  avgt   15   0.057 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15            10000  avgt   15   0.084 ±  0.001  ms/op AclAuthorizerBenchmark.testAuthorizer            15            50000  avgt   15   0.243 ±  0.001  ms/op ```","closed","","lbradstreet","2020-03-28T23:42:14Z","2020-03-30T17:41:05Z"
"","7643","HOTFIX: fix bug in VP test where it greps for the wrong log message","https://github.com/apache/kafka/pull/7642 but targeted at 2.3 due to metadata version change.  Needs to be cherry-picked back to 2.1","closed","tests,","ableegoldman","2019-11-04T22:46:23Z","2019-11-08T03:17:20Z"
"","8030","KAFKA-9492; Ignore record errors in ProduceResponse for older versions","https://github.com/apache/kafka/commit/f41a5c2c8632bfd0dc50321c1c69418db04f42f6 caused a regression in version compatibility. We populate record errors in ProduceResponse regardless of the request version.  ProduceResponse.toStruct(version) was attempting to populate `record_errors` if errors were present without checking if the version supported this. This PR adds a version check.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-02-01T16:57:21Z","2020-02-04T17:01:10Z"
"","8432","MINOR: Fix KafkaApis.filterAuthorized","https://github.com/apache/kafka/commit/90bbeedf52f4b6a411e9630dd132583afa4cd428 introduced a regression resulting in passing an action per resource name to the `Authorizer` instead of passing one per unique resource name. This PR refactors the signatures of both `filterAuthorized` and `authorize` to make them easier to test and adds a test for each.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-04-06T14:38:29Z","2020-08-11T06:50:57Z"
"","8422","KAFKA-9820: validateMessagesAndAssignOffsetsCompressed allocates unused iterator","https://github.com/apache/kafka/commit/3e9d1c1411c5268de382f9dfcc95bdf66d0063a0 introduced skipKeyValueIterator(s) which were intended to be used, but in this case were created but were not used in offset validation.  A subset of the benchmark results follow. Looks like a 20% improvement in validation performance and a 40% reduction in garbage allocation for 1-2 batch sizes.  **# Parameters: (bufferSupplierStr = NO_CACHING, bytes = RANDOM, compressionType = LZ4, maxBatchSize = 1, messageSize = 1000, messageVersion = 2)**  Before: Result ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation"":   64851.837 ±(99.9%) 944.248 ops/s [Average]                 (min, avg, max) = (64505.317, 64851.837, 65114.359), stdev = 245.218   CI (99.9%): [63907.589, 65796.084] (assumes normal distribution)                                                                                                      ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm"":   164088.003 ±(99.9%) 0.004 B/op [Average]                                                                                    (min, avg, max) = (164088.001, 164088.003, 164088.004), stdev = 0.001   CI (99.9%): [164087.998, 164088.007] (assumes normal distribution)  After:  Result ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation"":                                         78910.273 ±(99.9%) 707.024 ops/s [Average]                                                                                  (min, avg, max) = (78785.486, 78910.273, 79234.007), stdev = 183.612                                                        CI (99.9%): [78203.249, 79617.297] (assumes normal distribution)                                         ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm"":                                                                                                                                      96440.002 ±(99.9%) 0.001 B/op [Average]                                                                                     (min, avg, max) = (96440.002, 96440.002, 96440.002), stdev = 0.001                                                          CI (99.9%): [96440.002, 96440.003] (assumes normal distribution)      **# Parameters: (bufferSupplierStr = NO_CACHING, bytes = RANDOM, compressionType = LZ4, maxBatchSize = 2, messageSize = 1000, messageVersion = 2)**  Before: Result ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation"":                                         64815.364 ±(99.9%) 639.309 ops/s [Average]                                                                                  (min, avg, max) = (64594.545, 64815.364, 64983.305), stdev = 166.026                                                                                                                                                                                   CI (99.9%): [64176.056, 65454.673] (assumes normal distribution)                                                                                                                                                                                                                                                  ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm"":           163944.003 ±(99.9%) 0.001 B/op [Average]                                                                                    (min, avg, max) = (163944.002, 163944.003, 163944.003), stdev = 0.001                                                       CI (99.9%): [163944.002, 163944.004] (assumes normal distribution)                                       After: Result ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation"":   77075.096 ±(99.9%) 201.092 ops/s [Average]                 (min, avg, max) = (77021.537, 77075.096, 77129.693), stdev = 52.223   CI (99.9%): [76874.003, 77276.188] (assumes normal distribution)                                                                                                      ""org.apache.kafka.jmh.record.RecordBatchIterationBenchmark.measureValidation:·gc.alloc.rate.norm"":   96504.002 ±(99.9%) 0.003 B/op [Average]                                                                                     (min, avg, max) = (96504.001, 96504.002, 96504.003), stdev = 0.001   CI (99.9%): [96503.999, 96504.005] (assumes normal distribution)","closed","","lbradstreet","2020-04-04T00:06:35Z","2020-04-04T17:05:52Z"
"","7806","MINOR: Simplify the timeout logic to handle  protocol in Connect distributed system tests","Hopefully fixes a very flaky system test for `sessioned` protocol. The previous code checked explicitly for `compatible` and did not work with the latest `sessioned` protocol, and since this was to set the timeout it's simply easier to always set it for the longer time.  Should be backported to `2.4` and `2.3`.  Increasing the timeout (see #7789) was not sufficient.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-12-09T20:33:56Z","2020-10-16T05:51:00Z"
"","8308","MINOR: Update Scala to 2.12.11","Highlights: * Performance improvements in the ollections library: algorithmic improvements and changes to avoid unnecessary allocations. * Performance improvements in the compiler. * ASM was upgraded to 7.3.1, allowing the optimizer to run on JDK 13+.  Full release notes: https://github.com/scala/scala/releases/tag/v2.12.11  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-03-17T07:16:25Z","2020-03-18T16:33:19Z"
"","7991","MINOR Update config validate help message to point to the correct config validation endpoint","Hi,  When we submit a bad connector configuration we are asked to check it against `/connector-plugins/{connectorType}/config/validate` while it's `/connector-plugins/{connectorType}/config/validate`   This PR is to be consistent with the doc : https://kafka.apache.org/documentation/#connect_rest   StandaloneHerderTest OK on my local installation   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","nicolasguyomar","2020-01-21T14:34:06Z","2020-03-22T20:39:46Z"
"","7716","Group and Txn metadata topics should be queried directly from the controller","GroupMetadataManager and TransactionStateManager should use a direct broker-to-controller channel to query the number of partitions instead of relying on Zookeeper.  This change introduces a new class that always sends the request to the active controller. In case the cached controller isn't available or not the controller it closes the connection and tries to refresh itself from the local metadataCache until it finds the active controller.  `BrokerToControllerMetadataManager` manages the request queue that is consumed by the request thread and also controls its lifecycle. Lazy initialization is used as the means of creating the thread so it won't try to create it before there is an actual need for it. The public methods of this class supposed to implement the high level functions that are queried by various classes (in this case `GroupMetadataManager` and `TransactionStateManager`) and return `KafkaFuture` so that the users of this class can work asynchronously over a blocking connection that the `BrokerToControllerRequestThread` implements.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-11-19T22:19:48Z","2020-04-30T09:06:53Z"
"","7497","MINOR: Increase spotBugs max heap to 2 GB","Gradle 5.0 has reduced the heap size for workers and there have been reports of `GC overhead limit exceeded` errors.  Also see: https://discuss.gradle.org/t/spotbugsplugin-with-gradle-5-0-fails-gc-overhead-limit-exceeded/30461  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-11T04:41:29Z","2019-10-11T15:41:00Z"
"","7918","KAFKA-9188; Fix flaky test SslAdminClientIntegrationTest.testSynchronousAuthorizerAclUpdatesBlockRequestThreads","From the build failures in the JIRA, it looks like the test occasionally hits request timeout when running from Jenkins (I was able to recreate only with much smaller request timeouts). Since the test blocks requests threads while sending the ACL update requests, updated the test to tolerate timeouts and retry the request for that case. Added an additional check to verify that the requests threads are unblocked when the semaphore is released, ensuring that the timeout is not due to blocked threads.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-09T15:59:36Z","2020-01-10T11:23:25Z"
"","8039","KAFKA-9505: Only loop over topics-to-validate in retries","Found this bug from the repeated flaky runs of system tests, it seems to be long lurking but also would only happen if there are frequent rebalances / topic creation within a short time, which is exactly the case in some of our smoke system tests.  Also added a unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-02-05T00:08:26Z","2020-04-25T00:00:27Z"
"","7892","MINOR: Cleanup generic & throw syntax in connect","Found I was working with Kafka Connect.  - Use diamond syntax in generic - Remove unused throws - etc  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","twowinsh87","2020-01-03T08:33:08Z","2020-01-13T05:15:36Z"
"","7701","KAFKA-9198; Complete purgatory operations on receiving StopReplica","Force completion of delayed operations when receiving a StopReplica request. In the case of a partition reassignment, we cannot rely on receiving a LeaderAndIsr request in order to complete these operations because the leader may no longer be a replica. Previously when this happened, the delayed operations were left to eventually timeout.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-16T18:10:08Z","2019-11-18T23:11:43Z"
"","7729","MINOR: Rat should ignore generated directories","For some reason, PR builds are failing due to the `rat` license check even though it should ignore files included in `.gitignore`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-21T17:53:10Z","2019-11-21T18:52:00Z"
"","8400","KAFKA-9739: Fixes null key changing child node","For some context, when building a streams application, the optimizer keeps track of the key-changing operations and any repartition nodes that are descendants of the key-changer.  During the optimization phase (if enabled), any repartition nodes are logically collapsed into one.  The optimizer updates the graph by inserting the single repartition node between the key-changing node and its first child node.  This graph update process is done by searching for a node that has the key-changing node as one of its direct parents, and the search starts from the repartition node, going up in the parent hierarchy.   The one exception to this rule is if there is a merge node that is a descendant of the key-changing node, then during the optimization phase, the map tracking key-changers to repartition nodes is updated to have the merge node as the key.  Then the optimization process updates the graph to place the single repartition node between the merge node and its first child node.   The error in KAFKA-9739 occurred because there was an assumption that the repartition nodes are children of the merge node.  But in the topology from KAFKA-9739, the repartition node was a parent of the merge node.  So when attempting to find the first child of the merge node, nothing was found (obviously) resulting in `StreamException(Found a null keyChangingChild node for..)`  This PR fixes this bug by first checking that all repartition nodes for optimization are children of the merge node.  This PR includes a test with the topology from KAFKA-9739.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2020-04-01T00:03:50Z","2020-04-03T16:09:34Z"
"","7573","MINOR: Reset timer when all the buffer is drained and empty","For scenarios where the incoming traffic of all input partitions are small, there's a pitfall that the enforced processing timer is not reset after we have enforce processed ALL records. The fix itself is pretty simple: we just reset the timer when there's no buffered records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-10-21T22:12:42Z","2020-04-24T23:52:19Z"
"","8282","KAFKA-6145: add new assignment configs","For KIP-441 we intend to add 4 new configs:  1. assignment.acceptable.recovery.lag 2. assignment.balance.factor 3. assignment.max.extra.replicas 4. assignment.probing.rebalance.interval.ms  I think we should give them all a common prefix to make it clear they're related, but am open to better suggestions than just ""assignment""","closed","","ableegoldman","2020-03-11T22:43:50Z","2020-06-26T22:37:38Z"
"","7508","MINOR: Update authorization primitives in security.html","For Kafka 2.3 Add docs about ElectPreferredLeaders and IncrementalAlterConfigs authorizations   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-14T18:00:38Z","2020-02-06T16:45:02Z"
"","8445","KAFKA-9823: Remember the sent generation for the coordinator request","For join / sync / commit / heartbeat request, we would remember the sent generation in the created handler object, and then upon getting the error code, we could check whether the sent generation still matches the current generation. If not, it means that the member has already reset its generation or has participated in a new rebalance already. This means:  1. For join / sync-group request, we do not need to call reset-generation any more for illegal-generation / unknown-member. But we would still set the error since at a given time only one join/sync round-trip would be in flight, and hence we should not be participating in a new rebalance. Also for fenced instance error we still treat it as fatal since we should not be participating in a new rebalance, so this is still not expected.  2. For commit request, we do not set the corresponding error for illegal-generation / unknown-member / fenced-instance but raise rebalance-in-progress. For commit-sync it would be still thrown to user, while for commit-async it would be logged and swallowed.  3. For heartbeat request, we do not treat illegal-generation / unknown-member / fenced-instance errors and just consider it as succeeded since this should be a stale heartbeat which can be ignored.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-04-08T05:39:56Z","2020-04-24T23:44:43Z"
"","8467","MINOR: reduce allocations in log start and recovery checkpoints","For brokers with replica counts > 4000, allocations from logsByDir become substantial. logsByDir is called often by LogManager.checkpointLogRecoveryOffsets and LogManager.checkpointLogStartOffsets. The approach used is similar to the one from the checkpointHighwatermarks change in https://github.com/apache/kafka/pull/6741.  Are there better ways to structure out data structure to avoid creating logsByDir on demand for each checkpoint iteration? This micro-optimization will help as is, but if we can avoid doing this completely it'd be better.  JMH benchmark results: ``` Before: Benchmark                                                                      (numPartitions)  (numTopics)   Mode  Cnt        Score        Error   Units CheckpointBench.measureCheckpointLogStartOffsets                                             3          100  thrpt   15        2.233 ±      0.013  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3          100  thrpt   15      477.097 ±     49.731  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3          100  thrpt   15   246083.007 ±     33.052    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3          100  thrpt   15      475.683 ±     55.569  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3          100  thrpt   15   245474.040 ±  14968.328    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3          100  thrpt   15        0.001 ±      0.001  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3          100  thrpt   15        0.341 ±      0.268    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3          100  thrpt   15      129.000               counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3          100  thrpt   15       52.000                   ms CheckpointBench.measureCheckpointLogStartOffsets                                             3         1000  thrpt   15        0.572 ±      0.004  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3         1000  thrpt   15     1360.240 ±    150.539  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3         1000  thrpt   15  2750221.257 ±    891.024    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3         1000  thrpt   15     1362.908 ±    148.799  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3         1000  thrpt   15  2756395.092 ±  44671.843    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3         1000  thrpt   15        0.017 ±      0.008  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3         1000  thrpt   15       33.611 ±     14.401    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3         1000  thrpt   15      273.000               counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3         1000  thrpt   15      186.000                   ms CheckpointBench.measureCheckpointLogStartOffsets                                             3         2000  thrpt   15        0.266 ±      0.002  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3         2000  thrpt   15     1342.557 ±    171.260  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3         2000  thrpt   15  5877881.729 ±   3695.086    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3         2000  thrpt   15     1343.965 ±    186.069  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3         2000  thrpt   15  5877788.561 ± 168540.343    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3         2000  thrpt   15        0.081 ±      0.043  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3         2000  thrpt   15      351.277 ±    167.006    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3         2000  thrpt   15      253.000               counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3         2000  thrpt   15      231.000                   ms JMH benchmarks done  After: CheckpointBench.measureCheckpointLogStartOffsets                                             3          100  thrpt   15        2.809 ±     0.129  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3          100  thrpt   15      211.248 ±    25.953  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3          100  thrpt   15    86533.838 ±  3763.989    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3          100  thrpt   15      211.512 ±    38.669  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3          100  thrpt   15    86228.552 ±  9590.781    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3          100  thrpt   15       ≈ 10⁻³              MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3          100  thrpt   15        0.140 ±     0.111    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3          100  thrpt   15       57.000              counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3          100  thrpt   15       25.000                  ms CheckpointBench.measureCheckpointLogStartOffsets                                             3         1000  thrpt   15        1.046 ±     0.030  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3         1000  thrpt   15      524.597 ±    74.793  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3         1000  thrpt   15   582898.889 ± 37552.262    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3         1000  thrpt   15      519.675 ±    89.754  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3         1000  thrpt   15   576371.150 ± 55972.955    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3         1000  thrpt   15        0.009 ±     0.005  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3         1000  thrpt   15        9.920 ±     5.375    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3         1000  thrpt   15      111.000              counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3         1000  thrpt   15       56.000                  ms CheckpointBench.measureCheckpointLogStartOffsets                                             3         2000  thrpt   15        0.617 ±     0.007  ops/ms CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate                              3         2000  thrpt   15      573.061 ±    95.931  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.alloc.rate.norm                         3         2000  thrpt   15  1092098.004 ± 75140.633    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space                     3         2000  thrpt   15      572.448 ±    97.960  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Eden_Space.norm                3         2000  thrpt   15  1091290.460 ± 85946.164    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen                        3         2000  thrpt   15        0.010 ±     0.012  MB/sec CheckpointBench.measureCheckpointLogStartOffsets:·gc.churn.G1_Old_Gen.norm                   3         2000  thrpt   15       19.990 ±    24.407    B/op CheckpointBench.measureCheckpointLogStartOffsets:·gc.count                                   3         2000  thrpt   15      109.000              counts CheckpointBench.measureCheckpointLogStartOffsets:·gc.time                                    3         2000  thrpt   15       67.000                  ms JMH benchmarks done  ```  For the 2000 topic, 3 partition case, we see a reduction in normalized allocations from 5877881B/op to 1284190.774B/op, a reduction of 78%.  Some allocation profiles from a mid sized broker follow. I have seen worse, but these add up to around 3.8% on a broker that saw GC overhead in CPU time of around 30%. You could argue that this is relatively small, but it seems worthwhile for a low risk change. ![image](https://user-images.githubusercontent.com/252189/79058104-33e91d80-7c1e-11ea-99c9-0cf2e3571e1f.png) ![image](https://user-images.githubusercontent.com/252189/79058105-38add180-7c1e-11ea-8bfd-6e6eafb0c794.png)","closed","","lbradstreet","2020-04-12T01:03:03Z","2020-04-25T20:55:00Z"
"","8169","KAFKA-9610: Do not throw illegal state when remaining partitions are not empty","For `handleRevocation`, it is possible that previous onAssignment callback has cleaned up the stream tasks, which means no corresponding task could be found for given partitions. We should not throw here as this is expected behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-02-25T23:17:33Z","2020-02-26T22:29:19Z"
"","8046","MINOR: further InternalTopologyBuilder cleanup","Followup to KAFKA-7317 and KAFKA-9113, there's some additional cleanup we can do in InternalTopologyBuilder. Mostly refactors the subscription code to make the initialization more explicit and reduce some duplicated code in the update logic.   Also some minor cleanup of the `build` method","closed","","ableegoldman","2020-02-05T20:45:15Z","2020-02-10T22:08:38Z"
"","8194","KAFKA-3824 | MINOR | Trying to make autocommit behavior clear in the config s…","Following https://issues.apache.org/jira/browse/KAFKA-3824 and discussion that followed in Pull Request #1936, I think it would be even more useful to clarify the autocommit behaviour in the ConsumerConfig section itself.","closed","","amanullah92","2020-02-29T04:11:10Z","2020-04-06T14:11:33Z"
"","8470","KAFKA-8611 / Refactor KStreamRepartitionIntegrationTest","Follow-up PR on https://github.com/apache/kafka/pull/7170 that implements @mjsax 's [suggestions](https://github.com/apache/kafka/pull/7170#discussion_r406535443) regarding integration test parameters. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","lkokhreidze","2020-04-12T09:40:43Z","2020-04-15T12:13:59Z"
"","7956","KAFKA-9392; Clarify deleteAcls javadoc and add test for create/delete timing","Follow-on from PR #7911  to clarify the guarantee for deleteAcls and add a deterministic test to ensure we don't regress.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-14T13:13:24Z","2020-06-01T15:38:22Z"
"","8356","KAFKA-9441: Cleanup Streams metrics for removed task commit latency metrics","Follow up to #8218 (KIP-447) that remove usage of task commit latency sensor. However, the underlying metrics were still registered, even if unused. This PR does the follow up cleanup to remove the metrics entirely.  The thread level commit metrics are no rollups but tracked individually and thus are not affected.  Call for review @cadonna @abbccdda @guozhangwang","closed","kip,","mjsax","2020-03-25T23:59:29Z","2020-06-12T23:13:27Z"
"","7845","KAFKA-9310: Handle UnknownProducerId from RecordCollector.send","Follow on for #7748 .  Additionally handle synchronous UnknownProducerId in RecordCollectorImpl.send , which was overlooked in the prior PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-12-17T21:44:33Z","2019-12-21T18:30:43Z"
"","7853","Update introduction.html","Fixing small grammar issue with plural versions","closed","","HarshaLaxman","2019-12-19T01:41:04Z","2019-12-19T13:07:42Z"
"","8415","KAFKA-9806: __consumer_offsets is not created under insufficient ACLs","Fixing KAFKA-9806 bug: the __consumer_offsets topic gets created when a consumer try to consume on a new cluster for the first time. If there aren't sufficient cluster-level ACLs, FindCoordinator request succeed, anche __consumer_offsets gets created in zk, but subsequent UpdateMetadata and LeaderAndIsr requests fail, putting the offsets topic in a bad state. When this happens, it is not possible to consume even after updating ACLs (5a).  This PR aims to introduce a new check before creating the __consumer_offsets topic, so that it does not get created if there aren't sufficient ACLs.  This change was tested by following the steps reported in KAFKA-9806, and verifying that it is possible to consume after the code change (5b). Eg: 1. Create a topic 2. Set insufficient cluster level ACLs. Which precise ACL does not matter as long as ClusterAction initiated by administrative messages between brokers is denied.  3. Consume on the topic -> this request will fail with unauthorized errors on the client side as well 4. Remove ACLs or set ACLs to allow inter-broker communication 5a. (Before code change) Produce/consume on topic. Consumer won't be able to read any data. 5b. (After code change) Produce/consume on topic. Consumer are able to read data.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","paolo-moriello","2020-04-03T13:20:33Z","2020-04-09T15:49:58Z"
"","8230","KAFKA-9667: Connect JSON serde strip trailing zeros","Fixes: [KAFKA-9667](https://issues.apache.org/jira/browse/KAFKA-9667)  The Connect Json serde was recently enhanced to support serializing decimals as standard JSON numbers, (Original work done under [KAFKA-8595](https://issues.apache.org/jira/browse/KAFKA-8595)), e.g. `1.23`.  However, there is a bug in the implementation: it's stripping trailing zeros!  `1.23` is not the same as `1.230`.  The first is a number accurate to 2 decimal places, where as the later is accurate to 3 dp.  It is important that trailing zeros are not dropped when de(serializing) decimals.  For some use-cases it may be acceptable to drop the trailing zeros, but for others it definitely is not.  #### Current Functionality If a JSON object was to contain the number `1.230` then the Java JsonDeserializer would correctly deserialize this into a `BigDecimal`. The BigDecimal would have a scale of 3, which is correct.  However, if that same BigDecimal was then serialized back to JSON using the Java JsonSerializer it would incorrectly strip the zeros, serializing to `1.23`.   #### Expected Functionality When serializing, trailing zeros should be maintained.  For example, a BigDecimal such as `1.230`, (3 dp), should be serialized as `1.230`.   #### Compatibility Both the old serialized number, e.g. `1.23`, and the proposed corrected serialized number, e.g. `1.230`, are valid JSON numbers. Downstream consumers should have no issue deserializing either.  This is not super urgent, but would be good to get into the next point releases of 2.4 and 2.5.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","big-andy-coates","2020-03-05T17:40:40Z","2020-07-03T13:44:05Z"
"","7702","HOTFIX: Fix infinite loop in AbstractIndex.indexSlotRangeFor","Fixes regression from #5378 which causing an infinite loop in `binarySearch`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-17T06:33:45Z","2019-11-17T18:08:30Z"
"","8222","KAFKA-9650: include human readable units in ms and bytes configs","Fixes KAFKA-9650.","closed","","tombentley","2020-03-04T17:01:48Z","2020-05-14T18:56:15Z"
"","8205","KAFKA-9634: Add note about thread safety of ConfigProvider","Fixes KAFKA-9634.","closed","connect,","tombentley","2020-03-02T18:09:45Z","2020-03-22T20:39:33Z"
"","8204","KAFKA-9633: Ensure ConfigProviders are closed","Fixes KAFKA-9633.","closed","connect,","tombentley","2020-03-02T18:01:22Z","2020-10-23T05:50:34Z"
"","7731","KAFKA-9204: allow ReplaceField SMT to handle tombstone records","fixes https://issues.apache.org/jira/browse/KAFKA-9204  this PR allows the ReplaceField SMT to handle null values and null keys by just passing them through  Signed-off-by: Lev Zemlyanov   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","levzem","2019-11-21T22:27:08Z","2020-10-16T05:50:59Z"
"","7820","KAFKA-9293; Fix NPE in DumpLogSegments offsets parser and display tombstone keys","Fixes an NPE when UserData in a member's subscription is null. Also modifies the output logic so that we show the keys of tombstones for both group and transaction metadata. Also adds test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-12-11T22:56:17Z","2020-01-03T21:04:11Z"
"","8108","KAFKA-9533: ValueTransform forwards `null` values","Fixes a bug where `KStream#transformValues` would forward null values from the provided `ValueTransform#transform` operation.  A test was added for verification `KStreamTransformValuesTest#shouldEmitNoRecordIfTransformReturnsNull`. A parallel test for non-key ValueTransformer was not added, as they share the same code path.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mviamari","2020-02-13T17:51:14Z","2020-02-26T18:11:05Z"
"","7683","MINOR: fix flaky ConsumerBounceTest.testClose","Fixes `java.util.concurrent.ExecutionException: java.lang.AssertionError: Close finished too quickly 5999`.  The close test sets a close duration in milliseconds, but measures the time taken in nanoseconds. This leads to small error due to the resolution in each, where the close is deemed to have taken too little time.  When I measured the start and end with nanoTime, I found the time taken to close was `5999641566 ns (5999.6ms)` which seems close enough to be a resolution error. I've run the test 50 times and have not hit the ""Close finished too quickly"" issue again, whereas previously I hit a failure pretty quickly.","closed","","lbradstreet","2019-11-12T21:11:48Z","2019-11-14T22:58:06Z"
"","8233","KAFKA-9668: Iterating over KafkaStreams.getAllMetadata() results in ConcurrentModificationException","Fixes [KAFKA-9668](https://issues.apache.org/jira/browse/KAFKA-9668)  `KafkaStreams.getAllMetadata()` returns `StreamsMetadataState.getAllMetadata()`. All the latter methods is `synchronized` it returns a reference to internal mutable state.  Not only does this break encapsulation, but it means any thread iterating over the returned collection when the metadata gets rebuilt will encounter a `ConcurrentModificationException`.  This change:  * switches from clearing and rebuild `allMetadata` when `onChange` is called to building a new list and swapping this in. This is thread safe and has the benefit that the returned list is not empty during a rebuild: you either get the old or the new list.  * removes synchronisation from `getAllMetadata` and `getLocalMetadata`. These are returning member variables. Synchronisation adds nothing.  * changes `getAllMetadata` to wrap its return value in an unmodifiable wrapper to avoid breaking encapsulation.  * changes the getters in `StreamsMetadata` to wrap their return values in unmodifiable wrapper to avoid breaking encapsulation.  Unit tests have been added to cover both changes classes to ensure encapsulation and thread-safety are maintained.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","big-andy-coates","2020-03-05T22:24:58Z","2020-03-09T12:57:06Z"
"","7732","MINOR: Controller should process events without rate metrics","Fixes #7717, which did not actually achieve its intended effect. The event manager failed to process the new event because we disabled the rate metric, which it expected to be present.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-21T23:21:54Z","2019-11-22T05:58:47Z"
"","7914","KAFKA-9152; Improve Sensor Retrieval","Fixed to save only when there is no space for efficiency :)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","highluck","2020-01-09T01:55:47Z","2020-01-10T15:47:22Z"
"","7689","MINOR: fix typo in processor-api developer guide docs","Fixed a small typo on the Processor API page of the Kafka Streams developer guide docs. (""buildeer"" changed to ""builder"")  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","mgyarmathy","2019-11-13T22:43:18Z","2019-12-14T11:57:53Z"
"","7779","MINOR: Fix small typo in main README","Fixed a small typo in the main README file (""They are also are printed"" --> ""They are also printed"").  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","","bxji","2019-12-04T19:19:57Z","2019-12-18T15:19:18Z"
"","7636","CONFLUENT: Sync from apache/kafka (2 November 2019)","Fixed a minor conflict in `.gitignore` and fix compiler errors in `KafkaUtilities` due to `PartitionReplicaAssignment` rename to `ReplicaAssignment`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-03T05:06:16Z","2019-11-03T22:58:22Z"
"","7934","[MINOR]: Fix typos","Fix typos the word ""Visibilty"", I think the author means ""Visibility"", but the best is ""Visible""","closed","","dengziming","2020-01-12T06:10:33Z","2020-01-17T09:45:11Z"
"","8133","MINOR: Fix gradle error writing test stdout","Fix the following message we have been seeing in recent builds:  > ERROR: Failed to write output for test org.apache.kafka.streams.integration > .QueryableStateIntegrationTest.shouldBeAbleQueryStandbyStateDuringRebalance > java.lang.NullPointerException: Cannot invoke method write() on null object  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2020-02-18T17:46:20Z","2020-02-26T17:36:01Z"
"","8045","KAFKA-9506: Fix JmxReporter corner cases and optimize","Fix some inconsistencies in the case where multiple JmxReporter objects attempt to claim a single mbean.  Avoid holding a global lock when performing JMX operations.  Finally, handle close better. Some calling code continues performing operations after close, so close needs to clean up the internal state","open","","cmccabe","2020-02-05T18:44:58Z","2020-02-13T16:23:44Z"
"","7760","KAFKA-9156: fix LazyTimeIndex & LazyOffsetIndex concurrency","Fix of https://issues.apache.org/jira/browse/KAFKA-9156  Race condition in concurrent  `get` method invocation of lazy indexes might lead to multiple `OffsetIndex`/`TimeIndex` objects being concurrently created. When that happens position of `MappedByteBuffer` in `AbstractIndex` is advanced to the last entry which in turn leads to a critical `BufferOverflowException` being thrown whenever leader or replica tries to append a record to the segment.  Moreover, `file_=` setter is seemingly also vulnerable to the race, since multiple threads can attempt to set a new file reference as well as create new Time/OffsetIndex objects at the same time. This might lead to the discrepant File references being held by e.g. LazyTimeIndex and its TimeIndex counterpart.  This patch attempts to fix the issue by making sure that index objects are atomically constructed when loaded lazily via `get` method. Additionally, `file` reference modifications are also made atomic and thread safe.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","alexandrfox","2019-11-28T12:32:25Z","2019-12-03T09:52:19Z"
"","7688","KAFKA-9175: Update MirrorMaker 2 topic/partition metrics","Fix issue in `PartitionMetrics` which caused all topic-partition metrics to use the same sensors. Switched the byte and record metrics to `Meter` so they expose both a Rate and Count for consistency with other Kafka metrics (and monitoring).  Co-authored-by: Mickael Maison  Co-authored-by: Edoardo Comar    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-11-13T15:58:28Z","2019-11-13T20:54:43Z"
"","8450","KAFKA-7613: Enable -Xlint:rawtypes in clients, fixing warnings","Fix all existing javac warnings about use of raw types in clients add -Xlint:rawtypes to the clients the javac options in build.gradle. This addresses part of KAFKA-7613, but further work will be needed for the other warnings.","open","","tombentley","2020-04-08T15:01:58Z","2020-05-12T08:10:08Z"
"","8449","KAFKA-7613: Enable -Xlint:try, fixing warnings","Fix all existing javac warnings about try statements and remove `-Xlint:-try` from the javac options in build.gradle. This addresses part of KAFKA-7613, but further work will be needed for the other warnings.  A couple of `@SuppressWarnings(""try"")` were needed to avoid having to override `close()` on interfaces extending `AutoCloseable`. The problem with those is `AutoCloseable.close()` is declared to throw `Exception`, which allows `InterruptedException`, which generates a warning.  try-with-resources with an ignored variable causes a warning, and I favoured rewriting these rather than suppressing the warning.","open","","tombentley","2020-04-08T14:24:51Z","2020-07-03T12:25:54Z"
"","8406","MINOR: clean up Streams assignment classes and tests","First set of cleanup pushed to followup PR after KIP-441 Pt. 5. Main changes are:  1) Moved `RankedClient` and the static `buildClientRankingsByTask` to a new file 2) Moved `Movement` and the static `getMovements` to a new file (also renamed to `TaskMovement`) 3) Consolidated the many common variables throughout the assignment tests to the new `AssignmentTestUtils`  4) New utility to generate comparable/predictable UUIDs for tests, and removed the generic from `TaskAssignor` and all related classes  Keep in mind, this does not represent the sum whole of all cleanup we'd like to do. But if there's anything you had in mind that you feel would fit in with the other changes in this PR, feel free to point it out","closed","","ableegoldman","2020-04-02T04:01:35Z","2020-06-26T22:37:49Z"
"","8473","MINOR: avoid autoboxing in FetchRequest.PartitionData.equals","FetchRequest.PartitionData.equals unnecessarily uses Object.equals generating a lot of allocations due to boxing, even though primitives are being compared. This is shown in the allocation profile below. Note that the CPU overhead is negligble. ￼ ![image](https://user-images.githubusercontent.com/252189/79079019-46686300-7cc1-11ea-9bc9-44fd17bae888.png)","closed","","lbradstreet","2020-04-12T20:27:21Z","2020-04-15T15:31:20Z"
"","7983","MINOR:  remove Faulty test code","Faulty test code remove  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-19T08:46:45Z","2020-01-31T04:28:49Z"
"","7906","KAFKA-9383: Expose consumer group metadata","Expose consumer group metadata to empower transactional producer do offset commit with more credentials, such as generation.id, member.id and group.instance.id. The change happens essentially in ConsumerCoordinator where we maintain an internal reference of the group metadata and update it every time we are in a rejoin callback. The exposed struct is a deep-copy instead of the actual object living inside the ConsumerCoordinator to avoid impact from caller.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2020-01-08T01:39:07Z","2020-11-17T19:07:58Z"
"","7889","KAFKA-6614: configure internal topics with message.timestamp.type=CreateTime by default","Explicitly set the `message.timestamp.type` to `CreateTime` for internal topics so that brokers using `LogAppendTime` won't overwrite the timestamps set by Streams. This default can still be overridden by users who do want to use `LogAppendTime` for internal topics.  Note that this change will only take effect in apps that are new or have been reset, as we create topics with this configuration but don't alter the config for existing ones.","closed","streams,","ableegoldman","2020-01-03T02:34:15Z","2020-01-08T04:35:07Z"
"","8210","KAFKA-9640[WIP]: Allow use of RDMA for publishing, replicating, and consuming log entries","Experimental modification to Apache Kafka to use RDMA for publishing, replicating, and consuming log entries. Our evaluation of the resulting performance is ongoing, but we expect to see significant improvements in end-to-end latency, lower CPU utilization on the brokers, and greater scalability for consumers of a single topic partition.  This contribution is principally the work of Konstantin Taranov , formerly with Oracle Labs and currently at ETH Zurich.  This is a work in progress, not ready to be merged.","open","","stevebyan","2020-03-03T19:26:26Z","2020-03-03T19:26:26Z"
"","8321","MINOR: Use Exit.exit instead of System.exit in MM2","Exit.exit needs to be used in code instead of System.exit.   Particularly in integration tests using System.exit is disrupting because it exits the jvm process and does not only fail test correctly. Integration tests override procedures in Exit to protect against such cases.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2020-03-20T20:19:04Z","2020-03-21T00:10:30Z"
"","7687","KAFKA-9144; Track timestamp from txn markers to prevent early producer expiration","Existing producer state expiration uses timestamps from data records only and not from transaction markers. This can cause premature producer expiration when the coordinator times out a transaction because we drop the state from existing batches. This in turn can allow the coordinator epoch to revert to a previous value, which can lead to validation failures during log recovery. This patch fixes the problem by also leveraging the timestamp from transaction markers.   We also change the validation logic so that coordinator epoch is verified only for new marker appends. When replicating from the leader and when recovering the log, we only log a warning if we notice that the coordinator epoch has gone backwards. This allows recovery from previous occurrences of this bug.  Finally, this patch fixes one minor issue when loading producer state from the snapshot file. When the only record for a given producer is a control record, the ""last offset"" field will be set to -1 in the snapshot. We should check for this case when loading to be sure we recover the state consistently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-13T06:18:10Z","2020-01-09T21:47:27Z"
"","7500","Add ability to wait for all instances in an application to be RUNNING","Example output on failure:  ``` java.lang.AssertionError: Application did not reach a RUNNING state for all streams instances. Non-running instances: {org.apache.kafka.streams.KafkaStreams@77258e59=REBALANCING, org.apache.kafka.streams.KafkaStreams@22295ec4=REBALANCING}  	at org.junit.Assert.fail(Assert.java:89) 	at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning(IntegrationTestUtils.java:789) 	at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.standbyShouldNotPerformRestoreAtStartup(OptimizedKTableIntegrationTest.java:112) ```  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cpettitt-confluent","2019-10-11T19:30:52Z","2019-10-17T19:05:31Z"
"","8318","KAFKA-9451: Enable producer per thread for Streams EOS","Enabled producer per thread via KIP-447.  - add configs to enable EOS-beta and for a safe upgrade from older versions - for eos-beta, create a single shared producer and `StreamsProducer` over all tasks - when committing, commit all tasks  Call for review @abbccdda @guozhangwang","closed","kip,","mjsax","2020-03-19T20:08:44Z","2020-06-12T23:18:56Z"
"","7874","KAFKA-9294: Add tests for Named parameter","During the work on KIP-150, we encountered some inconsistencies with regard to `Named` parameter, ie, not all methods check for `null` (what should not be allowed).  Going over the code, and to make sure we don't forget any case, I reordered couple of methods to keep an overview (ie, same order in interface, implementation, and tests). I'll highlight the actual changes in the code.  Call for review @bbejeck @vvcephei","closed","streams,","mjsax","2019-12-30T00:15:03Z","2020-03-11T05:39:09Z"
"","7873","MINOR: JavaDoc cleanup","During the work on KIP-150, we encountered a couple of JavaDoc inconsistencies:  - different wording  - typos; incorrect/missing markup  - some classes/methods explain something, others don't  - refined some JavaDocs to be more precise  This PR tries to cleanup the JavaDocs accordingly.  Call for review @vvcephei @wcarlson5 @bbejeck","closed","streams,","mjsax","2019-12-27T23:51:13Z","2020-01-10T21:18:12Z"
"","8061","KAFKA-9517: Fix default serdes with FK join","During the KIP-213 implementation and verification, we neglected to test the code path for falling back to default serdes if none are given in the topology.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-02-07T19:30:45Z","2020-02-10T22:11:25Z"
"","8214","MINOR: Add an extra check in StreamThreadTest","During the handle-corruption logic, we first remove the partitions from the changelog reader (and hence from the restore consumer), and then add them back during the task.revive() function. During this period the test main thread may happen to call `addRecords` which could throw IllegalStateException: a race condition.  The fix here is to wait for a position() call to return 0, which means the partitions have been added back to the restore consumer, and the seek-to-beginning has been called as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","guozhangwang","2020-03-03T23:05:23Z","2020-04-24T23:51:58Z"
"","8054","KAFKA-9390: Make serde pseudo-topics unique","During the discussion for KIP-213, we decided to pass ""pseudo-topics"" to the internal serdes we use to construct the wrapper serdes for CombinedKey and hashing the left-hand-side value. However, during the implementation, this strategy wasn't fully implemented, and we wound up using the same topic name for a few different data types.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2020-02-06T20:57:26Z","2020-02-12T17:44:57Z"
"","8341","HOTFIX: replace log message in version probing test","During some refactoring we removed one of the log messages that the version probing system test looks for. Adds in a new log message and updates the test.","closed","tests,","ableegoldman","2020-03-24T20:56:17Z","2020-03-27T21:59:34Z"
"","7963","KAFKA-9235; Ensure transaction coordinator resigns after replica deletion","During a reassignment, it can happen that the current leader of a partition is demoted and removed from the replica set at the same time. In this case, we rely on the StopReplica request in order to stop replica fetchers and to clear the group coordinator cache. This patch adds similar logic to ensure that the transaction coordinator state cache also gets cleared.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-01-15T01:19:50Z","2020-01-16T21:50:25Z"
"","7945","KAFKA-8037: WIP Check deserialization on restoration of local state store from source topics","Draft implementation for local state store restoration similar to global state stores in https://github.com/apache/kafka/pull/7923  Currently without unit tests as the mocking and direct field access seem to be in conflict.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","pkleindl","2020-01-13T09:20:17Z","2020-01-13T09:20:17Z"
"","8228","KAFKA-9663: Doc some null returns in KafkaStreams","Documents the possibility of null return value in KafkaStreams.getMetadataForKey(), and KafkaStreams.queryMetadataForKey().  This fixes KAFKA-9663.","closed","streams,","tombentley","2020-03-05T12:00:52Z","2020-03-07T07:47:50Z"
"","8395","Added doc for KIP-535 and updated it for KIP-562","Documenting KIP-535 in the release notes and making some updates related to KIP-562.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","brary","2020-03-31T15:33:10Z","2020-06-04T00:16:25Z"
"","7516","KAFKA-8179: document upgrade path for KIP-429","Document the upgrade path for the consumer and for Streams (note that they differ significantly).  Needs to be cherry-picked to 2.4","closed","","ableegoldman","2019-10-15T01:54:07Z","2019-10-16T23:12:14Z"
"","8408","KAFKA-9810: Document Connect Root REST API on /","Document the supported endpoint at the top-level (root) REST API resource and the information that it returns when a request is made to a Connect worker.   Fixes an omission in documentation after KAFKA-2369 and KAFKA-6311 (KIP-238)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2020-04-02T18:21:59Z","2020-04-03T22:39:05Z"
"","7535","MINOR: Add documentation for foreign-key joins (KIP-213)","Docs for the KIP 213 change.  2 important notes: 1) I have put the KTable-KTable foreign key join in a separate section than the regular KTable Ktable join. It's fairly similar, but I have attempted to highlight the differences without getting into the underlying mechanics. Is this the correct approach? Or do I need to expose the mechanics too?  2) Can someone review the record + offset table at the end of the section to verify if it makes sense?   @guozhangwang @mjsax @vvcephei  not sure who else may be interested.","closed","docs,","bellemare","2019-10-16T20:54:56Z","2019-10-24T18:38:05Z"
"","7637","MINOR: return unmodifiableMap for PartitionStates.partitionStateMap","Discussion https://github.com/apache/kafka/pull/7576#issuecomment-549189987. Makes the map returned by partitionStateMap unmodifiable to prevent mutation.","closed","","lbradstreet","2019-11-04T04:17:56Z","2020-03-19T15:19:52Z"
"","8156","MINOR: fix ClassCastException handling","Detected this issue by chance. Seems to need a fix in older branches, too.  Originally, we put the code to handle class-cast-exception into `SinkNode` on purpose to have the error message only for sink topics (not for changelogs). But in hindsight, it seems better to pull it into the `RecordCollector` directly.  Call for review @ableegoldman @vvcephei","closed","streams,","mjsax","2020-02-22T08:35:55Z","2020-03-07T07:34:30Z"
"","7979","POC: Consolidate testing InternalProcessorContexts","Demonstrate the consolidation approach via: migrating KStreamSessionWindowAggregateProcessorTest to use MockInternalProcessorContext instead of InternalMockProcessorContext  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","vvcephei","2020-01-17T18:54:57Z","2020-01-17T19:16:41Z"
"","8002","KAFKA-9458 Fix windows clean log fail caused Runtime.halt","Delete or rename a file before close it is illegal in windows, so I changed the rename process to creating a status file and later we can compare the status and delete the segment files.   I have manually tested it on windows, please review it. though I haven't changed the test cases. If you're okay with this flow I can work on the test cases too.","closed","","hirik","2020-01-23T11:58:58Z","2020-03-11T10:11:07Z"
"","7903","Default partition grouper move internal","Default partition grouper moved to interanl packege  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-07T13:05:20Z","2020-01-10T16:17:48Z"
"","7902","KAFKA-7785 Default partition grouper to internal","Default partition grouper moved to interanl packege  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-07T12:56:47Z","2021-03-08T23:22:05Z"
"","8231","Set ""-Ywarn-unused"" compiler flags when using 2.12","Currently, when compiling using Scala 2.12, we do not get any unused warnings in the build stdout. The `-Xlint:unused` flag was added for Scala 2.13.  This change keeps the `-Xlint:unused` flag but also adds a set of `-Ywarn-unused` compiler flags to include unused warnings when compiling with Scala 2.12.  Example output from either setting `scala.version` to 2.12.10 (the default) or 2.13.1  ``` /kafka/core/src/main/scala/kafka/cluster/Partition.scala:303: parameter value replicaId in method createLog is never used   private[cluster] def createLog(replicaId: Int, isNew: Boolean, isFutureReplica: Boolean, offsetCheckpoints: OffsetCheckpoints): Log = {                                  ^ /kafka/core/src/main/scala/kafka/cluster/Partition.scala:481: parameter value controllerId in method makeLeader is never used   def makeLeader(controllerId: Int,                  ^ /kafka/core/src/main/scala/kafka/cluster/Partition.scala:483: parameter value correlationId in method makeLeader is never used                  correlationId: Int,                  ^ /kafka/core/src/main/scala/kafka/cluster/Partition.scala:552: parameter value controllerId in method makeFollower is never used   def makeFollower(controllerId: Int,                    ^ /kafka/core/src/main/scala/kafka/cluster/Partition.scala:554: parameter value correlationId in method makeFollower is never used                    correlationId: Int,                    ^ ```   References: * http://tpolecat.github.io/2017/04/25/scalac-flags.html * https://github.com/fthomas/refined/pull/280/files * https://docs.scala-lang.org/overviews/compiler-options/index.html","open","","mumrah","2020-03-05T18:06:39Z","2020-04-23T17:45:29Z"
"","8067","KAFKA-9254; Overridden topic configs are reset after dynamic default change (#7870)","Currently, when a dynamic change is made to the broker-level default log configuration, existing log configs will be recreated with an empty overridden configs. In such case, when updating dynamic broker configs a second round, the topic-level configs are lost. This can cause unexpected data loss, for example, if the cleanup policy changes from ""compact"" to ""delete.""  Reviewers: Rajini Sivaram , Jason Gustafson  (cherry picked from commit 0e7f867041959c5d77727c7f5ce32d363fa09fc2)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-02-08T00:22:11Z","2020-03-03T20:00:11Z"
"","8040","KAFKA-6607: Commit correct offsets for transactional input data","Currently, Kafka Streams commits ""offset + 1"" that may lead to incorrect ""consumer lag"" if the input topic is transactional, because the committed offset does ""step on"" the commit marker, instead of ""skipping it"".  With this PR, we commit ""offsetOfNextRecord"" or `consumer.position()` to step over potential transactional markers to fix this issue.  Call for review @guozhangwang @ableegoldman   This PR is against `2.5` branch on purpose to avoid conflict with the current Kafka Streams refactoring. After the refactoring is merged, we can port this PR to `trunk`.","closed","streams,","mjsax","2020-02-05T01:16:19Z","2020-02-11T22:00:16Z"
"","8270","KAFKA-9216: Enforce internal config topic settings for Connect workers during startup","Currently, if Kafka Connect will create its config backing topic with a fire and forget approach. This is fine unless someone has manually created that topic already with the wrong partition count.  In such a case Kafka Connect ""may"" run for some time. Especially if it's in standalone mode and once switched to distributed mode it will almost certainly fail.  To counter this I've added a check when the KafkaConfigBackingStore is starting. This check will throw a ConfigException if there is more than one partition in the backing store.  This exception is then caught upstream and logged by either: - class: DistributedHerder, method: run - class: ConnectStandalone, method: main  After a review I don't believe it impacts any other upstream code.  Finally, to supper this new functionality I've added a public method to KafkaBasedLog which returns the partition count and a variable to store this.  And, I've created a unit test in KafkaConfigBackingStoreTest to verify the behaviour.","closed","connect,","Evelyn-Bayes","2020-03-11T01:32:29Z","2020-06-07T19:42:01Z"
"","7620","KAFKA-8972 (2.4 blocker): TaskManager state should always be updated after rebalance","Currently when we identify version probing we return early from `onAssignment` and never get to updating the TaskManager and general state with the new assignment. Since we do actually give out ""real"" assignments even during version probing, a StreamThread should take real ownership of its tasks/partitions including cleaning them up in `onPartitionsRevoked` which gets invoked when we call `onLeavePrepare` as part of triggering the follow-up rebalance.  Every member will always get an assignment encoded with the lowest common version, so there should be no problem decoding a VP assignment. We should just allow `onAssignment` to proceed as usual so that the `TaskManager` is in a consistent state, and knows what all its tasks/partitions are when the first rebalance completes and the next one is triggered.  Should be cherry-picked to 2.4","closed","streams,","ableegoldman","2019-10-31T03:30:39Z","2019-11-01T23:18:56Z"
"","8050","MINOR: Include call name in TimeoutException","Currently when the AdminClient times out calls, it does not indicate their names. Many AdminClient methods run multiple calls making it hard to debug TimeoutExceptions like https://issues.apache.org/jira/browse/KAFKA-9463  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-02-06T16:47:50Z","2020-09-02T16:04:56Z"
"","7551","KAFKA-9056; Inbound/outbound byte metrics should reflect partial sends/receives","Currently we only record completed sends and receives in the selector metrics. If there is a disconnect in the middle of the respective operation, then it is not counted. The metrics will be more accurate if we take into account partial sends and receives.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-17T23:35:41Z","2019-10-23T15:45:09Z"
"","8436","KAFKA-6145: KIP-441 avoid unnecessary movement of standbys","Currently we add warmup and standby tasks, meaning we first assign up to max.warmup.replica warmup tasks, and then attempt to assign num.standby copies of each stateful task. This can cause unnecessary transient standbys to pop up for the lifetime of the warmup task, which are presumably not what the user wanted.  Note that we don’t want to simply count all warmups against the configured num.standbys, as this may cause the opposite problem where a standby we intend to keep is temporarily unassigned (which may lead to the cleanup thread deleting it). We should only count this as a standby if the destination client already had this task as a standby; otherwise, the standby already exists on some other client, so we should aim to give it back.","closed","","ableegoldman","2020-04-07T03:43:51Z","2020-06-26T22:40:19Z"
"","7766","KAFKA-8803: [Streams fix] retry for initTxn","Currently the initTransactions() API on StreamTask will throw uncaught timeout exception. It is not a desirable outcome as we normally would expect to retry upon timeout exception as this could indicate some temporary server side unavailability. Failing immediately is not ideal.  The fix is that if we hit non-fatal exception,  stream task keeps retrying `initTransactions` until succeed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-12-02T06:37:48Z","2019-12-06T18:20:08Z"
"","7657","KAFKA-9125: Use metadata cache GroupMetadataManager and TransactionStateManager","Currently the GroupMetadataManager and TransactionStateManager uses zookeeper to query the number of partitions of their respective internal topic. Instead of this it'd be much cheaper and better to use the metadataCache as brokers will have up-to-date information about these partitions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-11-06T15:55:46Z","2020-05-26T09:27:42Z"
"","8413","MINOR: Allow single struct to be a field in protocol spec","Currently protocol generation code has this restriction that a structure field need to be an array. There are however some case where fields need to be grouped but not be an array. This is for organizational purposes and mirrors program structure.  This change adds this capability to the protocol generation.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-04-03T03:38:31Z","2020-04-10T20:15:06Z"
"","7567","MINOR: Make it possible to produce messages with a null body in kafka console producer","Currently it is not possible to produce messages with a null body using the kafka-console-producer  Having this capability would be useful when dealing with compacted kafka topics  This patch adds a `null.value` argument that lets you specify a value that will be interpreted as NULL","open","","jelmerk","2019-10-21T17:50:24Z","2019-10-21T20:41:59Z"
"","8134","KAFKA-9546 Allow custom tasks through configuration","Currently FileStreamSourceConnector can only return a task of type FileStreamSourceTask. With this change the users can override it and provide a custom task class via configuration.  Testing was done via unit tests. There's one positive case (custom Task class provided through config) and one negative (invalid class java.io.File was provided). The already existing unit tests are testing the default behavior, when FileStreamSourceTask is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gcsaba2","2020-02-18T21:00:56Z","2020-09-29T15:48:44Z"
"","8460","MINOR: Only start log dir fetcher after LeaderAndIsr epoch validation","Currently a `LeaderAndIsr` request with a stale leader epoch for some partition may still result in the starting of the log dir fetcher for that partition (if the future log exists). I am not sure if this causes any correctness problem since we don't use any state from the request to start the fetcher, but it seems unnecessary to rely on this side effect.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-10T00:12:08Z","2020-04-10T03:15:44Z"
"","7717","MINOR: Controller should log UpdateMetadata response errors","Create a controller event for handling UpdateMetadata responses and log a message when a response contains an error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-11-20T08:22:32Z","2019-11-21T15:41:30Z"
"","8115","MINOR: Fix some flaky behavior in DeleteConsumerGroupsTest and clean up","Create a common `waitForGroupState` function to avoid duplicated code.  Also print out the difference between the actual and expected state in that function.  In a few tests, wait until the groups are in Stable state before proceeding.  If we don't do this, sometimes we try to tear the group down before it ever becomes stable, and get stuck waiting for a rebalance timeout (which is too long to allow the test to pass)","open","","cmccabe","2020-02-13T22:50:43Z","2020-02-20T07:29:45Z"
"","8427","KAFKA-9761: Use Admin Client default timeout instead of using 5sec timeout default","Corresponds to: https://issues.apache.org/jira/browse/KAFKA-9761  Changes the `timeout` flag in the `kafka-consumer-groups` tool to not default to 5000ms. It now uses the Admin Client default(30s currently) by not setting `options.timeoutMs` unless the timeout flag is specified, in which case it ends up using `(min(30s, timeout flag))` in the `AdminClientRunnable` method `sendEligibleCalls`.  Unit tests are provided that work with the default value and the old default 5000ms. There were already tests for an unreasonably short timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","MikeSchlosser16","2020-04-04T23:20:25Z","2020-04-25T16:59:31Z"
"","8015","KAFKA-9500: Fix topology bug in foreign key joins","Corrects a flaw leading to an exception while building topologies that include both: * A foreign-key join with the result _not_ explicitly materialized * An operation after the join that requires source materialization  Also corrects a flaw in TopologyTestDriver leading to output records being enqueued in the wrong order under some (presumably rare) circumstances.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-01-29T00:11:35Z","2020-02-12T05:00:08Z"
"","7698","KAFKA-9068: Updated the javadoc as per implementation","Corrected the javadoc as per the session store implementation.  ### Committer Checklist (excluded from commit message) - [*] Verify design and implementation  - [*] Verify test coverage and CI build status - [*] Verify documentation (including upgrade notes)","closed","streams,","satishbellapu","2019-11-16T10:13:26Z","2020-01-01T23:32:34Z"
"","7752","MINOR: Convert Stream-StreamJoin Integration Test to TTD","Convert `StreamStreamJoinIntegrationTest`  to TTD for more stable testing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-11-27T00:25:07Z","2019-12-05T22:41:37Z"
"","7777","MINOR: Convert last streams join test to TTD","Conversion of the last join integration test to use TTD   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-12-04T15:59:34Z","2019-12-05T22:42:13Z"
"","8213","KAFKA-9615: Clean up task/producer create and close","Consolidates task/producer management.  Now, exactly one component manages the creation and destruction of Producers, whether they are per-thread or per-task.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-03-03T22:16:50Z","2020-03-05T20:20:52Z"
"","8319","KAFKA-9734: Fix IllegalState in Streams transit to standby","Consolidate ChangelogReader state management inside of StreamThread to avoid having to reason about all execution paths in both StreamThread and TaskManager.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-03-19T21:58:55Z","2020-03-20T15:17:56Z"
"","8263","MINOR: Add method to Herder to control logging of connector configs during validation","Connector validation API is useful in validating connector configs before starting connector. And our application counts on it to generate friendly feedback to users. However, Connector validation API always log all configs when handling validation request since it creates AbstractConfig to check the overridable configs (introduced by #6624) and AbstractConfig, by default, logs all configs. In our case, the worker logs are filled with the following messages when there are a log of validation requests. ``` INFO AbstractConfig values: xxx xxx xxx ``` It seems to me the response of Connector validation API is good enough. By contrast, the logs are a bit noisy and useless.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","chia7712","2020-03-10T13:51:30Z","2020-03-24T23:44:56Z"
"","7768","KAFKA-9258 Check Connect Metrics non-null in task stop","Connect sometimes will stop task when start() has failed. In these cases, we must guard against NPE as noted in https://issues.apache.org/jira/browse/KAFKA-9258","closed","connect,","cyrusv","2019-12-02T21:28:08Z","2020-10-16T06:15:44Z"
"","8446","KAFKA-9804:Extract consumer configs out of PerfConfig","Configs for producer has been extracted out of PerfConfig in https://github.com/apache/kafka/pull/3613/commits. The remaining in PerfConfig are configs for consumer. And ConsumerPerformance also has configs for consumer. Separating these configs into two classes is not concise. So we can put all configs into class ConsumerPerformance.ConsumerPerfConfig.  Change-Id: I38e2fdf7c7930af786e03cbb5d9deb2f4ef50dee Signed-off-by: Jiamei Xie   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jiameixie","2020-04-08T07:19:47Z","2022-05-24T03:27:34Z"
"","8295","KAFKA-9627: Replace ListOffset request/response with automated protocol","Co-authored-by: Mickael Maison  Co-authored-by: Edoardo Comar   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-03-13T11:56:28Z","2020-09-24T13:54:28Z"
"","7494","HOT FIX: fix checkstyle in Streams system test","Checkstyle is broken on trunk due to replacing anonymous function with a lambda in one of the system tests, just formatting it so it won't complain.  Should be cherry-picked to 2.4","closed","","ableegoldman","2019-10-11T02:32:41Z","2019-10-11T23:29:54Z"
"","7783","KAFKA-9184 (port on 2.3): Redundant task creation and periodic rebalances after zombie Connect worker rejoins the group","Check connectivity with broker coordinator in intervals and stop tasks if coordinator is unreachable by setting `assignmentSnapshot` to null and resetting rebalance delay when there are no lost tasks. And, because we're now sometimes setting `assignmentSnapshot` to null and reading it from other methods and thread, made this member volatile and used local references to ensure consistent reads.      Adapted existing unit tests to verify additional debug calls, added more specific log messages to `DistributedHerder`, and added a new integration test that verifies the behavior when the brokers are stopped and restarted only after the workers lose their heartbeats with the broker coordinator.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-12-05T01:10:10Z","2020-10-16T06:15:44Z"
"","8333","Change ""matchingAclExists"" method visibility modifier to protected in AclAuthorizer","Changing method visibility modifier allows to make acl matching logic overridable by children components. This is useful in situations where the ACL matching logic is more complex (with custom authorizers) and/or when KafkaPrincipal is customized. Unfortunately KafkaPrincipal used inside ""matchingAclExists"" method is not built from the PrincipalBuilder but from static method SecurityUtils.parseKafkaPrincipal , this prevents the possibility to ovveride ""equals"" method in KafkaPrincipal and so to override the equality logic between two custom kafka principals.","open","","acittadino","2020-03-23T08:17:58Z","2020-03-23T08:17:58Z"
"","8155","KAFKA-9553: Improve measurement for loading groups and transactions","Changes: - Pull out sensor update to top-level - Shifted up the partition-loading sensor measurement for loading transactions - Added scheduler-time awareness for loading groups  Tests: - Modified test to reflect new scheduling call  Ops: - [x] Modify docs/ops.html; update metric description to include scheduling time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agam","2020-02-21T22:23:26Z","2020-03-20T06:35:17Z"
"","7672","MINOR: Proactively update producer topic access time.","Changes the ProducerMetadata to longer record a sentinel TOPIC_EXPIRY_NEEDS_UPDATE upon topic map emplacement, and instead set the expiry time directly. Previously the expiry time was being updated for all touched topics after a metadata fetch was processed, which could be seconds/minutes in the future.  Additionally propagates the current time further in the Producer, which should reduce the total number of current-time calls.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-11-08T23:01:05Z","2019-12-03T15:06:32Z"
"","7930","MINOR: Simplify the create partition metadata type.","Changed CreatePartitionMetadata to not include partition assignment information since this is not needed to generate a CreatePartitionResponse message.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-01-10T19:31:44Z","2020-01-13T17:50:20Z"
"","8243","KAFKA-8245: Fix Flaky Test DeleteConsumerGroupsTest#testDeleteCmdAllGroups (#8032)","Change unit tests to make sure the consumer group is in Stable state (i.e. consumers have completed joining the group)  (cherry picked from commit 350dce865ae5420a25496bc502c55de4c15bf71e)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-03-06T18:59:26Z","2020-03-09T13:35:24Z"
"","8241","KAFKA-8245: Fix Flaky Test DeleteConsumerGroupsTest#testDeleteCmdAllGroups (#8032)","Change unit tests to make sure the consumer group is in Stable state (i.e. consumers have completed joining the group)  (cherry picked from commit 350dce865ae5420a25496bc502c55de4c15bf71e)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-03-06T18:46:28Z","2020-03-09T13:35:49Z"
"","8234","KAFKA-8245: Fix Flaky Test DeleteConsumerGroupsTest#testDeleteCmdAllGroups (#8032)","Change unit tests to make sure the consumer group is in Stable state (i.e. consumers have completed joining the group)  (cherry picked from commit 350dce865ae5420a25496bc502c55de4c15bf71e)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-03-05T22:33:30Z","2020-03-06T18:39:16Z"
"","8399","KAFKA-3720: Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms","Change TimeoutException to BufferExhaustedException when no memory can be allocated for a record within max.block.ms  Refactored BufferExhaustedException to be a subclass of TimeoutException so existing code that catches TimeoutExceptions keeps working.  Added handling to count these Exceptions in the metric ""buffer-exhausted-records"".  Test Strategy There were existing test cases to check this behavior, which I refactored.  I then added an extra case to check whether the expected Exception is actually thrown, which was not covered by current tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2020-03-31T22:53:32Z","2020-08-20T15:44:54Z"
"","8312","KAFKA-9432 automated protocol for DescribeConfigs","Change the DescribeConfigs request and response to use the generated protocol messages.","closed","","tombentley","2020-03-18T17:47:52Z","2020-07-28T08:10:05Z"
"","8311","KAFKA-9434: automated protocol for alterReplicaLogDirs","Change the alterReplicaLogDirs request and response to use the generated protocol messages.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2020-03-18T11:13:14Z","2020-06-04T14:36:38Z"
"","8459","MINOR: Fix JavaDocs markup","Call fore review @bbejeck","closed","streams,","mjsax","2020-04-09T22:59:23Z","2020-04-10T02:42:27Z"
"","7758","KAFKA-9244: Update FK reference should unsubscribe old FK","Call for review @vvcephei @bellemare","closed","streams,","mjsax","2019-11-28T06:59:04Z","2019-11-30T05:33:33Z"
"","8291","MINOR: Fix build and JavaDoc warnings","Call for review @ijuma   Just fixing some JavaDocs warning and build warning about unused imports.","closed","","mjsax","2020-03-13T03:44:51Z","2020-03-17T01:23:27Z"
"","8028","KAFKA-9490: Fix generics for Grouped","Call for review @guozhangwang @vvcephei","closed","streams,","mjsax","2020-01-31T23:57:13Z","2020-02-05T00:24:19Z"
"","8273","MINOR: Fix generic types in StreamsBuilder and Topology","Call for review @cadonna @vvcephei @guozhangwang","closed","streams,","mjsax","2020-03-11T03:21:01Z","2020-03-19T21:29:22Z"
"","7546","MINOR: Fix JavaDoc warning","Call for review @bbejeck @guozhangwang","closed","streams,","mjsax","2019-10-17T18:32:19Z","2019-10-25T07:41:45Z"
"","8367","KAFKA-9719: Streams with EOS-beta should fail fast for older brokers","Call for review @abbccdda @guozhangwang   System test run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3856/","closed","kip,","mjsax","2020-03-26T20:05:39Z","2020-06-12T23:14:03Z"
"","8440","KAFKA-9832: extend Kafka Streams EOS system test","Call for review @abbccdda @guozhangwang   System test run passed: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3886/","closed","kip,","mjsax","2020-04-07T20:53:37Z","2020-06-12T23:11:13Z"
"","8331","KAFKA-9748: extend EosIntegrationTest for EOS-beta","Call for review @abbccdda @guozhangwang","closed","kip,","mjsax","2020-03-23T02:23:43Z","2020-06-12T23:13:11Z"
"","8251","MINOR: Code cleanup to improve generic typing","Call for review @abbccdda @guozhangwang","closed","streams,","mjsax","2020-03-07T19:07:02Z","2020-03-10T23:43:11Z"
"","7514","KAFKA-9033; Use consumer/producer identity in generated clientId","By default, if the user does not configure a `client.id`, then we use a very generic identifier, such as `consumer-15`. It is more useful to include identifying information when available such as `group.id` for the consumer and `transactional.id` for the producer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-15T00:17:59Z","2019-10-15T21:52:27Z"
"","7835","HOTFIX: bump system test versions","Bumped kafkatest/version.py to 2.2.3-SNAPSHOT Updated versions following instructions in streams_upgrade_test.py","closed","tests,","ableegoldman","2019-12-16T21:17:31Z","2020-06-26T22:38:07Z"
"","8284","KAFKA-9225: Bump rocksdb 5.18.3 to 5.18.4","Bump rocksdb 5.18.3 to 5.18.4 that supports all platforms. Issues about this version are https://github.com/facebook/rocksdb/pull/6497 and https://github.com/facebook/rocksdb/issues/6188  Change-Id: I3febec8e36550edcb7f88839cc1e2b2a54984564 Signed-off-by: Jiamei Xie   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jiameixie","2020-03-12T06:56:32Z","2020-06-11T01:52:45Z"
"","7570","MINOR: Use replicas instead of brokers in the reassignment API","Brokers assigned to a topic partition are called replicas. Fix reassignment API to be consistent with other APIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-10-21T20:11:57Z","2019-10-24T09:38:33Z"
"","7784","KAFKA-9241: Some SASL Clients not forced to re-authenticate","Brokers are supposed to force SASL clients to re-authenticate (and kill such connections in the absence of a timely and successful re-authentication) when KIP-368 SASL Re-Authentication is enabled via a positive connections.max.reauth.ms configuration value. There was a flaw in the logic that caused connections to not be killed in the absence of a timely and successful re-authentication if the client did not leverage the SaslAuthenticateRequest API (which was defined in KIP-152).","closed","","rondagostino","2019-12-05T02:04:47Z","2019-12-09T18:57:43Z"
"","7720","[Minor] Standardise ""variable-length"" vs ""variable length""","Both were used in the same sentence, which isn't necessarily clear.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Malakhit","2019-11-20T14:40:36Z","2019-11-21T13:37:59Z"
"","8036","Minor: fix the unused arguments used by formatting string","Both arguments are string type so the specifier should be ```%s```.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-02-03T16:39:23Z","2020-04-30T06:02:05Z"
"","8345","MINOR: Allow topics with `null` leader on MockAdminClient.createTopic.","Behaviour was accidentally changed recently on https://github.com/apache/kafka/pull/8244 to not allow the creation of topics with `null` leader. https://github.com/confluentinc/kafka-rest relies on this to test defensive behaviour against AdminClient returning a topic with `null` leader.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rigelbm","2020-03-25T11:28:59Z","2020-03-26T01:48:21Z"
"","8180","MINOR: Check store directory empty to decide whether throw task corrupted exception with EOS","Before we register the stores (and hence create the store dirs), we check if the task dir is empty except the lock / checkpoint files. Then later when loading the checkpoint files if we do not find the offsets AND the store dirs are not empty, meaning that the stores may be not empty, we treat it as task corrupted.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-27T00:11:49Z","2020-03-06T23:39:11Z"
"","8355","Suppress detailed responses for security-sensitive (PCI-DSS) environments","Before this PR, Kafka Connect's exception mapper would print an unchecked messages from unhandled stack traces. I am contributing this code change so that the community has the option to change this behavior based on a configuration option, ""error.rest.response.message.detail.enabled""  As this functionality is an API-level change, as it changes the response of handled errors, it should not be enabled by default. I have provided a unit test for handling the expected response if the option is enabled.","open","connect,","cwpenhale","2020-03-25T21:34:51Z","2020-04-30T08:17:01Z"
"","8065","KAKFA-9503: Fix TopologyTestDriver output order","Because of the recursive invocations of pipeRecord, records in recursive topologies would be appended to the TopologyTestDriver output buffer in depth-first order, rather than breadth-first, which is how KafkaStreams would do it.  This PR changes the processing algorithm from an (implicitly, via recursion) stack-based one to a queue-based one more similar to how KafkaSteams actually executes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-02-07T23:23:58Z","2020-02-12T06:00:59Z"
"","8043","KAFKA-6793: Unnecessary warning log message","Based on KIP-552 changes log level from WARN to DEBUG for the warning ""The configuration 'x' was supplied but isn't a known config.""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","artjock","2020-02-05T14:14:38Z","2020-10-23T05:19:21Z"
"","7935","KAFKA-9403: Remove BaseConsumer from Mirrormake","BaseConsumerRecord is deprecated but MirrorMaker using BaseConsumerRecord  Remove BaseConsumer from Mirrormaker  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-12T13:47:24Z","2020-01-31T04:43:21Z"
"","7524","KAFKA-8945/KAFKA-8947 backport","Backport of changes for [KAFKA-8945](https://issues.apache.org/jira/browse/KAFKA-8945) and [KAFKA-8947](https://issues.apache.org/jira/browse/KAFKA-8947) that should be compatible with pre-2.3 versions of Connect, as [requested on the ticket](https://issues.apache.org/jira/browse/KAFKA-8947?focusedCommentId=16952356&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16952356) for KAFKA-8947 after the initial fix was merged.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-10-15T23:21:44Z","2022-02-27T03:55:26Z"
"","7533","KAFKA-8945/KAFKA-8947 backport","Backport of changes for [KAFKA-8945](https://issues.apache.org/jira/browse/KAFKA-8945) and [KAFKA-8947](https://issues.apache.org/jira/browse/KAFKA-8947) that should be compatible with pre-2.1 versions of Connect.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-10-16T18:43:52Z","2020-10-16T06:15:40Z"
"","7656","KAFKA-9149 Avoid temp byte array creation when use ByteBufferSerializer","Avoid temp byte array creation when use ByteBufferSerializer","open","","chenxu14","2019-11-06T09:16:54Z","2019-11-07T03:14:54Z"
"","8378","KAFKA-9779: Add Stream System Test 2.5","As title, we should be adding the upgrade test after 2.5 release. ```  TC_PATHS=""tests/kafkatest/tests/streams/streams_upgrade_test.py"" bash tests/docker/run_tests.sh ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","abbccdda","2020-03-27T23:23:30Z","2020-04-15T22:59:04Z"
"","8343","KAFKA-9758: Doc changes for KIP-523 and KIP-527","As title, this PR adds doc changes for two mentioned KIPs in 2.5.  Visualized changes: ![image](https://user-images.githubusercontent.com/5845561/77491649-9ad39d80-6dfb-11ea-8273-1b6d03fb6dd5.png) ![image](https://user-images.githubusercontent.com/5845561/77491695-b8a10280-6dfb-11ea-93ff-1011c707c83e.png) ![image](https://user-images.githubusercontent.com/5845561/77491724-cf475980-6dfb-11ea-8ccb-21804f60fc5d.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","abbccdda","2020-03-25T01:06:05Z","2020-03-25T16:32:17Z"
"","8402","KAFKA-9793: Expand the try-catch for task commit in HandleAssignment","As title suggests, we would like to broaden this check so that we don't fail to close a doom-to-cleanup task.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-04-01T05:25:43Z","2020-04-06T05:50:27Z"
"","7952","KAFKA-9418: Add new sendOffsetsToTransaction API to KafkaProducer","As title suggests, the change is bringing in the consumer group metadata as part of the transaction API for correct fencing after 447. This PR mainly changes on the Producer end for compatible paths to old `sendOffsetsToTxn(offsets, groupId)` vs new `sendOffsetsToTxn(offsets, groupMetadata)`.  Some integration test extensions are also added to test out the validity of the new sendOffsets API.  This PR also contains some fixes towards the closed protocol PR.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","abbccdda","2020-01-13T23:32:18Z","2020-01-22T21:52:05Z"
"","8388","KAFKA-9759: Add doc change for KIP-562","As title suggests, released KIP-562 should be documented.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes) ![image](https://user-images.githubusercontent.com/5845561/77941064-6aae5380-726e-11ea-8cab-3462156b186d.png) ![image](https://user-images.githubusercontent.com/5845561/77941112-7ef25080-726e-11ea-974a-764476fc13f2.png)","closed","docs,","abbccdda","2020-03-30T17:08:00Z","2020-03-30T20:17:49Z"
"","8383","KAFKA-9784: Add OffsetFetch to group concurrency test","As title suggested, consumers would first do an OffsetFetch before starting the normal processing. It makes sense to add it to the concurrent test suite to verify whether there would be a blocking behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-29T01:57:13Z","2020-04-06T05:47:48Z"
"","8247","HOTFIX: fix StateManagerUtilTest and StreamTaskTest failures","As title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-07T04:09:00Z","2020-03-07T06:14:24Z"
"","7684","KAKFA-9178: remove changelog partitions from restoredPartitions","As the ticket suggested, we haven't got a chance to cleanup the `restoredPartitions` with the changelog topics associated with the specific task.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-11-13T00:16:16Z","2019-11-14T03:39:33Z"
"","8010","MINOR: Introduce 2.5-IV0 IBP","As the feature freeze approaches, we should support `2.5` as the inter.broker.protocol.version value. There are no new APIs so far, so `2.5` is effectively equivalent to `2.4`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-01-28T14:53:52Z","2020-01-31T17:42:17Z"
"","7762","KAFKA-9230: Refactor user-customizable Streams metrics","As proposed in KIP-444, the user customizable metrics shall be refactored. The refactoring consists of: - adding methods `addLatencyRateTotalSensor` and `addRateTotalSensor` to interface `StreamMetrics` - implement the newly added methods in `StreamsMetricsImpl` - deprecate methods `recordThroughput()` and `recordLatency()` in `StreamsMetrics`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-11-28T15:27:18Z","2020-06-12T23:27:01Z"
"","8056","KAFKA-9519: Deprecating ZK for ConfigCommand","As part of KIP-555, direct zookeeper access should be deprecated in the Kafka admin tools. This PR is the first of which to do so. This PR adds warnings for deprecation of the `--zookeeper` option for the `ConfigCommand` in `kafka-configs.sh.`  JIRA: https://issues.apache.org/jira/browse/KAFKA-9519  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","skaundinya15","2020-02-06T21:59:05Z","2020-02-08T00:08:36Z"
"","8407","KAFKA-9809: Shrink transaction timeout for streams","As documented in the KIP: ``` We shall set `transaction.timout.ms` default to 10000 ms (10 seconds) on Kafka Streams. For non-stream users, we highly recommend you to do the same if you want to use the new semantics. ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","abbccdda","2020-04-02T17:57:58Z","2020-06-12T23:14:46Z"
"","7485","KAFKA-8455: Add VoidSerde to Serdes","As discussed in [KIP-527](https://cwiki.apache.org/confluence/display/KAFKA/KIP-527%3A+Add+NothingSerde+to+Serdes) this PR introduces new VoidSerde.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","nizhikov","2019-10-10T13:50:05Z","2020-06-12T23:30:57Z"
"","8092","KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level","As described in the ticket, this message is logged at the error level but only indicates that a standby task was not created (as is the case if its subtopology is stateless). Moving this to debug level, and clarifying the implications in the log level.  Targeting this PR against 2.4, as the issue is incidentally fixed in trunk as part of the tech debt cleanup. We should also merge this fix to 2.5 but need to wait for the release, since this is obviously not a blocker","closed","","ableegoldman","2020-02-11T22:56:10Z","2020-04-15T21:14:56Z"
"","7941","KAFKA-9181; Maintain clean separation between local and group subscriptions in consumer's SubscriptionState","As described in KAFKA-9181, kafka.api.SaslGssapiSslEndToEndAuthorizationTest.testNoConsumeWithoutDescribeAclViaSubscribe occasionally hits unexpected TopicAuthorizationException even after the topic is removed from the subscription. The test uses small metadata refresh time and hence can see metadata responses before JoinGroup is processed. We currently rely on `onJoinPrepare` to reset SubscriptionState.groupSubscription, which accumulates topics until reset. If we process JoinGroup after a subscribe without a new `onJoinPrepare`, we leave the topic in `SubscriptionState.groupSubscription` and hence in metadata. This PR resets `groupSubscription` when sending JoinGroup request, when `ConsumerCoordinator.joinedSubscription` is updated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-01-12T16:51:09Z","2020-01-24T10:38:22Z"
"","8087","KAFKA-9525: add enforceRebalance method to Consumer API","As described in [KIP-568](https://cwiki.apache.org/confluence/display/KAFKA/KIP-568%3A+Explicit+rebalance+triggering+on+the+Consumer).  Waiting on acceptance of the KIP to write the tests, on the off chance something changes. But rest assured unit tests are coming ⚡️   Will also kick off existing Streams system tests which leverage this new API (eg version probing, sometimes broker bounce)","closed","kip,","ableegoldman","2020-02-11T03:46:51Z","2020-06-26T22:37:36Z"
"","8201","MINOR: comment apikey types in generated switch","As a developer, it would be convenient if the generated {request,response}HeaderVersion case statements in ApiMessageType.java included a comment to remind me which type each of them is so I don't need to manually cross-reference the newer/rarer ones.  Also include commented lines for the two special cases around ApiVersionsResponse and ControllerShutdownRequest which are hardcoded in the ApiMessageTypeGenerator.java and not covered by the message format json files.  Before: ```java     public short requestHeaderVersion(short _version) {         switch (apiKey) {             case 0:                 return (short) 1;             case 1:                 return (short) 1;             case 2:                 return (short) 1;             case 3:                 if (_version >= 9) {                     return (short) 2;                 } else {                     return (short) 1;                 }             // ...etc ```  After: ```java     public short requestHeaderVersion(short _version) {         switch (apiKey) {             case 0: // Produce                 return (short) 1;             case 1: // Fetch                 return (short) 1;             case 2: // ListOffset                 return (short) 1;             case 3: // Metadata                 if (_version >= 9) {                     return (short) 2;                 } else {                     return (short) 1;                 }             // ...etc ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dnwe","2020-03-02T14:40:13Z","2020-03-15T10:46:10Z"
"","7644","KAFKA-9139: Fix dynamic broker config definitions not resolving their type.","As a consequence, the values for the dynamic config's keys were being treated as sensitive, and therefore no value (null) was being passed back to the client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-11-04T22:56:17Z","2019-11-06T00:16:57Z"
"","8322","MINOR: Restore and global consumers should never have group.instance.id","And hence restore / global consumers should never expect FencedInstanceIdException.  When such exception is thrown, it means there's another instance with the same instance.id taken over, and hence we should treat it as fatal and let this instance to close out instead of handling as task-migrated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-03-20T20:51:02Z","2020-04-24T23:55:33Z"
"","7986","fixed an incoherence about existing Kafka APIs","Although APIs section in Kafka documentation lists 5 core APIs (https://kafka.apache.org/documentation/#api), introduction page in Kafka documentation lists 4 of them. I've added the missing list element to fix this incoherence.","closed","","commandini","2020-01-20T10:02:39Z","2020-01-20T16:49:41Z"
"","7747","Remove --zookeeper usages from kafka.py","Also use `pipes.quote` for building up commands (rather than string concatenation)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mumrah","2019-11-26T03:58:13Z","2019-12-03T15:56:10Z"
"","7703","MINOR: Avoid unnecessary tuple allocations in index binary search","Also tweaked it the computation to be more inline with standard binary search implementations.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ijuma","2019-11-17T18:54:35Z","2020-10-29T07:32:09Z"
"","7537","KAFKA-9053: AssignmentInfo#encode hardcodes the LATEST_SUPPORTED_VERSION","Also put in some additional logging that makes sense to add, and proved helpful in debugging this particular issue.   Unit tests verifying the encoded supported version were added.  This should get cherry-picked back to 2.1","closed","streams,","ableegoldman","2019-10-16T21:26:54Z","2019-10-17T05:25:33Z"
"","7715","Minor: Fix missing extra_kafka_opts  reference","Also change the default value for `per_node_server_prop_overrides` to None instead of an empty `dict`","closed","","mumrah","2019-11-19T15:31:57Z","2019-11-19T20:19:49Z"
"","7969","KAFKA-7317: Use collections subscription for main consumer to reduce metadata","Also addresses [KAFKA-8821](https://issues.apache.org/jira/browse/KAFKA-8821)  Note that we still have to fall back to using pattern subscription if the user has added any regex-based source nodes to the topology. Includes some minor cleanup on the side","closed","streams,","ableegoldman","2020-01-16T03:45:28Z","2020-06-26T22:38:02Z"
"","7972","KAFKA-9435: DescribeLogDirs automated protocol","Also add version 2 to make use of flexible versions, per KIP-482.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2020-01-16T12:12:56Z","2020-03-19T15:26:19Z"
"","7957","KAFKA-8768: DeleteRecords request/response automated protocol","Also add version 2 to make use of flexible versions, per KIP-482.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tombentley","2020-01-14T13:55:40Z","2020-03-13T17:48:48Z"
"","7725","KAFKA-9027, KAFKA-9028: Convert create/delete acls requests/response to use generated protocol","Also add support for flexible versions to both protocol types.  Co-authored-by: Rajini Sivaram  Co-authored-by: Jason Gustafson   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-21T04:42:47Z","2020-02-03T15:12:32Z"
"","7840","KAFKA-9307: Make transaction metadata loading resilient to previous errors","Allow transaction metadata to be reloaded, even if it already exists as of a previous epoch. This helps with cases where a previous become-follower transition failed to unload corresponding metadata.","closed","","dhruvilshah3","2019-12-17T06:34:10Z","2019-12-23T23:21:32Z"
"","7585","KAFKA-8986: Allow null as a valid default for tagged fields.","Allow tagged fields to have null defaults.  Fix a bunch of cases where this would previously result in null pointer dereferences.  Allow inferring FieldSpec#versions based on FieldSpec#taggedVersions.  Prefix 'key' with an underscore when it is used in the generated code, to avoid potential name collisions.  Allow setting hexadecimal constants and 64-bit contstants in spec files.  Test setting UUID constants.  Add a lot more test cases to SimpleExampleMessage.json.  Add MessageTestUtil#byteBufferToString for debugging.","closed","","cmccabe","2019-10-23T18:17:10Z","2019-11-21T00:40:19Z"
"","8068","MINOR: Update to Gradle 4.10.3","Aligns with the 2.1 branch and fixes the TLS handshake error when executed with Java 7. This is necessary for the 1.0 and 1.1 branches and we do it in 2.0 too for consistency.  (cherry picked from commit b39aa1205dd8b4e05a9ae58fd013ca50be2dd00f)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2020-02-08T00:58:58Z","2020-05-14T00:05:32Z"
"","7542","[KAFKA-9041] Flaky Test LogCleanerIntegrationTest#testIsThreadFailed","Aims to fix the flaky LogCleanerIntegrationTest#testIsThreadFailed by changing how metrics are cleaned.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-10-17T11:46:50Z","2019-10-19T17:13:20Z"
"","7565","KAFKA-8940: Tighten up SmokeTestDriver","After many runs of reproducing the failure (on my local MP5 it takes about 100 - 200 run to get one) I think it is more likely a flaky one and not exposing a real bug in rebalance protocol.  What I've observed is that, when the verifying consumer is trying to fetch from the output topics (there are 11 of them), it `poll(1sec)` each time, and retries 30 times if there's no more data to fetch and stop. It means that if there are no data fetched from the output topics for 30 * 1 = 30 seconds then the verification would stop (potentially too early). And for the failure cases, we observe consistent rebalancing among the closing / newly created clients since the closing is async, i.e. while new clients are added it is possible that closing clients triggered rebalance are not completed yet (note that each instance is configured with 3 threads, and in the worst case there are 6 instances running / pending shutdown at the same time, so a group fo 3 * 6 = 18 members is possible).  However, there's still a possible bug that in KIP-429, somehow the rebalance can never stabilize and members keep re-rejoining and hence cause it to fail. We have another unit test that have bumped up to 3 rebalance by @ableegoldman and if that failed again then it may be a better confirmation such bug may exist.  So what I've done so far for this test:  1. When closing a client, wait for it to complete closure before moving on to the next iteration and starting a new instance to reduce the rebalance churns.  2. Poll for 5 seconds instead of 1 to wait for longer time: 5 * 30 = 150 seconds, and locally my laptop finished this test in about 50 seconds.  3. Minor debug logging improvement; in fact some of them is to reduce redundant debug logging since it is too long and sometimes hides the key information.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","guozhangwang","2019-10-20T18:10:58Z","2020-04-24T23:58:27Z"
"","8220","KAFKA-9645: Fallback to unsubscribe during Task Migrated","After https://github.com/apache/kafka/pull/7312/, we could still return data during the rebalance phase, which means it could be possible to find records without corresponding tasks. We have to fallback to the unsubscribe mode during task migrated as the assignment should be cleared out to keep sync with task manager state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2020-03-04T04:24:45Z","2020-03-07T16:08:24Z"
"","7691","HOTFIX: safely clear all active state in onPartitionsLost","After a number of last minute bugs were found stemming from the incremental closing of lost tasks in StreamsRebalanceListener#onPartitionsLost, a safer approach to this edge case seems warranted. We initially wanted to be as ""future-proof"" as possible, and avoid baking further protocol assumptions into the code that may be broken as the protocol evolves. This meant that rather than simply closing all active tasks and clearing all associated state in `#onPartitionsLost(lostPartitions)` we would loop through the lostPartitions/lost tasks and remove them one by one from the various data structures/assignments, then verify that everything was empty in the end. This verification in particular has caused us significant trouble, as it turns out to be nontrivial to determine what should in fact be empty, and if so whether it is also being correctly updated.  Therefore, before worrying about it being ""future-proof"" it seems we should make sure it is ""present-day-proof"" and implement this callback in the safest possible way, by blindly clearing and closing all active task state. We log all the relevant state (at debug level) before clearing it, so we can at least tell from the logs whether/which emptiness checks were being violated.  Note that this is targeted at 2.4 (not trunk) and that I also picked over the minor fix from https://github.com/apache/kafka/pull/7686","closed","streams,","ableegoldman","2019-11-14T03:08:59Z","2019-11-19T22:22:37Z"
"","8430","KAFKA-9631: Fix support for optional fields in MockAdminClient","AdminClient's createTopics() method has a variant with two optional fields. However, using those two fields as empty would cause the MockAdminClient to throw an Exception while attempting to create a list with size -1. This PR fixes that by defaulting the value to 0 when those values are not passed.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","efgpinto","2020-04-05T18:06:01Z","2020-04-05T21:43:39Z"
"","8304","MINOR: cleanup and add tests to StateDirectoryTest","Adds tests for edge conditions of `listAllTaskDirectories` Also includes some minor cleanup of the StateDirectoryTest class","closed","","ableegoldman","2020-03-16T21:43:11Z","2020-03-18T00:20:59Z"
"","7804","KAFKA-7251; Add support for TLS 1.3","Adds support for TLSv1.3 in SslTransportLayer. Note that TLSv1.3 is only enabled from Java 11 onwards, so we test the code only when running with Java11 and above.  Tests run on this PR:   - SslTransportLayerTest: This covers testing of our SslTransportLayer and all tests are run with TLSv1.3 when running with Java 11. These tests are also run with TLSv1.2 for all Java versions   - SslFactoryTest: Also run with TLSv1.3 on Java 11 onwards in addition to TLSv1.2 for all Java versions   - SslEndToEndAuthorizationTest - Run only with TLSv1.3 on Java 11 onwards and only with TLSv1.2 on earlier Java versions. We have other versions of this test which use SSL that continue to be with TLSv1.2 on Java 11 to avoid reducing test coverage for TLSv1.2   Additional testing for TLSv1.3:   - Most tests that use SSL use TestSslUtils.DEFAULT_TLS_PROTOCOL_FOR_TESTS which is set to TLSv1.2. I have run all `clients` and `core` tests with `DEFAULT_TLS_PROTOCOL_FOR_TESTS=TLSv1.3` with Java 11.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-12-09T16:53:02Z","2019-12-19T14:13:04Z"
"","7664","KAFKA-9138: Add system test for relational joins","Adds a system test to verify the new foreign-key join introduced in KIP-213.  Also, adds a unit test and integration test to verify the test logic itself.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-11-08T02:38:45Z","2019-12-11T17:48:28Z"
"","8337","KAFKA-6145: Pt. 5 Implement high availability assignment","Adds a new `TaskAssignor` implementation, currently hidden behind an internal feature flag, that implements the high availability algorithm of KIP-441.  The main distinguishing feature of this new assignor is in assigning all active tasks to ""caught-up"" clients (if possible), and instead first assigning a new type of standby task, the ""warmup replica"", to clients who are not caught up. The assignor has 4 main steps: 1) stateful active task assignment: gets the initial/actual stateful assignment from `StateConstrainedBalancedAssignor` 2) warmup replica task assignment: gets the final/intended stateful assignment from `BalancedAssignor`, computes the movements between this and the above, and assigns warmup replicas where stateful tasks will ultimately end up 3) standby replica task assignment: distributes the `num.standby` replicas by choosing the next least loaded viable client (ie one that has no active, warmup, or standby version of this task already) 4) stateless active task assignment: distributes the stateless tasks across client, attempts to balance the active task load across all clients  (task load == num tasks per thread)  Note that step 3) is pretty simple, and loses some of the optimizations used by the `StickyTaskAssignor`. This is mostly to keep this PR small(er), and once merged we should refactor the task assignors to share parts of the standby assignment algorithm (and maybe stateless tasks too)","closed","","ableegoldman","2020-03-24T03:41:08Z","2020-06-26T22:37:45Z"
"","8349","MINOR: Add synchronization to the protocol name check","Addressing the comment from https://github.com/apache/kafka/pull/8324#discussion_r396273799  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-25T16:46:51Z","2020-03-26T09:48:23Z"
"","7774","KAFKA-6049: Add time window support for cogroup","adding TimeWindowedCogroupedKStream options to the cogroup operator  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2019-12-03T22:51:50Z","2020-08-18T17:17:56Z"
"","8182","MINOR: Update upgrade guide for ZK","Adding the ZK upgrade information to the `Notable Changes` section for 2.4.1   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-02-27T15:18:49Z","2020-02-27T23:01:44Z"
"","7782","KAFKA-6049: Add session window support for cogroup","adding SessionWindowedCogroupedKStream options to the cogroup operator  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2019-12-04T22:34:16Z","2020-08-18T17:17:46Z"
"","7616","MINOR: improve test coverage for dynamic LogConfig(s)","Adding a dynamically updatable log config is currently error prone, as it is easy to set them up as a val not a def and this would result in a dynamically updated broker default not applying to a LogConfig after broker restart.  This PR adds a guard against introducing these issues by ensuring that all log configs are exhaustively checked via a test.  For example, if the following line was a val and not a def, there would be a problem with dynamically updating broker defaults for the config.  https://github.com/apache/kafka/blob/4bde9bb3ccaf5571be76cb96ea051dadaeeaf5c7/core/src/main/scala/kafka/server/KafkaConfig.scala#L1216","closed","","lbradstreet","2019-10-30T22:43:53Z","2020-04-17T16:29:37Z"
"","8350","KAFKA-9760: Add KIP-447 protocol change to upgrade notes","Adding 447 protocol change as part of the upgrade doc `notable changes`, and some missing stream side change as well.  ![image](https://user-images.githubusercontent.com/5845561/77566577-188ebc00-6e83-11ea-8400-b930b4d970c2.png)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","abbccdda","2020-03-25T17:28:00Z","2020-03-30T20:14:01Z"
"","8232","KAFKA-9658: Fix removing user quotas","Adding (add-config) default user, user, or  quota and then removing it via delete-config does not update quota bound in ClientQuotaManager.Metrics for existing users or . This causes brokers to continue to throttle with the previously set quotas until brokers restart (or  stops sending traffic for sometime and sensor expires). This happens only when removing the user or user,client-id where there are no more quotas  to fall back to. Common example where the issue happens: Initial no quota state --> add default user quota --> remove default user quota.   The cause of the issue was `DefaultQuotaCallback.quotaLimit` was returning `null` when no default user quota set, which caused `ClientQuotaManager.updateQuotaMetricConfigs` to skip updating the appropriate sensor, which left it unchanged with the previous quota. Since `null` is an acceptable return value for `ClientQuotaCallback.quotaLimit`, which is already treated as unlimited quota in other parts of the code, this PR ensures that `ClientQuotaManager.updateQuotaMetricConfigs` updates the quotas for which  `ClientQuotaCallback.quotaLimit` returns `null` to unlimited quota.  Added 3 unit tests that failed before the fix in the PR.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2020-03-05T18:15:01Z","2020-03-10T19:30:59Z"
"","7899","MINOR: Fixes for adding new DSL naming page","Added re-direct for new page and added link to `Developer Guide` page  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2020-01-06T20:58:08Z","2020-01-06T23:46:05Z"
"","7626","MINOR: Added one test and some clarifying comments on tests with simulated EOS","Added one test and some comments to clarify how EOS is ""enabled"" for some of the tests.  Ran all streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-10-31T19:33:41Z","2019-11-07T15:20:40Z"
"","8225","KAFKA-4680: config mismatch exception on min isr > replication factor","Added new ConfigurationMismatchException. This is thrown on appendRecordsToLeader to avoid writing to leader if replication factor < min insync replicas and acks=all. At the moment (https://issues.apache.org/jira/browse/KAFKA-4680), Kafka throwns NotEnoughReplicasException in the case above. This PR aims to introduce a more specific exception for the case above. NotEnoughReplicasException is still used when there are not enough insync replicas when trying to append a record.  Changes: - add new ConfigurationMismatchException and related Error - updated documentation for min.insync.replicas - add test case for the new exception - updated existing tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","paolo-moriello","2020-03-05T11:34:09Z","2020-06-08T07:13:03Z"
"","8090","KAFKA-9537 - Cleanup error messages for abstract transformations","Added check if the transformation is abstract. If so throw an error message with guidance for the user.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","jcustenborder","2020-02-11T18:45:41Z","2020-05-15T17:14:21Z"
"","8414","KAFKA-8410: Part 1: processor context bounds","Add type bounds to the ProcessorContext, which bounds the types that can be forwarded to child nodes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2020-04-03T04:01:53Z","2020-06-12T23:15:00Z"
"","8024","KAFKA-9483; Add Scala KStream#toTable to the Streams DSL","Add Scala KStream#toTable to the Streams DSL https://issues.apache.org/jira/browse/KAFKA-9483  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","highluck","2020-01-31T04:39:41Z","2020-06-12T23:22:35Z"
"","7985","KAFKA-7658: Add KStream#toTable to the Streams DSL","Add KStream#toTable to the Streams DSL  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","highluck","2020-01-19T11:44:14Z","2020-06-12T23:22:26Z"
"","8184","KAKFA-9612 CLI Dynamic Configuration with file input","Add an option to kafka-configs.sh `--add-config-file` that adds the configs from a properties file. Testing: Added new tests to ConfigCommandTest.scala  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","WanderingStar","2020-02-27T15:58:00Z","2020-04-27T18:11:46Z"
"","8268","DOCS-3625: Add section to config topic: parameters controlled by Kafka Streams","Add a section on the parameters that Kafka Streams assigns. Also, a couple of other tweaks, like moving the `rocksdb.config.setter` section to be alphabetically ordered.  This content is ported from https://github.com/confluentinc/docs/pull/3604.","closed","docs,","JimGalasyn","2020-03-10T23:30:32Z","2020-03-26T18:21:48Z"
"","7522","KAFKA-8729: Add upgrade docs for KIP-467 on augmented produce response","Add a paragraph explaining the producer caller's expected behavior change on record validation failure scenarios that are improved by KIP-467.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-15T19:26:06Z","2020-04-24T23:50:08Z"
"","8480","MINOR: Add process(Test)Messages to the README","Add a hint to developers who sometimes suffer from rebuilding the source during branch switches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-04-14T05:31:05Z","2020-04-15T20:29:36Z"
"","8275","KAFKA-9709: add a confirm when kafka-server-stop.sh find multiple kafka instances to kill","add a confirm when kafka-server-stop.sh find multiple kafka instances, avoid mistakenly kill all instance of this machine.","open","","iamgd67","2020-03-11T08:24:17Z","2020-03-12T14:29:42Z"
"","8171","Adapt docs about metrics of Streams according to KIP-444","Adapts the docs about metrics of Streams according to https://cwiki.apache.org/confluence/display/KAFKA/KIP-444%253A+Augment+metrics+for+Kafka+Streams  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2020-02-26T13:09:05Z","2020-06-12T23:19:36Z"
"","7513","MINOR: AbstractRequestResponse should be an interface","AbstractRequestResponse should be an interface, since it has no concrete elements or implementation.  Move AbstractRequestResponse#serialize to RequestUtils#serialize and make it package-private, since it doesn't need to be public.","closed","","cmccabe","2019-10-14T23:59:18Z","2019-10-17T16:21:35Z"
"","7882","KAFKA-9729: avoid readLock in authorizer ACL lookups","A write lock is currently taken out whenever an ACL update is triggered. This update requires a round trip to ZK to get the ACLs for the resource (https://github.com/apache/kafka/pull/7882/files#diff-852b9cb2ceb2b85ec25b422f72c42620R489). This round trip to ZK can block any ACL lookups, which will block any requests and that require authorization, and their corresponding handler threads. This PR attempts to avoid these read locks by snapshotting the aclCache which is a threadsafe scala immutable.TreeMap.","closed","","lbradstreet","2020-01-01T05:30:35Z","2020-03-26T17:11:06Z"
"","8359","KAFKA-9778: Add validateConnector functionality to the EmbeddedConnectCluster","A validate endpoint should be added to enables the integration testing of validation functionalities, including validation success and assertion of specific error messages.   This PR adds a method `validateConnectorConfig` to the `EmbeddedConnectCluster` that pings the `/config/validate` endpoint with the given configurations. [More about the endpoint here.](https://kafka.apache.org/documentation/#connect_rest)  With this addition, the validations for the connector can be tested in a similar way integration tests currently use the `configureConnector` method, for ex: `connect.configureConnector(CONNECTOR_NAME, props);`. The validation call would look like: `ConfigInfos validateResponse = connect.validateConnectorConfig(CONNECTOR_CLASS_NAME, props);`.","closed","connect,","dosvath","2020-03-26T01:11:44Z","2020-04-02T23:52:57Z"
"","7793","MINOR: clarify node grouping of input topics using pattern subscription","A user recently [pointed out](https://issues.apache.org/jira/browse/KAFKA-9173) that we don't document how tasks are generated when using pattern subscription for input topics.  We should document this so users can better understand how their application will scale","closed","streams,","ableegoldman","2019-12-06T19:58:22Z","2019-12-14T11:58:05Z"
"","8365","MINOR: Fix jenkinsfile","A recent change to the Jenkinsfile caught exceptions and handled them, but declared the exception type as ""ignore"" even though it used the Exception. Fix that.  Tested and validated in ce-kafka (see https://github.com/confluentinc/ce-kafka/pull/1430) and backporting to ccs-kafka since this breaks Jenkins builds for failures where downstream-builds are unhappy.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","steverod","2020-03-26T16:30:31Z","2020-03-26T16:31:20Z"
"","8451","KAFKA-9835; Protect `FileRecords.slice` from concurrent write","A read from the end of the log interleaved with a concurrent write can result in reading data above the expected limit. In particular, this would allow a read above the high watermark. The root of the problem is consecutive calls to `sizeInBytes` in `FileRecords.slice` which do not account for an increase in size. This patch fixes the problem by using a single call to `sizeInBytes` and caching the result.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2020-04-08T16:02:08Z","2020-04-08T18:31:28Z"
"","8419","KAFKA-9739: 2.3 null child node fix","A port of #8400 for 2.3.  The process of sorting source and sink nodes changed in 2.4, so we can't cherry-pick the PR directly as we need to update the expected topology to what it would be in the 2.3 version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-04-03T19:54:06Z","2020-04-04T15:46:19Z"
"","8364","MINOR: Partition is under reassignment when adding and removing","A partition is under reassignment if the either the set of adding replicas or set removing replicas is non-empty.  Fix the test assertion such that it prints stdout on failure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-03-26T15:28:15Z","2021-07-14T19:48:57Z"
"","8125","KAFKA-9515: Upgrade ZooKeeper to 3.5.7","A couple of critical fixes:  ZOOKEEPER-3644: Data loss after upgrading standalone ZK server 3.4.14 to 3.5.6 with snapshot.trust.empty=true ZOOKEEPER-3701: Split brain on log disk full (3.5)   Full release notes: https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310801&version=12346098  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-02-15T21:23:01Z","2020-02-17T15:53:44Z"
"","8335","MINOR: Add implicit for Serde[UUID] to Streams Scala API","A `Serde` for `UUID` was added in 164ef9462e9d. Add an `implicit def` for it to `org.apache.kafka.streams.scala.Serdes` for convenience.","open","","vkorenev","2020-03-23T23:27:29Z","2020-03-23T23:27:29Z"
"","8362","MINOR: Don't process sasl.kerberos.principal.to.local.rules on client-side","`sasl.kerberos.principal.to.local.rules` is a broker-side config that is not used on clients. We should ignore this for client-side channel builders.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-26T09:03:53Z","2020-03-26T15:30:52Z"
"","7680","HOTFIX: Fix compilation error in DelayedFetchTest","`replicaManager.isAddingReplica` is not available in 2.4  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-11-11T20:37:47Z","2019-11-11T23:24:46Z"
"","7842","MINOR; Refactor KafkaAdminClientTest to reduce the boilerplate code","`KafkaAdminClientTest` contains many code repetitions which could be removed. This PR removes most of the boiler plate code.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-12-17T10:55:02Z","2019-12-18T06:14:02Z"
"","8277","MINOR: Bump version to 2.4.2-SNAPSHOT","`gradle.properties` was done separately, I missed doing these at the same time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-03-11T17:22:48Z","2020-03-12T01:47:52Z"
"","7887","KAFKA-9360: Fix heartbeat and checkpoint emission can not turn off","`emit.heartbeats.enabled` and `emit.checkpoints.enabled` are supposed to be the knobs to control if the heartbeat message or checkpoint message will be sent or not to the topics respectively. In our experiments, setting them to false will not suspend the activity in their SourceTasks, e.g. MirrorHeartbeatTask, MirrorCheckpointTask.  The observations are, when setting those knobs to false, huge volume of `SourceRecord` are being sent without interval, causing significantly high CPU usage of MirrorMaker 2 instance, GC time and congesting the single partition of the heartbeat topic and checkpoint topic.       The proposed fix in the following PR is to (1) explicitly check if `interval` is set to negative (e.g. -1), when the `emit.heartbeats.enabled` or `emit.checkpoints.enabled` is off. (2) if `interval` is indeed set to negative, put the thread in sleep mode for a while (e.g. 5 seconds) and return null, in order to prevent it from (1) hogging the cpu, (2) sending heartbeat or checkpoint messages to Kafka topics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ning2008wisc","2020-01-02T23:13:53Z","2020-01-30T10:43:35Z"
"","7707","HOTFIX: Fix unit tests that failed when executed from IDE","`ClientMetricsTest#shouldAddVersionMetric` and `ClientMetricsTest#shouldAddCommitIdMetric` failed because they assumed the existence of a properties file on the class path that was not there when executedfrom the IDE. `ClientMetricsTest#shouldAddCommitIdMetric` also failed when the test was executed from an archive of a release candidate. The cause is the missing `.git` directory from which the commit ID is extracted and written to the properties file by gradle.  Both issues are fixed by adding the file to `streams/src/test/resources`.  Additionally, I did some refactorings on the test class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2019-11-18T16:16:06Z","2019-11-19T01:05:29Z"
"","7652","KAFKA-9219: prevent NullPointerException when polling metrics from Kafka Connect","`assignmentSnapshot` may not always get initialized in some cases. If `assignmentSnapshot` is not initialized, registering `assigned-connectors` and `assigned-tasks` metrics upfront will cause NullPointerException when the two metrics are polled via JmxReporter later  ``` [2019-11-05 23:56:57,909] WARN Error getting JMX attribute 'assigned-tasks' (org.apache.kafka.common.metrics.JmxReporter:202) java.lang.NullPointerException 	at org.apache.kafka.connect.runtime.distributed.WorkerCoordinator$WorkerCoordinatorMetrics$2.measure(WorkerCoordinator.java:316) 	at org.apache.kafka.common.metrics.KafkaMetric.metricValue(KafkaMetric.java:66) 	at org.apache.kafka.common.metrics.JmxReporter$KafkaMbean.getAttribute(JmxReporter.java:190) 	at org.apache.kafka.common.metrics.JmxReporter$KafkaMbean.getAttributes(JmxReporter.java:200) 	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttributes(DefaultMBeanServerInterceptor.java:709) 	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttributes(JmxMBeanServer.java:705) 	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1449) 	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76) 	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309) 	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401) 	at javax.management.remote.rmi.RMIConnectionImpl.getAttributes(RMIConnectionImpl.java:675) 	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357) 	at sun.rmi.transport.Transport$1.run(Transport.java:200) 	at sun.rmi.transport.Transport$1.run(Transport.java:197) 	at java.security.AccessController.doPrivileged(Native Method) 	at sun.rmi.transport.Transport.serviceCall(Transport.java:196) 	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573) 	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:835) 	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688) 	at java.security.AccessController.doPrivileged(Native Method) 	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) 	at java.lang.Thread.run(Thread.java:748) [2019-11-05 23:57:02,821] INFO [Worker clientId=connect-1, groupId=backup-mm2] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:629) [2019-11-05 23:57:02,821] INFO [Worker clientId=connect-2, groupId=cv-mm2] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:609) [2019-11-05 23:57:07,822] INFO [Worker clientId=connect-2, groupId=cv-mm2] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:629) [2019-11-05 23:57:07,822] INFO Kafka MirrorMaker stopped. (org.apache.kafka.connect.mirror.MirrorMaker:191) ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ning2008wisc","2019-11-06T04:49:56Z","2019-11-27T11:46:50Z"
"","8310","HOTFIX: fix flaky StateDirectoryTest.shouldReturnEmptyArrayIfListFile…","```StateDirectoryTest.shouldReturnEmptyArrayIfListFilesReturnsNull``` always moves the stage dir to /tmp/state-renamed so it always fails if there is already a folder (for example, the stuff leaved by previous test)  We should just delete the folder to make stage dir disappear  related to #8304  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-17T13:34:06Z","2020-03-18T18:53:21Z"
"","8255","KAFKA-9686 MockConsumer#endOffsets should be idempotent","```scala     private Long getEndOffset(List offsets) {         if (offsets == null || offsets.isEmpty()) {             return null;         }         return offsets.size() > 1 ? offsets.remove(0) : offsets.get(0);     } ``` The above code has two issues. 1. It does not return the latest offset since the latest offset is at the end of offsets 1. It removes the element from offsets so MockConsumer#endOffsets gets non-idempotent  https://issues.apache.org/jira/browse/KAFKA-9686  The following flaky is fixed by this PR 1. KafkaBasedLogTest.testSendAndReadToEnd  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-09T11:36:28Z","2020-03-10T15:43:01Z"
"","8344","KAFKA-9750 Flaky test kafka.server.ReplicaManagerTest.testFencedError…","```scala       // change the epoch from 0 to 1 in order to make fenced error       replicaManager.becomeLeaderOrFollower(0, leaderAndIsrRequest(1), (_, _) => ())       TestUtils.waitUntilTrue(() => replicaManager.replicaAlterLogDirsManager.fetcherThreadMap.values.forall(_.partitionCount() == 0),         s""the partition=$topicPartition should be removed from pending state"") ``` The root cause is race condition. The partition is add to the end instead of being removed if the epoch in ReplicaAlterLogDirsThread is increased. This PR includes following changes. 1. controls the lock of ReplicaAlterLogDirsThread to make the fenced error happen almost. 1. wait for the completion of thread  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-25T10:11:13Z","2020-04-03T16:02:23Z"
"","8309","KAFKA-9730 add tag ""partition"" to BrokerTopicMetrics so as to observe…","```Partitioner``` enable us to dispatch the data to the specify partition as we wish. However, we can't observe the partition metrics if they are on the same broker. The root cause is that the key of topic metrics doesn't contain ""partition"" so all partitions on the same broker are merged into single metrics (see attachment).  **before** ![trunk](https://user-images.githubusercontent.com/6234750/76848178-30ac7d00-687e-11ea-8d28-6b7eba17c89b.jpg)  **after** ![patch](https://user-images.githubusercontent.com/6234750/76848197-3a35e500-687e-11ea-8fa3-ee36a9d59586.jpg)  [KAFKA-9730](https://issues.apache.org/jira/browse/KAFKA-9730)  [KIP-581](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=148642648)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-17T10:36:39Z","2020-05-29T15:20:04Z"
"","8303","Minor: fix flaky StateDirectoryTest.shouldOnlyListNonEmptyTaskDirecto…","```File.listFiles``` does not guarantee any order so the comparison of two list may fail.  related to #8267  ``` java.lang.AssertionError: expected:<[/tmp/kafka-k0Nn0/applicationId/0_0, /tmp/kafka-k0Nn0/applicationId/0_1]> but was:<[/tmp/kafka-k0Nn0/applicationId/0_1, /tmp/kafka-k0Nn0/applicationId/0_0]> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:120) 	at org.junit.Assert.assertEquals(Assert.java:146) 	at org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldOnlyListNonEmptyTaskDirectories(StateDirectoryTest.java:318) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2020-03-16T16:30:19Z","2020-03-17T04:05:22Z"
"","8381","MINOR: Exclude '**/*Suite.class' from test, unitTest and integrationTest","``` ./gradlew unitTest integrationTest \     --profile --no-daemon --continue -PtestLoggingEvents=started,passed,skipped,failed ""$@"" \     || { echo 'Test steps failed'; exit 1; } ```  The tasks ```unitTest``` and ```integrationTest``` used to run tests don't exclude the ```**/*Suite``` so the tests included by Suite class are executed two times. For example: ``` 11:42:25 org.apache.kafka.streams.integration.StoreQuerySuite > org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldBeAbleToQueryMapValuesState STARTED 11:42:26  11:42:26 org.apache.kafka.streams.integration.GlobalKTableIntegrationTest > shouldKStreamGlobalKTableJoin PASSED 11:42:30  11:42:30 org.apache.kafka.streams.integration.StoreQuerySuite > org.apache.kafka.streams.integration.QueryableStateIntegrationTest.shouldBeAbleToQueryMapValuesState PASSED ... 11:48:42 org.apache.kafka.streams.integration.QueryableStateIntegrationTest > shouldBeAbleToQueryMapValuesState STARTED 11:48:46  11:48:46 org.apache.kafka.streams.integration.QueryableStateIntegrationTest > shouldBeAbleToQueryMapValuesState PASSED ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2020-03-28T15:00:23Z","2020-03-30T03:28:59Z"
"","7561","KAFKA-7739: Tiered storage","[WIP] This is the initial **draft** version of the KIP-405. It includes the initial set of changes required for plugging in a `RemoteStorageManager`. We will update KIP-405 and this PR in the next few days with more details.  KIP-405 is located at https://s.apache.org/pk53b  This PR contains HDFS/S3  implementations to discuss `RemoteStorageManager` APIs and it will also be easier to review and test the proposed APIs end to end. This implementation will be removed before it is ready to be merged.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2019-10-19T17:55:47Z","2022-01-20T13:38:58Z"
"","7831","MINOR: add UPGRADE_FROM to 2.0-2.3 config docs","[This PR](https://github.com/apache/kafka/pull/7825) but for 2.0-2.3  Targeted against 2.3, should be cherry-picked back to 2.0 after which we can merge to kafka-site","closed","docs,","ableegoldman","2019-12-13T19:29:19Z","2020-01-02T23:45:41Z"
"","7778","KAFKA-9267: ZkSecurityMigrator should not create /controller node","[KAFKA-9267](https://issues.apache.org/jira/browse/KAFKA-9267)  ZkSecurityMigrator might create a PERSISTENT /controller node with null data, it will lead to controller can't elect.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","NanerLee","2019-12-04T16:19:04Z","2019-12-09T05:52:27Z"
"","8455","KAFKA-9845: Warn users about using config providers with plugin.path property","[Jira](https://issues.apache.org/jira/browse/KAFKA-9845)  The initially-attempted fix (present as the first commit in this PR) caused the transformed worker configuration to be passed to the `Plugins` instance which performs plugin path scanning, instead of the raw (pre-transform) worker configuration. This had the added benefit that worker configuration validation would take place _before_ plugin path scanning, which in some environments can take a while.  Unfortunately, as a side effect, the loading of properties of type `CLASS` from the `WorkerConfig` was broken. No clear fix is apparent due to this circular dependency issue: `Plugins` needs to be instantiated and `compareAndSwapWithDelegatingLoader` invoked before creating the worker config since it loads classes when instantiated, and the config provider logic specified in the worker config needs to be performed on the `plugin.path` property before instantiating the `Plugins` instance. This is especially complicated by the fact that config providers are loaded as plugins.  As a result, the new proposal is to simply warn the user whenever it appears they are using a config provider in the value for the `plugin.path` property and link to [KAFKA-9845](https://issues.apache.org/jira/browse/KAFKA-9845).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-04-09T15:38:07Z","2022-04-02T05:46:01Z"
"","8369","KAFKA-9771: Port patch for inter-worker Connect SSL from Jetty 9.4.25","[Jira](https://issues.apache.org/jira/browse/KAFKA-9771)  For reasons outlined in the ticket, we can't upgrade to a version of Jetty with the bug fixed, or downgrade to one prior to the introduction of the bug. Luckily, the actual fix is pretty straightforward and can be ported over to Connect for use until it's possible to upgrade to a version of Jetty with that bug fixed: https://github.com/eclipse/jetty.project/pull/4404/files#diff-58640db0f8f2cd84b7e653d1c1540913R2188-R2193  The changes here have been verified locally; currently investigating how they can best be tested via unit/integration/system tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-03-26T22:00:06Z","2020-10-16T05:46:08Z"
"","8360","KAFKA-9768: Fix handling of rest.advertised.listener config","[Jira](https://issues.apache.org/jira/browse/KAFKA-9768)  The `rest.advertised.listener` config is currently broken as setting it to `http` when listeners are configured for both `https` and `http` will cause the framework to choose whichever of the two listeners is listed first. The changes here attempt to fix this by checking not only that `ServerConnector::getName` begins with the specified protocol, but also that that protocol is immediately followed by an underscore, which the framework uses as a delimiter between the protocol and the remainder of the connector name.  An existing unit test for the `RestServer::advertisedUrl` method has been expanded to include a case that fails with the framework in its current state and passes with the changes in this PR.  See https://github.com/apache/kafka/blob/ee0ef09db4e450f3d32cc70b21eeca927820fefe/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java#L181 and https://github.com/apache/kafka/blob/ee0ef09db4e450f3d32cc70b21eeca927820fefe/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestServer.java#L186 for how the framework sets the name for the `ServerConnector` instances it creates.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-03-26T01:18:41Z","2022-02-27T03:55:09Z"
"","8357","KAFKA-9767: Add logging to basic auth rest extension","[Jira](https://issues.apache.org/jira/browse/KAFKA-9767)  The changes here are strictly targeted towards improving logging for the basic auth rest extension. However, in several sections, some refactoring is also performed in order to reduce nesting especially with the `else` halves of `if`/`else` statements.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-03-26T00:27:48Z","2020-05-24T14:09:41Z"
"","8165","KAFKA-9601: Stop logging raw connector config values","[Jira](https://issues.apache.org/jira/browse/KAFKA-9601)  whoopsie daisy","closed","connect,","C0urante","2020-02-25T06:23:26Z","2020-11-05T19:10:03Z"
"","8135","KAFKA-9570: Define SSL configs in all worker config classes, not just distributed","[Jira](https://issues.apache.org/jira/browse/KAFKA-9570)  All SSL-related configs are currently defined only in the `DistributedConfig` class, even though they are applicable for standalone mode as well (since standalone mode also supports the Connect REST API). Because of how these configs are parsed by the framework, it's currently impossible to configure Connect in standalone mode to use SSL for the REST API with a password-protected keystore, key, or truststore, and even if no password protection is required, SSL configs will not be picked up correctly by the worker if any of the worker configs start with the `listeners.https.` prefix.  These changes define the relevant SSL-related configs in the parent `WorkerConfig` class, which should fix how they are picked up in standalone mode.  A new unit test is added to verify that the `StandaloneConfig` picks up these configs correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-02-18T21:30:21Z","2020-06-05T21:02:18Z"
"","8118","KAFKA-9472: Remove deleted Connect tasks from status store","[Jira](https://issues.apache.org/jira/browse/KAFKA-9472)  Although the statuses for tasks are removed from the status store when their _connector_ is deleted, their statuses are not removed when only the task is deleted, which happens in the case that the number of tasks for a connector is reduced.  These changes add logic for deleting the statuses for those tasks from the status store whenever a rebalance has completed and the leader of a distributed cluster has detected that there are recently-deleted tasks. Standalone is also updated to accomplish this.  Unit tests for the `DistributedHerder` and `StandaloneHerder` classes are updated, and an integration test has been added.","closed","connect,","C0urante","2020-02-14T20:58:16Z","2020-05-25T16:10:04Z"
"","7823","KAFKA-9228: Force reconfiguration of tasks if converter or Kafka client configs are changed","[Jira](https://issues.apache.org/jira/browse/KAFKA-9228)  There's a subtle bug in the Connect framework that hasn't been a common issue due to the patterns many connector developers follow for their `Connector::taskConfigs` method (more on the cause of this is outlined in the Jira issue). It's possible that reconfiguring a connector but only altering its converters and/or Kafka clients will fail to propagate to the connector's tasks.  This change should fix that bug by caching the client and converter configs for running connectors in the herder and, if a change is detected to any of those configs, force a restart of the connector's tasks. This is done in both the standalone and distributed herder.  A unit test is added for the shared converter/client config extraction logic, and an integration test is added to confirm the fix which fails on the current `trunk` but passes on the source branch of this PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-12-12T00:40:20Z","2022-02-27T03:55:15Z"
"","7734","KAFKA-9223: Mask exit procedure in rebalance integration test to prevent call to System::exit","[Jira](https://issues.apache.org/jira/browse/KAFKA-9223)  We've been encountering some build instability that appears to be due to the `RebalanceSourceConnectorsIntegrationTest` class. Somehow, that test is causing an ungraceful shutdown of one or more of its embedded Connect workers, which then in turn invoke `Exit::exit`. The Connect integration test framework has support for overriding the behavior of `Exit::exit` to prevent it from calling `System::exit`; the changes here use that feature to help bring back build stability. Now, if the embedded Connect worker fails to shut down gracefully, a warning is logged but the test still passes and (more importantly), `System::exit` is never invoked.  If approved, this fix should be backported through to 2.3, when the test was introduced.  Tested locally 10x over with all integration tests. No other test appears to be invoking `Exit::exit`, so I've restricted the scope of the changes here to just the RebalanceSourceConnectorsIntegrationTest.","closed","","C0urante","2019-11-22T00:55:14Z","2019-11-22T17:16:22Z"
"","7593","KAFKA-9083: Various fixes/improvements for Connect's Values class","[Jira](https://issues.apache.org/jira/browse/KAFKA-9083)  The following functional changes are implemented here: • Top-level strings beginning with `""true""`, `""false""` and then followed by token delimiters (e.g., `""true}""` and `""false]""`) are parsed as strings, not as booleans • The empty array (`""[]""`) is now parsed as an array with a null value schema • The empty map (`""{}""`) is now parsed as a map with null key and value schemas • Arrays with all-null elements are now parsed successfully (whereas before an NPE was thrown) as an array with a null value schema • Maps with all-null values are now parsed as maps with null value schemas, but non-null key schemas • Strings that appear to be arrays at first but are missing comma delimiters (e.g., `""[0 1 2]""`) are now parsed as strings instead of arrays • A small improvement is made to the debug message generated when map parsing fails due to unexpected comma delimiters (""Unable to parse a map entry has no key or value"" is changed to ""Unable to parse a map entry with no key or value"") • A small improvement is made to the debug message generated when map parsing fails due to a missing `}` (""Map is missing terminating ']'"" is changed to ""Map is missing terminating '}'"") • A small improvement is made to the debug message generated when array or map parsing fails and parsing is reset to process the input as a string (""Unable to parse the value as a map"" is changed to ""Unable to parse the value as a map or an array"") • Embedded values that lack surrounding quotes (e.g., `foo` in `""[foo]""`) are no longer treated as strings; this is in line with the JSON-like representation that is meant to be supported by the `Values` class and prevents errors such as parsing `""[0 1 2]""` as an array containing a single string element with a value of `""0 1 2""` • The top-level string `""null""` is now parsed as `null` instead of the string `""null""`; this does not break attempts to convert the top-level string `""null""` into a string (which should also be the string `""null""`)  Every change (except for log message alterations) is verified with one or more unit tests, and several unit tests are also added to prevent regression in functionality that, while not currently broken, is subtle enough that it may be missed in future changes without tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-10-24T19:45:36Z","2020-10-16T06:15:42Z"
"","7532","KAFKA-9051: Prematurely complete source offset read requests for stopped tasks","[Jira](https://issues.apache.org/jira/browse/KAFKA-9051)  The changes here cause source offset readers to forcefully close when tasks fail to shut down within the graceful shutdown timeout period. When this happens, all pending and future offset read requests will throw an exception.  This is in line with the [API for the OffsetStorageReader class](https://kafka.apache.org/23/javadoc/org/apache/kafka/connect/storage/OffsetStorageReader.html#offsets-java.util.Collection-), which states that ""The only case when an exception will be thrown is if the entire request failed, e.g. because the underlying storage was unavailable."". If a task is blocked on reading offsets from Kafka to the point where it has failed to shut down within the graceful shutdown timeout period, it's safe to say that the offset read request has failed and as a result, throw an exception.  Initially, I considered just returning null values from the offset reader once closed; however, this may cause source tasks to mutate some external state as if there were no offset for the requested source partitions and may negatively impact other tasks that have since been started by this connector. An exception is safer, and with the distinction between throwing one when a task has exceeded its graceful shutdown period vs. just being scheduled to stop, should not damage the functionality of tasks running in a healthy environment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-10-16T18:12:41Z","2020-10-16T06:17:33Z"
"","7525","KAFKA-9046: Use top-level worker configs for connector admin clients","[Jira](https://issues.apache.org/jira/browse/KAFKA-9046)  The changes here are meant to find a healthy compromise between the pre- and post-KIP-458 functionality of Connect workers when configuring admin clients for use with DLQs. Before KIP-458, admin clients were configured using the top-level worker configs; after KIP-458, they are configured using worker configs with a prefix of `admin.` and then optionally overridden by connector configs with a prefix of `admin.override.`. The behavior proposed here is to use, in ascending order of precedence, the top-level worker configs, worker configs prefixed with `admin.`, and connector configs prefixed with `admin.override.`; essentially, use the pre-KIP-458 behavior by default but allow it to be overridden by the post-KIP-458 behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-10-15T23:39:48Z","2020-10-16T05:55:11Z"
"","8444","KAFKA-8869: Remove task configs for deleted connectors from config snapshot","[Jira](https://issues.apache.org/jira/browse/KAFKA-8869)  Currently, if a connector is deleted, its task configurations will remain in the config snapshot tracked by the `KafkaConfigBackingStore`. This causes issues with incremental cooperative rebalancing, which utilizes that config snapshot to determine which connectors and tasks need to be assigned across the cluster. Specifically, it first checks to see which connectors are present in the config snapshot, and then, for each of those connectors, queries the snapshot for that connector's task configs.  The lifecycle of a connector is for its configuration to be written to the config topic, that write to be picked up by the workers in the cluster and trigger a rebalance, the connector to be assigned to and started by a worker, task configs to be generated by the connector and then written to the config topic, that write to be picked up by the workers in the cluster and trigger a second rebalance, and finally, the tasks to be assigned to and started by workers across the cluster.  There is a brief period in between the first time the connector is started and when the second rebalance has completed during which those stale task configs from a previously-deleted version of the connector will be used by the framework to start tasks for that connector.  This fix aims to eliminate that window by preemptively clearing the task configs from the config snapshot for a connector whenever it has been deleted.  An existing unit test is modified to verify this behavior, and should provide sufficient guarantees that the bug has been fixed, since the cause of the behavior has been narrowed down to incorrect values in the `taskConfigs` field for the `ClusterConfigState` provided by the `KafkaConfigBackingStore`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2020-04-08T04:48:16Z","2022-02-27T03:55:07Z"
"","8069","KAFKA-9374: Make connector interactions asynchronous","[Jira ticket](https://issues.apache.org/jira/browse/KAFKA-9374)  These changes allow herders to continue to function even when a connector they are running hangs in its start, stop, initialize, validate, and/or config methods.  The main idea is to make these connector interactions asynchronous and accept a callback that can be invoked upon the completion (successful or otherwise) of these interactions. The distributed herder handles any follow-up logic by adding a new herder request to its queue in that callback, which helps preserve some synchronization and ordering guarantees provided by the current tick model.  If any connector refuses to shut down within a graceful timeout period, the framework will abandon it and potentially start a new connector in its place (in cases such as connector restart or reconfiguration).  Existing unit tests for the distributed herder and worker have been modified to reflect these changes, and a new integration test named `BlockingConnectorTest` has been added to ensure that they work in practice.","closed","connect,","C0urante","2020-02-08T02:36:53Z","2020-10-09T18:27:20Z"
"","7923","KAFKA-8037: Added deserialization check on restoration of global state stores from source topics","@vvcephei @guozhangwang First part for global state stores. A check for normal changelog topics is included, in this case the check is not performed.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","pkleindl","2020-01-09T22:21:54Z","2020-01-11T19:17:55Z"
"","7908","KAFKA-9068: Fix incorrect JavaDocs for `Stores.xxxSessionStore(...)`","@mjsax https://stackoverflow.com/questions/58426767/what-happens-if-session-window-ends-before-retention-period-and-inactivity-gap-e  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","zealiard","2020-01-08T07:46:35Z","2020-01-09T14:25:36Z"
"","7871","KAFKA-9334: Added more unit tests for Materialized class","@halorgium","closed","streams,","SainathB","2019-12-26T02:39:00Z","2019-12-31T20:29:55Z"
"","7723","KAFKA-9190: Close connections with expired authentication sessions","@hachikuji Here's the PR with two commits: one for the autoboxing/unboxing cleanup and the other for the close of the connection when an authentication session expires.  You had originally suggested invoking `selector.close(channel.id)` but I committed `close(channel.id)` instead because that invokes `selector.close()` but also performs additional cleanup associated with the termination of a connection.","closed","","rondagostino","2019-11-20T23:16:30Z","2019-12-04T16:39:40Z"
"","7520","KAFKA-9011: Scala bindings for flatTransform and flatTransformValues in KStream","@cadonna   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","kokachev","2019-10-15T07:04:05Z","2020-06-12T23:30:50Z"
"","8183","MINOR: Revert Jetty to 9.4.25","9.4.25 renamed closeOutput to completeOutput (https://github.com/eclipse/jetty.project/commit/c5acf965067478784b54e2d241ec58fdb0b2c9fe), which is a method used by recent Jersey versions including the latest (2.30.1). An example of the error:  > java.lang.NoSuchMethodError: org.eclipse.jetty.server.Response.closeOutput()V > 	at org.glassfish.jersey.jetty.JettyHttpContainer$ResponseWriter.commit(JettyHttpContainer.java:326)  The request still completes and hence why no test fails. We should think about how to improve the testing for this kind of problem, but I want to get the fix in before 2.5 RC0.  Credit to @rigelbm for finding this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-02-27T15:22:36Z","2020-02-27T21:06:49Z"
"","7601","MINOR: Fix JAAS configuration numbering","7.3.1.1 JAAS configuration for Kafka brokers was followed by 7.3.1.4 JAAS configuration for Kafka clients instead of 7.3.1.2.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","makubi","2019-10-27T20:50:41Z","2019-11-10T17:37:03Z"
"","7907","KAFKA-9384; Loop improvements","3 loops in one and this immutable  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-08T05:03:46Z","2020-01-18T15:21:56Z"
"","8416","KAFKA-9739:  Fixes null key changing child node","2.4 port of #8400 since cherry-picking not possible  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-04-03T16:29:02Z","2020-04-03T19:14:45Z"
"","8358","KAFKA-9756: Process more than one record of one task at a time","1. Within a single while loop, process the tasks in AAABBBCCC instead of ABCABCABC. This also helps the follow-up PR to time the per-task processing ratio to record less time, hence less overhead.  2. Add thread-level process / punctuate / poll / commit ratio metrics.  3. Fixed a few issues discovered (inline commented).  I am adding metrics test coverage for 2) above in another PR, which added more sensor / metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-03-26T01:01:31Z","2020-04-25T00:00:39Z"
"","8063","HOTFIX: Fix two test failures in JDK11","1. StoreChangelogReaderTest.shouldRequestCommittedOffsetsAndHandleTimeoutException[1] This is due to stricter ternary operator type casting  2. KStreamImplTest.shouldSupportTriggerMaterializedWithKTableFromKStream This is added recently where String typed values for , in J8 it is allowed but in J11 it is not allowed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-02-07T21:15:33Z","2020-04-24T23:52:05Z"
"","8058","KAFKA-9481: Graceful handling TaskMigrated and TaskCorrupted","1. Removed task field from TaskMigrated; the only caller that encodes a task id from StreamTask actually do not throw so we only log it. To handle it on StreamThread we just always enforce rebalance (and we would call onPartitionsLost to remove all tasks as dirty).  2. Added TaskCorruptedException with a set of task-ids. The first scenario of this is the restoreConsumer.poll which throws InvalidOffset indicating that the logs are truncated / compacted. To handle it on StreamThread we first close the corresponding tasks as dirty (if EOS is enabled we would also wipe out the state stores), and then revive them into the CREATED state.  3. Also fixed a bug while investigating KAFKA-9572: when suspending / closing a restoring task we should not commit the new offsets but only updating the checkpoint file.  4. Re-enabled the unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-02-07T00:44:06Z","2020-04-24T23:52:03Z"
"","8267","KAFKA-6647: Do note delete the lock file while holding the lock","1. Inside StateDirectory#cleanRemovedTasks, skip deleting the lock file (and hence the parent directory) until releasing the lock. And after the lock is released only go ahead and delete the parent directory if `manualUserCall == true`. That is, this is triggered from `KafkaStreams#cleanUp` and users are responsible to make sure that Streams instance is not started and hence there are no other threads trying to grab that lock.  2. As a result, during scheduled cleanup the corresponding task.dir would not be empty but be left with only the lock file, so effectively we still achieve the goal of releasing disk spaces. For callers of `listTaskDirectories` like KIP-441 (cc @ableegoldman to take a look) I've introduced a new `listNonEmptyTaskDirectories` which excludes such dummy task.dirs with only the lock file left.  3. Also fixed KAFKA-8999 along the way to expose the exception while traversing the directory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-03-10T23:13:53Z","2020-04-25T00:00:31Z"
"","7576","KAFKA-9048 Pt1: Remove Unnecessary lookup in Fetch Building","1. Get rid of `partitionStates` that creates a new `PartitionState` for each state since all the callers do not require it to be a Seq.  2. Modify ReplicaFetcherThread constructor to fix the broken benchmark path.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-22T16:41:00Z","2020-04-24T23:58:29Z"
"","8405","KAFKA-9801: Still trigger rebalance when static member joins in CompletingRebalance phase","1. Fix the direct cause of the observed issue on the client side: when heartbeat getting errors and resetting generation, we only need to set it to UNJOINED when it was not already in REBALANCING; otherwise, the join-group handler would throw the retriable UnjoinedGroupException to force the consumer to re-send join group unnecessarily.  2. Fix the root cause of the issue on the broker side: we should still trigger rebalance when static member joins in CompletingRebalance phase; otherwise the member.ids would be changed when the assignment is received from the leader, hence causing the new member.id's assignment to be empty.  3. Added log4j entries as a by-product of my investigation.  Testing coverage still in progress.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-04-02T01:25:04Z","2020-04-24T23:51:53Z"
"","7846","KAFKA-9113: Extract Producer to RecordCollector","1. Extract producer from StreamTask to the RecordCollector; make the task agnostic of the underlying embedded client and instead let RecordCollector handle clients and also interpret their thrown exception (AbstractTask still has a reference of the consumer in order to pause / resume partition, and also get committed offsets; the latter function may be moved into RecordCollector in a later PR).  1.a Producer.send() could throw KafkaException whose cause is ProducerFenced; 1.b. Producer.send() could return other exceptions as passed from the callback (ProducerFenced and UnknownProducerId); 1.c. Consumer.commit() could throw CommitFailedException; 1.d Producer.[otherAPI] and Consumer.[otherAPI] could throw other KafkaException, including TimeoutException.  1.a/b/c are interpreted as TaskMigratedException and rethrown from RecordCollector; 1.d are interpreted as general StreamsException and rethrown from RecordCollector.  Task would not handle any such exception, all of them thrown all the way up to the StreamThread; thread handles TaskMigratedException by closing the task ""uncleanly"", and then handles any other StreamsException as fatal and shutdown itself --- this is a major change in exception handling, e.g. today when close(clean) we try to immediately capture the exception within task, and then immediately handle by re-try close(unclean), which causes very messy hierarchy. **This is not completely done in this PR but we are going to that direction in future PRs.**  2. ProcessorStateManager and RecordCollector are created before StreamTask / StandbyTask and are passed in as parameters; this also have a good effect in unit tests that we can use nice mocks for them when only testing task functionalities. Also because of this I've removed a bunch of unit tests from the task level (so do not be afraid of the LOC size, the non-testing part is actually not huge), and will move them to state-manager / record-collector after we've done the whole cleanup. During this period the test coverage would be dropped a bit but we will eventually add them back to other classes.  3. Task close / suspend procedure are cleaned up based on the `clean` flag (the `isZombie` flag is now consolidated into the previous one). Also as a side-effect we fixed the issue that double checkpointing in committing / closing.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-12-17T23:37:28Z","2020-01-02T23:45:36Z"
"","8060","KAFKA-9274: Gracefully handle timeout exception","1. Delay the initialization (producer.initTxn) from construction to maybeInitialize; if it times out we just swallow and retry in the next iteration.  2. If completeRestoration (consumer.committed) times out, just swallow and retry in the next iteration.  3. For other calls (producer.partitionsFor, producer.commitTxn, consumer.commit), treat the timeout exception as fatal.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2020-02-07T18:32:39Z","2020-02-15T01:28:15Z"
"","7604","KAFKA-9110: Improve efficiency of disk reads when TLS is enabled","1. Avoid a buffer allocation and a buffer copy per file read. 2. Ensure we flush `netWriteBuffer` successfully before reading from disk to avoid wasted disk reads. 3. 32k reads instead of 8k reads to reduce the number of disk reads (improves efficiency for magnetic drives and reduces the number of system calls). 4. Update SslTransportLayer.write(ByteBuffer) to loop until the socket buffer is full or the src buffer has no remaining bytes. 5. Renamed MappedByteBuffers to ByteBufferUnmapper since it's also applicable for direct byte buffers. 6. Skip empty `RecordsSend` 7. Some minor clean-ups for readability.  I ran a simple consumer perf benchmark on a 6 partition topic (large enough not to fit into page cache) starting from the beginning of the log with TLS enabled on my 6 core MacBook Pro as a sanity check. This laptop has fast SSDs so it benefits less from the larger reads than the case where magnetic disks are used. Consumer throughput was ~260 MB/s before the changes and ~300 MB/s after (~15% improvement).  Credit to @junrao  for pointing out that this code could be more efficient.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-28T13:38:31Z","2019-11-05T12:51:31Z"
"","7849","MINOR: Refactor versions in `FutureSubscriptionInfo`","- simplifies class `FutureSubscriptionInfo` - makes the purpose of the code clearer   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-12-18T13:10:40Z","2019-12-18T17:16:43Z"
"","8073","MINOR: Fix a number of warnings in clients test","- Removed dead code - Removed unneeded annotations - Added generics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-02-08T15:05:02Z","2020-02-20T14:54:42Z"
"","7852","MINOR: Kafka Streams Scala API cleanup","- remove wildcard imports - avoid the usage of implicit conversions - some minor side cleanup  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-12-19T01:01:49Z","2019-12-20T21:18:30Z"
"","7584","KAFKA-8980: Refactor state-store-level streams metrics","- Refactors metrics according to KIP-444 - Introduces `StateStoreMetrics` as a central provider for state store metrics - Adds metric scope (a.k.a. store type) to the in-memory suppression buffer  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-10-23T12:09:34Z","2020-06-12T23:27:23Z"
"","7977","KAFKA-9441: Refactor Kafka Streams commit logic","- part of KIP-447 - create to producer per thread if eos-beta enabled - commit all tasks at once if eos-beta enabled  Call for review @abbccdda @guozhangwang","closed","kip,","mjsax","2020-01-17T02:49:07Z","2020-06-12T23:23:25Z"
"","8176","KAFKA-7906: Improve logging for failed leader elections","- Log election errors within the `kafka.controller` class hierarchy instead of within `state.changed.logger` - Log failure reason when present - Verified existing tests pass  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","agam","2020-02-26T21:31:02Z","2020-03-17T17:40:38Z"
"","7566","KAFKA-8968: Refactor task-level metrics","- Introduces TaskMetrics class - Introduces dropped-records - Replaces skipped-records with dropped-records with latest built-in   metrics version - Does not add standby-process-ratio and active-process-ratio - Does not refactor parent sensors for processor node metrics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-10-21T10:38:17Z","2020-06-12T23:27:45Z"
"","8111","KAFKA-9206: throw a KafkaException when encountering CORRUPT_MESSAGE","- If the completed fetch has an error code signifying a _corrupt message_, throw a `KafkaException` that notes the _offset and the topic-partition_. - Also added a test that triggers the warning and verifies it is thrown.  - Jira: https://issues.apache.org/jira/browse/KAFKA-9206  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agam","2020-02-13T19:58:39Z","2020-02-21T19:35:19Z"
"","7482","Fixes in org.apache.kafka.connect.health","- Fixed Parameter order - Fixed NPE check conditionals","closed","connect,","941design","2019-10-10T10:13:38Z","2020-03-22T00:49:12Z"
"","8162","KAFKA-9595: switch usage of `alterConfigs` to `incrementalAlterConfigs` for kafka-configs tool","- Also, some minor refactoring for common code - Test changes to `ConfigCommandTest` - Verifies builds, tests pass  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agam","2020-02-25T00:41:55Z","2020-12-16T22:54:43Z"
"","7961","KAFKA-9431: Expose API in KafkaStreams to fetch all local offset lags","- Adds KafkaStreams#allLocalOffsetLags(), which returns lag information of all active/standby tasks local to a streams instance  - LagInfo class encapsulates the current position in the changelog, endoffset in the changelog and their difference as lag  - Lag information is a mere estimate; it can over-estimate (source topic optimization), or under-estimate.  - Each call to allLocalOffsetLags() generates a metadata call to Kafka brokers, so caution advised  - Unit and Integration tests added.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vinothchandar","2020-01-14T23:11:26Z","2020-01-17T22:11:13Z"
"","8074","MINOR: Fix a number of warnings in mirror/mirror-client","- Added generics - Removed dead code - Close closeable  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-02-08T15:24:54Z","2020-03-26T11:59:25Z"
"","8462","KAFKA-9846: Filter active tasks for running state in KafkaStreams#allLocalStorePartitionLags()","- Added check that only treats running active tasks as having 0 lag   - Tasks that are neither restoring, nor running will report 0 as currentoffset position   - Fixed LagFetchIntegrationTest to wait till thread/instance reaches RUNNING before checking lag   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vinothchandar","2020-04-10T21:22:53Z","2020-04-22T18:21:56Z"
"","7996","KAFKA-9355: Fix bug that removed RocksDB metrics after failure in EOS","- Added `init()` method to `RocksDBMetricsRecorder` - Added call to `init()` of `RocksDBMetricsRecorder` to `init()` of RocksDB store - Added call to `init()` of `RocksDBMetricsRecorder` to `openExisting()` of segmented state stores - Adapted unit tests - Added integration test that reproduces the situation in which the bug occurred  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-01-22T02:43:31Z","2020-02-13T23:16:08Z"
"","8130","MINOR: trivial cleanups, javadoc errors, omitted StateStore tests, etc.","- Add omitted `[WindowStoreBuilderTest, TimestampedWindowStoreBuilderTest]#shouldThrowNullPointerIfMetricsScopeIsNull`: Other `StateStore`s has it. - Improve `Stores` Javadoc - Remove unused method + duplicated parameters - Remove duplicated spaces - Remove unthrown `Exception`s + align javadoc parameters - Remove unnecessary `public` from `[Sink,Source]TaskContext` - Add omitted `WindowStore`, `SessionStore` test cases: `GlobalStateStoreProviderTest`, `StreamThreadStateStoreProviderTest` (Compare it with `KeyValueStore` counterpart cases.) - Fix `MeteredTimestampedWindowStore` javadoc  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2020-02-18T05:58:18Z","2020-10-08T02:08:46Z"
"","8027","KAFKA-7658: Follow up to original PR","- add more unit tests - fix bug for auto-repartitioning - some code cleanup  Follow up to PR #7985. Call for review @vvcephei @highluck   For PR must be cherry-picked to `2.5` branch.","closed","kip,","mjsax","2020-01-31T22:34:57Z","2020-06-12T23:22:15Z"
"","8221","KAFKA-9561: update task input partitions after rebalance","*update input partitions of task and topology when rebalance occured*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","avalsa","2020-03-04T08:43:06Z","2020-05-27T17:41:04Z"
"","7569","Add bounded flush API to Producer.","*More detailed description of your change, This is the patch for [KIP-514](https://cwiki.apache.org/confluence/display/KAFKA/KIP-514%3A+Add+a+bounded+flush%28%29+API+to+Kafka+Producer)  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* Add a unit test to verify RecordAccumulator will throw TimeoutException if flush can not be finished in time. Add an integration test to verify KafkaProducer will throw TimeoutException if flush can not be finished in time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kidkun","2019-10-21T18:58:01Z","2020-03-17T00:54:30Z"
"","8393","MINOR: Fix typo in version of kafka folder in Dockerfile","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","michalxo","2020-03-31T14:24:19Z","2020-04-02T00:56:48Z"
"","8340","Added topicExists functions in AdminClient","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","makarushka","2020-03-24T14:17:01Z","2020-03-24T18:34:21Z"
"","8338","KAFKA-8890: KIP-519- Make SSL context/engine configuration extensible","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","maulin-vasavada","2020-03-24T06:59:51Z","2020-04-08T14:20:33Z"
"","8272","KAFKA-9701: Add more debug log on client to reproduce the issue","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-03-11T02:56:37Z","2020-03-14T21:54:41Z"
"","8219","Some minor changes for z/OS","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","zshuo","2020-03-04T03:56:00Z","2020-03-04T03:57:44Z"
"","8207","MINOR: Update year in NOTICE","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2020-03-02T20:49:05Z","2020-03-02T22:31:54Z"
"","8139","KAFKA-9575: Mention ZooKeeper 3.5.7 upgrade","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2020-02-19T18:35:25Z","2020-02-21T13:15:48Z"
"","8122","KAFKA-9274: Add retries for handling TimeoutExceptions","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2020-02-15T07:33:05Z","2020-07-21T00:29:22Z"
"","8120","KAFKA-9274: Graceful handle TimeoutException in GlobalStateManager","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2020-02-15T01:56:16Z","2020-07-21T00:29:19Z"
"","8110","DEBUG: Handle OutOfSequence error on Stream Thread","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","abbccdda","2020-02-13T19:19:21Z","2020-04-12T18:34:52Z"
"","8086","MINOR: fix and improve StreamsConfig JavaDocs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2020-02-11T01:59:57Z","2020-02-11T22:14:35Z"
"","8082","DO NOT MERGE","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2020-02-10T22:07:50Z","2020-02-11T01:45:44Z"
"","8042","MINOR: Fix typos introduced in KIP-559","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2020-02-05T10:15:32Z","2020-10-06T20:10:39Z"
"","8033","KAFKA-9487: Follow-up PR of Kafka-9445","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","brary","2020-02-02T06:56:45Z","2020-06-12T23:21:18Z"
"","7959","KAFKA-9426: OffsetsForLeaderEpochClient Use Switch Statement","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-14T15:22:32Z","2020-01-29T21:03:18Z"
"","7949","KAFKA-9415: Review Thread Interrupt Usage","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","belugabehr","2020-01-13T17:35:23Z","2020-03-23T01:01:35Z"
"","7944","KAFKA-9411: Change Name of Default Client ID and Include Java Version","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","belugabehr","2020-01-13T01:32:09Z","2020-01-28T13:58:22Z"
"","7943","KAFKA-9410: Make groupId Optional in KafkaConsumer","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-12T17:33:32Z","2020-01-16T05:22:56Z"
"","7940","KAFKA-9408: Use StandardCharsets UTF-8 instead of UTF-8 Name","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-12T16:48:26Z","2020-01-30T12:58:54Z"
"","7939","KAFKA-9407: Return Immutable List from SchemaSourceTask","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","belugabehr","2020-01-12T16:43:31Z","2020-03-24T00:25:20Z"
"","7938","KAFKA-9406: Modify CreateDelegationTokenOptions Renewers Implementation","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","belugabehr","2020-01-12T16:37:56Z","2020-01-13T16:30:49Z"
"","7937","KAFKA-9405: Use Map API computeIfAbsent Where Applicable","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-12T16:31:16Z","2020-01-29T21:00:23Z"
"","7936","KAFKA-9404: Use ArrayList instead of LinkedList in Sensor Class","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-12T16:25:35Z","2020-03-18T13:38:36Z"
"","7922","KAFKA-9396: Use JDK emptyIterator in ConnectHeaders Class","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","belugabehr","2020-01-09T20:53:26Z","2020-01-12T16:24:31Z"
"","7919","KAFKA-9129: Add Thread ID to the InternalProcessorContext","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2020-01-09T17:02:58Z","2020-01-31T07:10:21Z"
"","7915","[DO NOT MERGE] Get IBP version","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2020-01-09T06:07:59Z","2020-01-09T21:39:33Z"
"","7912","MINOR: Link to related class in the Admin client documentation","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2020-01-09T00:02:09Z","2021-07-14T19:48:56Z"
"","7905","KAFKA-9380: Do Not Accept Null Values in MemberAssignment Constructor","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","belugabehr","2020-01-07T21:12:22Z","2020-02-24T16:08:12Z"
"","7901","KAFKA-9375 Add thread names to kafka connect","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cryptoe","2020-01-07T10:16:35Z","2020-01-31T18:21:22Z"
"","7875","KAFKA-8534 - Fix retention size behaviour","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Evelyn-Bayes","2019-12-30T05:13:38Z","2020-01-28T14:23:13Z"
"","7872","KAFKA-9337 simplify mm2 initial configs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cryptoe","2019-12-27T10:47:15Z","2020-01-07T20:10:03Z"
"","7861","[DO NOT MERGE] Test PR","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dhruvilshah3","2019-12-23T07:28:57Z","2019-12-23T07:29:30Z"
"","7847","KAFKA-6049: extend Kafka Streams Scala API for cogroup (KIP-150)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2019-12-18T02:17:29Z","2020-06-12T23:26:32Z"
"","7830","[DO NOT MERGE]: Concurrent commit offset","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-12-13T17:09:54Z","2020-01-02T19:02:15Z"
"","7765","MINOR: Bump system test version from 2.2.1 to 2.2.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-12-01T23:03:45Z","2020-10-16T06:15:43Z"
"","7764","KAFKA-9255: MessageSet v1 protocol wrong specification","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ffosilva","2019-12-01T22:09:46Z","2019-12-02T14:39:29Z"
"","7749","KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-11-26T16:12:36Z","2020-10-06T20:11:01Z"
"","7676","DEBUG: add partition empty check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-11-11T07:38:09Z","2020-05-14T02:58:16Z"
"","7629","KAFKA-9072: Add Topology naming to the dev guide","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-10-31T22:29:44Z","2019-11-13T15:55:30Z"
"","7627","MINOR: Fix Kafka Streams JavaDocs with regard to new StreamJoined class","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-10-31T21:43:55Z","2019-11-10T08:07:45Z"
"","7571","MINOR: Streams upgrade system test cleanup","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-10-21T20:21:24Z","2019-10-24T14:28:52Z"
"","7552","MINOR: system test clean up","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-10-18T01:00:27Z","2019-10-21T14:51:16Z"
"","7550","MINOR:Upgrade guide updates for KIP-479","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-10-17T20:59:47Z","2019-10-22T00:28:29Z"
"","7547","MINOR: KIP-307 upgrade guide docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-10-17T19:49:18Z","2019-10-21T22:43:20Z"
"","7538","KAFKA-6049: Add non-windowed Cogroup operator (KIP-150)","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","wcarlson5","2019-10-16T22:35:23Z","2020-06-12T23:30:42Z"
"","7529","KAFKA-8496: System test for KIP-429 upgrades and compatibility","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-10-16T15:25:14Z","2019-10-17T07:12:33Z"
"","7515","MINOR: Update Kafka Streams upgrade docs for KIP-444, KIP-470, KIP-471, KIP-474, KIP-528","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2019-10-15T00:59:30Z","2019-10-16T18:56:30Z"
"","7512","WIP: Draft for KIP 280","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","senthilm-ms","2019-10-14T23:55:00Z","2019-10-14T23:56:19Z"
"","8076","KAFKA-9512: Flaky Test LagFetchIntegrationTest.shouldFetchLagsDuringRestoration","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*   - Added additional synchronization and increased timeouts to handle flakiness  - Added some pre-cautionary retries when trying to obtain lag map  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Ran locally the entire suite 100+ times (2x as much as it took to reproduce the original issue)  ![Screen Shot 2020-02-09 at 5 57 31 PM](https://user-images.githubusercontent.com/1179324/74115829-c4937680-4b65-11ea-9a97-6c657e40f4a9.png)  P.S: Also ran the two tests I added in `QueryableStateIntegrationTest`. They seem solid (standby test ran 1900 times without flaking out)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vinothchandar","2020-02-10T02:02:36Z","2020-02-14T16:49:54Z"
"","7602","KAFKA-9107; Change constructor of StandaloneHerder of test version","*More detailed description of your change, Constructor of StandaloneHerder for test version using  MemoryConfigBackingStore instead of interface name.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","xujianhai666","2019-10-28T01:57:10Z","2020-02-11T16:09:35Z"
"","8332","KAFKA-9688: kafka-topic.sh should show KIP-455 adding and removing replicas","*More detailed description of your change,  Print AR and RR info in describe topic command, if the current reassignment exists.   *Summary of testing strategy (including rationale) 1. Commented out step B2 to B8 in KafkaController::onPartitionReassignment so the adding replica and removing replica won’t be set to the empty set in memory.   2. Recompile kafka src 3. Create a reassign json file with content below: {""partitions"":              [{""topic"": ""my-replicated-topic"",                ""partition"": 0,                ""replicas"": [1,2] }],   ""version"":1 } 4. Ran the following scripts: $ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic $ bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic Topic: my-replicated-topic	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824 	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0 $ bin/kafka-reassign-partitions.sh --reassignment-json-file bin/reassign.json --execute --zookeeper localhost:2181 $ bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 2,1,0	Isr: 2,1,0	Adding Replicas: 	Removing Replicas: 0 5. Similarly, also tested no reassignment changes won’t throw a null ptr exception. (edited)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2020-03-23T06:53:33Z","2020-03-26T16:50:19Z"
"","7563","KAFKA-9016:Warn when log dir stopped serving replicas","*KAFKA-9016:Warn when log dir stopped serving replicas instead of info*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","uttpal","2019-10-20T08:02:08Z","2019-11-08T18:49:26Z"
"","7633","KAFKA-8953: Rename UsePreviousTimeOnInvalidTimestamp to UsePartitionTimeOnInvalidTimestamp","*Added deprecate annotation in UsePreviousTimeOnInvalidTimestamp and created a new class UsePartitionTimeOnInvalidTimestamp. Also replaced usage of UsePreviousTimeOnInvalidTimestam with new class name, don't know if this was necessary but can be reverted back if necessary*  *Also renamed the test class of UsePreviousTimeOnInvalidTimestamp with new class name*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","rabi-kumar","2019-11-01T12:58:41Z","2020-06-12T23:27:16Z"
"","8152","KAFKA-9580: Log clearer error messages when there is an offset out of range (Broker & Client Change)","**More detailed description of your change**  Broker side: Added an Info level log when throwing the offsetOutOfRangeException in log.scala. Also, utilized a new error class OffsetOutOfRangeExceptionWithOffsetValues, a subclass of OffsetOutOfRangeException, as a media to pass offset values into the response to the consumer client.  Client side: Added an Info level log when the fetcher gets OffsetOutOfRangeExceptionWithOffsetValues, which is the subclass of OffsetOutOfRangeException.   **Summary of testing strategy (including rationale)**  I changed the source code and deliberately threw offsetOutOfRangeException on the broker side. Then, I started the kafka console producer & consumer and receives:  `INFO [Log partition=test-0, dir=/tmp/kafka-logs] Received request for offset 0 for partition test-0, but we only have log segments in the range 0 to 40. (kafka.log.Log)`   on the broker console and  `INFO [Consumer clientId=consumer-console-consumer-28016-1, groupId=console-consumer-28016] Fetch offset 0 is out of range for partition test-0. We only have log segments in the range from 0 to 40. Resetting offset (org.apache.kafka.clients.consumer.internals.Fetcher)`  on the client console, which means the client successfully gets the logStartOffset and lastStableOffset of the partition, even when the broker throws OffsetOutOfRangeException.","open","","ctan888","2020-02-21T18:08:05Z","2020-03-30T05:44:59Z"
"","8283","[WIP] KAFKA-7983: supporting replication.throttled.replicas in dynamic broker configuration","**More detailed description of your change**  > In KIP-226, we added the support to change broker defaults dynamically. However, it didn't support changing leader.replication.throttled.replicas and follower.replication.throttled.replicas. These 2 configs were introduced in KIP-73 and controls the set of topic partitions on which replication throttling will be engaged. One useful case is to be able to set a default value for both configs to * to allow throttling to be engaged for all topic partitions. Currently, the static default value for both configs are ignored for replication throttling, it would be useful to fix that as well.  > leader.replication.throttled.replicas and follower.replication.throttled.replicas are dynamically set through ReplicationQuotaManager.markThrottled() at the topic level. However, these two properties don't exist at the broker level config and BrokerConfigHandler doesn't call ReplicationQuotaManager.markThrottled(). So, currently, we can't set leader.replication.throttled.replicas and follower.replication.throttled.replicas at the broker level either statically or dynamically.  In this patch, we introduced two new dynamic broker configs, both of them are type of boolean:  ""leader.replication.throttled"" (default: false)  ""follower.replication.throttled"" (default: false)  If ""leader.replication.throttled"" is set to ""true"", all leader brokers will be throttled. Similarly, if ""follower.replication.throttled"" is set to ""true"", all follower brokers will be throttled. The throttle mechanism is introduced in KIP-73.   To implement the broker level throttle, I added a new class variable to ReplicationQuotaManager. The BrokerConfigHandler will call updateBrokerThrottle() and update this class variable upon receiving the config change notification from ZooKeeper. ReplicationQuotaManager::isThrottled()  **Summary of testing strategy (including rationale)**  Unit Tests: Added ReplicationQuotaManagerTest::shouldBrokerLevelThrottleAffectAllTopicPartition() to test if all topic partitions will be throttled when the broker is throttled.  I'm currently working on adding more unit tests.  Integration tests: Start Kafka and ZooKeeper using localhost, try if the alter config command can successfully change the newly added dynamic configs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ctan888","2020-03-12T06:31:38Z","2021-09-20T14:46:07Z"
"","8014","KAFKA-9451: Pass group metadata into producer in Kafka Streams","**DO NOT MERGE**  Follow up to #7977 (note that the first commit is the squashed version of 7977; this commits need to be rebased after 7977 is merged).  Second commit (still need to add tests):  Part of KIP-447: when EOS is enabled in Kafka Streams, we need to pass the consumer's group metadata into the producer to use the new GroupCoordinator fencing mechanism. We also need to first commit all task individually before we commit the transaction or write the local checkpoint file.  During an upgrade, we will still use a producer per task (and rely on transactional fencing), however, we will already pass the metadata to the producer to enable GroupCoordinator fencing (in parallel to transactional fencing), to prepare for a second round of rebalancing that switches to the thread producer model and thus disables transactional fencing.","closed","kip,","mjsax","2020-01-28T22:03:19Z","2020-06-12T23:23:07Z"
"","8401","KAFKA-9740 Add a continue option for Kafka Connect error handling (KIP-582)","**DO NOT MERGE UNTIL KIP-582 IS APPROVED**  ### Pull Request for KIP-582  Some background: currently there are two error handling options in Kafka Connect, ""none"" and ""all"". Option ""none"" will config the connector to fail fast, and option ""all"" will ignore broken records.  If users want to store their broken records, they have to config a broken record queue, which is too much work for them in some cases.   Some sink connectors have the ability to deal with broken records, for example, a JDBC sink connector can store the broken raw bytes into a separate table, a S3 connector can store that in a zipped file.  Therefore, it would be ideal if Kafka Connect provides an additional option that sends the broken raw bytes to SinkTask directly.   Wiki: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=148642653  JIRA: https://issues.apache.org/jira/browse/KAFKA-9740  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","lizihan021","2020-04-01T02:53:06Z","2020-06-11T01:19:47Z"
"","8281","KAFKA-9708: Use PluginClassLoader during connector startup","* Use classloading prioritization from Worker::startTask in Worker::startConnector  Signed-off-by: Greg Harris   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-03-11T22:11:11Z","2020-05-08T22:58:15Z"
"","8429","MINOR: Enable fatal warnings with scala 2.13","* Upgrade to Scala 2.13.2 which introduces the ability to suppress warnings. * Upgrade to scala-collection-compat 2.1.6 as it introduces the @nowarn annotation for Scala 2.12. * While at it, also update scala-java8-compat to 0.9.1. * Fix compiler warnings and add @nowarn for the unfixed ones.  Scala 2.13.2 highlights (besides @nowarn):  * Rewrite Vector (using ""radix-balanced finger tree vectors""), for performance. Small vectors are now more compactly represented. Some operations are now drastically faster on large vectors. A few operations may be a little slower. * Matching strings makes switches in bytecode.  https://github.com/scala/scala/releases/tag/v2.13.2  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-04-05T04:22:41Z","2020-04-23T09:32:04Z"
"","8442","KAFKA-9830: Implement AutoCloseable in ErrorReporter and subclasses","* The DeadLetterQueueReporter has a KafkaProducer that it must close to clean up resources * Currently, the producer and its threads are leaked every time a task is stopped * Responsibility for cleaning up ErrorReporters is transitively assigned to the     ProcessingContext, RetryWithToleranceOperator, and WorkerSinkTask/WorkerSinkTask classes * One new unit test in ErrorReporterTest asserts that the producer is closed by the dlq reporter  Signed-off-by: Greg Harris   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-04-08T00:17:59Z","2020-04-30T21:36:31Z"
"","7888","MINOR: Remove compilation warnings","* Removed warnings    1. `kafka/core/src/test/scala/integration/kafka/server/DelayedFetchTest.scala:110: local val partition in method testReplicaNotAvailable is never used`    2. `kafka/core/src/test/scala/unit/kafka/admin/ConfigCommandTest.scala:527: local val alterResourceName in method verifyAlterBrokerConfig is never used`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","daehokimm","2020-01-03T01:16:12Z","2020-01-07T01:48:47Z"
"","7615","KAFKA-9086: Refactor processor-node-level metrics","* Refactors metrics according to KIP-444 * Introduces `ProcessorNodeMetrics` as a central provider for processor node metrics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-10-30T10:41:05Z","2019-11-19T18:05:27Z"
"","8008","MINOR: Update lz4, jetty and other minor dependency bumps","* lz4: fixes identified by oss-fuzz * jetty: fixes a few recent regressions * powermock: better support for Java 12+ * zstd-jni: minor fixes * httpclient: minor fixes * spotless-plugin: minor fixes * jmh: minor fixes  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2020-01-28T05:15:39Z","2020-01-28T13:20:00Z"
"","7677","MINOR: Update to Gradle 6.3","* Introduce `gradlewAll` script to replace `*All` tasks since the approach used by the latter doesn't work since Gradle 6.0 and it's unclear when, if ever, it will work again ( see https://github.com/gradle/gradle/issues/11301). * Update release script and README given the above. * Update zinc to 1.3.5. * Update gradle-versions-plugin to 0.28.0.  The major improvements in Gradle 6.0 to 6.3 are: - Improved incremental compilation for Scala - Support for Java 14 (although some Gradle plugins like spotBugs may need to be updated or disabled, will do that separately) - Improved scalac reporting, warnings are clearly marked as such, which is very helpful.  Tested `gradlewAll` manually for the commands listed in the README and release script. For `uploadArchive`, I tested it with a local Maven repository.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-11-11T14:38:21Z","2020-04-20T02:24:26Z"
"","7549","KAFKA-9057: Backport KAFKA-8819 and KAFKA-8340 before 2.0","* Includes fixes from PR-7315 * Omits ConfigProvider and Configurable test cases and plugins * Replaces Java 8 language features with suitable Java 7 features  Signed-off-by: Greg Harris   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2019-10-17T20:54:22Z","2020-10-16T06:15:41Z"
"","8280","KAFKA-9707: Fix InsertField.Key should apply to keys of tombstone records","* Fix typo that hardcoded .value() instead of abstract operatingValue * Add test for Key transform that was previously not tested  Signed-off-by: Greg Harris   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-03-11T21:49:11Z","2020-03-26T19:12:26Z"
"","7536","MINOR: Improve FK Join docs and optimize null-fk case","* Fix the formatting and wording of the foreign-key join javadoc * Optimize handling of `null` extracted foreign keys  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-10-16T21:21:22Z","2019-10-17T06:55:31Z"
"","8279","KAFKA-9706: Handle null keys/values in Flatten transformation","* Fix DataException thrown when handling tombstone events with null value * Passes through original record when finding a tombstone record * Add tests for schema and schemaless data  Signed-off-by: Greg Harris   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-03-11T21:26:46Z","2020-03-30T22:09:27Z"
"","8366","MINOR: optimize integration test shutdown","* delete topics before tearing down multi-node clusters to avoid leader elections during shutdown * tear down all nodes concurrently instead of sequentially  Right now on the current trunk, when I run `./gradlew clean :streams:test`, I get something like: ``` BUILD SUCCESSFUL in 12m 32s ``` Running the same command with this PR, I get: ``` BUILD SUCCESSFUL in 11m 23s ```  Not huge, but it helps. A side benefit is that we avoid filling the integration test logs with hundreds/thousands of lines of log messages during shutdown related to all the leader elections for all the topics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2020-03-26T19:45:45Z","2020-03-27T15:50:44Z"
"","7859","KAFKA-9324: Drop support for Scala 2.11 (KIP-531)","* Adjust build and documentation. * Use lambda syntax for SAM types in `core`, `streams-scala` and `connect-runtime` modules. * Remove `runnable` and `newThread` from `CoreUtils` as lambda syntax for SAM types make them unnecessary. * Remove stale comment in `FunctionsCompatConversions`, `KGroupedStream`, `KGroupedTable' and `KStream` about Scala 2.11, the conversions are needed for Scala 2.12 too. * Deprecate `org.apache.kafka.streams.scala.kstream.Suppressed` and use `org.apache.kafka.streams.kstream.Suppressed` instead. * Use `Admin.create` instead of `AdminClient.create`. Static methods in Java interfaces can be invoked since Scala 2.12. I noticed that MirrorMaker 2 uses `AdminClient.create`, but I did not change them as Connectors have restrictions on newer client APIs. * Improve efficiency in a few `Gauge` implementations by avoiding unnecessary intermediate collections. * Remove pointless `Option.apply` in `ZookeeperClient` `SessionState` metric. * Fix unused import/variable and other compiler warnings. * Reduce visibility of some vals/defs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-12-22T05:17:08Z","2020-01-06T18:52:58Z"
"","8417","KAFKA-8955: Add an AbstractResponse#errorCounts(Stream) and tidy","* Add AbstractResponse#errorCounts(Stream) to avoid having to call   AbstractResponse#errorCounts(Collection) with a computed collection. * A microbenchmark showed that using errorCounts(Stream) was    around 7.5 times faster than errorCounts(Collection). Using forEach()    loops with updateErrorCounts() is slightly faster, but is usually more    code. * Use updateErrorMap() consistently. * Replace for statements with forEach() for consistency. * Use singleton errorMap() consistently.","closed","","tombentley","2020-04-03T16:36:00Z","2020-04-23T07:21:50Z"
"","7558","MINOR: Added missed curly brackets to the log.error","* Added missed curly brackets to the log.error to the following file: connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java *  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","patro1322","2019-10-19T08:04:38Z","2020-03-22T04:49:13Z"
"","8178","HOTFIX: fix failing test PlaintextAdminIntegrationTest.testElectPreferredLeaders","(broken by https://github.com/apache/kafka/pull/3909)  - Confirmed the test passes after this (minor) change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agam","2020-02-26T22:01:08Z","2020-02-27T05:11:21Z"
"","8035","KAFKA-7489: Remove 0.9 compatibility checks from ConnectDistributedTest","(#7023) exposed an incompatibility between Kafka <=0.9 and Connect >0.9, in which the broker does not recognize a request for ApiVersions. For trunk and 2.4, this test case was removed rather than the issue addressed. This effectively backports the other half of (#7023) which was left out of (#7791).  Signed-off-by: Greg Harris   It is necessary to backport this to all of the branches that #7791 was backported to, namely `2.2` and `2.1`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2020-02-03T05:04:21Z","2020-10-16T06:15:45Z"
"","8099","cherry pick changes from apache/kafka","<< Incorrect PR opened from CCS >>","closed","","rite2nikhil","2020-02-12T20:22:24Z","2020-08-24T17:13:38Z"
"","8200","KAFKA-5876: IQ should throw different exceptions for different errors(part 1)","> [KIP-216: IQ should throw different exceptions for different errors](https://cwiki.apache.org/confluence/display/KAFKA/KIP-216%3A+IQ+should+throw+different+exceptions+for+different+errors)  KAFKA-5876's PR break into multiple parts. This PR is part 1: the new exceptions introduced in KIP-216  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vitojeng","2020-03-02T13:59:10Z","2020-07-23T03:03:39Z"
"","7628","KAFKA-9080: Addresses MessageFormatChangeTest.testCompatibilty with version 0.9.0.1","#7167 added a check for non-incremental offsets in `assignOffsetsNonCompressed`, which is not applicable for message format V0 and V1. Therefore, I added a condition to disable the check if the record version precedes V2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-10-31T22:21:38Z","2019-11-04T22:10:05Z"
"","7785","MINOR: disable JmxTool in kafkatest console-consumer by default","### what Provide the ability to optionally enable the JmxTool inside the kafkatest console-consumer  ### why For tests that don't use these JMX metrics, we should allow users to disable this metrics collection.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-12-05T17:40:16Z","2020-01-10T00:53:37Z"
"","7775","[WIP] MINOR: add configurable timeout/backoff to JmxTool","### what add two new configurable args to `JmxTool`: 1. `wait-timeout`  - In milliseconds, how long to wait for requested JMX objects. 2. `wait-backoff` - In milliseconds, the wait interval in between queries for requested JMX objects.  ### why Sometimes it may take some time for the JMX metrics of an application to appear. It would be nice to be able to configure this timeout. On top of that, control over the backoff would be nice too.  ### TODOs - [ ] test the case where we don't specify these options - [ ] confirm that the backoff works as expected  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","brianbushree","2019-12-03T23:25:25Z","2019-12-19T18:29:42Z"
"","7696","MINOR: add the missing parentheses in docs/streams/tutorial.html","### Committer Checklist (excluded from commit message) - [x] Verify documentation (including upgrade notes)","closed","","wangxinalex","2019-11-16T03:06:11Z","2019-11-18T16:41:41Z"
"","8475","KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2020-04-13T13:38:41Z","2020-04-17T21:03:44Z"
"","8465","MINOR: Fix grammar error message for InvalidRecordException","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2020-04-11T03:24:56Z","2020-04-21T06:52:58Z"
"","8458","KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2020-04-09T22:34:15Z","2020-04-13T13:37:19Z"
"","8386","MINOR: Improve close tests of caching state store","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2020-03-30T12:21:38Z","2020-06-08T10:03:21Z"
"","8314","MINOR: Fix javadoc warning in StreamsMetric","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-03-19T12:35:43Z","2020-03-19T17:12:25Z"
"","8300","MINOR: Fix typo in CreateTopicsResponse.json","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-03-14T10:59:51Z","2020-03-15T10:24:41Z"
"","8294","KAFKA-9718; Don't log passwords for AlterConfigs in request logs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-13T11:34:23Z","2020-05-05T21:13:55Z"
"","8262","KAFKA-6145: Add constrained balanced assignment algorithm","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-03-10T09:15:10Z","2020-03-20T18:51:25Z"
"","8238","KAFKA-9130: KIP-518 Allow listing consumer groups per state","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-03-06T16:31:31Z","2020-05-29T18:48:29Z"
"","8229","KAFKA-9661: Propagate includeSynonyms option to AdminClient in ConfigCommand","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-05T12:37:58Z","2020-03-05T16:58:59Z"
"","8227","KAFKA-9662: Wait for consumer offset reset in throttle test to avoid losing early messages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-03-05T11:53:26Z","2020-03-06T19:50:23Z"
"","8175","MINOR: Remove tag from metric to measure process-rate on source nodes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-02-26T21:07:48Z","2020-02-27T17:04:25Z"
"","8172","MINOR: Refactor process rate and latency metrics on thread-level","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2020-02-26T18:15:06Z","2020-03-02T16:22:02Z"
"","8129","Do not override NULL values with default values","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rgibaiev","2020-02-17T14:26:56Z","2020-02-17T14:33:29Z"
"","8128","MINOR: Add upgrade note about TLSv1 and TLSv1.1 being disabled in 2.5.0","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2020-02-17T10:03:27Z","2020-02-17T11:37:16Z"
"","8127","KAFKA-8835: KIP-352 docs update","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2020-02-17T09:58:43Z","2020-02-26T17:25:23Z"
"","8116","KAFKA-9562: part 1: ignore exceptions while flushing stores in close(dirty)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2020-02-13T23:18:37Z","2020-02-20T21:43:26Z"
"","8075","MINOR: Update schema field names in DescribeAcls Request/Response","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-02-08T20:06:05Z","2020-02-10T19:12:06Z"
"","8072","MINOR: Remove unwanted regexReplace on tests/kafkatest/__init__.py","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2020-02-08T10:13:17Z","2020-02-21T14:48:06Z"
"","8029","KAFKA-8147: Add changelog topic configuration to KTable suppress","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","highluck","2020-02-01T05:54:32Z","2020-06-23T04:26:04Z"
"","8019","KAFKA-8164: Improve test passing rate by rerunning flaky tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2020-01-29T13:53:33Z","2020-07-07T14:59:08Z"
"","8018","KAFKA-9480: Fix bug that prevented to measure task-level process-rate","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2020-01-29T04:31:43Z","2020-02-11T19:35:08Z"
"","8012","KAFKA-9474: Adds 'float64' to the RPC protocol types.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-01-28T19:33:39Z","2020-01-30T12:54:29Z"
"","7967","KAFKA-9449: Adds support for closing the producer's BufferPool.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-01-15T18:59:26Z","2020-01-19T20:33:02Z"
"","7960","KAFKA-9428: Add KeyQueryMetadata APIs to KafkaStreams","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vvcephei","2020-01-14T18:29:32Z","2020-06-12T23:23:38Z"
"","7953","MINOR: set scala version automatically based on gradle.properties","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","akatona84","2020-01-14T08:50:55Z","2020-02-22T16:45:20Z"
"","7951","MINOR: Fix connect:mirror checkstyle","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2020-01-13T21:24:34Z","2020-01-14T09:57:45Z"
"","7925","MINOR: Add upgrade guide for KAFKA-8421","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2020-01-10T00:14:11Z","2020-01-13T20:40:13Z"
"","7921","KAFKA-9395: Only acquire write lock in maybeShrinkIsr() if necessary.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2020-01-09T20:00:05Z","2020-01-14T18:46:42Z"
"","7854","KAFKA-9316: ConsoleProducer help info does not expose default supported properties","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-12-19T07:58:08Z","2019-12-20T00:53:55Z"
"","7841","KAFKA-9305: Add version 2.4 to Streams system tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2019-12-17T08:08:28Z","2019-12-20T22:21:13Z"
"","7838","MINOR: Add compatibility tests for 2.4.0","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-12-16T23:54:24Z","2019-12-17T14:14:33Z"
"","7818","MINOR: Update ZooKeeper upgrade notes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-12-11T14:43:58Z","2019-12-19T19:18:07Z"
"","7813","MINOR: Only update a request's local complete time in API handler if unset.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-12-10T21:40:44Z","2020-01-29T05:09:50Z"
"","7812","HOTFIX: Add comment to remind ordering restrictions on RegexSourceIntegrationTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-12-10T19:52:54Z","2020-04-24T23:44:27Z"
"","7810","MINOR: Improve javadoc of user-customizable metrics API","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-12-10T12:12:07Z","2019-12-16T20:55:58Z"
"","7807","MINOR: Remove line with addAll since collection passed into constructor call","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-12-09T20:43:42Z","2019-12-10T16:02:03Z"
"","7763","KAFKA-9251; Describing a non consumer group with the Admin API hangs forever","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-11-29T15:35:23Z","2020-10-06T20:10:57Z"
"","7694","MINOR: add dependency analysis debugging tasks","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","arvindth","2019-11-15T18:42:00Z","2019-11-20T18:22:11Z"
"","7678","KAFKA-9171: Handle ReplicaNotAvailableException during DelayedFetch","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-11-11T15:01:31Z","2019-11-11T20:03:07Z"
"","7667","MINOR: Adds entity-specific flags to ConfigCommand per KIP-543.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-11-08T18:10:18Z","2019-12-04T18:22:33Z"
"","7666","MINOR: Change topic-exists log for CreateTopics from INFO to DEBUG","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-11-08T14:44:25Z","2019-11-11T09:16:10Z"
"","7658","KAFKA-9150; DescribeGroup uses member assignment as metadata","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-11-06T17:19:07Z","2020-06-17T13:26:11Z"
"","7632","MINOR: Fix sensor retrieval in stand-by task's constructor","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-11-01T08:28:12Z","2019-11-01T18:55:30Z"
"","7623","MINOR: Replace some Java 7 style code with Java 8 style","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-10-31T14:55:45Z","2019-11-01T18:53:22Z"
"","7603","KAFKA-8977: Remove MockStreamsMetrics Since it is not a Mock","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","tests,","bibinss","2019-10-28T10:52:07Z","2019-11-17T17:41:13Z"
"","7583","MINOR: Fix command in kafka-reassign-partitions.sh docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-23T09:15:05Z","2019-10-26T20:16:27Z"
"","7564","KAFKA-8943: Move SecurityProviderCreator to org.apache.kafka.common.security.auth package","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-10-20T10:47:41Z","2019-10-21T07:48:34Z"
"","7560","KAFKA-9026: Use automatic RPC generation in DescribeAcls","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-19T17:08:40Z","2020-01-29T20:46:52Z"
"","7530","MINOR: Add upgrade docs for 2.4.0 release","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-10-16T17:16:38Z","2019-10-22T15:21:21Z"
"","7527","MINOR: Add metrics in operations doc for KIP-429 / KIP-467","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-16T02:34:08Z","2020-04-24T23:50:09Z"
"","7510","MINOR: Clarify wording around fault-tolerant state stores","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cstrombe","2019-10-14T19:49:30Z","2019-10-14T21:44:12Z"
"","7501","KAFKA-9030: Document client-level (a.k.a. instance-level) metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cadonna","2019-10-11T19:47:20Z","2019-10-15T21:33:01Z"
"","7492","Kafka 7499 production exception handler serialization exceptions","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2019-10-10T22:03:22Z","2020-08-18T17:18:15Z"
"","7490","KAFKA-8942: Document RocksDB metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cadonna","2019-10-10T19:22:24Z","2019-10-15T19:17:30Z"
"","7511","KAKFA-8950: Fix KafkaConsumer Fetcher breaking on concurrent disconnect","### Background  The KafkaConsumer Fetcher can sometimes get into an invalid state where it believes that there are ongoing fetch requests, but in fact there are none.  This leads to the Consumer continuing to long poll as if there are no messages available from the broker, and continuing to heartbeat and stay a part of the consumer group, but never receiving any new information from the Broker about the state of the partition or any new messages.  This may be caused by the heartbeat thread concurrently handling a disconnection event just after the fetcher thread submits a request which would cause the Fetcher to enter an invalid state where it believes it has ongoing requests to the disconnected node but in fact it does not. This is due to a thread safety issue in the Fetcher where it was possible for the ordering of the modifications to the `nodesWithPendingFetchRequests` to be incorrect - the Fetcher was adding it after the listener had already been invoked, which would mean that pending node never gets removed again.  ### Changes  This PR addresses that thread safety issue by ensuring that the pending node is added to the `nodesWithPendingFetchRequests` before the listener is added to the future, ensuring the finally block is called after the node is added.   ### Tests  I added a unit test which simulates the concurrent disconnect. The test validates that after the disconnections, the Fetcher is still able to send a request. In the case where we hit the threading issue, `sendFetches` will return 0 because it still thinks there are pending requests even though there are not. I modified the `MockClient` to add a new `wakeupHook` that gets invoked (if present) every time `client.wakeup` is called. This allows us to simulate another thread's actions during the `ConsumerNetworkClient#send`. Happy to find other ways of simulating the threading problems.  Thanks to @thomaslee for collaborating on the solution for this.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wiggzz","2019-10-14T20:17:34Z","2019-10-21T21:52:17Z"
"","8192","MINOR: add wait_for_assigned_partitions to console-consumer","## what/why the `throttling_test` was broken by this PR (https://github.com/apache/kafka/pull/7785) since it depends on the consumer having partitions-assigned before starting the producer  this PR provides the ability to wait for partitions to be assigned in the console consumer before considering it started.   ### caveat this does not support starting up the JmxTool inside the console-consumer for custom metrics while using this `wait_until_partitions_assigned` flag since the code assumes one JmxTool running per node.  I think a proper fix for this would be to make `JmxTool` its own standalone single-node service  ## alternatives we could use the `EndToEnd` test suite which uses the verifiable producer/consumer under the hood but I found that there were more changes necessary to get this working unfortunately (specifically doesn't seem like this test suite plays nicely with the `ProducerPerformanceService`)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2020-02-28T21:03:05Z","2021-08-27T23:39:27Z"
"","7612","KAFKA-8729: Change `PartitionResponse` to include all troubling records","## Background: Currently, whenever a batch is dropped because of`InvalidRecordException` or `InvalidTimestampException`, only the culprit record appears in `ProduceResponse.PartitionResponse.recordErrors`. However, after users try to resend that batch excluding the rejected message, the latter records are not guaranteed to be free of problems.  ## Changes: To address this issue, I changed the function signature of `validateKey`, `validateRecord` and `validateTimestamp` to return a Scala's `Option` object. Specifically, this object will hold the reason/message the current record in iteration fails and leaves to the callers (`convertAndAssignOffsetsNonCompressed`, `assignOffsetsNonCompressed`, `validateMessagesAndAssignOffsetsCompressed`) to gathered all troubling records into one place. Then, all these records will be returned along with the `PartitionResponse` object. As a result, if a batch contains more than one record errors, users see exactly which records cause the failure. `PartitionResponse.recordErrors` is a list of `RecordError` objects introduced by #7167 which include `batchIndex` denoting the relative position of a record in a batch and `message` indicating the reason of failure.  ## Gotchas: Things are particularly tricky when a batch has records rejected because of both `InvalidRecordException` and `InvalidTimestampException`. In this case, the `InvalidTimestampException` takes higher precedence. Therefore, the `Error` field in `PartitionResponse` will be encoded with `INVALID_TIMESTAMP`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-10-29T22:45:40Z","2019-11-08T22:22:35Z"
"","8121","KAFKA-6145: Pt 1. Bump protocol version and encode task lag map","""First"" PR for KIP-441: implement the protocol change so we can encode the task lag info in the subscription","closed","kip,","ableegoldman","2020-02-15T01:58:49Z","2020-06-26T22:37:34Z"
"","8384","MINOR: System test ZooKeeper upgrades","","closed","","rondagostino","2020-03-30T00:10:39Z","2020-04-02T17:53:50Z"
"","8315","KAFKA-9433: Use automated protocol for AlterConfigs request and response","","closed","","tombentley","2020-03-19T14:33:35Z","2020-04-09T13:59:25Z"
"","8211","COMDEV-340 Fix project category","","closed","","pzygielo","2020-03-03T21:03:06Z","2021-03-05T10:31:06Z"
"","8206","KAFKA-9625: Broker throttles are incorrectly marked as sensitive configurations","","closed","","cmccabe","2020-03-02T19:01:27Z","2020-03-19T20:34:30Z"
"","8102","MINOR: Fix poor backport that prevented compiling the MirrorMaker integration test","","closed","","rhauch","2020-02-13T00:33:03Z","2020-02-13T00:50:02Z"
"","8080","MINOR: improve error reporting in DescribeConsumerGroupTest","","closed","","cmccabe","2020-02-10T19:35:48Z","2020-02-17T19:45:39Z"
"","8044","MINOR: simplify KafkaProducerTest.java","","closed","","kun-song","2020-02-05T15:43:10Z","2020-02-08T14:14:22Z"
"","8041","MINOR: Better naming for variables denoting a forced unclean leader election","","open","","dhruvilshah3","2020-02-05T05:07:03Z","2020-02-12T10:09:37Z"
"","8038","MINOR: fix checkstyle issue in ConsumerConfig.java","","closed","","cmccabe","2020-02-04T20:33:44Z","2020-02-04T20:38:05Z"
"","7981","Backport pytest per-broker overrides to 2.0","","closed","","mumrah","2020-01-17T20:13:36Z","2020-03-26T05:04:13Z"
"","7976","MINOR: Fix typo in integration test class name","","closed","connect,","C0urante","2020-01-16T18:26:59Z","2020-10-16T06:15:44Z"
"","7863","[DO NOT MERGE] test","","closed","","dhruvilshah3","2019-12-23T20:11:57Z","2019-12-23T23:19:49Z"
"","7850","MINOR: Remove unused Metrics reference from KafkaConsumerMetrics","","closed","","cmccabe","2019-12-18T19:52:09Z","2019-12-19T18:53:18Z"
"","7848","KAFKA-9243 Update the javadocs to include TimestampKeyValueStore","","closed","streams,","miroswan","2019-12-18T08:25:11Z","2020-04-19T18:49:51Z"
"","7844","KAFKA-9309: Add the ability to translate Message classes to and from JSON","","closed","","cmccabe","2019-12-17T19:31:27Z","2020-04-09T20:11:37Z"
"","7839","KAFKA-9306: The consumer must close KafkaConsumerMetrics","","closed","","cmccabe","2019-12-17T01:16:41Z","2019-12-19T18:41:21Z"
"","7815","Kafka 8855 2.1","","closed","","cmccabe","2019-12-11T01:49:01Z","2019-12-11T23:55:45Z"
"","7750","MINOR: Add test to ensure log metrics are removed after deletion","","closed","","dhruvilshah3","2019-11-26T16:35:56Z","2019-12-04T02:04:54Z"
"","7746","KAFKA-9229: Fix incompatible change in InitProducerIdRequest","","closed","","cmccabe","2019-11-25T18:06:40Z","2019-11-26T00:16:05Z"
"","7741","KAFKA-7362: Delete stray partitions in log directory","","open","","dhruvilshah3","2019-11-23T02:24:27Z","2019-11-25T23:56:05Z"
"","7740","Add a note about signoffs","","closed","","mumrah","2019-11-23T00:38:38Z","2019-11-23T00:39:08Z"
"","7669","KAFKA-9165: Fix jersey warnings in Trogdor","","closed","","cmccabe","2019-11-08T20:48:00Z","2019-11-15T18:41:13Z"
"","7646","KAFKA-7504: prepare FetchResponses in the request handler threads","","open","","cmccabe","2019-11-05T01:23:13Z","2019-11-05T17:28:46Z"
"","7634","MINOR: do not treat member metadata as ordered","","closed","","cmccabe","2019-11-01T23:05:08Z","2019-11-06T18:38:22Z"
"","7595","KIP-541: Create a fetch.max.bytes configuration for the broker","","closed","","cmccabe","2019-10-25T00:26:49Z","2019-11-07T22:18:33Z"
"","7588","KAFKA-9091: Add a metric tracking the number of open connections with a given SSL cipher type","","closed","","cmccabe","2019-10-23T23:17:09Z","2019-12-06T19:20:17Z"
"","7575","MINOR: Explicitly mention that max.request.size validates uncompressed record sizes and max.message.bytes validates compressed record sizes","","closed","","stanislavkozlovski","2019-10-22T12:20:18Z","2019-11-14T17:04:54Z"
"","7531","KAFKA-9004 Prevent older clients from fetching from a follower","","closed","","mumrah","2019-10-16T17:21:20Z","2019-10-17T16:44:15Z"