"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","11572","KAFKA-13510: Connect APIs to list all connector plugins and retrieve …","…their configdefs  Implements [KIP-769](https://cwiki.apache.org/confluence/display/KAFKA/KIP-769%3A+Connect+APIs+to+list+all+connector+plugins+and+retrieve+their+configuration+definitions)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mimaison","2021-12-07T10:42:27Z","2022-03-03T15:39:54Z"
"","12022","MINOR:Remove repeated variable reset in KafkaController.scala","…s onControllerResignation The following variables  in kafkaController.scala are used for metric statistics： ```    offlinePartitionCount      preferredReplicaImbalanceCount     globalTopicCount      globalPartitionCount     topicsToDeleteCount      replicasToDeleteCount      ineligibleTopicsToDeleteCount      ineligibleReplicasToDeleteCount  ```  when execute the  KafkaController.process().updateMetrics() method, If the controller is not active, will reset the following values    ``` ..... globalTopicCount = if (!isActive) 0 else controllerContext.allTopics.size globalPartitionCount = if (!isActive) 0 else controllerContext.partitionWithLeadersCount .... ```  So we don't need to reset again when executing kafkaController.onControllerResignation()  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-04-09T04:48:50Z","2022-05-13T09:55:10Z"
"","12104","KAFKA-13746: Attempt to fix flaky test by waiting to fetch 2 topics f…","…rom describe topics command  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2022-04-28T02:45:39Z","2022-05-14T00:09:48Z"
"","12299","MINOR: Guard against decrementing `totalCommittedSinceLastSummary` du…","…ring rebalancing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jnh5y","2022-06-15T21:43:32Z","2022-06-16T16:40:26Z"
"","11816","MINOR: Adding kafka-storage.bat file (similar to kafka-storage.sh) fo…","…r windows.  `kafka-storage.sh` was added as part of pull request [#10043](https://github.com/apache/kafka/pull/10043). But in windows, the `.bat` file was not included and hence caused issue while starting server using kraft  Cc @junrao","closed","","GauthamM-official","2022-02-26T13:00:07Z","2022-03-14T16:20:29Z"
"","12421","Revert ""KAFKA-12887 Skip some RuntimeExceptions from exception handle…","…r (#11228)""  This reverts commit 4835c64f  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-07-18T19:08:07Z","2022-07-19T17:34:19Z"
"","12392","KAFKA-14053: Transactional producer should bump the epoch and skip ab…","…orting when a delivery timeout is encountered  When a transactional batch encounters delivery or request timeout, it can still be in-flight. In this situation, if the transaction is aborted, the abort marker might get appended to the log earlier than the in-flight batch. This can cause the LSO of a partition to be blocked infinitely, or can violate the processing guarantees. To avoid this situation, on a client side timeout, the transactional producer should skip aborting (EndTxnRequest), and bump the epoch instead. Since this is a fencing bump, the producer cannot safely continue, resulting in a fatal error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","urbandan","2022-07-08T10:53:18Z","2022-08-01T06:47:10Z"
"","11664","KAFKA-13582: TestVerifiableProducer.test_multiple_kraft_security_prot…","…ocols fails  KRaft brokers always use the first controller listener, so if there is not also a colocated KRaft controller on the node be sure to only publish one controller listener in `controller.listener.names` even when the inter-controller listener name differs.  System tests were failing due to unnecessarily publishing a second entry in `controller.listener.names` for a broker-only config and not also publishing a mapping for it in `listener.security.protocol.map`.  Removing the unnecessary entry in `controller.listener.names` solves the problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2022-01-10T17:18:22Z","2022-01-10T19:55:35Z"
"","12360","KAFKA-14032: Dequeue time for forwarded requests is ignored to set","…issues in statistics  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yufeiyan1220","2022-06-29T10:47:53Z","2022-07-06T20:21:29Z"
"","11886","KAFKA-13730: OAuth access token validation fails if it does not conta…","…in the ""sub"" claim  Removes the requirement of presence of sub claim in JWT access tokens, when clients authenticate via OAuth. This does not interfere with OAuth specifications and is to ensure wider compatibility with OAuth providers. Unit test added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fonaid","2022-03-11T17:49:29Z","2022-07-27T16:21:04Z"
"","12170","KAFKA-13875 Adjusted the output the topic describe output to include TopicID & se…","…gment.bytes  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","","Moovlin","2022-05-16T20:02:59Z","2022-06-05T07:13:02Z"
"","12167","KAFKA-13716 Added the DeleteRecordsCommandTest to test the CLI front end of the D…","…eleteRecordsCommand.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* Added a new test for the front end of the DeleteRecordsCommand command line tool.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*   ### Committer Checklist (excluded from commit message) - [x] Verify test coverage and CI build status - [x] Verify design and implementation  - [x] Verify documentation (including upgrade notes)","open","","Moovlin","2022-05-16T16:08:17Z","2022-06-03T13:14:06Z"
"","11834","[KAFKA-13687] Allowing kafka-dump-log.sh to limit the amount of batches while inspe…","…cting them   ### Summary This PR allows to limit the output batches while they are inspected via the `kafka-dump-log.sh`  script.  The idea is to take samples from the logsegments without affecting a production cluster  as the current script will read the whole files, this could create issues in term of page.  More information can be found in the ticket [here](https://issues.apache.org/jira/browse/KAFKA-13687)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sciclon2","2022-03-02T17:32:48Z","2022-03-03T06:46:13Z"
"","11517","KAFKA-13466:when kafka-console-producer.sh, delete unused config batch.size","…consistent with official docs# Please enter the commit message for your changes. Lines starting  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","peterwanner","2021-11-19T12:27:02Z","2022-03-07T06:32:03Z"
"","11748","KAFKA-12635: Don't emit checkpoints for partitions without any offset…","…-syncs   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-02-11T21:06:33Z","2022-05-16T15:44:19Z"
"","12248","KAFKA-13958: Expose logdirs total and usable space via Kafka API (KIP…","…-827)  This implements KIP-827: https://cwiki.apache.org/confluence/display/KAFKA/KIP-827%3A+Expose+logdirs+total+and+usable+space+via+Kafka+API  Add TotalBytes and UsableBytes to DescribeLogDirsResponse Add matching getters on LogDirDescription  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mimaison","2022-06-03T15:50:35Z","2022-06-14T12:20:33Z"
"","12364","MINOR: Improve Javadoc for OffsetCommitCallback-consuming commitAsync…","… methods  Having ""async"" in Kafka consumer API might encourage users to expect that there is some other internal thread that might deliver offset-commit confirmation, but actually it's usercode that needs to poll() to get the response. This PR attempts to add some disclaimer to cover this situation.  In example, the following blocks infinitely: ```         final Consumer kc = ....         kc.assign(Collections.singleton(new TopicPartition(""mytopic"", 0)));         kc.poll(2000);         final CountDownLatch latch = new CountDownLatch(1); // The latch should be released when we get a response.         kc.commitAsync(new OffsetCommitCallback() {             @Override             public void onComplete(final Map offsets, final Exception exception) {                 latch.countDown();             }         });         latch.await(); //","open","","adamkotwasinski","2022-06-29T20:54:51Z","2022-07-19T16:41:06Z"
"","12281","KAFKA-13971: Fix atomicity violations caused by improper usage of ConcurrentHashMap - part2","~~## Problem #1 in DelegatingClassLoader.java Atomicity violation in example such as: Consider thread T1 reaches line 228, but before executing context switches to thread T2 which also reaches line 228. Again context switches to T1 which reaches line 232 and adds a value to the map. T2 will execute line 228 and creates a new map which overwrites the value written by T1, hence change done by T1 would be lost. This code change ensures that two threads cannot initiate the TreeMap, instead only one of them will.~~  ## Problem #2 in RocksDBMetricsRecordingTrigger.java Atomicity violation in example such as: Consider thread T1 reaches line 40 but before executing it context switches to thread T2 which also reaches line 40. In a serialized execution order, thread T2 should have thrown the exception but it won't in this case. The code change fixes that.  Note that some other problems associated with use of concurrent hashmap has been fixed in https://github.com/apache/kafka/pull/12277","open","","divijvaidya","2022-06-10T10:10:08Z","2022-08-03T20:35:37Z"
"","11717","KAFKA-13619: zookeeper.sync.time.ms is no longer used","zookeeper.sync.time.ms is no longer used. But it is present in the documentation. In this fix, zookeeper.sync.time.ms has been removed from the implementation and documentation(docs/generated/kafka_config.html).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tyamashi-oss","2022-01-27T08:56:14Z","2022-02-02T13:59:13Z"
"","11819","MINOR: Optimize the generateFieldToString method of MessageDataGenerator","You can see their `toString` method in all xxx.java in `clients/src/generated/java` directory. For fields of type String, the `toString()` method is still called, which brings an unnecessary method call. example： AddOffsetsToTxnRequestData#toString() `@Override public String toString() {     return ""AddOffsetsToTxnRequestData(""         + ""transactionalId="" + ((transactionalId == null) ? ""null"" : ""'"" + transactionalId.toString() + ""'"")         + "", producerId="" + producerId         + "", producerEpoch="" + producerEpoch         + "", groupId="" + ((groupId == null) ? ""null"" : ""'"" + groupId.toString() + ""'"")         + "")""; } `  In fact, `transactionalId` and `groupId` are already String types.","open","","RivenSun2","2022-02-28T07:41:48Z","2022-05-30T03:16:37Z"
"","12252","MINOR: Fix docs in quickstart.html","Wrong link for connect-file-*.jar  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JK-Wang","2022-06-04T19:31:38Z","2022-07-03T08:50:18Z"
"","11683","KAFKA-6502: Update consumed offsets on corrupted records.","Without this patch, `RecordQueue` just skips records that failed to deserialize when `DeserializationExceptionHandler` is set to `LogAndContinueExceptionHandler`. This way, if the entire stream consists of corrupted records, the task never updated consumed offsets.  This patch introduces a new record type - `CorruptedRecord` that wraps a raw record that failed to deserialize. `RecordQueue`'s `headRecord` becomes `CorruptedRecord` if there were records in the `fifoQueue` and all of them failed to deserialize. In turn, `StreamTask#process` checks that a found record is not corrupted before processing it. If the record is an instance of `CorruptedRecord`, it updates the offset, sets `commitNeeded` to `true` and returns.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Gerrrr","2022-01-17T11:40:48Z","2022-01-20T17:26:51Z"
"","11909","KAFKA-13750: Client Compatability KafkaTest uses invalid idempotency configs","With the switch to idempotency as a default, ClientCompatibilityFeaturesTest.run_compatibility_test broke for versions prior to 0.11 where EOS was enabled. This PR disables idempotency for kafka versions prior to 0.11.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2022-03-16T23:53:00Z","2022-03-17T21:52:51Z"
"","12242","MINOR: Pin ducktape version to < 0.9","With newer ducktape versions than < 0.9 system tests may run into authentication issues with the AK system test infrastructure.  The version will be bumped up once we have infrastructure in place for newer paramiko versions brought in by ducktape 0.9.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-06-02T17:51:00Z","2022-06-02T18:21:23Z"
"","11970","MINOR: Move `KafkaYammerMetrics` to server-common","With major server components like the new quorum controller being moved outside of the `core` module, it is useful to have shared dependencies moved into `server-common`. An example of this is Yammer metrics which server components still rely heavily upon. All server components should have access to the default registry used by the broker so that new metrics can be registered and metric naming conventions should be standardized. This is particularly important in KRaft where we are attempting to recreate identically named metrics in the controller context.  This patch takes a step in this direction. It moves `KafkaYammerMetrics` into `server-common` and it implements standard metric naming utilities there. It does not yet replace the similarly named utilities in `KafkaMetricsGroup` because of its reliance on the implicit tag ordering of scala `Map` types. In a follow-up patch, we can replace these implicit metrics with explicitly ordered maps and ensure that tests cover them. Once this is done, we can replace the utilities in `KafkaMetricsGroup` with their analogues in `KafakYammerMetrics`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-03-30T01:24:26Z","2022-03-30T20:59:22Z"
"","12348","KAFKA-14016: Revoke more partitions than expected in Cooperative rebalance","With latest trunk branch's code we found that in Cooperative rebalance consumer will revoke more partitions than expected. Details here https://issues.apache.org/jira/browse/KAFKA-14016  So i want to start a PR to discuss the fix code. test will be added later.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","aiquestion","2022-06-26T08:50:31Z","2022-07-17T15:41:22Z"
"","12017","[MINOR] Fix DescribeLogDirs API error handling for older API versions","With KAFKA-13527 / KIP-784 we introduced a new top-level error code for the DescribeLogDirs API for versions 3 and above. However, the change regressed the error handling for versions less than 3 since the response converter fails to write the non-zero error code out (rightly) for versions lower than 3 and drops the response to the client which eventually times out instead of receiving an empty log dirs response and processing that as a Cluster Auth failure.  With this change, the API conditionally propagates the error code out to the client if the request API version is 3 and above. This keeps the semantics of the error handling the same for all versions and restores the behavior for older versions.  See current behavior in the broker log: ```bash ERROR] 2022-04-08 01:22:56,406 [data-plane-kafka-request-handler-10] kafka.server.KafkaApis - [KafkaApi-0] Unexpected error handling request RequestHeader(apiKey=DESCRIBE_LOG_DIRS, apiVersion=0, clientId=sarama, correlationId=1) -- DescribeLogDirsRequestData(topics=null) org.apache.kafka.common.errors.UnsupportedVersionException: Attempted to write a non-default errorCode at version 0 [ERROR] 2022-04-08 01:22:56,407 [data-plane-kafka-request-handler-10] kafka.server.KafkaRequestHandler - [Kafka Request Handler 10 on Broker 0], Exception when handling request org.apache.kafka.common.errors.UnsupportedVersionException: Attempted to write a non-default errorCode at version 0 ```  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status","closed","","aloknnikhil","2022-04-08T14:31:40Z","2022-04-08T23:48:49Z"
"","11933","KAFKA-13759: Disable idempotence by default in producers instantiated by Connect","With AK 3.0, idempotence was enabled by default in Kafka producers.   However, if idempotence is enabled, Connect won't be able to communicate via its producers with Kafka brokers older than version 0.11. Perhaps more importantly, for brokers older than version 2.8 the IDEMPOTENT_WRITE ACL is required to be granted to the principal of the Connect worker.   Therefore this commit disables producer idempotence by default to all the producers instantiated by Connect. Users can still choose to enable producer idempotence by explicitly setting the right worker and/or connector properties.   The changes were tested via existing unit, integration and system tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2022-03-23T00:03:44Z","2022-03-23T22:29:44Z"
"","11821","KAFKA-13672 Flaky test kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize() [WIP]","WIP PR to get better insight into flakiness in CI context.","closed","","LiamClarkeNZ","2022-03-01T03:06:22Z","2022-03-20T04:39:42Z"
"","11765","[WIP] State updater implementation","WIP  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-02-15T12:45:03Z","2022-07-29T08:16:13Z"
"","12476","K14130: Reduce RackAwarenesssTest to unit Test","While working on [KAFKA-13877](https://issues.apache.org/jira/browse/KAFKA-13877), I feel it's an overkill to introduce the whole test class as an integration test, since all we need is to just test the assignor itself which could be a unit test. Running this suite with 9+ instances takes long time and is still vulnerable to all kinds of timing based flakiness. A better choice is to reduce it as a unit test, similar to HighAvailabilityStreamsPartitionAssignorTest that just test the behavior of the assignor itself, rather than creating many instances hence depend on various timing bombs to not explode.  Since we mock everything, there's no flakiness anymore. Plus we greatly reduced the test runtime (on my local machine, the old integration takes about 35 secs to run the whole suite, while the new one take 20ms on average).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-08-03T02:02:25Z","2022-08-03T22:36:59Z"
"","11601","KAFKA-13711: Minor fixes for input topic management","While working on [#11600](https://github.com/apache/kafka/pull/11600) I noticed a few issues with how we manage topics in the TopologyMetadata, particularly surrounding the code to update and track input topics. This PR cleans that up and adds some further verification when processing possible updates from the subscription metadata or assignment","open","","ableegoldman","2021-12-14T06:37:54Z","2022-03-07T06:59:30Z"
"","12333","MINOR: fix `LogCleanerManagerTest.testLogsUnderCleanupIneligibleForCompaction()` for `Defaults.LogMessageTimestampType = ""LogAppendTime""`","While setting `Defaults.LogMessageTimestampType` to `""LogAppendTime""`, this error will throwed by `LogCleanerManagerTest.testLogsUnderCleanupIneligibleForCompaction()`. ``` java org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp kafka.common.RecordValidationException: org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp 	at kafka.log.LogValidator$.processRecordErrors(LogValidator.scala:578) 	at kafka.log.LogValidator$.$anonfun$assignOffsetsNonCompressed$1(LogValidator.scala:315) 	at java.lang.Iterable.forEach(Iterable.java:75) 	at kafka.log.LogValidator$.assignOffsetsNonCompressed(LogValidator.scala:292) 	at kafka.log.LogValidator$.validateMessagesAndAssignOffsets(LogValidator.scala:110) 	at kafka.log.UnifiedLog.append(UnifiedLog.scala:824) 	at kafka.log.UnifiedLog.appendAsLeader(UnifiedLog.scala:740) 	at kafka.log.LogCleanerManagerTest.testLogsUnderCleanupIneligibleForCompaction(LogCleanerManagerTest.scala:350) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725) 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140) 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84) 	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115) 	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45) 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37) 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104) 	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135) 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.util.ArrayList.forEach(ArrayList.java:1257) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at java.util.ArrayList.forEach(ArrayList.java:1257) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) 	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) 	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:108) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) 	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:96) 	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:75) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79) 	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75) 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) 	at com.sun.proxy.$Proxy2.stop(Unknown Source) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100) 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60) 	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) 	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133) 	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71) 	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) 	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) Caused by: org.apache.kafka.common.errors.InvalidTimestampException: One or more records have been rejected due to invalid timestamp ``` This is because `LogValidator` modifies the timestamp type in the received records, so I fixed this by regenerating the records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","fxbing","2022-06-22T13:43:45Z","2022-07-13T10:21:20Z"
"","12151","MINOR: parameter name fix for maxScalacThreads","While running unitTest and integrationTest by tweaking the scala parallel threads parameter, hit this issue where the command `./gradlew -PmaxScalacThreads=4 unitTest` errored out, pointing to the bug of incorrect parameter name reference expected vs used.   This commit is rectifying the parameter reference to be used.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jparag","2022-05-12T01:36:50Z","2022-05-12T06:51:28Z"
"","12138","MINOR: Followers should not have any remote replica states left over from previous leadership","While investigating https://github.com/apache/kafka/commit/4218fc61fedb02b78d35c88e56ab253baaf09f39, I noted that a follower doesn't clean up its remote replica states when it becomes a follower. So when it transition from leader to follower, remote replica states are kept around. This patch ensures that they are cleaned up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-09T10:01:49Z","2022-05-18T07:32:51Z"
"","11847","KAFKA-12648: fix NPE due to race condtion between resetting offsets and removing a topology","While debugging the flaky `NamedTopologyIntegrationTest. shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing` test, I did discover one real bug. The problem was that we update the TopologyMetadata's `builders` map (with the known topologies) inside the #removeNamedTopology call directly, whereas the StreamThread may not yet have reached the `poll()` in the loop and in case of an offset reset, we get an NP.e I changed the NPE to just log a warning for now, going forward I think we should try to tackle some tech debt by keeping the processing tasks and the TopologyMetadata in sync","closed","","ableegoldman","2022-03-04T18:28:38Z","2022-03-08T08:04:44Z"
"","11843","MINOR: Correct logging and Javadoc in FetchSessionHandler","While debugging some client issues I found out that some Javadocs in FetchSessionHandler got misleading / outdated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","adamkotwasinski","2022-03-03T20:56:24Z","2022-03-09T17:31:10Z"
"","12177","KAKFA-13410 Generate snapshot prior to metadata.version upgrade","When we update the `metadata.version` in KRaft, we want to take a snapshot of the metadata just prior to the record batch that includes the upgrade. By creating a snapshot, we have a convenient way of manually restoring the metadata to a previous state that does not include an upgrade. This is not required for correctness, but more so as a hedge against bugs encountered by an upgrade.","open","","mumrah","2022-05-18T19:27:05Z","2022-06-14T12:51:05Z"
"","11831","MINOR: Do not throw exception when InterBrokerSecurityProtocolProp value is empty string","When we set broker config as follows ``` security.inter.broker.protocol= inter.broker.listener.name=INNER ``` and restart broker, the exception `org.apache.kafka.common.config.ConfigException: Only one of inter.broker.listener.name and security.inter.broker.protocol should be set.` will be throw and server start failed.  This pr will  (1) add a nonEmpty check for value of InterBrokerSecurityProtocolProp to make through empty value case (2) detailed info When exception throw  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","maixiaohai","2022-03-02T12:39:08Z","2022-03-06T15:41:35Z"
"","12453","MINOR: Remove code of removed metric","When we removed metric skipped-records in 3.0 we missed to remove some code related to that metric.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-28T17:28:07Z","2022-07-29T14:53:02Z"
"","11791","KAFKA-13676: Commit successfully processed tasks on error","When we hit an exception when processing tasks we should save the work we have done so far. This will only be relevant with ALOS and EOS-v1, not EOS-v2. It will actually reduce the number of duplicated record in ALOS because we will not be successfully processing tasks successfully more than once in many cases.  This is currently enabled only for named topologies.  The behavior was rather throughly tested. There is a `EmitOnChangeIntegrationTest` that makes sure if there is an exception in a task, after replacing the thread and if the exception does not throw again the record would still be processed. I just added another task into the test such that even if the other task keep throwing exceptions and hence never complete processing its record, this new task that could process successfully would be able to make progress and commit its offset.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-02-18T17:11:22Z","2022-02-23T07:10:05Z"
"","11916","KAFKA-12703; Allow unencrypted private keys when using PEM files","When using PEM files, keyPassword has to be specified, while when setting private key and certificate chain as strings keyPassword is allowed to be null. This PR fixes this. Issue is described in more details here: https://issues.apache.org/jira/browse/KAFKA-12703  Updated the appropriate tests to reflect the code change.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","chromy96","2022-03-18T07:25:07Z","2022-05-16T07:25:05Z"
"","11604","MINOR: retry when deleting offsets for named topologies","When this was made I didn't expect deleteOffsetsResult to be set if an exception was thrown. But it is and to retry we need to reset it to null. Changing the KafkaStreamsNamedTopokogyWrapper for remove topology when resetting offsets  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-12-15T21:06:08Z","2021-12-17T15:28:31Z"
"","12159","KAFKA-13933: Fix stuck SSL tests in case of authentication failure","When there is an authentication error after the initial TCP connection, the selector never becomes READY, and these tests wait forever waiting for this state.  This is actually what happened to me while using an OpenJDK build that does not support the required cipher suites.","closed","","fvaleri","2022-05-13T15:07:20Z","2022-06-06T16:24:41Z"
"","11574","MINOR: Fix internal topic manager tests","When the unit tests of the internal topic manager test are executed on a slow machine (like sometimes in automatic builds) they sometimes fail with a timeout exception instead of the expected exception. To fix this behavior, this commit replaces the use of system time with mock time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-12-07T13:19:06Z","2021-12-07T17:23:53Z"
"","12187","KAFKA-13858; Kraft should not shutdown metadata listener until controller shutdown is finished","When the kraft broker begins controlled shutdown, it immediately disables the metadata listener. This means that metadata changes as part of the controlled shutdown do not get sent to the respective components. For partitions that the broker is follower of, that is what we want. It prevents the follower from being able to rejoin the ISR while still shutting down. But for partitions that the broker is leading, it means the leader will remain active until controlled shutdown finishes and the socket server is stopped. That delay can be as much as 5 seconds and probably even worse.  This PR revises the controlled shutdown procedure as follow: * The broker signals to the replica manager that it is about to start the controlled shutdown. * The broker requests a controlled shutdown to the controller. * The controller moves leaders off from the broker, removes the broker from any ISR that it is a member of, and writes those changes to the metadata log. * When the broker receives a partition metadata change, it looks if it is in the ISR. If it is, it updates the partition as usual. If it is not, it stops the fetcher/replica. This basically stops all the partitions for which the broker was part of their ISR.  When the broker is a replica of a partition but it is not in the ISR, the controller does not do anything. The leader epoch is not bumped. In this particular case, the follower will continue to run until the replica manager shuts down. In this time, the replica could become in-sync and the leader could try to bring it back to the ISR. We rely on https://github.com/apache/kafka/pull/12181 to ensure that does not happen.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-20T08:32:29Z","2022-05-30T08:13:08Z"
"","11830","KAFKA-13694: When the Broker side processes the ProduceRequest, it prints more specific information when the verification record fails.","When the Broker side processes the ProduceRequest, it prints more specific information when the verification record fails.  Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13694","closed","","RivenSun2","2022-03-02T08:14:30Z","2022-03-06T03:08:11Z"
"","11616","MINOR: update the description in Kraft example properties file","When testing Kraft mode, I found the description for `controller.listener.names` in the example `server.properties` is wrong and misleading. Fix the error, and update some other descriptions to make them clearer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-19T07:00:52Z","2022-03-08T13:12:25Z"
"","12337","KAFKA-10199: Remove main consumer from store changelog reader","When store changelog reader is called by a different thread than the stream thread, it can no longer use the main consumer to get committed offsets since consumer is not thread-safe. Instead, we would remove main consumer and leverage on the existing admin client to get committed offsets.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-23T22:26:46Z","2022-07-07T04:58:59Z"
"","11597","KAFKA-13539: Improve propagation and processing of SSL handshake failures","When server fails SSL handshake and closes its connection, we attempt to report this to clients on a best-effort basis. When IOException is detected in the client, we may proceed to close the connection before processing all the data from the server if we have data pending to be sent to the server. Server attempts to send any data that has been already wrapped, but may not wrap again after handshake failure, so error may not be propagated to clients. However, our tests assume that clients always detect  handshake failures. This PR attempts to wrap and send all data on the server-side after handshake failure and attempts to process all data on the client-side.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-12-13T11:50:38Z","2021-12-14T10:01:59Z"
"","12382","MINOR: kafka system tests should support larger EBS volumes for newer instances","When running with 4th generation instances supporting EBS only, we need to use a larger volume or else we run out of  disk space during a system test run.  This change also parameterizes the instance type as an env variable for easier testing.","closed","","lbradstreet","2022-07-05T20:23:53Z","2022-07-07T07:14:05Z"
"","12202","MINOR: fix connect system test runs with JDK 10+","When running system tests with JDK 10+, we hit the following error because util.py attempts to check the version variable for non-Kafka service objects.  ``` [INFO:2022-05-23 18:43:04,741]: RunnerClient: kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=SASL_SSL.metadata_quorum=REMOTE_KRAFT: on run 1/1 [INFO:2022-05-23 18:43:04,775]: RunnerClient: kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=SASL_SSL.metadata_quorum=REMOTE_KRAFT: Setting up... [INFO:2022-05-23 18:43:04,792]: RunnerClient: kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=SASL_SSL.metadata_quorum=REMOTE_KRAFT: Running... [INFO:2022-05-23 18:43:53,154]: RunnerClient: kafkatest.tests.connect.connect_test.ConnectStandaloneFileTest.test_file_source_and_sink.security_protocol=SASL_SSL.metadata_quorum=REMOTE_KRAFT: FAIL: AttributeError(""'ClusterNode' object has no attribute 'version'"") Traceback (most recent call last):   File ""kafka/venv/lib/python3.7/site-packages/ducktape/tests/runner_client.py"", line 175, in _do_run     data = self.run_test()   File ""/kafka/venv/lib/python3.7/site-packages/ducktape/tests/runner_client.py"", line 257, in run_test     return self.test_context.function(self.test)   File ""kafka/venv/lib/python3.7/site-packages/ducktape/mark/_mark.py"", line 433, in wrapper     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)   File ""kafka/tests/kafkatest/tests/connect/connect_test.py"", line 109, in test_file_source_and_sink     self.source.start()   File ""kafka/tests/kafkatest/services/connect.py"", line 123, in start     super(ConnectServiceBase, self).start()   File ""kafka/venv/lib/python3.7/site-packages/ducktape/services/service.py"", line 265, in start     self.start_node(node, **kwargs)   File ""kafka/tests/kafkatest/services/connect.py"", line 332, in start_node     self.start_and_wait_to_start_listening(node, 'standalone', remote_connector_configs)   File ""kafka/tests/kafkatest/services/connect.py"", line 138, in start_and_wait_to_start_listening     self.start_and_return_immediately(node, worker_type, remote_connector_configs)   File ""kafka/tests/kafkatest/services/connect.py"", line 126, in start_and_return_immediately     cmd = self.start_cmd(node, remote_connector_configs)   File ""kafka/tests/kafkatest/services/connect.py"", line 300, in start_cmd     cmd += fix_opts_for_new_jvm(node)   File ""/kafka/tests/kafkatest/services/kafka/util.py"", line 36, in fix_opts_for_new_jvm     if node.version == LATEST_0_8_2 or node.version == LATEST_0_9 or node.version == LATEST_0_10_0 or node.version == LATEST_0_10_1 or node.version == LATEST_0_10_2 or node.version == LATEST_0_11_0 or node.version == LATEST_1_0: AttributeError: 'ClusterNode' object has no attribute 'version' ```","closed","connect,","lbradstreet","2022-05-24T14:50:56Z","2022-05-25T17:41:34Z"
"","12399","KAFKA-14063: Prevent malicious tiny payloads from causing OOMs with variably sized collections","When parsing code receives a payload for a variable length field where the length is specified in the code as some arbitrarily large number (assume INT32_MAX for example) this will immediately try to allocate an ArrayList to hold this many elements, before checking whether this is a reasonable array size given the available data.   The fix for this is to instead throw a runtime exception if the length of a variably sized container exceeds the amount of remaining data. Then, the worst a user can do is force the server to allocate 8x the size of the actual delivered data (if they claim there are N elements for a container of Objects (i.e. not a byte string) and each Object bottoms out in an 8 byte pointer in the ArrayList's backing array).  This was identified by fuzzing the kafka request parsing code.","open","","dpcollins-google","2022-07-10T00:04:54Z","2022-07-19T16:42:09Z"
"","12021","MINOR: Re-use counter in mocking of LogSegment.size","When migrating from Easymock to Mockito, the mockito implemetnation didn't have the same semantic as the Easymock implementation.  Without this fix the mocking of LogSegment.size() always returns 0 because a new AtomicInteger was getting created for each invocation of LogSegment.size().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-04-09T00:02:56Z","2022-04-11T20:37:56Z"
"","12073","MINOR; Fix partition change record noop check","When LeaderRecoveryState was added to the PartitionChangeRecord, the check for being a noop was not updated. This commit fixes that and improves the associated test avoid this oversight in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2022-04-20T19:19:25Z","2022-04-21T16:05:50Z"
"","11869","KAFKA-13719: fix connector restart cause duplicate tasks","When kafka Connect restarts connector with includeTasks=true,  DistributedHerder start all task without filter the currentAssignments. This results in duplicate tasks and duplicate data。","closed","connect,","sunshujie1990","2022-03-09T11:08:55Z","2022-03-30T17:23:41Z"
"","11527","KAFKA-13357: store producer IDs in broker snapshots","When creating snapshots, controllers generate a ProducerIdsRecord indicating the highest producer ID that has been used so far. Brokers should generate the same record, so that the snapshots can be compared.  Also, fix a bug in MetadataDelta#finishSnapshot. The current logic will produce the wrong result if all objects of a certain type are completely removed in the snapshot. The fix is to unconditionally create each delta object.","closed","kip-500,","cmccabe","2021-11-22T21:49:46Z","2021-11-24T19:29:14Z"
"","12407","[BUG] Remove duplicate common.message.* from clients:test jar file","When consuming both `kafka-client:3.0.1` and `kafka-client:3.0.1:test` through maven a hygene tool was detecting multiple instances of the same class loaded into the classpath.  Verified this change by building locally with a before and after build with `./gradlew clients:publishToMavenLocal`, then used beyond compare to verify the contents.  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation     - Minor change to existing build process, the java classes was duplicated and unused. - [X] Verify test coverage and CI build status    - There should be no changes in test coverage and CI build status. - [X] Verify documentation (including upgrade notes)    - No documentation updates need to be made","open","","peternied","2022-07-14T20:10:12Z","2022-07-25T16:06:33Z"
"","12308","KAFKA-14009: update rebalance timeout in memory when consumers use st…","When consumers use static membership protocol and do not happen rebalance, consumers can not update rebalance timeout when users want to reduce rebalance timeout    *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Stephan14","2022-06-19T08:39:20Z","2022-07-24T05:04:18Z"
"","12390","KAFKA-14055; Txn markers should not be removed by matching records in the offset map","When cleaning a topic with transactional data, if the keys used in the user data happen to conflict with the keys in the transaction markers, it is possible for the markers to get removed before the corresponding data from the transaction is removed. This results in a hanging transaction or the loss of the transaction's atomicity since it would effectively get bundled into the next transaction in the log. Currently control records are excluded when building the offset map, but not when doing the cleaning. This patch fixes the problem by checking for control batches in the `shouldRetainRecord` callback.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-07-07T21:27:18Z","2022-07-10T17:16:39Z"
"","11961","KAFKA-13768: Don't mark expired batches as unresolved when using transactional producer","When an idempotent producer's request failed and we retried it and put it into accumulator again, and these batches eventually timeout and expired by the accumulator, it will be marked as `SequenceUnresolved` in `sendProducerData`. And it will finally make producer's epoch bumped.  But in transactional cases, there's no need to handle it this way. In `sendProducerData`, `failBatch` has already been called after batches expired. So the `TimeoutException` will be thrown out and the transaction will be abort safely. I didn't think out any case it will cause data duplication.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ddrid","2022-03-29T08:32:15Z","2022-04-14T12:55:32Z"
"","11861","MINOR: Fix flaky test cases SocketServerTest.remoteCloseWithoutBufferedReceives and SocketServerTest.remoteCloseWithIncompleteBufferedReceive","When a socket is closed, corresponding channel should be retained only if there is complete buffered requests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vincent81jiang","2022-03-07T21:37:15Z","2022-03-08T19:18:45Z"
"","12414","KAFKA-14073 Logging the reason for Snapshot","When a snapshot is taken it is due to either of the following reasons -  1. Max bytes were applied 2. Metadata version was changed  Once the snapshot process is started, it will log the reason that initiated the process.   Updated existing tests to include code changes required to log the reason. I was not able to check the logs when running tests - could someone guide me on how to enable logs when running a specific test case.   Example logs after the changes -  ``` [2022-07-25 14:34:39,769] INFO [Controller 3000] Generating a snapshot that includes (epoch=1, offset=0) after 91 committed bytes since the last snapshot, because max bytes exceeded. (org.apache.kafka.controller.QuorumController:1328) ```  ``` [2022-07-25 14:40:51,783] INFO [BrokerMetadataSnapshotter id=2] Creating a new snapshot at offset 5 because metadata version changed... (kafka.server.metadata.BrokerMetadataSnapshotter:66) ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ashmeet13","2022-07-17T13:10:45Z","2022-08-03T19:02:10Z"
"","11942","KAFKA-13767: Fetch from consumers should return immediately when preferred read replica is defined by the leader","When a replica selector is configured, the partition leader computes a preferred read replica for any fetch from the consumers. When the preferred read replica is not the leader, the leader returns the preferred read replica with `FetchDataInfo(LogOffsetMetadata.UnknownOffsetMetadata, MemoryRecords.EMPTY)` to the `ReplicaManager`. This causes the fetch to go into in the fetch purgatory because the exit conditions are not met. In turns out that the delayed fetch is not completed until the timeout is reached because the delayed fetch ignores partition with an unknown offset (-1). If the fetch contains only one partition, the fetch is unnecessarily delayed by the timeout time (500ms by default) to only inform the consumer that it has to read from a follower.  This patch fixes the issue by completing the fetch request immediately when a preferred read replica is defined.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-03-24T06:37:01Z","2022-04-21T07:14:13Z"
"","12271","KAFKA-13972; Ensure that replicas are stopped after cancelled reassignment","When a reassignment is cancelled, the controller must send `StopReplica` to all Adding replicas to ensure that the partition state is removed. Currently, this does not necessarily result in a bump to the leader epoch, which means that the `StopReplica` may be ignored by the Adding replica due to [KIP-570](https://cwiki.apache.org/confluence/display/KAFKA/KIP-570%3A+Add+leader+epoch+in+StopReplicaRequest). When this happens, the partition becomes stray and must be manually cleaned up.  We fix the problem here by ensuring that the leader epoch is bumped when a replica transitions to `OfflineReplica` even if the replica is not a leader or in the current ISR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","hachikuji","2022-06-08T22:39:06Z","2022-06-20T22:47:02Z"
"","12150","KAFKA-13837; Return an error from Fetch if follower is not a valid replica","When a partition leader receives a `Fetch` request from a replica which is not in the current replica set, the behavior today is to return a successful fetch response, but with empty data. This causes the follower to retry until metadata converges without updating any state on the leader side. It is clearer in this case to return an error, so that the metadata inconsistency is visible in logging and so that the follower backs off before retrying.   The question then is what error to return? In this patch, we use `UNKNOWN_LEADER_EPOCH` when the `Fetch` request includes the current leader epoch. The way I see this is that the leader is validating the (replicaId, leaderEpoch) tuple. When the leader returns `UNKNOWN_LEADER_EPOCH`, it means that the leader does not expect the given leaderEpoch from that replica. If the request does not include a leader epoch, then we use `NOT_LEADER_OR_FOLLOWER`. We can squint our eyes a little and take a similar interpretation for this case: the leader is rejecting the request because it does not think it should be the leader for that replica. But mainly these errors ensure that the follower will retry the request.  As a part of this patch, I have refactored the way that the leader updates follower fetch state. Previously, the process is slightly convoluted. We send the fetch from `ReplicaManager` down to `Partition.readRecords`, then we iterate over the results and call `Partition.updateFollowerFetchState`. It is more straightforward to update state directly as a part of `readRecords`. All we need to do is pass through the `FetchParams`. This also prevents an unnecessary copy of the read results.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-12T00:26:14Z","2022-05-19T03:58:21Z"
"","11939","KAFKA-13761: KafkaLog4jAppender deadlocks when idempotence is enabled","When a log entry is appended to a Kafka topic using KafkaLog4jAppender, the producer.send operation may hit a deadlock if the producer network thread also tries to append a log at the same log level. This issue is triggered when idempotence is enabled for the KafkaLog4jAppender and the producer tries to acquire the TransactionManager lock.  This is a temporary workaround to avoid deadlocks by disabling idempotence explicitly in KafkaLog4jAppender.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yyu1993","2022-03-23T17:24:50Z","2022-03-24T13:36:57Z"
"","12174","KAFKA-13907: Fix hanging ServerShutdownTest.testCleanShutdownWithKRaftControllerUnavailable","When a controlled shutdown is requested the broker tries to communicate the state change to the controller via a heartbeat request. [1]  In this test, the controller is not available so the request will fail. The current timeout behavior in a heartbeat request is to just keep retrying — which generally makes sense, just not in the context of a controlled shutdown.  When a heartbeat request times out, if we are in the middle of a controlled shutdown, we shouldn't just retry forever but rather just give up on trying to contact the controller and proceed with the controlled shutdown.  [1] https://github.com/apache/kafka/blob/f2d6282668a31b9a554563338f9178e2bba2833f/core/src/main/scala/kafka/server/BrokerLifecycleManager.scala#L217  *Summary of testing strategy* The test no longer fails  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","soarez","2022-05-17T15:04:54Z","2022-05-24T06:53:12Z"
"","12131","KAFKA-13879: Reconnect exponential backoff is ineffective in some cases","When a client connects to a SSL listener using PLAINTEXT security protocol, after the TCP connection is setup, the client considers the channel setup is complete. In reality the channel setup is not complete yet. The client then resets reconnect exponential backoff and issues API version request. Since the broker expects SSL handshake, the API version request will cause the connection to disconnect. Reconnect will happen without exponential backoff since it has been reset.  The proposed fix is not to reset reconnect exponential backoff when sending API version request. In the good case where the channel setup is complete, reconnect exponential backoff will be reset when the node becomes ready, which is after getting the API version response. The fix doesn't cover the case where clients do not send API version request and go directly to ready state.","closed","","chernyih","2022-05-06T23:15:03Z","2022-05-11T15:45:11Z"
"","12080","KAFKA-13843: Fix incorrect transactionManager state when receiving success response on an expired batch","When a batch's delivery timeout has expired but later receives a success response. Sender will call `transactionManager.handleCompletedBatch` without checking if it was completed before. And some states tracked in `topicPartitionBookkeeper` will be updated incorrectly.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-04-21T08:51:35Z","2022-04-24T07:07:17Z"
"","12286","KAFKA-13984: Fix TopicBasedRemoteLogMetadataManager#initializeResources should exit immediately when partition size of __remote_log_metadata  is not same as configured","When  executing `TopicBasedRemoteLogMetadataManager.initializeResources()`, if the result of `TopicBasedRemoteLogMetadataManager.isPartitionsCountSameAsConfigured() `is false.  It means that the actual number of partitions in the internal topic __remote_log_metadata is inconsistent with the number of partitions configured in our configuration file At this time, it is not reasonable to continue the subsequent initialization process.We should raise an error to exit. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","bozhao12","2022-06-13T12:46:56Z","2022-06-28T11:37:57Z"
"","11705","KAFKA-9847: add config to set default store type (KIP-591)","What I did in this PR: 1. add a stream config: `default.dsl.store` 2. Add the config `default.dsl.store` into `TopologyConfigs` 3. Add `Materialized.as(StoreType)` and `Materialized.withStoreType(StoreType)` API 4. publish the `public StreamsBuilder(TopologyConfig)` constructor 5. Create the expected store based on the type in for each implementation 6. Add tests for different kinds of stores (most of the added codes are in tests)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","showuon","2022-01-24T09:24:59Z","2022-03-17T03:32:49Z"
"","11954","MINOR: remove kafka-preferred-replica-election.sh in doc","We've removed `kafka-preferred-replica-election.sh` in https://github.com/apache/kafka/pull/10443 , but we forgot to update the doc. Fix it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-27T03:07:39Z","2022-03-27T14:54:10Z"
"","12261","MINOR: add java 8/scala 2.12 deprecation info in doc","We've deprecated java 8 and scala 2.12 in KIP-750 and KIP-751 since v3.0. We should add a note in notable changes in v3.0. And also update the `Java version` section in doc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-06-07T07:55:22Z","2022-06-09T03:04:31Z"
"","11857","KAFKA-12648: fix bug where thread is re-added to TopologyMetadata when shutting down","We used to call `TopologyMetadata#maybeNotifyTopologyVersionWaitersAndUpdateThreadsTopologyVersion` when a thread was being unregistered/shutting down, to check if any of the futures listening for topology updates had been waiting on this thread and could be completed. Prior to invoking this we make sure to remove the current thread from the TopologyMetadata's `threadVersions` map, but this thread is actually then re-added in the `#maybeNotifyTopologyVersionWaitersAndUpdateThreadsTopologyVersion` call.  To fix this, we should break up this method into separate calls for each of its two distinct functions, updating the version and checking for topology update completion. When unregistering a thread, we should only invoke the latter method","closed","","ableegoldman","2022-03-07T09:14:44Z","2022-03-08T07:59:44Z"
"","12166","KAFKA-13817 Always sync nextTimeToEmit with wall clock","We should sync nextTimeToEmit with wall clock on each method call to ensure throttling works correctly in case of clock drift. If we dont, then in the event of significant clock drift, throttling might not happen for a long time, this can hurt performance.  I've added a unit test to simulate clock drift and verify my change works.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","qingwei91","2022-05-16T13:57:00Z","2022-07-23T08:17:21Z"
"","12432","KAFKA-14095: Improve handling of sync offset failures in MirrorMaker","We should not treat UNKNOWN_MEMBER_ID and FENCED_INSTANCE_ID as unexpected errors in the Admin client. In MirrorMaker, check the result of committing offsets and log an useful error message in case that failed with UNKNOWN_MEMBER_ID.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","admin,","mimaison","2022-07-22T17:27:09Z","2022-08-01T10:59:44Z"
"","11810","KAFKA-13281: can remove topologies while in a created state","We should be able to change the topologies in a created state. This should include removing them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-02-26T00:19:51Z","2022-02-26T03:11:21Z"
"","11709","MINOR: fix testDescribeUnderMinIsrPartitionsMixed","We should assert row.length before reading row(0) and row(1); Otherwise we could get an ArrayIndexOutOfBoundsException out of blue which is not easy to read","closed","","ccding","2022-01-24T20:31:32Z","2022-01-25T01:47:47Z"
"","12380","MINOR: Remove ARM/PowerPC builds from Jenkinsfile","We see the ARM agent check timing out frequently. I think the 5 minute timeout might be too aggressive. I think the original intent of this check was to prevent one of these agent check timeouts from propagating to the top level, but that never really worked. So perhaps we don't need these.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-07-05T17:58:39Z","2022-08-03T22:34:31Z"
"","11736","MINOR: Use CRC32 from standard library and remove custom implementation","We only use CRC32 in the legacy record formats (V0 and V1) and the CRC32 implementation from the standard library has received various performance improvements over the years (https://bugs.openjdk.java.net/browse/JDK-8245512 is a recent example). We use CRC32C in record format V2.  Also worth noting that record formats V0 and V1 have been deprecated since Apache Kafka 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-02-06T20:26:41Z","2022-02-07T14:42:36Z"
"","11533","MINOR: reduce log cleaner offset memory usage in KRaftClusterTestKit","We now use 2MB as with the other test harnesses.","closed","","lbradstreet","2021-11-24T00:47:31Z","2021-11-25T17:56:47Z"
"","11681","KAFKA-8785: fix request timeout by waiting for metadata cache up-to-date","We initializing `adminClient`, we'll first fetch the metadata of the brokers. Usually, we expected the brokers are already up (and metadata are updated in server side). So that we can choose 1 node to connect to in the following request. But if we only fetch ""partial"" metadata of the brokers, and after the ""partial"" brokers shutdown, the `adminClient` will not work anymore due to no nodes to connect to.  So, the reason why this test is flaky is because we have race condition at the beginning of the test, when brokers are staring up, and the `adminClient` is requesting for brokers metadata. Once the `adminClient` only got broker2/broker3, the test will fail, because in these tests, broker2 and broker3 will be shutdown to test leader election.   Fix this issue by explicitly waiting for metadata cache up-to-date in `waitForReadyBrokers`, and let admin client get created after `waitForReadyBrokers`.  ``` java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: describeTopics 	at java.base/java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:395) 	at java.base/java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1999) 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165) 	at kafka.utils.TestUtils$.$anonfun$waitForBrokersOutOfIsr$1(TestUtils.scala:1897) 	at kafka.utils.TestUtils$.waitForBrokersOutOfIsr(TestUtils.scala:992) 	at kafka.admin.LeaderElectionCommandTest10.testPathToJsonFile(LeaderElectionCommandTest10.scala:155) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-15T07:58:45Z","2022-04-19T06:13:21Z"
"","12262","KAFKA-13942: Fix kraft timeout in LogOffsetTest","We have been seeing a lot of timeouts in `LogOffsetTest` when KRaft is enabled. The problem is the dependence on `MockTime`. In the KRaft broker, we need a steadily advancing time for events in `KafkaEventQueue` to get executed. In the case of the timeouts, the broker was stuck with the next heartbeat event in the queue. We depended on the execution of this event in order to send the next heartbeat and complete the `initialCatchUpFuture` and finish broker startup. This caused the test to get stuck during initialization, which is probably why the `@Timeout` wasn't working.   As far as I can tell, the test does not have a strong dependence on `MockTime`, so I have replaced it with system time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-06-07T18:40:42Z","2022-06-08T00:50:46Z"
"","11766","MINOR: Remove redundant forwarding integration tests","We have a few integration tests for the forwarding logic which were added prior to kraft being ready for integration testing. Now that we have enabled kraft in integration tests, these tests are redundant and can be removed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-15T20:52:09Z","2022-02-16T03:38:30Z"
"","12173","MINOR: Add 3.0 to streams system tests (for 3.1 branch)","We forgot to add them in the 3.1 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dajac","2022-05-17T11:16:50Z","2022-05-24T06:59:52Z"
"","11653","MINOR: improve test runtime by unblocking purgatory and quota manager threads","We currently end up waiting far too long to shutdown the purgatory and quota manager ShutdownableThread(s) and many of these shutdowns are performed serially. We now unblock these threads with a no-op message.  This brings the time required to run the scala 2.13/JDK 17 build down to 52 minutes from approx 90 minutes.","closed","","lbradstreet","2022-01-05T22:46:27Z","2022-01-06T09:22:20Z"
"","11576","KAFKA-13512: topicIdsToNames and topicNamesToIds allocate unnecessary maps","We are creating a new map unnecessarily for these methods. Remove the extra map creation and simply wrap in unmodifiable map.   I've also added a benchmark for the map method.  Here are some results when I limited partitions to 20 only.   Before change: ``` Benchmark                                 (partitionCount)  (topicCount)  Mode  Cnt   Score   Error  Units MetadataRequestBenchmark.testTopicIdInfo                20           500  avgt   15  16.942 ± 0.306  ns/op MetadataRequestBenchmark.testTopicIdInfo                20          1000  avgt   15  19.476 ± 0.339  ns/op MetadataRequestBenchmark.testTopicIdInfo                20          5000  avgt   15  18.989 ± 0.482  ns/op ```  After change: ``` Benchmark                                 (partitionCount)  (topicCount)  Mode  Cnt   Score   Error  Units MetadataRequestBenchmark.testTopicIdInfo                20           500  avgt   15  11.120 ± 0.336  ns/op MetadataRequestBenchmark.testTopicIdInfo                20          1000  avgt   15  11.173 ± 0.489  ns/op MetadataRequestBenchmark.testTopicIdInfo                20          5000  avgt   15  11.003 ± 0.042  ns/op ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-12-07T21:28:39Z","2021-12-08T22:20:39Z"
"","11734","MINOR: Do not use optional args in `ProducerStateManager`","We allowed `maxProducerIdExpirationMs` and `time` to be optional in the `ProducerStateManager` constructor. We generally frown on optional arguments since it is too easy to overlook them. In this case, I thought it was especially dangerous because `maxTransactionTimeoutMs` used the same type as `maxProducerIdExpirationMs`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-04T23:13:09Z","2022-02-05T19:00:17Z"
"","12102","HOTFIX: Only measure in nano when producer metadata refresh is required","We added the metadata wait time in total blocked time (https://github.com/apache/kafka/pull/11805). But we added it in the critical path of `send` which is called per-record, whereas metadata refresh only happens rarely. This way the cost of `time.nanos` becomes unnecessarily significant as we call it twice per record.  This PR moves the call to inside the `waitOnMetadata` callee and only when we do need to wait for a metadata refresh round-trip (i.e. we are indeed blocking).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-04-27T17:27:14Z","2022-04-28T21:41:51Z"
"","11877","KAKFA-13699: new ProcessorContext is missing methods","We added `currentSystemTimeMs()` and `currentStreamTimeMs()` to the `ProcessorContext` via KIP-622, but forgot to add both methods to the new `api.ProcessorContext`.","closed","kip,","mjsax","2022-03-10T20:03:32Z","2022-03-14T16:22:06Z"
"","11570","KAFKA-12648: Wait for all threads to be on an empty topology before unsubscribing","Wait for all threads to be on an empty topology before unsubscribing this prevents and exception for ""Must initialize prevActiveTasks from ownedPartitions before initializing remaining tasks.""  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-12-07T01:27:02Z","2021-12-10T19:25:57Z"
"","11551","fixed typo ""iff"" -> ""if""","very simple. just a typo fix.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","XiaoyiPeng","2021-11-30T03:32:28Z","2021-11-30T06:43:48Z"
"","11547","fixed typo ""Exsiting"" -> ""Existing""","very simple. just a typo fix.  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","","kurtostfeld","2021-11-29T07:09:41Z","2022-02-01T10:09:05Z"
"","11925","MINOR: Bump trunk to 3.3.0-SNAPSHOT","Version bumps on trunk following the creation of the 3.2 release branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-03-21T19:32:32Z","2022-03-21T20:37:05Z"
"","12463","MINOR; Bump trunk to 3.4.0-SNAPSHOT","Version bumps in trunk after the creation of the 3.3 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-08-01T13:19:56Z","2022-08-01T16:54:29Z"
"","12254","3.2","Version 3.2 runs on Windows, run kafka-server-start.bat Error will be reported(I suggest that since it supports running on windows, this problem should be solved)： [2022-06-06 09:28:34,124] ERROR Exiting Kafka due to fatal exception (kafka.Kafka$) java.io.UncheckedIOException: Error while writing the Quorum status from the file .\tmp\kraft-combined-logs\__cluster_metadata-0\quorum-state         at org.apache.kafka.raft.FileBasedStateStore.writeElectionStateToFile(FileBasedStateStore.java:155)         at org.apache.kafka.raft.FileBasedStateStore.writeElectionState(FileBasedStateStore.java:128)         at org.apache.kafka.raft.QuorumState.transitionTo(QuorumState.java:477)         at org.apache.kafka.raft.QuorumState.initialize(QuorumState.java:212)         at org.apache.kafka.raft.KafkaRaftClient.initialize(KafkaRaftClient.java:364)         at kafka.raft.KafkaRaftManager.buildRaftClient(RaftManager.scala:203)         at kafka.raft.KafkaRaftManager.(RaftManager.scala:125)         at kafka.server.KafkaRaftServer.(KafkaRaftServer.scala:76)         at kafka.Kafka$.buildServer(Kafka.scala:79)         at kafka.Kafka$.main(Kafka.scala:87)         at kafka.Kafka.main(Kafka.scala) Caused by: java.nio.file.FileSystemException: .\tmp\kraft-combined-logs\__cluster_metadata-0\quorum-state.tmp -> .\tmp\kraft-combined-logs\__cluster_metadata-0\quorum-state: This file is in use by another program and cannot be accessed by the process         at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)         at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)         at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:387)         at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)         at java.nio.file.Files.move(Files.java:1395)         at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:935)         at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:918)         at org.apache.kafka.raft.FileBasedStateStore.writeElectionStateToFile(FileBasedStateStore.java:152)         ... 10 more         Suppressed: java.nio.file.FileSystemException: .\tmp\kraft-combined-logs\__cluster_metadata-0\quorum-state.tmp -> .\tmp\kraft-combined-logs\__cluster_metadata-0\quorum-state: This file is in use by another program and cannot be accessed by the process                 at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)                 at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)                 at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)                 at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)                 at java.nio.file.Files.move(Files.java:1395)                 at org.apache.kafka.common.utils.Utils.atomicMoveWithFallback(Utils.java:932)                 ... 12 more","closed","","sunxue299","2022-06-06T01:48:34Z","2022-06-20T14:57:33Z"
"","12040","MINOR: Verify stopReplica if broker epoch not stale","Verify that ReplicaManager.stopReplica is called if the stop replica request doesn't result in a stale broker epoch error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-04-13T01:53:47Z","2022-04-13T16:05:30Z"
"","11923","KAFKA-6718 / Documentation","Validated by running kafka-site locally as described in [here](https://cwiki.apache.org/confluence/display/KAFKA/Setup+Kafka+Website+on+Local+Apache+Server).  Screenshots   ![Screenshot 2022-03-28 at 22 22 33](https://user-images.githubusercontent.com/8927925/160471424-0eaa151c-e281-4d25-99b0-6308c50b6476.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2022-03-21T12:12:53Z","2022-03-29T12:10:17Z"
"","12321","KAFKA-14012: Add warning to closeQuietly documentation about method references of null objects","Utils.closeQuietly method accepts `AutoCloseable` object, and close it and sometimes, method references of null objects are passed to it which causes npes. This PR fixes it..","closed","connect,","vamossagar12","2022-06-21T11:55:13Z","2022-07-29T05:46:55Z"
"","12287","KAFKA-13846: Use the new addMetricsIfAbsent API","Use the newly added function to replace the old `addMetric` function that may throw illegal argument exceptions.  Although in some cases concurrency should not be possible they do not necessarily remain always true in the future, so it's better to use the new API just to be less error-prone.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-13T18:53:33Z","2022-06-14T23:04:29Z"
"","11573","KAFKA-13507: GlobalProcessor ignores user specified names","Use the name specified via `consumed` parameter in `InternalStreamsBuilder#addGlobalStore` method for initializing the source name and processor name. If not specified, the names are generated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tamara-skokova","2021-12-07T12:01:56Z","2021-12-09T14:44:04Z"
"","12000","MINOR: Use the LEAVE_GROUP_ON_CLOSE_CONFIG configuration uniformly to avoid multiple codes using the `internal.leave.group.on.close` string alone","Use the LEAVE_GROUP_ON_CLOSE_CONFIG configuration uniformly to avoid multiple codes using the `internal.leave.group.on.close` string alone","closed","","RivenSun2","2022-04-06T06:58:21Z","2022-04-19T06:35:46Z"
"","12283","MINOR: Shutdown jmx reporter using internal utility method","Use the internal `Exit.addShutdownHook` to close jmx reporter since this internal version has some nicer logging.","closed","","divijvaidya","2022-06-10T16:37:32Z","2022-06-13T15:25:41Z"
"","11807","KAFKA-13698; KRaft authorizer should use host address instead of name","Use `InetAddress.getHostAddress` in `StandardAuthorizer` instead of `InetAddress.getHostName`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-25T20:03:37Z","2022-02-26T18:52:34Z"
"","11656","KAFKA-13579: upgrade netty/jetty/jackson to avoid vulnerability","upgrade netty/jetty/jackson to the latest version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-07T12:57:54Z","2022-02-02T22:21:25Z"
"","12051","MINOR: Update LICENSE-binary","Updates the license file.  Validate with   `./gradlewAll releaseTarGz` `tar xf core/build/distributions/kafka_2.13-3.3.0-SNAPSHOT.tgz` `cd xf kafka_2.13-3.3.0-SNAPSHOT` `for f in $(ls libs | grep -v ""^kafka\|connect\|trogdor""); do if ! grep -q ${f%.*} LICENSE; then echo ""${f%.*} is missing in license file""; fi; done`  If the last command does not output anything, the license file is up-do-date.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-04-14T20:14:09Z","2022-04-14T20:24:09Z"
"","12475","MINOR; Update scalafmt to latest version","Updates scalafmt to the latest version. This not only means migrating the checkstyle/.scalafmt.conf file but updating spotless as well. Due to improvements in the new version of scalafmt, it now also alphabetically sorts imports.  The diff to the actual .scala source files is due to the alphabetical sorting of imports. I verified that this PR passes spotless/scalacheck by running ` ./gradlew :spotlessScalaCheck` locally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mdedetrich","2022-08-02T20:36:27Z","2022-08-03T18:54:22Z"
"","12258","MINOR: Convert `ReassignPartitionsIntegrationTest` to KRaft","Updates relevant tests in `ReassignPartitionsIntegrationTest` for KRaft. We skip the JBOD and AlterPartition upgrade tests.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-06-06T21:39:13Z","2022-06-08T03:59:24Z"
"","12156","MINOR: Update release versions for upgrade tests with 3.1.1 release","Updates release versions in files that are used for upgrade test with the 3.1.1 release version.  (Also fixes a mistaken kafka-3.1.0 → kafka-3.2.0 in the Dockerfile)","closed","","tombentley","2022-05-13T08:25:51Z","2022-05-13T08:32:51Z"
"","11518","MINOR: Upgrade to Gradle 7.3.3","Updates Gradle to its newer version 7.3.3. This version includes the following relevant features: - Support for Java 17 - Support for Scala 3  For a further description of the release notes see: https://docs.gradle.org/7.3.3/release-notes.html  I did the update as per the description in Gradle's release notes: ``` ./gradlew wrapper --gradle-version=7.3.3 ```  This means `gradlew` script is updated to the newest version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-11-19T14:17:56Z","2022-02-02T14:06:01Z"
"","11840","KAFKA-13707: unify the naming convention of in-flight","Update the codes and comments in kafka-clients to unify the naming convention of in-flight, treated it as two words to alight with the configuration item `max.in.flight.requests.per.connection`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ArvinZheng","2022-03-03T19:25:46Z","2022-04-22T17:36:16Z"
"","12001","3.1.1 release notes","Update release notes for 3.1.1","closed","","tombentley","2022-04-06T09:41:28Z","2022-04-06T10:17:37Z"
"","12470","Update WordCountTransformerDemo comments","Update documentation comments in the demo code, after switch to the newer processor API (replacing the TransformerDemo)  ### Committer Checklist (excluded from commit message) - [ x] Verify design and implementation  - [x ] Verify test coverage and CI build status - [ x] Verify documentation (including upgrade notes)","open","","devdavidkarlsson","2022-08-02T07:56:06Z","2022-08-02T07:56:06Z"
"","11589","MINOR: update log and method name","update an unclear log message and method name  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-12-09T21:56:23Z","2021-12-16T03:43:48Z"
"","12260","MINOR: add note on IDEMPOTENT_WRITE ACL to 3.2.0 notable changes","Update 3.2.0 notable changes documentation to include a subsection on requiring IDEMPOTENT_WRITE permission when producing messages with default/idempotent configuration and broker version lower than 2.8.0.","open","","d-t-w","2022-06-07T01:21:01Z","2022-06-14T12:25:05Z"
"","12445","MINOR: remove unnecessary test stubbing","Unnecessary stubbing caused job failure  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lihaosky","2022-07-26T23:18:43Z","2022-07-27T02:21:12Z"
"","11760","KAFKA-13600: Kafka Streams - Fall back to most caught up client if no caught up clients exist","Unit test for case task assignment where no caught up nodes exist. Existing unit and integration tests to verify no other behaviour has been changed  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tim-patterson","2022-02-15T10:32:54Z","2022-03-28T14:56:57Z"
"","12101","MINOR: Fix minor typos in `PartitionChangeBuilder`","Two minor typos found while reading `PartitionChangeBuilder`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-27T13:55:43Z","2022-05-03T13:08:23Z"
"","12282","MINOR: Fail if Logging controller bean is not correctly registered","Two changes in this PR.  1. Remove synchronized block and simplify it by using a latch.  2. Do not ignore the return value for `CoreUtils.registerMBean`. Since the method does not throw an exception, the return type tells us whether the registration was successful or not.  Note that after this change we are throwing an exception if we fail to load the log4j controller BUT we do not fail if we fail to load a metrics reporter bean (existing behaviour).","open","","divijvaidya","2022-06-10T15:56:20Z","2022-07-07T10:06:24Z"
"","11718","MINOR: ConsoleConsumer should not always exit when Consumer::poll returns an empty record batch","Transactions system tests fail in trunk: http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2022-01-26--001.system-test-kafka-trunk--1643203818--confluentinc--master--100303aecc/report.html.  Error: ``` [2022-01-28 08:25:24,917] TRACE [Consumer clientId=console-consumer, groupId=concurrent_consumer] Returning empty records from `poll()` since the consumer's position has advanced for at least one topic partition (org.apache.kafka.clients.consumer.KafkaConsumer) [2022-01-28 08:25:24,917] ERROR Error processing message, terminating consumer process:  (kafka.tools.ConsoleConsumer$) org.apache.kafka.common.errors.TimeoutException ```  With https://github.com/apache/kafka/commit/ddb6959c6272d2039ed8c9f595634c3c9573f85e, `Consumer::poll` will return an empty record batch when position advances due to aborted transactions or control records. This makes the `ConsoleConsumer` exists because it assumes that `poll` returns due to the timeout being reached.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-28T15:48:37Z","2022-01-31T09:34:14Z"
"","11699","KAFKA-13583; Fix FetchRequestBetweenDifferentIbpTest flaky tests","Too short polling time leads to not getting the results we want, so I increase the polling time appropriately.Please tell me can this be done by optimizing the internal logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-01-21T16:31:15Z","2022-02-03T09:59:31Z"
"","11865","Switch ARM and PowerPC builds to Scala 2.13","todo: - Set java version explicitly - Rename stages   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mimaison","2022-03-08T16:54:38Z","2022-03-09T10:46:26Z"
"","11559","KAFKA-13435; Group won't consume partitions added after static member restart","TODO  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-12-01T15:39:21Z","2022-01-18T19:38:15Z"
"","12384","KAFKA-10199: Add methods to add and remove tasks to task manager","To integrate the state updater into the current code, we need the ability to add and remove tasks from the task manager. This functionality is needed to ensure that a task is managed either by the task manager or by the state updater but not by both.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-06T10:33:27Z","2022-07-18T11:30:53Z"
"","11651","MINOR: allow test reruns to be merged with original runs","This will need some better jenkins reporting in order to be mergeable.","open","","lbradstreet","2022-01-05T22:18:10Z","2022-01-06T00:42:16Z"
"","12192","MINOR: Use parameterized logging in StandardAuthorizer and StandardAuthorizerData","This updates StandardAuthorizer and StandardAuthorizerData to use parameterized logging per the SLF4J recommendation (see https://www.slf4j.org/faq.html). This also removes a couple if statements that explicitly check if trace is enabled, but the logger should handle not publishing the message and not constructing the String if trace is not enabled.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andymg3","2022-05-21T19:42:58Z","2022-05-22T16:08:16Z"
"","11902","MINOR: fix shouldWaitForMissingInputTopicsToBeCreated test","This test was falling occasionally. It does appear to be a matter of the tests assuming perfecting deduplication/caching when asserting the test output records, ie a bug in the test not in the real code. Since we are not assuming that it is going to be perfect I changed the test to make sure the records we expect arrive, instead of only those arrive.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-03-15T20:34:21Z","2022-03-15T20:54:48Z"
"","11485","KAFKA-13421; Reenable `testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup`","This test was disabled in https://github.com/apache/kafka/commit/af8100b94fda4a27511797233e9845078ae8a69f. The reason the test was failing is that it assumes that the reference to `servers` can be mutated directly. The implementation in `IntegrationTestHarness` is intended to allow this by returning a mutable buffer, but the implementation actually returns a copy of the underlying collection. This caused the test case to create multiple `KafkaServer` instances instead of one as intended because it was modifying the copy. This led to the broker registration failure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-11-10T20:59:09Z","2021-11-17T21:51:17Z"
"","11868","KAFKA-12648: fix flaky #shouldAddToEmptyInitialTopologyRemoveResetOffsetsThenAddSameNamedTopologyWithRepartitioning","This test has started to become flaky at a relatively low, but consistently reproducible, rate. Upon inspection, we find this is due to IOExceptions during the #cleanUpNamedTopology call -- specifically, most often a `DirectoryNotEmptyException` with an ocasional` FileNotFoundException`  Basically, signs pointed to having returned from/completed the `#removeNamedTopology` future prematurely, and moving on to try and clear out the topology's state directory while there was a streamthread somewhere that was continuing to process/close its tasks.  I believe this is due to updating the thread's topology version _before_ we perform the actual topology update, in this case specifically the act of eg clearing out a directory. If one thread updates its version and then goes to perform the topology removal/cleanup when the second thread finishes its own topology removal, this other thread will check whether all threads are on the latest version and complete any waiting futures if so -- which means it can complete the future before the first thread has actually completed the corresponding action","closed","","ableegoldman","2022-03-09T08:54:04Z","2022-03-13T04:13:01Z"
"","12186","MINOR: Deflake OptimizedKTableIntegrationTest","This test has been flaky due to unexpected rebalances during the test. This change fixes it by detecting an unexpected rebalance and retrying the test logic (within a timeout).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-05-19T22:15:22Z","2022-05-20T14:17:43Z"
"","11827","MINOR: fix flaky shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing","This test has been failing somewhat regularly due to going into the ERROR state before reaching RUNNING during the startup phase. The problem is that we are reusing the `DELAYED_INPUT_STREAM` topics, which had previously been assumed to be uniquely owned by a particular test. We should make sure to delete and re-create these topics for any test that uses them.","closed","","ableegoldman","2022-03-01T22:51:15Z","2022-03-04T18:31:38Z"
"","12387","KAFKA-10199: Add RESUME in state updater","This should be reviewed after https://github.com/apache/kafka/pull/12386.  1) Need to check `enforceRestoreActive` / `transitToUpdateStandby` when resuming a paused task. 2) Do not expose another `getResumedTasks` since I think its caller only need the `getPausedTasks`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-07-07T01:38:43Z","2022-07-19T16:44:30Z"
"","11960","MINOR: Disable SocketServerTest.closingChannelWithBufferedReceives and SocketServerTest.remoteCloseWithoutBufferedReceives","This reverts commit d706d6cac4622153973d131417e809ee57c60de0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-29T08:24:00Z","2022-03-29T12:31:15Z"
"","11873","MINOR: Revert ""KAFKA-13542: add rebalance reason in Kafka Streams (#11804)""","This reverts commit 2ccc834faa3fffcd5d15d2463aeef3ee6f5cea13. We were seeing serious regressions in our state heavy benchmarks. We saw that our state heavy benchmarks were experiencing a really bad regression. The State heavy benchmarks runs with rolling bounces with 10 nodes.  We regularly saw this exception ``` 17165 java.lang.OutOfMemoryError: Java heap space                                                                                                                                                                                               17166 Dumping heap to ./senrollingBounce10-suNONE-stMEM-cENABLED-lENABLED-snkNO_SINK-ks1000000-kdsequential-prt100-ec2i3.large-1,1-heap-dump-try-1 ... 17167 Heap dump file created [4129525648 bytes in 12.538 secs] 17168 217283 [kafka-coordinator-heartbeat-thread | benchmark] ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - [Consumer clientId=benchmark-727ac648-f5d5-43fb-95c3-549dc2da033d-StreamThread-1-consumer, groupId=benchm      ark] Heartbeat thread failed due to unexpected error 17169 java.lang.OutOfMemoryError: Java heap space 17169 java.lang.OutOfMemoryError: Java heap space 17170         at java.base/java.nio.HeapByteBuffer.(HeapByteBuffer.java:61) 17171         at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348) 17172         at org.apache.kafka.common.memory.MemoryPool$1.tryAllocate(MemoryPool.java:30) 17173         at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:113) 17174         at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:452) 17175         at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:402) 17176         at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:674) 17177         at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:576) 17178         at org.apache.kafka.common.network.Selector.poll(Selector.java:481) 17179         at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560) 17180         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265) 17181         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup(ConsumerNetworkClient.java:306) 17182         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$HeartbeatThread.run(AbstractCoordinator.java:1420) ```  I ran through a git bisect and found this commit. We verified that the commit right before did not have the same issues as this one did. I then reverted the problematic commit and ran the benchmarks again on this commit and did not see any more issues. We are still looking into the root cause, but for now since this isn't a critical improvement so we can remove it temporarily.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-03-09T22:32:18Z","2022-03-10T21:53:03Z"
"","12014","MINOR: Fix support for custom commit ids in the build","This regressed in ca375d8004c1 due to a typo. We need tests for our builds. :)  I verified that passing the commitId via `-PcommitId=123` works correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-04-07T21:31:37Z","2022-04-08T23:40:03Z"
"","12023","Create AbstractLogContext class with static and dynamic subclasses, remove DynamicPrefixLogger","This reduces duplication and boilerplate involved in creating wrapped `Logger` instances, and preserves the existing precedent of wrapping `LocationAwareLogger` instances with a `LocationAwareKafkaLogger` instance.  The `ContextualLogging::setLoggingContext` method is also altered to accept an `AbstractLoggingContext` instance instead of a dynamic prefix, which is more flexible and requires less boilerplate for classes that want to implement the mixin interface.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","C0urante","2022-04-09T17:25:33Z","2022-04-11T00:43:55Z"
"","12441","KAFKA-14108: Ensure both JUnit 4 and JUnit 5 tests run","This pull request addresses the problem reported in https://github.com/apache/kafka/pull/12285 and tracked in https://issues.apache.org/jira/browse/KAFKA-14108","closed","","clolov","2022-07-26T12:38:46Z","2022-07-29T15:21:26Z"
"","12302","KAFKA-14004: Migrate streams module to JUnit 5 - Part 3","This pull request addresses https://issues.apache.org/jira/browse/KAFKA-14004. It is the third of a series of pull requests which address the move of Kafka Streams tests from JUnit 4 to JUnit 5.  This pull request will be rebased on top of https://github.com/apache/kafka/pull/12285 once it is merged. The file streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java is only added in this pull request for the build to succeed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","clolov","2022-06-16T16:58:40Z","2022-07-11T09:33:30Z"
"","12301","KAFKA-14003: Migrate streams module to JUnit 5 - Part 2","This pull request addresses https://issues.apache.org/jira/browse/KAFKA-14003. It is the second of a series of pull requests which address the move of Kafka Streams tests from JUnit 4 to JUnit 5.  **This pull request will be rebased on top of https://github.com/apache/kafka/pull/12285 once it is merged. The file streams/src/test/java/org/apache/kafka/streams/integration/utils/IntegrationTestUtils.java is only added in this pull request for the build to succeed.**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","clolov","2022-06-16T16:14:54Z","2022-07-20T09:30:35Z"
"","12285","KAFKA-14001: Migrate streams module to JUnit 5 - Part 1","This pull request addresses https://issues.apache.org/jira/browse/KAFKA-14001. It is the first of a series of pull requests which address the move of Kafka Streams tests from JUnit 4 to JUnit 5.  _The below description is kept to track the history of this pull request._  ``` This is the first part of KAFKA-7342 (https://issues.apache.org/jira/browse/KAFKA-7342). Here are the decisions I made in this change: * Do not (yet) migrate files using Parameterized because they require more in-depth changes. A list of the files which I haven't changed can be found here (1). * Prefix assertTrue, assertFalse, assertNull etc. with Assertions. * Run IntelliJ's Optimize Imports and streams:spotlessApply on the streams module.  1. List of untouched tests. Once these are migrated then JUnit4 should no longer be used in Kafka Streams  streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/EOSUncleanShutdownIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/IQv2StoreIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinMaterializationIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/PositionRestartIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/SlidingWindowedKStreamIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinTopologyOptimizationIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreatorTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingWindowBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreIteratorTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapterTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedSegmentedBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/SessionStoreFetchTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedKeyValueStoreBuilderTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreFetchTest.java streams/src/test/java/org/apache/kafka/streams/integration/GlobalKTableEOSIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/KTableKTableForeignKeyJoinIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/ResetPartitionTimeIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/StreamStreamJoinIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/TableTableJoinIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImplTest.java streams/src/test/java/org/apache/kafka/streams/integration/EosV2UpgradeIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/KTableEfficientRangeQueryTest.java streams/src/test/java/org/apache/kafka/streams/integration/RangeQueryIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/SuppressionDurabilityIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregateTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreToProcessorContextAdapterTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/TimestampedKeyValueStoreMaterializerTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueIteratorFacadeTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/MergedSortedCacheWrappedWindowStoreKeyValueIteratorTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSegmentedBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBWindowStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedCachingPersistentWindowStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/TimestampedWindowStoreBuilderTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionTopicsTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingSessionBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/KeyValueStoreBuilderTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyKeyValueStoreFacadeTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBSessionStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/SessionKeySchemaTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/WindowKeySchemaTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/RocksDBMetricsRecorderTest.java streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/integration/TimeWindowedKStreamIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableSourceNodeTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingTimestampedWindowBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ListValueStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreFacadeTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimeOrderedWindowSegmentedBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/SessionStoreBuilderTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedWindowStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/WindowStoreBuilderTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/integration/AbstractJoinIntegrationTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractSessionBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractDualSchemaRocksDBSegmentedBytesStoreTest.java streams/src/test/java/org/apache/kafka/streams/state/internals/AbstractWindowBytesStoreTest.java ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","clolov","2022-06-12T06:31:45Z","2022-07-28T05:41:10Z"
"","11694","MINOR: deleteHorizonMs update to documentation and DumpLogSegments tool","This PR updates the documentation and tooling to match the changes made in https://github.com/apache/kafka/pull/10914  In the documentation, changes include the adding the new attribute and updating field names.  In the DumpLogSegments tool, when record batch information is printed, it will also include what the value of deleteHorizonMs is ( `OptionalLong.empty` or `OptionalLong[123456]` )","closed","","mattwong949","2022-01-20T00:06:28Z","2022-02-05T00:20:37Z"
"","12284","KAFKA-13980: Upgrade from Scala 2.12.15 to 2.12.16","This PR updates from Scala 2.12.15 to 2.12.16. Since its a binary compatible change there shouldn't be any problems.  Release notes can be found here https://github.com/scala/scala/releases/tag/v2.12.16  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mdedetrich","2022-06-11T08:38:50Z","2022-07-27T12:46:08Z"
"","12029","KAFKA-13815: Avoid reinitialization for a replica that is being deleted","This PR tries to avoid the reinitialization of the leader epoch cache and the partition metadata if the corresponding replica is being deleted. With this change, the asyncDelete method can run more efficiently, which means a StopReplica request with many partitions to be deleted can be processed more quickly.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gitlw","2022-04-11T13:47:59Z","2022-05-04T18:41:35Z"
"","12043","KAFKA-13828; Ensure reasons sent by the consumer are small","This PR reworks the reasons used in the ConsumerCoordinator to ensure that they remain reasonably short. The patch is trivial.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-13T08:03:11Z","2022-04-13T11:42:27Z"
"","12005","KAFKA-13803: Refactor Leader API Access","This PR refactors the leader API access in the follower fetch path.   Added a `LeaderEndPoint` interface which serves all access to the leader.   Added a `LocalLeaderEndPoint` and a `RemoteLeaderEndPoint` which implements the `LeaderEndPoint` interface to handle fetches from leader in local & remote storage respectively.  Modified `ReplicaAlterLogDirsThread` and `ReplicaFetcherThread` to accept a `LeaderEndPoint`, which is subsequently used to handle fetches.  Modified corresponding tests to account for the `LeaderEndPoint` refactor.  Currently in the process of adding test cases which directly test the fetch functions in `LocalLeaderEndPoint` and `RemoteLeaderEndPoint`.","closed","","rittikaadhikari","2022-04-06T22:09:20Z","2022-06-03T16:12:07Z"
"","12397","KAFKA-10199: Cleanup TaskManager and Task interfaces","This PR is to consolidate https://github.com/apache/kafka/pull/12338 and https://github.com/apache/kafka/pull/12384  In order to integrate with the state updater, we would need to refactor the TaskManager and Task interfaces. This PR achieved the following purposes:  1. Separate active and standby tasks in the Tasks placeholder, plus adding pendingActiveTasks and pendingStandbyTasks into Tasks. The exposed active/standby tasks from the Tasks set would only be mutated by a single thread, and the pending tasks hold for those tasks that are assigned but cannot be actively managed yet. For now they include two scenarios: a) tasks from unknown sub-topologies and hence cannot be initialized, b) tasks that are pending for being recycled from active to standby and vice versa. Note case b) would be added in a follow-up PR.  2. Extract any logic that mutates a task out of the Tasks / TaskCreators. Tasks should only be a place for maintaining the set of tasks, but not for manipulations of a task; and TaskCreators should only be used for creating the tasks, but not for anything else. These logic are all migrated into TaskManger.  3. While doing 2) I noticed we have a couple of minor issues in the code where we duplicate the closing logics, so I also cleaned them up in the following way: a) When closing a task, we first trigger the corresponding closeClean/Dirty function; then we remove the task from `Tasks` bookkeeping, and for active task we also remove its task producer if EOS-V1 is used. b) For closing dirty, we swallow the exception from close call and the remove task producer call; for closing clean, we store the thrown exception from either close call or the remove task producer, and then rethrow at the end of the caller. The difference though is that, for the exception from close call we need to retry close it dirty; for the exception from the remove task producer we do not need to re-close it dirty.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-07-08T23:54:59Z","2022-07-21T19:08:23Z"
"","12338","KAFKA-10199: Cleanup TaskManager and Task interfaces","This PR is to be reviewed after #12337.  In order to integrate with the state updater, we would need to refactor the TaskManager and Task interfaces. This PR achieved the following purposes:  1. Separate active and standby tasks in the Tasks placeholder, plus adding `pendingActiveTasks` and `pendingStandbyTasks` into Tasks. The exposed active/standby tasks from the Tasks set would only be mutated by a single thread, and the pending tasks hold for those tasks that are assigned but cannot be actively managed yet. For now they include two scenarios: a) tasks from unknown sub-topologies and hence cannot be initialized, b) tasks that are pending for being recycled from active to standby and vice versa. Note case b) would be added in a follow-up PR.   2. Extract any logic that mutates a task out of the Tasks / TaskCreators. Tasks should only be a place for maintaining the set of tasks, but not for manipulations of a task; and TaskCreators should only be used for creating the tasks, but not for anything else. These logic are all migrated into TaskManger.  3. While doing 2) I noticed we have a couple of minor issues in the code where we duplicate the closing logics, so I also cleaned them up in the following way: a) When closing a task, we first remove it from Tasks, and the trigger the corresponding `closeClean/Dirty` function; for active task, we also remove its task producer if EOS-V1 is used. b) For closing dirty, we swallow the exception from `close` call and the `remove task producer` call; for closing clean, we store the thrown exception from either `close` call or the `remove task producer`, and then rethrow at the end of the caller. The difference though is that, for the exception from `close` call we need to retry close it dirty; for the exception from the `remove task producer` we do not need to re-close it dirty.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-24T02:27:06Z","2022-07-08T23:55:18Z"
"","11837","KAFKA-6718 / Add rack awareness configurations to StreamsConfig","This PR is part of [KIP-708](https://cwiki.apache.org/confluence/display/KAFKA/KIP-708%3A+Rack+awareness+for+Kafka+Streams) and adds rack aware standby task assignment logic.  Rack aware standby task assignment won't be functional until all parts of this KIP gets merged.  Splitting PRs into three smaller PRs to make the review process easier to follow. Overall plan is the following:  ⏭️  Rack aware standby task assignment logic https://github.com/apache/kafka/pull/10851 ⏭️  Protocol change, add clientTags to SubscriptionInfoData https://github.com/apache/kafka/pull/10802 👉  Add required configurations to StreamsConfig (public API change, at this point we should have full functionality)  This PR implements last point of the above mentioned plan.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2022-03-03T07:05:14Z","2022-03-17T02:21:05Z"
"","11514","KAFKA-13480: Track Position in KeyValue stores","This PR is levering the previously added StateStoreContext::recordMetadata() to track the current position seen by a state store.","closed","streams,","patrickstuedi","2021-11-18T14:31:27Z","2021-11-25T00:28:01Z"
"","11646","[WIP] KAFKA-13566: producer exponential backoff implementation for KIP-580","This PR is dependent on https://github.com/apache/kafka/pull/11627.   Follow up https://github.com/apache/kafka/pull/8846, to complete the Implementation of [KIP-580](https://cwiki.apache.org/confluence/display/KAFKA/KIP-580%3A+Exponential+Backoff+for+Kafka+Clients): add producer exponential backoff  Updated classes: 1.  RecordAccumulator 2. TransactionManager  Co-Authored-By: Cheng Tan    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2022-01-04T07:36:30Z","2022-02-05T07:14:12Z"
"","11647","[WIP] KAFKA-13567: adminClient exponential backoff implementation for KIP-580","This PR is dependent on https://github.com/apache/kafka/pull/11627.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2022-01-04T13:34:17Z","2022-02-05T07:14:58Z"
"","12473","KAFKA-13133: Replace EasyMock and PowerMock with Mockito for AbstractHerderTest","This PR is created on top of abandoned PR https://github.com/apache/kafka/pull/11137. I have added the previous author's comment in this PR for attribution. I have also addressed the pending comments from @mimaison in this PR.  ## Notes to help the reviewer: - When we run the tests using @RunWith(MockitoJUnitRunner.StrictStubs.class) Mockito performs a verify() for all stubs that are mentioned, hence, there is no need to explicitly verify the stubs (unless you want to verify the number of times etc.). Note that this does not work for static mocks.  ## Testing  `./gradlew connect:runtime:unitTest --tests AbstractHerderTest` is successful.","open","connect,","divijvaidya","2022-08-02T16:56:47Z","2022-08-03T10:24:22Z"
"","12140","KAFKA-13891: reset generation when syncgroup failed with REBALANCE_IN_PROGRESS","This PR is a missing part of https://github.com/apache/kafka/pull/11451  Previous change want to solve https://issues.apache.org/jira/browse/KAFKA-13419, but in the final code didn't add code to reset generation id when SyncGroup received REBALANCE_IN_PROGRESS error.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aiquestion","2022-05-09T16:22:34Z","2022-06-13T03:21:12Z"
"","12367","MINOR: record lag max metric documentation enhancement","This PR is a documentation enhancement to elaborate of records-lag-max as discussed here https://issues.apache.org/jira/browse/KAFKA-13936?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17559436#comment-17559436","closed","","prashanthjbabu","2022-06-30T03:19:14Z","2022-06-30T16:30:42Z"
"","12427","KAFKA-10199: Add tasks to state updater when they are created","This PR introduces an internal config to enable the state updater.  If the state updater is enabled newly created tasks are added to the state updater.  The integration of the state updater starts with this PR and is not finished.  Additionally, this PR introduces a builder for mocks for tasks.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-21T15:52:19Z","2022-07-25T17:08:55Z"
"","12437","KAFKA-13769 Add tests for ForeignJoinSubscriptionProcessorSupplier","This PR introduces a test suite that could've caught the issue hotfixed in https://github.com/apache/kafka/pull/12420.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Gerrrr","2022-07-25T15:34:20Z","2022-07-27T18:58:12Z"
"","12036","KAFKA-13823 Feature flag changes from KIP-778","This PR includes the changes to feature flags that were outlined in KIP-778.  ## Single finalized version level  The UpdateFeatures RPC and the FeatureLevelRecord were updated to remove the max version level. Other changes to the RPC from KIP-778 include:  * Adding dry-run to the RPC so the controller can actually attempt the upgrade (rather than the client) * Introduce a upgrade type enum instead of ""allowDowngrade"" boolean  ## kafka-features.sh changes  This tool was overhauled in KIP-778 and now includes sub-commands.  ``` $ ./bin/kafka-features.sh --help usage: kafka-features [-h] --bootstrap-server BOOTSTRAP_SERVER [--command-config COMMAND_CONFIG] {describe,upgrade,downgrade,disable} ...  This tool manages feature flags in Kafka.  positional arguments:   {describe,upgrade,downgrade,disable}     describe             Describe one or more feature flags.     upgrade              Upgrade one or more feature flags.     downgrade            Upgrade one or more feature flags.     disable              Disable one or more feature flags. This is the same as downgrading the version to zero.  optional arguments:   -h, --help             show this help message and exit   --bootstrap-server BOOTSTRAP_SERVER                          A comma-separated list of host:port pairs to use for establishing the connection to the Kafka cluster.   --command-config COMMAND_CONFIG                          Property file containing configs to be passed to Admin Client. ```  Help output from the ""upgrades"" sub-command:  ``` $ ./bin/kafka-features.sh upgrade --help usage: kafka-features --bootstrap-server BOOTSTRAP_SERVER upgrade [-h] [--feature FEATURE] [--version VERSION] [--release RELEASE] [--dry-run]  optional arguments:   -h, --help             show this help message and exit   --dry-run              Perform a dry-run of this upgrade operation.  Upgrade specific features:   --feature FEATURE      A feature flag to upgrade. This option may be repeated for upgrading multiple feature flags.   --version VERSION      The version to upgrade to.  Upgrade to feature level defined for a given release:   --release RELEASE ```  Refer to [KIP-778](https://cwiki.apache.org/confluence/display/KAFKA/KIP-778%3A+KRaft+Upgrades) for more details on the new command structure.","closed","","mumrah","2022-04-12T19:58:32Z","2022-04-14T17:04:46Z"
"","11882","Improve producer Javadoc about send with acks = 0","This PR improves the Javadoc related to the producer `send(ProducerRecord record, Callback callback)` method in order to clarify that if the producer is configured with `acks = 0`, the callback is anyway called immediately (right after putting the record in the internal buffer as already explained) but with `offset = -1` because of not waiting for acknowledge from the broker.","closed","","ppatierno","2022-03-11T08:19:41Z","2022-03-15T11:34:16Z"
"","12188","KAFKA-10892: Shared Readonly State Stores","This PR implements KIP-813: support for shareable state stores.  Tests were created to validate the structure of the resulting topology.   Certain parts (MockProcessor eg.) have been migrated to the newer PAPI as well. Tests have been ran against the whole project to be sure this migration doesn't cause any side-effects.  The contribution is my original work and I license the work to the project under the project's open source license","open","kip,","calmera","2022-05-20T09:50:28Z","2022-07-07T02:04:38Z"
"","11491","KAFKA-13442: REST API endpoint for fetching a connector's config def","This PR implements a new Connect REST endpoint to fetch the config definition of a connector. This new API uses the HTTP GET method and is under the /connector-plugins/{connector-type}/config path. The motivation of this is that some UI management tools could benefit from fetching the config definitions to help the user with a sample connector configuration. Using the config definition of the connector for sample generation is an easy way to solve this problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","viktorsomogyi","2021-11-12T14:53:09Z","2022-02-25T13:35:58Z"
"","12240","KAFKA-13916; Fenced replicas should not be allowed to join the ISR in KRaft (KIP-841, Part 1)","This PR implements a first part of [KIP-841](https://cwiki.apache.org/confluence/display/KAFKA/KIP-841%3A+Fenced+replicas+should+not+be+allowed+to+join+the+ISR+in+KRaft). Specifically, it implements the following: * Adds a new metadata version. * Adds the InControlledShutdown field to the BrokerRegistrationRecord and BrokerRegistrationChangeRecord and bump their versions. The newest versions are only used if the new metadata version is enabled. * Writes a BrokerRegistrationChangeRecord with InControlledShutdown set when a broker requests a controlled shutdown. * Ensures that fenced and in controlled shutdown replicas are not picked as leaders nor included in the ISR. * Adds or extends unit tests.  The second half (the AlterPartition part) is implemented in https://github.com/apache/kafka/pull/12181.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-06-02T13:38:47Z","2022-06-07T19:43:29Z"
"","12181","KAFKA-13916; Fenced replicas should not be allowed to join the ISR in KRaft (KIP-841, Part 2)","This PR implements [KIP-841](https://cwiki.apache.org/confluence/display/KAFKA/KIP-841%3A+Fenced+replicas+should+not+be+allowed+to+join+the+ISR+in+KRaft). Specifically, it implements the following: * It introduces INELIGIBLE_REPLICA and NEW_LEADER_ELECTED error codes. * The KRaft controller validates the new ISR provided in the AlterPartition request and rejects the call if any replica in the new ISR is not eligible to join the the ISR - e.g. when fenced or shutting down. The leader reverts to the last committed ISR when its request is rejected due to this. * The partition leader also verifies that a replica is not fenced before trying to add it back to the ISR. If it is not eligible, the ISR expansion is not triggered at all. * Updates the AlterPartition API to use topic ids. Updates the AlterPartition manger to handle topic names/ids. Updates the ZK controller and the KRaft controller to handle topic names/ids depending on the version of the request used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-19T13:57:12Z","2022-06-14T17:36:09Z"
"","12044","KAFKA-12738: implement exponential backoff for tasks","This PR further improves the error handling for named topologies by extending it to include exponential backoff for individual task errors, instead of a constant-time 5s backoff.  The next PR will round out the error handling to cover full topology-wide backoff for unhealthy topologies where one or more tasks are failing repeatedly over a long period of time, but that is not going to be included in this one.","open","","ableegoldman","2022-04-13T13:57:56Z","2022-04-19T09:41:11Z"
"","12303","MINOR: Fix plugin.path link in quickstart","This PR fixes the link to the `plugin.path` documentation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","modhanami","2022-06-16T18:11:55Z","2022-06-19T14:29:17Z"
"","11987","MINOR: Fix flaky testClientDisconnectionUpdatesRequestMetrics()","This PR fixes the flaky SocketServerTest.testClientDisconnectionUpdatesRequestMetrics() test. When a response is sent, the request metrics get updated. But if the metrics get updated before expectedTotalTimeCount is defined, the expected count gets defined with an inaccurate value.  It looks like the order was accidentally swapped in this commit https://github.com/apache/kafka/commit/16d36f1674f4867b4dc498464df20a0e8e18eb3f.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","anatasiavela","2022-04-02T09:03:40Z","2022-04-04T07:10:34Z"
"","12075","KAFKA-13841: Fix a case where we were unable to place on fenced brokers in KRaft mode","This PR fixes a case where we were unable to place on fenced brokers In KRaft mode. Specifically, if we had a broker registration in the metadata log, but no associated heartbeat, previously the HeartbeatManager would not track the fenced broker. This PR fixes this by adding this logic to the metadata log replay path in ClusterControlManager.","closed","kip-500,","cmccabe","2022-04-20T21:38:32Z","2022-04-21T21:58:05Z"
"","12207","MINOR: Several fixes and improvements for FeatureControlManager","This PR fixes a bug where FeatureControlManager#replay(FeatureLevelRecord) was throwing an exception if not all controllers in the quorum supported the feature being applied. While we do want to validate this, it needs to be validated earlier, before the record is committed to the log. Once the record has been committed to the log it should always be applied if the current controller supports it.  Fix another bug where removing a feature was not supported once it had been configured. Note that because we reserve feature level 0 for ""feature not enabled"", we don't need to use Optional; we can just return a range of 0-0 when the feature is not supported.  In error messages, give more detail about what the problem is when a controller in the quorum cannot handle a new version. For example, ""Local controller 0 does not support this feature"" rather than just ""quorum does not support this feature"".  Allow the metadata version to be downgraded when UpgradeType.UNSAFE_DOWNGRADE has been set. Previously we were unconditionally denying this even when this was set.  Add a builder for FeatureControlManager, so that we can easily add new parameters to the constructor in the future. This will also be useful for creating FeatureControlManagers that are initialized to a specific MetadataVersion.","closed","","cmccabe","2022-05-24T23:46:15Z","2022-06-01T23:09:44Z"
"","11605","MINOR: replace lastOption call in LocalLog#flush() to prevent NoSuchElementException","This PR fixed the iterator access issue during segment flush where a NoSuchElementException may be thrown. This can particularly happen if the Iterable[LogSegment] is accessed by 2 threads in parallel. A particular case is when retention periodic work empties segments from the segments map, while concurrently the async flush operation tries to access the very last segment from the same map. Since Iterable[T].lastOption is not thread-safe, the parallel access sometimes fails with a NoSuchElementException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yyu1993","2021-12-15T22:33:28Z","2021-12-18T00:02:24Z"
"","12312","KAFKA-10199: Expose tasks in state updater","This PR exposes the tasks managed by the state updater. The state updater manages all tasks that were added to the state updater and that have not yet been removed from it by draining one of the output queues.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-06-20T08:26:08Z","2022-06-27T06:43:52Z"
"","11832","MINOR: Clean up AlterIsrManager code","This PR cleans up the code in the AlterIsrManager.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-02T12:51:27Z","2022-03-09T06:31:10Z"
"","12119","MINOR: Clarify how to provide multiple log.dir","This PR clarifies how to specify multiple log directories on the broker.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2022-05-03T22:12:52Z","2022-05-04T17:20:57Z"
"","11956","KAFKA-13660: Use reload4j instead of log4j12","This PR bumps the version of slf4j-log4j12 to 1.7.36 which automatically uses slf4j-reload4j. The slf4j-reload4j binding delegates log processing to the reload4j logging framework.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-03-28T13:53:17Z","2022-03-31T12:44:50Z"
"","12465","KAFKA-12950: Replace EasyMock and PowerMock with Mockito for KafkaStreamsTest","This PR build on top of https://github.com/apache/kafka/pull/11017. I have added the previous author's comment in this PR for attribution. I have also addressed the pending comments from @chia7712 in this PR.  Notes to help the reviewer: 1. Mockito has `mockStatic` method which is equivalent to PowerMock's method. 2. When we run the tests using `@RunWith(MockitoJUnitRunner.StrictStubs.class)` Mockito performs a verify() for all stubs that are mentioned, hence, there is no need to explicitly verify the stubs (unless you want to verify the number of times etc.). Note that this does not work for static mocks.  ## Testing Validated that test is run successfully as part of `./gradlew streams:unitTest`   ## Code coverage ### Before ![Screenshot 2022-08-02 at 19 02 42](https://user-images.githubusercontent.com/71267/182432732-86f53330-b982-4fa0-ac20-97ddc96bb66a.png)  ### After ![Screenshot 2022-08-02 at 19 02 49](https://user-images.githubusercontent.com/71267/182432748-33b4ab03-b387-44ca-8af6-d24f09afbc9b.png)","open","","divijvaidya","2022-08-01T16:22:11Z","2022-08-02T17:03:39Z"
"","12315","MINOR: the Kafka metadata shell should support connecting to a controller quorum","This PR allows us to support connecting the metadata shell to a live KRaft quorum via --controller, as specified in the original KIPs. In order to do this, the PR factors out a ""MetadataShellSource"" interface which is implemented by both a snapshot file and a live quorum.  Previously, the ""shell"" gradle submodule dependend on ""core."" This extended the gradle build time because ""core"" takes a long time to build.  It also tended to reduce the encapsulation of the shell. This PR makes ""core"" depend on ""shell"" instead. In order to support this, the new entry point for the tool is kafka.tools.MetadataShellTool rather than org.apache.kafka.shell.MetadataShell.  RaftManager.scala: make close() idempotent, as it is for nearly everything else. This makes cleanup logic easier.","open","","cmccabe","2022-06-20T13:53:12Z","2022-07-21T00:18:14Z"
"","12122","Upgrade tests for KAFKA-13769","This PR adds upgrade tests for the changes in Foreign Key join protocol in KAFKA-13769. New upgrade tests ensure that FK joins keep working after a 2-bounce upgrade despite the protocol changes. I added the tests for all Kafka versions starting from 2.4 - the version that introduced FK joins.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Gerrrr","2022-05-04T18:29:12Z","2022-05-14T00:21:28Z"
"","12128","KAFKA-10199: Implement adding active tasks to the state updater","This PR adds the default implementation of the state updater. The implementation only implements adding active tasks to the state updater.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-05T18:59:54Z","2022-05-24T15:56:29Z"
"","12161","KAFKA-13873 Add ability to Pause / Resume KafkaStreams Topologies","This PR adds the ability to pause and resume KafkaStreams instances as well as named/modular topologies.  Added an integration test to show how pausing and resuming works.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","jnh5y","2022-05-13T17:31:27Z","2022-06-16T14:06:02Z"
"","12270","KAFKA-10199: Implement removing active and standby tasks from the state updater","This PR adds removing of active and standby tasks from the default implementation of the state updater. The PR also includes refactorings that clean up the code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-06-08T20:40:22Z","2022-06-09T18:39:33Z"
"","12448","KAFKA-14114: Adding Metadata Log Processing Error Related Metrics","This PR adds in 3 metrics as described in KIP-859 * kafka.server:type=broker-metadata-metrics,name=publisher-error-count * kafka.server:type=broker-metadata-metrics,name=listener-batch-load-error-count * kafka.controller:type=KafkaController,name=ForceRenounceCount","open","","niket-goel","2022-07-28T01:55:42Z","2022-07-29T18:40:42Z"
"","11731","KAFKA-13293: Reloading SSL Engine Factory","This PR adds an optional `SslEngineFactory` implementation that decorates and manages a delegate. It is able to recreate the delegate either on a schedule, or in response to a SSL configuration patch being applied. The purpose of this implementation is to permit the automatic reload of renewed client certificates as described in [KAFKA-13293](href=""https://issues.apache.org/jira/browse/KAFKA-13293).  This implementation includes a supporting unit test.  Thanks go to my colleague @joedj for the implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","teabot","2022-02-03T15:56:10Z","2022-07-08T11:15:07Z"
"","12200","KAFKA-10199: Implement adding standby tasks to the state updater","This PR adds adding of standby tasks to the default implementation of the state updater.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-05-24T11:34:38Z","2022-06-08T20:37:45Z"
"","11716","MINOR: Add 3.0 and 3.1 to streams system tests","This PR adds 3.0 and 3.1 to the streams system tests. I updated the java code to not use deprecated methods/classes/interfaces anymore. It does not compile otherwise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-26T15:55:19Z","2022-01-28T09:06:34Z"
"","11812","KAFKA-12738: address minor followup and consolidate integration tests of PR #11787","This PR addresses the remaining nits from the final review of https://github.com/apache/kafka/pull/11787  It also deletes two integration test classes which had only one test in them, and moves the tests to another test class file to save on the time to bring up an entire embedded kafka cluster just for a single run","closed","","ableegoldman","2022-02-26T04:44:36Z","2022-03-01T21:12:28Z"
"","11818","KAFKA-12558: Do not prematurely mutate partiton state and provide con…","This PR addresses the issue described here [KAFKA-12558](https://issues.apache.org/jira/browse/KAFKA-12558).   Additionally, The PR also allows to configure the max outstanding syncs in MirrorSourceTask because it is currently [hardcoded](https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceTask.java#L53). A lot of offset syncs messages are lost during burst of messages in the source cluster or when the MirrorMaker has a lot to catch up (fist run or being inactive for a while). In such scenario it will take a while to sync the offsets in the destination cluster with partitions without regular activity even with reaching the maximum parallelism - 1 task per partition.  The PR tries to mitigate the issue by providing a way to change the maximum allowed concurrent offset syncs so that there are less offset syncs loses.  Here are my steps to reproduce the offset syncs issue because of the max outstanding syncs limited to 10: 1. Source topic with 12 partitions and 1400 messages with minimal activity. Messages are getting produced on daily basis 2. Run MirrorMaker2 process within the destination cluster network with offset syncs topic location set to target and 5 tasks 3. 372 offset syncs messages arrived in the destination cluster offset syncs topic 4. 9 out of 12 partitions are not synced correctly in the destination cluster 5. Waiting for hours and more for new messages to arrive in source Kafka cluster which will sync the correct offsets","open","","emilnkrastev","2022-02-27T12:58:42Z","2022-02-27T12:58:42Z"
"","11503","KAFKA-13456: Tighten KRaft config checks/constraints","This patch tightens the configuration checks related to KRaft configs by adding the following constraints:  * `control.plane.listener.name` is confirmed to be empty in KRaft mode whenever a config object is created as opposed to later when the broker is given the config and tries to start. * `controller.listener.names` is required to be empty for the non-KRaft (i.e. ZooKeeper) case.  A ZooKeeper-based cluster that sets this config will fail to restart until this config is removed. * There must be no advertised listeners when running just a KRaft controller (i.e. when `process.roles=controller`).  This means neither `listeners` nor `advertised.listeners` (if the latter is explicitly defined) can contain a listener that does not also appear in `controller.listener.names`. * When running a KRaft broker (i.e. when `process.roles=broker` or `process.roles=broker,controller`), advertised listeners (which was already checked to be non-empty via the check that the inter-broker listener appear there) must not include any listeners appearing in `controller.listener.names`. * When running a KRaft controller (i.e. when `process.roles=controller` or `process.roles=broker,controller`) `controller.listener.names` must be non-empty and every one must appear in `listeners` * When running just a KRaft broker (i.e. when `process.roles=broker`) `controller.listener.names` must be non-empty and none of them can appear in `listeners`.  This was indirectly checked previously, but the indirect checks did not catch all cases. * When running just a KRaft broker we log a warning if more than one entry appears in `controller.listener.names` because only the first entry is used.  We also map the `CONTROLLER` listener name to the `PLAINTEXT` security protocol by default when using KRaft as a convenience.  This patch adds appropriate unit tests.  I have manually confirmed that our KRaft Quickstart config files in `config/kraft` remain valid (both for the combined and separate cases).  I also confirmed that KRaft system test configs remain valid (see sanity check test output below).  ``` ================================================================================ SESSION REPORT (ALL TESTS) ducktape version: 0.8.1 session_id:       2021-11-23--001 run time:         6 minutes 52.763 seconds tests run:        5 passed:           5 failed:           0 ignored:          0 ================================================================================ test_id:    kafkatest.sanity_checks.test_bounce.TestBounce.test_simple_run.metadata_quorum=COLOCATED_KRAFT.quorum_size=3 status:     PASS run time:   1 minute 56.752 seconds -------------------------------------------------------------------------------- test_id:    kafkatest.sanity_checks.test_bounce.TestBounce.test_simple_run.metadata_quorum=REMOTE_KRAFT.quorum_size=3 status:     PASS run time:   1 minute 49.306 seconds -------------------------------------------------------------------------------- test_id:    kafkatest.sanity_checks.test_bounce.TestBounce.test_simple_run.metadata_quorum=COLOCATED_KRAFT.quorum_size=1 status:     PASS run time:   59.592 seconds -------------------------------------------------------------------------------- test_id:    kafkatest.sanity_checks.test_bounce.TestBounce.test_simple_run.metadata_quorum=REMOTE_KRAFT.quorum_size=1 status:     PASS run time:   1 minute 9.941 seconds -------------------------------------------------------------------------------- test_id:    kafkatest.sanity_checks.test_bounce.TestBounce.test_simple_run.metadata_quorum=ZK.quorum_size=1 status:     PASS run time:   56.642 seconds -------------------------------------------------------------------------------- ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-11-15T22:07:50Z","2021-12-10T21:28:57Z"
"","12245","KAFKA-13410; Add a --release-version flag for storage-tool","This patch removes the `--metadata-version` and adds a `--release-version` to the kafka-storage tool. Format command help:  ``` usage: kafka-storage format [-h] --config CONFIG --cluster-id CLUSTER_ID [--ignore-formatted] [--release-version RELEASE_VERSION]  optional arguments:   -h, --help             show this help message and exit   --config CONFIG, -c CONFIG                          The Kafka configuration file to use.   --cluster-id CLUSTER_ID, -t CLUSTER_ID                          The cluster ID to use.   --ignore-formatted, -g   --release-version RELEASE_VERSION, -r RELEASE_VERSION                          A release version to use for the initial metadata.version. The default is (3.3-IV2) ```  This change is somewhat breaking since we are removing `--metadata-version` which was introduced in 1135f22eaf on May 18, but it has not been released yet.","closed","kip-500,","mumrah","2022-06-02T19:36:47Z","2022-06-07T18:25:40Z"
"","12085","KAFKA-13790; ReplicaManager should be robust to all partition updates from kraft metadata log","This patch refactors the `Partition.makeLeader` and `Partition.makeFollower` to be robust to all partition updates from the KRaft metadata log. Particularly, it ensures the following invariants: - A partition update is accepted if the partition epoch is equal or newer. The partition epoch is updated by the AlterPartition path as well so we accept an update from the metadata log with the same partition epoch in order to fully update the partition state. - The leader epoch state offset is only updated when the leader epoch is bumped. - The follower states are only updated when the leader epoch is bumped. - Fetchers are only restarted when the leader epoch is bumped. This was already the case but this patch adds unit tests to prove/maintain it.  In the mean time, the patch unifies the state change logs to be similar in both ZK and KRaft world.  Note that this PR focuses on the metadata log path. We might consider relaxing the leader epoch based fencing mechanism used on the ZK path now that the Partition is more robust.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-22T13:59:42Z","2022-05-09T18:47:17Z"
"","12081","MINOR: Refactor `kafka.cluster.Replica`","This patch refactors `kafka.cluster.Replica`, it usages and tests. This is part of the work in [KAFKA-13790](https://issues.apache.org/jira/browse/KAFKA-13790).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-21T11:12:28Z","2022-04-25T20:43:35Z"
"","12214","MINOR: Consolidate FinalizedFeatureCache into MetadataCache","This patch rearranges some code to remove the FinalizedFeatureCache and replace it with MetadataCache. Since features are included in the MetadataImage in KRaft mode (where feature flags are actually used), it makes sense to expose them through MetadataCache.  One catch was that the ZK controller needed access to the feature flags cache and I didn't want to pass MetadataCache into KafkaController. For this case I pulled out an interface of the two accessors the controller used.  This patch also adds a MetadataVersion getter on the MetadataCache interface.","closed","","mumrah","2022-05-25T20:30:56Z","2022-05-26T20:25:58Z"
"","11502","KAFKA-13071; Deprecate support for changing acls through the authorizer","This patch marks the following arguments as deprecated in kafka-acls.sh as documented in [KIP-604](https://cwiki.apache.org/confluence/display/KAFKA/KIP-604%3A+Remove+ZooKeeper+Flags+from+the+Administrative+Tools): --authorizer, --authorizer-properties, and --zk-tls-config-file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-11-15T20:25:26Z","2021-11-16T19:25:00Z"
"","11805","KAFKA-13692: include metadata wait time in total blocked time","This patch includes metadata wait time in total blocked time. First, this patch adds a new metric for total producer time spent waiting on metadata, called metadata-wait-time-ms-total. Then, this time is included in the total blocked time computed from StreamsProducer","closed","","rodesai","2022-02-25T02:58:59Z","2022-03-24T16:55:34Z"
"","11725","KAFKA-13221; Implement `PartitionsWithLateTransactionsCount` metric","This patch implements a new metric `PartitionsWithLateTransactionsCount` which tracks the number of partitions with late transactions in the cluster. This metric was documented in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-01T02:04:44Z","2022-02-02T19:57:13Z"
"","12455","KAFKA-14111 Fix sensitive dynamic broker configs in KRaft","This patch fixes an issue in KRaft where sensitive dynamic broker configs were failing to get updated on the brokers. In the ZK code path, we expect the sensitive config values to be encrypted in-place, and so the update logic was decrypting these values. In KRaft, we do not encrypt the values in ConfigRecords regardless of the type.  This PR defines a new passthrough password encoder which is used in KRaft mode only.  Most of the test cases in DynamicBrokerReconfigurationTest have been converted to also run in KRaft mode.","closed","","mumrah","2022-07-28T19:48:24Z","2022-08-03T18:00:23Z"
"","12171","MINOR: Convert admin integration tests","This patch enables KRaft support in `PlaintextAdminIntegrationTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-17T01:31:34Z","2022-05-18T18:07:13Z"
"","12269","KAFKA-13966 Set curClaimEpoch after we enqueue bootstrap write","This patch does two things to tighten up races during bootstrap. One is to simply move the volatile write until after the ""bootstrapMetadata"" event has been enqueued. The other thing is to prepend ""bootstrapMetadata"" to the queue.  From the JIRA, this test was flaky when we see the ""registerBroker"" event precede the ""boostrapMetadata"" event  ``` handleLeaderChange() start appendWriteEvent(registerBroker) appendWriteEvent(bootstrapMetadata) handleLeaderChange() finish registerBroker() -> writes broker registration to log bootstrapMetadata() -> writes bootstrap metadata to log ```  The test harness in QuorumControllerTest is polling the controllers to see when their current leader epoch matches the expected leader epoch  ```java         QuorumController activeController() throws InterruptedException {         AtomicReference value = new AtomicReference(null);         TestUtils.retryOnExceptionWithTimeout(20000, 3, () -> {             LeaderAndEpoch leader = logEnv.leaderAndEpoch();             for (QuorumController controller : controllers) {                 if (OptionalInt.of(controller.nodeId()).equals(leader.leaderId()) &&               --->  controller.curClaimEpoch() == leader.epoch()) {","closed","","mumrah","2022-06-08T17:46:02Z","2022-06-23T15:29:22Z"
"","12071","MINOR: Rename `ZkVersion` to `PartitionEpoch`","This patch does some initial cleanups in the context of KAFKA-13790. Mainly, it renames `ZkVersion` field to `PartitionEpoch` in the `LeaderAndIsrRequest`, the `LeaderAndIsr` and the `Partition`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-20T12:19:26Z","2022-04-22T18:38:20Z"
"","12483","KAFKA-14136 Generate ConfigRecord even if the value is unchanged","This patch changes the AlterConfigs behavior in KRaft mode to match that of ZK. When we receive a LegacyAlterConfig or IncrementalAlterConfig that does _not_ change a value for a key, we will still generate a ConfigRecord.  This is to allow certain refresh behavior on the broker side (e.g., reloading trust stores and key stores).  The DynamicBrokerReconfigurationTests which reload key stores and trust stores are enabled in this PR to validate the new behavior.  Also, a small fix for KAFKA-14115 is included.","open","","mumrah","2022-08-03T21:34:24Z","2022-08-03T22:22:17Z"
"","12050","KAFKA-13830 MetadataVersion integration for KRaft controller","This patch builds on #12072 and adds controller support for `metadata.version`. The kafka-storage tool now allows a user to specify a specific metadata.version to bootstrap into the cluster, otherwise the latest version is used.  Upon the first leader election of the KRaft quroum, this initial metadata.version is written into the metadata log. When writing snapshots, a FeatureLevelRecord for metadata.version will be written out ahead of other records so we can decode things at the correct version level.  This also includes additional validation in the controller when setting feature levels. It will now check that a given metadata.version is supportable by the quroum, not just the brokers.","closed","kip-500,","mumrah","2022-04-14T19:38:31Z","2022-05-18T19:09:04Z"
"","11957","MINOR: Fix display names for parameterized KRaft and ZK tests","This patch adds display names for KRaft and ZK tests. Without this, it becomes hard to understand in Jenkins test reports which test failed. With this addition, it becomes more clear which method in the test suite fails.","closed","","skaundinya15","2022-03-28T19:05:45Z","2022-03-28T23:20:21Z"
"","12031","KAFKA-13651; Add audit logging to `StandardAuthorizer`","This patch adds audit support through the `kafka.authorizer.logger` logger  to `StandardAuthorizer`. It follows the same conventions as `AclAuthorizer` with a similarly formatted log message. When `logIfAllowed` is set in the `Action`, then the log message is at DEBUG level; otherwise, we log at trace. When `logIfDenied` is set, then the log message is at INFO level; otherwise, we again log at TRACE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2022-04-11T17:20:04Z","2022-04-13T17:33:15Z"
"","12082","MINOR: Create case class to encapsulate fetch parameters and simplify handling","This patch adds a new case class `FetchParams` which encapsulates the parameters of the fetch request. It then uses this class in `DelayedFetch` directly instead of `FetchMetadata`. The intent is to reduce the number of things we need to change whenever we need to pass through new parameters. The patch also cleans up `ReplicaManagerTest` for more consistent usage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-04-22T00:40:24Z","2022-05-10T20:24:24Z"
"","11687","KAFKA-13592:Fix flaky test ControllerIntegrationTest.testTopicIdUpgradeAfterReassigningPartitions","this issue similar to https://github.com/apache/kafka/pull/11666.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-01-18T11:05:48Z","2022-06-07T00:56:49Z"
"","12400","KAFKA-13648: KRaft ClusterInstance does not allow for deferred start","This issue happens cause the `cluster.startup()` invoked already and the solution is just check the `clusterConfig.isAutoStart` in the  `BeforeTestExecutionCallback`.  In addition I believe checking just a broker state is not sufficient  and it's better to invoke `cluster.waitForReadyBrokers()` instead but I didn't change it, cause it was out of scope of the issue. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","blcksrx","2022-07-11T15:12:47Z","2022-08-02T22:28:50Z"
"","12097","MINOR: Make TopicPartitionBookkeeper and TopicPartitionEntry top level","This is the first step towards refactoring the `TransactionManager` so that it's easier to understand and test. The high level idea is to push down behavior to `TopicPartitionEntry` and `TopicPartitionBookkeeper` and to encapsulate the state so that the mutations can only be done via the appropriate methods.  Inner classes have no mechanism to limit access from the outer class, which presents a challenge when mutability is widespread (like we do here).  As a first step, we make `TopicPartitionBookkeeper` and `TopicPartitionEntry` top level and rename them and a couple of methods to make the intended usage clear and avoid redundancy.  To make the review easier, we don't change anything else except access changes required for the code to compile. The next PR will contain the rest of the refactoring.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-04-26T16:02:49Z","2022-05-11T16:30:47Z"
"","11677","KAFKA-13410 Add KRaft metadata.version for KIP-778","This is part of the implementation of KIP-778.  ## Introduce metadata.version A new feature flag `metadata.version` is defined. Brokers statically declare their supported versions in BrokerFeatures. Controllers do so in a similar way with QuorumFeatures. Brokers cannot register with the quorum unless they support the current `metadata.version` in use by the quorum.    ## Bootstrapping When using the `kafka-storage.sh format` command, we now can specify which `metadata.version` to bootstrap the cluster with.   ``` ./bin/kafka-storage.sh format -t LMmPeCgST7S6F3NgoVcrwA -c config/kraft/server.properties -v 2 ```  If no `metadata.version` is given, the latest stable version is automatically selected. The selected value is written into the `meta.properties` file as the ""initial.metadata.version"" property.  ```properties # #Thu Jan 13 14:05:12 EST 2022 cluster.id=LMmPeCgST7S6F3NgoVcrwA version=1 init.metadata.version=2 node.id=1 ```  > For KRaft preview clusters, meta.properties will already exist and not include the new ""initial.metadata.version"" property. In this case, the controller will automatically select version 1 on startup.  On first start up with this PR, the controller that is first elected leader upon startup will initialize a FeatureLevelRecord into the metadata log. The version used for this bootstrapping is read from ""initial.metadata.version"" in meta.properties. If an invalid version was read from the properties file, the controller will immediately shutdown.  ## kafka-features.sh changes  This tool is completely overhauled and now has sub-commands.  ``` $ ./bin/kafka-features.sh --help usage: kafka-features [-h] --bootstrap-server BOOTSTRAP_SERVER [--command-config COMMAND_CONFIG] {describe,upgrade,downgrade,disable} ...  This tool manages feature flags in Kafka.  positional arguments:   {describe,upgrade,downgrade,disable}     describe             Describe one or more feature flags.     upgrade              Upgrade one or more feature flags.     downgrade            Upgrade one or more feature flags.     disable              Disable one or more feature flags. This is the same as downgrading the version to zero.  optional arguments:   -h, --help             show this help message and exit   --bootstrap-server BOOTSTRAP_SERVER                          A comma-separated list of host:port pairs to use for establishing the connection to the Kafka cluster.   --command-config COMMAND_CONFIG                          Property file containing configs to be passed to Admin Client. ```  Help output from the ""upgrades"" sub-command:  ``` $ ./bin/kafka-features.sh upgrade --help usage: kafka-features --bootstrap-server BOOTSTRAP_SERVER upgrade [-h] [--feature FEATURE] [--version VERSION] [--release RELEASE] [--dry-run]  optional arguments:   -h, --help             show this help message and exit   --dry-run              Perform a dry-run of this upgrade operation.  Upgrade specific features:   --feature FEATURE      A feature flag to upgrade. This option may be repeated for upgrading multiple feature flags.   --version VERSION      The version to upgrade to.  Upgrade to feature level defined for a given release:   --release RELEASE ```  Refer to [KIP-778](https://cwiki.apache.org/confluence/display/KAFKA/KIP-778%3A+KRaft+Upgrades) for more details on the new command structure.  ## Upgrading metadata.version This PR only includes support for upgrading the metadata.version, not downgrading. An upgrade is performed using the ""kafka-features.sh"" tool and sends an UpdateFeature request to the controller (via a broker).  ``` ./bin/kafka-features.sh --bootstrap-server localhost:9092 upgrade --feature metadata.version --version 2 ```  When the controller handles this request, it verifies the new version and then commits a FeatureLevelRecord. A new FeatureLevelListener interface on the controller can be registered with FeatureControlManager for components which need to react to changes in the metadata.version. For now, the only use case is updating the internal state of QuorumControl to reflect the new version.  Brokers will see the new metadata.version once the FeatureLevelRecord has been replicated by the broker's quorum peer. The broker has a similar MetadataVersionChangeListener which is not currently used. We may want to remove this until we have a use case.","closed","","mumrah","2022-01-13T18:27:46Z","2022-04-15T00:01:41Z"
"","12339","MINOR: set UNKNOWN_MEMBER_ID explicitly when deleting group instance id","This is not a bug fix, but it makes code more readable. We expect the member id to be undefined(`UNKNOWN_MEMBER_ID`) when we are removing group instance id. Hence, it would be better to set `UNKNOWN_MEMBER_ID` explicitly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2022-06-24T05:13:32Z","2022-06-24T13:56:06Z"
"","12244","HOTFIX: only try to clear discover-coordinator future upon commit","This is another way of fixing KAFKA-13563 other than #11631.  Instead of letting the consumer to always try to discover coordinator in pool with either mode (subscribe / assign), we defer the clearance of discover future upon committing async only. More specifically, under manual assign mode, there are only three places where we need the coordinator:  1) commitAsync (both by the consumer itself or triggered by caller), this is where we want to fix. 2) commitSync, which we already try to re-discovery coordinator. 3) committed (both by the consumer itself based on reset policy, or triggered by caller), which we already try to re-discovery coordinator.  The benefits are that for manual assign mode that does not try to trigger any of the above three, then we never would be discovering coordinator. The original fix in #11631 would let the consumer to discover coordinator even if none of the above operations are required.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-02T18:22:26Z","2022-06-06T18:07:37Z"
"","11871","KAFKA-12879: Addendum to reduce flakiness of tests","This is an addendum to the KAFKA-12879 (#11797) to fix some tests that are somewhat flaky when a build machine is heavily loaded (when the timeouts are too small).  - Add an if check to void sleep(0) - Increase timeout in the tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","philipnee","2022-03-09T20:13:18Z","2022-03-09T20:37:49Z"
"","12120","Add mini test","This is a super simple test that verifies zk and kafka can start up. The purpose is mostly to verify any changes to ducktape, ducker-ak or underlying test infrastructure, but it is also useful for onboarding new engineers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","stan-confluent","2022-05-04T02:20:57Z","2022-07-13T00:06:42Z"
"","12350","KAFKA-13856:MirrorCheckpointTask meets ConcurrentModificationException","This is a simple and clear commit,with no side effect.","open","","Justinwins","2022-06-27T02:26:35Z","2022-06-27T15:48:42Z"
"","12401","Minor: replace .kafka with .log in implementation documentation","This is a minor change required to align the documentation to the current implementation.","closed","docs,","fvaleri","2022-07-11T15:48:41Z","2022-07-20T10:44:57Z"
"","12100","KAFKA-13785: [6/N][Emit final] Copy: Emit final for TimeWindowedKStreamImpl","This is a copy PR of https://github.com/apache/kafka/pull/11896, authored by @lihaosky   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-04-27T05:14:21Z","2022-05-03T16:42:24Z"
"","12135","KAFKA-13785: [7/N][Emit final] emit final for sliding window","This is a copy PR of #12037.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-05-07T15:23:27Z","2022-05-14T02:29:08Z"
"","12127","KAFKA-13785: [8/N][emit final] time-ordered session store","This is a copy PR of #11917. The major diffs are:  1) Avoid extra byte array allocation for fixed upper/lower range serialization. 2) Rename some class names to be more consistent.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-05-05T16:59:23Z","2022-05-06T02:02:22Z"
"","12259","HOTFIX: only try to clear discover-coordinator future upon commit","This is a cherrypick commit of 3.1. Another way of fixing [KAFKA-13563](https://issues.apache.org/jira/browse/KAFKA-13563) other than https://github.com/apache/kafka/pull/11631.   Instead of letting the consumer to always try to discover coordinator in pool with either mode (subscribe / assign), we defer the clearance of discover future upon committing async only. More specifically, under manual assign mode, there are only three places where we need the coordinator:  * commitAsync (both by the consumer itself or triggered by caller), this is where we want to fix. * commitSync, which we already try to re-discovery coordinator. * committed (both by the consumer itself based on reset policy, or triggered by caller), which we already try to re-discovery coordinator.  The benefits are that for manual assign mode that does not try to trigger any of the above three, then we never would be discovering coordinator. The original fix in https://github.com/apache/kafka/pull/11631 would let the consumer to discover coordinator even if none of the above operations are required.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-06T21:44:51Z","2022-06-07T14:27:03Z"
"","12046","KAFKA-10360: Allow disabling JMX Reporter (KIP-830)","This implements KIP-830: https://cwiki.apache.org/confluence/display/KAFKA/KIP-830%3A+Allow+disabling+JMX+Reporter It adds a new configuration jmx.reporter.enable that can be set to false to disable the JMX Reporter   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","mimaison","2022-04-13T16:00:14Z","2022-06-14T11:19:31Z"
"","12406","MINOR: Fix options for old-style Admin.listConsumerGroupOffsets","This fixes the options set in commit https://github.com/apache/kafka/commit/beac86f049385932309158c1cb49c8657e53f45f for the old style API. Timeout was not being copied to the new options. The copy is error-prone, so changing to use the provided options directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2022-07-14T16:05:49Z","2022-07-15T08:21:35Z"
"","11654","KAFKA-4090: Validate SSL connection in client","This fix is a copy-paste from https://github.com/apache/kafka/pull/8066","closed","","gurinderu","2022-01-06T11:50:04Z","2022-01-06T12:15:01Z"
"","12298","KAFKA-13998: JoinGroupRequestData 'reason' can be too large","This fix follows the pattern which is established in `AbstractCoordinator.java` of setting the request reason with the method `requestRejoin`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jnh5y","2022-06-15T21:34:01Z","2022-06-30T15:55:01Z"
"","12402","remove min_cluster_size() calls","This ducktape PR - https://github.com/confluentinc/ducktape/pull/336 - deprecates min_cluster_spec() method which was the only method in ducktape framework which was actually calling min_cluster_size() method (which, in turn, has been deprecated for a while).  This PR removes all usages of min_cluster_size() since it won't be used for anything useful in ducktape in the future. If we want to be on the safe side, we should only merge this one after we release new ducktape version with that PR, though it's probably ok either way.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","stan-confluent","2022-07-13T05:34:36Z","2022-07-13T05:34:36Z"
"","12420","KAFKA-13769 Fix version check in SubscriptionJoinForeignProcessorSupplier","This commit changes the version check from` !=` to `>` as the `process` method works correctly on both version 1 and 2. `!=` incorrectly throws on v1 records.  I don't think that we need additional tests for this change: 1. We already test that the v0 record get `primaryPartition` is equal to `null` in [an existing unit test](https://github.com/apache/kafka/blob/edad31811c49856cc1b8e76de6e3f3a6c02802cd/streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerdeTest.java#L132-L133). 2. If `primaryPartition` is equal to `null`, then [the RecordCollector will act as if no partition was specified](https://github.com/apache/kafka/blob/edad31811c49856cc1b8e76de6e3f3a6c02802cd/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java#L135-L160), so the way it used to work in v0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Gerrrr","2022-07-18T16:49:38Z","2022-07-18T21:12:42Z"
"","12206","KAFKA-13888: Addition of Information in DescribeQuorumResponse about Voter Lag","This commit adds an Admin API handler for DescribeQuorum Request and also adds in two new fields LastFetchTimestamp and LastCaughtUpTimestamp to the DescribeQuorumResponse as described by KIP-836.  This commit does not implement the newly added fields. Those will be added in a subsequent commit.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","niket-goel","2022-05-24T21:44:54Z","2022-06-15T16:20:15Z"
"","12457","KAFKA-14104: Add CRC validation when iterating over Metadata Log Records","This commit adds a check to ensure the RecordBatch CRC is valid when iterating over a Batch of Records using the RecordsIterator. The RecordsIterator is used by both Snapshot reads and Log Records reads in Kraft. The check can be turned off by a class parameter and is on by default.","open","","niket-goel","2022-07-28T22:59:38Z","2022-08-03T21:45:51Z"
"","11678","Expose MetadataCache through testkit framework","This cleans up a few things in ClusterInstance to make it more usable. Also fixes some old ""raft"" references.  I also adjusted the parameter injection that ClusterTestExtensions is doing to allow test classes to directly inject the specific type. This allows for easier implementation specific tests. For example, getting direct access to a MetadataImage when running a KRaft cluster:  ```java    @ClusterTest(clusterType = Type.KRAFT, brokers = 1, controllers = 1)     public void testKRaftMetadataImage(KRaftClusterInstance instance) {         MetadataImage image = instance.brokerCurrentMetadataImages().get(0);         assertTrue(image.cluster().brokers().containsKey(0));     } ```  This should make it easier to write a class of tests where we want running controllers/brokers, but don't want to be restricted to using the APIs to test things.","open","","mumrah","2022-01-13T22:25:29Z","2022-01-13T22:30:58Z"
"","11537","KAFKA-13477 [WIP]: Add Task ID and Connector Name to Connect Task Context","This change is related to [KIP-803](https://cwiki.apache.org/confluence/x/4pKqCw).  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","sarah-story","2021-11-24T19:17:34Z","2022-04-20T00:36:26Z"
"","12291","KAFKA-13987: Isolate REST request timeout changes in Connect integration tests","This causes the artificial reductions in the Connect REST request timeout to be more isolated. Specifically, they now only take place in the tests that need them (instead of any tests that happen to be running after the reduction has taken place and before it has been reset), and they are only performed for the requests that are expected to time out, before being immediately reset. This should help reduce spurious test failures (especially in slow environments like Jenkins) for all Connect integration tests that interact with the REST API, not just the `BlockingConnectorTest` test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2022-06-14T01:18:08Z","2022-06-23T15:02:48Z"
"","12290","MINOR: Stop leaking threads in BlockingConnectorTest","These tests currently create threads that block forever until the JVM is shut down. This change unblocks those threads once their respective test cases are finished.  This is valuable not only for general code hygiene and resource utilization, but also for laying the groundwork for reusing an embedded Connect cluster across each of these test cases, which would drastically reduce test time. That's left for a follow-up PR, though.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-06-14T01:10:33Z","2022-06-28T22:40:20Z"
"","11501","MINOR: Fix FetchSessionBenchmark","There were a few small things I didn't clean up in KAFKA-13111. Clean up here.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-11-15T19:25:06Z","2021-11-16T09:39:36Z"
"","11790","MINOR: Cleanup admin creation logic in integration tests","There seemed to be a little sloppiness in the integration tests in regard to admin client creation. Not only was there duplicated logic, but it wasn't always clear which listener the admin client was targeting. This made it difficult to tell in the context of authorization tests whether we were indeed testing with the right principal. As an example, we had a method in `TestUtils` which was using the inter-broker listener implicitly. This meant that the test was using the broker principal which had super user privilege. This was intentional, but I think it would be clearer to make the dependence on this listener explicit. This patch attempts to clean this up a bit by consolidating some of the admin creation logic and making the reliance on the listener clearer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-18T01:00:00Z","2022-02-24T15:37:29Z"
"","12098","MINOR: Fix event output inconsistencies in TransactionalMessageCopier","There is some strangeness and inconsistency in the messages written by `TransactionalMessageCopier` to stdout. Here is a sample of two messages.  Progress message: ``` {""consumed"":33000,""stage"":""ProcessLoop"",""totalProcessed"":33000,""progress"":""copier-0"",""time"":""2022/04/24 05:40:31:649"",""remaining"":333} ``` The `transactionalId` is set to the value of the `progress` key.  And a shutdown message: ``` {""consumed"":33333,""shutdown_complete"":""copier-0"",""totalProcessed"":33333,""time"":""2022/04/24 05:40:31:937"",""remaining"":0} ``` The `transactionalId` this time is set to the `shutdown_complete` key and there is no `stage` key.  This patch fixes these issues with the following:  1. Use a separate key for the `transactionalId`. 2. Drop the `progress` and `shutdown_complete` fields. 3. Use `stage=ShutdownComplete` in the shutdown message.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-04-26T21:27:22Z","2022-04-29T17:02:26Z"
"","12198","MINOR: Avoid possibly resolvable name in tests","There is a reasonable chance that the name `admin` might actually be resolvable in certain corporate development environments.  *Summary of testing strategy* These two tests started passing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","soarez","2022-05-23T16:55:06Z","2022-06-20T10:52:17Z"
"","11702","MINOR: fix NPE in iqv2","There is a brief window between when the store is registered and when it is initialized when it might handle a query, but there is no context. We treat this condition just like a store that hasn't caught up to the desired position yet.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-01-22T01:45:01Z","2022-01-25T19:28:03Z"
"","12304","KAFKA-13880: Remove DefaultPartitioner from StreamPartitioner","There are some considerata embedded in this seemingly straight-forward PR that I'd like to explain here. The StreamPartitioner is used to send records to three types of topics:  1) repartition topics, where key should never be null. 2) changelog topics, where key should never be null. 3) sink topics, where only non-windowed key could be null and windowed key should still never be null.  Also, the StreamPartitioner is used as part of the IQ to determine which host contains a certain key, as determined by the case 2) above.  This PR's main goal is to remove the deprecated producer's default partitioner, while with those things in mind such that:  1) We want to make sure for not-null keys, the default murmur2 hash behavior of the streams' partitioner stays consistent with producer's new built-in partitioner. 2) For null-keys (which is only possible for non-window default stream partition, and is never used for IQ), we would fix the issue that we may never rotate to a new partitioner by setting the partition as `null` hence relying on the newly introduced built-in partitioner.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2022-06-16T18:53:46Z","2022-06-18T03:17:07Z"
"","11592","KAFKA-13501: Avoid state restore via rebalance if standbys are enabled","There are certain scenario in which Kafka Streams wipes out local state and rebuilt it from scratch. This is a thread local cleanup, ie, no rebalance is triggered, and we end up with an offline task until state restoration finished.  If standby tasks are enable, it might actually make sense to trigger a rebalance instead, to get the task re-assigned to the instance hosting the standby so get the task active again quickly.","open","streams,","vamossagar12","2021-12-10T08:34:45Z","2022-01-19T19:33:41Z"
"","11676","KAFKA-13605: checkpoint position in state stores","There are cases in which a state store neither has an in-memory position built up nor has it gone through the state restoration process. If a store is persistent (i.e., RocksDB), and we stop and restart Streams, we will have neither of those continuity mechanisms available. This ticket is to fill in that gap.","closed","","patrickstuedi","2022-01-13T13:21:17Z","2022-01-27T19:47:19Z"
"","12276","MINOR: Remove ReplicaManagerTest.initializeLogAndTopicId","The workaround is not required with mockito.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-06-09T14:32:18Z","2022-06-14T16:10:19Z"
"","11555","MINOR: Correct usage of ConfigException in file and directory config providers","The two-arg variant is intended to take a property name and value, not an exception message and a cause.  As-is, this leads to confusing log messages like:  ``` org.apache.kafka.common.config.ConfigException: Invalid value java.nio.file.NoSuchFileException: /my/missing/secrets.properties for configuration Could not read properties from file /my/missing/secrets.properties ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-11-30T16:29:44Z","2022-02-27T03:53:55Z"
"","11968","add toString method","The toString method is missing.","closed","","chenzhongyu11","2022-03-29T15:57:07Z","2022-03-31T14:26:13Z"
"","11487","KAFKA-13445: Add ECDSA test for JWT validation","The tests for OAuth JWT validation all assume usage of RSA, but we need to have ECDSA support there too.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-11-10T23:21:05Z","2021-11-16T00:20:27Z"
"","11848","MINOR: Restructure ConsistencyVectorIntegrationTest","The test experienced a flaky failure, but there were too many unknowns to determine the cause. This change restructures the test to make the cause more clear.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-03-05T03:14:33Z","2022-03-08T20:00:02Z"
"","11996","MINOR: Fix flaky testIdleConnection() test","The test expects that the connection becomes idle before the mock time is moved forward, but the processor thread runs concurrently and may run some activity on the connection after the mock time is moved forward, thus the connection never expires.  The solution is to wait until the message is received on the socket, and only then wait until the connection is unmuted (it's not enough to wait for unmuted without waiting for message being received on the socket, because the channel might have not been muted yet).","closed","","anatasiavela","2022-04-05T00:32:39Z","2022-04-06T14:35:26Z"
"","12185","MINOR: Fix buildResponseSend test cases for envelope responses","The test cases we have in `RequestChannelTest` for `buildResponseSend` construct the envelope request incorrectly. Basically they confuse the envelope context and the reference to the wrapped envelope request object. This patch fixes `TestUtils.buildEnvelopeRequest` so that the wrapped request is built properly. It also fixes the dependence on this incorrect construction and consolidates the tests in `RequestChannelTest` to avoid duplication.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-19T20:48:56Z","2022-05-30T18:34:36Z"
"","12137","MINOR: Consolidate StreamsException and TaskCorruptedException","The TaskCorruptedException extends StreamsException and both exceptions have the capability to store the task IDs that caused the exception. The only difference is that StreamsException only stores one single exception whereas TaskCorruptedException stores multiple exception.  This PR proposed to consolidate the two exception by storing multiple task IDs in StreamsException.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-05-09T08:32:04Z","2022-05-24T12:02:33Z"
"","12027","MINOR: Fix TestDowngrade.test_upgrade_and_downgrade","The second validation does not verify the second bounce because the verified producer and the verified consumer are stopped in `self.run_validation`. This means that the second `run_validation` just spit out the same information as the first one. It seems to me that we should just run the validation at the end.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-11T07:15:53Z","2022-04-18T21:22:33Z"
"","11908","KAFKA-13748: Do not include file stream connectors in Connect's CLASSPATH and plugin.path by default","The purpose of this patch is to stop including the non-production grade connectors that are meant to be used for demos and quick starts by default in the CLASSPATH and plugin.path of Connect deployments. The package of these connector still be shipped with the Apache Kafka distribution and will be available for explicit inclusion.   The changes have been tested through the system tests and the existing unit and integration tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2022-03-16T19:11:28Z","2022-03-30T23:23:01Z"
"","11761","MINOR: Add autoupdate of gradle checksum together with gradle version update","The problem that during gradle version update `./gradlew wrapper --gradle-version ` it removes checksums This PR adds autoupdate of gradle's checksums in `gradle-wrapper.properties` The idea was taken from OpenSearch https://github.com/opensearch-project/OpenSearch/blob/27ed6fc82c7db7a3a741499f0dbd7722fa053f9d/build.gradle#L451  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","snuyanzin","2022-02-15T10:48:41Z","2022-02-15T10:48:41Z"
"","11975","MINOR: Improve the log output information of SocketServer","The printed object changes from `endpoint` to `endpoint.listenerName`, so the prompt information of the log should be kept uniform","closed","","RivenSun2","2022-03-31T07:57:23Z","2022-04-04T13:49:27Z"
"","11669","MINOR: Replace if/else with match in KafkaZkClient#getPartitionAssignmentForTopics","The PR apache#4196 (commit f300480f) replaced `if/else` with `case-match` in `KafkaZkClient`. The method`getPartitionAssignmentForTopics` seems to be missed in the patch, while similar ones like `getFullReplicaAssignmentForTopics`, `getReplicaAssignmentAndTopicIdForTopics`, etc. adopted the `case-match` style.  This PR changes it to make the code style more consistent.","closed","","lmr3796","2022-01-12T07:12:45Z","2022-02-03T09:13:51Z"
"","11489","KAFKA-13446: Remove JWT access token from logs","The OAuth code logs the access token on both the client and the server, potentially exposing service account details. Remove all logging entries to prevent this from leaking.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-11-10T23:36:59Z","2021-11-15T09:10:06Z"
"","12341","HOTFIX: Fix NPE in StreamTask#shouldCheckpointState","The mocks were not setup correctly in StreamTask#shouldCheckpointState which caused a null pointer exception during test execution.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-06-24T09:24:07Z","2022-06-24T10:19:23Z"
"","11755","MINOR: Fix sensor removal assertion in `MetricTest.testRemoveInactiveMetrics`","The MetricTest  testRemoveInactiveMetrics()  should  have tested to remove inactive metrics， but it was not tested the sensor s2 when time passed，the code should be modified to test sensor s2  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","keashem","2022-02-14T12:10:32Z","2022-02-16T02:36:13Z"
"","11795","MINOR: Optimize StorageTool#formatCommand, remove return result","The method StorageTool#formatCommand will always return 0 if execute success, so it might be better to remove this return result.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ruanwenjun","2022-02-22T12:11:12Z","2022-02-22T12:12:45Z"
"","11693","MINOR: Convert LogLoader into a class","The logic for log loading is encapsulated in `LogLoader`. Currently all the methods are static and we pass the parameters through a separate object `LogLoaderParams`. It seems simpler to turn `LogLoader` into a normal object and get rid of `LogLoaderParams`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-01-19T19:55:21Z","2022-01-26T20:54:08Z"
"","11593","KAFKA-13528: KRaft RegisterBroker should validate that the cluster ID matches","The KRaft controller should validate that the clusterID matches before allowing a broker to register in  the cluster.","closed","kip-500,","cmccabe","2021-12-10T18:50:24Z","2022-01-06T18:28:40Z"
"","11511","MINOR: Brokers in KRaft don't need controller listener","The KRaft brokers should not list the names in `controller.listener.names` in `listeners` because brokers do not bind to those endpoints. This commit also removes the extra changes to the security protocol map because the `PLAINTEXT` protocol doesn't require additional configuration.  To fully support all of the security protocol configuration additional changes to `QuorumTestHarness` are needed. Those changes can be made when migrating integration tests that need this functionality.  ### committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-11-17T21:31:49Z","2021-11-19T21:03:28Z"
"","11759","MINOR: improve JavaDocs for ReadOnlySessionStore","The JavaDocs explaining the `findSession` search interval is correct, but hard to understand. We should be more explicit how it works.","closed","streams,","mjsax","2022-02-14T23:41:52Z","2022-02-16T16:40:50Z"
"","11744","MINOR: Fix JavaDoc of OffsetIndex#append","The Java doc for the thrown exception is added in (apache#4975) https://github.com/apache/kafka/pull/4975/files#diff-f3714fa2bb7e07c857d2cafde9dcb5d310fafd2cceed9f4124cc6342671a2c89R137  ~By the time it was already a typo.~   It was documenting the error thrown from `relativeOffset`, but the Exception from line ```scala         throw new InvalidOffsetException(s""Attempt to append an offset ($offset) to position $entries no larger than"" +           s"" the last offset appended (${_lastOffset}) to ${file.getAbsolutePath}."") ``` is not in the Java doc, which can be confusing.","closed","","lmr3796","2022-02-09T12:51:29Z","2022-06-08T22:58:23Z"
"","11666","KAFKA-13591; Fix flaky test `ControllerIntegrationTest.testTopicIdCreatedOnUpgrade`","The issue is that when `zkClient.getTopicIdsForTopics(Set(tp.topic)).get(tp.topic)` is called after the new controller is brought up, there is not guarantee that the controller has already written the topic id to the topic znode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-11T15:48:21Z","2022-01-21T16:38:44Z"
"","12343","MINOR: Update unit/integration tests to work with the IBM Semeru JDK","The IBM Semeru JDK use the OpenJDK security providers instead of the IBM security providers so test for the OpenJDK classes first where possible and test for Semeru in the java.runtime.name system property otherwise.  Use a real but empty KafkaThread object in KafkaProducerTest instead of a mock because the Semeru Thread class implementation has some extra checks that are called and fail when the mock is used.  Tested with IBM and Temurin JDKs. Added a unit test for the new Java.isIbmSemeru() method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jonathan-albrecht-ibm","2022-06-24T17:23:21Z","2022-07-22T16:32:23Z"
"","12047","MINOR:Metric clear when ReplicaManager shutdown","The following metrics also need to be closed when replicaManager shutdown ``` ""IsrExpandsPerSec"" ""IsrShrinksPerSec"" ""FailedIsrUpdatesPerSec"" ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-04-14T01:05:46Z","2022-04-15T08:44:05Z"
"","12063","KAFKA-13835: Fix two bugs related to dynamic broker configs in KRaft","The first bug is that we are calling reloadUpdatedFilesWithoutConfigChange when a topic configuration is changed, but not when a broker configuration is changed. This is backwards. This function must be called only for BROKER configs, and never for TOPIC configs. (Also, this function is called only for specific broker configs, not for cluster configs.)  The second bug is that there were several configurations such as `max.connections` which are related to broker listeners, but which do not involve creating or removing new listeners. We should support these configurations in KRaft. Only adding or removing listeners is unsupported, not changes to existing listeners. This PR fixes the configuration change validation to support this. (This also removes a very puzzling error mesage stating that we are adding or removing listeners, when we are not).","closed","","cmccabe","2022-04-18T21:57:56Z","2022-04-19T20:17:20Z"
"","12374","KAFKA-14039 Fix AlterConfigPolicy usage in KRaft","The existing AlterConfigPolicy behavior in ZK clusters is to only validate configs that have been included into the IncrementalAlterConfigs request. While this could be considered a bug, we should make KRaft have the same behavior as ZK with regards to pluggable interfaces like this.  This patch changes the behavior of ConfigurationControlManager to only pass configs to the AlterConfigPolicy that were included in the request. The full set of configs is still passed through the ConfigurationValidator defined by the controller, but only altered configs are passed down to the AlterConfigPolicy.","closed","","mumrah","2022-07-01T18:15:20Z","2022-07-15T19:48:36Z"
"","11758","MINOR: Clarify logging behavior with errors.log.include.messages property","The docs are a little misleading and some users can be confused about the exact behavior of this property.  References: - [LogReporter::report and LogReporter::message](https://github.com/apache/kafka/blob/b5b590cb6711617cbb3c978de9f8bb0903291efa/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java#L51-L75) - [ConnectorConfig::includeRecordDetailsInErrorLog](https://github.com/apache/kafka/blob/b5b590cb6711617cbb3c978de9f8bb0903291efa/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java#L264-L266) - [ProcessingContext::toString](https://github.com/apache/kafka/blob/b5b590cb6711617cbb3c978de9f8bb0903291efa/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java#L164-L188) - [SourceRecord::toString](https://github.com/apache/kafka/blob/b5b590cb6711617cbb3c978de9f8bb0903291efa/connect/api/src/main/java/org/apache/kafka/connect/source/SourceRecord.java#L129-L135) - [ConnectRecord::toString](https://github.com/apache/kafka/blob/b5b590cb6711617cbb3c978de9f8bb0903291efa/connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java#L137-L149)","closed","","C0urante","2022-02-14T20:45:59Z","2022-02-22T23:10:49Z"
"","11965","KAFKA-13778: Fetch from follower should never run the preferred read replica selection","The design purpose of the code is that only the leader broker can determine the preferred read-replica.   ```  // If we are the leader, determine the preferred read-replica         val preferredReadReplica = clientMetadata.flatMap(           metadata => findPreferredReadReplica(partition, metadata, replicaId, fetchInfo.fetchOffset, fetchTimeMs)) ```  But in fact, since the broker does not judge whether it is the leader or not, the follower will also execute the preferred read-replica selection.  ```  partition.leaderReplicaIdOpt.flatMap { leaderReplicaId =>       // Don't look up preferred for follower fetches via normal replication        if (Request.isValidBrokerId(replicaId))         None       else {             ...... ```","closed","","bozhao12","2022-03-29T14:08:19Z","2022-04-06T06:29:03Z"
"","12049","KAFKA-10888: Sticky partition leads to uneven produce msg","The design is described in detail in KIP-794 https://cwiki.apache.org/confluence/display/KAFKA/KIP-794%3A+Strictly+Uniform+Sticky+Partitioner.  Implementation notes:  The default partitioning logic is moved to the BuiltInPartitioner class (there is one object per topic).  The object keeps track of how many bytes are produced per-partition and once the amount exceeds batch.size, switches to the next partition (note that partition switch decision is decoupled from batching).  The object also keeps track of probability weights that are based on the queue sizes (the larger the queue size is the less chance for the next partition to be chosen).  The queue sizes are calculated in the RecordAccumulator in the `ready` method, the method already enumerates all partitions so we just add some extra logic into the existing O(N) method.  The partition switch decision may take O(logN), where N is the number partitions per topic, but it happens only once per batch.size (and the logic is avoided when all queues are of equal size).  Produce bytes accounting logic is lock-free.  When partitioner.availability.timeout.ms is non-0, RecordAccumulator keeps stats on ""node latency"" which is defined as the difference between the last time the node had a batch waiting to be send and the last time the node was ready to take a new batch.  If this difference exceeds partitioner.availability.timeout.ms we don't switch to that partition until the node is ready.  The corresponding unit tests are added / modified.  The perf test results are in the KIP-794.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","artemlivshits","2022-04-14T16:38:08Z","2022-06-30T00:04:12Z"
"","11938","MINOR: Clarify how to publish specific projects to the local repo","The current README instruction for local publishing boils the ocean by building and installing every jar in the project with both 2.12 and 2.13. While that is some times what people want to do, they are also often trying to just build a specific jar.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2022-03-23T14:51:57Z","2022-03-23T21:42:18Z"
"","11729","MINOR: fix control plane listener + kraft error message","The current error message suggests that controller.listener.names is a replacement for control.plane.listener.name. This is incorrect since these configurations have very different functions. This PR deletes the incorrect message.","closed","kip-500,","cmccabe","2022-02-02T19:44:43Z","2022-02-03T18:08:04Z"
"","12379","KAFKA-10199: Remove call to Task#completeRestoration from state updater","The call to Task#completeRestoration calls methods on the main consumer. The state updater thread should not access the main consumer since the main consumer is not thread-safe. Additionally, Task#completeRestoration changed the state of active tasks, but we decided to keep task life cycle management outside of the state updater.  Task#completeRestoration should be called by the stream thread on restored active tasks returned by the state udpater.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-05T11:32:47Z","2022-07-06T10:36:16Z"
"","11926","KAFKA-13714: Fix cache flush position","The caching store layers were passing down writes into lower store layers upon eviction, but not setting the context to the evicted records' context. Instead, the context was from whatever unrelated record was being processed at the time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-03-22T05:16:01Z","2022-03-24T03:36:11Z"
"","12064","KAFKA-12841: Remove an additional call of onAcknowledgement","The bug was introduced in https://github.com/apache/kafka/pull/11689/ that an additional onAcknowledgement was made using the InterceptorCallback class.  This is undesirable since onSendError will attempt to call onAcknowledgement once more.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","philipnee","2022-04-18T23:04:14Z","2022-04-27T08:19:32Z"
"","12090","KAFKA-13852: Kafka Acl documentation bug for wildcard '*'","The bug for wildcard '\*' in Kafka Acl documentation(docs/security.html). In this fix, add single quotes for the wildcard '*' in the script.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Hongten","2022-04-23T14:02:55Z","2022-04-24T08:51:00Z"
"","11700","MINOR: RegisterBroker should use an atomic append","The batch of records that registers a new broker should be committed atomically. This doesn't matter right now, because we only create a single record to register the broker. But if we create multiple records in the future, this could matter. In order to avoid confusion, this should use ControllerResult#atomicOf.","closed","kip-500,","cmccabe","2022-01-21T16:33:08Z","2022-01-21T18:09:14Z"
"","12410","MINOR: Remove unused ShutdownableThread class and ineffective ThreadedTest classes","The `ShutdownableThread` class isn't used anywhere outside of tests, and the `ThreadedTest` class only works in cases where the logic that's being tested uses a `ShutdownableThread`. Both of these classes are removed, and the `DistributedHerderTest` class is updated to properly report unexpected exceptions that take place on other threads (which appears to be the original purpose of the `ThreadedTest` class, although it was not actually doing this anywhere).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-07-14T23:57:13Z","2022-07-26T01:56:51Z"
"","11484","KAFKA-13443: Kafka broker exits when OAuth enabled and certain configuration not specified","The `sasl.oauthbearer.jwks.endpoint.retry.backoff.ms` and `sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms` configuration options were added to the `SaslConfig` class but their default values were not added to `KafkaConfig`. As a result, when the OAuth validation feature is enabled in the broker and those two configuration values aren't explicitly provided by the user, the broker exits.  The fix is to define them in the `KafkaConfig` class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-11-10T19:13:19Z","2021-11-17T10:18:40Z"
"","11946","KAFKA-13770: Restore compatibility with KafkaBasedLog using older Kafka brokers","The `retryEndOffsets(…)` method in `TopicAdmin` recently added (KAFKA-12879, #11797) to allow the `KafkaBasedLog.start()` method to retry any failures reading the last offsets for a topic. However, this introduce a regression when talking to older brokers (0.10.x or earlier).  The `KafkaBasedLog` already had logic that expected an `UnsupportedVersionException` thrown by the admin client when a Kafka API is not available on an older broker, but the new retry logic in `TopicAdmin` did not account for this and wrapped the exception, thereby breaking the `KafkaBasedLog` logic and preventing startup.  The fix is to propagate this `UnsupportedVersionException` from the `TopicAdmin.retryEndOffsets(…)` method. Added a new unit test that first replicated the problem before the fix, and verified the fix corrects the problem.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2022-03-24T21:52:55Z","2022-03-25T02:40:10Z"
"","11486","KAFKA-13444: Fix OAuthCompatibilityTool help and add SSL options","The `OAuthCompatibilityTool` is missing the SSL configuration options. In addition, the help text is incorrect.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-11-10T23:04:59Z","2021-11-16T21:33:04Z"
"","11815","MINOR: Eliminate FileRecords mutable flag","The `mutable` flag in `FileRecords` doesn't appear to be necessary. I've removed it in this PR to help simplify the code a bit.  **Tests:** I'm relying on existing tests.","closed","","kowshik","2022-02-26T10:18:33Z","2022-03-11T02:24:29Z"
"","11914","MINOR: Correct Connect docs on connector/task states","The `DESTROYED` state is represented internally as a tombstone record when running in distributed mode ([1]) and by the removal of the connector/task from the in-memory status map when running in standalone mode ([2], [3]). As a result, it will never appear to users of the REST API, and we should remove mention of it from our docs so that developers creating tooling against the REST API don't write unnecessary logic to account for that state.  [1] - https://github.com/apache/kafka/blob/3dacdc5694da5db283524889d2270695defebbaa/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaStatusBackingStore.java#L318  [2] - https://github.com/apache/kafka/blob/3dacdc5694da5db283524889d2270695defebbaa/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryStatusBackingStore.java#L64-L65  [3] - https://github.com/apache/kafka/blob/3dacdc5694da5db283524889d2270695defebbaa/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryStatusBackingStore.java#L77-L78  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2022-03-17T16:53:00Z","2022-04-13T12:40:38Z"
"","12309","KAFKA-14007: Invoking connect headers.close method on shutdown","The [HeaderConverter interface](https://github.com/apache/kafka/blob/1e21201ea24389bdaccb8a462f3a53e356b58a58/connect/api/src/main/java/org/apache/kafka/connect/storage/HeaderConverter.java#L27) extends Closeable, but HeaderConverter::close is never actually invoked anywhere. We can and should start invoking it, probably wrapped in [Utils::closeQuietly](https://github.com/apache/kafka/blob/1e21201ea24389bdaccb8a462f3a53e356b58a58/clients/src/main/java/org/apache/kafka/common/utils/Utils.java#L999-L1010) so that any invalid logic in that method for custom header converters that has to date gone undetected will not cause new task failures.  This PR addresses the above concern by invoking HeaderConverter.close in cases of shutdowns.","closed","connect,","vamossagar12","2022-06-19T10:42:28Z","2022-07-29T05:47:19Z"
"","11872","KAFKA-12879: Remove extra sleep","Tests didn't catch the extra sleep kept in #11871 after optimization  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2022-03-09T21:11:30Z","2022-03-09T21:11:46Z"
"","11752","KAFKA-13666 Don't Only ignore test exceptions for windows OS for certain tests.","Tests are swallowing exceptions for supported operating systems , which could hide regressions. Update test driver to check for operating system and ignore and log exception only for windows.","closed","streams,","free2create","2022-02-12T22:29:13Z","2022-02-18T13:49:03Z"
"","11643","Fix flakiness in SaslClientsWithInvalidCredentialsTest#testProducerWithAuthenticationFailure","Test SaslClientsWithInvalidCredentialsTest#testProducerWithAuthenticationFailure constantly fails in both 2.11 and 2.12 build with exceeding the timeout limit of 5000ms when expecting certain exceptions, one example run is https://github.com/linkedin/kafka/runs/4695386759?check_suite_focus=true. But there're places where the timeout is defined as 10000ms or 60000ms by default in partitionsFor call, local run of this test also constanly fails with a 30s timeout. Thus increasing the timeout check to default maxBlockMs to avoid flaky test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kehuum","2022-01-03T22:51:36Z","2022-01-03T22:51:57Z"
"","11949","KAFKA-4801: don't verify assignment during broker up and down","Test failed with  ``` org.opentest4j.AssertionFailedError: expected:  but was:  ``` In this test, we have another thread to let broker down and up, to test if consumer can still work as expected. During the broker down and up, we tried to verify the assignment is as what we expected. But the rebalance will keep triggering while broker down and up. It doesn't make sense to verify the assignment here. Remove it to make the test reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-25T08:52:02Z","2022-03-25T17:16:50Z"
"","11976","KAFKA-13771: Support to explicitly delete delegationTokens that have expired but have not been automatically cleaned up","Support to explicitly delete delegationTokens that have expired but have not been automatically cleaned up","open","","RivenSun2","2022-03-31T08:32:35Z","2022-05-30T03:11:52Z"
"","11985","MINOR: Supplement the description of `Valid Values` in the documentation of `compression.type`","Supplement the description of `Valid Values` in the documentation of `compression.type`","closed","","RivenSun2","2022-04-02T05:16:13Z","2022-04-13T04:28:38Z"
"","11505","KAFKA-13449: Comment optimization for parameter log.cleaner.delete.retention.ms","Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13449  Author: RivenSun2 riven.sun@zoom.us  Reviewers: Luke Chen showuon@gmail.com","closed","","RivenSun2","2021-11-16T01:56:52Z","2021-11-16T13:08:40Z"
"","12311","MINOR: Remove extra commas in upgrade steps documentation","Starting from version 2.5.0, in the upgrade steps document, there is an extra comma  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-06-20T08:14:56Z","2022-06-20T08:57:50Z"
"","12223","MINOR: Correctly mark some tests as integration tests","Some integration tests are not correctly marked to run as integration tests and are run as unit tests.","closed","","divijvaidya","2022-05-28T17:39:29Z","2022-06-09T14:02:01Z"
"","11846","(docs) Add JavaDocs for org.apache.kafka.common.security.oauthbearer.secured","Single-line change to build.gradle to render javadocs for new org.apache.kafka.common.security.oauthbearer.secured package (part of [KIP-768](https://issues.apache.org/jira/browse/KAFKA-13202))  Cherry-pick of https://github.com/apache/kafka/pull/11811 for 3.1  cc @junrao","closed","","justinrlee","2022-03-04T14:45:49Z","2022-03-04T17:11:53Z"
"","11811","(docs) Add JavaDocs for org.apache.kafka.common.security.oauthbearer.secured","Single-line change to `build.gradle` to render javadocs for new `org.apache.kafka.common.security.oauthbearer.secured` package (part of [KIP-768](https://issues.apache.org/jira/browse/KAFKA-13202))  cc @junrao","closed","","justinrlee","2022-02-26T04:10:42Z","2022-03-09T22:34:19Z"
"","11901","KAFKA-13741 Don't generate Uuid with a leading ""-""","Since we use URL-safe base64 encoded Uuid's for cluster ID, it is possible for dash (""-"") characters to be present in the ID string. This causes problems with the argument parsing for things like `kafka-storage.sh format`.  This patch regenerates a Uuid if it has a leading `-`.  This issue is not a problem for generated topic IDs since our command line tools use topic name. However, with this patch, topic IDs (and any other internal Uuid we generate) will not have leading ""-"".","closed","kip-500,","mumrah","2022-03-15T15:17:41Z","2022-03-16T15:54:04Z"
"","12089","MINOR: Rename `AlterIsrManager` to `AlterPartitionManager`","Since we have changed the `AlterIsr` API to `AlterPartition`, it makes sense to rename `AlterIsrManager` as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-04-22T22:38:34Z","2022-04-26T16:34:19Z"
"","11959","MINOR: log warning when topology override for cache size is non-zero","Since the topology-level cache size config only controls whether we disable the caching layer entirely for that topology, setting it to anything other than `0` has no effect. The actual cache memory is still just split evenly between the threads, and shared by all topologies.  It's possible we'll want to change this in the future, but for now we should make sure to log a warning so that users who do try to set this override to some nonzero value are made aware that it doesn't work like this.  Also includes [some minor refactoring](https://github.com/apache/kafka/pull/11796#pullrequestreview-924047442) plus an [off-by-one fix](https://github.com/apache/kafka/pull/11796#discussion_r837073268) from https://github.com/apache/kafka/pull/11796","closed","","ableegoldman","2022-03-29T06:28:47Z","2022-03-31T02:41:01Z"
"","11969","KAFKA-13649: Implement early.start.listeners and fix StandardAuthorizer loading","Since the StandardAuthorizer relies on the metadata log to store its ACLs, we need to be sure that we have the latest metadata before allowing the authorizer to be used. However, if the authorizer is not usable for controllers in the cluster, the latest metadata cannot be fetched, because inter-node communication cannot occur. In the initial commit which introduced StandardAuthorizer, we punted on the loading issue by allowing the authorizer to be used immediately. This commit fixes that by implementing early.start.listeners as specified in KIP-801. This will allow in superusers immediately, but throw the new AuthorizerNotReadyException if non-superusers try to use the authorizer before StandardAuthorizer#completeInitialLoad is called.  For the broker, we call StandardAuthorizer#completeInitialLoad immediately after metadata catch-up is complete, right before unfencing. For the controller, we call StandardAuthorizer#completeInitialLoad when the node has caught up to the high water mark of the cluster metadata partition.  This PR refactors the SocketServer so that it creates the configured acceptors and processors in its constructor, rather than requiring a call to SocketServer#startup A new function, SocketServer#enableRequestProcessing, then starts the threads and begins listening on the configured ports. enableRequestProcessing uses an async model: we will start the acceptor and processors associated with an endpoint as soon as that endpoint's authorizer future is completed.  Also fix a bug where the controller and listener were sharing an Authorizer when in co-located mode, which was not intended.","closed","","cmccabe","2022-03-29T20:52:55Z","2022-05-12T21:48:36Z"
"","12221","KAFKA-13941: Reenable ARM in Jenkinsfile","Since https://issues.apache.org/jira/browse/INFRA-23305 appears to have been resolved, we should be able to reenable the ARM build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-27T19:40:46Z","2022-05-27T20:22:08Z"
"","11584","MINOR: improve logging","Simply reading log by sorting partitions.","closed","consumer,","mjsax","2021-12-09T06:10:00Z","2022-02-11T18:50:47Z"
"","12477","[MINOR] - update scala version in bin scripts (2.13.8)","Signed-off-by: morsak   This PR updates scala versions inside `bin/scripts` (i.e., `kafka-run-class.sh`).  https://github.com/apache/kafka/pull/12273 fix modified only inside `gradle.properties` but not inside the scripts.","open","","see-quick","2022-08-03T12:52:01Z","2022-08-03T12:56:02Z"
"","11764","Install missing 'tc' utility - iproute2 for systemtests","Signed-off-by: Michal T   Some system tests are failing on missing `tc` traffic control utility, which is provided on debian in iproute2 package. This fixes issues with failing tests  ``` File ""/opt/kafka-dev/tests/kafkatest/services/trogdor/trogdor.py"", line 325, in done 21:07:43      raise RuntimeError(""Failed to gracefully stop %s: got task error: %s"" % (self.id, error)) 21:07:43  RuntimeError: Failed to gracefully stop rate-1000: got task error: worker.start() exception: org.apache.kafka.common.utils.Shell$ExitCodeException: sudo: tc: command not found ```  Some tests from  ``` kafkatest.tests.core.round_trip_fault_test.RoundTripFaultTest.test_produce_consume_with_latency kafkatest.tests.core.network_degrade_test.NetworkDegradeTest.test_rate ```","closed","","michalxo","2022-02-15T12:35:44Z","2022-02-16T11:56:06Z"
"","11740","Dockerfile typo - wrong used version","Signed-off-by: Michal T   Simple typo in version in Dockerfile","closed","","michalxo","2022-02-08T14:14:38Z","2022-02-08T17:03:58Z"
"","11769","Use default non-zero value for retries in producer","Signed-off-by: Michal T   Recent changes to idempotent client uncovered issue with Verifiable producer using 0 retries instead of default int_max. This was failing multiple system tests and should fix the issue.  PerformanceProducer is also affected.","closed","","michalxo","2022-02-16T06:44:54Z","2022-02-23T10:31:19Z"
"","11500","KAFKA-13455: Add steps to run Kafka Connect to quickstart","Signed-off-by: Katherine Stanley","closed","connect,","katheris","2021-11-15T17:06:34Z","2021-11-22T13:43:56Z"
"","12391","[DO NOT MERGE] KAFKA-10199: Add task updater metrics","Should only be reviewed after https://github.com/apache/kafka/pull/12387.  Needs discussion on KIPs. And also we should only merge this PR after the whole integration is done.  I propose adding the following metrics to state updater:  * restoring-active-tasks: count * restoring-standby-tasks: count * paused-active-tasks: count * paused-standby-tasks: count * idle-ratio: percentage * restore-ratio: percentage * checkpoint-ratio: percentage  // idle-ratio + restore-ratio + checkpoint-ratio should == 1 * restore-records-total: count * restore-records-rate: rate * restore-call-rate: rate  And also deprecate the following metrics in stream thread: * poll-ratio: percentage * standby-process-ratio: percentage  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2022-07-08T01:20:04Z","2022-07-20T19:14:21Z"
"","11680","MINOR: remove version check when setting reason for Join/LeaveGroupRequest","set reason field for all api versions in `JOIN_GROUP` and `LEAVE_GROUP` to test cases where newer clients speak with older brokers.  confirmed that removing `""ignorable: true""` from the reason field fails the updated test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2022-01-14T16:30:01Z","2022-01-17T09:20:54Z"
"","11852","MINOR: Set interrupted flag","Set interrupted flag after catch InterruptedException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-03-05T08:38:24Z","2022-03-05T11:54:24Z"
"","11824","KAFKA-12648: standardize startup timeout to fix some flaky NamedTopologyIntegrationTest tests","Seen a few of the new tests added [fail on PR builds](https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-11812/3/testReport/org.apache.kafka.streams.integration/NamedTopologyIntegrationTest/Build___JDK_11_and_Scala_2_13___shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing/) lately with the classic  ``` java.lang.AssertionError: Expected all streams instances in [org.apache.kafka.streams.processor.internals.namedtopology.KafkaStreamsNamedTopologyWrapper@7fb3e6b0] to be RUNNING within 30000 ms, but the following were not: {org.apache.kafka.streams.processor.internals.namedtopology.KafkaStreamsNamedTopologyWrapper@7fb3e6b0=ERROR} ```  Since Jenkins unfortunately truncates the logs before we even get to running this specific test (due to large kafka & Config logging, see #11823) so I can't say for sure that it's environmental vs an actual issue, but bumping up the timeout will help us rule this out if we continue to see failures. We already had some tests using the 30s timeout while others were bumped all the way up to 60s, I figured we should try out a default timeout of 45s and if we still see failures in specific tests we can go from there","closed","","ableegoldman","2022-03-01T06:38:00Z","2022-03-01T21:16:25Z"
"","12358","KAFKA-13988:Fix mm2 auto.offset.reset=latest not working","see https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-13988?filter=allopenissues  I bumped into the same issue in production env, and i worked around in this way (as PR ).  With `topicPartitionOffsets.forEach(consumer::seek);`,  mm2 seeks all topic partitions even though there have been no commits  in mm2-offset topic .For example, if A-0 is a topic parition in source target , and mm2 has never synced it before; then mm2 will seek to 0 for A-0, and sync from 0 .   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Justinwins","2022-06-29T02:02:53Z","2022-06-29T02:03:28Z"
"","12126","KAFKA-8713 KIP-581: Add new conf serialize.accept.optional.null in connect-json","See details on KAFKA-8713 and KIP-581  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","pan3793","2022-05-05T12:57:53Z","2022-07-07T18:23:58Z"
"","11610","KAFKA-13549: Add repartition.purge.interval.ms","See [KIP-811](https://cwiki.apache.org/confluence/display/KAFKA/KIP-811%3A+Add+config+repartition.purge.interval.ms+to+Kafka+Streams)  Repartition records are explicitly deleted once they have been fully consumed. Currently, this is done every time the Task is committed, resulting in ""delete records"" requests being sent every `commit.interval.ms` milliseconds.  When `commit.interval.ms` is set very low, for example when `processing.guarantee` is set to `exactly_once_v2`, this causes delete records requests to be sent extremely frequently, potentially reducing throughput and causing a high volume of log messages to be logged by the brokers.  Disconnecting delete records requests from the commit interval resolves this problem.  We now only explicitly delete records for a repartition topic when we commit, if it's been at least `repartition.purge.interval.ms` milliseconds since the last time we deleted records.  Because we still require a commit to trigger record deletion, the lower-bound of `repartition.purge.interval.ms` is effectively capped at the `commit.interval.ms`.  For compatibility, the default `repartition.purge.interval.ms` is set to 30 seconds, the same as the default `commit.interval.ms`. Users who have configured a different `commit.interval.ms` may need to review and change `repartition.purge.interval.ms`.  Unlike `commit.interval.ms`, we don't dynamically change the default for `repartition.purge.interval.ms` when EOS processing is enabled, as it's important not to flood brokers with the record deletions, and we want a sensible default.  This code is my own work and is licensed to the Apache Kafka project under the terms of the same license (ASL 2) as the project itself.  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [X] Verify documentation (including upgrade notes)","closed","kip,","nicktelford","2021-12-17T15:27:08Z","2022-03-15T22:55:47Z"
"","12351","KAFKA-14023:MirrorCheckpointTask.syncGroupOffset does not have to che…","see  https://issues.apache.org/jira/browse/KAFKA-14023 .   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Justinwins","2022-06-27T02:33:29Z","2022-06-27T15:49:43Z"
"","11690","KAFKA-13599: Upgrade RocksDB to 6.27.3","RocksDB v6.27.3 has been released and it is the first release to support s390x. RocksDB is currently the only dependency in gradle/dependencies.gradle without s390x support.  RocksDB v6.27.3 has added some new options that require an update to streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter.java but no other changes are needed to upgrade.  I have run the unit/integration tests locally on s390x and also the :streams tests on x86_64 and they pass.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jonathan-albrecht-ibm","2022-01-18T19:52:30Z","2022-02-02T14:24:07Z"
"","11967","KAFKA-13776: Upgrade RocksDB from 6.27.3 to 6.29.4.1","RocksDB 6.27.3 does not run on arm64 M1 Macs which would prevent people on this platform to run Kafka Streams. Thus, this PR upgrades RocksDB to 6.29.4.1 which contains the following fix to allow to run RocksDB on arm64 M1 Macs:  https://github.com/facebook/rocksdb/issues/7720  The source compatibility report between 6.27.3 and 6.29.4.1 (attached to the ticket) reports a couple of incompatibilities. However, the incompatibilities do not seem to affect Kafka Streams' backwards compatibility. - The changes to class `RocksDB` only apply when inheriting from RocksDB. RocksDB is not exposed to users in Streams. - The changes to class `WriteBatch` and class `WriteBatchInterface` also only apply with inheritance. Both classes are not exposed to users in Streams. -The change to enum `SanityLevel` seem also not to apply to Streams since `SanityLevel` is only used in `ConfigOptions` which is only used to load options from files and properties objects. Loading options from files or properties is not exposed to users in Streams.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-03-29T14:46:23Z","2022-03-30T12:05:57Z"
"","11932","Revert ""KAFKA-7077: Use default producer settings in Connect Worker""","Reverts apache/kafka#11475  Connect already allows users to enable idempotent producers for connectors and the Connect workers. Although Kafka producers enabled idempotency by default in 3.0, due to compatibility requirements and the fact that [KIP-318](https://cwiki.apache.org/confluence/display/KAFKA/KIP-318%3A+Make+Kafka+Connect+Source+idempotent) hasn't been explicitly approved, the changes here are reverted. A separate commit will explicitly disable idempotency in producers instantiated by Connect by default until KIP-318 is approved and scheduled for release.","closed","","kkonstantine","2022-03-22T22:20:42Z","2022-03-23T01:27:30Z"
"","12111","KAFKA-13865:  Fix ResponseSendTimeMs metric  in RequestChannel.scala  was removed repeatedly","ResponseSendTimeMs metric was removed on line 576， but we removed it again on line 578.","closed","","bozhao12","2022-05-01T00:27:56Z","2022-05-02T09:02:53Z"
"","11577","KAFKA-13515: Fix KRaft config validation issues","Require that topics exist before topic configurations can be created for them.  Merge the code from ConfigurationControlManager#checkConfigResource into ControllerConfigurationValidator to avoid duplication.  Add KRaft support to DynamicConfigChangeTest.  Split out tests in DynamicConfigChangeTest that don't require a cluster into DynamicConfigChangeUnitTest to save test time.","closed","","cmccabe","2021-12-07T23:36:59Z","2021-12-10T20:30:01Z"
"","11546","Fix storage metadata comparison in storage tool","Reproduce current issue:  ```shell $ sed -i 's|log.dirs=/tmp/kraft-combined-logs|+log.dirs=/tmp/kraft-combined-logs,/tmp/kraft-combined-logs2' ./config/kraft/server.properties  $ ./bin/kafka-storage.sh format -t R19xNyxMQvqQRGlkGDi2cg -c ./config/kraft/server.properties Formatting /tmp/kraft-combined-logs Formatting /tmp/kraft-combined-logs2  $ ./bin/kafka-storage.sh info -c ./config/kraft/server.properties Found log directories:   /tmp/kraft-combined-logs   /tmp/kraft-combined-logs2  Found metadata: {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}  Found problem:   Metadata for /tmp/kraft-combined-logs2/meta.properties was {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1}, but other directories featured {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1} ```  It's reporting that same metadata are not the same...  With this fix:  ```shell $ ./bin/kafka-storage.sh info -c ./config/kraft/server.properties Found log directories:   /tmp/kraft-combined-logs   /tmp/kraft-combined-logs2  Found metadata: {cluster.id=R19xNyxMQvqQRGlkGDi2cg, node.id=1, version=1} ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","zigarn","2021-11-28T21:31:03Z","2022-02-10T17:56:16Z"
"","11534","KAFKA-12939: After migrating processors, search the codebase for missed migrations","Replacing for new Processor API where `@SuppressWarnings(""deprecation"") // Old PAPI. Needs to be migrated.`   There are a few places where Processor Context is used and Processors could not be fully migrated and will require other changes on the codebase:  `DeserializationExceptionHandler#handle`:  (not sure if this one requires a KIP to change) ```     @SuppressWarnings(""deprecation"") // Old PAPI. Needs to be migrated.     DeserializationHandlerResponse handle(final ProcessorContext context,                                           final ConsumerRecord record,                                           final Exception exception); ```  `ProcessorParameters.oldProcessorSupplier` and `StatefulProcessorNode`:  ``` public class ProcessorParameters {      // During the transition to KIP-478, we capture arguments passed from the old API to simplify     // the performance of casts that we still need to perform. This will eventually be removed.     @SuppressWarnings(""deprecation"") // Old PAPI. Needs to be migrated.     private final org.apache.kafka.streams.processor.ProcessorSupplier oldProcessorSupplier; ```  ```         // temporary hack until KIP-478 is fully implemented         @SuppressWarnings(""deprecation"") // Old PAPI. Needs to be migrated.         final org.apache.kafka.streams.processor.ProcessorSupplier oldProcessorSupplier =             processorParameters().oldProcessorSupplier();         if (oldProcessorSupplier != null && oldProcessorSupplier.stores() != null) {             for (final StoreBuilder storeBuilder : oldProcessorSupplier.stores()) {                 topologyBuilder.addStateStore(storeBuilder, processorName);             }         } ```  and within tests:  - *TransformTest: include old `ProcessorContext`. This will require a new Transformer API with new PAPI to change. - ProcessorTopologyTest: contains test to validate old PAPI processors. - MockProcessor/MockProcessorTest   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-11-24T14:20:44Z","2022-02-14T09:57:41Z"
"","11723","MINOR: replace statement lambda with expression lambda","Replace statement lambda with expression lambda.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hi-rustin","2022-01-31T08:23:36Z","2022-02-01T10:03:43Z"
"","11721","KAFKA-13629: Use faster algorithm for ByteUtils sizeOfXxx algorithm","Replace loop with a branch-free implementation.  Include: - Unit tests that includes old code and new code and runs through several ints/longs. - JMH benchmark that compares old vs new performance of algorithm.  JMH results with JDK 17.0.2 and `compiler` blackhole mode are 2.8-3.4 faster with the new implementation. In a real application, a 6% reduction in CPU cycles was observed in the `send()` path via flamegraphs.  ``` ByteUtilsBenchmark.testSizeOfUnsignedVarint                            thrpt    4  1472440.102 ±  67331.797  ops/ms ByteUtilsBenchmark.testSizeOfUnsignedVarint:·async                     thrpt               NaN                  --- ByteUtilsBenchmark.testSizeOfUnsignedVarint:·gc.alloc.rate             thrpt    4       ≈ 10⁻⁴               MB/sec ByteUtilsBenchmark.testSizeOfUnsignedVarint:·gc.alloc.rate.norm        thrpt    4       ≈ 10⁻⁷                 B/op ByteUtilsBenchmark.testSizeOfUnsignedVarint:·gc.count                  thrpt    4          ≈ 0               counts ByteUtilsBenchmark.testSizeOfUnsignedVarintSimple                      thrpt    4   521333.117 ± 595169.618  ops/ms ByteUtilsBenchmark.testSizeOfUnsignedVarintSimple:·async               thrpt               NaN                  --- ByteUtilsBenchmark.testSizeOfUnsignedVarintSimple:·gc.alloc.rate       thrpt    4       ≈ 10⁻⁴               MB/sec ByteUtilsBenchmark.testSizeOfUnsignedVarintSimple:·gc.alloc.rate.norm  thrpt    4       ≈ 10⁻⁶                 B/op ByteUtilsBenchmark.testSizeOfUnsignedVarintSimple:·gc.count            thrpt    4          ≈ 0               counts ByteUtilsBenchmark.testSizeOfVarlong                                   thrpt    4  1106519.633 ±  16556.502  ops/ms ByteUtilsBenchmark.testSizeOfVarlong:·async                            thrpt               NaN                  --- ByteUtilsBenchmark.testSizeOfVarlong:·gc.alloc.rate                    thrpt    4       ≈ 10⁻⁴               MB/sec ByteUtilsBenchmark.testSizeOfVarlong:·gc.alloc.rate.norm               thrpt    4       ≈ 10⁻⁶                 B/op ByteUtilsBenchmark.testSizeOfVarlong:·gc.count                         thrpt    4          ≈ 0               counts ByteUtilsBenchmark.testSizeOfVarlongSimple                             thrpt    4   324435.607 ± 147754.813  ops/ms ByteUtilsBenchmark.testSizeOfVarlongSimple:·async                      thrpt               NaN                  --- ByteUtilsBenchmark.testSizeOfVarlongSimple:·gc.alloc.rate              thrpt    4       ≈ 10⁻⁴               MB/sec ByteUtilsBenchmark.testSizeOfVarlongSimple:·gc.alloc.rate.norm         thrpt    4       ≈ 10⁻⁶                 B/op ByteUtilsBenchmark.testSizeOfVarlongSimple:·gc.count                   thrpt    4          ≈ 0               counts ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jasonk000","2022-01-30T19:21:56Z","2022-02-08T05:43:54Z"
"","12247","MINOR: replace ACL_AUTHORIZER attribute with ZK_ACL_AUTHORIZER","Replace ACL_AUTHORIZER attribute with ZK_ACL_AUTHORIZER in tests. Required after the changes merged with https://github.com/apache/kafka/pull/12190  Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","AnGg98","2022-06-03T06:03:57Z","2022-06-03T15:50:50Z"
"","12054","MINOR: update comment in `LocalLog.replaceSegments()`","replace `asyncDeleteSegment` function name to `deleteSegmentFiles` in `LocalLog.replaceSegments()`'s comment  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fxbing","2022-04-15T07:25:34Z","2022-04-15T13:00:12Z"
"","11850","MINOR: Reorder keyword in KafkaException.java","Reorder keyword as familiar Java style in KafkaExcpeition.java  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-03-05T08:07:49Z","2022-03-05T08:11:42Z"
"","11851","MINOR: Reorder keyword in KafkaException.java","Reorder keyword as familiar Java style in KafkaException.java  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-03-05T08:16:26Z","2022-03-06T07:18:58Z"
"","11530","MINOR: Update javadoc of `SnapshotWriter.createWithHeader`","Rename unnecessary comment information：SnapshotWriter.createWithHeader()","closed","","socutes","2021-11-23T08:53:35Z","2021-11-30T02:31:09Z"
"","12389","MINOR: refactor result string","Removes duplicate result strings and avoids mistakes when changing the string format  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pch8388","2022-07-07T10:51:06Z","2022-07-11T08:03:47Z"
"","11771","Build fix: removed unused import in RemoteIndexCacheTest","Removed unused import in RemoteIndexCacheTest. Verified with `./gradlew build` and `./gradlew :core:compileTestScala`.  ### Committer Checklist (excluded from commit message) - [N/A] Verify design and implementation  - [N/A] Verify test coverage and CI build status - [N/A] Verify documentation (including upgrade notes)","closed","","Hangleton","2022-02-16T17:19:43Z","2022-02-16T17:20:07Z"
"","11849","MINOR: Remove unused field in Topic.java","Remove unused field in Topic.java  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-03-05T07:55:21Z","2022-03-06T07:17:56Z"
"","11684","MINOR: remove un used exception","remove un used exception  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","highluck","2022-01-17T16:12:07Z","2022-02-01T15:56:45Z"
"","12231","Minor: Remove execute permission for source file","Remove the executable flag for the source code file, to make it looks clean.  Minor fix.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","theidexisted","2022-05-31T10:48:03Z","2022-06-05T03:33:33Z"
"","11521","MINOR: Polish code in test of streams","Remove some warnings : ) 1. Replace 'System.out.println(String.format(xxx))' by `System.out.printf(""xxx%n"")` 2. Assign generic class for `KStream` 3. Remove the unused lines `@SuppressWarnings(""unchecked"")`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JooKS-me","2021-11-22T06:29:56Z","2021-11-24T02:48:50Z"
"","11692","MINOR: Upgrade jetty-server to 9.4.44.v20210927","Release notes: https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.44.v20210927  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-19T15:50:57Z","2022-01-20T13:22:25Z"
"","11719","MINOR: remove redundant argument in logging","related to #11451  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2022-01-30T05:30:18Z","2022-01-30T15:15:59Z"
"","12377","KAFKA-14008: Add docs for Streams metrics introduced in KIP-846","Related JIRA ticket - https://issues.apache.org/jira/browse/KAFKA-14008 Related KIP - https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=211886093","closed","docs,","clolov","2022-07-05T09:54:50Z","2022-07-14T00:49:53Z"
"","12091","KAFKA-12943: update aggregating documentation","Regarding documentation glitch reported in [KAFKA-12943](https://issues.apache.org/jira/browse/KAFKA-12943), indeed implicit types are not automatically resolved unless explicitly defined in this case.   As confirmed in: https://stackoverflow.com/questions/51040555/the-method-withvalueserde-in-the-type-materialized-is-not-applicable/51049472","closed","docs,","MarcoLotz","2022-04-23T19:27:54Z","2022-07-07T21:00:16Z"
"","12313","MINOR: Add the change statement of reload4j in Notable changes of 3.1.1","Referring to KAFKA-13660, reload4j is a very important change that solves the security problem of log4j. It was declared in Notable changes in 3.2.0, but missing in Notable changes in 3.1.1  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-06-20T08:39:48Z","2022-07-12T14:51:20Z"
"","12072","KAFKA-13854 Refactor ApiVersion to MetadataVersion","Refactoring ApiVersion to MetadataVersion to support both old IBP versioning and new KRaft versioning (feature flags) for KIP-778.   IBP versions are now encoded as enum constants and explicitly prefixed w/ `IBP_` instead of `KAFKA_`, and having a `LegacyApiVersion` vs `DefaultApiVersion` was not necessary and replaced with appropriate parsing rules for extracting the correct shortVersions/versions.","closed","kip-500,","ahuang98","2022-04-20T18:19:16Z","2022-05-02T23:33:28Z"
"","12267","KAFKA-13947 Use %d formatting for integers rather than %s","Refactored from using %s to %d in String.format when trying to print integers. More information available at https://issues.apache.org/jira/browse/KAFKA-13947  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","clolov","2022-06-08T15:35:37Z","2022-06-10T11:55:52Z"
"","12220","MINOR: Disable ARM builds","Recently, the ARM nodes have not been schedulable in our PR builds which leads to the ARM stage hanging for 2 hours until it is timed out.   https://ci-builds.apache.org/job/Kafka/job/kafka-pr/view/change-requests/job/PR-12213/3/  This patch adds a no-op stage with a shorter timeout to check if the agent is available. This will give faster feedback on the state of the ARM build.  Also, this patch disables the ARM build.","closed","","mumrah","2022-05-26T15:10:45Z","2022-05-26T20:09:39Z"
"","12020","KAFKA-13818: Add generation to consumer assignor logs","Reading assignor logs is really confusing in large part because they are spread across different layers of abstraction (the ConsumerCoordinator  and the ConsumerPartitionAssignor, which in Streams consists of several layers of its own). Each layer in the abstraction reports useful information that only it has access to, but because they are split over multiple lines, with multiple members in the cluster, and (often) multiple rebalances taking place in rapid succession, it's often hard to understand which logs are part of which rebalance.  This PR attempts to improve the situation by adding a common prefix to all assignment logs that includes the current generation number. Then, debuggers can pull out all logs for a particular rebalance with a simple `grep ""generationId=5""`. In order to add this useful context without violating encapsulation, the PR simply adds a generic ""dynamic log context"" that assignors can optionally participate in (by mixing in `ContextualLogging`).  I chose to keep these changes restricted to private APIs for the moment. If this approach proves useful, we can propose a KIP at a later date to make the APIs public.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2022-04-08T18:18:01Z","2022-04-30T21:02:52Z"
"","11801","KAFKA-12738: send LeaveGroup request when thread dies to optimize replacement time","Quick followup to #11787 to optimize the impact of the task backoff by reducing the time to replace a thread. I noticed it would take around the `session.timeout` for the new thread to come up and start processing after a thread-fatal error, and realized we weren't sending a LeaveGroup request when a thread hit an exception and died.   Removing the `session.timeout.ms` override in the ErrorHandlingIntegrationTest.shouldBackOffTaskAndEmitDataWithinSameTopology test causes it to revert to the new default value of this config, which is 45s.  Since the current task backoff is a constant 15s, without this change the integration test would fail when using the 45s session timeout. With this fix, we can now remove the config override and verify that the backoff works with the default configuration  This also speeds up the test greatly, it now takes under .5s whereas previously it was taking 45s+","closed","","ableegoldman","2022-02-24T21:59:19Z","2022-02-25T00:18:14Z"
"","11867","KAFKA-12648: fix #getMinThreadVersion and include IOException + topologyName in StreamsException when topology dir cleanup fails","Quick fix to make sure we log the actual source of the failure both in the actual log message as well as the StreamsException that we bubble up to the user's exception handler, and also to report the offending topology by filling in the StreamsException's `taskId` field.  Also prevents a `NoSuchElementException` from being thrown when trying to compute the minimum topology version across all threads when the last thread is being unregistered during shutdown.  This aims to fix some flakiness that has shown up in the NamedTopologyIntegrationTest's `#shouldAddToEmptyInitialTopologyRemoveResetOffsetsThenAddSameNamedTopologyWithRepartitioning` recently","closed","","ableegoldman","2022-03-09T05:19:03Z","2022-03-10T00:30:56Z"
"","11977","KAFKA-13787 explicitly delete left over state directory contents to a…","Proposed fix for https://issues.apache.org/jira/browse/KAFKA-13787   Delete specific stateDir before checking if it's empty.","open","","nico-javadev","2022-03-31T08:53:32Z","2022-04-18T16:58:46Z"
"","12118","MINOR : Handle javax.ws.rs.NotFoundException in  ConnectExceptionMapper","Proposal to catch valid 404 exceptions, triggered by any HTTP request to a non existent path on the Connect REST API, higher in the code, not to log an ERROR log  on line 61 which can be seen as a false alarm  I was not able to find any existing unit test for this exception mapper  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","nicolasguyomar","2022-05-03T14:09:46Z","2022-08-01T12:48:36Z"
"","11945","KAFKA-13769: Explicitly route FK join results to correct partitions","Prior to this commit FK response sink routed FK results to SubscriptionResolverJoinProcessorSupplier using the primary key.  There are cases, where this behaviour is incorrect. For example, if KTable key serde differs from the data source serde which might happen without a key changing operation.  Instead of determining the resolver partition by serializing the PK this patch includes target partition in SubscriptionWrapper and SubscriptionResponseWrapper payloads. Default FK response-sink partitioner extracts the correct partition from the value and routes the message accordingly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","Gerrrr","2022-03-24T16:26:04Z","2022-04-15T18:28:57Z"
"","12026","MINOR: Change the log output information in the KafkaConsumer assign method","Printing `Subscribed` in the assign method can confuse users who are clearly not using the `subscribe` mode. Even though the bottom layer of assign&subscribe uses SubscriptionState to store assignments, it is recommended to modify the output here.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-11T01:29:19Z","2022-04-11T02:06:57Z"
"","11941","KAFKA-13749: CreateTopics in KRaft must return configs","Previously, when in KRaft mode, CreateTopics did not return the active configurations for the topic(s) it had just created. This PR addresses that gap. We will now return these topic configuration(s) when the user has DESCRIBE_CONFIGS permission. (In the case where the user does not have this permission, we will omit the configurations and set TopicErrorCode. We will also omit the number of partitions and replication factor data as well.)  For historical reasons, we use different names to refer to each topic configuration when it is set in the broker context, as opposed to the topic context.  For example, the topic configuration ""segment.ms"" corresponds to the broker configuration ""log.roll.ms"".  Additionally, some broker configurations have synonyms. For example, the broker configuration ""log.roll.hours"" can be used to set the log roll time instead of ""log.roll.ms"".  In order to track all of this, this PR adds a table in LogConfig.scala which maps each topic configuration to an ordered list of ConfigSynonym classes.  (This table is then passed to KafkaConfigSchema as a constructor argument.)  Some synonyms require transformations. For example, in order to convert from ""log.roll.hours"" to ""segment.ms"", we must convert hours to milliseconds. (Note that our assumption right now is that topic configurations do not have synonyms, only broker configurations. If this changes, we will need to add some logic to handle it.)  This PR makes the 8-argument constructor for ConfigEntry public. We need this in order to make full use of ConfigEntry outside of the admin namespace. This change is probably inevitable in general since otherwise we cannot easily test the output from various admin APIs in junit tests outside the admin package.  Testing:  This PR adds PlaintextAdminIntegrationTest#testCreateTopicsReturnsConfigs. This test validates some of the configurations that it gets back from the call to CreateTopics, rather than just checking if it got back a non-empty map like some of the existing tests. In order to test the configuration override logic, testCreateDeleteTopics now sets up some custom static and dynamic configurations.  In QuorumTestHarness, we now allow tests to configure what the ID of the controller should be. This allows us to set dynamic configurations for the controller in testCreateDeleteTopics. We will have a more complete fix for setting dynamic configuations on the controller later.  This PR changes ConfigurationControlManager so that it is created via a Builder. This will make it easier to add more parameters to its constructor without having to update every piece of test code that uses it.  It will also make the test code easier to read.","closed","","cmccabe","2022-03-24T05:43:04Z","2022-04-01T17:50:29Z"
"","11750","KAFKA-12648: avoid modifying state until NamedTopology has passed validation","Previously we were only verifying the new query could be added after we had already inserted it into the TopologyMetadata, so we need to move the validation upfront.  Also adds a test case for this and improves handling of NPE in case of future or undiscovered bugs","closed","","ableegoldman","2022-02-12T02:58:52Z","2022-02-15T21:07:15Z"
"","12352","KAFKA-13978: Don't skip exceptions in Kafka streams custom handlers","Previously `IllegalArgumentException` and `IllegalStateException` errors would always trigger an application shutdown. With this change those errors are treated the same as all other exceptions thrown while an application processes a stream, and are processed by the registered uncaught exception handler.  Tested using unit and integration tests. Since this is removing a feature, no new tests were added.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Smeb","2022-06-27T07:48:50Z","2022-06-27T08:00:36Z"
"","11541","KAFKA-13498: keeping track of latest seen position in different state stores","Previously [KAFKA-13480](https://github.com/apache/kafka/pull/11514) was adding position tracking to the RocksDB and InMemory KV store. This PR adds other stores such as CachingKVStore, and all Session and Windowed variants.","closed","","patrickstuedi","2021-11-25T15:16:01Z","2021-12-01T17:49:10Z"
"","11920","KAFKA-13672: Race condition in DynamicBrokerConfig","Prevent race condition in `DynamicBrokerConfig` that can prevent configuration being dynamically applied when broker is running slowly.  The race condition occurs when multiple `alterBrokerConfigs` requests are being processed concurrently (but also seems to require the broker to be running under heavy load so that the request processing is slow enough for this issue to occur). Two or more separate threads can interact with the mutable buffer caching `Reconfigurable` objects inside `DynamicBrokerConfig`, and one thread may add a `Reconfigurable` to that buffer while another thread is iterating over it, causing the second thread to hit a `ConcurrentModificationException`.  Firstly in `KafkaApis#handlerAlterConfigsRequest`, the [preprocess](https://github.com/apache/kafka/blob/52621613fd386203773ba93903abd50b46fa093a/core/src/main/scala/kafka/server/KafkaApis.scala#L2633) method call ultimately reaches down into `DynamicBrokerConfig#processReconfiguration` to validate (and only validate) new settings. This validation process iterates over the reconfigurables, but does not modify the buffer.  However, the `ConfigChangedNotificationHandler` that responds to config changes being stored in ZK, also [has a code path](https://github.com/apache/kafka/blob/81e709c4e2554f045a9961e520c0b7cc1a7b3495/core/src/main/scala/kafka/server/ZkConfigManager.scala#L125) that ultimately ends up in `DynamicBrokerConfig#processReconfiguration`, but unlike the `KafkaApis` code path, this one is actually applying config changes.   So while `KafkaApis` is iterating over the reconfigurables, `ConfigChangedNotificationHandler` can add a reconfigurable to the buffer, causing the concurrent modification exception.       *This race condition was made visible by an integration test `DynamicBrokerReconfigurationTest#testThreadPoolResize` which was originally considered flaky, as it repeatedly failed under the CI build, but passed locally. It had been temporarily disabled due to this perceived flakiness. The ideal approach to testing is to re-enable the integration test.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","LiamClarkeNZ","2022-03-20T14:40:37Z","2022-03-24T04:06:33Z"
"","11823","MINOR: set log4j.logger.kafka and all Config logger levels to ERROR for Streams tests","Pretty much any time we have an integration test failure that's flaky or only exposed when running on Jenkins through the PR builds, it's impossible to debug if it cannot be reproduced locally as the logs attached to the test results have truncated the entire useful part of the logs. This is due to the logs being flooded at the beginning of the test when the Kafka cluster is coming up, eating up all of the allotted characters before we even get to the actual Streams test. Setting ` log4j.logger.kafka` to `ERROR` greatly improves the situation and cuts down on most of the excessive logging in my local runs. To improve things even more and have some hope of getting the part of the logs we actually need, I also set the loggers for all of the Config objects to ERROR, as these print out the value of every single config (of which there are a lot) and are not useful as we can easily figure out what the configs were if necessary by just inspecting the test locally.  Hopefully this will help us debug and then fix some of the flaky tests within Streams that show up on PR builds","closed","","ableegoldman","2022-03-01T06:32:06Z","2022-03-02T05:58:26Z"
"","11897","MINOR: Polish Javadoc for EpochState","Polish Javadoc for EpochState","closed","","ijliym","2022-03-15T06:35:44Z","2022-03-15T23:59:31Z"
"","12418","KAFKA-13414: Replace PowerMock/EasyMock with Mockito in connect.storage.KafkaOffsetBackingStoreTest","Partially addressing https://issues.apache.org/jira/browse/KAFKA-13414. Other test files will be updated in other pull requests.","open","connect,","clolov","2022-07-18T15:59:36Z","2022-08-03T16:22:30Z"
"","11787","KAFKA-12738: track processing errors and implement constant-time task backoff","Part 1 in the initial series of error handling for named topologies*:  Part 1: Track tasks with errors within a named topology & implement constant-time based task backoff Part 2: Implement exponential task backoff to account for recurring errors Part 3: Pause/backoff all tasks within a named topology in case of a long backoff/frequent errors for any individual task  *note: not sure whether some or all of this would require a KIP, so I'm limiting the error handling work to just named topologies for now","closed","","ableegoldman","2022-02-17T11:57:52Z","2022-02-27T00:52:04Z"
"","12324","KAFKA-12699: Override the default handler for stream threads if the stream's handler is used","override the default handler for stream threads if the stream's handler is used. We do no want the java default handler triggering when a thread is replaced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-06-21T19:11:20Z","2022-07-19T20:38:45Z"
"","11800","KAFKA-13689: Optimized the printing of AbstractConfig logs, and stripped unknownKeys from unusedKeys","Optimized the printing of AbstractConfig logs, and stripped unknownKeys from unusedKeys","closed","","RivenSun2","2022-02-24T02:37:05Z","2022-03-08T01:43:13Z"
"","11978","KAFKA-13786:  Optimized documentation for control.plane.listener.name parameter","Optimized documentation for control.plane.listener.name parameter","closed","","RivenSun2","2022-03-31T10:14:26Z","2022-04-01T08:23:29Z"
"","11768","MINOR: Optimize the matches method of AccessControlEntryFilter","Optimize the matches method of AccessControlEntryFilter","closed","","RivenSun2","2022-02-16T05:20:44Z","2022-02-17T12:56:58Z"
"","11940","KAFKA-13689: optimize the log output of logUnused method","optimize the log output of logUnused method.","closed","","RivenSun2","2022-03-24T00:57:12Z","2022-03-24T16:29:40Z"
"","11839","KAFKA-13706: remove closed connections from MockSelector.ready","On connection close, remove closed connection from MockSelector.ready  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vincent81jiang","2022-03-03T18:43:35Z","2022-03-04T08:51:53Z"
"","12375","what is Best Practices of kafka mirror maker 2","Now, i config mm2,properties this below  clusters=DC1,DC2 DC1.bootstrap.servers = broker1-dev1:30011, broker2-dev1:30012, broker3-dev1:30013 DC2.bootstrap.servers = broker1-dev2:30011, broker2-dev2:30012, broker3-dev2:30013  DC1.config.storage.replication.factor = 3 DC2.config.storage.replication.factor = 3  DC1.offset.storage.replication.factor = 3 DC2.offset.storage.replication.factor = 3  DC1.status.storage.replication.factor = 3 DC2.status.storage.replication.factor = 3  DC1->DC2.enabled = true DC2->DC1.enabled = true  offset-syncs.topic.replication.factor = 3 heartbeats.topic.replication.factor = 3 checkpoints.topic.replication.factor = 3  topics = .* groups = .*  task.max = 2 replication.factor = 3 refresh.topics.enabled = true sync.topics.configs.enabled = true refresh.topics.interval.seconds = 10   topic.blacklist = .*[\-\.]internal, .*\.replica, __consumer_offsets groups.blacklist = console-consumer-.*, connect-.*, __.*  DC1->DC2.emit.heartbeats.enabled = true DC1->DC2.emit.checkpoints.enabled = true  DC2->DC1.emit.heartbeats.enabled = true DC2->DC1.emit.checkpoints.enabled = true What is the expected behavior? I have a test case this below.  Create topic on DC1 name topicTest1, It will replica sync to DC2 and on the DC2 will have topic in name DC1.testTopic1. next step, I produce message to topicTest1 on DC1 and message will sync to DC1.testTopic1 on DC2. last step I produce message to DC1.testTopic1 on DC2 but message not able sync to topicTest1 on DC1. I have a question. in test last step at bullet 3. the message is produced to DC1.testTopic1 on DC2 should already sync to topicTest1 on DC1, or not sync ?  If not sync, please explain me","open","","phongtatyaemsomphong","2022-07-04T07:37:18Z","2022-07-12T09:14:56Z"
"","12255","MINOR: adjust logging levels in Stream tests","Now that we've turned off logging in the brokers/zookeeper/config classes we can finally see at least some of the logs where Streams is actually doing something when trying to debug tests from a failed PR build. But I've noticed we still have some flooding of warnings from the `NetworkClient` and info-level junk from `Metadata`, so to maximize the visible useful logs we should filter out everything bu the producer/consumer client themselves (in addition to Streams) fine-grained logging","closed","","ableegoldman","2022-06-06T05:21:23Z","2022-06-08T09:02:41Z"
"","12190","KAFKA-13923; Generalize authorizer system test for kraft","Now that KRaft supports an authorizer (from KIP-801), we should be covering it in our system tests. I kept around the test with KRaft and the zk `AclAuthorizer` since it seems useful in KRaft to test an implementation which uses an external source.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-20T23:35:50Z","2022-05-23T16:47:15Z"
"","12182","MINOR: Use dynamic metadata.version check for KIP-704","Now that `metadata.version` has been integrated with the controller #12050, we need to make use of the dynamic nature of the feature flag. This patch adds a supplier that is passed down to ReplicationControlManager so that the leader recovery feature can be toggled on/off.  IBP/MetadataVersion 3.2 added the `LeaderRecoveryState` tagged field to PartitionRecord. If we are downgrading from 3.2 to 3.0, the downgrade of the feature flag will cause the active controller to stop populating this field, but the value will still be present in existing records. However, since this is a tagged field, it won't cause any compatibility problems when we actually downgrade the server binaries (the tagged field will just be ignored).","open","","mumrah","2022-05-19T14:43:20Z","2022-06-14T13:24:33Z"
"","11594","How to define the quantity of consumption groups","Now I have more than 30 topics to consume, and each topic has 6 partitions. I need to write files through the consumer program. I don't know how many consumer groups to create? I understand that a consumer can consume multiple topics,  I wonder if the number of consumer groups is important？","closed","","ayu-programer","2021-12-11T10:26:11Z","2022-02-04T19:48:24Z"
"","12241","MINOR: Fix docs in upgrade.html","NotLeaderForPartitionException  has been deprecated and replaced with NotLeaderOrFollowerException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-06-02T14:32:30Z","2022-06-15T21:45:45Z"
"","11679","MINOR: make JoinGroupRequest and LeaveGroupRequest 'Reason' field ignorable","newer clients see errors when including reason in JoinGroupRequest to older broker: ``` Attempted to write a non-default reason at version 7.  ``` make reason field in JoinGroupRequest and LeaveGroupRequest ignorable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2022-01-13T23:32:09Z","2022-01-14T09:32:34Z"
"","12334","KAFKA-13981: Adding new record type handler in metadata shell","New records handled in this PR: `FEATURE_LEVEL_RECORD`: featureLevels/FEATURE_NAME -> level `BROKER_REGISTRATION_CHANGE_RECORD`:     -> brokers/BROKER_ID/isFenced -> true/false    -> brokers/BROKER_ID/registration -> json content `ACCESS_CONTROL_ENTRY_RECORD`: acls/ACL_UUID/data -> json content `REMOVE_ACCESS_CONTROL_ENTRY_RECORD`: remove acls/ACL_UUID `NO_OP_RECORD`: no op  Tests added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2022-06-23T07:09:03Z","2022-06-23T13:21:52Z"
"","12067","KAFKA-13780: Generate OpenAPI file for Connect REST API","New gradle task: connect:runtime:genConnectOpenAPIDocs that generates connect_rest.yaml under docs/generated This task is executed when siteDocsTar runs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mimaison","2022-04-19T13:12:52Z","2022-06-10T09:35:49Z"
"","12039","KAFKA-13725: KIP-768 OAuth code mixes public and internal classes in same package","Move classes into a sub-package of `internal` named `secured` that matches the layout more closely of the `unsecured` package.  Replaces the concrete implementations in the former packages with sub-classes of the new package layout and marks them as deprecated. If anyone is already using the newer OAuth code, this should still work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kirktrue","2022-04-13T00:11:06Z","2022-06-22T21:19:44Z"
"","11512","MINOR: Modify the Exception type of the testCommitOffsetAsyncNotCoordinator method","Modify the Exception type of the testCommitOffsetAsyncNotCoordinator method to make this test method more valuable","closed","","RivenSun2","2021-11-18T13:14:56Z","2021-11-19T09:02:40Z"
"","11648","MINOR: add default-replication-factor to MockAdminClient","MockAdminClient should add `default.replication.factor` to the config of each node. Otherwise, `describeConfigs()` does not return the default replication factor correctly.","closed","admin,","mjsax","2022-01-05T04:21:24Z","2022-01-05T22:31:55Z"
"","11492","KAFKA-13452: MM2 shouldn't checkpoint when offset mapping is unavailable","MM2 checkpointing reads the offset-syncs topic to create offset mappings for committed consumer group offsets. In some corner cases, it is possible that a mapping is not available in offset-syncs - in that case, MM2 simply copies the source offset, which might not be a valid offset in the replica topic at all. This can cause issues when auto offset sync is also turned on. Updated checkpointing logic to not create checkpoints for topic partitions without a valid offset mapping.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2021-11-12T15:04:12Z","2022-04-28T13:29:37Z"
"","12152","MINOR: Fix DSL typo in streams docs","Minor typo.","closed","","milindmantri","2022-05-12T18:37:29Z","2022-05-20T07:29:00Z"
"","12431","MINOR : typo in trace log","Minor typo in the RFT trace log message   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","nicolasguyomar","2022-07-22T08:33:46Z","2022-07-23T14:15:21Z"
"","12288","HOTFIX: null check for ProducerRecord when computing sizeInBytes","Minor followup to #12235  that adds a null check on the record key in the new ClientUtils#producerRecordSizeInBytes utility method, as there are valid cases in which we might be sending records with null keys to the Producer, such as a simple `builder.stream(""non-keyed-input-topic"").filter(...).to(""output-topic"")`","closed","","ableegoldman","2022-06-13T22:07:41Z","2022-06-14T05:27:06Z"
"","12086","Minor : cleanup.policy is a comma separated list","Minor doc improvement  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nicolasguyomar","2022-04-22T15:15:56Z","2022-06-08T00:54:20Z"
"","12201","MINOR: Replace left single quote with single quote in Connect worker's log message","Minor change to use ' and not LEFT SINGLE QUOTATION MARK in that log, as it's the only place we are using such quote and it can break some ingestion pipeline  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","nicolasguyomar","2022-05-24T13:54:04Z","2022-06-15T06:39:07Z"
"","11710","MINOR: MiniTrogdorCluster mutates objects from other threads","MiniTrogdorCluster spins up agents from a different thread when scheduling them, but does not use volatiles in these objects. It's not clear that the updated fields are visible.  The hope is that this will fix a failure where it test becomes stuck waiting for MiniTrogdorCluster to shutdown: ``` 	Suppressed: java.lang.InterruptedException 		at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014) 		at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088) 		at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1475) 		at java.util.concurrent.Executors$DelegatedExecutorService.awaitTermination(Executors.java:675) 		at org.apache.kafka.trogdor.coordinator.TaskManager.waitForShutdown(TaskManager.java:699) 		at org.apache.kafka.trogdor.coordinator.Coordinator.waitForShutdown(Coordinator.java:133) 		at org.apache.kafka.trogdor.common.MiniTrogdorCluster.close(MiniTrogdorCluster.java:289) 		at org.apache.kafka.trogdor.coordinator.CoordinatorTest.testTaskRequestWithOldStartMsGetsUpdated(CoordinatorTest.java:595) 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 		at java.lang.reflect.Method.invoke(Method.java:498) 		at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:688) 		at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60) 		at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131) 		at org.junit.jupiter.engine.extension.TimeoutInvocation.proceed(TimeoutInvocation.java:46) ```","closed","","lbradstreet","2022-01-25T00:11:19Z","2022-02-03T09:57:22Z"
"","11998","KAFKA-13801: Kafka server does not respect MetricsReporter contract for dynamically configured reporters","MetricsReporter.contextChange contract states the method should always be called first before MetricsReporter.init is called. This is done correctly for reporters enabled by default (e.g. JmxReporter) but not for metrics reporters configured dynamically.  This fixes the call ordering for dynamically configured metrics reporter and updates tests to enforce ordering.","closed","","xvrl","2022-04-05T19:31:52Z","2022-04-07T18:42:29Z"
"","12336","MINOR: Support KRaft in GroupAuthorizerIntegrationTest","Making changes similar to `AuthorizerIntegrationTest` in order to support KRaft with `GroupAuthorizerIntegrationTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-06-23T18:47:46Z","2022-06-27T23:01:15Z"
"","11668","MINOR: Fix confusion between EventQueueProcessingTimeMs and EventQueueTimeMs","Make sure that the event queue processing time histogram gets updated and add tests that verify that the update methods modify the correct histogram.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2022-01-12T02:55:25Z","2022-01-12T05:48:40Z"
"","11556","MINOR: Pass along compression type to snapshot writer","Make sure that the compression type is passed along to the RecordsSnapshotWriter constructor when creating the snapshot writer using the static createWithHeader method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-11-30T19:03:55Z","2021-12-01T17:41:50Z"
"","12412","KAFKA-14079 - Ack failed records in WorkerSourceTask when error tolerance is ALL","Make sure that if error tolerance is set to ALL that record failures are acked so the task will commit future record offsets properly and remove the records from internal tracking preventing a memory leak","closed","connect,","cshannon","2022-07-16T16:22:01Z","2022-07-19T15:58:39Z"
"","12415","KAFKA-14079 - Ack failed records in WorkerSourceTask when error tolerance is ALL","Make sure that if error tolerance is set to ALL that record failures are acked so the task will commit future record offsets properly and remove the records from internal tracking preventing a memory leak","closed","connect,","cshannon","2022-07-17T22:36:29Z","2022-07-18T22:07:21Z"
"","11911","KAFKA-13463: Make pause behavior consistent between cooperative and eager protocols","Make pause behavior consistent between cooperative and eager protocols","open","","RivenSun2","2022-03-17T09:18:46Z","2022-06-14T13:24:54Z"
"","12066","KAFKA-13834: fix drain batch starving issue","Maintains a drainIndex for each node","closed","","ruanliang-hualun","2022-04-19T06:33:19Z","2022-04-24T02:13:05Z"
"","11538","KAFKA-10712; Update release scripts to Python3","Made the following changes so that it works in Python3: * Use print() instead of print * Decode all of the cmd outputs from binary to string using UTF-8 * Encode string to binary using UTF-8 when necessary * Use input instead of raw_input * Replace usages of `basestring` by `str`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-11-25T09:46:41Z","2021-11-29T09:42:17Z"
"","11536","MINOR; Update merge script to work against Python3","Made the following changes so that it works in Python3:  1. Use `print()` instead of `print` 2. Use urllib instead of urllib2 3. Decode all of the cmd outputs from binary to string using UTF-8 4. Use `input` instead of `raw_input` 5. Use `next()` on the iterator returned by `filter()` instead `__get_item__()` 6. Fix any error returned by `pylint`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-11-24T18:23:49Z","2021-12-10T18:56:06Z"
"","12015","MINOR: A few code cleanups in DynamicBrokerConfig","Made a few edits while reading that file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-04-08T07:52:07Z","2022-04-09T09:42:45Z"
"","12168","Minor: Remove extraneous code in LocalLogManager","Looks like this was some left over test code.","closed","","mumrah","2022-05-16T18:07:38Z","2022-05-18T14:58:09Z"
"","12424","RELEASE: Add 3.2 upgrade docs","Looking through the issues fixed in https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.2.1, I didn't see any changes to public APIs (config/metrics/CLI/etc) or any changes to default behaviors. I picked three major issues to include in the release notes.  * https://issues.apache.org/jira/browse/KAFKA-14062 OAuth refresh problem -- the driver for this release * https://issues.apache.org/jira/browse/KAFKA-14079 a major Connect OOM issue * https://issues.apache.org/jira/browse/KAFKA-14024 3.2.0 consumer regression","closed","","mumrah","2022-07-20T14:14:34Z","2022-07-20T19:26:27Z"
"","12213","MINOR: Add timeout to LogOffsetTest","LogOffsetTest seems to be hanging as reported in https://issues.apache.org/jira/browse/KAFKA-13938. This patch adds a timeout to see if it addresses the issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-25T20:19:49Z","2022-05-26T20:19:06Z"
"","11532","MINOR: Fix system test test_upgrade_to_cooperative_rebalance","Log messages were changed in the AssignorConfiguration (https://github.com/apache/kafka/pull/11490) that are also used for verification in system test StreamsCooperativeRebalanceUpgradeTest.test_upgrade_to_cooperative_rebalance.  This commit fixes the test and adds comments to the log messages that point to the test that needs to be updated in case of changes to the log messages.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-11-23T16:49:57Z","2021-11-25T09:50:49Z"
"","11808","KAFKA-13281: list all named topologies","List all the named topologies that have been added to this client  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-02-25T22:09:49Z","2022-02-26T03:04:08Z"
"","11525","MINOR: Share notification code for coordinators","Leader election and resignation logic for the Group Coordinator and Transaction Coordinator is the same. Share this logic by refactoring this code to a higher order method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2021-11-22T18:21:25Z","2021-11-24T17:00:38Z"
"","12433","KAFKA-14093: Use single-worker Connect cluster when testing fenced leader recovery","Largely copied from [Jira](https://issues.apache.org/jira/browse/KAFKA-14093):  The Javadoc for the `testFencedLeaderRecovery` test case calls out that it uses a one-node cluster, and the [check we do](https://github.com/apache/kafka/blob/679e9e0cee67e7d3d2ece204a421ea7da31d73e9/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExactlyOnceSourceIntegrationTest.java#L493-L494) to make sure that the leader is up and running (and has had a chance to instantiate its transactional producer for the config topic) relies on that fact. However, we don't reduce the cluster size to one in the test, so right now it's running with the default of three workers.  As a result, it's possible that the readiness check we're doing hits a follower worker and passes before the leader has had a chance to finish startup.  These changes fix that issue by ensuring that the test runs with only a single worker. In addition, the transactional producers brought up for testing are given meaningful client IDs so that they can be easily distinguished from the producers brought up by the embedded Connect cluster.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-07-22T20:28:02Z","2022-07-25T19:45:54Z"
"","12467","KAFKA-14129: KRaft must check manual assignments for createTopics are contiguous","KRaft should validate that manual assignments given to createTopics are contiguous. In other words, they must start with partition 0, and progress through 1, 2, 3, etc. ZK mode does this, but KRaft mode previously did not. Also fix a null pointer exception when the placement for partition 0 was not specified.  Convert over AddPartitionsTest to use KRaft. This PR converts all of the test except for some of the placement logic tests, which will need to be redone for KRaft mode in a future change.  Fix null pointer exception in KRaftMetadataCache#getPartitionInfo.  Specifically, we should not assume that the partition will be found in the hash map. This is another case where we had ""Some(x)"" but it should be ""Option(x).""  BrokerLifecycleManager should be initialized in the constructor of BrokerServer. Otherwise there will be a null pointer exception if BrokerServer#brokerState is called prior to BrokerServer#start.","closed","","cmccabe","2022-08-01T22:25:35Z","2022-08-02T22:39:50Z"
"","12396","KAFKA-14051: Create metrics reporters in KRaft remote controllers","KRaft remote controllers do not yet support dynamic reconfiguration (https://issues.apache.org/jira/browse/KAFKA-14057).  Until we implement that, in the meantime we see that the instantiation of the configured metric reporters is actually performed as part of the wiring for dynamic reconfiguration.  Since that wiring does not exist yet for KRaft remote controllers, this patch refactors out the instantiation of the metric reporters from the reconfiguration of them and adjusts the controller startup sequence to explicitly instantiate the reporters if the controller is a remote one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2022-07-08T17:05:18Z","2022-07-21T23:59:05Z"
"","12395","MINOR: Run MessageFormatChangeTest in ZK mode only","KRaft mode will not support writing messages with an older message format (2.8) since the min supported IBP is 3.0 for KRaft. Testing support for reading older message formats will be covered by https://issues.apache.org/jira/browse/KAFKA-14056  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ahuang98","2022-07-08T16:51:43Z","2022-07-13T06:47:07Z"
"","11720","KAFKA-13625: Fix inconsistency in dynamic application log levels","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-817%3A+Fix+inconsistency+in+dynamic+application+log+levels (not accepted yet).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","dongjinleekr","2022-01-30T06:31:46Z","2022-02-04T07:56:21Z"
"","11688","KAFKA-13435; Static membership protocol should let the leader skip assignment (KIP-814)","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-814%3A+Static+membership+protocol+should+let+the+leader+skip+assignment  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-18T16:52:59Z","2022-02-14T10:55:42Z"
"","11993","KAFKA-13654: Extend KStream process with new Processor API","KIP-820 implementation.  Changes:  - Implement changes proposed in the KIP. - Adapt existing methods to implement FixedKeyProcessor instead of Processor, to guarantee fixed keys.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2022-04-03T12:08:14Z","2022-04-19T16:58:14Z"
"","11971","KAFKA-13783; Remove reason prefixing in JoinGroupRequest and LeaveGroupRequest","KIP-800 introduced a mechanism to pass a reason in the join group request and in the leaver group request. A default reason is used unless one is provided by the user. In this case, the custom reason is prefixed by the default one.  When we tried to used this in Kafka Streams, we noted a significant degradation of the performances, see https://github.com/apache/kafka/pull/11873. It is not clear wether the prefixing is the root cause of the issue or not. To be on the safe side, I think that we should remove the prefixing. It does not bring much anyway as we are still able to distinguish a custom reason from the default one on the broker side.  This patch removes prefixing the user provided reasons. So if a the user provides a reason, the reason is used directly. If the reason is empty or null, the default reason is used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-30T08:12:57Z","2022-04-07T08:47:47Z"
"","11966","KAFKA-13418: Support key updates with TLS 1.3","Key updates with TLS 1.3 trigger code paths similar to renegotiation with TLS 1.2. Update the read/write paths not to throw an exception in this case (kept the exception in the `handshake` method).  With the default configuration, key updates happen after 2^37 bytes are encrypted. There is a security property to adjust this configuration, but the change has to be done before it is used for the first time and it cannot be changed after that. As such, it is best done via a system test (filed KAFKA-13779).  To validate the change, I wrote a unit test that forces key updates and manually ran a producer workload that produced more than 2^37 bytes. Both cases failed without these changes and pass with them.  Note that Shylaja Kokoori attached a patch with the SslTransportLayer fix and hence included them as a co-author of this change.  Co-authored-by: Shylaja Kokoori  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-03-29T14:32:58Z","2022-03-30T09:04:34Z"
"","11506","kafka_2.10-0.10.0.0 shutdown","kafka_2.10-0.10.0.0  CentOS Linux release 7.9.2009  [2021-11-15 11:03:20,741] INFO [Group Metadata Manager on Broker 0]: Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.GroupMetadataManager) [2021-11-15 11:13:01,769] INFO [Kafka Server 0], shutting down (kafka.server.KafkaServer) [2021-11-15 11:13:01,771] INFO [Kafka Server 0], Starting controlled shutdown (kafka.server.KafkaServer) [2021-11-15 11:13:01,813] INFO [Kafka Server 0], Controlled shutdown succeeded (kafka.server.KafkaServer) [2021-11-15 11:13:01,815] INFO [Socket Server on Broker 0], Shutting down (kafka.network.SocketServer) [2021-11-15 11:13:01,823] INFO [Socket Server on Broker 0], Shutdown completed (kafka.network.SocketServer) [2021-11-15 11:13:01,824] INFO [Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool) [2021-11-15 11:13:01,825] INFO [Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool) [2021-11-15 11:13:01,828] INFO [ThrottledRequestReaper-Produce], Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:02,435] INFO [ThrottledRequestReaper-Produce], Stopped  (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:02,436] INFO [ThrottledRequestReaper-Produce], Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:02,436] INFO [ThrottledRequestReaper-Fetch], Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:03,187] INFO [ThrottledRequestReaper-Fetch], Stopped  (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:03,187] INFO [ThrottledRequestReaper-Fetch], Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper) [2021-11-15 11:13:03,189] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis) [2021-11-15 11:13:03,191] INFO [Replica Manager on Broker 0]: Shutting down (kafka.server.ReplicaManager) [2021-11-15 11:13:03,192] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager) [2021-11-15 11:13:03,195] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager) [2021-11-15 11:13:03,195] INFO [ExpirationReaper-0], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,383] INFO [ExpirationReaper-0], Stopped  (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,383] INFO [ExpirationReaper-0], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,383] INFO [ExpirationReaper-0], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,530] INFO [ExpirationReaper-0], Stopped  (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,530] INFO [ExpirationReaper-0], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,546] INFO [Replica Manager on Broker 0]: Shut down completely (kafka.server.ReplicaManager) [2021-11-15 11:13:03,547] INFO Shutting down. (kafka.log.LogManager) [2021-11-15 11:13:03,729] INFO Shutdown complete. (kafka.log.LogManager) [2021-11-15 11:13:03,731] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.GroupCoordinator) [2021-11-15 11:13:03,731] INFO [ExpirationReaper-0], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,750] INFO [ExpirationReaper-0], Stopped  (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,750] INFO [ExpirationReaper-0], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,750] INFO [ExpirationReaper-0], Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,887] INFO [ExpirationReaper-0], Stopped  (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,887] INFO [ExpirationReaper-0], Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:03,887] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.GroupCoordinator) [2021-11-15 11:13:03,910] INFO [Kafka Server 0], shut down completed (kafka.server.KafkaServer)  kafka shutdown.Ask what reason?","closed","","yunfeng79","2021-11-16T11:56:09Z","2021-12-10T18:57:55Z"
"","12440","KAFKA-14107: Upgrade Jetty version for CVE fixes","KAFKA-14107 Upgrade Jetty for CVE fixes.  Jetty: [CVE-2022-2048](https://nvd.nist.gov/vuln/detail/CVE-2022-2048) and [CVE-2022-2047](https://nvd.nist.gov/vuln/detail/CVE-2022-2047) - Fixed by upgrading to 9.4.48.v20220622  Signed-off-by: Andrew Borley","open","","ajborley","2022-07-26T12:32:37Z","2022-07-26T12:32:37Z"
"","12106","KAFKA-13861: Fix the validateOnly behavior for CreatePartitions requests in KRaft mode","KAFKA-13861: Fix the validateOnly behavior for `CreatePartitions` requests in KRaft mode  The KRaft implementation of the `CreatePartitions` ignores the `validateOnly` flag in the request and creates the partitions if the validations are successful. Fixed the behavior not to create partitions upon validation if the `validateOnly` flag is true.  Add ControllerApi and ReplicationControlManager tests to verify the correct behavior.","closed","","akhileshchg","2022-04-29T00:21:32Z","2022-05-04T17:31:46Z"
"","11994","MINOR: Mention KAFKA-13748 in release notes","KAFKA-13748 removed the FileStreamSinkConnector and FileStreamSinkConnector from the default classpath. Although these are provided for example purposes only, it makes sense to mention this in the release notes for users who use them in their own examples.","closed","","tombentley","2022-04-04T14:45:16Z","2022-04-06T09:29:44Z"
"","11799","KAFKA-13688: Fix a couple incorrect metrics in KafkaController","KAFKA-13688: This change removes unnecessary and incorrect set creations over the topics to be deleted during update metrics, computing both replicasToDeleteCount and ineligibleReplicasToDeleteCount in a single scan. Future metrics calculated over topics to be deleted should build on this scan for better performance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chb2ab","2022-02-23T21:26:06Z","2022-03-26T08:41:35Z"
"","12416","KAFKA-13559: Fix issue where responses intermittently takes 300+ ms to respond, even when the server is idle.","KAFKA-13559: Fix issue where responses intermittently takes 300+ ms to respond, even when the server is idle.  Processing request got delayed by 300 ms in the following condition: 1. Client-Server communication uses SSL socket 2. More than one requests are in the same network packet  This 300 ms delay occurs because the socket has no data but the buffer has data. And the sequence of events that leads to this situation is the following (high level):  Step 1 - Client sends more than one requests in the same network packet. Step 2 - Server processes the 1st request. While doing this, SslTransportLayer reads all of the bytes (containing multiple requests) from the socket and stores it in the buffer. Step 3 - Server sends the response for the 1st request. Step 4 - Server processes the 2nd request. This request is taken from the SslTransportLayer buffer, instead of the socket. Because of this, ""select(timeout)"" blocks for 300 ms. THIS IS WHERE THE DELAY IS.  From producer side, this happens when you produce continuous records in a tight loop and then suddenly stop for more than 300 ms.  To fix this, Selector set ""madeReadProgressLastPoll"" to ""true"" after unmuting the channel, if there's data in the buffer.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","badaiaqrandista","2022-07-18T00:02:01Z","2022-07-26T09:24:31Z"
"","11558","KAFKA-13323 Fixed variable name in KafkaConsumer","KAFKA-13323 Fixed variable name in KafkaConsumer  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vijaykriishna","2021-12-01T14:34:53Z","2022-02-10T22:13:43Z"
"","11535","KAFKA-13476: Increase resilience timestamp decoding Kafka Streams","Kafka Streams decodes the Offset Metadata to extract timestamps. If this metadata is not in Base64 the application fails, with no way to recover except creating a new custom offset commit. The change is to catch the IllegalArgumentException that the Base64 Decoder throws, log that an exception occurred and return the UNKNOWN value instead.  As this is a smaller change I've only updated the decode Timestamp unit tests to test this scenario.  This is my first contribution, so I probably need to take some more actions to get this accepted, any help is appreciated.","closed","streams,","richard-axual","2021-11-24T14:52:35Z","2022-01-06T08:30:08Z"
"","11493","KAFKA-12959: Distribute standby and active tasks across threads to better balance load between threads","Kafka Streams - Currently while distributing the standby tasks streams does not check if there are threads without any tasks or with less number of tasks. This can lead to few threads getting assigned both active and standby tasks when are threads within the same instance without any tasks assigned.  This PR takes into account active task assignment when assigning standbys to threads to achieve a better balance of tasks across threads  ### Committer Checklist (excluded from commit message) * [ ]  Verify design and implementation * [ ]  Verify test coverage and CI build status * [ ]  Verify documentation (including upgrade notes)","closed","","tim-patterson","2021-11-12T20:43:15Z","2022-03-05T08:11:53Z"
"","11496","KAFKA-13454: kafka has duplicate configuration information log information printin…","kafka has duplicate configuration information log information printing during startup,repeated information printing will bring confusion to users.It is better to add log information before and after repeating the configuration information","closed","","zzccctv","2021-11-15T09:32:17Z","2021-12-07T01:08:27Z"
"","11583","POC of passing serdes around in IQv2","Just experimenting with what would happen if we pass serdes around and just skip translating data in the Metered stores at all.  I think this winds up being a little messy because didn't want to build in `K` and `V` type bounds on the query, so there's no bound on the serde types either, which means that both the store's query handler and the user of IQ have to cast after de/serialization.  There also doesn't seem to be a way to generalize the notion of a ""lazily deserialized result"", since the object returned by a query may not be a plain value, but it might be an Iterator of values, or a Future value, etc.  I might have just wound up on the wrong track here, though, so I'm going to take a break and come back later to see if I have better ideas.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2021-12-09T05:17:37Z","2021-12-20T18:22:10Z"
"","12013","Reenable flaky tests","Just a test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-04-07T18:40:42Z","2022-07-29T08:15:30Z"
"","11870","MINOR: jmh.sh swallows compile errors","jmh.sh runs tasks in quiet mode which swallows compiler errors. This is a pain and I frequently have to edit the shell script to see the error.","closed","","lbradstreet","2022-03-09T16:08:44Z","2022-03-10T23:19:02Z"
"","11689","Fixed documentation and handles null topicPartition for KAFKA-12841","Jira: https://issues.apache.org/jira/browse/KAFKA-12841  Using the `InterceptorCallback` wrapper in the case of `ApiException` so that we will adhere correctly to the `Callback` contract for `onCompletion` specifying a valid (dummy) `TopicPartition`.   Removed some documentation from Callback.java that stated ""except topicPartition"" as we are assigning -1 to the topicPartition if it doesn't exist.  The changes is based on https://issues.apache.org/jira/browse/KAFKA-3303  ### Committer Checklist (excluded from commit message)  * [ ]  Verify design and implementation  * [ ]  Verify test coverage and CI build status  * [ ]  Verify documentation (including upgrade notes)","closed","","philipnee","2022-01-18T19:16:42Z","2022-04-18T23:19:00Z"
"","12329","KAFKA-14010: AlterPartition request won't retry when receiving retriable error","Jira: [KAFKA-14010](https://issues.apache.org/jira/browse/KAFKA-14010)  When `Partition#submitAlterPartition` submits ISR update and send out the Alter_Isr request, we'll register a callback with `Future.whenComplete` [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/cluster/Partition.scala#L1585). Later, when the Alter_Isr response received, we use `try/finally` to first trigger the callback, and remove the `unsentIsrUpdates` later [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/AlterPartitionManager.scala#L362-L369). Because of this, inside the callback, when we found there is retriable error, we'll retry, but then, we found there's an ""in-flight"" reqeust because `unsentIsrUpdates` is not removed, which causes the retry didn't actually send out successfully.  In this PR, I did: 1. fix the above described bug by removing the `unsentIsrUpdates` before calling callback. I've confirmed in the callback, we never care about the status of `unsentIsrUpdates`. ~~2. Currently, when we tried to enqueue an ISR update failed due to there's `unsentIsrUpdates`, we throw out `OperationNotAttemptedException`, and later, we'll treat it as no-op from controller, so, we will not retry and also change the state to ""committed"" [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/cluster/Partition.scala#L1631), which is unexpected. Change it to `IllegalStateException`, so that we'll treat it as ""other exception"", and will retry this ISR update.~~  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-06-22T02:53:01Z","2022-06-30T02:07:16Z"
"","12028","KAFKA-13804: output the reason why broker exit unexpectedly during startup","JIRA: [KAFKA-13804](https://issues.apache.org/jira/browse/KAFKA-13804)  improve the log readability by adding the exception why broker exit during startup at the end of logs.  The updated log output will be like this: ``` ... [2022-04-07 18:19:33,005] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer) java.io.IOException: No space left on device   at kafka.server.KafkaServer.startup(KafkaServer.scala:461)   at kafka.Kafka$.main(Kafka.scala:110)   at kafka.Kafka.main(Kafka.scala) [2022-04-07 18:19:33,007] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer) [2022-04-07 18:19:33,008] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer) [2022-04-07 18:19:33,016] INFO [KafkaServer id=0] Controlled shutdown request returned successfully after 6ms (kafka.server.KafkaServer) .... [2022-04-07 18:19:33,227] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats) [2022-04-07 18:19:33,227] INFO App info kafka.server for 0 unregistered (org.apache.kafka.common.utils.AppInfoParser) [2022-04-07 18:19:33,227] INFO [KafkaServer id=0] shut down completed   // old log [2022-04-11 15:06:27,405] ERROR Exiting Kafka (kafka.Kafka$)  // updated log [2022-04-11 15:06:27,405] ERROR Exiting Kafka due to fatal exception during startup (kafka.Kafka$) java.io.IOException: No space left on device   at kafka.server.KafkaServer.startup(KafkaServer.scala:461)      at kafka.Kafka$.main(Kafka.scala:110)      at kafka.Kafka.main(Kafka.scala)  [2022-04-12 11:07:08,466] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-04-11T10:34:46Z","2022-05-06T09:25:33Z"
"","11631","KAFKA-13563: clear FindCoordinatorFuture for non consumer group mode","jira: [KAFKA-13563](https://issues.apache.org/jira/browse/KAFKA-13563)  After KAFKA-10793, we clear the findCoordinatorFuture in 2 places:      1. heartbeat thread     2. AbstractCoordinator#ensureCoordinatorReady  But in non consumer group mode with group id provided (for offset commitment. So that there will be consumerCoordinator created), there will be no (1)heartbeat thread , and it only call (2)AbstractCoordinator#ensureCoordinatorReady when 1st time consumer wants to fetch committed offset position. That is, after 2nd lookupCoordinator call, we have no chance to clear the findCoordinatorFuture , and causes the offset commit never succeeded.    To avoid the race condition as KAFKA-10793 mentioned, it's not safe to clear the findCoordinatorFuture in the future listener. So, I think we can fix this issue by calling AbstractCoordinator#ensureCoordinatorReady when coordinator unknown in non consumer group case, under each ConsumerCoordinator#poll.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-30T10:14:11Z","2022-02-06T23:10:26Z"
"","12068","MINOR: Fix warnings in Gradle 7","JavaExec.main and Report.destination have been deprecated and will be removd in Gradle 8. Use the new fields (mainClass and outputLocation) instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-04-19T14:45:29Z","2022-04-20T10:41:27Z"
"","12328","MINOR: Add ineligible replica reason to log message","It's useful if the message about ineligible replicas added in https://github.com/apache/kafka/commit/f83d95d9a28267f7ef7a7b1e584dcdb4aa842210#diff-a52da12861f5f9722a73c354a650e4b9af3e7ccce560d7e0d1e67c299e000b58 explains the reason the replica is ineligible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-06-21T22:40:32Z","2022-06-22T20:27:47Z"
"","12273","MINOR: Update Scala to 2.13.8 in gradle.properties","It was updated in dependencies.gradle but this one was probably left out.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2022-06-09T08:15:46Z","2022-06-09T12:54:58Z"
"","11900","MINOR: Fix class comparison in `AlterConfigPolicy.RequestMetadata.equals()`","It was checking 'o' against itself.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","idank","2022-03-15T12:44:27Z","2022-03-22T08:53:53Z"
"","11762","MONOR: Remove repeate create ZkConfigRepository","It seems the `ZkConfigRepository` is just a wrapper of `zkClient`, so  we don't need to create a new one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ruanwenjun","2022-02-15T11:48:30Z","2022-03-16T08:51:57Z"
"","12215","MINOR: Collect metadata log dir in kraft system tests","It is useful to collect the directory for `__cluster_metadata` in system tests. We use a separate directory from user partitions, so it must be configured separately.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-25T20:55:58Z","2022-05-26T00:36:59Z"
"","11891","KAFKA-13727; Preserve txn markers after partial segment cleaning","It is possible to clean a segment partially if the offset map is filled before reaching the end of the segment. The highest offset that is reached becomes the new dirty offset after the cleaning completes. The data above this offset is nevertheless copied over to the new partially cleaned segment. Hence we need to ensure that the transaction index reflects aborted transactions from both the cleaned and uncleaned portion of the segment. Prior to this patch, this was not the case. We only collected the aborted transactions from the cleaned portion, which means that the reconstructed index could be incomplete. This can cause the aborted data to become effectively committed. It can also cause the deletion of the abort marker before the corresponding data has been removed (i.e. the aborted transaction becomes hanging).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-03-14T20:26:24Z","2022-03-15T19:26:23Z"
"","11548","KAFKA-13487:Create a topic partition directory based on the size of the directory","issue:[KAFKA-13487](https://issues.apache.org/jira/browse/KAFKA-13487)  In the actual production environment, the file size generated by each subject partition is different. As a result, the disk usage of the base directory is unbalanced.It is not convenient to use the existing script to migrate the topic replica(such as kafka-reassign-partitions.sh)  Add Broker-level configuration items:**log.directory.select.strategy**, there are two values: **partition** and **size** * **partition**: Sort by the number of directories under each base directory (log.dirs) (the current version of the new partition creation strategy) * **size**: Sort by the size of each directory (log.dirs), the smallest directory has the highest allocation right  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","Asura7969","2021-11-29T10:05:33Z","2021-12-06T16:39:08Z"
"","11644","KAFKA-13433:JsonConverter's method convertToJson when field is optional with default value and value is null, return default value.","Issue is [here](https://issues.apache.org/jira/browse/KAFKA-13433) .  Problem: JsonConverter's method convertToJson when field is optional with default value and value is null, return default value. It should return `null` value.I found this scene when use debezium as cdc source. I create a table with a field (optional with default value), insert into a row with 'null' value, but got default value from debezium, with debug source code I found kafka-connect class 'JsonConverter'.","open","connect,","GOODBOY008","2022-01-04T07:08:34Z","2022-05-12T15:14:58Z"
"","12462","[KAFKA-9965] Fix accumulator tryAppend, so that fresh new producerBatch is created","Issue - partitioner.partition is called [once](https://github.com/apache/kafka/blob/1cc1e776f703b180f4bd979e8a551805b3bdc94e/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1012). For this partition -> `accumulator` would try to find existing batch present to fill record in.  If not, it would abortAppend on record in this partition buffer, and flag `abortForNewBatch`.  With `abortForNewBatch`, partitioner.partition is called again! RoundRobin partition would increment the counter twice for the same record (when key, and record.partition is null). For even number of partitions, only half of them are filled up.   Since every second partition gets records filled in batch, `accumulator` will always find empty batch for current partition, and hence it will always get skipped.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sudeshwasnik","2022-08-01T12:38:03Z","2022-08-01T18:22:58Z"
"","12230","MINOR: Catch InvocationTargetException explicitly and propagate underlying cause","InvocationTargetException always wraps an underlying cause. It makes sense to catch it as soon as possible and only propagate the underlying cause.  # Change 1. Replace ""legacy"" [1] with `getCause()` 2. Propagate the cause after catching InvocationTargetException   [1] ""legacy as per the docs"" https://docs.oracle.com/javase/8/docs/api/java/lang/reflect/InvocationTargetException.html","open","","divijvaidya","2022-05-31T10:42:01Z","2022-08-03T18:50:57Z"
"","12456","MINOR: convert some more junit tests to support KRaft","Introduce TestUtils#waitUntilLeaderIsElectedOrChangedWithAdmin, a ZK-free alternative to TestUtils#waitUntilLeaderIsElectedOrChanged.  Convert PlaintextProducerSendTest, SslProducerSendTest, TransactionsWithMaxInFlightOneTest, AddPartitionsToTxnRequestServerTest and KafkaMetricsReporterTest to support KRaft","closed","","cmccabe","2022-07-28T22:01:46Z","2022-07-29T20:36:47Z"
"","11739","MINOR: Reduce cold scala compilation time by 15% via scalac backend parallelism","Introduce `maxScalacThreads` and set the default to the lowest of `8` and the number of processors available to the JVM. The number `8` was picked empirically, the sweet spot is between 6 and 10.  On my desktop, `./gradlew clean core:compileScala core:compileTestScala` improved from around 60s to 51s (15% reduction) with this change.  While at it, we improve the build output to include more useful information at the start: build id, max parallel forks, max scala threads and max test retries.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-02-08T13:30:38Z","2022-02-08T19:03:22Z"
"","12356","KAFKA-10493: Drop out-of-order records in KTable","Instead of emitting a warning when adding out-of-order (by timestamp) records to a `KTable`, we should discard these records silently.  Unfortunately, this creates a problem for `KTable`s produced from a source-topic that has been optimized with the source-topic optimization, i.e. when using `StreamsBuilder#table(...)`.  If a user topic that is being used as a changelog (by the source-topic optimiation) has compaction enabled, it's possible that the topic-compaction will eliminate newer (by timestamp) records, because compaction is (currently) offset-based.  To ensure we don't produce a `KTable` that is inconsistent with the source-topic in this scenario, we must continue to add all records from these topics in to the materialized store, irrespective of timestamp.  To facilitate this, `KTableSource` is now explicitly instructed by the topology when it's processing records from an optimized source-topic, via the new `sourceTopicOptimized` field.","open","","nicktelford","2022-06-28T10:13:01Z","2022-06-28T10:13:24Z"
"","12263","KAFKA-13939: Only track dirty keys if logging is enabled.","InMemoryTimeOrderedKeyValueBuffer keeps a Set of keys that have been seen in order to log them for durability. This set is never used nor cleared if logging is not enabled. Having it be populated creates a memory leak. This change stops populating the set if logging is not enabled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jnewhouse","2022-06-07T18:42:13Z","2022-06-16T21:27:38Z"
"","11802","[RFC][1/N]add new RocksDBTimeOrderedWindowStore","Initial State store implementation for `TimedWindow` and `SlidingWindow`.   [RocksDBTimeOrderedWindowStore.java](https://github.com/apache/kafka/compare/trunk...lihaosky:final-store?expand=1#diff-b174ffbf49b0195880185ff49b98ff3bb0300d35939686aa7772ef84b1f8ba38) contains one `RocksDBTimeOrderedSegmentedBytesStore`  which contains `index` and `base` schema.   [PrefixedWindowKeySchemas.java](https://github.com/apache/kafka/compare/trunk...lihaosky:final-store?expand=1#diff-1efce76341bb9f0793d5a3fc1fdd05a016fc31db788da8d0597142fa551c4bab) implements keyschema for time ordered base store and key ordered index store.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-02-24T22:25:29Z","2022-03-31T05:09:35Z"
"","11973","MINOR: Increase wait in ZooKeeperClientTest","Increase wait in ZooKeeperClientTest.testReinitializeAfterAuthFailure so that the testcase of https://github.com/apache/kafka/pull/11563 actually fails without the corresponding source code fix. Followup of https://issues.apache.org/jira/browse/KAFKA-13461.  Co-Authored-By: Gantigmaa Selenge   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2022-03-30T13:43:19Z","2022-04-19T09:23:24Z"
"","12194","MINOR: Include listener name in SocketServer acceptor and processor log context","Includes the listener name when logging out messages in the various network threads.  Before: ``` [2022-05-22 09:01:01,331] DEBUG Processor 2 listening to new connection from /127.0.0.1:64381 (kafka.network.Processor:62) [2022-05-22 09:01:01,331] DEBUG Processor 1 listening to new connection from /127.0.0.1:64380 (kafka.network.Processor:62) [2022-05-22 09:01:01,331] DEBUG Processor 0 listening to new connection from /127.0.0.1:64379 (kafka.network.Processor:62) ```  After: ``` [2022-05-22 09:15:47,772] DEBUG [SocketServer listenerType=ZK_BROKER, nodeId=0, listener=PLAINTEXT] Processor 0 listening to new connection from /127.0.0.1:64611 (kafka.network.Processor:62) ```  Tested by running a SocketServer unit test and verifying that messages were logged with an appropriate log prefix  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","splett2","2022-05-22T16:19:03Z","2022-06-17T14:12:51Z"
"","11881","MINOR: revert back to 60s session timeout for static membership test","In this PR: https://github.com/apache/kafka/pull/11236/files#diff-bd4be654a82d362772b2010a0fa22a44916ff0da241ca7e6072741f7ef710136L689-R696 , we tried to reduce the session timeout for running tests faster. However, we accidentally overrode the session timeout in static membership tests, where we expected to set to a longer session timeout to test the static member won't trigger rebalance. Revert it back to 60 seconds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-11T07:11:29Z","2022-04-21T18:51:31Z"
"","11528","MINOR: update rocksDb memory management doc","In the RocksDb memory management [doc](https://kafka.apache.org/30/documentation/streams/developer-guide/memory-mgmt.html#rocksdb), we mentioned in the footnote that there's a rocksdb bug caused the `strict_capacity_limit` boolean parameter in the LRUCache constructor can't be set to `true`. However, the bug is already fixed in 6.11.4 (see [here](https://github.com/facebook/rocksdb/issues/6247#issuecomment-706523718)), and we're using 6.22 now, so the note can be removed.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-11-23T07:02:41Z","2021-11-25T09:20:32Z"
"","11562","KAFKA-12648: extend IQ APIs to work with named topologies","In the new NamedTopology API being worked on, state store names and their uniqueness requirement is going to be scoped only to the owning topology, rather than to the entire app. In other words, two different named topologies can have different state stores with the same name.  This is going to cause problems for some of the existing IQ APIs which require only a name to resolve the underlying state store. We're now going to need to take in the topology name in addition to the state store name to be able to locate the specific store a user wants to query  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tolgadur","2021-12-02T15:10:08Z","2021-12-09T11:54:28Z"
"","11950","KAFKA-12875: Change Log layer segment map mutations to avoid absence of active segment","In the log layer, if all segments from a log are deleted before a new segment is created, the log will have no active segment for a brief time. This will allows for a race condition to happen if other threads are reading the active segment at the same time which will can result in a NoSuchElementException.  This PR: - introduces a new function which creates a new segment and then deletes the given segment. This would ensure the log will have an active segment at all times. - fixes two places where segments are deleted before new segment is created.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yyu1993","2022-03-25T14:25:25Z","2022-03-31T17:56:08Z"
"","12468","KAFKA-13877: Fix flakiness in RackAwarenessIntegrationTest","In the current test, we check for tag distribution immediately after everyone is on the running state, however due to the fact of the follow-up rebalances, ""everyone is now in running state"" does not mean that the cluster is now stable. In fact, a follow-up rebalance may occur, upon which the local thread metadata would return empty which would cause the distribution verifier to fail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-08-02T01:56:28Z","2022-08-03T16:17:42Z"
"","12162","KAFKA-13899: Use INVALID_CONFIG error code consistently in AlterConfig APIs","In the AlterConfigs/IncrementalAlterConfigs zk handler, we return `INVALID_REQUEST` and `INVALID_CONFIG` inconsistently. The problem is in `LogConfig.validate`. We may either return `ConfigException` or `InvalidConfigException`. When the first of these is thrown, we catch it and convert to `INVALID_REQUEST`. If the latter is thrown, then we return `INVALID_CONFIG`. It seems more appropriate to return `INVALID_CONFIG` consistently. Note that the KRaft implementation already does this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-13T17:56:28Z","2022-05-17T00:44:43Z"
"","11691","KAFKA-13598: enable idempotence producer by default and validate the configs","In short, after v3.0, the default producer is still not idempotence enabled.  In v3.0, we make `enable.idempotence` default to true, but we didn't adjust the validator and the `idempotence` enabled check method, so that if user didn't explicitly enable idempotence, this feature won't be turned on. In this PR, I did:  1. fix the `ProducerConfig#idempotenceEnabled` method, to make it correctly detect if `idempotence` is enabled or not 2. remove some unnecessary config overridden and checks due to we already default `acks`, `retries` and `enable.idempotence` configs. 3. move the config validator for idempotence producer from `KafkaProducer` into `ProducerConfig`. The config validation should be the responsibility of `ProducerConfig` class. 4. add an `AbstractConfig#hasKeyInOriginals` method, to avoid `originals` configs get copied and only want to check the existence of the key.  5. fix many broken tests. As mentioned, we didn't actually enable idempotence in v3.0. After this PR, there are some tests broken due to some different behavior between idempotence and non-idempotence producer. Fix them 6. add tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-19T07:08:54Z","2022-03-23T07:51:35Z"
"","12471","MINOR; Use underscore for variable initialization in BrokerServer","In Scala its standard practice to use _ whenever you are initializing variables. In regards to implementation, for object references _ initialization maps to null so there is no chance in behaviour.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mdedetrich","2022-08-02T08:40:20Z","2022-08-03T19:40:15Z"
"","12314","KAFKA-13903: Queue size metric in QuorumController","In order to stay in-line with existing queue metrics in ControllerEventManager mbean, we need to include a queue size metric in KRaft mode.  The current size of the underlying queue in QuorumController needs to be exposed as:  ``` kafka.controller:type=ControllerEventManager,name=EventQueueSize ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)   [➡ Link to JIRA](https://issues.apache.org/jira/browse/KAFKA-13903)","open","","soarez","2022-06-20T11:48:33Z","2022-07-26T13:26:47Z"
"","11611","MINOR: prefix topics if internal config is set","In order to move a topology to another runtime without having to copy over the internal topics it would be good to have the option to not prefix the internal topics with the application ID. So this change will introduce a new config that if set will be the internal topic prefix   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-12-17T20:52:45Z","2022-01-11T00:08:48Z"
"","12129","MINOR: Fix link to old doc in quickstart","In Kafka's quickstart a link points to the 2.5 Kafka Streams demo. This PR fixes this link.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-06T08:09:50Z","2022-05-06T13:16:04Z"
"","11853","KAFKA-13710: bring the InvalidTimestampException back for record error","In https://github.com/apache/kafka/pull/11830, we remove the `InvalidTimestampException` case and failed 2 test cases. It looks like we intended to treat the `INVALID_TIMESTAMP` error as special case. So, added back the `INVALID_TIMESTAMP` case to fix the tests.   Failed test: kafka.server.ProduceRequestTest.testProduceWithInvalidTimestamp() kafka.api.PlaintextProducerSendTest.testSendWithInvalidCreateTime()  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-06T03:05:41Z","2022-03-08T06:28:17Z"
"","12403","KAFKA-13166 Fix missing ControllerApis error handling","In ControllerApis, we are making use of the CompletableFuture-s returned by QuorumController to add callbacks via `whenComplete`. Since these chained methods on CompletableFuture return a _new_ CompletableFuture instance, the ControllerPurgatory does not capture any of downstream exceptions when completing the original instance. The only way to see these errors is to call `get` on the chained future, or to add additional completion stages to handle the error explicitly. However, we do not want to call `get` since that would block the request threads.  This patch changes the signature of the request handlers in ControllerApis to all return CompletableFuture. It also converts all of the `CompletableFuture#whenComplete` callbacks to `CompletableFuture#handle`. This allows us to directly handle exceptions thrown by the controller in the individual request handlers. A single `whenComplete` error handler is added in `ControllerApis#handle` which will ensure we log exceptions thrown by the response builders (or other logic in the chained `handle` calls).   The resulting future chain now looks like:  ```scala controller.doStuff()     .handle { (result, exception) =>        // Transform result or exception into a response      }     .whenComplete { (result, exception) =>        // Handle errors thrown in ""handle""     } ```  The future is still completed by ControllerPurgatory, and that is when the completion stages are run. We do not call `get` in ControllerApis to avoid blocking the request threads.  There is some opportunity for cleanup and consolidation of error handling in ControllerApis, but I'd rather make a separate PR for that.","closed","","mumrah","2022-07-13T18:44:04Z","2022-07-26T23:08:59Z"
"","12320","KAFKA-13702: Connect RestClient overrides response status code on request failure","In case the submitted request status is >=400, the connect RestClient [throws](https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L133) a ConnectRestException with the proper response code, but it gets intercepted and [rethrown with 500 status code](https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L147), effectively overriding the actual failure status.  Includes a new unit-test class for the RestClient  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","elkkhan","2022-06-21T11:08:25Z","2022-07-20T09:29:00Z"
"","12372","KAFKA-14036; Set local time in `ControllerApis` when `handle` returns","In `ControllerApis`, we are missing the logic to set the local processing end time after `handle` returns. As a consequence of this, the remote time ends up reported as the local time in the request level metrics. The patch adds the same logic we have in `KafkaApis` to set `apiLocalCompleteTimeNanos`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-06-30T23:07:01Z","2022-07-01T04:07:21Z"
"","11613","MINOR: Update streamResetter option description","In [KIP-171](https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application), We added support to allow users to specify to reset offsets to a specific position, not only to the earliest. But the tool description doesn't reflect this change. Update it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tools,","showuon","2021-12-18T07:54:54Z","2022-01-19T19:37:57Z"
"","11612","MINOR: Update stream resetter option description","In [KIP-171](https://cwiki.apache.org/confluence/display/KAFKA/KIP-171+-+Extend+Consumer+Group+Reset+Offset+for+Stream+Application), We added support to allow users to specify to reset offsets to a specific position, not only to the earliest. But the tool description doesn't reflect this change. Update it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-18T07:51:00Z","2021-12-18T07:52:20Z"
"","11745","KAFKA-13661; Consistent permissions for CreatePartitions API","In #11649, we fixed one permission inconsistency between kraft and zk authorization for the `CreatePartitions` request. Previously kraft was requiring `CREATE` permission on the `Topic` resource when it should have required `ALTER`. A second inconsistency is that kraft was also allowing `CREATE` on the `Cluster` resource, which is not supported in zk clusters and was not documented in KIP-195: https://cwiki.apache.org/confluence/display/KAFKA/KIP-195%3A+AdminClient.createPartitions. This patch fixes this inconsistency and adds additional test coverage for both cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2022-02-09T22:36:57Z","2022-02-11T23:01:09Z"
"","12099","MINOR: Improve ssl description in zero-copy docs","Improvements to PR #12052, improve ssl description in zero-copy docs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-27T04:15:43Z","2022-04-29T15:44:40Z"
"","11930","MINOR: Improve the readability of the DelegationTokenManager code","Improve the readability of the DelegationTokenManager code","open","","RivenSun2","2022-03-22T12:57:04Z","2022-03-31T08:21:47Z"
"","12070","KAFKA-13838: Improve the poll method of ConsumerNetworkClient","Improve the poll method of ConsumerNetworkClient  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","RivenSun2","2022-04-20T05:16:13Z","2022-06-06T01:38:09Z"
"","11947","MINOR: Improve the description of principal under different mechanisms of sasl","Improve the description of principal under different mechanisms of sasl.","closed","","RivenSun2","2022-03-25T04:32:25Z","2022-04-15T09:09:20Z"
"","12069","MINOR: Improve postProcessAndValidateIdempotenceConfigs method","Improve postProcessAndValidateIdempotenceConfigs method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-20T02:20:49Z","2022-04-21T06:43:27Z"
"","12052","KAFKA-13799: Improve documentation for Kafka zero-copy","Improve documentation for Kafka zero-copy. Kafka combines pagecache and zero-copy to greatly improve message consumption efficiency. But zero-copy only works in PlaintextTransportLayer.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-15T01:48:52Z","2022-04-27T03:53:46Z"
"","11779","KAFKA-10000: Zombie fencing (KIP-618)","Implements the zombie fencing logic described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-ZombieFencing) (except for the portion already covered by https://github.com/apache/kafka/pull/11778).  Relies on changes from: - https://github.com/apache/kafka/pull/11777 - https://github.com/apache/kafka/pull/11778  Note that none of the logic here actually causes zombie fencing to take place, it only implements the internal API required to perform zombie fencing. Downstream PRs will actually put this logic into play.","closed","","C0urante","2022-02-17T04:15:17Z","2022-06-10T16:40:30Z"
"","11776","KAFKA-10000: Add new preflight connector config validation logic (KIP-618)","Implements the preflight validation logic described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-RESTAPIpre-flightvalidation).  Relies on changes from: - https://github.com/apache/kafka/pull/11774 - https://github.com/apache/kafka/pull/11775","closed","","C0urante","2022-02-17T03:58:28Z","2022-06-02T12:26:57Z"
"","11777","KAFKA-10000: Add producer fencing API to admin client (KIP-618)","Implements the new admin client API described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-AdminAPItoFenceoutTransactionalProducers).  Note that this API is not used by the Connect framework in this PR. It will be leveraged by Connect in downstream PRs.","closed","","C0urante","2022-02-17T04:01:26Z","2022-03-03T15:40:47Z"
"","11778","KAFKA-10000: Use transactional producer for config topic (KIP-618)","Implements the behavior described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Leaderaccesstoconfigtopic): using a transactional producer for writes to the config topic that should only be performed by the leader of the cluster.  Relies on changes from: - https://github.com/apache/kafka/pull/11774 - https://github.com/apache/kafka/pull/11775","closed","","C0urante","2022-02-17T04:05:24Z","2022-06-07T15:13:52Z"
"","11783","KAFKA-10000: System tests (KIP-618)","Implements system tests for [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors).  Relies on changes from: - https://github.com/apache/kafka/pull/11782","open","connect,","C0urante","2022-02-17T04:44:15Z","2022-07-27T22:06:46Z"
"","11781","KAFKA-10000: Per-connector offsets topics (KIP-618)","Implements support for per-connector offsets topics as described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Per-connectoroffsetstopics).  Relies on changes from: - https://github.com/apache/kafka/pull/11780","closed","","C0urante","2022-02-17T04:34:16Z","2022-07-15T06:02:20Z"
"","11695","KAFKA-13595: Allow producing records with null fields in ConsoleProducer","Implements KIP-810: https://cwiki.apache.org/confluence/display/KAFKA/KIP-810%3A+Allow+producing+records+with+null+values+in+Kafka+Console+Producer  ConsoleProducer accepts a new setting, null.marker, that allows settings the record key, value or headers to null. This can be used to produce ""tombstone"" records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-01-20T18:01:41Z","2022-02-01T17:56:08Z"
"","11599","KAFKA-13527: Add top-level error code field to DescribeLogDirsResponse","Implements KIP-784   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mimaison","2021-12-13T19:35:33Z","2022-02-01T17:53:36Z"
"","11796","KAFKA-13152: Replace ""buffered.records.per.partition"" with ""input.buffer.max.bytes""","Implements KIP-770.","closed","kip,","vamossagar12","2022-02-22T17:58:35Z","2022-03-29T07:22:10Z"
"","11782","KAFKA-10000: Integration tests (KIP-618)","Implements embedded end-to-end integration tests for [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors), and brings together previously-decoupled logic from upstream PRs.  Relies on changes from: - https://github.com/apache/kafka/pull/11781 - https://github.com/apache/kafka/pull/11779 - https://github.com/apache/kafka/pull/11776","closed","","C0urante","2022-02-17T04:42:00Z","2022-07-06T02:35:05Z"
"","11780","KAFKA-10000: Exactly-once source tasks (KIP-618)","Implements a source task wrapper (`ExactlyOnceWorkerSourceTask`) that follows the behavior described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Atomicoffsetwrites) for writing source records and their offsets in transactions with user-configurable (and sometimes connector-defined) boundaries.  Relies on changes from: - https://github.com/apache/kafka/pull/11772 - https://github.com/apache/kafka/pull/11775","closed","connect,","C0urante","2022-02-17T04:28:07Z","2022-06-13T15:04:27Z"
"","11557","KAFKA-13491: IQv2 framework","Implements a new interactive query framework, as described in KIP-805 (https://cwiki.apache.org/confluence/x/85OqCw).  No public queries are added in this PR, just the framework and tests.  Also, position tracking and bounding is not implemented in this PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-11-30T22:23:26Z","2021-12-04T02:01:47Z"
"","11560","KAFKA-7589: Allow configuring network threads per listener","Implements [KIP-788](https://cwiki.apache.org/confluence/display/KAFKA/KIP-788%3A+Allow+configuring+num.network.threads+per+listener). The number of network threads can be set per listener using the following syntax: `listener.name..num.network.threads=`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-12-01T17:24:51Z","2022-01-13T10:24:54Z"
"","11733","KAFKA-13587; Implement leader recovery for KIP-704","Implementation of the protocol for starting and stopping leader recovery after an unclean leader election. This includes the management of state in the controllers (legacy and KRaft) and propagating this information to the brokers. This change doesn't implement log recovery after an unclean leader election.  ### Protocol Changes For the topic partition state znode, the new field ""leader_recovery_state"" was added. If the field is missing the value is assumed to be RECOVERED.  ALTER_PARTITION was renamed from ALTER_ISR. The CurrentIsrVersion field was renamed to PartitionEpoch. The new field LeaderRecoveryState was added.  The new field LeaderRecoverState was added to the LEADER_AND_ISR request. The inter broker protocol version is used to determine which version to send to the brokers.  A new tagged field for LeaderRecoveryState was added to both the PartitionRecord and PartitionChangeRecord.  ### Controller For both the KRaft and legacy controller the LeaderRecoveryState is set to RECOVERING, if the leader was elected out of the ISR, also known as unclean leader election. The controller sets the state back to RECOVERED after receiving an ALTER_PARTITION request with version 0, or with version 1 and with the LeaderRecoveryState set to RECOVERED.  Both controllers preserve the leader recovery state even if the unclean leader goes offline and comes back online before an RECOVERED ALTER_PARTITION is sent.  The controllers reply with INVALID_REQUEST if the ALTER_PARTITION either: 1. Attempts to increase the ISR while the partition is still RECOVERING 2. Attempts to change the leader recovery state to RECOVERING from a RECOVERED state.  ### Topic Partition Leader The topic partition leader doesn't implement any log recovery in this change. The topic partition leader immediately marks the partition as RECOVERED and sends that state in the next ALTER_PARTITION request.  ### Topic Partition Follower The topic partition follower will reject FETCH requests with NOT_LEADER_OR_FOLLOWER while the leader is RECOVERING. The partition follower can receive FETCH requests because of KIP-392. The partition follower will start accepting requests after the leader has recovered and the follower has received a RECOVERED leader state through the LEADER_AND_ISR or through the partition records in the KRaft mode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-02-04T03:37:27Z","2022-03-18T16:24:16Z"
"","11498","Add recordMetadata() to StateStoreContext","Implementation of KIP-791:  https://cwiki.apache.org/confluence/display/KAFKA/KIP-791%3A+Add+Record+Metadata+to+State+Store+Context","closed","","patrickstuedi","2021-11-15T09:43:54Z","2021-11-16T16:51:41Z"
"","12235","KAFKA-13945: add bytes/records consumed and produced metrics","Implementation of [KIP-846: Source/sink node metrics for Consumed/Produced throughput in Streams](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=211886093)  Adds the following INFO node-level metrics for the total bytes/records consumed and produced:  - bytes-consumed-total - records-consumed-total - bytes-produced-total - records-produced-total","closed","","ableegoldman","2022-06-01T12:23:08Z","2022-06-19T23:24:52Z"
"","11549","KAFKA-13382: Automatic storage formatting","Implementation of [KIP-785](https://cwiki.apache.org/confluence/display/KAFKA/KIP-785%3A+Automatic+storage+formatting).  This patch introduces two new configuration properties:  * `cluster.id` - String, the cluster ID * `auto.format.storage` - Bool, feature toggle  When both are set, any storage directory that needs formatting - with the storage tool - will be automatically formatted during initialization.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","soarez","2021-11-29T17:25:16Z","2022-01-31T12:33:02Z"
"","12347","KAFKA-13919: expose log recovery metrics","Implementation for [KIP-831](https://cwiki.apache.org/confluence/display/KAFKA/KIP-831%3A+Add+metric+for+log+recovery+progress).   In this PR, I did: 1. add `remainingLogsToRecover` metric for the number of remaining logs for each log.dir to be recovered 2. add `remainingSegmentsToRecover` metric for the number of remaining segments for the current log assigned to the recovery thread. 3. remove these metrics after log loaded completely 4. add tests (it's quite complicated to test it...)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-06-25T09:17:09Z","2022-07-22T03:32:47Z"
"","11598","KAFKA-13479: Implement range and scan queries","Implement the RangeQuery as proposed in KIP-805  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vpapavas","2021-12-13T17:44:13Z","2021-12-16T17:09:02Z"
"","11582","KAFKA-13525: Implement KeyQuery in Streams IQv2","Implement the KeyQuery as proposed in KIP-796  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2021-12-09T03:58:40Z","2021-12-20T18:22:08Z"
"","12195","MINOR: implement BrokerRegistrationChangeRecord","Implement BrokerRegistrationChangeRecord as specified in KIP-746. This is a more flexible record than the single-purpose Fence / Unfence records.","closed","","cmccabe","2022-05-23T03:01:47Z","2022-06-01T23:33:08Z"
"","11893","KAFKA-13682; KRaft Controller auto preferred leader election","Implement auto leader rebalance for KRaft by keeping track of the set of topic partitions which have a leader that is not the preferred replica. If this set is non-empty then schedule a leader balance event for the replica control manager.  When applying `PartitionRecord`s and `PartitionChangeRecord`s to the `ReplicationControlManager`, if the elected leader is not the preferred replica then remember this topic partition in the set of `imbalancedPartitions`.  Anytime the quorum controller processes a `ControllerWriteEvent` it schedules a rebalance operation if the there are no pending rebalance operations, the feature is enabled and there are imbalance partitions.  This KRaft implementation only supports the configurations properties `auto.leader.rebalance.enable` and `leader.imbalance.check.interval.seconds`. The configuration property `leader.imbalance.per.broker.percentage` is not supported and ignored.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2022-03-14T21:22:51Z","2022-03-18T21:30:55Z"
"","11864","KAFKA-13717: skip coordinator lookup in commitOffsetsAsync if offsets is empty","If offsets is empty, no need to lookup coordinator. doCommitOffsetsAsync call will be completed locally.  Issue description: https://issues.apache.org/jira/browse/KAFKA-13717   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vincent81jiang","2022-03-08T04:06:37Z","2022-03-10T02:52:05Z"
"","11563","KAFKA-13461: Don't re-initialize ZK client session after auth failure if connection still alive","If JAAS configuration does not contain a Client section for ZK clients, an auth failure event is generated. If this occurs after the connection is setup in the controller, we schedule reinitialize(), which causes controller to resign. In the case where SASL is not mandatory and the connection is alive, controller maintains the current session and doesn't register its watchers, leaving it in a bad state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-12-02T19:47:18Z","2021-12-02T22:10:37Z"
"","11675","KAFKA-12648: POC for committing tasks on error","If a task has an exception while processing the it commits any tasks that were successfully processed before entering the error handling logic.  This is just the concept will include tests once the basic design is settled on  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-01-12T21:24:01Z","2022-01-25T20:15:39Z"
"","11711","MINOR: handle appending to configuration values that are empty better","If a configuration value is empty or contains only whitespace, appending to it should not result in a comma getting added to the configuration value. So, for example, adding ""first"" to "" "" should result in ""first"", not "" ,first"". Fix this and add a unit test.","open","","cmccabe","2022-01-25T03:56:19Z","2022-01-25T16:27:41Z"
"","11588","KAFKA-13485: Restart connectors after RetriableException raised from Task::start()","If a `RetriableException` is raised from `Task::start()`, this doesn't trigger an attempt to start that connector again. I.e. the restart functionality currently is only implemented for exceptions raised from `poll()/put()`. Triggering restarts also upon failures during `start()` would be desirable, so to circumvent temporary failure conditions like a network hickup which currrently require a manual restart of the affected tasks, if a connector for instance establishes a database connection during `start()`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","blcksrx","2021-12-09T17:02:28Z","2022-04-20T00:40:02Z"
"","11595","MINOR: timeout waitForBlock in connect BlockingConnectorTest","I've noticed some builds timing out on BlockingConnectorTest. This adds a timeout around the latch usage.","closed","","lbradstreet","2021-12-11T18:21:11Z","2021-12-15T14:25:17Z"
"","11672","KAFKA-13577: Replace easymock with mockito in kafka:core - part 1","I've fully replaced easymock with mockito in core. As it's a pretty large change, I'm splitting it in a few PRs (2 or 3) so reviewing it is not a complete nightmare.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-01-12T18:12:43Z","2022-02-09T16:02:32Z"
"","11921","MINOR: Small cleanups in the AclAuthorizer","I was reading the AclAuthorizer and I made a few small cleanups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-21T07:48:08Z","2022-03-21T10:23:34Z"
"","12208","MINOR: Improve code style in FenceProducersHandler","I was reading `FenceProducersHandler` and found a couple of code style inconsistencies. This PR fixes them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-25T09:26:22Z","2022-05-30T06:56:37Z"
"","12316","MINOR: Include kafka record inside RecordDeserializationException","I want to handle the record where serialize Exception occurred in errorHandler. Shouldn't the exception include the record?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","taehwa-song","2022-06-21T06:47:39Z","2022-06-27T09:29:56Z"
"","12124","rename isDone to hasComplete","I used deep learning to discover that the method 'isDone' changed in 4af50bb8600c37ee2e3597fba9a54a29cef94afa. The code snippet before and after the change is as follows. ``` public boolean isDone() {         return isDone;     } ```  ``` public boolean isDone() {         return result.get() != INCOMPLETE_SENTINEL;     } ``` We thought we could rename `isDone` to `hasComplete` with a more specific meaning.  We are actually looking at analysing the reasons for renaming  identifiers and we would like to get advice from more experienced people, so would appreciate your feedback from one of the following perspectives.  1. `(merge)`you accept this renaming opportunity and agree with the recommended name  2. `(agree and not recommend)`you accept the renaming but disagree with the recommended name, do you think the recommended name means the same thing or is the recommended name simply not suitable? 3. `(not agree and useful)`Would our suggested renaming opportunity be useful for your other refactoring activities? 4. `(not agree and not fix) `You do not think our suggested renaming opportunity is appropriate. Is this because you think the identifier meaning does not need to be renamed or is it some other issue? We look forward to your feedback!  Thank you!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","konL","2022-05-05T07:51:22Z","2022-05-18T18:49:49Z"
"","12349","KAFKA-14024: Consumer keeps Commit offset in onJoinPrepare in Cooperative rebalance","I think this is introduce in https://issues.apache.org/jira/browse/KAFKA-13310.    https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java#L752  we didn't wait for client to receive commit offset response here, so onJoinPrepareAsyncCommitCompleted will be false in cooperative rebalance, and client will loop in invoking onJoinPrepare.  i think EAGER mode don't have this problem because it will revoke the partitions even if onJoinPrepareAsyncCommitCompleted=false and will not try to commit next round.  Besides, there's also another bug found during fixing this bug. Before [KAFKA-13310](https://issues.apache.org/jira/browse/KAFKA-13310), we commitOffset sync with rebalanceTimeout, which will retry when retriable error until timeout. After [KAFKA-13310](https://issues.apache.org/jira/browse/KAFKA-13310), we thought we have retry, but we'll retry after partitions revoking. That is, even though the retried offset commit successfully, it still causes some partitions offsets un-committed, and after rebalance, other consumers will consume overlapping records.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aiquestion","2022-06-26T09:27:28Z","2022-07-21T02:39:06Z"
"","11927","KAFKA-13735/KAFKA-13736: Reenable SocketServerTest.closingChannelWithBufferedReceives and SocketServerTest.remoteCloseWithoutBufferedReceives","I think that those tests have been fixed by https://github.com/apache/kafka/commit/b27000ec6af6edfe8a6958dfcc3c0745667e25f4 so we can reenable them. I ran the CI 10 times (tests got run 50 times), no failures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-22T09:01:51Z","2022-03-29T08:24:50Z"
"","12232","MINOR:rm deprecated method","I think createIOThreadRatioMeterLegacy is deprecated, so we need instead of this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-05-31T14:31:19Z","2022-06-15T16:53:35Z"
"","11828","MINOR: Improve test assertions for IQv2","I saw one isolated test failure for these tests, but I couldn't confirm whether the result was missing a whole partition or whether the iterator for the partition was empty, so I'm adding the actual result to the failure message to tell the difference.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-03-01T23:03:13Z","2022-03-02T02:30:33Z"
"","11958","MINOR: Fix log4j entry in RepartitionTopics that have only one parameter while two required","I noticed two issues in the log4j entry:  1. It's formatted as ""{}...{}"" + param1, param2; effectively it is one param only, and the printed line is effectively mis-aligned: we always print `Subtopology [sourceTopics set] was missing source topics {}` 2. Even fix 1) is not enough, since `topologyName` may be null. On the other hand I think the original goal is not to print the topology name but the sub-topology id since it's within the per-sub-topology loop.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-03-28T22:32:45Z","2022-03-29T04:19:23Z"
"","11937","MINOR: A few cleanups in BrokerToControllerChannelManager","I made a few edits to make the code style more consistent while reading that file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-03-23T10:45:34Z","2022-03-24T10:29:10Z"
"","12147","MINOR: Rename remaining `zkVersion` to `partitionEpoch` in `PartitionTest`","I have missed a few in https://github.com/apache/kafka/commit/7c8c65fc54b9eb0386787632ff8315e097d99818.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-11T07:09:46Z","2022-05-17T06:58:47Z"
"","11747","MINOR: Clearer field names for ProducerIdsRecord and related classes","I found the current naming of the fields a little confusing in regard to whether the block range was inclusive or exclusive. This patch tries to improve naming to make this clearer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-10T22:07:35Z","2022-02-12T00:14:31Z"
"","12134","MINOR: Modify the string literal to the corresponding variable in RequestChannel.scala.","I found that the string literal is used even though the corresponding variable exists.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","runom","2022-05-07T09:21:41Z","2022-05-24T12:37:15Z"
"","12368","[MINOR] Dead code from BrokerEndPoint removed","I found that several `BrokerEndPoint` methods nod used in production code. It seems methods can be safely removed:  * `createBrokerEndPoint(Int, String)` * `readFrom(buffer: ByteBuffer)` * `connectionString(): String` * `writeTo(buffer: ByteBuffer)` * `sizeInBytes: Int`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2022-06-30T16:25:49Z","2022-07-01T17:29:22Z"
"","11887","KAFKA-13690: Fix flaky test in EosIntegrationTest","I found a couple of flakiness with the integration test.  1) IQv1 on stores failed although getting the store itself is covered with timeouts, since the InvalidStoreException is upon the query (`store.all()`). I changed to the util function with IQv2 whose timeout/retry covers the whole procedure. Example of such failure is: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11802/11/tests/  2) With ALOS we should not check that the output, as well as the state store content is exactly as of processed once, since it is possible that during processing we got spurious task-migrate exceptions and re-processed with duplicates. I actually cannot reproduce this error locally, but from the jenkins errors it seems possible indeed. Example of such failure is: https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-11433/4/tests/  3) Some minor cleanups.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-03-11T23:20:05Z","2022-03-14T22:42:11Z"
"","11753","MINOR: Add license header in suppressions.xml","I find there is no Apache License header in `suppressions.xml`, this pr is adding the license header.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ruanwenjun","2022-02-13T08:29:17Z","2022-03-16T08:51:37Z"
"","11895","MINOR: Disable those flaky tests","I collected a list of the most flaky tests observed lately, checked / created their corresponding tickets, and mark them as ignored for now. Many of these failures are:   0) Failing very frequently in the past (at least in my observations). 1) not investigated for some time. 2) have a PR for review (mostly thanks to @showuon !), but not reviewed for some time.  Since 0), these tests failures are hindering our development; and from 1/2) above, people are either too busy to look after them, or honestly the tests are not considered as providing values since otherwise people should care enough to panic and try to resolve. So I think it's reasonable to disable all these tests for now. If we later learned our lesson a hard way, it would motivate us to tackle flaky tests more diligently as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-03-14T22:30:03Z","2022-03-15T06:02:01Z"
"","11737","MINOR: Fix 3.1 upgrade notes for idempotence bug","I accidentally pulled in the 3.2 notes into the 3.1 branch after cherry-picking https://github.com/apache/kafka/pull/11691.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-07T18:02:10Z","2022-02-08T19:19:51Z"
"","12472","KAFKA-14134: Replace EasyMock with Mockito for WorkerConnectorTest","https://issues.apache.org/jira/browse/KAFKA-14134  From https://issues.apache.org/jira/browse/KAFKA-7438:  > Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to use.","open","connect,","yashmayya","2022-08-02T13:24:47Z","2022-08-03T09:17:06Z"
"","12225","KAFKA-13946: setMetadataDirectory() method in builder for ControllerNode has no parameters","https://issues.apache.org/jira/browse/KAFKA-13946  Added parameter `metadataDirectory` to `setMetadataDirectory()` so that `this.metadataDirectory` would not be set to itself.","closed","","clara0","2022-05-29T17:07:35Z","2022-05-30T22:55:07Z"
"","12083","A function is optimized to maintain a starting index for each node","https://issues.apache.org/jira/browse/KAFKA-13834","closed","","eyys","2022-04-22T04:47:25Z","2022-08-02T03:32:02Z"
"","12139","KAFKA-13821: Update Kafka Streams WordCount demo to new Processor API","https://issues.apache.org/jira/browse/KAFKA-13821  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tang7526","2022-05-09T12:13:00Z","2022-06-29T01:40:21Z"
"","11743","KAFKA-13660: Switch log4j12 to reload4j","https://issues.apache.org/jira/browse/KAFKA-13660  This bumps the slf4j version to 1.7.36 and swaps out log4j 1.2.17 with reload4j 1.2.19  Signed-off-by: Mike Lothian   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","FireBurn","2022-02-09T12:09:50Z","2022-03-31T13:02:11Z"
"","12074","HOTFIX: fix broken trunk due to conflicting and overlapping commits","https://github.com/apache/kafka/pull/11896 had a green Jenkins build, but before it was merged https://github.com/apache/kafka/pull/11993 was merged and both PRs conflict breaking the build.  This PR updates the changes of https://github.com/apache/kafka/pull/11896 to not use the method that was deprecated via https://github.com/apache/kafka/pull/11993.","closed","","mjsax","2022-04-20T20:30:39Z","2022-04-27T17:55:19Z"
"","11952","MINOR: Fix stream-join metadata","https://github.com/apache/kafka/pull/11356 inadvertently changed the (undefined) header forwarding behavior of stream-stream joins.  This change does not define the behavior, but just restores the prior undefined behavior for continuity's sake. Defining the header-forwarding behavior is future work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-03-25T20:01:59Z","2022-03-28T16:38:19Z"
"","12317","MINOR: Fix AlterPartitionManager topic id handling in response handler","https://github.com/apache/kafka/commit/f83d95d9a28267f7ef7a7b1e584dcdb4aa842210 introduced topic ids in the AlterPartitionRequest/Response and we just found a bug in the request handling logic. The issue is the following.  When the `AlterPartitionManager` receives the response, it builds the `partitionResponses` mapping `TopicIdPartition` to its result. `TopicIdPartition` is built from the response. Therefore if version < 2 is used, `TopicIdPartition` will have the `ZERO` topic id. Then the `AlterPartitionManager` iterates over the item sent to find their response. If an item has a topic id in its `TopicIdPartition` and version < 2 was used, it cannot find it because one has it and the other one has not.  This patch fixes the issue by using `TopicPartition` as a key in the `partitionResponses` map. This ensures that the result can be found regardless of the topic id being set or not.  Note that the case where version 2 is used is handled correctly because we already have logic to get back the topic name from the topic id in order to construct the `TopicPartition`.  `testPartialTopicIds` test was supposed to catch this but it didn't due to the ignorable topic id field being present. This patch fixes the test as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-06-21T06:50:54Z","2022-06-21T16:24:37Z"
"","12330","KAFKA-4279: REST API for filtering Connector plugins by type","https://cwiki.apache.org/confluence/display/KAFKA/KIP-850%3A+REST+API+for+filtering+Connector+plugins+by+type  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ashwinpankaj","2022-06-22T08:00:20Z","2022-06-24T03:18:20Z"
"","11722","KAFKA-13630: reduce amount of time that producer network thread holds batch queue lock","Hold the `deque` lock for only as long as is required to collect and make a decision in `ready()` and `drain()` loops. Once this is done, remaining work can be done without lock, so release it. This allows producers to continue appending.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jasonk000","2022-01-30T19:51:25Z","2022-03-09T17:15:29Z"
"","12413","MINOR: Upgrade to Gradle 7.5","Highlights: * The default Scala Zinc version was updated from 1.3.5 to 1.6.1 * Multiple Checkstyle tasks may now run in parallel within a project * Support for Java 18 * Much more responsive continuous builds on Windows and macOS * Improved diagnostics for dependency resolution  Some of our tests require java.util and java.lang modules to be open, so do it explicitly given the following Gradle bug fix:  > When running on Java 9+, Gradle no longer opens the java.base/java.util > and java.base/java.lang JDK modules for all Test tasks. In some cases, > this would cause code to pass during testing but fail at runtime.  Release notes: https://docs.gradle.org/7.5/release-notes.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-07-17T08:20:13Z","2022-07-28T14:37:46Z"
"","12340","KAFKA-14018:Support p12 with sha256","Hi, my first PR. Our partner changed the encryption algorithm of the p12  certificate from SHA1 to SHA256 for some reason. As a result, Kafka reported a connection error due to the wrong password.But we found the root cause is that the keytool of JDK8 does not support this encryption format.  Solve it with BouncyCastle","open","","gddsop","2022-06-24T09:03:50Z","2022-07-10T06:56:07Z"
"","12331","KAFKA-1194: changes needed to run on Windows","Hi,    I have come across [this comment](https://issues.apache.org/jira/browse/KAFKA-2170?focusedCommentId=17477226&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17477226) by [Maksim Zinal](https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mzinal).  > I've created a quick patch which seems to fix the issue on my system and on the Kafka version I use (2.8.0). >  > I believe that the reason of lock failures is the use of RandomAccessFile Java API to open the files in some cases, while in other cases FileChannel.open() is used instead. When opening files with RandomAccessFile under Windows, FILE_SHARE_DELETE flag is not set, which leads to ""access denied"" errors when trying to rename or delete the open files. FileChannel.open() sets the FILE_SHARE_DELETE by default, as I checked on JDK 8 and 11. >  > Here's the link to the branch based on tag 2.8.0: https://github.com/zinal/kafka/tree/2.8.0_KAFKA-1194 >  > Here are the exact changes implemented: https://github.com/zinal/kafka/compare/2.8.0...zinal:2.8.0_KAFKA-1194 (plus jcenter and grgit stuff needed to run the build).  This change addresses some ""access denied"" errors which occur on Windows. [KAFKA-1194](https://issues.apache.org/jira/browse/KAFKA-1194), [KAFKA-2427](https://issues.apache.org/jira/browse/KAFKA-2427), [KAFKA-6059](https://issues.apache.org/jira/browse/KAFKA-6059), [KAFKA-6188](https://issues.apache.org/jira/browse/KAFKA-6188), [KAFKA-6200](https://issues.apache.org/jira/browse/KAFKA-6200), [KAFKA-7575](https://issues.apache.org/jira/browse/KAFKA-7575), [KAFKA-8145](https://issues.apache.org/jira/browse/KAFKA-8145), [KAFKA-9458](https://issues.apache.org/jira/browse/KAFKA-9458)  Stackowerflow [45141541](https://stackoverflow.com/questions/45141541/kafka-topic-data-is-not-getting-deleted-in-windows), [45599625](https://stackoverflow.com/questions/45599625/kafka-unable-to-start-kafka-process-can-not-access-file-00000000000000000000), [48114040](https://stackoverflow.com/questions/48114040/exception-during-topic-deletion-when-kafka-is-hosted-in-docker-in-windows), [50755827](https://stackoverflow.com/questions/50755827/accessdeniedexception-when-deleting-a-topic-on-windows-kafka), [67555122](https://stackoverflow.com/questions/67555122/getting-java-nio-file-accessdeniedexception-on-kafka-on-windows)  **Testing** I deployed Kafka 2.8.1. with the original patch provided by [Maksim Zinal](https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mzinal) in January on ~50 Windows servers. Each Kafka instance has only one queue. Several processes per Kafka instance write their logs in to the queue and one process reads the queue and processed the data.  It seems that [these issues mentioned on stackowerflow](https://stackoverflow.com/questions/45599625/kafka-unable-to-start-kafka-process-can-not-access-file-00000000000000000000) by Windows users are gone.  I am currently testing Kafka 3.3. with the latest version of the patch which was modified according to @divijvaidya requests.  Kafka 3.3. log files  - [controller.log](https://github.com/apache/kafka/files/9072570/controller.log) - [server.log](https://github.com/apache/kafka/files/9072571/server.log) - [log-cleaner.log](https://github.com/apache/kafka/files/9072572/log-cleaner.log)  You can see in the logs that Kafka managed to delete timeindex files.   Thank you for your time.","open","","MPeli","2022-06-22T12:52:23Z","2022-07-25T07:46:36Z"
"","12446","MINOR : lower unregistered info to debug","Hi team,  This log gets generated quite often depending on the AK component we use, and to my knowledge does not need to be set to INFO, hence proposing we lower to DEBUG   Thank you   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","nicolasguyomar","2022-07-27T12:45:32Z","2022-07-27T12:45:32Z"
"","12394","Minor : Improve Exception log","Hi team,  The current wording of such exception :  `NotEnoughReplicasException: The size of the current ISR Set(2) is insufficient to satisfy the min.isr requirement of 2 for partition X`  Makes it look like the ISR size is 2 and SHOULD satisfy the min.isr, while 2 in the Set(2) means only broker.id=2 is in ISR  I did not find unit test to be updated that would check this exception wording  Submitting this minor PR to remove any confusion and improve the message, what do you think ?   Thank you  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","nicolasguyomar","2022-07-08T12:28:02Z","2022-07-19T16:40:27Z"
"","12378","MINOR : lower Metadata info log to debug for topic ID change","Hi team,  Minor suggestion to lower that new log to DEBUG as we used not to have any INFO log in this class and it looks like something has changed when you start a client while it's just the first metadata response result which is triggering the log with a somehow worrisome  change from null to an actual topicID, as if something was not OK before, because null, and now it's good  Let me know what you think  Thank you  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","nicolasguyomar","2022-07-05T11:17:04Z","2022-07-20T17:12:12Z"
"","11586","KAFKA-13516: Connection level metrics are not closed","Here is the fix. The core of this approach is using a `ConcurrentHashMap` to manage connection ids and their corresponding sensors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-12-09T13:04:42Z","2022-02-14T11:13:30Z"
"","11579","KAFKA-13518: Update gson dependency","Here is the fix. Since [spotbugs 4.5.1 was released just 12 hours ago](https://github.com/spotbugs/spotbugs/releases/tag/4.5.1), it would take a little bit to be synched with maven central.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-12-08T11:52:25Z","2022-06-11T13:43:56Z"
"","12430","KAFKA-14020: change method visibility","Hello, @artemlivshits As discussed in https://github.com/apache/kafka/pull/12365 the minor enhancement related to changing method visibility.  -- Regards, Eugene","open","","etolbakov","2022-07-22T07:27:44Z","2022-07-22T07:27:44Z"
"","12388","KAFKA-14013: Limit the length of the `reason` field sent on the wire","Hello David @dajac , James @jnh5y  I found an open JIRA ticket - https://issues.apache.org/jira/browse/KAFKA-14013 and decided to come up with a suggestion. Could you please take a look if you have a spare minute? Happy to adjust the changes if that's required.  -- Regards, Eugene","closed","","etolbakov","2022-07-07T07:23:22Z","2022-07-13T08:49:33Z"
"","12025","KAFKA-13802: handle uncaught exception","handle uncaught exception  [KAFKA-13802](https://issues.apache.org/jira/browse/KAFKA-13802)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoeCqupt","2022-04-10T17:31:35Z","2022-04-11T02:54:10Z"
"","11561","MINOR: Bump version of grgit to 4.1.1","grgit 4.1.0 caused unsupported version error during gradle builds. The reason was that grgit 4.1.0 uses always the latest JGit version internally. Unfortunately, the latest JGit version was compiled with a Java version later than Java 8 which caused the unsupported version error during gradle builds for Java 8.  grgit 4.1.1 fixed this issue by upper bounding the version of JGrit to a version that is still compiled with Java 8. Consequently, we can remove the hotfix we merged in PR #11554  and instead bump the grgit version from 4.1.0 to 4.1.1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-12-02T14:01:03Z","2021-12-09T12:49:22Z"
"","11587","HOTFIX: Bump version of grgit fro 4.1.0 to 4.1.1","grgit 4.1.0 caused the following error with gradle  > Could not resolve all artifacts for configuration ':classpath'.    > Could not resolve org.eclipse.jgit:org.eclipse.jgit:latest.release.      Required by:          project : > org.ajoberstar.grgit:grgit-core:4.1.0 ...  making it impossible to build the branch.   grgit 4.1.1 fixes this issue.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-12-09T14:33:40Z","2021-12-16T15:21:11Z"
"","12123","KAFKA-13830 Introduce metadata.version for KRaft","From https://github.com/apache/kafka/pull/12050:  > This patch includes a new metadata.version which is planned to replace IBP in KRaft clusters as specified in KIP-778. The kafka-storage tool now allows a user to specify a specific metadata.version to bootstrap into the cluster, otherwise the latest version is used. Upon the first leader election of the KRaft quroum, this initial metadata.version is written into the metadata log. When writing snapshots, a FeatureLevelRecord for metadata.version will be written out ahead of other records so we can decode things at the correct version level. This also includes additional validation in the controller when setting feature levels. It will now check that a given metadata.version is supportable by the quroum, not just the brokers.","closed","","ahuang98","2022-05-04T20:36:02Z","2022-05-06T04:21:55Z"
"","11660","MINOR: fix comment in ClusterConnectionStates","Found minor typo in ClusterConnectionStates.java  ### More detailed description of your change `iff` --> `if`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-01-09T07:08:19Z","2022-01-11T14:24:34Z"
"","12292","MINOR: KRaft nodes not shutdown correctly when using one controller in colocated mode","Found a bug in `KafkaService.stop_node`. `round(self.num_nodes_controller_role / 2)` rounds to the closest even choice. When only one controller is used in colocated mode, it rounds to zero. If the colocated controller is the first node shutdown, the remaining ones fails to stop within the 60s timeout because they are stuck in controlled shutdown. Those nodes are eventually killed. It is better to use `match.ceil` here.  We don't have any system tests affected by this though. It is still worth fixing in my opinion.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-06-14T07:59:04Z","2022-06-15T14:45:51Z"
"","11578","KAFKA-13514: Fix flakey testLargeAssignmentAndGroupWithNonEqualSubscription","For one `StickyAssignorTest#testLargeAssignmentAndGroupWithNonEqualSubscription` takes 60s is too long.   I think this test is trying to prove we can run large assignment with general sticky assignor. Using 1000 consumer with 1M partition should be able to verify it.  Reduce the timeout from 60s to 40s, and reduce the consumer count from 2000 -> 1000. In local env, before change, the time took 44s, after change, time took 16s.  If before change, in jenkins we took around 60s, we now, took around 20s.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-08T06:32:16Z","2022-01-06T07:26:03Z"
"","11751","MINOR: Add state directory to exceptions regarding state diretory","For debugging it is useful to see the actual state directory when an exception regarding the state directory is thrown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-02-12T21:15:22Z","2022-02-16T19:32:00Z"
"","11590","HOTFIX: fix failing StreamsMetadataStateTest tests","Followup to [#11562](https://github.com/apache/kafka/commit/e20f102298cc3e056e079cbd1cb33913a9ade0ce# ) to fix tests","closed","","ableegoldman","2021-12-10T00:14:44Z","2021-12-10T00:19:57Z"
"","11686","KAFKA-12648: invoke exception handler for MissingSourceTopicException with named topologies","Followup to #11600 to invoke the streams exception handler on the MissingSourceTopicException, without killing/replacing the thread","closed","","ableegoldman","2022-01-18T06:52:37Z","2022-01-25T18:37:35Z"
"","12158","MINOR:A few code cleanUps in KafkaController","Following variables in kafkaController are used for metric statistics： ```    offlinePartitionCount      preferredReplicaImbalanceCount     globalTopicCount      globalPartitionCount     topicsToDeleteCount      replicasToDeleteCount      ineligibleTopicsToDeleteCount      ineligibleReplicasToDeleteCount  ``` When Controller goes from active to non-active, these variables will be reset to 0. Currently, we will perform reset operations in `KafkaController.onControllerResignation() `and `KafkaController.updateMetrics()` . in fact, whether it is an active controller or a non-active controller, as long as it  receives events related to controller change, The method` KafkaController.updateMetrics()` will be executed, and decide whether to reset the above variables. So the reset operations in `KafkaController.onControllerResignation() ` can actually be removed.","closed","","bozhao12","2022-05-13T09:55:05Z","2022-05-17T22:40:05Z"
"","12041","MINOR: ignore unused configuration when ConsumerCoordinator is not constructed","Following PR #11940, ignore unused config when ConsumerCoordinator is not constructed  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-13T04:22:55Z","2022-05-13T18:49:57Z"
"","11999","[MINOR] fix(streams): align variable names","Following @dotjdk feedback https://github.com/apache/kafka/pull/10390#issuecomment-1080499059, this PR it's adjusting variables names.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2022-04-05T21:22:04Z","2022-04-11T21:17:02Z"
"","11617","KAFKA-13557: Remove swapResult from the public API","Follow-on from https://github.com/apache/kafka/pull/11582 . Removes a public API method in favor of an internal utility method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-12-20T18:44:53Z","2021-12-21T01:04:13Z"
"","12006","KAFKA-13794: Follow up to fix comparator","Follow up to original PR #11991   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-04-07T01:39:34Z","2022-04-28T13:26:57Z"
"","12008","KAFKA-13439: move upgrade note to stream upgrade doc","Follow up on #11490   After change: ![image](https://user-images.githubusercontent.com/43372967/162137459-da233926-a48a-4437-8296-07d5269039ee.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2022-04-07T06:50:11Z","2022-04-07T06:50:43Z"
"","11627","KAFKA-13565: add consumer exponential backoff for KIP-580","Follow up https://github.com/apache/kafka/pull/8846, to complete the Implementation of [KIP-580](https://cwiki.apache.org/confluence/display/KAFKA/KIP-580%3A+Exponential+Backoff+for+Kafka+Clients): add consumer exponential backoff  Updated classes: 1.  Fetcher 2. Metadata 3. AbstractCoordinator 4. ConsumerCoordinator 5. SubscriptionState   Co-Authored-By: Cheng Tan    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-12-26T07:40:10Z","2022-03-02T12:54:12Z"
"","11674","KAFKA-13577: Replace easymock with mockito in kafka:core - part 3","Follow up from https://github.com/apache/kafka/pull/11672 and https://github.com/apache/kafka/pull/11673  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-01-12T18:17:46Z","2022-02-11T15:16:28Z"
"","11673","KAFKA-13577: Replace easymock with mockito in kafka:core - part 2","Follow up from https://github.com/apache/kafka/pull/11672  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-01-12T18:16:55Z","2022-02-10T08:16:01Z"
"","11509","KAFKA-13370: add unit test for offset-commit metrics","Follow up for https://github.com/apache/kafka/pull/11413, to add tests for offset-commit metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-11-17T08:20:34Z","2021-11-19T06:22:12Z"
"","12404","MINOR: Fix QueryResult Javadocs","Fixes the `QueryResult` javadocs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2022-07-14T10:01:26Z","2022-07-18T11:39:34Z"
"","11609","KAFKA-12648: fixes for query APIs with named topologies","Fixes some issues with the NamedTopology version of the IQ methods that accept a `topologyName` argument, and adds tests for all.","closed","","ableegoldman","2021-12-17T05:35:32Z","2022-01-25T13:49:24Z"
"","12326","MINOR: fix no_op_record exception when running /bin/kafka-metadata-shell.sh","Fixes exception that appears due to record type of NO_OP_RECORD when running /bin/kafka-metadata-shell.sh","open","","jmannooparambil","2022-06-21T19:55:39Z","2022-06-25T02:37:05Z"
"","11630","MINOR: Remove duplicate IQv2 query handler","Fixes duplicate query result handling logic that was introduced in https://github.com/apache/kafka/pull/11598  During the implementation of #11598, this logic was moved from the  stores into `StoreQueryUtils.handleBasicQueries`, but we overlooked removing this block.  Thanks to @patrickstuedi  for pointing this out during the review of https://github.com/apache/kafka/pull/11567  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-12-28T03:24:13Z","2022-01-03T04:20:29Z"
"","11995","KAFKA-13782; Ensure correct partition added to txn after abort on full batch","Fixes a regression introduced in https://github.com/apache/kafka/pull/11452. Following [KIP-480](https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner), the `Partitioner` will receive a callback when a batch has been completed so that it can choose another partition. Because of this, we have to wait until the batch has been successfully appended to the accumulator before adding the partition in `TransactionManager.maybeAddPartition`. This is still safe because the `Sender` cannot dequeue a batch from the accumulator until it has been added to the transaction successfully.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-04-04T20:04:24Z","2022-04-06T05:20:19Z"
"","11992","MINOR: Fix wrong configuration of `inter.broker.security.protocol` for communication between brokers","Fix wrong configuration of inter.broker.security.protocol for communication between brokers","closed","","RivenSun2","2022-04-03T08:03:57Z","2022-04-04T15:27:34Z"
"","11928","KAFKA-13739: Sliding window with no grace period not working","Fix upperbound for sliding window, making it compatible with no grace period (kafka-13739)  Added unit test for early sliding window and ""normal"" sliding window for both events within one time difference (small input) and above window time difference (large input).  Fixing this window interval may slightly change stream behavior but probability to happen is extremely slow and may not have a huge impact on the result given.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","tiboun","2022-03-22T09:39:17Z","2022-03-31T20:27:40Z"
"","12003","MINOR: Improve Gradle Caching and Fix Deprecations","Fix UP-TO-DATE check in `create*VersionFile` tasks - `create*VersionFile` tasks explicitly declared output UP-TO-DATE status as being false. This change properly sets the inputs to `create*VersionFile` tasks to the `commitId` and `version` values and sets `receiptFile` locally rather than in an extra property.  Enable output caching for `process*Messages` tasks - `process*Messages` tasks did not have output caching enabled. This change enables that caching, as well as setting a property name and RELATIVE path sensitivity.  Fix existing Gradle deprecations  - Replaces `JavaExec#main` with `JavaExec#mainClass` - Replaces `Report#destination` with `Report#outputLocation` - Adds a `generator` configuration to projects that need to resolve the `generator` project (rather than referencing the runtimeClasspath of the `generator` project from other project contexts.  These changes were found using Gradle Build Scans:  Before: https://scans.gradle.com/s/cxiwbgk3rvxf6 After: https://scans.gradle.com/s/3tg777zuldq5k  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","clayburn","2022-04-06T13:17:48Z","2022-04-21T15:25:20Z"
"","12007","MINOR: Fix method javadoc and typo in comments","Fix typo in Fetcher.java and method annotation in SubscriptionState.java  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-04-07T02:35:28Z","2022-04-07T10:16:12Z"
"","12002","MINOR: Fix typo and method annotation","Fix typo in Fetcher.java  and method annotation in SubscriptionState.java ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-04-06T11:07:17Z","2022-04-07T09:02:23Z"
"","12016","KAFKA-9981:  Mirrormaker2 stop sync data when modify topic partition in ""Running a dedicated MirrorMaker cluster"" mode","fix the https://issues.apache.org/jira/browse/KAFKA-9981","closed","","yangl","2022-04-08T09:26:08Z","2022-04-25T02:43:47Z"
"","11569","DOCS-9992: Fix method name in table-table FK left join example","Fix the foreign-key left join example to use the `leftJoin` method.  cc @mikebin","open","","JimGalasyn","2021-12-06T20:48:21Z","2021-12-06T20:48:47Z"
"","11713","MINOR: Fix bug of empty position in windowed and session stores","Fix the bug where the Metered window and session stores are returning an empty position although the query result contains the correct position. Also added checks in the store integration test about the position.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vpapavas","2022-01-25T17:20:18Z","2022-01-25T19:47:07Z"
"","12205","MINOR: Fix missing UNREGISTER_BROKER call in KafkaApis","Fix some bugs in the KRaft unregisterBroker API and add a junit test.      1. kafka-cluster-tool.sh unregister should fail if no broker ID is passed.      2. UnregisterBrokerRequest must be marked as a KRaft broker API so that KRaft brokers can receive it.      3. KafkaApis.scala must forward UNREGISTER_BROKER to the controller.","closed","","cmccabe","2022-05-24T20:39:33Z","2022-05-26T21:08:23Z"
"","11519","MINOR: Update error log in `DefaultSslEngineFactory#createTrustStoreFromPem`","fix log message to match correctly indicate truststore instead of keystore.","closed","","defhacks","2021-11-19T21:15:02Z","2021-11-25T02:29:59Z"
"","11591","KAFKA-12648: fix IllegalStateException in ClientState after removing topologies","Fix for one of the causes of failure in the NamedTopologyIntegrationTest: `org.apache.kafka.streams.errors.StreamsException: java.lang.IllegalStateException: Must initialize prevActiveTasks from ownedPartitions before initializing remaining tasks.`  This exception could occur if a member sent in a subscription where all of its `ownedPartitions` were from a named topology that is no longer recognized by the group leader, eg because it was just removed from the client. We should filter each ClientState based on the current topology only so the assignor only processes the partitions/tasks it can identify. The member with the out-of-date tasks will eventually clean them up when the `#removeNamedTopology` API is invoked on them","closed","","ableegoldman","2021-12-10T08:30:43Z","2021-12-10T22:26:27Z"
"","11742","KAFKA-13636: Fix for the group coordinator issue where the offsets are deleted for unstable groups","Fix for bug where group coordinator can garbage collect offsets for groups that are in flux: https://issues.apache.org/jira/browse/KAFKA-13636)  cc: @dajac  Tested: Added a test; ensured that it fails without the patch and passes with the path. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","prince-mahajan","2022-02-09T05:35:34Z","2022-02-10T16:38:00Z"
"","11898","KAFKA-7540: commit offset sync before close","Fix flaky `ConsumerBounceTest#testClose` test.  In this test, we tried to close the consumer, and then start another one, to make sure the new consumer will start to consume records from the previously committed offsets.  We intended to commit offsets before consumer close, to preserve the commit offsets. In https://github.com/apache/kafka/pull/11340#discussion_r742512311 , we accidentally refactor it by commiting async during fixing another bug. If we commit async before close, we will send out a commit request without blocking the close process, and then, the request might fail sending to the broker due to client closed (connection is broken), which cause the commit never succeeded.   To fix this issue, we have to commit synchronously before closing the consumer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-15T08:36:18Z","2022-03-21T08:51:22Z"
"","12243","MINOR: fix doc","fix doc and improve code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-06-02T18:17:22Z","2022-06-03T07:56:13Z"
"","12107","MINOR: fix ClientQuotasRequestTest.testAlterClientQuotasBadIp","Fix ClientQuotasRequestTest.testAlterClientQuotasBadIp so that it uses actually unresolvable hostnames. The previous choices ""ip"" and ""abc-123"" are now resolvable.","closed","","cmccabe","2022-04-30T01:14:22Z","2022-05-02T16:06:19Z"
"","11866","MINOR: Offset Result is separate","fix a rename issue so that blocking on offset reset is possible   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-03-08T21:30:14Z","2022-03-10T17:30:29Z"
"","11696","MINOR: fix bug in AbstractFetcherManagerTest on INFO level","Fix a bug where AbstractFetcherManagerTest would fail with ""Unexpected method call AbstractFetcherThread.sourceBroker()"" when core logging was turned on.","closed","","cmccabe","2022-01-20T21:25:23Z","2022-01-21T18:07:23Z"
"","11948","KAFKA-10405: set purge interval explicitly","Failed message: ``` org.opentest4j.AssertionFailedError: Condition not met within timeout 60000. Repartition topic restore-test-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition not purged data after 60000 ms. ==> expected:  but was:  	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:55) 	at org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:40) 	at org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:210) 	at org.apache.kafka.test.TestUtils.lambda$waitForCondition$3(TestUtils.java:320) 	at org.apache.kafka.test.TestUtils.retryOnExceptionWithTimeout(TestUtils.java:368) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:317) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:301) 	at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:291) 	at org.apache.kafka.streams.integration.PurgeRepartitionTopicIntegrationTest.shouldRestoreState(PurgeRepartitionTopicIntegrationTest.java:213) ```   In [KIP-811](https://cwiki.apache.org/confluence/display/KAFKA/KIP-811%3A+Add+config+repartition.purge.interval.ms+to+Kafka+Streams), we added a new config `repartition.purge.interval.ms` to set repartition purge interval. In this flaky test, we expected the purge interval is the same as commit interval, which is not correct anymore (default is 30 sec). Set the purge interval explicitly to fix this issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-25T08:30:49Z","2022-03-26T02:45:55Z"
"","12295","KAFKA-13586: Prevent exception thrown during connector update from crashing distributed herder","Exceptions thrown by `ConfigProvider`s cause Connect's `DistributedHerder` to crash when configs are updated. This appears to be a bug in `processConnectorConfigUpdates`; every other call to `startConnector` is wrapped in a try-catch to prevent immediate exceptions from crashing the herder.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","dstelljes","2022-06-15T01:24:08Z","2022-07-27T22:02:45Z"
"","11884","unpin ducktape dependency to always use the newest version (py3 edition)","Ensures we always have the latest published ducktape version. This way whenever we release a new one, we won't have to cherry pick a bunch of commits across a bunch of branches.  Tested via branch builder: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4801/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2022-03-11T09:27:43Z","2022-03-11T12:18:19Z"
"","11883","Unpin ducktape","Ensures we always have the latest published ducktape version (compatible with python 2). This way whenever we release a new one, we won't have to cherry pick a bunch of commits across a bunch of branches.  Tested via branch builder: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4800/  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2022-03-11T09:23:20Z","2022-03-11T12:17:00Z"
"","12277","KAFKA-13971:Atomicity violations caused by improper usage of ConcurrentHashMap","ensure thread safe  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","Kvicii","2022-06-09T16:40:55Z","2022-07-26T02:03:47Z"
"","12033","KAFKA-13807: Fix incrementalAlterConfig and refactor some things","Ensure that we can set log.flush.interval.ms at the broker or cluster level via IncrementalAlterConfigs. This was broken by KAFKA-13749, which added log.flush.interval.ms as the second synonym rather than the first. Add a regression test to DynamicConfigChangeTest.  Create ControllerRequestContext and pass it to every controller API. This gives us a uniform way to pass through information like the deadline (if there is one) and the Kafka principal which is making the request (in the future we will want to log this information).  In ControllerApis, enforce a timeout for broker heartbeat requests which is equal to the heartbeat request interval, to avoid heartbeats piling up on the controller queue. This should have been done previously, but we overlooked it.  Add a builder for ClusterControlManager and ReplicationControlManager to avoid the need to deal with a lot of churn (especially in test code) whenever a new constructor parameter gets added for one of these.  In ControllerConfigurationValidator, create a separate function for when we just want to validate that a ConfigResource is a valid target for DescribeConfigs. Previously we had been re-using the validation code for IncrementalAlterConfigs, but this was messy.","closed","kip-500,","cmccabe","2022-04-11T23:50:06Z","2022-04-15T23:07:26Z"
"","12176","MINOR: Enable KRaft in `TransactionsTest`","Enable support for KRaft in `TransactionsTest`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2022-05-18T18:02:08Z","2022-05-18T21:08:00Z"
"","12217","MINOR: Enable kraft support in quota integration tests","Enable kraft support in `BaseQuotaTest` and its extensions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2022-05-26T01:16:20Z","2022-06-01T23:56:10Z"
"","11619","MINOR: allocate 2MB to offset map in connect EmbeddedKafkaCluster","EmbeddedKafkaCluster in other projects use 2MB for their offset map to reduce memory consumption in test runs. Generally we allocate multiple of these offset maps, one for each broker.","closed","connect,","lbradstreet","2021-12-21T17:31:46Z","2022-02-02T14:10:07Z"
"","11624","KAFKA-13553: add PAPI KV store tests for IQv2","During some recent reviews, @mjsax pointed out that StateStore layers are constructed differently the stores are added via the PAPI vs. the DSL.  This PR adds PAPI construction to the IQv2StoreIntegrationTest so that we can ensure IQv2 works on every possible state store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-12-22T18:45:27Z","2022-01-06T03:04:40Z"
"","11650","KAFKA-13553: Add PAPI Window and Session store tests for IQv2","During some recent reviews, @mjsax pointed out that StateStore layers are constructed differently the stores are added via the PAPI vs. the DSL.  This PR adds PAPI construction for Window and Session stores to the IQv2StoreIntegrationTest so that we can ensure IQv2 works on every possible state store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-01-05T21:47:59Z","2022-01-06T05:16:37Z"
"","12279","KAFKA-10199: Commit the restoration progress within StateUpdater","During restoring, we should always commit a.k.a. write checkpoint file regardless of EOS or ALOS, since if there's a failure we would just over-restore them upon recovery so no EOS violations happened.  Also when we complete restore or remove task, we should enforce a checkpoint as well; for failing cases though, we should not write a new one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-10T01:20:58Z","2022-06-24T17:27:18Z"
"","12088","KAFKA-12841 bug fix and tests","Duplicated to https://github.com/apache/kafka/pull/12064 - but the tests aren't being fired up there. So created this PR to trigger the tests. The bug was introduced in https://github.com/apache/kafka/pull/11689 that an additional onAcknowledgement was made using the InterceptorCallback class. This is undesirable since onSendError will attempt to call onAcknowledgement once more.  Committer Checklist (excluded from commit message)  Verify design and implementation  Verify test coverage and CI build status  Verify documentation (including upgrade notes)","closed","","philipnee","2022-04-22T17:52:26Z","2022-04-26T02:45:31Z"
"","11618","KAFKA-13558: NioEchoServer fails to close resources","Due to resource leaks in the `NioEchoServer`, at times it won't start properly to accept clients and will throw an exception in the `ServerSocketChannel.accept()` call. Previous to this change, the error was not being logged. The logged error was that there were too many open files.  Using the `UnixOperatingSystemMXBean`, I was able to detect that use of the `NioEchoServer` creates several file descriptors but does not close them. This then caused the client to never be able to connect to the server, so the waitForCondition failed intermittently.  This change closes the internal `Selector` and the `AcceptorThread`'s selector so that the file descriptors are reclaimed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-12-20T18:52:53Z","2022-02-02T14:11:56Z"
"","11730","MINOR: Use ducktape version 0.7.17","Ducktape 0.7.17 locked a couple of dependencies to their last py2 compatible versions - this ensures more or less stable python 2 builds for kafkatest (until some other package breaks it). Since 2.7 and above uses ducktape 0.8.x and python 3, this change does not apply there.  See these PRs for details on ducktape: https://github.com/confluentinc/ducktape/pull/291 https://github.com/confluentinc/ducktape/pull/292  Tested: - Ran `python setup.py develop` in a clean python 2.7.18 virtualenv to ensure all dependencies are installed without conflicts - Ran a small system test with `ducker-ak` just to ensure it still works ok     - had to hack the dockerfile since openjdk:8 image does not include py2 anymore it seems? Hacked something together to have some test run (it failed due to some java deps it seems, but ducktape part worked ok)     - because of this, the change to the Dockerfile is pretty much irrelevant - Ran a system test job on jenkins - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4776      - used a custom test that simply brings up zk and kafka  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2022-02-03T02:57:31Z","2022-02-03T11:51:15Z"
"","12280","MINOR: Reuse gradle daemon for scala compilation (WIP)","Draft PR testing the keep alive option for the gradle daemon when compiling Scala code (only present in the gradle nightly atm).  After changing a single line in a test file in the streams-scala module, the time to compile was reduced from 5.8 seconds to 1 second by changing the keep alive option from `SESSION` to `DAEMON`.  `DAEMON` keep alive option: > Task :streams:streams-scala:compileTestScala > Watching 860 directories to track changes > Caching disabled for task ':streams:streams-scala:compileTestScala' because: >   Build cache is disabled > Task ':streams:streams-scala:compileTestScala' is not up-to-date because: >   Input property 'source' file /home/ijuma/src/kafka/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/JoinedTest.scala has changed. > Watching 852 directories to track changes > Watching 851 directories to track changes > Compiling with Zinc Scala compiler. > Prepared Zinc Scala inputs: 0.005 secs > compiling 16 Scala sources to /home/ijuma/src/kafka/streams/streams-scala/build/classes/scala/test ... > done compiling > Completed Scala compilation: 0.964 secs > Watching 859 directories to track changes > Watching 860 directories to track changes > :streams:streams-scala:compileTestScala (Thread[Execution worker Thread 35,5,main]) completed. Took 0.991 secs.  `SESSION` keep alive option (the default):  > Task :streams:streams-scala:compileTestScala > Watching 860 directories to track changes > Caching disabled for task ‘:streams:streams-scala:compileTestScala’ because: >   Build cache is disabled > Task ‘:streams:streams-scala:compileTestScala’ is not up-to-date because: >   Input property ‘source’ file /home/ijuma/src/kafka/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/JoinedTest.scala has changed. > Watching 852 directories to track changes > Watching 851 directories to track changes > Starting process ‘Gradle Worker Daemon 6’. Working directory: /home/ijuma/.gradle/workers Command: /usr/java/jdk-17.0.2/bin/java -Xss4m -XX:+UseParallelGC --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.nio.file=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.regex=ALL-UNNAMED --add-opens=java.base/java.util.stream=ALL-UNNAMED --add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.base/java.time=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED @/home/ijuma/.gradle/.tmp/gradle-worker-classpath1215126969431063029txt -Xmx2048m -Dfile.encoding=UTF-8 -Duser.country=US -Duser.language=en -Duser.variant [worker.org](http://worker.org/).gradle.process.internal.worker.GradleWorkerMain ‘Gradle Worker Daemon 6’ > Successfully started process ‘Gradle Worker Daemon 6’ > Started Gradle worker daemon (0.227 secs) with fork options DaemonForkOptions{executable=/usr/java/jdk-17.0.2/bin/java, minHeapSize=null, maxHeapSize=2048m, jvmArgs=[-Xss4m, -XX:+UseParallelGC, --add-opens=java.base/java.io=ALL-UNNAMED, --add-opens=java.base/java.nio=ALL-UNNAMED, --add-opens=java.base/java.nio.file=ALL-UNNAMED, --add-opens=java.base/java.util.concurrent=ALL-UNNAMED, --add-opens=java.base/java.util.regex=ALL-UNNAMED, --add-opens=java.base/java.util.stream=ALL-UNNAMED, --add-opens=java.base/java.text=ALL-UNNAMED, --add-opens=java.base/java.time=ALL-UNNAMED, --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED], keepAliveMode=SESSION}. > Compiling with Zinc Scala compiler. > Prepared Zinc Scala inputs: 0.183 secs > compiling 16 Scala sources to /home/ijuma/src/kafka/streams/streams-scala/build/classes/scala/test ... > done compiling > Completed Scala compilation: 5.256 secs > Watching 859 directories to track changes > Watching 860 directories to track changes > :streams:streams-scala:compileTestScala (Thread[Execution worker Thread 29,5,main]) completed. Took 5.79 secs.  See https://github.com/gradle/gradle/issues/20579 for more details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ijuma","2022-06-10T03:30:41Z","2022-06-10T16:09:35Z"
"","12354","Kafka 13816: Downgrading Connect rebalancing protocol from incremental to eager causes duplicate task instances","Downgrading Connect rebalancing protocol from incremental to eager causes duplicate task instances. This PR aims to address this issue .","open","","vamossagar12","2022-06-27T12:16:08Z","2022-07-28T01:31:17Z"
"","11981","KAFKA-13791: Fix FetchResponse#`fetchData` and `forgottenTopics`: Assignment of lazy-initialized members should be the last step with double-checked locking","Double-checked locking can be used for lazy initialization of volatile fields, but only if field assignment is the last step in the synchronized block. Otherwise, you run the risk of threads accessing a half-initialized object.  The problem is consistent with [KAFKA-13777](https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-13777)  #https://github.com/apache/kafka/pull/11963","closed","","yun-yun","2022-04-01T02:52:41Z","2022-04-05T07:27:32Z"
"","11885","MINOR: Bump latest 3.0 version to 3.0.1","Do not merge until the 3.0.1 artifacts appear in Maven  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-03-11T10:33:18Z","2022-03-16T10:43:40Z"
"","11788","KAFKA-13673: disable idempotence when config conflicts","Disable idempotence when conflicting config values for `acks`, `retries` and `max.in.flight.requests.per.connection` are set by the user. For the former two configs, we log at `info` level when we disable idempotence due to conflicting configs. For the latter, we log at `warn` level since it's due to an implementation detail that is likely to be surprising.  This mitigates compatibility impact of enabling idempotence by default.  Added unit tests to verify the change in behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-02-17T13:16:31Z","2022-03-03T13:55:15Z"
"","11490","KAFKA-13439: Deprecate eager rebalance protocol in kafka stream","Deprecate eager rebalance protocol in kafka stream and log warning message when `upgrade.from` is set to 2.3 or lower. Also add a note in `upgrade` doc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-11-12T07:04:01Z","2022-04-07T06:28:16Z"
"","12479","Convert DeleteTopicTest, DeleteTopicsRequestWithDeletionDisabledTest, and RackAwareAutoTopicCreationTest to run in KRaft mode","DeleteTopicsRequestWithDeletionDisabledTest and RackAwareAutoTopicCreationTest are still a work in progress  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ahuang98","2022-08-03T14:51:17Z","2022-08-03T16:33:36Z"
"","11628","Delete docs/documentation directory","Deleted empty duplicate documentation directory  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  The files in the docs/documentation/streams folder (and index) were empty duplicates of the ones in docs/streams (and index)   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Not tested   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kamelCased","2021-12-26T10:43:37Z","2021-12-26T10:43:37Z"
"","11649","KAFKA-13646: Implement KIP-801: KRaft authorizer","Currently, when using KRaft mode, users still have to have an Apache ZooKeeper instance if they want to use AclAuthorizer. We should have a built-in Authorizer for KRaft mode that does not depend on ZooKeeper. This PR introduces such an authorizer, called StandardAuthorizer. See KIP-801 for a full description of the new Authorizer design.  Authorizer.java: add aclCount API as described in KIP-801. StandardAuthorizer is currently the only authorizer that implements it, but eventually we may implement it for AclAuthorizer and others as well.  ControllerApis.scala: fix a bug where createPartitions was authorized using CREATE on the topic resource rather than ALTER on the topic resource as it should have been.  QuorumTestHarness: rename the controller endpoint to CONTROLLER for consistency (the brokers already called it that). This is relevant in AuthorizerIntegrationTest where we are examining endpoint names. Also add the controllerServers call.  TestUtils.scala: adapt the ACL functions to be usable from KRaft, by ensuring that they use the Authorizer from the current active controller.  BrokerMetadataPublisher.scala: add broker-side ACL application logic.  Controller.java: add ACL APIs. Also add a findAllTopicIds API in order to make junit tests that use KafkaServerTestHarness#getTopicNames and KafkaServerTestHarness#getTopicIds work smoothly.  AuthorizerIntegrationTest.scala: convert over testAuthorizationWithTopicExisting (more to come soon)  QuorumController.java: add logic for replaying ACL-based records. This means storing them in the new AclControlManager object, and integrating them into controller snapshots. It also means applying the changes in the Authorizer, if one is configured. In renounce, when reverting to a snapshot, also set newBytesSinceLastSnapshot to 0.  There are a few follow-up items not in this PR. One is actually creating the new acl count metric. Another is implementing the ""start after we reach the high water mark"" authorizer logic. Also, this doesn't implement the new busy error code. Finally, we need to implement logIfDenied.","closed","kip-500,","cmccabe","2022-01-05T19:15:04Z","2022-02-09T18:38:52Z"
"","11575","KAFKA-13511: Add support for different unix precisions in TimestampConverter SMT","Currently, the SMT TimestampConverter can convert Timestamp from either source String, Long or Date into target String, Long or Date.  The problem is that Long source or target is required to be epoch in milliseconds.  In many cases, epoch is represented with different precisions. This leads to several Jira tickets :  [KAFKA-12364](https://issues.apache.org/jira/browse/KAFKA-12364): add support for date from int32 to timestampconverter [KAFKA-10561](https://issues.apache.org/jira/browse/KAFKA-10561): Support microseconds precision for Timestamps I propose to add a new config to TimestampConverter called ""epoch.precision"" which defaults to ""millis"" so as to not impact existing code, and allows for more precisions : seconds, millis, micros. ````json ""transforms"": ""TimestampConverter"", ""transforms.TimestampConverter.type"": ""org.apache.kafka.connect.transforms.TimestampConverter$Value"", ""transforms.TimestampConverter.field"": ""event_date"", ""transforms.TimestampConverter.epoch.precision"": ""micros"", ""transforms.TimestampConverter.target.type"": ""Timestamp"" ```` Exactly like ""format"" field which is used as input when the source in String and output when the target.type is string, this new field would be used as input when the field is Long, and as output when the target.type is ""unix""  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","twobeeb","2021-12-07T15:41:40Z","2022-02-22T16:17:16Z"
"","12078","Minor:Fix method scalaDoc in DelayedFetch.scala","Currently, the consumer supports pulling data from the leader replica,  and also supports pulling data from the follower replica.  Therefore, Whether fetching data from the leader or the follower, we have verified the leaderEpoch in the tryComplete method, so the method scalaDoc also needs to be updated","open","","bozhao12","2022-04-21T06:53:18Z","2022-05-09T17:11:09Z"
"","11657","KAFKA-13552: Fix BROKER and BROKER_LOGGER in KRaft","Currently, KRaft does not support setting BROKER_LOGGER configs (it always fails.) Additionally, there are several bugs in the handling of BROKER configs. They are not properly validated on the forwarding broker, and the way we apply them is buggy as well. This PR fixes those issues.  KafkaApis: add support for doing validation and log4j processing on the forwarding broker. This involves breaking the config request apart and forwarding only part of it. Adjust KafkaApisTest to test the new behavior, rather than expecting forwarding of the full request.  MetadataSupport: remove MetadataSupport#controllerId since it duplicates the functionality of MetadataCache#controllerId. Add support for getResourceConfig and maybeForward.  ControllerApis: log an error message if the handler throws an exception, just like we do in KafkaApis.  ControllerConfigurationValidator: add JavaDoc.  Move some functions that don't involve ZK from ZkAdminManager to DynamicConfigManager. Move some validation out of ZkAdminManager and into a new class, ConfigAdminManager, which is not tied to ZK.  ForwardingManager: add support for sending new requests, rather than just forwarding existing requests.  BrokerMetadataPublisher: do not try to apply dynamic configurations for brokers other than the current one. Log an INFO message when applying a new dynamic config, like we do in ZK mode. Also, invoke reloadUpdatedFilesWithoutConfigChange when applying a new non-default BROKER config.  QuorumController: fix a bug in ConfigResourceExistenceChecker which prevented cluster configs from being set. Add a test for this class.  Rename DynamicConfigManager to ZkConfigManager, since it is responsible for managing configurations in ZK.  Fix the handling of cluster configs in ZkConfigRepository.  Fix the a bug which prevented dynamically reconfiguring the log cleaner in KRaft mode. We were checking whether the cleaner was null prior to invoking LogManager#startup, which caused the cleaner to always be null.","closed","kip-500,","cmccabe","2022-01-07T19:16:01Z","2022-01-22T00:00:22Z"
"","11682","KAFKA-13524: Add IQv2 query handling to the caching layer","Currently, IQv2 forwards all queries to the underlying store. We add this bypass to allow handling of key queries in the cache. If a key exists in the cache, it will get answered from there. As part of this PR, we realized we need access to the position of the underlying stores. So, I added the method `getPosition` to the public API and ensured all state stores implement it. Only the ""leaf"" stores (Rocks*, InMemory*) have an actual position, all wrapping stores access their wrapped store's position.   Ran IQv2StoreIntegrationTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vpapavas","2022-01-17T09:45:18Z","2022-01-26T15:36:40Z"
"","11735","MINOR: Check the help and version options firstly","Currently, `bin/kafka-consumer-groups.sh --version` reqiures unnecessary `bootstrap-server` option. ``` % bin/kafka-consumer-groups.sh --version Missing required argument ""[bootstrap-server]"" Option                                  Description ------                                  ----------- --all-groups                            Apply to all consumer groups. --all-topics                            Consider all topics assigned to a                                           group in the `reset-offsets` process. ... --version                               Display Kafka version. ```  ``` % bin/kafka-consumer-groups.sh --version --bootstrap-server=localhost:9092 3.2.0-SNAPSHOT (Commit:21c3009ac12f79d0) ```  This PR fixes this problem.  ``` % bin/kafka-consumer-groups.sh --version 3.2.0-SNAPSHOT (Commit:efe4bfd2c49997f7) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","runom","2022-02-05T07:58:59Z","2022-05-08T05:29:30Z"
"","12032","MINOR: Change `AlterPartition` validation order in `KafkaController`","Currently we validate recovery state before checking leader epoch. It seems more intuitive to validate leader epoch first since the leader might be working with stale state. This patch fixes this and adds a couple additional validations.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-04-11T22:59:56Z","2022-04-25T16:37:03Z"
"","12289","KAFKA-13957: Fix flaky shouldQuerySpecificActivePartitionStores test","Currently the tests fail because there is a missing predicate in the `retrievableException` which causes the test to fail, i.e. the current predicates  ```java containsString(""Cannot get state store source-table because the stream thread is PARTITIONS_ASSIGNED, not RUNNING""), containsString(""The state store, source-table, may have migrated to another instance""), containsString(""Cannot get state store source-table because the stream thread is STARTING, not RUNNING"") ``` wasn't complete. Another one needed to be added, namely ""The specified partition 1 for store source-table does not exist."". This is because its possible for   ```java assertThat(getStore(kafkaStreams2, storeQueryParam2).get(key), is(nullValue())); ```          or  ```java assertThat(getStore(kafkaStreams1, storeQueryParam2).get(key), is(nullValue())); ```  (depending on which branch) to be thrown, i.e. see ``` org.apache.kafka.streams.errors.InvalidStateStorePartitionException: The specified partition 1 for store source-table does not exist.  	at org.apache.kafka.streams.state.internals.WrappingStoreProvider.stores(WrappingStoreProvider.java:63) 	at org.apache.kafka.streams.state.internals.CompositeReadOnlyKeyValueStore.get(CompositeReadOnlyKeyValueStore.java:53) 	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.lambda$shouldQuerySpecificActivePartitionStores$5(StoreQueryIntegrationTest.java:223) 	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.retryUntil(StoreQueryIntegrationTest.java:579) 	at org.apache.kafka.streams.integration.StoreQueryIntegrationTest.shouldQuerySpecificActivePartitionStores(StoreQueryIntegrationTest.java:186) ```  This happens when the stream hasn't been initialized yet. I have run the test around 12k times using Intellij's JUnit testing framework without any flaky failures. The PR also does some minor refactoring regarding moving the list of predicates into their own functions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mdedetrich","2022-06-13T22:16:05Z","2022-07-04T20:03:18Z"
"","12222","MINOR: Send kraft raft/controller logs to controller log in systests","Currently the only place we see controller/raft logging in system tests is `server-start-stdout-stderr.log` where they are mixed with all other logs. It is more convenient to send them to `controller.log` as we do for zk tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-05-27T20:44:06Z","2022-05-30T16:21:41Z"
"","11813","KAFKA-12648: fix #add/removeNamedTopology blocking behavior when app is in CREATED","Currently the #add/removeNamedTopology APIs behave a little wonky when the application is still in CREATED. Since adding and removing topologies runs some validation steps there is valid reason to want to add or remove a topology on a dummy app that you don't plan to start, or a real app that you haven't started yet. But to actually check the results of the validation you need to call `get()` on the future, so we need to make sure that `get()` won't block forever in the case of no failure -- as is currently the case","closed","","ableegoldman","2022-02-26T05:22:37Z","2022-03-04T17:58:56Z"
"","11809","MINOR: create KafkaConfigSchema and TimelineObject","Create KafkaConfigSchema to encapsulate the concept of determining the types of configuration keys. This is useful in the controller because we can't import KafkaConfig, which is part of core. Also introduce the TimelineObject class, which is a more generic version of TimelineInteger / TimelineLong.","closed","","cmccabe","2022-02-25T23:40:08Z","2022-03-02T22:26:35Z"
"","12061","MINOR: Correct spelling errors in KafkaRaftClient","Correct spelling errors in KafkaRaftClient  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-17T07:07:34Z","2022-04-18T17:58:58Z"
"","11929","MINOR: s390x Stage","Copy of https://github.com/apache/kafka/pull/11899  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mimaison","2022-03-22T11:06:07Z","2022-07-27T14:50:16Z"
"","12443","MINOR: Convert some junit tests to kraft","Convert ProducerCompressionTest, MirrorMakerIntegrationTest, EdgeCaseRequestTest to kraft.  Make it explicit that ServerShutdownTest#testControllerShutdownDuringSend is ZK-only.","closed","","cmccabe","2022-07-26T18:52:15Z","2022-07-27T17:38:34Z"
"","12155","MINOR: convert some tests to KRaft","Convert EndToEndClusterIdTest, ConsumerGroupCommandTest, ListConsumerGroupTest, and LogOffsetTest to test KRaft mode.","closed","","cmccabe","2022-05-12T21:59:08Z","2022-05-14T00:29:50Z"
"","12079","MINOR: Consistent with the trunk branch of apache","Consistent with the trunk branch of apache.   @showuon Hi luke, in fact every time I create a PR, I duplicate a new branch from my latest trunk branch to make code changes.  Every time my PR has a large number of unrelated commits, the root cause is that my trunk branch has a lot of commits ahead of apache:trunk, which leads to the PRs I create later will bring these commits.  Please help to review, thanks.","closed","","RivenSun2","2022-04-21T08:23:47Z","2022-04-21T08:41:48Z"
"","11860","discarded","Conflict in Jenkinsfile from AK commit: [bbb2dc54a0f45bc5455f22a0671adde206dcfa29](https://github.com/apache/kafka/commit/bbb2dc54a0f4)  from PR: [11833](https://github.com/apache/kafka/pull/11833)  I have dropped the change as it doesn't pertain to CCS.  All other files merged cleanly.","closed","","soondenana","2022-03-07T21:15:45Z","2022-03-07T21:16:54Z"
"","11835","MINOR: refactor how ConfigurationControl checks for resource existence","ConfigurationControl methods should take a boolean indicating whether the resource is newly created, rather than taking an existence checker object. The boolean is easier to understand. Also add a unit test of existing checking failing (and succeeding).","closed","","cmccabe","2022-03-03T01:26:10Z","2022-03-15T23:05:37Z"
"","12048","MINOR: Remove redundant conditional judgments in Selector.clear()","Condition 'sendFailed' is always 'false' when reached  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-14T08:17:17Z","2022-04-15T00:41:55Z"
"","12096","KAFKA-13794: Fix comparator of inflightBatchesBySequence in TransactionsManager (round 3)","Conceptually, the ordering is defined by the producer id, producer epoch and the sequence number. This set should generally only have entries for the same producer id and epoch, but there is one case where we can have conflicting `remove` calls and hence we add this as a temporary safe fix.  We'll follow-up with a fix that ensures the original intended invariant.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-04-26T13:47:39Z","2022-04-28T20:32:25Z"
"","12179","[KAFKA-13848] Clients remain connected after SASL re-authentication f…","Clients remain connected and able to produce or consume despite an expired OAUTHBEARER token.  The problem can be reproduced using the https://github.com/acsaki/kafka-sasl-reauth project by starting the embedded OAuth2 server and Kafka, then running the long running consumer in OAuthBearerTest and then killing the OAuth2 server thus making the client unable to re-authenticate.  Root cause seems to be SaslServerAuthenticator#calcCompletionTimesAndReturnSessionLifetimeMs failing to set ReauthInfo#sessionExpirationTimeNanos when tokens have already expired (when session life time goes negative), in turn causing KafkaChannel#serverAuthenticationSessionExpired returning false and finally SocketServer not closing the channel.  The issue is observed with OAUTHBEARER but seems to have a wider impact on SASL re-authentication.","closed","","acsaki","2022-05-19T07:30:48Z","2022-06-13T02:54:07Z"
"","11585","MINOR: Cleanup for #11513","Clean up some minor things that were left over from PR https://github.com/apache/kafka/pull/11513   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vpapavas","2021-12-09T10:29:42Z","2021-12-09T19:23:02Z"
"","11859","MINOR: fix the query prefix to not have an extra ""-""","Clean up from topic prefix. updated tests accordingly  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2022-03-07T20:37:03Z","2022-03-08T19:20:00Z"
"","11862","MINOR: fix the query prefix to not have an extra hyphon","Clean up from topic prefix. updated tests accordingly   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wcarlson5","2022-03-08T00:23:21Z","2022-03-08T00:23:21Z"
"","11620","MINOR: check for raft threads in verifyNoUnexpectedThreads","Checks that any threads with `raft` in their name do not become stray.","open","","lbradstreet","2021-12-21T17:54:56Z","2022-02-04T22:48:22Z"
"","11943","KAFKA-10095: Add stricter assertion in LogCleanerManagerTest","Changes: 1. Add a stricter assertion in LogCleanerManagerTest  2. Minor cosmetic simplification in Scala   Testing: LogCleanerManagerTest run is successful. All unit test runs are successful using `./gradlew unitTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","divijvaidya","2022-03-24T14:07:17Z","2022-04-06T14:04:25Z"
"","12310","MINOR: change Streams topic-level metrics tag from 'topic-name' to 'topic'","Changes the tag name from `topic-name` to just `topic` to conform to the way this tag is named elsewhere (ie in the clients)  Also addresses the [followup feedback](https://github.com/apache/kafka/pull/12235#pullrequestreview-997922994) from @cadonna on the [original PR](https://github.com/apache/kafka/pull/12235):  1. fixes a comment about dynamic topic routing 2. fixes some indentation in `MockRecordCollector` 3. Undoes the changes to `KStreamSplitTest.scala` and `TestTopicsTest` which are no longer necessary after [this hotfix](https://github.com/apache/kafka/pull/12288)","closed","","ableegoldman","2022-06-19T23:28:10Z","2022-06-21T11:10:36Z"
"","11708","MINOR: Upgrade netty to 4.1.73.Final (3.0 branch)","Changelog: https://github.com/netty/netty/issues?q=is%3Aclosed+milestone%3A4.1.73.Final  Reviewers: Manikumar Reddy   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-24T17:42:54Z","2022-01-25T07:59:50Z"
"","11706","MINOR: Upgrade netty to 4.1.73.Final","Changelog: https://github.com/netty/netty/issues?q=is%3Aclosed+milestone%3A4.1.73.Final  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-24T13:09:12Z","2022-01-24T16:08:07Z"
"","11529","KAFKA-12932: Interfaces for SnapshotReader and SnapshotWriter","Change the snapshot API so that SnapshotWriter and SnapshotReader are interfaces. Change the existing types SnapshotWriter and SnapshotReader to use a different name and to implement the interfaces introduced by this issue.","closed","kip-500,","socutes","2021-11-23T08:09:52Z","2021-12-01T02:22:09Z"
"","11989","KAFKA-13792: Fix UncleanLeaderElectionTest package path","Change `UncleanLeaderElectionTest` class package path.  `original`: kafka.integration  `fix`: unit.kafka.integration  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-04-02T16:49:08Z","2022-04-02T17:11:56Z"
"","11507","Terminating process due to signal SIGTERM","CentOS 7.9 kafka_2.13-2.7.0 kafka cluster IP:192.168.0.xx    192.168.0.xx    192.168.0.xx  Error log: [2021-11-15 11:13:01,762] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler) [2021-11-15 11:13:01,767] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer) [2021-11-15 11:13:01,774] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer) [2021-11-15 11:13:01,845] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(normal-event-project-bf2421b9ec2b48219d3d161aa50d9ebb-0, normal-event-project-856932a7e5404249bd8178be80fd809c-0, event-with-wrong-prop-project-377e3bd74fe74a98bbaa5e9639ed6d93-0, topsense-flink-event-1399648264327925760-reg-0, abnormal-event-project-cc188cecb5dc448182b8d5d94ef58023-0, topsense-exception-0, normal-event-project-873bb169b83848b19f8739f96b95f098-0, abnormal-event-project-283cbef60c8c4363b9d9c03ddb98fcf8-0, event-with-wrong-prop-project-9687fcf0dedd4a3382cea74a09317716-0, normal-event-project-44b7faa09eab496682eae2835c935b43-0, received-data-project-56e11bbe731345f9b234f28a35936acd-0, original-data-project-bf2421b9ec2b48219d3d161aa50d9ebb-0, abnormal-event-project-w-0, abnormal-event-project-bfc0fe135a194c5a88cc1006294ef7c2-0, event-with-wrong-prop-project-7b9e3bb645ca49919e8e714bd8f05a54-0, received-data-project-3bdb32eb00004c4291fcee0a23859d19-0, original-data-project-7cb655bcc9744908843dd4b85e721c36-0, original-data-project-1410475991847014400-0, received-data-project-64e023c29b714ed2986fde59dac5935c-0, dongxin_final_account_pay-0, normal-event-0, topsense-flink-event-1410475631933788160-reg-0, event-with-wrong-prop-project-070a88750b2c4febb138151b3efcbd47-0, original-data-0, event-with-wrong-prop-project-64e023c29b714ed2986fde59dac5935c-0, original-data-project-283cbef60c8c4363b9d9c03ddb98fcf8-0, original-data-project-856932a7e5404249bd8178be80fd809c-0, original-data-project-8c85bb45585e4304b975b6683c94c889-0, received-data-project-1382362631821725696-0, event-with-wrong-prop-project-b7fd666219d34e589862ecbadda36c82-0, abnormal-event-project-1382362631821725696-0, event-with-wrong-prop-project-3648e1cd8cf84348942d115255a1817a-0, topsense-flink-event-1410475631933788160-pay-0, received-data-project-44b7faa09eab496682eae2835c935b43-0, event-with-wrong-prop-project-bfc0fe135a194c5a88cc1006294ef7c2-0, normal-event-project-1407278060830593024-0, event-with-wrong-prop-project-41a479f83af34e1189cced159f8210d0-0, original-data-project-1399648264327925760-0, received-data-project-b7016b06e96e4fefa55fac08c6980872-0, normal-event-project-7b9e3bb645ca49919e8e714bd8f05a54-0, received-data-project-05ceaec87673424a9d442bee5182c533-0, received-data-project-856932a7e5404249bd8178be80fd809c-0, normal-event-project-377e3bd74fe74a98bbaa5e9639ed6d93-0, abnormal-event-project-9687fcf0dedd4a3382cea74a09317716-0, debezium_dws_dongxin_daily_kpi-0, normal-event-project-1111111-0, normal-event-project-1403170430596026368-0, received-data-project-710b618c371341ebb8eaafe8131724ce-0, event-with-wrong-prop-project-e239dd677edb4af0a71b4c3101ca3e56-0, event-with-wrong-prop-project-873bb169b83848b19f8739f96b95f098-0, received-data-project-e80344cbd81b46a899bebd5a8fa442e2-0, original-data-project-e239dd677edb4af0a71b4c3101ca3e56-0, received-data-project-2a284018298f46e7a9a29a98194c1c28-0, abnormal-event-project-3bdb32eb00004c4291fcee0a23859d19-0, original-data-project-377e3bd74fe74a98bbaa5e9639ed6d93-0, dongxin_retry_account_reg_retention-0, normal-event-project-036f1cd216734ae3bd53017fbb3b2741-0, abnormal-event-project-8c85bb45585e4304b975b6683c94c889-0, event-with-wrong-prop-project-b18756b9715b4728bc3fb3fbc17b6a00-0, normal-event-project-1382362631821725696-0, received-data-project-377e3bd74fe74a98bbaa5e9639ed6d93-0, original-data-project-w-0, normal-event-project-9687fcf0dedd4a3382cea74a09317716-0, event-with-wrong-prop-project-w-0, abnormal-event-project-e80344cbd81b46a899bebd5a8fa442e2-0, debezium_dwd_dongxin_account_reg-0, topsense-flink-event-1410475631933788160-login-0, event-with-wrong-prop-project-7cb655bcc9744908843dd4b85e721c36-0, event-with-wrong-prop-project-1399648264327925760-0, event-with-wrong-prop-project-1408268520894107648-0, received-data-project-1407297406042771456-0, original-data-project-b7fd666219d34e589862ecbadda36c82-0, received-data-project-cc188cecb5dc448182b8d5d94ef58023-0, original-data-project-8814c981968b40928c03b74e100507c0-0, received-data-project-9687fcf0dedd4a3382cea74a09317716-0, abnormal-event-project-1407297406042771456-0, abnormal-event-project-b18756b9715b4728bc3fb3fbc17b6a00-0, normal-event-project-48edd0d91da647dbbf4b070b66c607bd-0, event-with-wrong-prop-project-856932a7e5404249bd8178be80fd809c-0, event-with-wrong-prop-project-44b7faa09eab496682eae2835c935b43-0, received-data-project-1407297168498364416-0) (kafka.server.ReplicaFetcherManager) [2021-11-15 11:13:01,905] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer) [2021-11-15 11:13:01,912] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) [2021-11-15 11:13:01,912] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) [2021-11-15 11:13:01,912] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) [2021-11-15 11:13:01,913] INFO [SocketServer brokerId=1] Stopping socket server request processors (kafka.network.SocketServer) [2021-11-15 11:13:01,920] INFO [SocketServer brokerId=1] Stopped socket server request processors (kafka.network.SocketServer) [2021-11-15 11:13:01,921] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool) [2021-11-15 11:13:01,922] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool) [2021-11-15 11:13:01,927] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,946] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,946] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,947] INFO [KafkaApi-1] Shutdown complete. (kafka.server.KafkaApis) [2021-11-15 11:13:01,948] INFO [ExpirationReaper-1-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,995] INFO [ExpirationReaper-1-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,995] INFO [ExpirationReaper-1-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:01,997] INFO [TransactionCoordinator id=1] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator) [2021-11-15 11:13:01,998] INFO [ProducerId Manager 1]: Shutdown complete: last producerId assigned 8000 (kafka.coordinator.transaction.ProducerIdManager) [2021-11-15 11:13:01,999] INFO [Transaction State Manager 1]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager) [2021-11-15 11:13:01,999] INFO [Transaction Marker Channel Manager 1]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager) [2021-11-15 11:13:01,999] INFO [Transaction Marker Channel Manager 1]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager) [2021-11-15 11:13:01,999] INFO [Transaction Marker Channel Manager 1]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager) [2021-11-15 11:13:02,000] INFO [TransactionCoordinator id=1] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator) [2021-11-15 11:13:02,000] INFO [GroupCoordinator 1]: Shutting down. (kafka.coordinator.group.GroupCoordinator) [2021-11-15 11:13:02,001] INFO [ExpirationReaper-1-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,176] INFO [ExpirationReaper-1-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,176] INFO [ExpirationReaper-1-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,176] INFO [ExpirationReaper-1-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,263] INFO [ExpirationReaper-1-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,263] INFO [ExpirationReaper-1-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,265] INFO [GroupCoordinator 1]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator) [2021-11-15 11:13:02,267] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager) [2021-11-15 11:13:02,267] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler) [2021-11-15 11:13:02,268] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler) [2021-11-15 11:13:02,268] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler) [2021-11-15 11:13:02,269] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager) [2021-11-15 11:13:02,270] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,270] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Shutting down (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,274] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=1317996094, epoch=40216793) to node 2: (org.apache.kafka.clients.FetchSessionHandler) java.io.IOException: Client was shutdown before response was read 	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109) 	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110) 	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211) 	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301) 	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136) 	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135) 	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118) 	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96) [2021-11-15 11:13:02,274] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Error sending fetch request (sessionId=1888531809, epoch=17995123) to node 3: (org.apache.kafka.clients.FetchSessionHandler) java.io.IOException: Client was shutdown before response was read 	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:109) 	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:110) 	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:211) 	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:301) 	at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:136) 	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:135) 	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:118) 	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96) [2021-11-15 11:13:02,281] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,281] INFO [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,282] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Stopped (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,291] INFO [ReplicaFetcher replicaId=1, leaderId=3, fetcherId=0] Shutdown completed (kafka.server.ReplicaFetcherThread) [2021-11-15 11:13:02,303] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager) [2021-11-15 11:13:02,304] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager) [2021-11-15 11:13:02,304] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager) [2021-11-15 11:13:02,304] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,391] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,391] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,391] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,394] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,394] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,394] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,456] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,456] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,456] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,621] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,621] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) [2021-11-15 11:13:02,642] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager) [2021-11-15 11:13:02,643] INFO [broker-1-to-controller-send-thread]: Shutting down (kafka.server.BrokerToControllerRequestThread) [2021-11-15 11:13:02,643] INFO [broker-1-to-controller-send-thread]: Stopped (kafka.server.BrokerToControllerRequestThread) [2021-11-15 11:13:02,643] INFO [broker-1-to-controller-send-thread]: Shutdown completed (kafka.server.BrokerToControllerRequestThread) [2021-11-15 11:13:02,643] INFO [broker-1-to-controller-send-thread]: Shutdown completed (kafka.server.BrokerToControllerRequestThread) [2021-11-15 11:13:02,644] INFO Shutting down. (kafka.log.LogManager) [2021-11-15 11:13:02,661] INFO [ProducerStateManager partition=topsense-flink-event-1408268520894107648-reg-0] Writing producer snapshot at offset 891 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,675] INFO [ProducerStateManager partition=event-with-wrong-prop-0] Writing producer snapshot at offset 182152 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,678] INFO [ProducerStateManager partition=normal-event-project-1407297168498364416-0] Writing producer snapshot at offset 54811 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,679] INFO [ProducerStateManager partition=normal-event-project-e3ab1f74334149cab19e3f2b5a6150ee-0] Writing producer snapshot at offset 197081 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,679] INFO [ProducerStateManager partition=__consumer_offsets-29] Writing producer snapshot at offset 7456 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,680] INFO [ProducerStateManager partition=normal-event-project-1407297406042771456-0] Writing producer snapshot at offset 135 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,682] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 7693 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,684] INFO [ProducerStateManager partition=received-data-project-1407296120006250496-0] Writing producer snapshot at offset 7833494 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,685] INFO [ProducerStateManager partition=__consumer_offsets-2] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,686] INFO [ProducerStateManager partition=dongxin_fail_account_reg_retention-0] Writing producer snapshot at offset 143978 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,687] INFO [ProducerStateManager partition=abnormal-event-project-1399648264327925760-0] Writing producer snapshot at offset 1357 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,688] INFO [ProducerStateManager partition=abnormal-event-project-1410475631933788160-0] Writing producer snapshot at offset 505 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,691] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1407297406042771456-0] Writing producer snapshot at offset 16 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,694] INFO [ProducerStateManager partition=__consumer_offsets-41] Writing producer snapshot at offset 24904 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,698] INFO [ProducerStateManager partition=dongxin_final_account_pay-0] Writing producer snapshot at offset 16091 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,699] INFO [ProducerStateManager partition=normal-event-0] Writing producer snapshot at offset 68610353 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,701] INFO [ProducerStateManager partition=received-data-project-e3ab1f74334149cab19e3f2b5a6150ee-0] Writing producer snapshot at offset 197130 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,702] INFO [ProducerStateManager partition=abnormal-event-project-1407296120006250496-0] Writing producer snapshot at offset 31 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,735] INFO [ProducerStateManager partition=received-data-project-1399648264327925760-0] Writing producer snapshot at offset 41145739 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,761] INFO [ProducerStateManager partition=received-data-project-1410475631933788160-0] Writing producer snapshot at offset 17264070 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,763] INFO [ProducerStateManager partition=__consumer_offsets-38] Writing producer snapshot at offset 4979 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,764] INFO [ProducerStateManager partition=normal-event-project-1407296120006250496-0] Writing producer snapshot at offset 77015 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,764] INFO [ProducerStateManager partition=__consumer_offsets-35] Writing producer snapshot at offset 2593 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,765] INFO [ProducerStateManager partition=__consumer_offsets-26] Writing producer snapshot at offset 6 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,766] INFO [ProducerStateManager partition=abnormal-event-0] Writing producer snapshot at offset 182349 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,767] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1399648264327925760-0] Writing producer snapshot at offset 1357 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,769] INFO [ProducerStateManager partition=dongxin_retry_account_reg_retention-0] Writing producer snapshot at offset 89036 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,769] INFO [ProducerStateManager partition=__consumer_offsets-47] Writing producer snapshot at offset 12 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,770] INFO [ProducerStateManager partition=operation-login-0] Writing producer snapshot at offset 179973 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,786] INFO [ProducerStateManager partition=original-data-0] Writing producer snapshot at offset 4050 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,834] INFO [ProducerStateManager partition=normal-event-project-1408268520894107648-0] Writing producer snapshot at offset 94 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,835] INFO [ProducerStateManager partition=topsense-flink-event-1408268520894107648-pay-0] Writing producer snapshot at offset 45 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,842] INFO [ProducerStateManager partition=original-data-project-e3ab1f74334149cab19e3f2b5a6150ee-0] Writing producer snapshot at offset 197117 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,843] INFO [ProducerStateManager partition=topsense-flink-event-1399648264327925760-login-0] Writing producer snapshot at offset 9457 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,844] INFO [ProducerStateManager partition=operation-create_role-0] Writing producer snapshot at offset 133 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,846] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 2499 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,847] INFO [ProducerStateManager partition=abnormal-event-project-1408268520894107648-0] Writing producer snapshot at offset 55 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,848] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1408268520894107648-0] Writing producer snapshot at offset 55 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,848] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1407297168498364416-0] Writing producer snapshot at offset 36 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,849] INFO [ProducerStateManager partition=abnormal-event-project-1407297406042771456-0] Writing producer snapshot at offset 16 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,849] INFO [ProducerStateManager partition=topsense-flink-event-1410475631933788160-pay-0] Writing producer snapshot at offset 38445 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,896] INFO [ProducerStateManager partition=normal-event-project-1399648264327925760-0] Writing producer snapshot at offset 161499 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,915] INFO [ProducerStateManager partition=normal-event-project-1410475631933788160-0] Writing producer snapshot at offset 75034 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,917] INFO [ProducerStateManager partition=event-with-wrong-prop-project-e3ab1f74334149cab19e3f2b5a6150ee-0] Writing producer snapshot at offset 20 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,917] INFO [ProducerStateManager partition=original-data-project-1bd902d8c3fb4e66ba7bdf70995666c5-0] Writing producer snapshot at offset 829 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,918] INFO [ProducerStateManager partition=__consumer_offsets-44] Writing producer snapshot at offset 5205 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,919] INFO [ProducerStateManager partition=received-data-project-1407297168498364416-0] Writing producer snapshot at offset 5431387 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,920] INFO [ProducerStateManager partition=topsense-flink-event-1410475631933788160-reg-0] Writing producer snapshot at offset 78449 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,922] INFO [ProducerStateManager partition=__consumer_offsets-5] Writing producer snapshot at offset 2489 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,924] INFO [ProducerStateManager partition=topsense-flink-event-1410475631933788160-login-0] Writing producer snapshot at offset 640230 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,925] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,926] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1410475631933788160-0] Writing producer snapshot at offset 505 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,927] INFO [ProducerStateManager partition=original-data-project-1407297406042771456-0] Writing producer snapshot at offset 1248 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,929] INFO [ProducerStateManager partition=__consumer_offsets-11] Writing producer snapshot at offset 5955 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,929] INFO [ProducerStateManager partition=__consumer_offsets-20] Writing producer snapshot at offset 2495 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,931] INFO [ProducerStateManager partition=received-data-project-1bd902d8c3fb4e66ba7bdf70995666c5-0] Writing producer snapshot at offset 1269 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,931] INFO [ProducerStateManager partition=topsense-flink-event-1399648264327925760-reg-0] Writing producer snapshot at offset 5541 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,932] INFO [ProducerStateManager partition=topsense-exception-0] Writing producer snapshot at offset 78 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,933] INFO [ProducerStateManager partition=normal-event-project-1bd902d8c3fb4e66ba7bdf70995666c5-0] Writing producer snapshot at offset 386 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,933] INFO [ProducerStateManager partition=abnormal-event-project-e3ab1f74334149cab19e3f2b5a6150ee-0] Writing producer snapshot at offset 20 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,934] INFO [ProducerStateManager partition=topsense-flink-event-1408268520894107648-login-0] Writing producer snapshot at offset 3034 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,936] INFO [ProducerStateManager partition=__consumer_offsets-14] Writing producer snapshot at offset 2492 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,937] INFO [ProducerStateManager partition=received-data-project-44b7faa09eab496682eae2835c935b43-0] Writing producer snapshot at offset 175 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,939] INFO [ProducerStateManager partition=received-data-project-1407297406042771456-0] Writing producer snapshot at offset 1894 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,939] INFO [ProducerStateManager partition=abnormal-event-project-1407297168498364416-0] Writing producer snapshot at offset 36 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,940] INFO [ProducerStateManager partition=__consumer_offsets-17] Writing producer snapshot at offset 23250 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,941] INFO [ProducerStateManager partition=event-with-wrong-prop-project-1407296120006250496-0] Writing producer snapshot at offset 31 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,942] INFO [ProducerStateManager partition=dwd_dongxin_account_reg-0] Writing producer snapshot at offset 51754 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,942] INFO [ProducerStateManager partition=dongxin_final_account_reg_retention-0] Writing producer snapshot at offset 559307 (kafka.log.ProducerStateManager) [2021-11-15 11:13:02,978] INFO [ProducerStateManager partition=received-data-project-1408268520894107648-0] Writing producer snapshot at offset 57026 (kafka.log.ProducerStateManager) [2021-11-15 11:13:03,035] INFO Shutdown complete. (kafka.log.LogManager) [2021-11-15 11:13:03,056] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread) [2021-11-15 11:13:03,056] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread) [2021-11-15 11:13:03,056] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread) [2021-11-15 11:13:03,057] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient) [2021-11-15 11:13:03,169] INFO Session: 0x10005c91c9b0002 closed (org.apache.zookeeper.ZooKeeper) [2021-11-15 11:13:03,169] INFO EventThread shut down for session: 0x10005c91c9b0002 (org.apache.zookeeper.ClientCnxn) [2021-11-15 11:13:03,170] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient) [2021-11-15 11:13:03,171] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,006] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,006] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,006] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,227] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,228] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,228] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,865] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,865] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,865] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,896] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,896] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper) [2021-11-15 11:13:04,898] INFO [SocketServer brokerId=1] Shutting down socket server (kafka.network.SocketServer) [2021-11-15 11:13:04,924] INFO [SocketServer brokerId=1] Shutdown completed (kafka.network.SocketServer) [2021-11-15 11:13:04,925] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics) [2021-11-15 11:13:04,925] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics) [2021-11-15 11:13:04,925] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics) [2021-11-15 11:13:04,935] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats) [2021-11-15 11:13:04,936] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser) [2021-11-15 11:13:04,937] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)  Ask what reason?","closed","","yunfeng79","2021-11-16T12:20:27Z","2021-12-10T18:58:17Z"
"","11875","KAFKA-13721: asymetric join-winodws should not emit spurious left/outer join results","Call for review @spena @guozhangwang","closed","streams,","mjsax","2022-03-10T09:11:26Z","2022-03-15T17:32:14Z"
"","11596","MINOR: bump version in kraft readme","Bump the version from 3.0 to 3.1 in kraft readme, since we are basically have the same state in v3.0 and v3.1.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-12T08:25:19Z","2022-04-21T10:39:23Z"
"","11888","MINOR: Pass materialized to the inner KTable instance","Bug Fix: the mapValues method with key-value mapper and materialized did not pass materialized to the inner KTable representation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","msigmond","2022-03-14T00:08:44Z","2022-03-21T09:03:16Z"
"","12442","KAFKA-10199: Bookkeep tasks during assignment for use with state updater","Bookkeeps tasks to be recycled, closed, and updated during handling of the assignment. The bookkeeping is needed for integrating the state updater.  These change is hidden behind internal config STATE_UPDATER_ENABLED. If the config is false Streams should not use the state updater and behave as usual.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-26T14:48:08Z","2022-07-28T20:28:48Z"
"","12342","KAFKA-10888: Revert #12049 f7db603","Benchmarking indicates that #12049 was responsible for a 10% performance regression in the Producer. We are not sure why, but reverting the commit shows a return to nominal performance, so there is a strong indication that this one commit is indeed the culprit.  Results: Commit: e3202b9999 (the parent of the problematic commit) TPut: **118k±1k**  Commit: f7db6031b8 (#12049) TPut: **106k±1k**  (every commit that we have tested from trunk after f7db6031b8 is also around **106k**)  This PR, reverting f7db6031b8 TPut: **116k±1k**  We propose to first revert the commit in question and then re-introduce the feature later, rather than trying to debug and fix the feature in trunk. The longer this commit stays live, the more code will be based on it, and the harder it will be in the future to extract, debug, or fix it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2022-06-24T17:06:32Z","2022-07-12T21:07:35Z"
"","12447","KAFKA-14124: improve quorum controller fault handling","Before trying to commit a batch of records to the __cluster_metadata log, the active controller should try to apply them to its current in-memory state. If this application process fails, the active controller process should exit, allowing another node to take leadership. This will prevent most bad metadata records from ending up in the log and help to surface errors during testing.  Similarly, if the active controller attempts to renounce leadership, and the renunciation process itself fails, the process should exit. This will help avoid bugs where the active controller continues in an undefined state.  In contrast, standby controllers that experience metadata application errors should continue on, in order to avoid a scenario where a bad record brings down the whole controller cluster.  The intended effect of these changes is to make it harder to commit a bad record to the metadata log, but to continue to ride out the bad record as well as possible if such a record does get committed.  This PR introduces the FaultHandler interface to implement these concepts. In junit tests, we use a FaultHandler implementation which does not exit the process. This allows us to avoid terminating the gradle test runner, which would be very disruptive. It also allows us to ensure that the test surfaces these exceptions, which we previously were not doing (the mock fault handler stores the exception).  In addition to the above, this PR fixes a bug where RaftClient#resign was not being called from the renounce() function. This bug could have resulted in the raft layer not being informed of an active controller resigning.","open","","cmccabe","2022-07-28T00:11:03Z","2022-08-03T18:34:20Z"
"","12373","MINOR: Fix static mock usage in TaskMetricsTest","Before this PR the calls to the static methods on StreamsMetricsImpl were just calls and not a verification on the mock. This miss happened during the switch from EasyMock to Mockito.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-01T13:55:57Z","2022-07-03T01:48:08Z"
"","12454","MINOR: Fix static mock usage in ThreadMetricsTest","Before this PR the calls to the static methods on StreamsMetricsImpl were just calls and not a verification on the mock. This miss happened during the switch from EasyMock to Mockito.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-28T17:48:55Z","2022-07-28T20:32:46Z"
"","12325","MINOR: Fix static mock usage in StateStoreMetricsTest","Before this PR the calls to the static methods on StreamsMetricsImpl were just calls and not a verification on the mock. This miss happened during the switch from EasyMock to Mockito.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-06-21T19:34:52Z","2022-06-23T07:44:50Z"
"","12323","MINOR: Fix static mock usage in ProcessorNodeMetricsTest","Before this PR the calls to StreamsMetricsImpl.addInvocationRateAndCountToSensor() were just calls and not a verification on the mock. This miss happened during the switch from EasyMock to Mockito.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-06-21T14:34:33Z","2022-06-23T07:45:22Z"
"","12322","MINOR: Fix static mock usage in NamedCacheMetricsTest","Before this PR the call to `StreamsMetricsImpl.addAvgAndMinAndMaxToSensor()` was just a call and not a verification on the mock. This miss happened during the switch from EasyMock to Mockito.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-06-21T13:33:54Z","2022-06-23T07:45:47Z"
"","11863","KAFKA-13689: Revert AbstractConfig code changes","Because the previous code involves the modification of the public module, a KIP needs to be provided; At the same time, the consideration of some scenarios was ignored, so it was decided to revert the previous code changes","closed","","RivenSun2","2022-03-08T02:20:16Z","2022-03-10T02:54:10Z"
"","11715","KAFKA-13618;`Exptected` rename to `Expected`","BatchAccumulator `Exptected` rename to `Expected`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-01-26T13:42:37Z","2022-01-26T22:52:34Z"
"","11738","MINOR: move non-management methods from TaskManager to Task Executor","Basic refactoring with no logical changes to lay the groundwork & facilitate reviews for error handling work.  This PR just moves all methods that go beyond the management of tasks into a new TaskExecutor class, such as processing, committing, and punctuating. This breaks up the ever-growing TaskManager class so it can focus on the tracking and updating of the tasks themselves, while the TaskExecutor can focus on the actual processing. In addition to cleaning up this code this should make it easier to test this part of the code.","closed","","ableegoldman","2022-02-08T09:06:00Z","2022-02-18T08:39:41Z"
"","12449","KAFKA-12947: Replace EasyMock and PowerMock with Mockito for StreamsMetricsImplTest","Based on the [Replace EasyMock and PowerMock with Mockito Jira](https://issues.apache.org/jira/browse/KAFKA-7438 ) ticket:  >Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to use.  Since the original PR #10881 (KAFKA-12947 Replace EasyMock and PowerMock with Mockito for Streams…) author is no longer accessible, this PR is created to fix what was started more than a year ago.  Each test change has a separate commit  (which can make it a little bit easier to review).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dplavcic","2022-07-28T04:15:54Z","2022-08-03T19:51:03Z"
"","11980","fix: make sliding window works without grace period (#kafka-13739)","backport of kafka-13739","closed","streams,","tiboun","2022-03-31T20:26:27Z","2022-04-27T16:11:13Z"
"","12361","KAFKA-14010: (3.2) AlterPartition request won't retry when receiving retriable error","back port the change in https://github.com/apache/kafka/pull/12329 to 3.2 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-06-29T13:41:01Z","2022-06-29T13:41:47Z"
"","12362","KAFKA-14010: AlterPartition request won't retry when receiving retriable error","Back port the change in https://github.com/apache/kafka/pull/12329 to 3.2 branch because it changes a lot in trunk release. Basically the change is the same as previous PR, just some minor things different, like no metadata supplier, use zkVersion instead of partitionEpoch, and some small things.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-06-29T13:42:55Z","2022-07-01T02:51:57Z"
"","12077","MINOR: Avoid using strings directly for the auto.offset.reset configuration of ConsumerConfig","Avoid using strings directly for the auto.offset.reset configuration of ConsumerConfig  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-04-21T06:30:49Z","2022-04-24T12:54:44Z"
"","12398","KAFKA-14062: OAuth client token refresh fails with SASL extensions","Authored by @emissionnebula  What ---- Kafka client is adding and removing the SASL extensions alternatively at the time of token refresh. During the window when the extensions are not present in the subject. If a connection to a broker is reattempted, it fails with the error that the extensions are missing.  See [KAFKA-14062](https://issues.apache.org/jira/browse/KAFKA-14062) for more information.  Why ---- In clients, a Subject object is maintained which contains two sets each for Private and Public Credentials. Public Credentials includes the extensions. These values are stored in a `SaslExtensions` object which internally maintains these in a HashMap.   At the time of token refresh, a SaslExtensions object with these extensions is added to the public credentials set. As a next step, the refresh thread tries to logout the client for the older credentials. So it tries to remove the older token (private credential) and older SaslExtensions object (public credential) from the sets maintained in the Subject object.   SaslExtensions Class overrides the `equals` and `hashcode` functions and directly calls the `equals` and `hashcode` functions of HashMap. So at the time refresh when a new SaslExtensions object is added, because the extension values don't change, it results in a no-op because the hashes of the existing SaslExtensions object and the new object will be equals. But in the logout step, the only SaslExtensions object present in the set gets removed.  After removing the extensions in 1st refresh, the extensions will get added again at the time of 2nd refresh. So, this addition and removal keep happening alternatively.  The addition and removal of private credentials (tokens) from Subject work just fine because the tokens are always different.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2022-07-09T19:13:39Z","2022-07-12T08:58:20Z"
"","11606","MINOR: Add shutdown tests for KRaft","Augments existing shutdown tests for KRaft.  Adds the ability to update configs in KRaft tests, and in both the ZK and KRaft cases to be able to update configs without losing the server's log directory and data.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-12-15T23:12:32Z","2022-01-26T07:38:20Z"
"","11671","KAFKA-13388; Kafka Producer nodes stuck in CHECKING_API_VERSIONS","At the moment, the `NetworkClient` will remain stuck in the `CHECKING_API_VERSIONS` state forever if the `Channel` does not become ready. To prevent this from happening, this patch changes the logic to transition to the `CHECKING_API_VERSIONS` only when the `ApiVersionsRequest` is queued to be sent out. With this, the connection will timeout if the `Channel` does not become ready within the connection setup timeout. Once the `ApiVersionsRequest` is queued up, the request timeout takes over.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-12T14:31:22Z","2022-01-26T21:57:45Z"
"","11919","MINOR: Unify the log output of JaasContext.defaultContext","At present, KafkaServer also supports sasl.jaas.config configuration, so the log output form can be unified.","open","","RivenSun2","2022-03-20T03:59:21Z","2022-04-19T02:41:05Z"
"","12115","Update README.md","As someone new to Kafka, it would have been welcoming to see a succinct high-level overview of what Kafka is as the first piece of documentation. As I clicked on the home page of Kafka website, I read about seemingly disconnected pieces of what Kafka could do but the project didn't succeed in taking on a tangible form or differentiating itself from similar scaling systems. This leaves it up to the user to either quit or continue to push into a third attempt of research - but now with more apprehension. I thought my additions (quoted from the Kafka website) would add some clarity and decrease the barrier for those who may want to quickly learn about the system.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","LeonSamuel","2022-05-03T00:57:19Z","2022-06-01T22:01:43Z"
"","12438","KAFKA-13868: Replace YouTube embedded video with links on streams page","As per the [discussion in the community](https://lists.apache.org/thread/p24xvbf8nkvxpbj668vc0g3x3lojsnk4), we want to replace the embedded YouTube videos with hyperlinks to satisfy the [ASF privacy policy](https://privacy.apache.org/faq/committers.html).  This code change replaces the embedded videos from the streams web page on the website with hyperlinks.  **Before** ![Screenshot 2022-07-25 at 20 05 02](https://user-images.githubusercontent.com/71267/180844431-82f159de-51c2-4d84-895b-2b58efab3a85.png)  **After** ![Screenshot 2022-07-25 at 19 58 01](https://user-images.githubusercontent.com/71267/180844335-7d4cf22b-543f-4800-9eff-18389cf4318b.png)","closed","","divijvaidya","2022-07-25T18:10:16Z","2022-07-26T15:45:50Z"
"","11513","KAFKA-13506: Write and restore position to/from changelog","As part of the consistency work, state stores (RocksDB* and ChangeLogging*) have a consistency vector that contains the latest seen offset of every partition. We need this information on StandBy servers as well. To achieve this, we persist the offset in the headers of each record that is written to the changelog topic. This way, Standby servers can re-create the consistency vector during restoration.  Note, that we change the schema of the changelog records to include headers. If consistency is not enabled, the headers contain the version information for the record format.  There are three kinds of tests: integration test that checks that the offsets are written to the changelog and restored at the standby,  unit tests that tests restoration and unit test that tests writing to changelog.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vpapavas","2021-11-18T14:18:56Z","2021-12-08T17:58:36Z"
"","12249","MINOR; Test last committed record offset for Controllers","As part of KIP-835, LastCommittedRecordOffset was added to the KafkaController metric type. Make sure to test that metric.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2022-06-03T16:07:20Z","2022-06-08T17:45:08Z"
"","11515","KIP-795 Make AbstractCoordinator part of the public API","As part of KIP-795, this PR relocates some classes from the `org.apache.kafka.clients.consumer.internals` package to the `org.apache.kafka.clients.consumer` package, in order to make them part of Kafka's public API.  No new functionality has been added or removed.  The methods included in the new `Coordinator` interface have forcibly made public, and so has methods from other internal classes (Hearbeat, ConsumerNetworkClient) that are referenced from the newly public classes.","closed","","hgeraldino","2021-11-18T16:48:25Z","2022-02-07T14:08:09Z"
"","11997","KAFKA-6204, KAFKA-7402: ProducerInterceptor should implement AutoCloseable","As part of KIP-376 we had ConsumerInterceptor implement AutoCloseable but forgot to do the same for ProducerInterceptor. This fixes the inconsistency and also fixes KAFKA-6204 at the same time.","closed","","xvrl","2022-04-05T19:17:10Z","2022-04-06T03:24:43Z"
"","12365","KAFKA-14020: Performance regression in Producer","As part of KAFKA-10888 work, there were a couple regressions introduced:  - A call to time.milliseconds() got moved under the queue lock, moving it back outside the lock.  The call may be expensive and cause lock contention.  Now the call is moved back outside of the lock.  - The reference to ProducerRecord was held in the batch completion callback, so it was kept alive as long as the batch was alive, which may increase the amount of memory in certain scenario and cause excessive GC work.  Now the reference is reset early, so the ProducerRecord lifetime isn't bound to the batch lifetime.  Tested via manually crafted benchmark, lock profile shows ~15% lock contention on the ArrayQueue lock without the fix and ~5% lock contention with the fix (which is also consistent with pre-KAFKA-10888 profile).  Alloc profile shows ~10% spent in ProducerBatch.completeFutureAndFireCallbacks without the fix vs. ~0.25% with the fix (which is also consistent with pre-KAFKA-10888 profile).  Will add a proper jmh benchmark for producer (looks like we don't have one) in a follow-up change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","artemlivshits","2022-06-29T20:56:05Z","2022-07-22T00:10:44Z"
"","11571","KAFKA-13496: Add reason to LeaveGroupRequest (KIP-800)","As part of [KIP-800](https://cwiki.apache.org/confluence/display/KAFKA/KIP-800%3A+Add+reason+to+JoinGroupRequest+and+LeaveGroupRequest), this patch aims to add a reason to why a consumer is leaving the group and log it in the broker.  for JoinGroupRequest: https://github.com/apache/kafka/pull/11566  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2021-12-07T05:01:04Z","2022-01-13T15:06:12Z"
"","11566","KAFKA-13495: Add reason to JoinGroupRequest (KIP-800)","As part of [KIP-800](https://cwiki.apache.org/confluence/display/KAFKA/KIP-800%3A+Add+reason+to+JoinGroupRequest+and+LeaveGroupRequest), this patch aims to add a reason to why a consumer is joining a group and log it in the broker.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2021-12-03T23:03:11Z","2022-01-13T15:01:38Z"
"","12237","MINOR: Update the kafka-reassign-partitions script command in documentation","As mentioned in Notable changes in 2.6.0 : There are several notable changes to the reassignment tool kafka-reassign-partitions.sh following the completion of [KIP-455](https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment). This tool now requires the --additional flag to be provided when changing the throttle of an active reassignment.  It is recommended that the actual operation guidance document should also be updated to avoid causing confusion to users. Otherwise, following the original documentation, the user will see an exception: `Cannot execute because there is an existing partition assignment.  Use --additional to override this and create a new partition assignment in addition to the existing one. The --additional flag can also be used to change the throttle by resubmitting the current reassignment.`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-06-02T05:32:41Z","2022-06-06T05:20:04Z"
"","12226","KAFKA-13890: Improve documentation of `ssl.keystore.type` and `ssl.truststore.type`","As mentioned in [KIP-651](https://cwiki.apache.org/confluence/display/KAFKA/KIP-651+-+Support+PEM+format+for+SSL+certificates+and+private+key)  > A new key store type `PEM` will be added for key and trust stores. Both ssl.keystore.type and ssl.truststore.type may specify PEM in addition to JKS and PKCS12.  Now the documentation of these two parameters does not indicate which optional values are available, only a default value is shown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-05-30T02:53:24Z","2022-06-17T14:31:13Z"
"","11841","KAFKA-13705: CoreUtils.swallow uses logging parameter instead of Logger","As described in the issue, the:  - Javadoc says: **The logging instance to use for logging the thrown exception.**  But logging parameter it is never used.  The testing strategy used was to run the `CoreUtilsTest` class and all test projects classes to check if any was broken.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","Corlobin","2022-03-03T20:37:39Z","2022-03-06T20:11:16Z"
"","11844","KAFKA#13702 - Connect RestClient overrides response status code on request failure","As described in Jira:  In case the submitted request status is >=400, the connect RestClient [throws](https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L133) a ConnectRestException with the proper response code, but it gets intercepted and [rethrown with 500 status code](https://github.com/apache/kafka/blob/8047ba3800436d6162d0f8eb707e28857ab9eb68/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/rest/RestClient.java#L147), effectively overriding the actual failure status.  **So this PR fixes this problem - simple catch the exception and rethrow it, keeping the original http status.**  It was difficult to test because `RestClient` class doens't has any tests on it (Maybe another issue?). So I ran all tests to check if everything was ok.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","connect,","Corlobin","2022-03-03T21:52:08Z","2022-07-20T10:11:25Z"
"","11991","KAFKA-13794: Fix comparator of inflightBatchesBySequence in TransactionManager","As described in https://issues.apache.org/jira/browse/KAFKA-13794, producer batches in inflightBatchesBySequence are not being removed correctly. One batch may be removed by another batch with the same sequence number. This patch defines a comparator explicitly, we can only remove a batch if it equals to the original one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-04-03T07:33:31Z","2022-04-07T04:05:21Z"
"","12209","KAFKA-13930: Add 3.2.0 Streams upgrade system tests","Apache Kafka 3.2.0 was recently released. Now we need to test upgrades from 3.2 to trunk in our system tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-25T13:26:59Z","2022-06-21T14:33:40Z"
"","12210","KAFKA-13930: Add 3.2.0 to core upgrade and compatibility system tests","Apache Kafka 3.2.0 was recently released. Now we need to test upgrades and compatibility with 3.2 in core system tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-25T13:49:31Z","2022-06-03T07:13:11Z"
"","11600","KAFKA-12648: handle MissingSourceTopicException for named topologies","Another source of flakiness we found in the NamedTopologyIntegrationTest was an ocasional MissingSourceTopicException that was causing the application to shut down. We created all source topics ahead of time in the tests, leading us to discover this [race condition](https://issues.apache.org/jira/browse/KAFKA-13543) in the consumer client which can lead to spurious MissingSourceTopicExceptions when the metadata hasn't finished updating after a change in the consumer's subscription.  In addition to finding a workaround for this bug, throwing this MissingSourceTopicException and shutting down the entire app is itself a bug in the NamedTopology feature -- we should not stop all clients and prevent any further processing of the completely valid topologies just because one (or more) topologies were added that are missing their source topics. We can just remove those topologies from the assignment for the time being, and wait until the metadata has finished updating or the user has created the input topics to start assigning tasks from them.  This work entails two things: a) Avoid throwing a MissingSourceTopicException inside the #assign method when named topologies are used, and just remove those topologies which are missing any of their input topics from the assignment.  b) Trigger the uncaught exception handler with a MissingSourceTopicException for each of the topologies that are missing topics, but don't shut down the thread -- we just want to make sure this issue is made visible to the user.  This PR addresses (a), while (b) is done in the followup PR https://github.com/apache/kafka/pull/11686","closed","","ableegoldman","2021-12-14T06:17:23Z","2022-01-18T19:49:24Z"
"","12426","MINOR: Fix broken link to Streams tutorial","Also fix `Transforming Data Pt. 2` video title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-07-21T09:55:31Z","2022-07-21T13:22:45Z"
"","12371","KAFKA-14035: Fix NPE caused by missing null check in SnapshottableHashTable::mergeFrom()","Also adds a simple test case that fails without the check","closed","","niket-goel","2022-06-30T22:11:21Z","2022-07-01T04:03:54Z"
"","11552","KAFKA-13488: Producer fails to recover if topic gets deleted midway","Allow LeaderEpoch to be re-assigned to the new value from the Metadata Response if oldTopicId is not present in the cache. This is needed because oldTopicId is removed from the cache if the topic gets deleted but the LeaderEpoch is not removed. Hence, metadata for the newly recreated topic won't be accepted unless we allow oldTopicId to be null.  This is a fix on top of earlier made #10952 and #11004 PRs but still don't solve the bug mentioned in KAFKA-13488. This is now fixed in this PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","prat0318","2021-11-30T07:15:12Z","2021-12-16T15:16:14Z"
"","12464","MINOR; Synchronize access to snapshots' TreeMap","All access to the snapshot mutable tree map needs to be synchronized.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-08-01T14:39:43Z","2022-08-01T17:32:32Z"
"","12411","KAFKA-14078; Do leader/epoch validation in Fetch before checking for valid replica","After the fix for https://github.com/apache/kafka/pull/12150, if a follower receives a request from another replica, it will return UNKNOWN_LEADER_EPOCH even if the leader epoch matches. We need to do epoch leader/epoch validation first before we check whether we have a valid replica.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-07-15T05:32:00Z","2022-07-25T20:24:41Z"
"","11767","KAFKA-13598: set log4j appender to default acks","After https://github.com/apache/kafka/pull/11691, the KafkaLog4JAppender causes the process to crash due to the mismatch between the new produce default idempotent mode and the appender's overridden `acks=1` property.  * Removes the `acks=1` override, so the appender falls back on the producer's default acks config.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2022-02-16T03:21:56Z","2022-02-17T16:11:54Z"
"","11772","KAFKA-10000: Add new metrics for source task transactions (KIP-618)","Adds the new metrics described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Newmetrics) for source task producer transaction sizes.  Note that these metrics are not published in this PR, just defined. They will be published in a downstream PR.","closed","","C0urante","2022-02-17T03:25:57Z","2022-02-23T13:52:26Z"
"","11773","KAFKA-10000: Add new source connector APIs related to exactly-once support (KIP-618)","Adds the new exactly-once-related source connector APIs described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-ConnectorAPIexpansions).  Note that these APIs are not used by the framework in this PR, just defined. They will be used in downstream PRs.","closed","","C0urante","2022-02-17T03:30:06Z","2022-05-06T13:47:12Z"
"","11775","KAFKA-10000: Add all public-facing config properties related to exactly-once source support (KIP-618)","Adds the new connector- and worker-level properties described in [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Newproperties).  Relies on changes from: - https://github.com/apache/kafka/pull/11774 - https://github.com/apache/kafka/pull/11773  Note that these properties have no effect on the behavior of the Connect framework (beyond some rudimentary config validation checks) in this PR. The behavioral changes for these properties will be implemented in downstream PRs.","closed","","C0urante","2022-02-17T03:43:08Z","2022-05-17T12:17:43Z"
"","12458","MINOR: Adds KRaft versions of most streams system tests","Adds the annotation `@matrix(metadata_quorum=quorum.all_non_upgrade)` to many existing tests, which runs them with each of zookeeper and remote_kraft nodes.  This skips tests which use various forms of Kafka versioning since those seem to have issues with KRaft at the moment.  Running these tests with KRaft will require a followup PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","AlanConfluent","2022-07-29T00:46:00Z","2022-08-01T19:16:54Z"
"","11774","KAFKA-10000: Widely-used utility methods (KIP-618)","Adds some reusable utility methods for [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors) that facilitate:  - Overriding (and logging a warning) user-supplied properties if they do not contain an expected value - Generating an array of the names (or, to be precise, `toString` representations) of every known value for an enumerable type (which is useful for defining `ConfigDef` validators in conjunction with existing validators such as `CaseInsensitiveValidString`)  Note that these utility methods are not used in this PR. They will be used in downstream PRs.","closed","","C0urante","2022-02-17T03:36:58Z","2022-02-23T13:52:11Z"
"","12117","MINOR: Note that slf4j-log4j in version 1.7.35+ should be used","Adds a note to the upgrade notes to use slf4j-log4j version 1.7.35+ [1] or slf4j-reload4j to avoid possible compatibility issues originating from the logging framework [2].  [1] https://www.slf4j.org/manual.html#swapping [2] https://www.slf4j.org/codes.html#no_tlm  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-03T10:46:47Z","2022-06-02T12:04:17Z"
"","12114","MINOR: Note that slf4j-log4j in version 1.7.35+ should be used","Adds a note to the upgrade notes to use slf4j-log4j version 1.7.35+ [1] or slf4j-reload4j to avoid possible compatibility issues originating from the logging framework [2].  [1] https://www.slf4j.org/manual.html#swapping [2] https://www.slf4j.org/codes.html#no_tlm   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-02T15:18:26Z","2022-05-03T10:49:38Z"
"","12422","KAFKA-13982: Move WorkerConfigTransformerTest to use Mockito","Addressing https://issues.apache.org/jira/browse/KAFKA-13982.","closed","connect,","clolov","2022-07-19T16:18:28Z","2022-07-26T13:39:28Z"
"","12423","KAFKA-13158: Move ConnectClusterStateImpl to use Mockito","Addressing https://issues.apache.org/jira/browse/KAFKA-13158","closed","","clolov","2022-07-20T09:26:35Z","2022-07-26T12:19:08Z"
"","11899","Add s390x build stage","Adding s390x build stage. Uses Jdk11 and scala 2.13","open","","Nayana-ibm","2022-03-15T12:38:51Z","2022-06-14T13:26:23Z"
"","11792","Replace EasyMock/PowerMock with Mockito in DistributedHerderTest","Adding mockitoJunitJupiter test dependency to :connect:runtime project  Converting class level mocks from EasyMock -> Mockito  Changing first test (testJoinAssignment) to use Mockito constructs, along with its assertion helper methods","open","connect,","jeff303","2022-02-19T23:25:56Z","2022-07-26T16:23:21Z"
"","11732","MINOR: enable KRaft in ConfigCommandIntegrationTest","Adding KRaft and ZK params to ConfigCommandIntegrationTest wherever appropriate.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","ahuang98","2022-02-04T01:10:29Z","2022-04-25T22:11:14Z"
"","12095","MINOR: Adding client Base64 serializer and deserializer","Adding client serialization for Base64 which enables two use cases. First, these allow messages encoded in base64 to not incur the encoding penalty of a larger file size. Second, binary encoded messages may be used more simply on the command line.  This is a minor change. For testing the class files were added to an existing `kafka-client.jar` and exercised with `kafka-console-producer.sh` and `kafka-console-consumer.sh` in Windows and Ubuntu 20.04.  This is my own work and I license it to this project.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","LatentLag","2022-04-26T10:11:21Z","2022-07-27T16:57:07Z"
"","12357","MINOR:add more log info when setting up a mm2 instance in order to co…","Added some logs to make it easy to find out what has happened when mm2 goes wrong  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Justinwins","2022-06-29T01:51:45Z","2022-06-29T01:51:45Z"
"","11955","KAFKA-12380 Executor in Connect's Worker is not shut down when the worker is","added shutdown for executors and tests for the same  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","karuturi","2022-03-28T13:39:25Z","2022-05-02T10:45:55Z"
"","11707","KAFKA-13607 Fix of the empty values to be treated as if they were absent","Added normalization to the input parameters of the createKeystore and createTruststore  After the change the empty string values of path and passwords are treated as null. The change allows the existing entries to be replaced with empty values.  The scenario is when the final configuration of a client (consumer, producer or admin) is defined as a template and then refined with specific values. In particular in mTLS configuration the template may specify JKS/PKCS12 keystore, but the final configuration uses KIP-651 PEM coded strings.","open","","piotrsmolinski","2022-01-24T16:32:47Z","2022-03-04T19:42:29Z"
"","11623","Added checkpoint truncation utility with inmemory writers instead of temporary files in RemoteLogManager.","Added checkpoint truncation utility with inmemory writers instead of temporary files in RemoteLogManager.  This PR is built on top of #11390   Authors: satishd@apache.org, kamal.chandraprakash@gmail.com  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-12-22T09:10:41Z","2021-12-24T02:14:24Z"
"","11944","MINOR: Add extra notice about IQv2 compatibility","Added an extra notice about IQv2's API compatibility, as discussed in the KIP-796 vote thread.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-03-24T14:28:08Z","2022-03-24T19:05:59Z"
"","12010","KAFKA-13793: Add validators for configs that lack validators","Add validators for configs that lack validators","closed","","RivenSun2","2022-04-07T09:13:26Z","2022-05-09T18:29:17Z"
"","12216","MINOR: add timeouts to streams integration tests","Add timeouts to streams integration tests. Hopefully this will fix the Jenkins hangs we have been seeing.","closed","","cmccabe","2022-05-25T23:58:17Z","2022-05-31T21:22:17Z"
"","11820","[RFC][2/N] add restoration logic for RocksDBTimeOrderedWindowStore","Add restoration logic for `RocksDBTimeOrderedWindowStore`. Changelog's key is serialized from `WindowKeySchema` and transform key format to match index and base store's format    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-03-01T00:19:09Z","2022-03-31T05:09:44Z"
"","12018","KAFKA-13542: add rebalance reason in Kafka Streams","Add rebalance reason in Kafka Streams. Ran benchmark and it looks good.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lihaosky","2022-04-08T16:26:19Z","2022-04-13T11:49:31Z"
"","11804","KAFKA-13542: add rebalance reason in Kafka Streams","Add rebalance reason in Kafka Streams.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","lihaosky","2022-02-25T00:49:38Z","2022-03-01T20:49:36Z"
"","11829","KAFKA-13785: [2/N][emit final] add processor metadata to be committed with offset","Add processor metadata to be committed with offset to broker. Adding this so that we can store last processed window time gracefully.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-03-02T01:02:47Z","2022-03-31T16:48:22Z"
"","11794","MINOR: Add links to connector configs in TOC","Add links to Sink and Source connectors configurations  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-02-20T17:00:36Z","2022-02-22T13:34:28Z"
"","12353","MINOR: Add indent space after hyperlink","Add indent space after hyperlink  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","RivenSun2","2022-06-27T09:59:44Z","2022-07-01T17:30:32Z"
"","11568","DOCS-3261: Add anchor to AK Streams page for use cases","Add an anchor to the **Kafka Streams use cases** section of the [Streams Overview](https://kafka.apache.org/30/documentation/streams/), to enable linking directly to this section from external content.","open","","JimGalasyn","2021-12-06T20:17:13Z","2021-12-06T20:17:13Z"
"","11494","MINOR: Improve KafkaStreamsTest: testInitializesAndDestroysMetricsReporters","Add additional asserts for KafkaStreamsTest: testInitializesAndDestroysMetricsReporters to help diagnose if it flakily fails in the future.  * MockMetricsReporter gets initialized only once during KafkaStreams construction, so make assert check stricter by ensuring initDiff is one. * Assert KafkaStreams is not running before we validate whether MockMetricsMetricsReporter close count got incremented after streams close.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mkandaswamy","2021-11-15T03:01:07Z","2021-11-16T09:50:16Z"
"","12141","MINOR; DeleteTopics version tests","Add a DeleteTopics test for all supported versions. Convert the DeleteTopicsRequestTest to run against both ZK and KRaft mode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","jsancio","2022-05-09T18:21:43Z","2022-05-12T20:05:00Z"
"","11698","MINOR: Update files with 3.1.0","Add 3.0.0 and 3.1.0 in various places. I will follow up with other PRs to update the compatibility tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-21T15:51:36Z","2022-01-21T20:31:01Z"
"","11806","KAFKA-13697; KRaft authorizer should support AclOperation.ALL","AclOperation.ALL implies all other operation types, but we are not checking for it in StandardAuthorizer. The patch fixes the issue and adds some test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2022-02-25T18:23:37Z","2022-02-25T23:43:22Z"
"","12335","MINOR: Fix group coordinator is unavailable log","A space is missing in the log.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-06-23T11:57:43Z","2022-06-24T09:48:12Z"
"","12130","MINOR: Fix RecordContext Javadoc","A prior commit accidentally changed the javadoc for RecordContext. In reality, it is not reachable from api.Processor, only Processor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2022-05-06T14:57:58Z","2022-05-06T16:32:10Z"
"","11854","[DO NOT MERGE] POC: type-safe processValues","A POC for how to do type-safe processValues for KIP-820  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2022-03-06T18:36:45Z","2022-03-07T19:35:28Z"
"","11520","KAFKA-13468: Consumers may hang because IOException in Log# does not trigger KafkaStorageException","A patch for [KAFKA-13468](https://issues.apache.org/jira/browse/KAFKA-13468)  P.S. We found and described this issue in Kafka 2.8.0. We confirmed that this issue holds from Kafka version 2.8.0 to 3.0.0, but currently in the trunk branch, `core/src/main/scala/kafka/log/Log.scala` is renamed to `core/src/main/scala/kafka/log/UnifiedLog.scala` and there are some small code changes. However, this issue still holds, so we are still submitting the pull request for the fix for the trunk branch (the fix is slightly different from the fix in 2.8.0). And we also propose that the fix should be also applied to version 2.8.0 and 3.0.0, etc, with another pull request.  Another issue is that currently we just catch the IOException and throw a new KafkaStorageException. But we are thinking whether we should also use `logDirFailureChannel.maybeAddOfflineLogDir` to handle the IOException, such as https://github.com/apache/kafka/blob/ebb1d6e21cc9213071ee1c6a15ec3411fc215b81/core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala#L92-L120 and https://github.com/apache/kafka/blob/ebb1d6e21cc9213071ee1c6a15ec3411fc215b81/core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala#L126-L139 If so, `logDirFailureChannel.maybeAddOfflineLogDir` would crash the node according to the protocol in https://github.com/apache/kafka/blob/ebb1d6e21cc9213071ee1c6a15ec3411fc215b81/core/src/main/scala/kafka/server/ReplicaManager.scala#L268-L277 and https://github.com/apache/kafka/blob/ebb1d6e21cc9213071ee1c6a15ec3411fc215b81/core/src/main/scala/kafka/server/ReplicaManager.scala#L327-L332 ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","functioner","2021-11-20T04:03:10Z","2022-02-10T02:49:06Z"
"","11504","KAFKA-13457: SocketChannel in Acceptor#accept is not closed upon IOException","A patch for [KAFKA-13457](https://issues.apache.org/jira/browse/KAFKA-13457) ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","functioner","2021-11-16T01:21:14Z","2021-11-25T04:09:05Z"
"","11554","HOTFIX: Set version of jgit to avoid unsupported version error","A new version of JGit that is used by grgit that is used by gradle causes the following error:  org/eclipse/jgit/storage/file/FileRepositoryBuilder has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0  The reason is that version 6.0.0.202111291000-r of JGrit was compiled with a newer Java version than Java 8, probably Java 11.  Explicitly setting the version of JGrit in gradle to 5.12.0.202106070339-r fixes the issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-11-30T13:19:43Z","2021-11-30T18:10:23Z"
"","11580","KAFKA-13517. Add ConfigurationKeys to ConfigResource class","A list of `ConfigResource` class is passed as argument to `AdminClient::describeConfigs` api to indicate configuration of the entities to fetch. The `ConfigResource` class is made up of two fields, name and type of entity. Kafka returns all configurations for the entities provided to the admin client api.  This admin api in turn uses `DescribeConfigsRequest` kafka api to get the configuration for the entities in question. In addition to name and type of entity whose configuration to get, Kafka `DescribeConfigsResource` structure also lets users provide `ConfigurationKeys` list, which allows users to fetch only the configurations that are needed.  However, this field isn't exposed in the `ConfigResource` class that is used by `AdminClient`, so users of `AdminClient` have no way to ask for specific configuration. The API always returns all configurations. Then the user of the `AdminClient::describeConfigs` go over the returned list and filter out the config keys that they are interested in.  This results in boilerplate code for all users of `AdminClient::describeConfigs` api, in addition to  being wasteful use of resource. It becomes painful in large cluster case where to fetch one configuration of all topics, we need to fetch all configuration of all topics, which can be huge in size.  Creating this Jira to add same field (i.e. `ConfigurationKeys`) to the `ConfigResource` structure to bring it to parity to `DescribeConfigsResource` Kafka API structure. There should be no backward compatibility issue as the field will be optional and will behave same way if it is not specified (i.e. by passing null to backend kafka api)  Added unit and integration test to test the behavior.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","soondenana","2021-12-08T21:04:46Z","2022-02-01T10:09:42Z"
"","12451","MINOR: Update comment on verifyTaskGenerationAndOwnership method in DistributedHerder","`verifyTaskGenerationAndOwnership` was added in https://github.com/apache/kafka/pull/11779 and was used by exactly once source tasks in https://github.com/apache/kafka/pull/11780. The current description for the method is outdated and this PR updates it.","closed","connect,","yashmayya","2022-07-28T08:43:26Z","2022-07-29T03:37:49Z"
"","11912","KAFKA-13752: Uuid compare using equals in java","`Uuid.ZERO_UUID == new Uuid(0L, 0L)` is true in scala, but in java is false.   Fix it to avoid incorrect judgment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fxbing","2022-03-17T12:09:28Z","2022-03-22T08:39:33Z"
"","11545","MINOR: replace Thread.isAlive by Thread.is_alive for Python code","`Thread.isAlive` was removed from python 3.9 (see https://bugs.python.org/issue37804), and the default version of python 3 in `openjdk:8` image is `3.9.2`. Hence, the following error is produced when running system tests.  ``` Traceback (most recent call last):   File ""/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py"", line 133, in run     data = self.run_test()   File ""/usr/local/lib/python3.9/dist-packages/ducktape/tests/runner_client.py"", line 190, in run_test     return self.test_context.function(self.test)   File ""/usr/local/lib/python3.9/dist-packages/ducktape/mark/_mark.py"", line 429, in wrapper     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)   File ""/opt/kafka-dev/tests/kafkatest/tests/client/quota_test.py"", line 157, in test_quota     producer.run()   File ""/usr/local/lib/python3.9/dist-packages/ducktape/services/service.py"", line 318, in run     self.stop()   File ""/opt/kafka-dev/tests/kafkatest/services/monitor/http.py"", line 97, in stop     super(HttpMetricsCollector, self).stop()   File ""/usr/local/lib/python3.9/dist-packages/ducktape/services/background_thread.py"", line 84, in stop     super(BackgroundThreadService, self).stop()   File ""/usr/local/lib/python3.9/dist-packages/ducktape/services/service.py"", line 281, in stop     self.stop_node(node)   File ""/opt/kafka-dev/tests/kafkatest/services/monitor/http.py"", line 109, in stop_node     self._forwarders[idx].stop()   File ""/opt/kafka-dev/tests/kafkatest/services/monitor/http.py"", line 191, in stop     if self._accept_thread.isAlive(): AttributeError: 'Thread' object has no attribute 'isAlive' ```  The replacement is `Thread.is_alive`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-11-28T15:03:50Z","2021-11-29T10:49:15Z"
"","11665","KAFKA-13585; Fix flaky test `ReplicaManagerTest.testReplicaAlterLogDirsWithAndWithoutIds`","`ReplicaManagerTest.testReplicaAlterLogDirsWithAndWithoutIds` fails regularly because `assertFetcherHasTopicId` can't validate the topic id in the fetch state. The issue is quite subtile. Under the hood, `assertFetcherHasTopicId` acquires the `partitionMapLock` in the fetcher thread. `partitionMapLock` is also acquired by the the `processFetchRequest` method. If `processFetchRequest` acquires it before `assertFetcherHasTopicId` can check the fetch state, `assertFetcherHasTopicId` has not chance to verify the state anymore because `processFetchRequest` will remove the fetch state before releasing the lock.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-11T15:10:43Z","2022-01-26T07:14:04Z"
"","11564","MINOR: improve consoleProducer option description","`kafka-console-producer.sh` provides users some options to configure. But some of them are ambiguous to users to map the option to a specific producer config. Ex: `timeout` option, is actually configuring the `linger.ms` config in producer. And `max-partition-memory-bytes` option is actually the `batch.size` config in producer, etc.)   In this PR, I tried to map the option to a producer config, if not clear. And also side fix some expected wrong types, and update the `retries` defaults.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-12-03T06:50:45Z","2022-01-04T10:10:34Z"
"","11641","MINOR: update the comment for Utils.atomicMoveWithFallback","`flushDir` will also throw `IOException` if `needFlushParentDir` is enabled. Update the comment to reflect the caveat.","closed","","liuchang0520","2022-01-03T01:35:52Z","2022-01-12T18:25:52Z"
"","12197","KAFKA-13929: Replace legacy File.createNewFile() with NIO.2 Files.createFile()","`File.createNewFile()` returns a boolean signifying whether the file creation was successful or not. There are multiple places in the Kafka code base where we are not checking the value of the returned boolean.  Replacing it with the `Files.createFile()` API will decrease the chances of inadvertent bugs in the code base since `Files.createFile()` API thrown exceptions when the creation is not successful.  Note that this is one of many PRs which would help us leverage new features introduced in NIO.2 Java APIs. The parent task is listed at https://issues.apache.org/jira/browse/KAFKA-13928","closed","","divijvaidya","2022-05-23T10:21:41Z","2022-06-10T14:43:16Z"
"","11741","MINOR: Move ext block above allprojects block in build.gradle","`ext` contains definitions that should be accessible in `allprojects` (even though we don't use any right now).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-02-08T18:07:53Z","2022-02-08T20:43:50Z"
"","12012","KIP-829: (console consumer) add print.topic property","``` Usage: ./kafka-console-consumer.sh --bootstrap-server kafka:9092 --include '.*'  --property print.topic=true  Example output: Topic:hello.world	{""msg"": ""abc""} ```  Purpose: Until now, there is no way to print all messages from all topics while maintaining the topic information. This PR adds this functionallity.  [KIP-829](https://cwiki.apache.org/confluence/display/KAFKA/KIP-829%3A+%28console-consumer%29+add+print.topic+property)","open","kip,","EvansJahja","2022-04-07T11:36:52Z","2022-04-22T14:09:54Z"
"","12011","MINOR: add print.topic property in console consumer","``` Usage: ./kafka-console-consumer.sh --bootstrap-server kafka:9092 --include '.*'  --property print.topic=true  Example output: Topic:hello.world	{""msg"": ""abc""} ```  Purpose: Until now, there is no way to print all messages from all topics while maintaining the topic information. This PR adds this functionallity.","closed","","EvansJahja","2022-04-07T10:33:21Z","2022-04-07T11:20:30Z"
"","12211","MINOR: fix number of nodes used in test_compatible_brokers_eos_v2_enabled","``` 14:26:43 [WARNING - 2022-05-14 21:26:43,595 - runner_client - log - lineno:305]: RunnerClient: kafkatest.tests.streams.streams_broker_compatibility_test.StreamsBrokerCompatibility.test_compatible_brokers_eos_v2_enabled.broker_version=2.7.1: Test requested 30 nodes, used only 4 14:26:43 [WARNING:2022-05-14 21:26:43,614]: RunnerClient: kafkatest.tests.streams.streams_broker_compatibility_test.StreamsBrokerCompatibility.test_compatible_brokers_eos_v2_enabled.broker_version=2.7.1: Test requested 30 nodes, used only 4 14:26:43 [INFO:2022-05-14 21:26:43,634]: RunnerClient: kafkatest.tests.streams.streams_broker_compatibility_test.StreamsBrokerCompatibility.test_compatible_brokers_eos_v2_enabled.broker_version=2.7.1: Summary: 14:26:43 [INFO:2022-05-14 21:26:43,653]: RunnerClient: kafkatest.tests.streams.streams_broker_compatibility_test.StreamsBrokerCompatibility.test_compatible_brokers_eos_v2_enabled.broker_version=2.7.1: Data: None 14:26:43 [WARNING:2022-05-14 21:26:43,678]: Test kafkatest.tests.streams.streams_broker_compatibility_test.StreamsBrokerCompatibility.test_compatible_brokers_eos_v2_enabled.broker_version=2.8.1 is using entire cluster. It's possible this test has no associated cluster metadata. ```","closed","","lbradstreet","2022-05-25T14:58:18Z","2022-05-25T18:03:07Z"
"","11625","MINOR: Fix malformed html in javadoc of `ScramMechanism`","`./gradlew clean aggregatedJavadoc` with JDK 17 fails without this fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-12-22T22:38:15Z","2021-12-23T20:15:28Z"
"","12257","[DO NOT MERGE] test branch for debugging #12235 (KAFKA-13945)","_Not a real PR -- do not review or merge_  This is just for testing on the side, the real PR is https://github.com/apache/kafka/pull/12235","closed","","ableegoldman","2022-06-06T06:20:41Z","2022-06-07T03:04:11Z"
"","12256","[DO NOT MERGE] test branch for debugging #12235 (KAFKA-13945)","_Not a real PR -- do not review or merge_  This is just for testing on the side, the real PR is https://github.com/apache/kafka/pull/12235","closed","","ableegoldman","2022-06-06T06:18:15Z","2022-06-07T03:03:55Z"
"","11663","assign only one time for size()","_(This is my first PR for Kafka as junior engineer. This suggestion might be silly)_  If `.size()` is called when it is needed, JVM has to calculate all the time. Maybe this is not good for performance. Hence what about assigning it one time and use many times?   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-01-10T14:49:47Z","2022-01-16T09:30:15Z"
"","12359","KAFKA-13983: Fail the creation with ""/"" in resource name in zk ACL","[Problem](https://issues.apache.org/jira/browse/KAFKA-13983) Sanitizing the resource name have a compatibility issue with already existing ACLs, and also there are very few special characters which are not allowed by the zookeeper which can be found [here](https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html) so we decided to fail the creation with `/` in the resource name since this is allowed by zookeeper but it's creating the problem in acl operation.  Not allowed characters by zookeeper:  ``` The null character (\u0000) cannot be part of a path name. (This causes problems with the C binding.)  The following characters can't be used because they don't display well, or render in confusing ways: \u0001 - \u0019 and \u007F - \u009F.  The following characters are not allowed: \ud800 -uF8FFF, \uFFF0-uFFFF, \uXFFFE - \uXFFFF (where X is a digit 1 - E), \uF0000 - \uFFFFF.  The ""."" character can be used as part of another name, but ""."" and "".."" cannot alone be used to indicate a node along a path, because ZooKeeper doesn't use relative paths. The following would be invalid: ""/a/b/./c"" or ""/a/b/../c"".  The token ""zookeeper"" is reserved.  ```","closed","","singhnama","2022-06-29T06:03:09Z","2022-07-08T10:17:48Z"
"","11621","[LI-HOTFIX] Resolve the bootstrap server when cluster metadata hasn't been refreshed for a long time","[LI-HOTFIX] Resolve the bootstrap server when cluster metadata hasn't been refreshed for a long time      This patch adds a config li.client.cluster.metadata.expire.time.ms which controls the max time cluster metadata can remain unchanged. On NetworkClient.poll, if this  timeout has been reached and the client has tried half of the nodes in the original cached node set and failed, it will try to resolve the bootstrap servers again and us e the newly resolved nodes to pick a leastLoadedNode to send updateMetadataRequest.   This is to avoid following two scenarios:      consumer has been idle for a long time, and whole cluster has been swapped. This case, all the cached nodes are invalid and resolve bootstrap is needed. consumer hasn't refreshed metadata for a long time and some brokers in the cluster had been moved to another cluster, and the client randomly picks up the moved broker to send md request and get a response for a different cluster. In this case, we simply reject the stale md response and resolve bootstrap when conditions are met. TICKET = LI_DESCRIPTION = LIKAFKA-40759, EXIT_CRITERIA = MANUAL this is not going to merged with upstream   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kehuum","2021-12-21T18:34:45Z","2021-12-21T18:36:28Z"
"","12393","WIP: KAFKA-12549 Prototype for transactional state stores","[KIP-844 Transactional state stores](https://cwiki.apache.org/confluence/display/KAFKA/WIP%3A+KIP-844%3A+Transactional+State+Stores)  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Gerrrr","2022-07-08T12:13:20Z","2022-08-03T14:55:44Z"
"","12175","KIP-840: Config file option for MessageReader/MessageFormatter in ConsoleProducer/ConsoleConsumer","[KIP-840](https://cwiki.apache.org/confluence/x/bBqhD)  `kafka-console-producer.sh` & `kafka-console-consumer.sh` scripts have a `--property` that can be set multiple times. This PR allows to set those properties through a config file with `--config` instead of multiple `--property` options.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","zigarn","2022-05-17T16:55:12Z","2022-07-07T07:47:25Z"
"","11724","KAFKA-13633: Add vararg KStream#merge API","[KIP-819](https://cwiki.apache.org/confluence/display/KAFKA/KIP-819%3A+Merge+multiple+KStreams+in+one+operation)  The `KStream#merge` API has been modified to permit merging many KStreams together in a single operation.  The unnamed vararg variant of the API wholly replaces the previous unnamed singleton variant, as they would conflict. The API remains source-compatible with the singleton API because vararg parameters can be provided a single parameter at the call-site.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nicktelford","2022-01-31T15:49:41Z","2022-01-31T16:00:21Z"
"","12219","KAFKA-9436-1: (Simplified than 9436 PR) New Kafka Connect SMT for plainText => Struct(or Map)","[KIP link](https://cwiki.apache.org/confluence/display/KAFKA/KIP+678%3A+New+Kafka+Connect+SMT+for+plainText+%3D%3E+Struct%28or+Map%29+with+Regex)  Re branching and PR (about #7965) with reviewed fix from Chris Egerton ( https://lists.apache.org/thread/xb57l7j953k8dfgqvktb09y31vzpm1xx https://lists.apache.org/thread/20954n2g5wjdrts740ft3rnlx1ogh7gb )  > 1. I wonder if it's necessary to include support for type casting with this > SMT. We already have a Cast SMT ( > https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java) > that can parse multiple fields of a structured record value with differing > types. Would it be enough for your new SMT to only produce string values > for its structured data, and then allow users to perform casting logic > using the Cast SMT afterward?  > 2. It seems like the ""struct.field"" property is similar; based on the > examples, it looks like when the SMT is configured with a value for that > property, it will first pull out a field from a structured record value > (for example, it would pull out the value "" > https://kafka.apache.org/documentation/#connect"" from a map of {""url"": "" > https://kafka.apache.org/documentation/#connect""}), then parse that field's > value, and replace the entire record value (or key) with the result of the > parsing stage. It seems like this could be accomplished using the > ExtractField SMT ( > https://github.com/apache/kafka/blob/trunk/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/ExtractField.java) > as a preliminary step before passing it to your new SMT. Is this correct? > And if so, could we simplify the interface for your SMT by removing the > ""struct.field"" property in favor of the existing ExtractField SMT?  1. CAST function removed ( use combination with Cast SMT ( 2. struct.field option removed ( use combination with EXtractField SMT )    New SMT  plain text => struct(map) regex group condition with ordered key name compatible with single plain text input and struct field input plain text   ### sample1 ~~~ ""111.61.73.113 - - [08/Aug/2019:18:15:29 +0900] \""OPTIONS /api/v1/service_config HTTP/1.1\"" 200 - 101989 \""http://local.test.com/\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\"""" SMT connect config with regular expression below can easily transform a plain text to struct (or map) data. ""transforms"": ""TimestampTopic, RegexTransform"", ""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ParseStructByRegex$Value"", ~~~  ~~~ ""transforms.RegexTransform.regex"": ""^([\\d.]+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \""(GET|POST|OPTIONS|HEAD|PUT|DELETE|PATCH) (.+?) (.+?)\"" (\\d{3}) ([0-9|-]+) ([0-9|-]+) \""([^\""]+)\"" \""([^\""]+)\""""  ""transforms.RegexTransform.mapping"": ""IP,RemoteUser,AuthedRemoteUser,DateTime,Method,Request,Protocol,Response,BytesSent,Ms,Referrer,UserAgent"" ~~~  ### sample2  ~~~ dev_kafka_pc001_1580372261372"" ~~~  ~~~ ""transforms"": ""RegexTransform"", ""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ParseStructByRegex$Value"",  ""transforms.RegexTransform.regex"": ""^(.{3,4})_(.*)_(pc|mw|ios|and)([0-9]{3})_([0-9]{13})"" ""transforms.RegexTransform.mapping"": ""env,serviceId,device,sequence,datetime"" ~~~","open","kip,","whsoul","2022-05-26T02:51:00Z","2022-07-26T02:04:29Z"
"","12385","MINOR: Expose client information on RequestContext as additional public API beyond request logs (continuation of KIP 511)","[KIP 511](https://cwiki.apache.org/confluence/display/KAFKA/KIP-511%3A+Collect+and+Expose+Client%27s+Name+and+Version+in+the+Brokers) introduced a [ClientInformation](https://github.com/apache/kafka/blob/99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/clients/src/main/java/org/apache/kafka/common/network/ClientInformation.java) class that wraps software (client) name and version and is also set as a property on [RequestContext](https://github.com/apache/kafka/blob/99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/clients/src/main/java/org/apache/kafka/common/requests/RequestContext.java), except unfortunately there's no getter to retrieve this information.  The [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-511%3A+Collect+and+Expose+Client%27s+Name+and+Version+in+the+Brokers) implemented protocol support, a registry to set it at the network layer per session and integrated with [RequestContext](https://github.com/apache/kafka/blob/d35283f011a797902fc9c4d896a1a6f039eb7d06/clients/src/main/java/org/apache/kafka/server/authorizer/Authorizer.java#L101), but unfortunately the only ""public API"" for this information is the broker request logs.  This change exposes client information to custom authorisers as well via `RequestConext`, where it can be programatically used in a pluggable fashion as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","methodmissing","2022-07-06T23:03:59Z","2022-07-19T16:28:10Z"
"","12481","Kafka 14115","[KAFKA-14115] Password configs are logged in plaintext in KRaft   While updating the config for a broker ConfigurationControlManager is logging sensitive config values (listener.name.external.ssl.key.password).  ConfigResource(type=BROKER, name='1'): set configuration listener.name.external.ssl.key.password to bar   We need to redact these values the same as BrokerMetadataPublisher   Updating broker 1 with new configuration : listener.name.external.ssl.key.password -> [hidden]  Changes: updated isSensitive method to check if the config name contains the string password and used the same while logging config values.  @mumrah Can you please review this PR ?","open","","premkamal23","2022-08-03T17:30:32Z","2022-08-03T17:35:58Z"
"","12434","KAFKA-14099 - Fix request logs in connect","[KAFKA-14099](https://issues.apache.org/jira/browse/KAFKA-14099)  Restore request logs in connect. `RequestLogHandler` seems to not work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","zigarn","2022-07-23T07:47:22Z","2022-08-01T12:32:16Z"
"","12376","KAFKA-14044: Upgrade Netty and Jackson versions for CVE fixes","[KAFKA-14044](https://issues.apache.org/jira/projects/KAFKA/issues/KAFKA-14044) Upgrade Netty and Jackson for CVE fixes.  Netty: [CVE-2022-24823](https://www.cve.org/CVERecord?id=CVE-2022-24823) - Fixed by upgrading to 4.1.78 Jackson: [CVE-2020-36518](https://www.cve.org/CVERecord?id=CVE-2020-36518) - Fixed by upgrading to 2.13.3","closed","","tomncooper","2022-07-04T17:07:23Z","2022-07-05T06:16:33Z"
"","11607","KAFKA-13544: fix FinalizedFeatureChangeListener deadlock","[KAFKA-13544](https://issues.apache.org/jira/browse/KAFKA-13544?jql=project%20%3D%20KAFKA%20AND%20status%20%3D%20Open%20AND%20assignee%20in%20(EMPTY))   kafka broker server shutdown hook cause deadlock.    ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","JoeCqupt","2021-12-16T18:10:37Z","2021-12-23T20:17:05Z"
"","11508","KAFKA-9279: Fail producer transactions for asynchronously-reported, synchronously-encountered ApiExceptions","[Jira](https://issues.apache.org/jira/browse/KAFKA-9279)  Some types of exceptions that the producer reports are encountered synchronously during a call to `KafkaProducer::send`, but reported asynchronously via the resulting `Future` instead of being thrown directly from the call to `send`. One common example of this is when the size of a record exceeds the client-side `max.message.bytes` configuration property; when that happens, a failed future is returned immediately from `KafkaProducer::send`.  This is all fine, but the current behavior does not take into account the guarantees surrounding the transactional producer's `commitTransaction` method, which are that: > ...if any of the `send(ProducerRecord)` calls which were part of the transaction hit irrecoverable errors, this method will throw the last received exception immediately and the transaction will not be committed. So all `send(ProducerRecord)` calls in a transaction must succeed in order for this method to succeed.  The changes in this PR cause producers to fail on `commitTransaction` if any prior calls to `send` resulted in one of these aynchronously-reported, synchronously-encountered exceptions.  An alternative approach could be to throw these exceptions directly to the caller in `send`, but that would alter producer behavior for all users instead of just ones using transactional producers; that tradeoff seems less desirable just to fix an issue that only affects transactional producers.  Unit tests are added that cover a variety of cases that may lead to previously-missed exceptions and verify that they cause `KafkaProducer::commitTransaction` to fail as expected.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-11-16T21:24:02Z","2022-01-26T16:26:33Z"
"","11986","KAFKA-7509: Clean up incorrect warnings logged by Connect","[Jira](https://issues.apache.org/jira/browse/KAFKA-7509)  ### Summary of changes  - Skip the calls to `AbstractConfig::logUnused` made by [KafkaConsumer](https://github.com/apache/kafka/blob/62ea4c46a9be7388baeaef1c505d3e5798a9066f/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L815), [KafkaProducer](https://github.com/apache/kafka/blob/62ea4c46a9be7388baeaef1c505d3e5798a9066f/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L432), and [KafkaAdminClient](https://github.com/apache/kafka/blob/62ea4c46a9be7388baeaef1c505d3e5798a9066f/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L595) instances when the original config map is an instance of a [RecordingMap](https://github.com/apache/kafka/blob/62ea4c46a9be7388baeaef1c505d3e5798a9066f/clients/src/main/java/org/apache/kafka/common/config/AbstractConfig.java#L608-L612) - Modify `ConsumerConfig::appendDeserializerToConfig` and `ProducerConfig::appendSerializerToConfig` to preserve `RecordingMap` instances passed in to their constructors (or more precisely, create clones of those instances that retain the ""recording"" behavior of the original) so that all properties used by those consumers/producers are marked as used with the original `RecordingMap` - Use `WorkerConfig::originals` as the baseline when constructing configs to pass to Kafka clients that are used by the worker to manage its internal Kafka topics, so that all properties in the worker config that are used by those Kafka clients are marked as used in the `WorkerConfig` - Ignore all properties in the worker config that are transparently passed through to configurations for other components that: - - Perform their own logging for unused properties (such as producers and consumers used by connector instances, whose properties can be specified in a worker config with the `producer.` and `consumer.` prefixes, respectively) - - Are used transparently by the worker without accessing via either `AbstractConfig::get` (or one of its strongly-typed variants) or by invoking `Map::get` on the result of `AbstractConfig::originals` (or one of its prefixed variants) (such as internal topic settings) - - Are not constructed during worker startup, but instead brought up later (such as the default key, value, and header converters, which are instantiated on a case-by-case basis when bringing up connectors) - Log warnings for all unused (and non-ignored) properties in the `WorkerConfig` after worker startup has taken place - Disable all warnings for unused properties when constructing admin clients used by connectors as those include the top-level worker config, which is guaranteed to contain properties like `key.converter` that are not used by the admin client - Permit all warnings for unused properties when constructing producers and consumers used by connectors as those do not include the top-level worker config and unused properties should not be expected in these cases - Automatically ignore all automatically-injected metrics context properties that are added by the Connect framework when configuring Kafka clients since these are always provided (when Connect brings up Kafka clients) but are not always used  I also fixed a bug introduced in https://github.com/apache/kafka/pull/8455 that causes a spurious warning to be logged when the worker config doesn't include a value for the `plugin.path` property.  ### Testing  I've verified this locally with a variety of cases including typos in the worker config (`gorup.id` instead of `group.id`), typos in connector client properties included in the worker config (`producer.clinet.id` instead of `producer.client.id`), correctly-skipped connector client properties included in the worker config (`consumer.max.poll.records`), connector client interceptor properties included in the worker config (`producer.interceptor.classes`, `some.interceptor.property.that.is.used`, `some.interceptor.property.that.is.not.used`), use of the DLQ topic in a sink connector, and use of automatic topic creation in a source connector. If this approach looks reasonable, I can automate these tests, probably by capturing logging output during an integration test run and asserting that warnings were issued only for the expected set of properties.  ### Edge cases  Note that the `RecordingMap` class is subtly broken at the moment in that it doesn't take into account calls to `Map::forEach`, `Map::entrySet`, `Map::keySet`, `Map::values`, `Map::getOrDefault`, `Map::compute`, `Map::computeIfPresent`, etc. This comes into play with cases like when custom settings are specified for internal topics (see [TopicAdmin.NewTopicBuilder::config](https://github.com/apache/kafka/blob/62ea4c46a9be7388baeaef1c505d3e5798a9066f/connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java#L224-L240)). We may not want to invest too heavily into this approach for controlling warning messages if we want to develop a truly flexible solution that can be easily used by both internal and external components.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-04-02T05:48:26Z","2022-04-08T18:10:29Z"
"","12444","KAFKA-14101: Improve documentation for consuming from embedded Kafka cluster topics in Connect integration testing framework","[Jira](https://issues.apache.org/jira/browse/KAFKA-14101)  Depends on https://github.com/apache/kafka/pull/12429, which should implement a logical fix for KAFKA-14101. This follow-up PR is intended to help harden our integration tests against the mistakes that caused KAFKA-14101 by: 1. Renaming `EmbeddedKafkaCluster::consume` to `EmbeddedKafkaCluster::consumeAtLeast` in order to clarify behavior and help distinguish between it and `EmbeddedKafkaCluster::consumeAll`. 2. Adding a note to the Javadocs for `EmbeddedKafkaCluster::consumeAtLeast` warning about out-of-order consumption and data missing from topic partitions.  Though ready for review now, this PR should not be merged until https://github.com/apache/kafka/pull/12429 is merged.","open","connect,","C0urante","2022-07-26T20:46:36Z","2022-08-01T14:23:42Z"
"","12429","KAFKA-14089: Only check for committed seqnos after disabling exactly-once support in Connect integration test","[Jira](https://issues.apache.org/jira/browse/KAFKA-14089)  This is a potential fix for the flakiness in the `ExactlyOnceSourceIntegrationTest::testSeparateOffsetsTopic` test. There's also a few minor improvements to prevent unnecessary `ERROR`-level log messages for shutdown of cancelled exactly-once source tasks, and unnecessary `WARN`-level log messages when creating exactly-once source task producers.  This fix should make the test resilient to unclean task and worker shutdown by (as the title indicates) only verifying data emitted by the source tasks up to the latest-committed offset; data after that point may exist in the topic written to by the task, but does not have to be accurate in order to retain the at-least-once delivery guarantees provided when exactly-once support is disabled.  Full disclosure: I haven't encountered any failures while running this locally, but I also haven't been able to replicate the failures described in the ticket, either.  This should also address [KAFKA-14101](https://issues.apache.org/jira/browse/KAFKA-14101), by tweaking the `testConnectorBoundary` method to use `consumeAll` instead of `consume`. The latter runs the risk of consuming out of order, and if we only consume as many messages as we want to verify, there may be unexpected gaps in those messages when we examine them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2022-07-21T22:56:41Z","2022-07-28T15:35:57Z"
"","12409","KAFKA-14058: Migrate ExactlyOnceWorkerSourceTaskTest from EasyMock and PowerMock to Mockito","[Jira](https://issues.apache.org/jira/browse/KAFKA-14058)  Some notes:  1. Introduced new `ConcurrencyUtils` and `MockitoUtils` classes for reusable testing logic that will likely be used in the near future for [KAFKA-14059](https://issues.apache.org/jira/browse/KAFKA-14059) and [KAFKA-14060](https://issues.apache.org/jira/browse/KAFKA-14060). 2. Refactored a lot of common logic into dedicated methods, which reduces test size and should make tests easier to write. 3. Doubled the default record batch size from 1 to 2. This provides better coverage and, after all the refactoring from step 2, required no modifications to mocking, verification, or assertion logic anywhere in the test suite. 4. Stopped inheriting from the `ThreadedTest` class as it does nothing (filed https://github.com/apache/kafka/pull/12410 to apply this change across the board). 5. Once this looks good enough to merge, I'll begin applying the same changes to the `WorkerSourceTaskTest` and possibly `AbstractWorkerSourceTaskTest` test suites.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-07-14T23:26:15Z","2022-07-26T01:56:41Z"
"","12366","KAFKA-14021: Implement new KIP-618 APIs in MirrorSourceConnector","[Jira](https://issues.apache.org/jira/browse/KAFKA-14021)  Implements the new `SourceConnector::exactlyOnceSupport` method in the source connector used by MirrorMaker 2. Since the connector tracks offsets using the Kafka Connect framework, exactly-once support is possible. However, we require that the consumer used to read from the source cluster is configured with the `read_committed` isolation level, since otherwise records from aborted and uncommitted transactions would be replicated. In the future, we may relax this constraint and replicate transaction boundaries directly, which would arguably also provide exactly-once support, but we'll have to update the Java consumer library to surface transaction boundary information in order to do this.  Does not implement the new SourceConnector::canDefineTransactionBoundaries method as the default is to return `ConnectorTransactionBoundaries.UNSUPPORTED`, which is correct for this connector as it is incapable of defining its own transaction boundaries  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-06-30T01:29:05Z","2022-07-26T01:56:34Z"
"","12355","KAFKA-14017: Implement new KIP-618 APIs in FileStreamSourceConnector","[Jira](https://issues.apache.org/jira/browse/KAFKA-14017)  Implements the new `SourceConnector::exactlyOnceSupport` method in the file source connector - When reading from stdin, returns `ExactlyOnceSupport.UNSUPPORTED` as we do not track offsets - When reading from a file, returns `ExactlyOnceSupport.SUPPORTED` as we do track offsets and, as long as the only modifications to the file after the connector is created are appends, those offsets should be perfectly accurate  Does not implement the new `SourceConnector::canDefineTransactionBoundaries` method as the default is to return `ConnectorTransactionBoundaries.UNSUPPORTED`, which is correct for this connector as it is incapable of defining its own transaction boundaries  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-06-27T23:48:35Z","2022-07-26T01:56:25Z"
"","12307","KAFKA-14006: Parameterize WorkerConnectorTest suite","[Jira](https://issues.apache.org/jira/browse/KAFKA-14006)  Tweaks this test class to add complete coverage for both source and sink connectors. This helped unearth a `NullPointerException` in the shutdown logic for sink connectors introduced by a not-yet-released change for KIP-618.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-06-18T18:42:00Z","2022-07-26T01:56:15Z"
"","12264","KAFKA-13967: Document guarantees for producer callbacks on transaction commit","[Jira](https://issues.apache.org/jira/browse/KAFKA-13967)  Also added some `` tags to help organize the rendered Javadocs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-06-08T04:27:48Z","2022-06-10T17:15:16Z"
"","12136","KAFKA-13773: catch kafkaStorageException to avoid broker shutdown directly","[jira](https://issues.apache.org/jira/browse/KAFKA-13773)   When logManager startup and loadLogs, we expect to catch any `IOException` (ex: out of space error) and turn the log dir into offline. Later, we'll handle the offline logDir in `ReplicaManager` [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1914), so that the `cleanShutdown` file won't be created when all logDirs are offline. The reason why the broker shutdown with cleanShutdown file after full disk is because during loadLogs and do log recovery, we'll write `leader-epoch-checkpoint` file [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogLoader.scala#L184). And if any IOException thrown, we'll wrap it as `KafkaStorageException` and rethrow [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/checkpoints/CheckpointFileWithFailureHandler.scala#L37-L42). And since we don't catch `KafkaStorageException`, so the exception is caught in the other place and go with clean shutdown path.  This PR is to fix the issue by catching the `KafkaStorageException` with `IOException` cause exceptions during loadLogs, and mark the logDir as offline to let the `ReplicaManager` handle the offline logDirs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-05-09T02:41:22Z","2022-06-04T08:22:19Z"
"","12019","KAFKA-13764: Improve balancing algorithm for Connect incremental rebalancing","[Jira](https://issues.apache.org/jira/browse/KAFKA-13764)  Depends on https://github.com/apache/kafka/pull/11983  The primary goal of this PR is to address several outstanding issues with incremental rebalancing that lead to stable-but-unbalanced clusters. However, other small bug fixes are also applied, and some liberty is taken with refactoring to improve readability and flexibility in the code base.  ~This should also address [KAFKA-12495](https://issues.apache.org/jira/browse/KAFKA-12495), and includes an adapted [test case](https://github.com/apache/kafka/pull/10367/files#diff-244a837479b827768c3daaabee8c7ad2064731377f1ad26a11bd890214252b7fR211-R358) from https://github.com/apache/kafka/pull/10367, which addresses that issue but with a different approach.~  High-level changes: - Refine the logic for load-balancing revocations: - - Perform load-balancing revocations any time the cluster appears imbalanced and there are still connectors and tasks that can be revoked from workers, instead of only when the number of workers in the cluster changes - - Remove the ""rough estimation"" logic and replace it with a precise calculation of exactly how to allocate all currently-configured connectors and tasks as evenly as possible across a cluster - - Account for load-balancing revocations when assigning new and lost connectors and tasks across the cluster - Improve code quality: - - Extract the `ConnectorsAndTasks` class into its own file, enrich it and its builder class with developer-friendly methods, make its contents completely immutable, and use `Set` instead of generic `Collection` instances to store connectors and tasks - - Where possible, identify logic that is shared for connectors and tasks (`IncrementalCooperativeAssignor::assignConnectors` and `IncrementalCooperativeAssignor::assignTasks`, for example) and abstract it into a single reusable method - - Use the `final` keyword for base and derived sets in `IncrementalCooperativeAssignor::performTaskAssignment` (tracking mutations across a 100+ line method is difficult) - - Reword unnecessary and confusing comments (""... is a derived set from the set difference of ..."" is not very informative) - - Reorganize the grouping of methods in `IncrementalCooperativeAssignor` to place static utility methods together at the bottom of the class - - Demote visibility of testing-only methods and fields from `protected` to package-private (`protected` implies that the field/method is intended for use by subclasses, which is not the case for any of these) - Testing: - - Add several new tests to cover a variety of new cases, many of which result in imbalanced allocation with the current rebalancing logic, but which are all correctly handled with the improvements in this PR - - Add a few testing utility methods to help ""hand wave"" test cases without having to specify fine-grained expectations like how many rounds of rebalance are required to reach stability after some changes have been applied to the cluster - - Add coverage to all tests that ensures that no connectors or tasks are both revoked and assigned from the sam worker, and that the leader's view of the complete assignment of connectors and tasks across the cluster appears to be correct after each rebalance - Miscellaneous: - - Demote a ton of noisy `DEBUG`-level log messages to `TRACE`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-04-08T17:18:23Z","2022-07-30T14:23:16Z"
"","11974","KAFKA-13763 (1): Improve unit testing coverage and flexibility for IncrementalCooperativeAssignor","[Jira](https://issues.apache.org/jira/browse/KAFKA-13763)  This is strictly a testing refactor. No functional changes are made; this can be easily verified by confirming that the only affected file is the `IncrementalCooperativeAssignorTest.java` test suite.  These changes were initially discussed during review of https://github.com/apache/kafka/pull/10367, which partially focused on improving readability in the unit tests for incremental rebalancing in Connect.  The goals here include: 1. Simplify the logic that has to be manually specified on a per-test-case basis for simulating a rebalance (accomplished by extracting common logic into reusable utility methods such as `performRebalance` and `addNewEmptyWorkers`) 2. Reduce the cognitive burden for following testing logic by removing unnecessary fields (like `expectedMemberConfigs` and `assignments`) and assertions (like the redundant checks for leader and leader URL) 3. Add powerful, granular, and reusable utility methods that can provide stronger guarantees about the state of a cluster across successive rebalances without forcing people to track this state in their heads (accomplished by replacing the existing `assertAssignments` method with `assertWorkers`, `assertEmptyAssignment`, `assertConnectorAllocations`, and `assertTaskAllocations`, and by adding the new `assertBalancedAllocation` and `assertCompleteAllocation` methods) 4. Refactor common logic for testing utilities to be more concise and reduce the number of Java 8 streams statements that have to be understood in order to read through a test case 5. Fix a bug in the assertion logic for checking for duplicates currently present [here](https://github.com/apache/kafka/blob/b2cb6caa1e9267c720c00fa367a277ee8509baea/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java#L1447-L1451) and [here](https://github.com/apache/kafka/blob/b2cb6caa1e9267c720c00fa367a277ee8509baea/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java#L1456-L1460) ([`List::removeAll`](https://docs.oracle.com/javase/8/docs/api/java/util/List.html#removeAll-java.util.Collection-) removes _all_ occurrences of any element contained in the collection passed to the method) 6. Remove an incorrect assertion in the `assertNoReassignments` (now renamed to `assertNoRedundantAssignments`) method that there should be no duplicated connectors or tasks in the assignments reported by each worker to the leader during rebalance (this is unnecessary and even contradicts logic used for testing in cases like [this](https://github.com/apache/kafka/blob/b2cb6caa1e9267c720c00fa367a277ee8509baea/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignorTest.java#L1112-L1125) where we intentionally simulate a worker with a duplicated set of connectors and/or tasks rejoining a cluster; the only reason this bug wasn't surfaced sooner is because the bug mentioned in the prior point covers it)  Once merged, this should allow for cleaner, faster test writing when adding new cases for incremental rebalancing, such as with https://github.com/apache/kafka/pull/10367.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-03-30T21:54:42Z","2022-04-05T19:45:52Z"
"","11983","KAFKA-13763 (2): Refactor IncrementalCooperativeAssignor for improved unit testing","[Jira](https://issues.apache.org/jira/browse/KAFKA-13763)  Builds on the changes from https://github.com/apache/kafka/pull/11974, which exclusively touched on the `IncrementalCooperativeAssignorTest` test suite.  The goals here include:  1. Create an overloaded variant of the `IncrementalCooperativeAssignor::performTaskAssignment` method that is more testing friendly by:     1. Returning the pre-serialization allocation and revocation of connectors and tasks across the cluster in a newly-introduced `ClusterAssignment` class, which eliminates the current pattern of creating a mock `IncrementalCooperativeAssignor` class, spying on one of its private methods, and capturing the argument passed to that spied-upon method     2. Accepting new parameters for the current snapshot of the config topic, the last-completed generation ID, and the current generation ID, which eliminates the need to create and manage a mocked `WorkerCoordinator` instance during testing     3. Not requiring parameters for the leader, config topic offset, or protocol version as these do not affect the logic for allocating connectors and tasks across a cluster     4. Only requires a `Map` for the set of currently-running connectors and tasks across the cluster, instead of a `Map`, which contains unnecessary information like the leader, leader URL, protocol version, and config topic offset 2. Simplify the parameter list for the `IncrementalCooperativeAssignor::handleLostAssignments` method, which in turn simplifies the logic for testing this class 3. Capture repeated Java 8 streams logic in simple, reusable, easily-verifiable utility methods added to the `ConnectUtils` class  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-04-01T19:00:44Z","2022-05-10T00:07:52Z"
"","11890","KAFKA-13731: Allow standalone workers to be started without providing any connector configurations","[Jira](https://issues.apache.org/jira/browse/KAFKA-13731)  (Copied from Jira):  In order to start a standalone Connect worker, it's currently necessary to provide at least two command-line arguments:  - The path to the worker config file - One or more paths to a connector config file  This may be due to the now-inaccurate belief that standalone workers do not support the Connect REST API, which can be used to create/reconfigure/delete connectors at runtime. However, standalone mode does in fact expose the Connect REST API, and since that allows for connectors to be created after bringing up the worker, it is unnecessary to force users to supply at least one config for a connector to be run on the worker at startup time.  These changes remove that limitation from standalone mode, and update the Connect docs to be explicit about support for the REST API when running standalone.  No tests are added as the functional changes are trivial. I did perform an extremely rudimentary sanity check to ensure that, with these changes, a standalone worker could be brought up with just a worker config file and no connector configs, and a file sink connector could be created on that worker via the REST API after it finished startup.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2022-03-14T18:04:05Z","2022-07-26T01:54:37Z"
"","11770","KAFKA-13669: Demote empty offset commit messages for source tasks to DEBUG level","[Jira](https://issues.apache.org/jira/browse/KAFKA-13669)  Enough users have provided feedback about this message being scary/spammy/misleading that we should take their thoughts into account and demote this message to a lower log level.","closed","","C0urante","2022-02-16T14:18:41Z","2022-02-17T01:53:24Z"
"","11894","KAFKA-13613: Remove hard dependency on HmacSHA256 algorithm for Connect","[Jira](https://issues.apache.org/jira/browse/KAFKA-13613)  Some JVMs don't come with the `HmacSHA256` algorithm out of the box, but do come with other key generation and/or MAC algorithms. However, it's impossible at the moment to run Connect on such a JVM, because the defaults for the `inter.worker.*.algorithm` properties are validated during worker startup, and that validation includes a check to make sure that the algorithm in question is provided by the worker's JVM.  To address this, automatic validation using the `ConfigDef::Validator` interface is disabled and all KIP-507 [1] related config validation is moved into the `DistributedConfig` constructor, which allows validation to take place only for the algorithms that the worker is actually configured to use. Any of these algorithms may still be the default, but if not, the default will never be validated.  In addition, a bug in the logic for ensuring that a worker's signature algorithm is included in its list of verification algorithms is fixed. The existing logic checks to see if its **key generation** algorithm is included in the list of verification algorithms, which does not adhere to KIP-507 [1] (see docstring for the `inter.worker.verification.algorithms` property), and serves no practical purpose.  [1] - https://cwiki.apache.org/confluence/display/KAFKA/KIP-507%3A+Securing+Internal+Connect+REST+Endpoints#KIP507:SecuringInternalConnectRESTEndpoints-ProposedChanges  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-03-14T22:19:17Z","2022-07-05T18:15:32Z"
"","11615","KAFKA-13546: Do not fail connector if default topic creation group is explicitly specified","[JIRA](https://issues.apache.org/jira/browse/KAFKA-13546)  ### Problem:  If user specifies `default` group as part of topic.creation.groups config, that is causing the connector to fail.  According to [KIP-158](https://cwiki.apache.org/confluence/display/KAFKA/KIP-158%3A+Kafka+Connect+should+allow+source+connectors+to+set+topic-specific+settings+for+new+topics), specifying `default` group as part of topic.creation.groups config should throw a warning, but not let connector fail.  ### Fix Remove the `default` topic creation group from user input if specified explicitly, and log a warning message  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","venkatteki","2021-12-19T05:27:45Z","2022-08-02T18:03:57Z"
"","11608","KAFKA-13533: Clean up resources on failed connector and task startup","[Jira](https://issues.apache.org/jira/browse/KAFKA-13533)  Introduces and leverages a new `Closeables` utility class that can be used to track accrued `AutoCloseable` objects via a try-with-resources block. Also switches the `Worker` class over to use `Plugins::withClassLoader` wherever possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2021-12-16T21:42:21Z","2022-08-01T14:23:20Z"
"","11903","KAFKA-13497: Add trace logging to RegexRouter","[JIRA](https://issues.apache.org/jira/browse/KAFKA-13497)  Adds runtime logging to the `RegexRouter` to show exactly which topics get routed where.  As noted in the Jira ticket, this is similar to existing logging in the `Cast` SMT: https://github.com/apache/kafka/blob/620f1d88d80fdf8150bd0b75be307bc4a2d3a0ea/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Cast.java#L174  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-03-16T01:51:18Z","2022-03-18T09:36:52Z"
"","11526","KAFKA-13472: Correct last-committed offsets tracking for sink tasks after partial revocation","[Jira](https://issues.apache.org/jira/browse/KAFKA-13472)  The `WorkerSinkTask.lastCommittedOffsets` field is now added to (via `Map::putAll`) after a successful offset commit, instead of being completely overwritten. In order to prevent this collection from growing indefinitely, elements are removed from it after topic partitions are revoked from the task's consumer.  Two test cases are added to `WorkerSinkTaskTest`:  - A basic test to verify the ""rewind for redelivery"" behavior when a task throws an exception from `SinkTask::preCommit`; surprisingly, no existing test cases appear to cover this scenario - A more sophisticated test to verify this same behavior, but with a few rounds of cooperative consumer rebalancing beforehand that expose a bug in the current logic for the `WorkerSinkTask` class  The `VerifiableSinkTask` class is also updated to only flush the requested topic partitions in its `flush` method. This is technically unrelated to the issue addressed by this PR and can be moved to a separate PR if necessary; including it here as the original context for identifying this bug was debugging failed system tests and the logic in this part of the tests was originally suspected as a cause of the test failure.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-11-22T21:45:26Z","2021-11-29T20:18:09Z"
"","11524","KAFKA-13469: Block for in-flight record delivery before end-of-life source task offset commit","[Jira](https://issues.apache.org/jira/browse/KAFKA-13469)  Although committing source task offsets without blocking on the delivery of all in-flight records is beneficial most of the time, it can lead to duplicate record delivery if there are in-flight records at the time of the task's end-of-life offset commit.  A best-effort attempt is made here to wait for any such in-flight records to be delivered before proceeding with the end-of-life offset commit for source tasks. Connect will block for up to `offset.flush.timeout.ms` milliseconds before calculating the latest committable offsets for the task and flushing those to the persistent offset store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-11-22T18:08:21Z","2021-11-30T16:39:09Z"
"","12191","KAFKA-12657: Increase timeouts in Connect integration tests","[Jira](https://issues.apache.org/jira/browse/KAFKA-12657)  As an initial step to address the notoriously flaky `BlockingConnectorTest` test suite, we can try increasing test timeouts.  This approach may not be sufficient, and even if it is, it's still suboptimal. Although it may address flakiness on Jenkins, it will make genuine failures harder to detect when testing local changes. Additionally, if the workload on Jenkins continues to increase, we'll probably have to bump these timeouts in the future again at some point.  Potential next steps, for this PR and beyond: 1. Stop leaking threads that block during test runs 2. Instead of artificially reducing the REST request timeout at the beginning of every test, reduce it selectively right before issuing a REST request that is expected to time out, and then immediately reset it. 3. Eliminate artificial reduction of the REST request timeout entirely, as it may be negatively impacting other Connect integration tests that are being run concurrently. 4. Test repeatedly on Jenkins, ideally at least 50 times 5. Gather information on the number of CPU cores available to each Jenkins node and the distribution of how many threads are allocated over a given time period (maybe a day?); this is especially relevant since local testing indicates that these tests all do much better when parallelism is reduced, which shouldn't be too surprising considering that each Connect integration test spins up separate threads for at least one Zookeeper node, one Kafka broker, one Connect worker, and usually at least one connector and one task.  I'd like to test these changes as a first step before investigating any of the above (except maybe items 1 and 2, which should be fairly straightforward). To trigger new runs I plan on pushing empty commits or, if those do not trigger new Jenkins runs, dummy commits. If this is objectionable let me know and hopefully we can find a suitable alternative.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2022-05-21T13:15:13Z","2022-06-02T08:21:08Z"
"","12092","KAFKA-13834: add normal test case","@showuon  I have add the normal test case, please help to review the pr, thanks  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ruanliang-hualun","2022-04-24T03:01:59Z","2022-04-24T09:06:20Z"
"","12224","KAFKA-13943: Make LocalLogManager implementation consistent with the RaftClient interface contract","1\ As per the interface contract for `RaftClient.scheduleAtomicAppend()` [1], it should throw a `NotLeaderException` exception when the provided current leader epoch does not match the current epoch. However, the current `LocalLogManager`'s implementation of the API returns a LONG_MAX instead of throwing an exception. This change fixes the behaviour and makes it consistent with the interface contract.  2\ As per the interface contract for `RaftClient.resign(epoch)` [2] if the parameter epoch does not match the current epoch, this call will be ignored. But in the current `LocalLogManager` implementation the leader epoch might change when the thread is waiting to acquire a lock on `shared.tryAppend()` (note that tryAppend() is a synchronized method). In such a case, if a NotALeaderException is thrown (as per code change in 1\ above), then resign should be ignored.    [1] https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/RaftClient.java#L145  [2] https://github.com/apache/kafka/blob/trunk/raft/src/main/java/org/apache/kafka/raft/RaftClient.java#L208","closed","","divijvaidya","2022-05-28T19:16:08Z","2022-07-07T10:06:37Z"
"","11845","MINOR: update the reassign-partition script results in docs","1. Remove [decommissioning brokers](https://kafka.apache.org/documentation/#basic_ops_decommissioning_brokers)  section When reading `decommissioning brokers` section in document, it makes me confused: ![image](https://user-images.githubusercontent.com/43372967/156732442-3e8452dd-fc7c-419a-9330-58b141cf2ccd.png)  > The partition reassignment tool does not have the ability to automatically generate a reassignment plan for decommissioning brokers yet.  We should be able to auto generate a reassignment plan for decommissioning brokers via `--generate` argument > the reassignment needs to ensure that all the replicas are not moved from the decommissioned broker to only one other broker.  We already can move the replicas evenly across all the brokers > To make this process effortless, we plan to add tooling support for decommissioning brokers in the future.   This section is written in 2015, and I don't think we still have plan to support it (or we already supported)  Remove the section.  2. Update the `--generate` replicas plan with current version.  Also, the example in the `--generate` command, it assign all the partition preferred leader to the same one (broker 5), which will confuse users to think this tool is unreliable. After running a test, I confirmed we can generate a plan evenly assigning the replicas across the brokers. Update it. ![image](https://user-images.githubusercontent.com/43372967/156733715-52c118cd-2269-4405-8cda-f3c879ca0155.png)  3. Update the `throttle` updated command by adding `--additional` After KIP-455 and this [PR](https://github.com/apache/kafka/pull/8891/), we have to provide `--additional` flag to allow change the throttle config.   4. Update the reassign-partition script output with current version   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-03-04T09:16:11Z","2022-06-02T13:33:11Z"
"","12344","MINOR: Use mock time in DefaultStateUpdaterTest","1. For most tests we would need an auto-ticking mock timer to work with draining-with-timeout functions. 2. For tests that check for never checkpoint we need no auto-ticking timer to control exactly how much time elapsed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-24T17:27:06Z","2022-06-29T19:33:03Z"
"","12439","KAFKA-10199: Further refactor task lifecycle management","1. Consolidate the task recycle procedure into a single function within the task. The current procedure now becomes: a) task.recycleStateAndConvert, at end of it the task is in `closed` while its stateManager is retained, and the manager type has been converted; 2) create the new task with old task's fields and the stateManager inside the creators. 2. Move the task execution related metadata into the corresponding `TaskExecutionMetadata` class, including the task idle related metadata (e.g. successfully processed tasks); reduce the number of params needed for `TaskExecutor` as well as `Tasks`. 3. Move the task execution related fields (embedded producer and consumer) and task creators out of `Tasks` and migrated into `TaskManager`. Now the `Tasks` is only a bookkeeping place without any task mutation logic. 4. When adding tests, I realized that we should not add task to state updater right after creation, since it was not initialized yet, while state updater would validate that the task's state is already `restoring / running`. So I updated that logic while adding unit tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-07-25T21:00:34Z","2022-07-28T21:31:12Z"
"","12363","HOTFIX: Correct ordering of input buffer and enforced processing sensors","1. As titled, fix the right constructor param ordering. 2. Also added a few more loglines.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-06-29T17:08:49Z","2022-07-03T17:09:49Z"
"","12370","KAFKA-13785: [10/N][emit final] more unit test for session store and disable cache for emit final sliding window","1. Added more unit test for `RocksDBTimeOrderedSessionStore` and `RocksDBTimeOrderedSessionSegmentedBytesStore ` 2. Disable cache for sliding window if emit strategy is `ON_WINDOW_CLOSE`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lihaosky","2022-06-30T21:24:45Z","2022-07-12T17:57:11Z"
"","12386","KAFKA-10199: Add PAUSE in state updater","1. Add pause action to task-updater.  2. When removing a task, also check in the paused tasks in addition to removed tasks. 3. Also I realized we do not check if tasks with the same id are added, so I add that check in this PR as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-07-07T01:29:45Z","2022-07-18T23:43:32Z"
"","11652","MINOR: Add num threads logging upon shutdown","1. Add num of threads logging upon shutdown. 2. Prefix the shutdown thread with client id.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-01-05T22:29:46Z","2022-01-06T19:28:34Z"
"","12204","[9/N][Emit final] Emit final for session window aggregations","1. Add a new API for session windows to range query session window by end time (KIP related). 2. Augment session window aggregator with emit strategy. 3. Minor: consolidated some dup classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2022-05-24T17:26:58Z","2022-06-29T16:22:41Z"
"","12306","KAFKA-13976: Improvements for OpenAPI specs","1) Added relevant APIResponses for all API endpoints. 2) Documented response codes/query-parameters/path-parameters/request body for all Connect related API endpoints.","open","connect,","vamossagar12","2022-06-18T17:54:23Z","2022-07-27T10:41:56Z"
"","12275","MINOR: Change groupInstanceId type from Optional to String in ConsumerGroupMetadata and GroupRebalanceConfig","- There is Optional groupInstanceId field in ConsumerGroupMetadata class and GroupRebalanceConfig class and that is one of the Optional anti-patterns so I changes to String and use getter method to return Optional type. - Add new ConsumerGroupMetadata constructor for if groupInstanceId not used.","open","","il-kyun","2022-06-09T12:42:02Z","2022-06-09T12:42:02Z"
"","12296","KAFKA-13996: log.cleaner.io.max.bytes.per.second can be changed dynamically","- Problem to be solved   - log.cleaner.io.max.bytes.per.second cannot be changed dynamically using bin/kafka-configs.sh   - For details, problem cause, reproduction procedure etc, refer to https://issues.apache.org/jira/browse/KAFKA-13996  - Implementation:   - Add updateDesiredRatePerSec() on Throttler   - Call updateDesiredRatePerSec() of Throttler with new log.cleaner.io.max.bytes.per.second value in reconfigure() of Log Cleaner   - I implemented the feature to be similar to [reconfigure() of SocketServer](https://github.com/apache/kafka/blob/fa59be4e770627cd34cef85986b58ad7f606928d/core/src/main/scala/kafka/network/SocketServer.scala#L336-L357)  - Alternative implementation considered (not adopted):   - re-instantiate Throttler with new log.cleaner.io.max.bytes.per.second value in reconfigure() of Log Cleaner      - However, since many parameter specifications are required to instantiate Throttler, I chose to be similar to SocketServer and update only log.cleaner.io.max.bytes.per.second  - Test:   - Added unit test in case of updating DesiredRatePerSec of Throttler   - I confirmed by manual testing that log.cleaner.io.max.bytes.per.second can be changed using bin/kafka-configs.sh:       - With this implementation, log.cleaner.io.max.bytes.per.second for Log Cleaner works as expected.       >   [2022-06-15 22:44:03,089] INFO [kafka-log-cleaner-thread-0]:         Log cleaner thread 0 cleaned log my-topic-0 (dirty section = [57585, 86901])         2,799.3 MB of log processed in 596.0 seconds (4.7 MB/sec).         Indexed 2,799.2 MB in 298.1 seconds (9.4 Mb/sec, 50.0% of total time)         Buffer utilization: 0.0%         Cleaned 2,799.3 MB in 298.0 seconds (9.4 Mb/sec, 50.0% of total time)         Start size: 2,799.3 MB (29,317 messages)         End size: 0.1 MB (1 messages)         100.0% size reduction (100.0% fewer messages)  (kafka.log.LogCleaner)`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tyamashi-oss","2022-06-15T14:13:53Z","2022-07-08T13:24:40Z"
"","12113","MINOR: Small cleanups in connect/mirror","- Make a few fields `final` - Remove unnecessary `throws`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-05-02T08:46:51Z","2022-05-10T06:49:57Z"
"","12272","MINOR: Update the README file in examples.","- I think the Intellij should be changed to IntelliJ IDEA. For details, please refer to the official website https://www.jetbrains.com/idea/ and https://en.wikipedia.org/wiki/IntelliJ_IDEA. - Modify Intellij to IntelliJ IDEA. - This change is a trivial rework / code cleanup without any test coverage.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","liuzhuang2017","2022-06-09T03:26:44Z","2022-07-20T08:37:02Z"
"","12478","KAFKA-13952 fix RetryWithToleranceOperator to respect infinite retries configuration","- https://issues.apache.org/jira/browse/KAFKA-13952: RetryWithToleranceOperator doesn't respect infinite retries config - i.e. when `errors.retry.timeout` is set to `-1` - From https://cwiki.apache.org/confluence/display/KAFKA/KIP-298%3A+Error+Handling+in+Connect:  >errors.retry.timeout: [-1, 0, 1, ... Long.MAX_VALUE], where -1 means infinite duration. - Also, from the config doc: https://github.com/apache/kafka/blob/0c4da23098f8b8ae9542acd7fbaa1e5c16384a39/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java#L129-L130 - This PR fixes the issue in `RetryWithToleranceOperator` along with a couple of unit tests to ensure the same","open","","yashmayya","2022-08-03T14:26:48Z","2022-08-03T14:26:48Z"
"","11990","KAFKA-13792: Fix wrong package name in Scala test files","- Fix wrong package name in `QuorumImplementation` and `UncleanLeaderElectionTest` files. - import classes  [Related Jira ticket](https://issues.apache.org/jira/browse/KAFKA-13792)  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YeonCheolGit","2022-04-02T17:11:34Z","2022-04-10T16:23:55Z"
"","11642","MINOR: Improve Connect docs","- Fix indendation of code blocks - Add links to all SMTs and Predicates   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-01-03T19:12:14Z","2022-02-22T23:35:45Z"
"","12251","MINOR: fix streams tutorial","- Fix inconsistency in store name in streams tutorial  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ocadaruma","2022-06-03T23:58:16Z","2022-06-04T07:59:20Z"
"","12474","KAFKA-13133 Replace EasyMock and PowerMock with Mockito for AbstractHerderTest","- Builds on top off https://github.com/apache/kafka/pull/11137 by @wycccccc which seems to have gone stale (so they should be credited as a co-author if this PR is merged) - Rebased onto latest `trunk`, fixed conflicts and replaced usage of `EasyMock` with `Mockito` in newly added tests - Addressed review comments by @mimaison on https://github.com/apache/kafka/pull/11137","closed","","yashmayya","2022-08-02T20:34:09Z","2022-08-03T09:24:57Z"
"","12042","KAFKA-13810 Document behavior of KafkaProducer.flush() w.r.t callbacks","*Update javadoc of `KafkaProducer.flush()` to state that all callbacks have been called before the method returns.*  *Since this updates the javadoc only, testing was generating the javadoc and reviewing it.*  This contribution is my original work and I license the work to the project under the project's open source license.","open","","karstenspang","2022-04-13T06:02:42Z","2022-04-30T21:02:49Z"
"","12154","KAFKA-12828 Remove deprecated methods in KeyQueryMetadata","*Remove the deprecated `getActiveHost()`, `getStandbyHosts()` and `getPartition()` methods in `KeyQueryMetadata`*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","HorizonNet","2022-05-12T20:23:13Z","2022-05-12T20:23:13Z"
"","12125","KAFKA-13864: provide the construct interceptor for KafkaProducer and KafkaConsumer","*Provide the constructor for producer and consumer to configure the interceptor.   ### Committer Checklist (excluded from commit message) -  Verify design and implementation  -  Verify test coverage and CI build status -  Verify documentation (including upgrade notes)","open","kip,","lqjack","2022-05-05T12:08:19Z","2022-05-18T07:21:41Z"
"","12094","MINOR: fix html generation syntax errors","*More detailed description of your change, The html document generation has some errors in it, specifically related to protocols. The two issues identified and resolved are:  - Missing `` closing tags added - Invalid usage of a `` tag as a wrapper element for `` elements. Changed the `` tag to be a ``.   *Summary of testing strategy (including rationale) Tested by running `./gradlew siteDocsTar` and observing that the output was properly formed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mt206","2022-04-25T21:17:37Z","2022-04-26T23:51:12Z"
"","12300","Update zookeeper version to 3.8.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* This change is a maintenance update to zookeeper client version  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kobihikri","2022-06-16T15:04:48Z","2022-06-22T07:27:28Z"
"","12199","KAFKA-13609: Use INVALID_CONFIG when config value is null or broker config validation fails","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* 1. Use INVALID_CONFIG when the config value is null  2. Use INVALID_CONFIG when broker config validation fails  *Summary of testing strategy (including rationale)* Existing test cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-05-24T08:32:14Z","2022-05-24T10:56:10Z"
"","12469","KAFKA-13914: Add command line tool kafka-metadata-quorum.sh","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Add `MetadataQuorumCommand` to describe quorum status, I'm trying to use arg4j style command format, currently, we only support one sub-command which is ""describe"".  ``` # describe quorum status kafka-metadata-quorum.sh --bootstrap-server localhost:9092 describe  # specify AdminClient properties kafka-metadata-quorum.sh --bootstrap-server localhost:9092 --command-config config.properties describe ```  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  MetadataQuorumCommandTest and MetadataQuorumCommandErrorTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-08-02T04:52:07Z","2022-08-03T03:19:28Z"
"","12369","Minor: add standard deviation measurement to the end to end latency test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  We have been trying to benchmark a DDS application by comparing it to Kafka. Added standard deviation measurement in the end to end latency test.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","cliu0013","2022-06-30T21:08:37Z","2022-06-30T21:08:37Z"
"","11510","MINOR: Fix `client.quota.callback.class` doc","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  The HTML output of [client.quota.callback.class](https://kafka.apache.org/documentation/#brokerconfigs_client.quota.callback.class) is slightly broken.   ![image](https://user-images.githubusercontent.com/11722533/142249366-d97b9fe7-874d-471f-80f1-cbd443b9a180.png)   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","joel-hamill","2021-11-17T17:17:05Z","2021-11-18T10:05:38Z"
"","12318","MINOR: Support --release in FeatureCommand","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Support --release in FeatureCommand, currently, we only have one feature which is metadata.version.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Added a test case in FeatureCommandTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-06-21T08:28:31Z","2022-07-29T05:45:12Z"
"","11488","fix: add command options to fix error","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  I'm following the quick start section in the documentation. When I run the command in the quickstart to create topic with the kafka_2.13-3.0.0, I've got error message `Missing required argument ""[partitions]""`.  The latest Kafka requires a partition option and replication factor option, so I added the options to the command. The 3 for the partitions and 1 for the replication factor I've added is just an example works fine for me.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hojongs","2021-11-10T23:26:05Z","2021-11-28T10:18:36Z"
"","12246","KAFKA-13718: kafka-topics describe topic with default config will show `segment.bytes` overridden config","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Effectively, commenting out the log.segment.bytes value to treat it as a default value instead of STATIC_BROKER_VALUE. Should have no effect given that the value is hardcoded in the code. Upon running integration tests, test which pass consistently passed.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Moovlin","2022-06-02T19:45:41Z","2022-06-03T13:43:36Z"
"","12035","KAFKA-13217: Reconsider skipping the LeaveGroup on close() or add an overload that does so","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  [implementation overview - KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-812%3A+Introduce+another+form+of+the+%60KafkaStreams.close%28%29%60+API+that+forces+the+member+to+leave+the+consumer+group)   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  @ableegoldman @guozhangwang","closed","kip,","sayantanu-dey","2022-04-12T14:25:35Z","2022-07-21T06:17:05Z"
"","12450","KAFKA-13809: Propagate full connector configuration to tasks in FileStream connectors","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  - https://issues.apache.org/jira/browse/KAFKA-13809 : The FileStream connectors don't propagate all connector configs to their tasks. - https://issues.apache.org/jira/browse/KAFKA-9228 : client overrides, converter configs, SMT configs may not be propagated to tasks on connector config updates. This isn't an issue for most connectors because connector configs are propagated to the tasks in the connector implementations. This isn't the case for the FileStream connectors, however.  - This PR updates the FileStream connectors to be in-line with how most other connectors generate task configs (which is a good thing because these connectors are intended to be example connector implementations), and also works around the bug from https://issues.apache.org/jira/browse/KAFKA-9228 - Minor: Also update the inaccurate source and sink connector Javadocs to align with the [source](https://github.com/apache/kafka/blob/0c5f5a7f8b3628e991459ba9cff414c675676b8b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSourceTask.java#L39-L41) and [sink](https://github.com/apache/kafka/blob/0c5f5a7f8b3628e991459ba9cff414c675676b8b/connect/file/src/main/java/org/apache/kafka/connect/file/FileStreamSinkTask.java#L36-L38) task Javadocs  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  - Manually tested the sink and source connectors and ensured that updates to SMT configs are reflected without manually restarting the tasks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","yashmayya","2022-07-28T08:19:41Z","2022-08-03T04:33:43Z"
"","12293","KAFKA-13963: Clarified java doc for processors api","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","tkaszuba","2022-06-14T08:23:52Z","2022-06-28T01:50:50Z"
"","12153","MINOR: Clarify impact of num.replica.fetchers","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","joel-hamill","2022-05-12T18:38:38Z","2022-05-16T16:38:07Z"
"","11803","KAFKA-13691: Rename target topic to custom name","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [*] Verify design and implementation  - [*] Verify test coverage and CI build status - [*] Verify documentation (including upgrade notes)","open","kip,","ecararus","2022-02-25T00:49:28Z","2022-03-01T15:50:10Z"
"","11964","KAFKA-13778: Fix follower broker also always execute preferred read-replica selection","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  The design purpose of the code is that only the leader broker can determine the preferred read-replica. But in fact, since the broker does not judge whether it is the leader or not, the follower will also execute the preferred read-replica selection.","closed","","bozhao12","2022-03-29T13:44:44Z","2022-03-29T15:14:09Z"
"","12419","fix KAFKA-14082","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","FreerXY","2022-07-18T16:17:04Z","2022-07-31T01:56:21Z"
"","12383","REVERT: Kip-770","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2022-07-06T00:53:30Z","2022-07-07T18:19:45Z"
"","12345","MINOR: Support KRaft in ReplicaFetchTest","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ahuang98","2022-06-24T22:51:42Z","2022-06-25T17:25:56Z"
"","12268","[MINOR] Fix typo","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2022-06-08T17:10:51Z","2022-06-08T20:51:41Z"
"","12266","3.1 spiffe id in front","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","MathieuTheerensKlarrio","2022-06-08T12:33:43Z","2022-06-08T12:34:09Z"
"","12193","test check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hudeqi","2022-05-22T05:52:33Z","2022-05-25T13:16:40Z"
"","12172","test12345","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","VedaGao","2022-05-17T06:59:37Z","2022-05-18T18:08:20Z"
"","12169","MINOR: improve description of `commit.interval.ms` config","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2022-05-16T19:12:55Z","2022-06-15T05:29:28Z"
"","12164","Update upgrade.html","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xjin-Confluent","2022-05-13T22:25:57Z","2022-05-16T11:23:11Z"
"","12149","KAFKA-13668: Retry upon missing initProducerId due to authorization error","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","philipnee","2022-05-11T20:58:06Z","2022-06-23T20:53:37Z"
"","12121","KAFKA-13846: Adding overloaded addMetricIfAbsent method","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2022-05-04T17:43:50Z","2022-06-15T21:15:44Z"
"","12038","[WIP] KAFKA-13421","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","philipnee","2022-04-12T23:43:52Z","2022-04-27T16:17:00Z"
"","11982","MINOR: Adjustments for jacoco","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","akatona84","2022-04-01T14:49:09Z","2022-04-01T14:49:09Z"
"","11979","MINOR: Clean up of TransactionManager and RecordAccumulator","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-03-31T13:46:18Z","2022-04-07T06:24:24Z"
"","11924","MINOR: fix typos in TransactionManager.java","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-03-21T12:30:00Z","2022-03-22T12:55:55Z"
"","11907","Kip 714 process client telemetry data","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sarat-kakarla","2022-03-16T15:16:49Z","2022-03-16T15:17:43Z"
"","11880","MINOR: Fix comments in TransactionsTest","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ddrid","2022-03-11T06:51:50Z","2022-03-11T07:42:44Z"
"","11876","Minor typo: ""result is a"" > ""result in a""","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","aSemy","2022-03-10T18:42:48Z","2022-03-10T19:08:36Z"
"","11874","Fix typos in configuration docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2022-03-09T23:08:02Z","2022-05-04T17:27:15Z"
"","11856","Repair how often index entries are generated","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Lovzi","2022-03-07T08:43:34Z","2022-03-16T00:30:18Z"
"","11855","MINOR: set `batch-size` option value into batch.size config in consoleProducer","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","peterwanner","2022-03-07T06:07:51Z","2022-03-15T11:40:11Z"
"","11838","KAFKA-1372: separate 400 error from 500 error in RestClient","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","blcksrx","2022-03-03T17:04:56Z","2022-07-20T10:46:00Z"
"","11826","2.8","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yyj8","2022-03-01T07:49:24Z","2022-03-04T03:13:44Z"
"","11825","3.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yyj8","2022-03-01T07:41:16Z","2022-03-04T03:13:44Z"
"","11822","[WIP] MINOR: add test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2022-03-01T03:38:52Z","2022-03-25T06:33:04Z"
"","11798","yes added","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","madhusyath5620","2022-02-23T14:01:07Z","2022-02-24T09:38:07Z"
"","11793","MINOR: Skip fsync on parent directory to start Kafka on ZOS","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hongyiz6639","2022-02-20T08:18:31Z","2022-02-25T03:50:00Z"
"","11712","WIP: Put failed tasks to end of processing list","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-01-25T16:49:33Z","2022-07-29T08:15:49Z"
"","11704","KAFKA-13590: rename InternalTopologyBuilder#topicGroups","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sayantanu-dey","2022-01-23T07:07:17Z","2022-04-09T10:32:44Z"
"","11703","KAFKA-13588: consolidate `changelogFor` methods to simplify the generation of internal topic names","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sayantanu-dey","2022-01-22T12:33:04Z","2022-04-20T18:39:03Z"
"","11697","MINIOR: add join timeout to avoid waiting forever","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-21T03:20:13Z","2022-01-24T02:33:53Z"
"","11685","Update dependencies.gradle","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shubhamsingh002","2022-01-17T17:08:16Z","2022-02-09T12:10:47Z"
"","11661","[WIP] MINOR: shutdown thread test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-10T03:54:31Z","2022-02-05T07:03:50Z"
"","11658","[WIP] MINOR: test leaderElection","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2022-01-08T09:16:57Z","2022-01-23T07:53:44Z"
"","11632","KIP-714 client WiP","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-12-30T19:09:59Z","2021-12-30T19:10:07Z"
"","11565","KAFKA-13504: Retry connect internal topics' creation in case of InvalidReplicationFactorException","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","akatona84","2021-12-03T14:21:34Z","2022-07-29T16:12:27Z"
"","11543","Update release.py","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shharrnam","2021-11-26T07:33:28Z","2022-02-05T05:30:44Z"
"","11542","KAFKA-7360 Fixed code snippet in v2.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vijaykriishna","2021-11-25T17:30:34Z","2021-11-26T07:37:55Z"
"","11540","KAFKA-7360 Fixed code snippet","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vijaykriishna","2021-11-25T13:42:03Z","2021-11-25T17:29:14Z"
"","11531","Dependency report generation using owaspSuppression for false positives","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","naanagon","2021-11-23T11:42:21Z","2021-11-23T11:47:33Z"
"","11516","MINOR: Use MessageDigest equals when comparing signature","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","naanagon","2021-11-19T12:14:59Z","2022-02-21T20:57:59Z"
"","11953","KAFKA-13772: Partitions are not correctly re-partitioned when the fetcher thread pool is resized","*More detailed description of your change, Fix bug of resizing fetch threads. Partitions are not correctly re-partitioned when the fetcher thread pool is resized.  There is a method `addFetcherForPartitions` which add partitions to `fetcherThreadMap` within the iteration of `fetcherThreadMap` for all fetchers, which could lead to skipping some fetchers and these fetchers remain their topic partition assignment. Plus, there is no unit test for this case so that this bug remains from 1.1.0 to the latest version.  This pull request is tend to make this bug fixed with unit test for resizing and code refectory included. The idea is just to collect all topic partition from fetch threads and add them distributed to new threads when the number of fetchers changes.  *Summary of testing strategy (including rationale) The unit test is to check every partition with `getFetchId` for new fetcher thread and also check `failedPartitions` with new fetcher thread number.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yufeiyan1220","2022-03-26T10:17:15Z","2022-03-31T12:54:38Z"
"","11858","KAFKA-13785: [Emit final][3/N][KIP-825] introduce a new API to control when aggregated results are produced","*More detailed description of your change, Change window api to add a emit final option for `SlidingWindow`, `SessionWindow` and `TimedWindow`.  *Summary of testing strategy (including rationale)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-03-07T17:50:27Z","2022-03-31T16:21:04Z"
"","11727","MINOR:fix AbstractStickyAssignor doc","*More detailed description of your change,  fix AbstractStickyAssignor doc  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Kvicii","2022-02-02T15:55:22Z","2022-02-03T09:54:55Z"
"","12238","KAFKA-13955: Fix failing KRaftClusterTest tests","*More detailed description of your change* Will will generate NoOpRecord periodically to increase metadata LEO, however, when a broker startup, we will wait until its metadata LEO catches up with the controller LEO, we generate NoOpRecord every 500ms and send heartbeat request every 2000ms.   It's almost impossible for a broker to catch up with the controller LEO if the broker sends a query request every 2000ms but the controller LEO increases every 500ms, so the tests in `KRaftClusterTest` will fail.  ![image](https://user-images.githubusercontent.com/26023240/171584207-82a33962-a2f0-4af9-a0bb-09a42bb030e1.png)   *Summary of testing strategy (including rationale)* After this change, the tests in `KRaftClusterTest` all succeed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","dengziming","2022-06-02T08:07:56Z","2022-06-03T18:47:29Z"
"","12024","MINOR: Move some TopicCommand and ConfigCommand integration tests to unit tests","*More detailed description of your change* When working on #11910, I found some tests don't need a ZK or KRaft cluster, moving those tests out will save us 20s of time approximately.  *Summary of testing strategy (including rationale)*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-09T23:30:26Z","2022-04-15T08:35:53Z"
"","12053","MINOR: Displaying default configs better in MetadataShell","*More detailed description of your change* When debugging some bugs related to configs, I find we are unable to show default broker/topic configs since the resourceName="""". I changed it to  similar to how we trait default client quotas.  *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-15T05:31:55Z","2022-04-24T10:23:27Z"
"","12109","KAFKA-13863: Prevent null config value when create topic in KRaft mode","*More detailed description of your change* When creating topics with customized configs, we should keep consistency with behavior in zk mode.  *Summary of testing strategy (including rationale)* Change `PlaintextAdminIntegrationTest.testCreateTopicsReturnsConfigs` to support KRaft mode.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-30T10:38:02Z","2022-05-19T16:46:49Z"
"","11913","MINOR: Remove scala KafkaException","*More detailed description of your change* We use org.apache.kafka.common.KafkaException instead of kafka.common.KafkaException  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-03-17T12:36:13Z","2022-03-21T21:56:26Z"
"","11603","MINOR: MetadataShell should handle ProducerIdsRecord","*More detailed description of your change* We store producer IDs in broker snapshots in #11527, I think we could also add the ability to inspect it using MetadataShell.  *Summary of testing strategy (including rationale)* I tested this locally: ``` [ Kafka Metadata Shell ] >> cat /producerIds/broker 1 >> cat /producerIds/blockStart 0 >> cat /producerIds/blockEnd 999 >>  ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-12-14T12:20:50Z","2022-02-12T19:11:22Z"
"","12108","KAFKA-13862: Support Append/Subtract multiple config values in KRaft mode","*More detailed description of your change* We can append/subtract multiple values in kraft mode, for example: append/subtract topic config ""cleanup.policy"" with value=""delete,compact"" will end up treating ""delete,compact"" as a value not 2 values.  *Summary of testing strategy (including rationale)* Change `PlaintextAdminIntegrationTest.testValidIncrementalAlterConfigs` to support KRaft mode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-30T09:30:36Z","2022-05-10T22:35:22Z"
"","11659","KAFKA-13503: Validate broker configs for KRaft","*More detailed description of your change* Validate broker configs for KRaft  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-01-08T15:35:16Z","2022-02-07T07:43:34Z"
"","11934","KAFKA-12908: Load raft snapshot heuristic (WIP)","*More detailed description of your change* TODO   *Summary of testing strategy (including rationale)* TODO  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-03-23T05:48:50Z","2022-03-23T06:02:23Z"
"","12059","MINOR: Enable testUpdateFeaturesWithForwarding","*More detailed description of your change* This test was removed in #11667 since UpdateFeatures is not properly handled in KRaft mode, now we can bring it back since UpdateFeatures is properly handled after #12036.  *Summary of testing strategy (including rationale)* No  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-16T04:40:46Z","2022-06-14T13:25:56Z"
"","11655","KAFKA-13316; Enable KRaft mode in CreateTopics tests","*More detailed description of your change* This PR follows #11629 to enable `CreateTopicsRequestWithForwardingTest` and `CreateTopicsRequestWithPolicyTest` in KRaft mode.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-01-07T04:58:02Z","2022-02-10T22:10:24Z"
"","11522","KAFKA-13242: Ensure UpdateFeatures is properly handled in KRaft","*More detailed description of your change* This patch fixes several problems with the UpdateFeatures API in KRaft:  KafkaApis did not properly forward this request type to the controller. ControllerApis did not handle the request type. Some other small fixes.  *Summary of testing strategy (including rationale)* 1. unit test in FeatureControlManagerTest 2. I tried to converted UpdateFeaturesTest to use KRaft, but I encountered some problems, for example, we use totally different class in KRaft and zk mode, and we can overwrite data in zk path but we can't overwrite data in KRaft metadata topic, so I decided to handle this in a future PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","dengziming","2021-11-22T11:20:06Z","2022-04-15T02:08:23Z"
"","11645","KAFKA-13005: Support jbod in KRaft","*More detailed description of your change* This is based on #9577  *Summary of testing strategy (including rationale)* 1. Add unit test for the relating logic 2. change LogDirFailureTest to support KRaft  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-01-04T07:15:39Z","2022-01-04T07:15:39Z"
"","12062","KAFKA-13833: Remove the min_version_level from the finalized version written to ZooKeeper","*More detailed description of your change* This is a subtask for KIP-778, use a Short to represent the finalized version, and remove min_version_level from the corresponding ZNode. Core changes are : 1. Bump IBP and FeatureZNode version, Change schema of features from `Map[String, Map[String, Short]]` to `Map[String, Short]` 2. Remove min_version_level from ZNode, but don't change the JSON schema of ZkData  *Summary of testing strategy (including rationale)* Existing test case and newly added test case.  following tasks: Change ApiversionResponse.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-18T06:22:14Z","2022-05-25T18:02:34Z"
"","12265","KAFKA-13968: Fix 3 major bugs of KRaft snapshot generating","*More detailed description of your change* There are 3 bugs when a broker generates a snapshot.  1. Broker should not generate snapshots until it starts publishing. Before a broker starts publishing, `BrokerMetadataListener._publisher=None`,  so `_publisher.foreach(publish)` will do nothing, so `featuresDelta.metadataVersionChange().isPresent` is always true, so we will generating a snapshot on every commit since we believe metadata version has changed, here are the logs, note offset 1 is a LeaderChangeMessage so there is no snapshot:  ``` [2022-06-08 13:07:43,010] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 0... (kafka.server.metadata.BrokerMetadataSnapshotter:66) [2022-06-08 13:07:43,222] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 2... (kafka.server.metadata.BrokerMetadataSnapshotter:66) [2022-06-08 13:07:43,727] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 3... (kafka.server.metadata.BrokerMetadataSnapshotter:66) [2022-06-08 13:07:44,228] INFO [BrokerMetadataSnapshotter id=0] Creating a new snapshot at offset 4... (kafka.server.metadata.BrokerMetadataSnapshotter:66) ```  2. We should compute `metadataVersionChanged` before `_publisher.foreach(publish)` After `_publisher.foreach(publish)` the `BrokerMetadataListener_delta` is always Empty, so `metadataVersionChanged` is always false, this means we will never trigger snapshot generating even metadata version has changed.  3. We should try to generate a snapshot when starting publishing When we started publishing, there may be a metadata version change, so we should try to generate a snapshot before first publishing.  *Summary of testing strategy (including rationale)* A unit test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-06-08T07:11:49Z","2022-07-13T02:08:28Z"
"","12274","KAFKA-13959: Controller should unfence Broker with busy metadata log","*More detailed description of your change* The reason is a little complex, the two keys to this problem are: 1. `KafkaRaftClient.MAX_FETCH_WAIT_MS==MetadataMaxIdleIntervalMs == 500ms`. We rely on fetchPurgatory to complete a FetchRequest, in details, if `FetchRequest.fetchOffset >= log.endOffset`, we will wait for 500ms to send a FetchResponse. 2. The follower needs to send one more FetchRequest to get the HW.  here is the event sequence: 1. When starting the leader(active controller) LEO=m+1(m is the offset of the last record), leader HW=m(because we need more than half of the voters to reach m+1) 2. follow(standby controller) and observer(broker) send FetchRequest(fetchOffset=m) 3.1. leader receives FetchRequest, set leader HW=m and waits 500ms before send FetchResponse 3.2. leader send FetchResponse(HW=m) 3.3 broker receive FetchResponse(HW=m), set metadataOffset=m. 4. leader append `NoOpRecord`, LEO=m+2. leader HW=m 5. looping 1-4  After change MAX_FETCH_WAIT_MS=200(less than half of MetadataMaxIdleIntervalMs), this problem can be solved temporary.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-06-09T10:48:50Z","2022-08-02T06:42:37Z"
"","12065","KAFKA-13788: Use AdminClient.incrementalAlterConfigs in ConfigCommand","*More detailed description of your change* The bug can be reproduced following KAFKA-13788, when creating an invalid broker config, we wouldn't treat it as invalid and will treat it as sensitive, and if we want to create a valid broker config, we will fail since the deprecated `AdminClient.alterConfigs` requires we specify all sensitive configs.  1. If we user `AdminClient.incrementalAlterConfigs`, this can be avoided. 2. I find we lack integration tests for altering topic and broker configs, so I added a bunch of test cases. 3. As a convenience, I added KRaft support for `ConfigCommandIntegrationTest`  *Summary of testing strategy (including rationale)* A integration test `testUpdateConfigNotAffectedByInvalidConfig`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-19T02:23:40Z","2022-05-19T11:26:01Z"
"","12157","MINOR: Support co-resident mode in KRaft TestKit","*More detailed description of your change* The behavior between co-resident mode and discrete mode is different, so we should support co-resident mode too. I also find a bug in co-resident, see KAFKA-13228.  *Summary of testing strategy (including rationale)* Changed existing clusterTest to support co-resident mode except for ApiVersionsRequestTest, which is related to a bug.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-05-13T08:45:47Z","2022-07-06T08:03:21Z"
"","12116","MINOR: Remove identical test cases in MetadataVersionTest","*More detailed description of your change* Removed some tests in `MetadataVersionTest` which are totally identical to those in `ApiVersionsResponseTest`. I think we removed `ApiVersionTest` but copied the tests in them to both `MetadataVersionTest` and `ApiVersionsResponseTest`.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-05-03T06:30:56Z","2022-05-04T03:10:39Z"
"","11785","MINOR: Remove unused AdminZkClient in MetadataSupport","*More detailed description of your change* Remove unused AdminZkClient in MetadataSupport  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-02-17T10:08:01Z","2022-02-18T06:31:45Z"
"","11935","MINOR: Remove some unused code","*More detailed description of your change* Remove 2 classes no longer used.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-03-23T06:24:07Z","2022-04-01T03:39:51Z"
"","11910","KAFKA-13743: Prevent topics with conflicting metrics names from being created in KRaft mode","*More detailed description of your change* In zk mode, the topic ""foo_bar"" will conflict with ""foo.bar"" because of limitations in metric names, we should implement this in KRaft mode. Add an itcase in TopicCommandIntegrationTest and change TopicCommandIntegrationTest to support KRaft mode.  *Summary of testing strategy (including rationale)* Added a unit test, and also an integration test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-03-17T08:38:38Z","2022-04-14T12:50:15Z"
"","12093","KAFKA-13620: Use different metric name for KRaft SocketServer","*More detailed description of your change* In traditional we only have one SocketServer and one KafkaRequestHandlerPool in a process. In KRaft cluster, especially in co-resident mode, there may be multiple SocketServer and KafkaRequestHandlerPool, so we should avoid using the same MetricName. I just added a prefix for the SocketServer and KafkaRequestHandlerPool in ControllerServer, to differentiate between the ones in BrokerServer.  Note that I didn't change Acceptor and Processor, since their metric name tags contain a listenerName, which will always be different for ControllerServer and BrokerServer.  *Summary of testing strategy (including rationale)* 1. integration tests. 2. Tested locally, now there are 2 KafkaRequestHandlerPool metrics in KRaft co-resident mode, compared to only one before.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-24T06:13:22Z","2022-04-24T06:18:55Z"
"","12060","KAFKA-13832: Fix flaky testAlterAssignment","*More detailed description of your change* In KRaft mode the metadata is not propagate in time, so we should should wait for it before make assertions.   *Summary of testing strategy (including rationale)* No  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-16T12:15:14Z","2022-04-19T07:08:30Z"
"","11936","MINOR: GetOffsetShell should ignore partitions without offsets","*More detailed description of your change* In KIP-815 we replaced `KafkaConsumer` with `AdminClient` in GetOffsetShell. In the previous implementation, partitions were just ignored if there is no offset for them, however, we will print -1 instead now, This PR fix this inconsistency.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-03-23T06:49:10Z","2022-03-31T02:38:38Z"
"","11726","KAFKA-13637: Use default.api.timeout.ms as default timeout value for KafkaConsumer.endOffsets","*More detailed description of your change* In KafkaConsumer, we use `request.timeout.ms` in `endOffsets` whereas `default.api.timeout.ms` when in `beginningOffsets` and `offsetsForTimes`, we should use `default.api.timeout.ms` for all of them.   *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-02-02T06:49:47Z","2022-02-03T09:32:27Z"
"","12236","MINOR: Use right enum value for broker registration change","*More detailed description of your change* In `ClusterControlManager.replay` and `ReplicationControlManager`, we use `BrokerRegistrationFencingChange.FENCE` when unfencing a broker and use `BrokerRegistrationFencingChange.UNFENCE` when fencing a broker, this is confusing.  *Summary of testing strategy (including rationale)* Didn't change any logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-06-02T03:37:07Z","2022-08-02T12:38:53Z"
"","12234","MINOR: KRaft controller should return right features in ApiVersionResponse","*More detailed description of your change* I find many error logs when running tests, here is one of them: https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-12232/1/testReport/kafka.server/KRaftClusterTest/Build___JDK_8_and_Scala_2_12___testUnregisterBroker__/  ``` [2022-05-31 15:56:28,190] ERROR [Controller 3000] Error replaying record FeatureLevelRecord(name='metadata.version', featureLevel=5) at last offset 0. (org.apache.kafka.controller.QuorumController:1188) java.lang.RuntimeException: Controller cannot support feature metadata.version at version 5 	at org.apache.kafka.controller.FeatureControlManager.replay(FeatureControlManager.java:269) 	at org.apache.kafka.controller.QuorumController.replay(QuorumController.java:1166) 	at org.apache.kafka.control ...[truncated 844207 chars]... ```  this is because we just return empty supported features in Controller ApiVersionResponse, so the `quorumSupportedFeature` will always return empty: https://github.com/apache/kafka/blob/4c9eeef5b2dff9a4f0977fbc5ac7eaaf930d0d0e/metadata/src/main/java/org/apache/kafka/controller/QuorumFeatures.java#L89-L94  *Summary of testing strategy (including rationale)* After this change, these error logs disappeared.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-06-01T07:50:23Z","2022-06-14T11:10:27Z"
"","11626","MINOR: Ensure AlterIsr is forwarded to Controller in KRaft","*More detailed description of your change* Ensure AlterIsr is forwarded to Controller in KRaft  *Summary of testing strategy (including rationale)* I want to use some existing ITCase of AlterIsr but I can't find one, so I am working on this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-12-24T07:13:03Z","2021-12-25T02:32:21Z"
"","12110","MINOR: Enable some AdminClient integration tests","*More detailed description of your change* Enable KRaft in AdminClientWithPoliciesIntegrationTest and PlaintextAdminIntegrationTest There are some tests not enabled or not as expected yet: 1. testNullConfigs, see KAFKA-13863 2. testDescribeCluster and testMetadataRefresh, currently we don't get the real controller in KRaft mode so the test may not run as expected.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-30T12:30:57Z","2022-05-18T16:39:27Z"
"","12112","MINOR: Fix flaky testDescribeUnderReplicatedPartitions","*More detailed description of your change* Currently, we are waiting for metadataCache to bookkeeper the partition info, this isn't enough, we should wait until the partition ISR is less than AR.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-05-01T02:39:27Z","2022-05-12T12:55:42Z"
"","11889","KAFKA-13667: Make listeners mandatory in kraft mode","*More detailed description of your change* Currently, default ""listeners"" value for kraft broker node is permitted but it's not allowed for kraft controller node and combine node, this is not very elegant so we'd better make ""listeners"" mandatory.    *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-03-14T10:38:15Z","2022-05-03T06:12:41Z"
"","12103","KAFKA-13850: Show missing record type in MetadataShell","*More detailed description of your change* Currently, 8 record types are missing in MetadataShell:  1. AccessControlEntryRecord and RemoveAccessControlEntryRecord are added in KIP-801, FeatureLevelRecord and maybe RemoveFeatureLevelRecord will be added in KIP-778, I added these 4 record types in MetadataShell. 2. AccessControlRecord will not be used since we use AccessControlEntryRecord. 3. UserScramCredentialRecord, DelegationTokenRecord and BrokerRegistrationChangeRecord will be used in the future, but we can't make sure whether their schema be changed, just like AccessControlRecord which is outdated. so I didn't added them here.  *Summary of testing strategy (including rationale)* Unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-28T02:09:06Z","2022-08-03T02:39:48Z"
"","11746","MINOR: Use timeout config in KafkaAdminClient","*More detailed description of your change* Currently we use a fixed value 3600000 as requestTimeout when creating NetworkClient in KafkaAdminClient, it's better to use the `request.timeout.ms` config like KafkaConsumer and KafkaProducer.  In fact, this change has no effect unless request.timeout.ms is set to a value greater than 3600000.  *Summary of testing strategy (including rationale)*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-02-10T13:10:05Z","2022-02-13T04:31:39Z"
"","11786","MINOR: Add KRaft broker api to protocol docs","*More detailed description of your change* Currently in the [kafka protocols page](https://kafka.apache.org/protocol), we miss some KRaft protocol, for example DescribeQuorumRequest(55) ![image](https://user-images.githubusercontent.com/26023240/154472292-b315a9dd-3051-4a04-9823-6426ab36f77f.png)   *Summary of testing strategy (including rationale)* After this change, DescribeQuorumRequest is listed in the table: ![image](https://user-images.githubusercontent.com/26023240/154473509-8331586a-4bcd-46e4-9eb8-6b29181284f6.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-02-17T11:36:37Z","2022-05-11T14:56:36Z"
"","12165","KAFKA-13905: Fix failing ServerShutdownTest.testCleanShutdownAfterFailedStartupDueToCorruptLogs","*More detailed description of your change* Before #11969, We will throw an `ExecutionException(KafkaStorageException)` in `BrokerServer.startup`, and the outside ExecutionException is removed in finally block. After #11969, We will throw a `RuntimeException(ExecutionException(KafkaStorageException))`, so this test is constantly failing since the Exception type is not consistent.  To fix this, we just need to invoke `getCause` twice to remove RuntimeException and ExecutionException.  *Summary of testing strategy (including rationale)* This test is no longer failing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-05-16T08:40:20Z","2022-05-17T18:41:03Z"
"","12105","KAFKA-13859: Disable idempotence on SCRAM authentication","*More detailed description of your change* After upgrading from 2.X to 3.0.1 we set ""enable.idempotence""=true by default, but the `InitProducerIdRequest` involves the  `TRANSACTIONAL_ID` are authorized to write if we are using SCRAM authentication, or it will fail.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-28T07:35:32Z","2022-05-03T02:12:01Z"
"","12076","MINOR: Fix SubscriptionInfoData name in exception message","*More detailed description of your change* A small fix because of SubscriptionInfo.json has been renamed to SubscriptionInfoData.json  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-04-21T05:42:27Z","2022-04-30T21:02:48Z"
"","11667","MINOR; Enable Kraft in ApiVersionTest","*More detailed description of your change* 1. Replace ClusterTestExtensions with QuorumTestHarness 2. Enable Kraft in ApiVersionTest  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-01-12T02:01:58Z","2022-02-15T18:16:04Z"
"","11918","MINOR: Small improvements for KAFKA-13587","*More detailed description of your change* 1. Fix a bug: comparing values of types Byte and LeaderRecoveryState using `==` will always yield false 2. Show LeaderRecoveryState in MetadataShell  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-03-19T01:50:03Z","2022-03-21T21:33:52Z"
"","12294","KAFKA-13990: KRaft controller should return right features in ApiVersionResponse","*More detailed description of your change*  Updating metadata.version using AdminClient will fail with the following exception:  ``` java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.InvalidUpdateVersionException: Invalid update version 7 for feature metadata.version. Controller 3000 does not support this feature. ```  This is because we just return empty supported features in Controller ApiVersionResponse, so `QuorumFeatures.reasonNotSupported` will always assume we don't support this version.  *Summary of testing strategy (including rationale)* KRaftClusterTest.testUpdateMetadataVersion  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-06-14T11:14:59Z","2022-07-21T03:02:47Z"
"","11784","KAFKA-13228; Ensure ApiVersionRequest is properly handled KRaft co-resident mode","*More detailed description of your change*  This is based on #12157  Normally, a broker connects to the active controller to learn its ApiVersions, and in co-resident KRaft mode, a broker gets the ApiVersions directly from binary code if the broker is also the activate controller.  But we only concerns `ApiKeys.zkBrokerApis()` when we call `NodeApiVersions.create()`, we should use `ApiKeys.controllerApiVersions` when in KRaft mode.   *Summary of testing strategy (including rationale)*  When I described quorum in Kraft mode I got `org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support DESCRIBE_QUORUM`.  After this change, the DESCRIBE_QUORUM request was property handled and got a correct response: ``` TopicData(topicName='__cluster_metadata', partitions=[PartitionData(partitionIndex=0, errorCode=0, leaderId=1, leaderEpoch=30, highWatermark=141, currentVoters=[ReplicaState(replicaId=1, logEndOffset=141)], observers=[])]) ```  And I also add an integration test for this.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-02-17T09:47:41Z","2022-07-05T22:19:00Z"
"","11951","KAFKA-13836: Improve KRaft broker heartbeat logic","*More detailed description of your change*  Don't advertise an offset to the controller until it has been published  *Summary of testing strategy (including rationale)* This is really arduous work, I added a test in BrokerLifecycleManagerTest to test the whole state conversion.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2022-03-25T15:39:58Z","2022-05-20T08:01:54Z"
"","12084","KAFKA-13845: Add support for reading KRaft snapshots in kafka-dump-log","*More detailed description of your change*  1. Support metadata snapshot suffix "".checkpoint"" 2. print metadata snapshot header and footer   Here is the output of the command: ``` kafka-dump-log.sh --cluster-metadata-decoder --files path/to/logs/00000000000000000000-0000000000.checkpoint  Dumping /var/folders/xs/1lh3bwpj2674ch_3wyqbcv6c0000gn/T/kafka-3001968433464842910/kafka-269189/00000000000000000000-0000000000.checkpoint Snapshot end offset: 0, epoch: 0 baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: true deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1653548892941 size: 83 magic: 2 compresscodec: none crc: 3406426388 isvalid: true | offset: 0 CreateTime: 1653548892941 keySize: 4 valueSize: 11 sequence: -1 headerKeys: [] SnapshotHeader {""version"":0,""lastContainedLogTimestamp"":10000} baseOffset: 1 lastOffset: 4 count: 4 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 83 CreateTime: 1653548892941 size: 237 magic: 2 compresscodec: none crc: 2671055946 isvalid: true | offset: 1 CreateTime: 1653548892941 keySize: -1 valueSize: 36 sequence: -1 headerKeys: [] payload: {""type"":""REGISTER_BROKER_RECORD"",""version"":0,""data"":{""brokerId"":0,""incarnationId"":""AAAAAAAAAAAAAAAAAAAAAA"",""brokerEpoch"":10,""endPoints"":[],""features"":[],""rack"":"""",""fenced"":true}} | offset: 2 CreateTime: 1653548892941 keySize: -1 valueSize: 36 sequence: -1 headerKeys: [] payload: {""type"":""REGISTER_BROKER_RECORD"",""version"":0,""data"":{""brokerId"":1,""incarnationId"":""AAAAAAAAAAAAAAAAAAAAAA"",""brokerEpoch"":20,""endPoints"":[],""features"":[],""rack"":"""",""fenced"":true}} | offset: 3 CreateTime: 1653548892941 keySize: -1 valueSize: 31 sequence: -1 headerKeys: [] payload: {""type"":""TOPIC_RECORD"",""version"":0,""data"":{""name"":""test-topic"",""topicId"":""is0zVR4mQQmbu-3eNwcUJA""}} | offset: 4 CreateTime: 1653548892941 keySize: -1 valueSize: 45 sequence: -1 headerKeys: [] payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":0,""topicId"":""onrG6joYREqEnYDbNzxIXA"",""isr"":[0,1,2],""leader"":1}} baseOffset: 5 lastOffset: 5 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: true deleteHorizonMs: OptionalLong.empty position: 320 CreateTime: 1653548892941 size: 75 magic: 2 compresscodec: none crc: 581232483 isvalid: true | offset: 5 CreateTime: 1653548892941 keySize: 4 valueSize: 3 sequence: -1 headerKeys: [] SnapshotFooter {""version"":0} ```  we can skip record metadata ``` kafka-dump-log.sh --cluster-metadata-decoder --skip-record-metadata --files path/to/logs/00000000000000000000-0000000000.checkpoint  Dumping /var/folders/xs/1lh3bwpj2674ch_3wyqbcv6c0000gn/T/kafka-3001968433464842910/kafka-269189/00000000000000000000-0000000000.checkpoint Snapshot end offset: 0, epoch: 0 baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: true deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1653548892941 size: 83 magic: 2 compresscodec: none crc: 3406426388 isvalid: true  baseOffset: 1 lastOffset: 4 count: 4 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 83 CreateTime: 1653548892941 size: 237 magic: 2 compresscodec: none crc: 2671055946 isvalid: true  payload: {""type"":""REGISTER_BROKER_RECORD"",""version"":0,""data"":{""brokerId"":0,""incarnationId"":""AAAAAAAAAAAAAAAAAAAAAA"",""brokerEpoch"":10,""endPoints"":[],""features"":[],""rack"":"""",""fenced"":true}}  payload: {""type"":""REGISTER_BROKER_RECORD"",""version"":0,""data"":{""brokerId"":1,""incarnationId"":""AAAAAAAAAAAAAAAAAAAAAA"",""brokerEpoch"":20,""endPoints"":[],""features"":[],""rack"":"""",""fenced"":true}}  payload: {""type"":""TOPIC_RECORD"",""version"":0,""data"":{""name"":""test-topic"",""topicId"":""is0zVR4mQQmbu-3eNwcUJA""}}  payload: {""type"":""PARTITION_CHANGE_RECORD"",""version"":0,""data"":{""partitionId"":0,""topicId"":""onrG6joYREqEnYDbNzxIXA"",""isr"":[0,1,2],""leader"":1}} baseOffset: 5 lastOffset: 5 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: true deleteHorizonMs: OptionalLong.empty position: 320 CreateTime: 1653548892941 size: 75 magic: 2 compresscodec: none crc: 581232483 isvalid: true ```  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2022-04-22T07:32:28Z","2022-06-01T21:49:00Z"
"","12132","MINOR: fix typo for QUORUM_FETCH_TIMEOUT_MS_DOC","*Fix type","closed","","lqjack","2022-05-07T03:30:56Z","2022-05-10T02:47:23Z"
"","12319","MINOR: `CoreUtils.swallow` use `Logging` in parameters","*`CoreUtils.swallow` not use `Logging` in parameters, fixed it.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fxbing","2022-06-21T10:19:11Z","2022-06-21T12:12:55Z"
"","11797","KAFKA-12879: Revert changes from KAFKA-12339 and instead add retry capability to KafkaBasedLog","**Summary** Fixes the compatibility issue regarding KAFKA-12879 by reverting the changes to the admin client from KAFKA-12339 (#10152), and instead perform the retries within the KafkaBasedLog via a new `TopicAdmin.retryEndOffsets(..)`.  This method will delegate to the existing `TopicAdmin.endOffsets(...)` and will retry on RetriableException until the _retries_ (max retries) is hit, or until a specified timeout.    This change should be backward compatible to the KAFKA-12339.  Notable changes are: **Reverted KAFKA-12339**  **TopicAdmin** Added new `retryEndOffsets(...)` method that utilizes the new `RetryUtil` to list end offsets for a set of topic partitions and perform retries upon any RetriableException (including UnknownTopicOrPartitionException, TimeoutException).  **KafkaBasdLog** During startup calls `TopicAdmin.retryEndOffsets(...)` to read end offsets for the log topic and retries if the topic is not available (or any RetriableException).  **RetryUtil** A general utility that takes a Callable function, the total duration to retry (may be 0), and backoff time in ms, and that calls the function at least once, and performs retries using the parameters when the function throws RetriableException (e.g. UnknownTopicOrPartitionException, TimeoutException)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","philipnee","2022-02-23T00:09:06Z","2022-03-09T18:39:29Z"
"","12381","KAFKA-13474: Allow reconfiguration of SSL certs for broker to controller connection","**Scenario** The scenario is explained in details in https://issues.apache.org/jira/browse/KAFKA-13474 but as a summary: When a certificate is rotated on a broker via dynamic configuration and the previous certificate expires, the broker to controller connection starts failing with `SSL Handshake failed`.  **Why** A similar fix was earlier performed in https://github.com/apache/kafka/pull/6721 but when `BrokerToControllerChannelManager` was introduced in v2.7, we didn't enable dynamic reconfiguration for it's channel.  **Summary of testing strategy (including rationale)** Add a test which fails prior to the fix done in the PR and succeeds afterwards. The bug wasn't caught earlier because there was no test coverage to validate the scenario.  Note: I would suggest that we backport this fix to all versions until 2.7","closed","","divijvaidya","2022-07-05T18:05:36Z","2022-07-12T06:33:30Z"
"","11814","MINOR: Ensure LocalLog.flush() is immune to recoveryPoint change by different thread","**Issue:** Imagine a scenario where two threads T1 and T2 are inside `UnifiedLog.flush()` **concurrently**:  * `KafkaScheduler` thread **T1** -> The periodic work calls `LogManager.flushDirtyLogs()` which in turn calls `UnifiedLog.flush()`. For example, this can happen due to `log.flush.scheduler.interval.ms` [here](https://github.com/apache/kafka/blob/8cca18d7b99d5905a84ccabb813d6a27bc8f44db/core/src/main/scala/kafka/log/LogManager.scala#L467-L471).  * `KafkaScheduler` thread **T2** -> A `UnifiedLog.flush()` call is triggered asynchronously during segment roll [here](https://github.com/apache/kafka/blob/8cca18d7b99d5905a84ccabb813d6a27bc8f44db/core/src/main/scala/kafka/log/UnifiedLog.scala#L1501).  Supposing if thread T1 advances the recovery point beyond the flush offset of thread T2, then this could trip the check within `LogSegments.values()` [here](https://github.com/apache/kafka/blob/8cca18d7b99d5905a84ccabb813d6a27bc8f44db/core/src/main/scala/kafka/log/LogSegments.scala#L136) for thread T2, when it is called from `LocalLog.flush()` [here](https://github.com/apache/kafka/blob/8cca18d7b99d5905a84ccabb813d6a27bc8f44db/core/src/main/scala/kafka/log/LocalLog.scala#L171). The exception causes the `KafkaScheduler` thread to die, which is not desirable.  **Fix:** We fix this by ensuring that `LocalLog.flush()` is immune to the case where the recoveryPoint advances beyond the flush offset.  **Tests:** I was able to test this manually by introducing barriers in the code to help simulate the race condition. As such, this is a hard case to write an automated unit test for, so I haven't added a new test case in this PR. So I'm mostly just relying on code review and also ensure there are no regressions in existing tests.","closed","","kowshik","2022-02-26T09:37:27Z","2022-03-01T18:55:18Z"
"","11962","KAFKA-13775: CVE-2020-36518 - Upgrade jackson-databind to 2.12.6.1","**CVE-2020-36518** vulnerability affects jackson-databind (see https://github.com/advisories/GHSA-57j2-w4cx-62h2).  Upgrading to `jackson-databind` version `2.12.6.1` addresses this CVE.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edwin092","2022-03-29T10:48:11Z","2022-04-01T01:40:49Z"
"","11539","MINOR: Update doc for 3.1","* Update main version of main documentation and add link to previous version; * Update quick start guide (links do not work yet obviously); * Add upgrade section.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-11-25T12:57:08Z","2021-12-03T08:55:38Z"
"","12250","KAFKA-13935 Fix static usages of IBP in KRaft mode","* Set the minimum supported MetadataVersion to 3.0-IV1 * Remove MetadataVersion.UNINITIALIZED * Relocate RPC version mapping for fetch protocols into MetadataVersion * Replace static IBP calls with dynamic calls to MetadataCache  A side effect of removing the UNINITIALIZED metadata version is that the FeatureControlManager and FeatureImage will initialize themselves with the minimum KRaft version (3.0-IV1).   The rationale for setting the minimum version to 3.0-IV1 is so that we can avoid any cases of KRaft mode running with an old log message format (KIP-724 was introduced in 3.0-IV1). As a side-effect of increasing this minimum version, the feature level values decreased by one.","closed","","mumrah","2022-06-03T22:24:11Z","2022-06-13T18:23:29Z"
"","12144","MINOR: reload4j build dependency fixes","* Replace `log4j` with `reload4j` in `copyDependantLibs`. Since we have   some projects that have an explicit `reload4j` dependency, it   was included in the final release release tar - i.e. it was effectively   a workaround for this bug. * Exclude `log4j` and `slf4j-log4j12` transitive dependencies for   `streams:upgrade-system-tests`. Versions 0100 and 0101   had a transitive dependency to `log4j` and `slf4j-log4j12` via   `zkclient` and `zookeeper`. This avoids classpath conflicts that lead   to [NoSuchFieldError](https://github.com/qos-ch/reload4j/issues/41) in   system tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-05-10T16:28:24Z","2022-05-11T03:31:13Z"
"","11763","MINOR: Remove unused params in ZkConfigManager","* Remove changeExpirationMs and time in ZkConfigManager, since this two params is not used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ruanwenjun","2022-02-15T11:57:28Z","2022-02-17T06:09:18Z"
"","12148","MINOR: Remove unnecessary log4j-appender dependency and tweak explicit log4j dependency","* Remove `log4j-appender` dependency from `tools`, `trogdor` and `shell` * Remove explicit `log4j` dependency from `trogdor` and `tools`. * Add `compileOnly` dependency from `tools` to `log4j` (same approach as   `core`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ijuma","2022-05-11T14:18:44Z","2022-06-15T17:01:06Z"
"","11789","MINOR: Print usage in StorageTool","* Print help in StorageTool when no args input.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ruanwenjun","2022-02-17T14:56:08Z","2022-02-18T10:56:58Z"
"","11757","MINOR: Clarify producer idempotence default in upgrade docs","* Mention `acks=1` to `acks=all` change in 3.0.0 upgrade docs * Have a separate section for 3.0.1 and 3.1.1 as some may skip the   3.0.0/3.1.0 section when upgrading to a bug fix. * Move the 3.0.0 note to the top since it's more impactful than the   other changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-02-14T15:20:00Z","2022-02-15T02:45:55Z"
"","11754","MINOR: Optimize collection method in Utils","* It's better to initialize collection size when create a collection like set, and the constructor like `HashSet(Collection","open","","ruanwenjun","2022-02-13T18:20:19Z","2022-02-21T02:10:02Z"
"","12133","improvement the compare logic","* Improvement the compare logic","closed","","lqjack","2022-05-07T06:04:39Z","2022-05-30T23:01:38Z"
"","11984","MINOR: Upgrade build and test dependencies","* gradle: 7.3.3 -> 7.4.2   Configuration cache improvements and several other improvements.   https://docs.gradle.org/7.4.2/release-notes.html * dependencycheck gradle plugin: 6.5.3 -> 7.0.3   Minor fixes. * spotbugs gradle plugin: 5.0.5 -> 5.0.6   Minor fixes.   https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/5.0.6 * jmh: 1.34 -> 1.35   Fixes and profiler improvements.   https://mail.openjdk.java.net/pipermail/jmh-dev/2022-March/003422.html * jqwik: 1.6.3 -> 1.6.5   Various tweaks and some breaking changes that don't seem to affect us.   https://github.com/jlink/jqwik/releases/tag/1.6.4   https://github.com/jlink/jqwik/releases/tag/1.6.5 * mockito: 4.3.1 -> 4.4.0   Add feature to verify static methods calls in order and minor fixes/improvements.   https://github.com/mockito/mockito/releases/tag/v4.4.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-04-01T21:42:41Z","2022-04-06T14:27:03Z"
"","11581","KAFKA-13522: add position tracking and bounding to IQv2","* Fill in the Position response in the IQv2 result. * Enforce PositionBound in IQv2 queries. * Update integration testing approach to leverage consistent queries.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-12-08T22:53:04Z","2021-12-11T07:01:37Z"
"","12405","KAFKA-13572 Fix negative preferred replica imbalanced count metric","* Currently, `preferredReplicaImbalanceCount` calculation has a race that becomes negative when topic deletion is initiated simultaneously.     * Please refer [KAFKA-13572's comment](https://issues.apache.org/jira/browse/KAFKA-13572?focusedCommentId=17566872&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17566872) for the details * This PR addresses the problem by fixing `cleanPreferredReplicaImbalanceMetric` to be called only once per topic-deletion procedure  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ocadaruma","2022-07-14T15:20:21Z","2022-07-18T07:48:15Z"
"","12212","Kafka 13888 new fields","* create file if not exist update lastCaughtUpTimestamp update lastFetchTimestamp *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lqjack","2022-05-25T16:29:58Z","2022-06-15T17:56:59Z"
"","12408","KAFKA-14076: Fix issues with KafkaStreams.CloseOptions","* Addresses issues with `KafkaStreams.close(CloseOptions)`. * Adds an integration test for this new functionality.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","jnh5y","2022-07-14T20:58:27Z","2022-07-21T14:41:40Z"
"","11817","KAFKA-13438: Replace EasyMock and PowerMock with Mockito in WorkerTest","(Sub task of KAFKA-7438)  Replace all usages of Powermock and Easymock in connect.runtime.WorkerTest with Mockito. One bit of Powermock functionality could not be replicated with Mockito - which was verifying constructor arguments passed to a new instance of WorkerSourceTask within the worker, but this was not critical to test functionality.   The test has been removed from the list of tests that had to be excluded when Gradle is running under Java 16+ due to Powermock, and successful execution was verified with Java 17.0.1 Temurin.","closed","","LiamClarkeNZ","2022-02-27T02:21:40Z","2022-03-18T01:10:59Z"
"","11553","KAFKA-8396: Clean up Transformer API","(Requires KIP)  Introduces a internal `ValueProcessor` and `ValueProcessorContext` to validate keys do not change between processing and forwarding values to child processors.  This validation only applies when processing, and not when adding punctuators to the topology.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jeqo","2021-11-30T11:57:37Z","2022-01-21T00:13:58Z"
"","11523","KAFKA-10627: Added support for Connect TimestampConverter to convert multiple fields using a comma-separated list, and multiple input formats when parsing from a string (fixed after rebase).","> Note this PR is copied from #9492 where I mistakenly did a rebase on a very old change which added thousands of commits, so this is a second attempt to get a very clean PR for this change instead.   I have made an update to **TimestampConverter** Connect transform to address the main issues that I logged in [KAFKA-10627](https://issues.apache.org/jira/browse/KAFKA-10627).  Namely, that it now ...  - supports multiple fields via a new configuration parameter `fields` as a comma-separated list of field names. The old parameter `field` is still supported for compatibility but the value is moved to the new parameter. - supports a DateTimeFormatter-compatible pattern string that can support multiple timestamp formats for parsing input of string values to whatever target you configure (e.g. parsing strings to Timestamp type).    - `format` config is now split into two: `format.input` and `format.output` but you can still just send `format` by itself if you do not need to use a more complicated input pattern. When providing only `format`, the string pattern which you provide will be used for both `format.input` and `format.output`.  I realized that kafka is using `java.util.Date` everywhere and as part of its core types (including in Schemas, values, etc).  In theory it would be good over time to upgrade to `java.time` classes but on first reflection it seems like quite a big overhaul to do this.  So instead I focused on the specific problem at hand: parsing strings into `Date` where the strings can come in different formats.  So for this part alone I changed to use `DateTimeFormatter` so we can use multiple patterns to match input strings and convert them to a `java.util.Date` after.  I also updated some of the way the Config parameters and values work, to bring in line with the other classes and similar to what I did with #9470.  #### String Input and Output Timestamp Format updates  Because now for input formats we allow multiple different possibilities using pattern matching, this does not work for the output format of a Timestamp to a String (which was another possibility of this transform).  So I have changed the configuration a bit... now there are three parameters:  - `format` which is the original one. You can still use this one, and it will set both input (parsing) and output (Date/Timestamp to string format) based on this format. - `format.input` is a new parameter, where you can specify a DateTimeFormatter-compatible pattern string that supports multiple different formats in case you have a mix in your data.  For just one example, now you can use something like this as `format.input` and it will catch a lot of different variations which you might see in one timestamp field: `""[yyyy-MM-dd[['T'][ ]HH:mm:ss[.SSSSSSSz][.SSS[XXX][X]]]]""` - `format.output` is a new parameter which only controls the output of a Date/Timestamp to target type of `string`. This is the same as before and still uses `SimpleDateFormat` to create the output string, it is just controlled in a separate parameter now.  I also added some code which checks the value of each of these three.  Basically it forces you to use either `format`, or one or both of the new parameters -- you cannot mix the old and new together.  In the end, `format.input` and `format.output` are the ones used in the rest of the logic, but the code first compares `format` against these values and sets the value for both of the new parameters depending on what was sent in the config.  #### Support for multiple fields instead of one single field  I changed the `field` parameter to now be called `fields` and supports multiple values as a comma-separated list.  I used this new `ConfigUtils.translateDeprecatedConfigs` method to provide automatic translation of of the old parameter to the new one as well.  With this change I also updated the `apply` methods so that they loop through each field and check against the list of `fields`.  Now you can specify a comma-separated list of multiple fields to have the same input format/output type applied.  Unit tests have been added for both new updates (string formatting and multiple field support).  As I looked at this one then I realized that maybe it would be good to add `recursive` support similar to what I have done in #9470 but I guess that can come at another day!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","joshuagrisham","2021-11-22T16:58:13Z","2021-12-06T10:11:09Z"
"","11963","KAFKA-13777: Fix FetchResponse#responseData: Assignment of lazy-initialized members should be the last step with double-checked locking","> Double-checked locking can be used for lazy initialization of volatile fields, but only if field assignment is the last step in the synchronized block. Otherwise you run the risk of threads accessing a half-initialized object.  https://rules.sonarsource.com/java/RSPEC-3064","closed","","yun-yun","2022-03-29T11:07:47Z","2022-04-01T01:43:24Z"
"","12482","Merge up to Apache Kafka 3.3 branching point","> $ git merge-base apache-github/3.3 apache-github/trunk > 23c92ce79366e86ca719e5e51c550c27324acd83  > $ git show 23c92ce79366e86ca719e5e51c550c27324acd83 > commit 23c92ce79366e86ca719e5e51c550c27324acd83 > Author: SC  > Date:   Mon Jul 11 11:36:56 2022 +0900 > >    MINOR: Use String#format for niceMemoryUnits result (#12389) > >    Reviewers: Luke Chen , Divij Vaidya   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-08-03T20:58:52Z","2022-08-03T20:59:20Z"
"","11714","KAFKA-13614: Don't apply leader replication quota to consumer fetches","### What In the fetch path, we check `shouldLeaderThrottle` regardless of whether the read is coming from a consumer or follower broker. This results in replication quota being applied to consumer fetches.  ### Testing Added unit tests for when fetch requests can be immediately satisfied and for purgatory fetches.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2022-01-26T01:22:20Z","2022-01-27T07:21:57Z"
"","11842","KAFKA-13687: Limiting the amount of bytes to be read in a segment logs","### Summary This PR allows to limit the output batches while they are inspected via the `kafka-dump-log.sh`  script.  The idea is to take samples from the logsegments without affecting a production cluster  as the current script will read the whole files, this could create issues related to performance.  Please see the [KIP-824](https://cwiki.apache.org/confluence/display/KAFKA/KIP-824%3A+Allowing+dumping+segmentlogs+limiting+the+batches+in+the+output)   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","sciclon2","2022-03-03T20:41:47Z","2022-04-06T15:48:43Z"
"","11836","[KAFKA-13687]  Allowing dumping logs for a small segment","### Summary This PR allows to limit the output batches while they are inspected via the `kafka-dump-log.sh`  script.  The idea is to take samples from the logsegments without affecting a production cluster  as the current script will read the whole files, this could create issues in term of page.  More information can be found in the ticket [here](https://issues.apache.org/jira/browse/KAFKA-13687)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sciclon2","2022-03-03T06:45:28Z","2022-03-03T18:07:16Z"
"","12461","Minor: enable index for emit final sliding window","### Summary Enable index for sliding window emit final case as it's faster to fetch windows for particular key  ### Testing Existing test cases  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lihaosky","2022-07-29T20:23:26Z","2022-07-29T21:47:57Z"
"","12183","KAFKA-13883: Implement NoOpRecord and metadata metrics","### Public Changes 1. A new configuration flag metadata.max.idle.interval.ms 2. A new metrics group ""broker-metadata-metrics"" to the brokers 3. Four new metrics to the KafkaController metric group 4. Increase the IBP and metadata.version 5. A new metadata  record called NoOpRecord which is only written if we have a matching metadata version  ### Implementation Changes The KRaft controller schedules an event to write NoOpRecord to the metadata log if the metadata version supports this feature. This event is scheduled at the interval defined in metadata.max.idle.interval.ms. Brokers and controllers were improved to ignore the NoOpRecord when replaying the metadata log.  The replica control manager was also changed to accept a metadata version supplier to determine if leader recovery state is supported.  ### Other Changes Fixed a bug where metadata version 3.3-IV1 was not marked as changing the metadata.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2022-05-19T17:27:10Z","2022-06-01T17:48:49Z"
"","12160","KAFKA-13889: Fix AclsDelta to handle ACCESS_CONTROL_ENTRY_RECORD quickly followed by REMOVE_ACCESS_CONTROL_ENTRY_RECORD for same ACL","### JIRA https://issues.apache.org/jira/browse/KAFKA-13889  ### Description - Fixes `AclsDelta` to handle `ACCESS_CONTROL_ENTRY_RECORD` quickly followed by `REMOVE_ACCESS_CONTROL_ENTRY_RECORD` for same ACL - As explained in the JIRA, in https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/image/AclsDelta.java#L64 we store the pending deletion in the `changes` Map. This could override a creation that might have just happened. This is an issue because in `BrokerMetadataPublisher` this results in us making a `removeAcl` call which finally results in https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/metadata/authorizer/StandardAuthorizerData.java#L203 being executed and this code throws an exception if the ACL isnt in the Map yet. If the `ACCESS_CONTROL_ENTRY_RECORD` event never got processed by `BrokerMetadataPublisher` then the ACL wont be in the Map yet. - So the fix here is to remove the entry from the `changes` Map if the ACL doesnt exist in the image yet.   ### Testing - Added unit tests for new behavior   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andymg3","2022-05-13T17:18:51Z","2022-05-19T23:24:34Z"
"","12305","MINOR: Add __cluster_metadata topic to list of internal topics","### Details - Adds the new KRaft `__cluster_metadata` topic to `INTERNAL_TOPICS` which tracks internal topics  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","andymg3","2022-06-17T18:22:00Z","2022-07-29T23:05:02Z"
"","11917","KAFKA-13785: [8/N][emit final] time-ordered session store","### Description Time ordered session store implementation. I introduced `AbstractRocksDBTimeOrderedSegmentedBytesStore ` to make it generic for `RocksDBTimeOrderedSessionSegmentedBytesStore` and `RocksDBTimeOrderedSegmentedBytesStore`  ### Testing Unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-03-18T18:48:11Z","2022-05-05T23:12:25Z"
"","11896","KAFKA-13785: [6/N][Emit final] emit final for TimeWindowedKStreamImpl","### Description Initial implementation to emit final for TimeWindowedKStreamImpl. This PR is on top of https://github.com/apache/kafka/pull/12030  ### Testing Unit test and integration test","closed","kip,","lihaosky","2022-03-15T00:13:59Z","2022-05-03T21:18:14Z"
"","12428","fix flaky test test_standby_tasks_rebalance","### Description In this test, when third proc join, sometimes there are other rebalance scenarios such as followup joingroup request happens before syncgroup response was received by one of the proc and the previously assigned tasks for that proc is then lost during new joingroup request. This can result in standby tasks assigned as `3, 1, 2`. This PR relax the expected assignment of `2, 2, 2` to a range of `[1-3]`.  ### Testing Ran existing test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  ### Some backgroud from Guozhang: I talked to Hao Li offline and also inspected the code a bit, and tl;dr is that I think the code logic is correct (i.e. we do not really have a bug), but we need to relax the test verification a little bit. The general idea behind the subscription info is that:  * When a client joins the group, its subscription will try to encode all its current assigned active and standby tasks, which would be used as prev active and standby tasks by the assignor in order to achieve some stickiness.  * When a client drops all its active/standby tasks due to errors, it does not actually report all empty from its subscription, instead it tries to check its local state directory (you can see that from `TaskManager#getTaskOffsetSums` which populates the `taskOffsetSum`. For active task, its offset would be “-2” a.k.a. `LATEST_OFFSET`, for standby task, its offset is an actual numerical number.     So in this case, the proc2 which drops all its active and standby tasks, would still report all tasks that have some local state still, and since it was previously owning all six tasks (three as active, and three as standby), it would report all six as standbys, and when that happens the resulted assignment as @Hao Li verified, is indeed the un-even one.     So I think the actual “issue“ happens here, is when proc2 is a bit late sending the sync-group request, when the previous rebalance has already completed, and a follow-up rebalance has already triggered, in that case, the resulted un-even assignment is indeed expected. Such a scenario, though not common, is still legitimate since in practice all kinds of timing skewness across instances can happen. So I think we should just relax our verification here, i.e. just making sure that each instance has at least one standby replica at the end, not exactly evenly as “2, 2, 2”.","closed","","lihaosky","2022-07-21T17:08:51Z","2022-07-29T20:58:44Z"
"","12037","KAFKA-13785: [7/N][Emit final] emit final for sliding window","### Description Implementation to emit final for sliding window agg. This PR is on top of https://github.com/apache/kafka/pull/11896  ### Testing pending unit test and integration test","closed","","lihaosky","2022-04-12T22:49:13Z","2022-05-14T02:29:21Z"
"","11892","KAFKA-13785: [Emit final][4/N] add time ordered store factory","### Description Add factory to create time ordered store supplier  ### Testing Added new unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-03-14T21:17:13Z","2022-03-31T16:20:48Z"
"","12030","KAFKA-13785: [5/N][emit final] cache for time ordered window store","### Description A new cache for `RocksDBTimeOrderedWindowStore`. Need this because `RocksDBTimeOrderedWindowStore`'s key ordering is different from `CachingWindowStore` which has issues for `MergedSortedCacheWindowStoreIterator`  ### Summary of testing strategy (including rationale) Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lihaosky","2022-04-11T16:51:36Z","2022-04-20T18:09:13Z"
"","11879","KAFKA-13728: fix PushHttpMetricsReporter no longer pushes metrics when network failure is recovered.","### Description  The class `PushHttpMetricsReporter` no longer pushes metrics when network failure is recovered.   I debugged the code and found the problem here : https://github.com/apache/kafka/blob/dc36dedd28ff384218b669de13993646483db966/tools/src/main/java/org/apache/kafka/tools/PushHttpMetricsReporter.java#L214-L221     When we submit a task to the `ScheduledThreadPoolExecutor` that needs to be executed periodically, if the task throws an exception and is not swallowed, the task will no longer be scheduled to execute.  ### Conclusion So when an IO exception occasionally occurs on the network, we should swallow it rather than throw it in task `HttpReporter`.","closed","","XiaoyiPeng","2022-03-11T02:06:08Z","2022-03-21T00:57:49Z"
"","11878","fix flaky EosIntegrationTest.shouldCommitCorrectOffsetIfInputTopicIsTransactional[at_least_once]","### Description  In this test, we started Kafka Streams app and then write to input topic in transaction. It's possible when streams commit offset, transaction hasn't finished yet. So the streams committed offset could be less than the eventual `endOffset`.  This PR moves the logic of writing to input topic before starting streams app.  ### Summary of testing strategy (including rationale) Run test 30 times and didn't see failure. Without this change, it always failed when running 20 times.  ``` for (( c=1; c>  test_output 2>&1; done ```  Output: https://gist.github.com/lihaosky/ab0661a5e453d2e0970dece6a29641f2  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","lihaosky","2022-03-10T23:42:21Z","2022-03-31T05:09:10Z"
"","12009","MINOR: Fix method annotation","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-04-07T09:07:47Z","2022-04-07T09:30:49Z"
"","12480","MINOR: Small cleanups in integration.kafka tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mimaison","2022-08-03T16:30:46Z","2022-08-03T16:30:46Z"
"","12466","[WIP] KAFKA-10199: Handle task closure and recycling from state updater","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2022-08-01T17:02:40Z","2022-08-01T17:02:40Z"
"","12436","MINOR: Use builder for mock task in DefaultStateUpdaterTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2022-07-25T13:03:12Z","2022-07-26T08:12:21Z"
"","12435","MINOR: Fix method javadoc and typo in comments","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","bozhao12","2022-07-24T05:19:52Z","2022-07-24T05:19:52Z"
"","12417","KAFKA-13917: Avoid calling lookupCoordinator() in tight loop (#12180)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2022-07-18T11:26:29Z","2022-07-22T01:52:47Z"
"","12346","KAFKA-6945: Add docs about KIP-373","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2022-06-25T07:19:09Z","2022-07-05T12:57:17Z"
"","12327","WIP - DO NOT MERGE - KIP-714 #718","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2022-06-21T21:31:22Z","2022-07-19T22:19:32Z"
"","12253","MINOR: A fewer method javadoc and typo fix","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-06-06T00:07:05Z","2022-06-06T19:25:06Z"
"","12239","MINOR: Small fixes in docs/upgrade.html","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-06-02T09:28:19Z","2022-06-02T10:05:33Z"
"","12227","MINOR: inline metrics in RecordAccumulator","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-05-30T09:04:57Z","2022-06-01T07:48:34Z"
"","12218","MINOR:Fix typo in ClusterTestExtensionsTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-05-26T02:01:43Z","2022-05-27T20:38:54Z"
"","12180","KAFKA-13917: Avoid calling lookupCoordinator() in tight loop","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2022-05-19T13:40:38Z","2022-06-11T02:42:40Z"
"","12178","MINOR:Fix typo in ReplicaManagerTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bozhao12","2022-05-18T23:19:40Z","2022-05-19T02:28:47Z"
"","12146","MINOR: Remove kraft authorizer from list of missing features","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2022-05-10T21:53:30Z","2022-05-11T16:46:41Z"
"","12143","MINOR: Update release versions for upgrade tests with ongoing release","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-05-10T10:26:17Z","2022-05-10T12:47:47Z"
"","12058","MINOR: Scala cleanups in core","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-04-15T14:11:18Z","2022-04-20T13:10:50Z"
"","12057","MINOR: Make link in quickstart dynamic","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-04-15T14:11:18Z","2022-04-15T14:22:17Z"
"","12056","MINOR:Fix test in ReplicaManagerTest.scala","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","bozhao12","2022-04-15T08:55:09Z","2022-05-09T12:40:08Z"
"","12055","[MINOR] Update upgrade documentation for 3.2","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2022-04-15T08:19:14Z","2022-04-15T08:21:06Z"
"","11972","MINOR: Fix doc variable typos in `TopicConfig`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fxbing","2022-03-30T12:24:53Z","2022-03-31T01:54:53Z"
"","11906","MINOR: Doc updates for Kafka 3.0.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-03-16T10:57:09Z","2022-04-04T15:47:21Z"
"","11833","KAFKA-13671: Add ppc64le build stage","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-03-02T13:48:36Z","2022-03-08T17:19:31Z"
"","11756","MINOR: Small cleanups in connect:runtime","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-02-14T14:35:35Z","2022-02-16T19:02:27Z"
"","11749","MINOR: Small cleanups in mirror/mirror-client","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2022-02-11T22:16:08Z","2022-02-14T09:17:57Z"
"","11701","MINOR: Add 3.0 and 3.1 to broker and client compatibility tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-21T20:46:01Z","2022-01-25T15:22:52Z"
"","11670","MINOR: Update year in NOTICE","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2022-01-12T08:31:52Z","2022-01-12T08:46:54Z"
"","11622","MINOR: Update LICENSE for 3.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-12-21T21:12:13Z","2021-12-22T08:22:35Z"
"","11614","POC: refactor IQv2 to push serdes down instead of transforming in Metered","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2021-12-19T01:06:40Z","2021-12-20T18:22:10Z"
"","11602","MINOR: Reset java.security.auth.login.config in ZK-tests to avoid config reload affecting subsequent tests","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-12-14T11:34:28Z","2021-12-15T14:30:39Z"
"","11499","KAFKA-10199: Add interface for state updater","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-11-15T16:03:18Z","2022-02-23T18:13:28Z"
"","11497","MINOR: Bump 2.7 to use 2.7.2","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-11-15T09:35:35Z","2021-11-22T11:05:43Z"
"","11495","MINOR: Bump 2.6 to use 2.6.3","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-11-15T08:54:25Z","2021-11-22T10:34:13Z"
"","12004","KAFKA-10095: Add stricter assertion in LogCleanerManagerTest","### Changes:  Add a stricter assertion in LogCleanerManagerTest Minor cosmetic simplification in Scala  ### Testing: LogCleanerManagerTest run is successful. All unit test runs are successful using ./gradlew unitTest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","divijvaidya","2022-04-06T14:07:22Z","2022-04-20T12:29:18Z"
"","12045","KAFKA-12319: Change calculation of window size used to calculate `Rate`","## Why does the test fail? `ConnectionQuotasTest.testListenerConnectionRateLimitWhenActualRateAboveLimit()` sends 600 connection creation requests at a rate of 40/s with a listenerQuotaConnectionRateLimit set to 30/s. The test asserts that even though the rate of requests is higher than threshold, due to correct throttling, the measured rate at the completion of 600 requests is 30 +- epsilon. The value of epsilon is set to 7 which is exceeded from time to time leading to flaky test failures.  ## The problem Currently, calculate of rate function (used for rate limiting) holds the following assumptions: > 1. Start time of a new sample window is time at which first event in that window is recorded. > 2. If we don't have `quota.window.num` windows retained, assume that we have prior windows with zero records while calculating the rate.  These assumptions lead to wrong calculation of rate in certain scenario as described below.   Consider a scenario when we have some initial requests, followed by a small gap without any requests and then another bunch of requests. More specifically:  Configuration = [quota.window.size.seconds](https://kafka.apache.org/documentation/#brokerconfigs_quota.window.size.seconds)= 1s [quota.window.num](https://kafka.apache.org/documentation/#brokerconfigs_quota.window.num) = 2  listenerName.[max.connection.creation.rate](https://kafka.apache.org/documentation/#brokerconfigs_max.connection.creation.rate) = 30/s`  Record events (E) at timestamps: E1 | CurrentTimeStamp (T1) | Window#1 (start time = T1) E2 | T2 = T1 + 30ms | Window#1 E3 | T3 = T1 + 995ms | Window#1 < No events from T3 to T4 >  E4 | T4 = T1 + 1020ms | Window#2 (start time = T1 + 1020ms) E5 | T5 = T1 + 2010ms | Window#2  Rate calculated as per current implementation: Rate at T1 = 1 / (length of hypothetical prior samples + time elapsed for current sample) = 1 / (1 + 0) = 1 events per second Rate at T2 = 2 / (1 + 0.030) = 1.94 events per second Rate at T3 = 3/ (1 + 0.995) = 1.5 events per second Rate at T4 = 4/ (now - start time of oldest window) = 4 / 1.02 = 3.92 events per second  When calculating rate for T5, first ""obsolete windows"" are purged, i.e. any window with start time < T5 - (quota.window.size.seconds * quota.window.num), thus, Window#1 is purged (because T1 < T5-2s)  Rate at T5 = 2/ (length of hypothetical prior samples + time elapsed for current sample) = 2 / 1.99 = 1.005 events per second  **Note how the rate calculation for T5 has fallen back to using the assumption that there exists prior windows with zero events (due to purge) whereas we do actually have a previous window with > 0 events in it. Hence, rate calculated at T5 is incorrect.** In worst case scenarios Window#1 could have large number of events in it but calculation of rate towards end of Window#2 would ignore all those earlier events leading to an incorrect low value of current rate. For throttling use cases, this would lead to allowing more events (since current observed rate is low) and thus, violating the contract to maintain a sustained `max.connection.creation.rate`  The flaky test `ConnectionQuotasTest.testListenerConnectionRateLimitWhenActualRateAboveLimit` suffers from the problem described here from time to time leading to higher rate of connection creation than expected.  ## The solution The solution is to remove assumption 1 stated earlier. Instead replace assumption 1 with:  > Start time of a new sample window is the nearest time at which the window should have started assuming no gaps.  The nearest time is calculated as   ``` currentWindowStartTimeMs = recordingTimeMs - ((recordingTimeMs - previousWindowEndtime) % config.timeWindowMs())  where  recordingTimeMs is time of first record in a window previousWindowEndtime is end time for previous window calculated as previousWindowStartTime + quota.window.size.seconds config.timeWindowMs is quota.window.size.seconds ```  With the solution, T5 moves to 3rd window (window rollover occurs at T1 + 2000ms) and the rate at T5 becomes:  Rate at T5 = 2/ (now - start time of oldest window) = 2 / 1.010 = 1.98 events per second  This scenario has also been added as a unit test in `MetricsTest.java`  ## Code changes 1. Changes in `SampledStat#record()` to make the change in assumption 1 as described above. The change is made when rollover into a window occurs. 2. Add new tests in `MetricsTest.java`  3. Cosmetic syntax changes across files.  ## Testing - [x] New test added to validate the change in assumption. - [x] `./gradlew unitTest` passes. - [x] `./gradlew integrationTest` passes.  ## Longer term solutions 1. Longer term I think we should move to a sliding window based approach to calculate rate instead of the fixed window approach applied today.  2. The current rate limiting approach also allows short burst of traffic. There should be a configurable option for the users to choose between the approach which allows short bursts vs. an approach where system tried to maintain a smooth rate over time such that at no time does it go beyond the allocated threshold.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","divijvaidya","2022-04-13T15:02:16Z","2022-06-15T11:01:32Z"
"","12184","KAFKA-13911: Fix the rate window size calculation for edge cases","## Problem Implementation of connection creation rate quotas in Kafka is dependent on two configurations: [quota.window.num](https://kafka.apache.org/documentation.html#brokerconfigs_quota.window.num) AND [quota.window.size.seconds](https://kafka.apache.org/documentation.html#brokerconfigs_quota.window.size.seconds)  The minimum possible values of these configuration is 1 as per the documentation. However, when we set 1 as the configuration value, we can hit a situation where rate is calculated as NaN (and hence, leads to exceptions). This specific scenario occurs when an event is recorded at the start of a sample window.  ## Solution This patch fixes this edge case by ensuring that the windowSize over which Rate is calculated is at least 1ms (even if it is calculated at the start of the sample window).  ## Test Added a unit test which fails before the patch and passes after the patch  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","divijvaidya","2022-05-19T17:33:26Z","2022-07-13T15:30:41Z"
"","12189","Fix flaky test TopicCommandIntegrationTest.testDescribeAtMinIsrPartitions(String).quorum=kraft","## Problem Flaky test as failed in CI https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-12184/1/tests/   The test fails because it does not wait for metadata to be propagated across brokers before killing a broker which may lead to it getting stale information. Note that a similar test was done in #12104 for a different test.  ## Change Wait for metadata propagation to complete after killing the broker.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","divijvaidya","2022-05-20T10:33:12Z","2022-05-21T18:42:55Z"
"","12145","KAFKA-13892: Dedupe RemoveAccessControlEntryRecord in deleteAcls response of AclControlManager","## JIRA https://issues.apache.org/jira/browse/KAFKA-13892  ### Details In https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java#L143 we loop through the ACL filters and and add `RemoveAccessControlEntryRecord` records to the response list for each matching ACL. There's a bug where if two (or more) filters match the same ACL, we create two (or more) `RemoveAccessControlEntryRecord` records for the same ACL. This is an issue because upon replay we throw an exception (https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java#L195) if the ACL is not in the in-memory data structures which will happen to the second `RemoveAccessControlEntryRecord`.  I don't think we need to dedupe `List`. It contains a list of results, where each result contains the ACLs that matched the filter. It should be OK for the same ACL to be in multiple AclDeleteResult results because it really could match multiple filters.   ### Testing - Added a unit test that tests the new behavior - Ran pre-existing `AclControlManager` tests: ```  kafka % ./gradlew metadata:test --tests org.apache.kafka.controller.AclControlManagerTest --rerun-tasks  > Configure project : Starting build with version 3.3.0-SNAPSHOT (commit id 796b648d) using Gradle 7.4.2, Java 1.8 and Scala 2.13.6 Build properties: maxParallelForks=12, maxScalacThreads=8, maxTestRetries=0  > Task :raft:processMessages MessageGenerator: processed 1 Kafka message JSON files(s).  > Task :metadata:processMessages MessageGenerator: processed 19 Kafka message JSON files(s).  > Task :clients:processMessages MessageGenerator: processed 144 Kafka message JSON files(s).  > Task :clients:processTestMessages MessageGenerator: processed 2 Kafka message JSON files(s).  > Task :metadata:spotbugsMain [main] INFO edu.umd.cs.findbugs.ExitCodes - Calculating exit code...  > Task :metadata:test  AclControlManagerTest > testValidateFilter() PASSED  AclControlManagerTest > testValidateNewAcl() PASSED  AclControlManagerTest > testLoadSnapshot() PASSED  AclControlManagerTest > testDeleteDedupe() PASSED  AclControlManagerTest > testCreateAclDeleteAcl() PASSED  AclControlManagerTest > testAddAndDelete() PASSED  BUILD SUCCESSFUL in 34s 29 actionable tasks: 29 executed ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andymg3","2022-05-10T19:14:29Z","2022-05-10T22:26:58Z"
"","12087","KAFKA-13851: Add integration tests for DeleteRecords API","## Changes Added integration tests to test the DeleteRecords API for the following scenarios: 1. Happy case when the records are deleted 2. Error case when an invalid topic is provided 3. Error case when an invalid offset is provided  ## Testing `./gradlew unitTest` is successful locally   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","divijvaidya","2022-04-22T17:05:58Z","2022-05-16T07:50:50Z"
"","12459","KAFKA-13036: Replace EasyMock and PowerMock with Mockito for RocksDBMetricsRecorderTest","## Changes 1. Migrate to Mockito 2. Add more assertive checks using `verify` 3. Minor indentation fixes  ## Testing `./gradlew streams:unitTest --tests RocksDBMetricsRecorderTest` is successful  ## Notes to help the reviewers 1. Mockito uses `MockitoJUnitRunner.StrictStubs.class` to assert that all stubs are verified. 2. Mockito does not have the concept of ""register-activate"" represented by ""when - replay"" in EasyMock. Hence, you only need to use ""when"" and that is all. No ""replay"" is required. 3. `PowerMock.reset` is replaced by `Mockito.reset` 4. Mockito does not have to distinguish between ""niceMock"" and a normal ""mock"". All mocks are ""nicemocks"". 5. Mockito can mock static methods using `mockStatic` 6. When `verify(mockObj).functionName()` is called, it is equivalent to `verify(mockObj, times(1)).functionName()` i.e. it validates that the function in invoked on mocked object exactly once.  ## Code coverage ### Before ![Screenshot 2022-08-02 at 18 14 26](https://user-images.githubusercontent.com/71267/182423056-b6bfdc1a-4587-434f-ba1d-9bd55e045a33.png)  ### After ![Screenshot 2022-08-02 at 18 14 41](https://user-images.githubusercontent.com/71267/182423081-43091937-d77a-45e3-8119-d66aea415dad.png)","open","","divijvaidya","2022-07-29T17:03:58Z","2022-08-03T18:32:08Z"
"","12460","MINOR: Upgrade mockito test dependencies","## Changes - **mockito: 4.4.0 -> 4.6.1** (https://github.com/mockito/mockito/releases) Most important updates:   - Regression? Strictness set in @MockitoSettings ignored after upgrade from 4.5.1 to 4.6.0 https://github.com/mockito/mockito/issues/2656   - Fixes https://github.com/mockito/mockito/issues/2648 : Add support for customising strictness via @mock annotation and MockSettings https://github.com/mockito/mockito/pull/2650   ## Why is this change needed? ### Update 1 According to the [Mockito documentation](https://javadoc.io/doc/org.mockito/mockito-core/latest/org/mockito/Mockito.html#when(T)) : > Although it is possible to verify a stubbed invocation, usually it's just redundant. Let's say you've stubbed foo.bar(). If your code cares what foo.bar() returns then something else breaks(often before even verify() gets executed). If your code doesn't care what get(0) returns then it should not be stubbed.   While working on the [Replace EasyMock and PowerMock with Mockito for StreamsMetricsImplTest ](https://issues.apache.org/jira/browse/KAFKA-12947) I noticed that described behavior wasn't applied when you create a new `mock` like this.  ```java         final Metrics metrics = mock(Metrics.class);         when(metrics.metric(metricName)).thenReturn(null);          ... invoke SUT          verify(metrics).metric(metricName); // this should be redundant (according to docs)  ```  After further investigation I figured out that described behaviour wasn't implemented until`v4.6.1`.  With this change we are now able to mock objects like this:  ```java    Foo explicitStrictMock = mock(Foo.class, withSettings().strictness(Strictness.STRICT_STUBS)); ``` - link to docs: [MockSettings.html#strictness](https://javadoc.io/static/org.mockito/mockito-core/4.6.1/org/mockito/quality/Strictness.html#STRICT_STUBS)  ### Update 2 It looks like I can accomplish the same thing by using the `@RunWith(MockitoJUnitRunner.StrictStubs.class) ` instead of the `@RunWith(MockitoJUnitRunner.class)` so mockito dependency version update is not mandatory, but it would be nice to stay up-to-date and use the latest version (it's up to MR reviewer to decide if we are going to merge this now, or just close the MR and update mockito version later).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dplavcic","2022-07-29T18:07:19Z","2022-07-31T04:57:16Z"
"","12034","(draft) Kafka-12319: Improve rate calculation for first sample window","## Background Apache Kafka's rate-limiting algorithm uses a variation of fixed window algorithm where the duration of time window is controlled by two configurations: [quota.window.num](https://kafka.apache.org/documentation/#brokerconfigs_quota.window.num) and [quota.window.size.seconds](https://kafka.apache.org/documentation/#brokerconfigs_quota.window.size.seconds). When a client connection is received, rate of connections at that timestamp is calculated by observing the number of requests in the duration of time window defined using the configurable parameters. If the rate is above the acceptable threshold, the connection is made to wait for a duration calculated to ensure that when a connection is accepted, the overall rate should be less than (or equal to) acceptable threshold.  This algorithm has certain edge cases to handle rate calculation, such as the scenarios where 1\ extended period of time when no connections are received or when 2\ the rate calculation is occurring within the first ever window with no prior samples. Currently, both these cases are handled in the same manner. When no prior windows are available, an assumption is made that a prior window exists with 0 data points. This is an effective way to solve scenario 1 but it leads to problems for scenario 2. Let us demonstrate using examples.  Consider configuration as `config.timeWindowMs() = 1s`, `config.samples() = 2`, max connection create rate = 30/s. A workload with a uniform rate of 40/s (1  connection per 25 ms) is started.  Record events (E) at timestamps: E1 = CurrentTimeStamp (T1) E2 = T1 + 25ms E3 = T1 + 50ms ... ... E30 = T1 + 725ms  The rate calculated as per the current algorithm (assuming a pre-existing window with 0 events) would be: - Rate T1 + 20ms = 1/ (prior window size + current window elapsed time) = 1 / (1 + 0.02) = 0.98 events per second - Rate calculated at T1 + 90ms = 3/1.09s = 2.75 events per second - Rate calculated at T1 + 730ms = 30/1.73s = 17.34 events per second  Note that even after 30 events in that window, the rate is still set to ~17 events per second, which would prevent the throttling to kick-in. This would lead to a situation where the first window allows more requests than acceptable limit to pass through and thus, to compensate, the second window serves less traffic, the third window again increases the traffic and so on....  This leads to an uneven distribution traffic across windows.  Note that this un-even distribution is the primary cause of flakiness of `ConnectionQuotasTest.testListenerConnectionRateLimitWhenActualRateAboveLimit()`  ## Solution  As a solution, this PR distinguishes between the scenario 1 and 2 above. During the `Rate` calculation, windowSize method has been modified to treat the case of first window separately from the case when no traffic has been received in prior windows. For the first window, the windowSize is calculated as the overall length of first window. Using the new change, the above calculation changes to:  The rate calculated as per the new algorithm (assuming a pre-existing window with 0 events) would be: - Rate T1 + 20ms = 1/ (current window size) = 1 / 1 = 1 events per second - Rate calculated at T1 + 90ms = 3/1s = 3 events per second - Rate calculated at T1 + 730ms = 30/1s = 30 events per second  Note how the throttling kicks-in as expected after 30 events and thus leading to a smoother distribution of traffic across windows. ## Code Changes 1. Modifications in `Rate.java#windowSize()` method to distinguish between scenario 1 and 2. 2. Add a method `isCurrentSampleInFirstWindow` in  3. Fix the `ConnectionQuotasTest.testListenerConnectionRateLimitWhenActualRateAboveLimit()` test to run for 20s as expected (earlier, there was a difference between what ) 4. Add new tests for scenario 1 and scenario 2 in `MetricsTest.java` 5. Modified existing tests for throttling / quota management to remove the assumptions of calculation based on prior empty windows ## Testing All unit test pass using `./gradlew unitTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","divijvaidya","2022-04-12T13:25:00Z","2022-04-13T15:02:46Z"
"","12229","MINOR: Include the inner exception stack trace when re-throwing an exception","# Problem:  While wrapping the caught exception into a custom one, information about the caught exception is being lost, including information about the stack trace of the exception.  # Change When re-throwing an exception, we make sure to include the stack trace. Otherwise pertinent debug information is lost.","open","","divijvaidya","2022-05-31T10:12:18Z","2022-08-03T20:44:31Z"
"","12228","KAFKA-13950: Fix resource leaks","# Problem We are not properly closing Closeable resources in the code base at multiple places.  # Change This code change fixes multiple of these leaks. Note that  1. `UncheckedIOException` is used to create minimal code changes else the propagation of checked IoException will lead to multiple changes at different places.  2. `Utils.closeQuietly` is used in this code change where author thinks that propagation of original exception is essential.","open","","divijvaidya","2022-05-31T09:34:39Z","2022-07-13T10:18:22Z"
"","12233","MINOR: Clean up tmp files created by tests","# Problem There are a bunch of tests which do not clean up after themselves. This leads to accumulation of files in the tmp directory of the system on which the tests are running.   # Change This code change fixes some of the main culprit tests which leak the files in the temporary directory.","closed","","divijvaidya","2022-05-31T15:16:09Z","2022-06-16T23:46:07Z"
"","12452","KAFKA-14122: Fix flaky test DynamicBrokerReconfigurationTest#testKeyStoreAlter","# Background At the beginning of the test, we create a producer (say P1) & consumer (say C1) (with enable_auto_commit=true, auto.commit.interval.ms = 5000 and groupId=""group1""). P1 and C1 continuously write and read messages throughout the test and at the end we assert that all messages were received and **no duplicates were received**.  During the execution of the test, we create more producers and consumers as we dynamically change configuration and assert sanity of produce/consumer operation. When creating a new consumer (say C2), **we create it in the same consumer group as C1, i.e. groupId=""group1""**.  # Problem When C2 is created, it triggers a rebalance within the consumer group ""group1"" which already has C1. Thus, consumption of C1 is disrupted. Since C1 uses `enable_auto_commit=true`, there is a possibility that it reads duplicate messages after rebalance. When it reads duplicate messages after rebalance, it causes the test to fail.  # Solution Do not disturb the operation of C1 when introducing C2 by creating C2 with a separate group ""group2"". This fixes the flakiness of the test.","closed","","divijvaidya","2022-07-28T10:18:59Z","2022-08-03T09:20:09Z"
"","12425","Change test defaults","","open","","cmccabe","2022-07-21T08:14:54Z","2022-07-22T19:32:28Z"
"","12332","documentation - quickstart use cases link fix","","closed","","vilmos","2022-06-22T13:42:08Z","2022-08-01T12:07:59Z"
"","12297","KAFKA-13846: Follow up PR to address review comments","","closed","","vamossagar12","2022-06-15T17:49:45Z","2022-07-08T04:55:50Z"
"","12278","MINOR: add AuthorizerNotReadyException","","open","","cmccabe","2022-06-09T22:21:38Z","2022-07-31T04:56:57Z"
"","12203","Fix typo in processor api developer guide","","closed","","amir","2022-05-24T16:21:23Z","2022-05-30T22:57:47Z"
"","12196","KAFKA-14084: Implement SCRAM support in KRaft","","open","","cmccabe","2022-05-23T08:09:07Z","2022-07-18T21:10:57Z"
"","12163","KAFKA-13900 Support Java 9 direct ByteBuffer Checksum methods","","open","","franz1981","2022-05-13T18:18:48Z","2022-07-18T16:12:32Z"
"","12142","MINOR: install Exit.exit handler in BrokerMetadataPublisherTest","","closed","","cmccabe","2022-05-09T20:11:06Z","2022-05-10T19:44:42Z"
"","11988","MINOR: fix typo in FetchRequest.json","","closed","","bozhao12","2022-04-02T10:07:22Z","2022-04-06T06:29:03Z"
"","11931","Fix LICENSE-binary","","closed","","tombentley","2022-03-22T15:22:17Z","2022-03-22T17:43:11Z"
"","11922","Update upgrade.html for 3.1.1","","closed","","tombentley","2022-03-21T11:36:30Z","2022-03-22T15:20:35Z"
"","11915","MINOR: Fix `ConsumerConfig.ISOLATION_LEVEL_DOC`","","closed","","guizmaii","2022-03-18T02:02:16Z","2022-03-18T08:55:05Z"
"","11905","MINOR: Fix incorrect log for out-of-order KTable","","closed","","tchiotludo","2022-03-16T10:45:43Z","2022-03-18T02:01:05Z"
"","11904","MINOR: Fix `ConsumerConfig.ISOLATION_LEVEL_DOC`","","closed","","guizmaii","2022-03-16T07:26:46Z","2022-03-18T02:02:42Z"
"","11728","Cherrypick KAFKA-4090: Validate SSL connection in client to 3.1.x","","closed","","azhur","2022-02-02T18:39:18Z","2022-02-02T18:40:12Z"
"","11662","Add github ci","","closed","","gurinderu","2022-01-10T06:28:00Z","2022-01-10T06:30:14Z"
"","11640","MINOR: enable KRaft on ListConsumerGroupTest","","closed","kip-500,","cmccabe","2022-01-02T09:29:28Z","2022-05-12T22:04:14Z"
"","11639","MINOR: enable KRaft in EndToEndClusterIdTest","","closed","kip-500,","cmccabe","2022-01-02T07:53:47Z","2022-05-12T22:04:55Z"
"","11638","MINOR: enable KRaft in DeleteTopicsRequestTest","","open","kip-500,","cmccabe","2022-01-02T02:54:26Z","2022-01-10T23:35:48Z"
"","11637","MINOR: enable KRaft in MetadataRequestTest","","closed","kip-500,","cmccabe","2022-01-02T02:31:58Z","2022-01-06T05:53:57Z"
"","11636","MINOR: enable KRaft in LogOffsetTest","","closed","kip-500,","cmccabe","2022-01-02T02:16:25Z","2022-05-12T22:05:26Z"
"","11635","MINOR: enable KRaft in MinIsrConfigTest","","closed","kip-500,","cmccabe","2022-01-02T02:01:12Z","2022-01-06T22:01:43Z"
"","11634","MINOR: enable KRaft in TopicCommandIntegrationTest","","closed","kip-500,","cmccabe","2022-01-02T01:52:40Z","2022-05-12T20:53:05Z"
"","11633","MINOR: support KRaft in TransactionsExpirationTest","","closed","kip-500,","cmccabe","2022-01-02T01:36:52Z","2022-01-06T22:53:37Z"
"","11629","MINOR: enable KRaft mode in CreateTopicsRequestTest","","closed","kip-500,","cmccabe","2021-12-27T20:27:53Z","2022-01-06T17:59:34Z"
"","11567","KAFKA-13494: WindowKeyQuery and WindowRangeQuery","","closed","streams,","patrickstuedi","2021-12-06T14:01:10Z","2022-01-03T04:29:35Z"
"","11550","KRaft to KRaft upgrades","","closed","kip-500,","mumrah","2021-11-29T18:57:22Z","2022-02-01T17:17:43Z"
"","11544","MINOR: some code cleanups in the controller","","closed","","cmccabe","2021-11-28T00:06:09Z","2021-12-10T23:32:38Z"