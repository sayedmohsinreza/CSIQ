"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","11202","MINOR: Fixed ""snasphot"" naming issue","修复”snasphot”命名问题. 改为”snapshot”.","closed","","socutes","2021-08-12T01:24:18Z","2021-08-12T01:25:52Z"
"","10658","POC for CheckStyle 8.42 regression (with 'Unnecessary Parentheses' errors)","⚠️  Not meant to be merged.  Related to https://github.com/checkstyle/checkstyle/issues/9957   @romani please note that I intentionally fine-grained this PR to a few commits (with appropriate messages, I hope you will find them useful). Rationale: my intention was to dissect important parts bit-by-bit (let me know in case you want me to change something).   **_Notable temporary changes:_** - checkstyle gradle `maxError` property is introduced (just for counting error counting purposes) - module `Indentation` is disable via checkstyle.xml (in this contex those errors are not relevant)  **_How to execute CheckStyle gradle tasks:_**  ` ./gradlew checkstyleMain checkstyleTest`  https://github.com/dejan2609/kafka/blob/apache-kafka-with-checkstyle-8.42/README.md#running-code-quality-checks  **_CheckStyle gradle configuration / config files:_**  - https://github.com/dejan2609/kafka/blob/apache-kafka-with-checkstyle-8.42/build.gradle#L588 - https://github.com/dejan2609/kafka/blob/apache-kafka-with-checkstyle-8.42/build.gradle#L712 - https://github.com/dejan2609/kafka/blob/apache-kafka-with-checkstyle-8.42/checkstyle/checkstyle.xml ``` build.gradle gradle/dependencies.gradle /chekstyle/ folder (with checkstyle config files) ```  ℹ️ Note: also related to this Kafka PR: #10656","closed","","dejan2609","2021-05-09T15:43:35Z","2021-05-16T14:53:40Z"
"","11304","MINOR: Supplement unit test for KAFKA-13175  : Optimization TopicExis…","…tsException,When a topic is marked for deletion.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangdaixai","2021-09-07T05:44:32Z","2021-09-07T20:50:35Z"
"","11433","KAFKA-13295: Avoiding Transation timeouts arising due to long restora…","…tion times in EOS  The PR aims to avoid transaction timeouts arising due to long restoration times of new tasks. In particular, during assignment of active tasks, if there are any open transactions that need commit, this change would ensure those are done.","open","","vamossagar12","2021-10-25T17:04:02Z","2022-04-26T08:36:27Z"
"","11185","KAFKA-13175; Better indicates for created  topic is marked for deleti…","…on. kafka.zk.AdminZkClient  (#129)  This patch optimization prompts when a created topic is marked for deletion. "" Topic is marked for deletion.""  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangdaixai","2021-08-06T12:07:30Z","2021-08-18T08:08:37Z"
"","11371","KAFKA-13337: fix of possible java.nio.file.AccessDeniedException during Con…","…nect plugin directory scan  org.apache.kafka.connect.runtime.isolation.PluginUtils.pluginUrls scans a path and collects plugin candidates from there. However, if a directory is not readable, it fails with AccessDeniedException instead of skipping it. This commit fixes this minor problem with the change of plugin path filter used during scanning.  I've also added a unit test for proof.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","heritamas","2021-09-30T15:48:51Z","2022-01-17T14:03:45Z"
"","10669","KAFKA-12769: Backport too 2.8 of KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while bui…","…lding SslTransportLayer  This is a cherry picked commit of https://github.com/apache/kafka/pull/10059 to branch 2.8  Original commit message: This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-11T07:25:57Z","2021-05-12T07:13:35Z"
"","10672","KAFKA-12769: Backport to 2.7 of KAFKA-8562; SaslChannelBuilder - Avoid (reverse) DNS lookup while bui…","…lding SslTransportLayer  This is a cherry picked commit of #10059 to branch 2.7  This patch moves the `peerHost` helper defined in `SslChannelBuilder` into `SslFactor`. `SaslChannelBuilder` is then updated to use a new `createSslEngine` overload which relies on `peerHost` when building its `SslEngine`. The purpose is to avoid the reverse DNS in `getHostName`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-11T07:40:00Z","2021-05-12T07:13:25Z"
"","10662","KAFKA-12747: Fix flaky test RocksDBStoreTest.shouldReturnUUIDsWithStr…","…ingPrefix  Fixes the test by checking if there is a prefix collision. Following comment on [JIRA ticket](https://issues.apache.org/jira/browse/KAFKA-12747?focusedCommentId=17338612&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17338612), in case of prefix clash, we check that the returned value can be either of both (a or b).  Alternatively, we could re-generate the second UUID in case of prefix clash until they are clash-free.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-10T11:58:28Z","2021-05-10T17:08:54Z"
"","10975","KAFKA-13035  updated documentation for connector restart REST API to …","…include the tasks restart behavior   Targets AK 3.0.0 where KAFKA-4793 will get released.  @kkonstantine and @rhauch Could you please review and see if the changes look good.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kpatelatwork","2021-07-05T19:12:57Z","2021-07-06T18:31:40Z"
"","11038","KAFKA-13069: Add magic number to DefaultKafkaPrincipalBuilder.KafkaPr…","…incipalSerde  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-13T16:46:01Z","2021-07-20T21:40:03Z"
"","10530","KAFKA-10231 fail bootstrap of Rest server if the host name in the adv…","…ertised uri is invalid and other nodes can't reach it. Node names have rules about what characters they can have and maximum length like in RFC-1123. The node-node communication over REST API won't happen if this node's advertised URL to the cluster has an invalid host name, and the error message in logs isn't very helpful.   This PR adds a new behavior by using the java IDN class to expose the detailed error message and fails the server bootstrap.   @C0urante , @rhauch and @kkonstantine  please review  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kpatelatwork","2021-04-12T22:43:06Z","2021-04-22T21:16:31Z"
"","11443","KAFKA-13411: This code enforces the creation of the correct sasl client (OAuthBear…","…erSaslClient) when the mechanism is OAUTHBEARER. There were issues when the client was loaded from different contexts which contains other sasl client implementations also for the OAUTHBEARER mechanism (example running in a wildfly context)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","shankarb27","2021-10-27T21:14:29Z","2021-11-02T05:03:59Z"
"","10549","KAFKA-8605 log an error message when we detect multiple copies of sam…","…e plugin class in the plugin paths  *A plugin like a converter or connector can exist multiple times in the plugin-path and the user may not realize that one of those copies will be used. The copy that will be used really depends on the operating system and how the filesystem gives us the jar listing, therefore we need to log an error message so the user is aware of this conflict and can remove the offending copy*   @rhauch @gharris1727 @C0urante  please review   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","kpatelatwork","2021-04-16T22:01:36Z","2021-04-20T20:35:08Z"
"","10649","KAFKA-12762: Use connection timeout when polling the network for new …","…connections  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2021-05-07T15:36:49Z","2021-09-17T13:35:54Z"
"","10741","[KAFKA-12644] Add Missing Class-Level Javadoc to Exception Classes","…ache.kafka.common.errors.ApiException  Added missing class-level javadocs to Exception classes  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","izzyacademy","2021-05-20T21:53:29Z","2021-05-21T15:41:13Z"
"","11226","KAFKA-13175; Optimization TopicExistsException,When a topic is marked…","… for deletion.  After a topic is deleted, the topic is marked for deletion, create topic with the same name throw exception topic already exists. It should throw exception the topic is marked for deletion. I can choose to wait for the topic to be completely deleted. If the topic is still not deleted for a long time, we need to check the reason why it is not deleted.  Signed-off-by: yangshengwei   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangdaixai","2021-08-18T08:54:39Z","2021-09-07T05:47:51Z"
"","10982","MINOR: Update dropwizard metrics to 4.1.12.1","ZooKeeper was updated to v3.6.3 via https://github.com/apache/kafka/pull/10918.  However, it was noted in that PR discussion (https://github.com/apache/kafka/pull/10918#discussion_r663412933) that the dropwizard metrics-core library has since been updated from v3.2.5 to v4.1.12.1 for 3.7.x releases (via https://github.com/apache/zookeeper/commit/13fe0d0ffb9fd2c379b9b430aaaf9ee75acfceba).  Since there were no code changes associated with this library version bump in ZooKeeper, and since we wish to avoid potential CVEs if possible, we can consider updating to this newer version now assuming that system tests pass. I will attach system test results when they complete.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-06T19:32:17Z","2021-07-09T14:41:09Z"
"","11469","MINOR: disable zookeeper.sasl.client to avoid false error","Zookeeper connection always does SASL checks currently. That behavior produces false warnings to kafka log. For example:  ### using PLAINTEXT ``` [2021-11-05 11:39:33,738] INFO Opening socket connection to server caijiapngdeiMac/192.168.50.178:19516. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn) ```  ### using SASL_PLAINTEXT with `java.security.auth.login.config` ``` [2021-11-05 11:52:37,130] ERROR [ZooKeeperClient Kafka server] Auth failed. (kafka.zookeeper.ZooKeeperClient) ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-11-05T11:54:52Z","2022-02-05T19:34:22Z"
"","11295","KAFKA-13270: Set JUTE_MAXBUFFER to 4 MB by default","ZooKeeper 3.6.0 changed the default configuration for JUTE_MAXBUFFER from 4 MB to 1 MB. This causes a regression if Kafka tries to retrieve a large amount of data across many znodes – in such a case the ZooKeeper client will repeatedly emit a message of the form ""java.io.IOException: Packet len <####> is out of range"".  We restore the 3.4.x/3.5.x behavior unless the caller has set the property (note that ZKConfig auto configures itself if certain system properties have been set).  I added a unit test that fails without the change and passes with it.  I also refactored the code to streamline the way we handle parameters passed to KafkaZkClient and ZooKeeperClient.   See https://github.com/apache/zookeeper/pull/1129 for the details on why the behavior changed in 3.6.0.  Credit to @rondagostino for finding and reporting this issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-09-04T00:00:53Z","2021-09-06T16:18:47Z"
"","11423","Update doap_Kafka.rdf","Wrong syntax; which is why Kafka does not appear in BigData category on projects.a.o","closed","","sebbASF","2021-10-21T15:55:44Z","2022-01-30T21:52:27Z"
"","11345","KAFKA-13603: Allow the empty active segment to have missing offset index during recovery","Within a LogSegment, the TimeIndex and OffsetIndex are lazy indices that don't get created on disk until they are accessed for the first time. However, Log recovery logic expects the presence of an offset index file on disk for each segment, otherwise, the segment is considered corrupted.  This PR introduces a `forceFlushActiveSegment` boolean for the `log.flush` function to allow the shutdown process to flush the empty active segment, which makes sure the offset index file exists.  Co-Author:    Kowshik Prakasam   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-09-20T19:43:17Z","2022-01-27T23:00:50Z"
"","10888","KAFKA-12955: Fix LogLoader to pass materialized view of segments for deletion","Within `LogLoader.removeAndDeleteSegmentsAsync()`, we should force materialization of the `segmentsToDelete` iterable, to make sure the results of the iteration remain valid and deterministic. We should also pass only the materialized view to the logic that deletes the segments, as otherwise we could end up deleting the wrong segments.  Note: At another spot where there is similar logic, we already do things right: https://github.com/apache/kafka/blob/6b005b2b4eece81a5500fb0080ef5354b4240681/core/src/main/scala/kafka/log/Log.scala#L1890  **Tests:** Added the missing unit test coverage to `LogLoaderTest.testLogEndLessThanStartAfterReopen()`. Previously the test was in `LogTest` suite but it has been moved over now to `LogLoaderTest` suite, as it deserves to be there instead. This test fails without the fix, but passes with the fix. These are the important lines in the test that provides the necessary assertions: ``` @Test def testLogEndLessThanStartAfterReopen(): Unit = {    ...    ...    ...    // Validate that the remaining segment matches our expectations    val onlySegment = log.segments.firstSegment.get    assertEquals(startOffset, onlySegment.baseOffset)    assertTrue(onlySegment.log.file().exists())    assertTrue(onlySegment.lazyOffsetIndex.file.exists())    assertTrue(onlySegment.lazyTimeIndex.file.exists()) } ```","closed","","kowshik","2021-06-16T07:55:16Z","2021-06-16T22:02:50Z"
"","11331","KAFKA-13111: Re-evaluate Fetch Sessions when using topic IDs","With the changes for topic IDs, we have a different flow. When a broker receives a request, it uses a map to convert the topic ID to topic names. If the topic ID is not found in the map, we return a top level error and close the session. This decision was motivated by the difficulty to store “unresolved” partitions in the session. In earlier iterations we stored an “unresolved” partition object in the cache, but it was somewhat hard to reason about and required extra logic to try to resolve the topic ID on each incremental request and add to the session. It also required extra logic to forget the topic (either by topic ID if the topic name was never known or by topic name if it was finally resolved when we wanted to remove from the session.)  One helpful simplifying factor is that we only allow one type of request (uses topic ID or does not use topic ID) in the session. That means we can rely on a session continuing to have the same information. We don’t have to worry about converting topics only known by name to topic ID for a response and we won’t need to convert topics only known by ID to name for a response.  This PR introduces a change to store the ""unresolved partitions"" in the cached partition object. If a version 13+ request is sent with a topic ID that is unknown, a cached partition will be created with that fetch request data and a null topic name. On subsequent incremental requests, unresolved partitions may be resolved with the new IDs found in the metadata cache. When handling the request, getting all partitions will return a TopicIdPartition object that will be used to handle the request and build the response. Since we can rely on only one type of request (with IDs or without), the cached partitions map will have different keys depending on what fetch request version is being used.   This PR involves changes both in FetchSessionHandler and FetchSession. Some major changes are outlined below.  1. FetchSessionHandler: Forgetting a topic and adding a new topic with the same name -  We may have a case where there is a topic foo with ID 1 in the session. Upon a subsequent metadata update, we may have topic foo with ID 2. This means that topic foo has been deleted and recreated. When sending fetch requests version 13+ we will send a request to add foo ID 2 to the session and remove foo ID 1. Otherwise, we will fall back to the same behavior for versions 12 and below  2. FetchSession: Resolving in Incremental Sessions - Incremental sessions contain two distinct sets of partitions. Partitions that are sent in the latest request that are new/updates/forgotten partitions and the partitions already in the session. If we want to resolve unknown topic IDs we will need to handle both cases.     * Partitions in the request  - These partitions are either new or updating/forgetting previous partitions in the session. The new partitions are trivial. We either have a resolved partition or create a partition that is unresolved. For the other cases, we need to be a bit more careful.          * For updated partitions we have a few cases – keep in mind, we may not programmatically know if a partition is an update:             1. partition in session is resolved, update is resolved: trivial              2. partition in session is unresolved, update is unresolved: in code, this is equivalent to the case above, so trivial as well              3. partition in session is unresolved, update is resolved: this means the partition in the session does not have a name, but the metadata cache now contains the name –  to fix this we can check if there exists a cached partition with the given ID and update it both with the partition update and with the topic name.              4. partition in session is resolved, update is unresolved: this means the partition in the session has a name, but the update was unable to be resolved (ie, the topic is deleted) – this is the odd case. We will look up the partition using the ID. We will find the old version with a name but will not replace the name. This will lead to an UNKNOWN_TOPIC_OR_PARTITION or INCONSISTENT_TOPIC_ID error which will be handled with a metadata update. Likely a future request will forget the partition, and we will be able to do so by ID.              5. Two partitions in the session have IDs, but they are different: only one topic ID should exist in the metadata at a time, so likely only one topic ID is in the fetch set. The other one should be in the toForget. We will be able to remove this partition from the session. If for some reason, we don't try to forget this partition — one of the partitions in the session will cause an inconsistent topic ID error and the metadata for this partition will be refreshed — this should result in the old ID being removed from the session. This should not happen if the FetchSessionHandler is correctly in sync.          * For the forgotten partitions we have the same cases:             1. partition in session is resolved, forgotten is resolved: trivial              2. partition in session is unresolved, forgotten is unresolved: in code, this is equivalent to the case above, so trivial as well              3. partition in session is unresolved, forgotten is resolved: this means the partition in the session does not have a name, but the metadata cache now contains the name –  to fix this we can check if there exists a cached partition with the given ID and try to forget it before we check the resolved name case.              4. partition in session is resolved, update is unresolved: this means the partition in the session has a name, but the update was unable to be resolved (ie, the topic is deleted) We will look up the partition using the ID. We will find the old version with a name and be able to delete it.              5. both partitions in the session have IDs, but they are different: This should be the same case as described above. If we somehow do not have the ID in the session, no partition will be removed. This should not happen unless the Fetch Session Handler is out of sync.      * Partitions in the session - there may be some partitions in the session already that are unresolved. We can resolve them in forEachPartition using a method that checks if the partition is unresolved and tries to resolve it using a topicName map from the request. The partition will be resolved before the function using the cached partition is applied.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-09-16T18:18:48Z","2021-11-15T09:06:16Z"
"","11459","KAFKA-13394: Topic IDs should be removed from PartitionFetchState if they are no longer sent by the controller","With KAFKA-13102, we added topic IDs to the InitialFetchState and the PartitionFetchState in order to send fetch requests using topic IDs when ibp is 3.1.   However, there are some cases where we could initially send topic IDs from the controller and then no longer to do so (controller changes to an IBP < 2.8). If we do not remove from the PartitionFetchState and one broker is still IBP 3.1, it will try to send a version 13 fetch request to brokers that no longer have topic IDs in the metadata cache. This could leave the cluster in a state unable to fetch from these partitions.  This PR removes the topic IDs from the PartitionFetchState if the log contains a topic ID but the request does not. This means that we will always handle a leader and isr request if there is no ID in the request but an ID in the log.  Such a state should be transient because we are either  * upgrading the cluster and somehow switched between a new IBP controller and an old one --> and will eventually have all new IBP controllers/brokers. * downgrading the cluster --> will eventually have all old IBP controllers/brokers and will restart the broker/delete the partition metadata file for them.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-11-02T23:38:52Z","2021-11-18T16:32:25Z"
"","11462","MINOR: Set mock correctly in RocksDBMetricsRecorderTest","With a nice mock in `RocksDBMetricsRecorderTest#shouldCorrectlyHandleHitRatioRecordingsWithZeroHitsAndMisses()` and `RocksDBMetricsRecorderTest#shouldCorrectlyHandleAvgRecordingsWithZeroSumAndCount()` were green although `getTickerCount()` was never called. The tests were green because EasyMock returns 0 for a numerical return value by default if no expectation is specified. Thus, commenting out the expectation for `getTickerCount()` did not change the result of the test.  This commit changes the mock to a default mock and fixes the expectation to expect `getAndResetTickerCount()`. Now, commenting out the expectation leads to a test failure.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-11-03T13:13:52Z","2021-11-18T17:14:37Z"
"","10533","KAFKA-10493: Drop out-of-order KTable records (WIP)","WIP PR (did not update all test yet).  The proposal is to be _strict_ and use `context.streamTime()` to drop out-of-order records. The advantage is, that we can drop out-of-order records even if there is no table state store.  Call for review @guozhangwang @ableegoldman @vvcephei @cadonna @spena","open","streams,","mjsax","2021-04-13T07:12:13Z","2021-04-13T07:12:13Z"
"","11211","KAFKA-12960: Enforcing strict retention time for WindowStore and Sess…","WindowedStore and SessionStore do not implement a strict retention time in general. We should consider to make retention time strict: even if we still have some record in the store (due to the segmented implementation), we might want to filter expired records on-read. This might benefit PAPI users.  This PR, adds the filtering behaviour in the Metered store so that, it gets automatically applied for cases when a custom state store is implemented","open","","vamossagar12","2021-08-13T12:42:01Z","2022-07-13T17:48:37Z"
"","10871","KAFKA-8940: decrease session timeout to make test faster and reliable","While there might still be some issue about the test as described [here](https://issues.apache.org/jira/browse/KAFKA-8940?focusedCommentId=17214850&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17214850) by @ableegoldman , but I found the reason why this test failed quite frequently recently. It's because we increased the session timeout to 45 sec in KIP-735.  The failed messages is like this: ``` java.lang.AssertionError: tagg is missing verifying suppressed min-suppressed verifying min-suppressed with 10 keys verifying suppressed sws-suppressed verifying min with 10 keys verifying max with 10 keys verifying dif with 10 keys verifying sum with 10 keys verifying cnt with 10 keys verifying avg with 10 keys ``` Or ``` java.lang.AssertionError: verifying tagg fail: key=562 tagg=[ConsumerRecord(topic = tagg, partition = 0, leaderEpoch = 0, offset = 2, CreateTime = 1623347258886, serialized key size = 3, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 562, value = 1)] expected=0 	 taggEvents: [ConsumerRecord(topic = tagg, partition = 0, leaderEpoch = 0, offset = 2, CreateTime = 1623347258886, serialized key size = 3, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 562, value = 1)] verifying suppressed min-suppressed verifying min-suppressed with 10 keys verifying suppressed sws-suppressed verifying min with 10 keys verifying max with 10 keys verifying dif with 10 keys verifying sum with 10 keys verifying cnt with 10 keys verifying avg with 10 keys avg fail: key=7-1006 actual=300.5952380952381 expected=506.5 ``` Or ``` java.lang.AssertionError: verifying tagg fail: key=694 tagg=[ConsumerRecord(topic = tagg, partition = 0, leaderEpoch = 0, offset = 9, CreateTime = 1623338149617, serialized key size = 3, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 694, value = 1)] expected=0 	 taggEvents: [ConsumerRecord(topic = tagg, partition = 0, leaderEpoch = 0, offset = 9, CreateTime = 1623338149617, serialized key size = 3, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 694, value = 1)] verifying suppressed min-suppressed verifying min-suppressed with 10 keys verifying suppressed sws-suppressed verifying min with 10 keys verifying max with 10 keys verifying dif with 10 keys verifying sum with 10 keys verifying cnt with 10 keys verifying avg with 10 keys ``` The failure are due to the processing is not completed in time as described below.   We can check the jenkins failing trend in `trunk` branch [here](https://ci-builds.apache.org/job/Kafka/job/kafka/job/trunk/217/testReport/junit/org.apache.kafka.streams.integration/SmokeTestDriverIntegrationTest/history/): ![image](https://user-images.githubusercontent.com/43372967/121793776-0ee2a700-cc35-11eb-9648-f93fe3e34976.png) This test never failed since build # 168, until build # 206 and later  The reason why increasing session timeout affected this test is because in this test, we will keep adding new stream clients and remove old one, to maintain only 3 stream clients alive. The problem here is, when old stream closed, we won't trigger rebalance immediately due to the stream clients are all static members as described in KIP-345, which means, we will trigger trigger group rebalance only when `session.timeout` expired. That said, when old client closed, we'll have at least 45 sec with some tasks not working.   Also, in this test, we have 2 timeout conditions to fail this test before verification passed: 1. 6 minutes timeout 2. polling 30 times (each with 5 seconds) without getting any data. (that is, 5 * 30 = 150 sec without consuming any data)  For (1), in my test under 45 session timeout, we'll create 8 stream clients, which means, we'll have 5 clients got closed. And each closed client need 45 sec to trigger rebalance, so we'll have 45 * 5 = 225 sec (~4 mins) of the time having some tasks not working.  For (2), during new client created and old client closed, it need some time to do rebalance. With 45 session timeout, we only got ~100 sec left. In slow jenkins env, it might reach the 30 retries without getting any data timeout.  Therefore, decreasing session timeout can make this test completes faster and more reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-13T03:24:47Z","2021-06-15T03:49:49Z"
"","10640","KAFKA-10847: Set StreamsConfig on InternalTopologyDriver before writing topology","While running some tests, I noticed the KSTREAMS-OUTERSHARED store, used in left/outer joins, was still added in the list of state stores when the `StreamsConfig.InternalConfig.ENABLE_KSTREAMS_OUTER_JOIN_SPURIOUS_RESULTS_FIX` flag was false. When this flag is false, the shared store should not be added to the join nodes.  Testing - Added unit tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","spena","2021-05-06T12:52:42Z","2021-05-07T00:27:24Z"
"","11016","KAFKA-13058; AlterConsumerGroupOffsetsHandler does not handle partition errors correctly.","While reviewing https://github.com/apache/kafka/pull/10973, I have noticed that `AlterConsumerGroupOffsetsHandler` does not handle partition errors correctly. The issue is that any partition error fails the entire future instead of being passed as an error for its corresponding partition. `KafkaAdminClientTest#testOffsetCommitWithMultipleErrors` reproduces the bug.  The regression was introduced by KIP-699.  Context: https://github.com/apache/kafka/pull/10973#discussion_r667335981.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-07-10T16:47:23Z","2021-07-19T14:49:22Z"
"","10819","MINOR: LogLoader: Add log identifier in few missing areas","While reading the code, I noticed that `LogLoader` had a few points where the log identifier was not printed as part of the log message. I've fixed it in this PR.  **Tests:** Rely on existing tests.","closed","","kowshik","2021-06-04T11:06:19Z","2021-06-04T16:40:36Z"
"","11391","MINOR: Update LICENSE-binary","While preparing 2.7.2 I noticed the LICENSE file was out of date.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-12T14:11:12Z","2021-10-12T15:09:46Z"
"","11389","MINOR: Update LICENSE-binary","While preparing 2.6.3 I noticed the LICENSE file was out of date.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-12T09:22:18Z","2021-10-12T09:42:35Z"
"","11085","MINOR: reduce debug log spam and busy loop during shutdown","While investigating some system test failures I found the Streams logs often got up to several GB in size, with much of it being due to these three lines being logged on repeat many times per ms: ``` [2021-07-19 11:15:26,880] DEBUG [] Invoking poll on main Consumer  [2021-07-19 11:15:26,880] DEBUG [] Main Consumer poll completed in 0 ms and fetched 0 records  [2021-07-19 11:15:26,880] DEBUG [] Thread state is already PENDING_SHUTDOWN, skipping the run once call  ``` Even though they're debug logs this is just way too much printed way too often, and also reveals how much cpu we're wasting just spinning in this busy loop doing nothing else. It seems the situation was that the thread had been told to shut down but was waiting for an in-progress rebalance to complete before it could exit the poll loop and complete its shutdown. Turns out we were polling with `Duration.ZERO` in this case and therefore just running through the loop again and again and again within a single millisecond.  So, we should be passing in a larger timeout to `poll()` when the thread is in `PENDING_SHUTDOWN` to let it actually wait for the ongoing rebalance to finish. Also threw in a few miscellaneous fixes/cleanups that I came across on the side","open","","ableegoldman","2021-07-20T02:14:11Z","2021-07-22T02:23:33Z"
"","10878","KAFKA-12898; Owned partitions in the subscription must be sorted","While investigating https://issues.apache.org/jira/browse/KAFKA-12896, I have noticed that the leader was always sending the same subscribed partitions but not always in the order. The group coordinator compares the provided subscription with the store subscription based on their bytes representation. So if the subscribed partitions are not in the same order, the group coordinator would consider that they are different and rebalance the group.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-06-14T15:32:31Z","2021-06-24T08:10:11Z"
"","11263","MINOR: remove unused Properties from GraphNode#writeToTopology","While going through the StreamsBuilder#build method to see if any of the steps actually modify internal state, I noticed this GraphNode method accepts a Properties input parameter but never uses it in any of its implementations. We can remove this parameter to clean things up, and make it clear that writing nodes to the topology doesn't involve the app properties.","closed","","ableegoldman","2021-08-26T05:02:26Z","2021-08-27T00:19:04Z"
"","11442","KAFKA-7883 add schema.namespace support to SetSchemaMetadata SMT in Kafka Connect","When you use Kafka Connect and Avro Converter with Schema Registry you may need to transform your source schema namespace. Currently `SetSchemaMetadata` provide a way to transform the namespace by specifying in the `schema.name` attribute both actual namespace being concatenated with the actual schema name: ```javascript {   ""transforms"" : ""TransformSchema"",   ""transforms.TransformSchema.type"" : ""org.apache.kafka.connect.transforms.SetSchemaMetadata$Value"",   ""transforms.TransformSchema.schema.name"" : ""my.new.namespace.NewSchemaName"" } ``` But this way you are forced to specify the schema name even when you want to preserve the original schema name. In some cases you just may need to replace possibly an empty namespace of your source with the actual namespace fitting ""your.java.package"" and leave schema name as it is. Moreover as mentioned in [KAFKA-7883](https://issues.apache.org/jira/browse/KAFKA-7883), when use JDBC connector with multiple tables in the source database what you may want is to preserve the table names but specify some non-empty namespace for them. To support this we have to provide a way to append a schema namespace to the schema name (doesn't matter being updated or not). In this PR a new `schema.namespace` attribute is added to `SetSchemaMetadata` letting developer to decide either to transform namespace only or transform both schema name and schema namespace by using either `schema.namespace` together with `schema.name` or just single `schema.name` attribute. An example: ```javascript {   ""transforms"" : ""TransformSchema"",   ""transforms.TransformSchema.type"" : ""org.apache.kafka.connect.transforms.SetSchemaMetadata$Value"",   ""transforms.TransformSchema.schema.namespace"" : ""my.new.namespace"",   ""transforms.TransformSchema.schema.name"" : ""NewSchemaName"",   ""transforms.TransformSchema.schema.version"" : 42 } ```","open","kip,","mnegodaev","2021-10-27T19:09:24Z","2022-08-02T06:59:19Z"
"","11387","Add kafka topic sync offset lag metrics by JMX","When we use mirror maker , we very concerned about sync offset lags.We wonder Progress of all data synchronization,  I  definition this sync lag. In other words ,the lag  is the topc partition offset delay between soruce cluster and target clusterI use JMX to export metrics to show  the lag.In MirrorSourceTask realtime to calc lag. *Calc the sync lag, the lag means topic partition latest offset  minus the current sourceTask sync topic offset.* *lag = latest offset in source cluster - current sync offset in source cluster*  ![image](https://user-images.githubusercontent.com/20434652/136654258-c75eab49-44df-42ec-9595-2e8f98d03b9d.png)","open","","aalinyu","2021-10-09T10:22:06Z","2021-10-09T10:22:06Z"
"","10763","KAFKA-12520: Ensure log loading does not truncate producer state unless required","When we find a .swap file on startup, we typically want to rename and replace it as .log, .index, .timeindex, etc. as a way to complete any ongoing replace operations. These swap files are usually known to have been flushed to disk before the replace operation begins.  One flaw in the current logic is that we recover these swap files on startup and as part of that, end up truncating the producer state and rebuild it from scratch. This is unneeded as the replace operation does not mutate the producer state by itself. It is only meant to replace the .log file along with corresponding indices. Because of this unneeded producer state rebuild operation, we have seen multi-hour startup times for clusters that have large compacted topics.  This patch fixes the issue. With ext4 ordered mode, the metadata are ordered and no matter it is a clean/unclean shutdown. As a result, we rework the recovery workflow as follows.  1. If there are any `.cleaned` files, we delete all `.swap` files with higher/equal offsets due to KAFKA-6264. We also delete the `.cleaned` files. If no `.cleaned` file, do nothing for this step. 2. If there are any `.log.swap` files left after step 1, they, together with their index files, must be renamed from `.cleaned` and are complete (renaming from `.cleaned` to `.swap` is in reverse offset order). We rename these `.log.swap` files and their corresponding index files to regular files, while deleting the original files from compaction or segment split if they haven't been deleted. 3. Do log splitting for legacy log segments with offset overflow (KAFKA-6264) 4. If there are any other index swap files left, they must come from partial renaming from `.swap` files to regular files. We can simply rename them to regular files.  credit: some code is copied from @dhruvilshah3 's PR: https://github.com/apache/kafka/pull/10388  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-05-25T21:23:10Z","2021-06-29T16:17:14Z"
"","10558","KAFKA-12684: Fix noop set is incorrectly replaced with succeeded set from LeaderElectionCommand","When using the kafka-election-tool for preferred replica election, if there are partitions in the elected list that are in the preferred replica, the list of partitions already in the preferred replica will be replaced by the successfully elected partition list.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-19T02:42:15Z","2021-04-25T13:15:50Z"
"","11283","KAFKA-13249: Always update changelog offsets before writing the checkpoint file","When using EOS checkpointed offsets are not updated to the latest offsets from the changelog because the `maybeWriteCheckpoint` method is only ever called when `commitNeeded=false`. This change will force the update if `enforceCheckpoint=true` .  I have also added a test which verifies that both the state store and the checkpoint file are completely up to date with the changelog after the app has shutdown.","closed","streams,","hutchiko","2021-08-29T21:58:17Z","2021-09-30T17:57:23Z"
"","10540","MINOR: update default for field in DeleteTopicsRequest","When using DeleteTopicsRequest with topic IDs, the topic name field should be null. Before, the code was programmatically assigning null, but it will be easier and less error prone to simply set that as the default.   Tested using the previously created `DeleteTopicsRequestTest.java` file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-04-14T23:24:21Z","2021-04-15T15:01:26Z"
"","10613","KAFKA-10847: Set shared outer store to an in-memory store when in-memory stores are supplied","When users supply in-memory stores for left/outer joins, then the internal shared outer store must be switch to in-memory store too. This will allow users who want to keep all stores in memory to continue doing so.  Tests? - Added unit tests to validate topology and left/outer joins work fine with an in-memory shared store.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","spena","2021-04-29T19:30:57Z","2021-05-05T17:21:52Z"
"","11199","KAFKA-13194: bound cleaning by both LSO and HWM when firstUnstableOffsetMetadata is None","When the high watermark is contained in a non-active segment, we are not correctly bounding it by the hwm. This means that uncommitted records may overwrite committed data. I've separated out the bounding point tests to check the hwm case in addition to the existing active segment case.  See https://issues.apache.org/jira/browse/KAFKA-13194 for a further explanation.","closed","","lbradstreet","2021-08-11T20:04:10Z","2021-08-13T22:35:59Z"
"","11005","MINOR: Print the cached broker epoch","When the broker epochs do not match make sure to print both broker epochs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-08T23:57:29Z","2021-07-12T23:08:08Z"
"","11082","KAFKA-13104: Controller should notify raft client when it resigns","When the active controller encounters an event exception it attempts to renounce leadership. Unfortunately, this doesn't tell the RaftClient that it should attempt to give up leadership. This will result in inconsistent state with the RaftClient as leader but with the controller as inactive.  We should change this implementation so that the active controller asks the RaftClient to resign.   https://issues.apache.org/jira/browse/KAFKA-13104","closed","","dielhennr","2021-07-19T21:18:37Z","2021-07-21T03:39:18Z"
"","10925","MINOR: To verify segment.hasOverflow, the path of the segment should be printed","When refer to the return ""Check whether the last offset of the last batch in this segment overflows the indexes"", if the result is not expected, the path of the segment should be printed so that users can find problems.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xinzhuxiansheng","2021-06-24T17:46:21Z","2021-06-28T03:32:12Z"
"","11467","MINOR: fix java doc in kafkaProducer","When reading KafkaProducer java doc, I found there are some things out-of-date. Update it to avoid to mislead users.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-11-04T12:54:07Z","2021-11-18T11:14:58Z"
"","11189","KAFKA-13161: Update replica partition state and replica fetcher state on follower update","When processing the topics delta, make sure that the replica manager partition state and replica fetcher state matches the information included in the topic delta.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-08-09T14:57:35Z","2021-08-10T19:56:31Z"
"","10633","KAFKA-12751: Reset AlterIsr in-flight state for duplicate update requests","When processing AlterIsr request, KafkaController returns success without updating version if the requested LeaderAndIsr is the same as the persisted LeaderAndIsr: https://github.com/apache/kafka/blob/fe16912dfc59aa9f2379b904df89c9531cc9a2d5/core/src/main/scala/kafka/controller/KafkaController.scala#L2310  This PR ensures that we reset in-flight state in this case so that future updates are not blocked.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-05-05T14:35:54Z","2021-05-17T18:31:40Z"
"","11277","KAFKA-13244: Control Which Brokers Host Partitions of Newly Created Topics","When new topics are created through the admin interface, the Kafka controller creates the partition assignments for these topics according to the algorithm defined in `AdminUtils.assignReplicasToBrokers`. All (alive) brokers in the cluster become candidates for hosting partitions.  However, sometimes cluster administrators don't want certain brokers to host partitions for newly created topics. This can be for a variety of reasons, e.g.: - a broker may be on a host that is slated for retirement and about to be shut down - a broker may be acting as a canary of a new Kafka version and is being tested with a controlled set of topic-partitions, and should be restricted from hosting any other topics - similarly, a broker may in use for administrative operations, such as dumping log segments, and consequently may be unstable and ideally as little-used as possible  One solution is for the cluster admins to not give out CREATE permissions to clients, and instead provide a service/abstraction for creating new topics with explicit partition assignments. Unfortunately, in a world with Kafka Streams applications, this is too restrictive: Kafka Streams clients expect to use the Kafka tooling to create topics when their topology changes, or when they need to reset their apps.  Since there is currently there is no way to control which brokers are eligible to be assigned newly created partitions, I am proposing a new configuration parameter, `create.topic.broker.filter.policy.class.name` to allow cluster administrators to provide a pluggable class to control whether a broker should be allowed to host new partitions. The class implements a simple interface to give a binary yes/no verdict on each broker:  ```java boolean isAllowedToHostPartitions(Broker broker); ```  This follows a pattern similar to `create.topic.policy.class.name` and `alter.topic.policy.class.name`, which also provide cluster-level control over other aspects of topic creation.  This contribution is my original work and I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","mjaschob-twilio","2021-08-27T17:40:37Z","2021-10-18T08:31:03Z"
"","11292","KAFKA-13264: fix inMemoryWindowStore backward fetch not in reversed order","When introducing backward iterator for WindowStroe in https://github.com/apache/kafka/pull/9138, we forgot to make ""each segment"" in reverse order (i.e. in `descendingMap`) in `InMemoryWindowStore`. Fix it and add integration tests for it.  Currently, in Window store, we store records in [segments -> [records] ].  For example: window size = 500, input records:  key: ""a"", value: ""aa"", timestamp: 0 ==> will be in [0, 500] window key: ""b"", value: ""bb"", timestamp: 10 ==> will be in [0, 500] window key: ""c"", value: ""cc"", timestamp: 510 ==> will be in [500, 1000] window  So, internally, the ""a"" and ""b"" will be in the same segment, and ""c"" in another segments. segments: [0 /* window start */, records], [500, records]. And the records for window start 0 will be ""a"" and ""b"". the records for window start 500 will be ""c"".  Before this change, we did have a reverse iterator for segments, but not in ""records"". So, when doing backwardFetchAll, we'll have the records returned in order: ""c"", ""a"", ""b"", which should be ""c"", ""b"", ""a"" obviously.  So, back to the question: why did the original test cases not catch this issue? It's because the test input are all in different window start timestamp, which will have different different segments:  ```  private void putFirstBatch(final WindowStore store,                                @SuppressWarnings(""SameParameterValue"") final long startTime,                                final InternalMockProcessorContext context) {         context.setRecordContext(createRecordContext(startTime));         store.put(0, ""zero"", startTime);         store.put(1, ""one"", startTime + 1L);         store.put(2, ""two"", startTime + 2L);         store.put(3, ""three"", startTime + 2L);  // <-- this is the new record I added, to test multiple records in the same segment case          store.put(4, ""four"", startTime + 4L);         store.put(5, ""five"", startTime + 5L);     } ```   I added an additional record for `AbstractWindowBytesStoreTest` test. `InWindowStoreFetchTest`, we will produce records in timestamp:0, 1, 500, 501, 502, which will be put into window: [0, 500] * 2 and [500, 1000] * 3. And we try to fetch them forward/backward, to see if the results are as expected, i.e.: in reverse order should be 502, 501, 500, 1, 0.  The behavior works as expected in RocksDBWindowStore.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-09-02T08:12:39Z","2021-09-13T21:41:00Z"
"","10553","MINOR: Fix the negative time difference value from Log.scala","When I tested the kraft mode, I found that the time taken to load the snapshot in the startup log was negative.  [2021-04-18 00:19:37,377] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Recovering unflushed segment 0 (kafka.log.Log) [2021-04-18 00:19:37,382] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log) [2021-04-18 00:19:37,389] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log) [2021-04-18 00:19:37,393] INFO [Log partition=@metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took **-2ms** for snapshot load and 1ms for segment recovery from offset 0 (kafka.log.Log)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-17T17:12:27Z","2021-04-19T11:54:05Z"
"","10742","MINOR: Add log identifier/prefix printing in Log layer static functions","When https://github.com/apache/kafka/pull/10478 was merged, we accidentally lost the identifier/prefix string that we used to previously log to stderr from some of the functions in the `Log` class. In this PR, I have reinstated the identifier/prefix logging in these functions, so that the debuggability is restored.  **Tests:** Ran existing unit tests and checked the output. Noticed that the log identifier/prefix shows up from the lines wherever it is additionally logged from now.","closed","","kowshik","2021-05-20T21:55:10Z","2021-05-24T17:39:46Z"
"","10720","KAFKA-12815: preserve timestamp when getting value from upstream state store","When getting a value from an upstream state store, we should ensure that the record timestamp is accessible when applying a `Transformer`.","closed","streams,","mjsax","2021-05-19T01:45:14Z","2021-05-20T00:34:38Z"
"","10620","KAFKA-12736: KafkaProducer.flush holds onto completed ProducerBatch(s) until flush completes","When flush is called a copy of incomplete batches is made. This means that the full ProducerBatch(s) are held in memory until the flush has completed. For batches where the existing memory pool is used this is not as wasteful as the memory will be returned to the pool, but for non pool memory it can only be GC'd after the flush has completed. Rather than use copyAll we can make a new array with only the produceFuture(s) and await on those.","closed","","lbradstreet","2021-04-30T14:23:37Z","2021-05-17T13:34:33Z"
"","11098","KAFKA-13099; Transactional expiration should account for max batch size","When expiring transactionalIds, we group the tombstones together into batches. Currently there is no limit on the size of these batches, which can lead to `MESSAGE_TOO_LARGE` errors when a bunch of transactionalIds need to be expired at the same time. This patch fixes the problem by ensuring that the batch size respects the configured limit. Any transactionalIds which are eligible for expiration and cannot be fit into the batch are postponed until the next periodic check.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-07-20T23:47:55Z","2021-08-11T13:13:15Z"
"","11266","KAFKA-13233 Log zkVersion in more places","When debugging issues with partition state, it's very useful to know the zkVersion that was written. This patch adds the zkVersion of LeaderAndIsr in a few more places.","closed","","mumrah","2021-08-26T17:11:38Z","2021-08-27T18:35:38Z"
"","11048","KAFKA-13083: In KRaft mode, fix issue with ISR in manual partition assignments","When creating a new topic that includes manually assigned partitions, we must check if the nodes are unfenced before adding them to the new ISR.  This PR adds two unit tests of this behavior in ReplicationControlManagerTest, and fixes a test in QuorumControllerTest that needed to unfenced nodes. Also some test edits to reduce verbosity and repetitiveness.","closed","","cmccabe","2021-07-14T07:09:42Z","2021-07-14T17:52:31Z"
"","11365","HOTFIX: fix checkstyle after cherrypick","When cherrypicking #11337 back to the 3.0 branch and resolving a minor merge conflict, I forgot to `git add` the removal of some imports that were breaking checkstyle","closed","","ableegoldman","2021-09-29T01:18:22Z","2021-09-29T01:24:02Z"
"","11366","HOTFIX: add import to fix checkstyle","When cherrypicking #11218 back to the 2.8 branch and resolving a minor merge conflict, I forgot to `git add` after adding a missing import that was breaking checkstyle","closed","","ableegoldman","2021-09-29T01:28:57Z","2021-09-29T01:36:05Z"
"","11013","KAFKA-13056; Do not rely on broker for snapshots if controller is co-resident","When a node is serving as both broker and controller, we should only rely on the controller to write new snapshots.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-07-09T21:47:40Z","2021-08-11T13:13:15Z"
"","10921","KAFKA-13096: Ensure queryable store providers is up to date after adding stream thread","When a new thread is added the queryable store providers continues to use the store providers it was given when KafkaStreams was instantiated.  I wanted to keep QueryableStoreProviders immutable, so this meant I had to make the queryableStoreProvider field in KafkaStreams class mutable to allow this change.  This is tested via an integration test where, after adding a thread, producing messages with different keys shows that, with the previous code, the keys are not in the store and after the change they are queryable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","PhilHardwick","2021-06-23T08:47:06Z","2021-07-21T20:36:56Z"
"","10863","KAFKA-12890; Consumer group stuck in `CompletingRebalance`","When a group transitions to the `CompletingRebalance` state, the group coordinator sets up `DelayedHeartbeat` for each member of the group. It does so to ensure that the member sends a sync request within the session timeout. If it does not, the group coordinator rebalances the group. Note that here, `DelayedHeartbeat` is used here for this purpose. `DelayedHeartbeat` are also completed when member heartbeats.  The issue is that https://github.com/apache/kafka/pull/8834 has changed the heartbeat logic to allow members to heartbeat while the group is in the `CompletingRebalance` state. This was not allowed before. Now, if a member starts to heartbeat while the group is in the `CompletingRebalance`, the heartbeat request will basically complete the pending `DelayedHeartbeat` that was setup previously for catching not receiving the sync request. Therefore, if the sync request never comes, the group coordinator does not notice anymore.  This patch introduced a new delayed operation which effectively ensure that a SyncGroup request is received from any stable members of the groups within the rebalance timeout. The timer starts when the group transitions to the `CompletingRebalance` state.  Note that I think that this is a bit more strict that we used to have prior to https://github.com/apache/kafka/pull/8834 as we were previously relying on the session timeout to expire in this case. Thougths?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-06-10T13:41:38Z","2021-06-17T11:46:23Z"
"","11230","KAFKA-12840; Removing `compact` cleaning on a topic should abort on-going compactions","When `compact` is removed from the `cleanup.policy` of a topic, the compactions of that topic should be aborted.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-08-18T15:01:53Z","2021-08-24T07:57:17Z"
"","11190","MINOR: Increase smoke test production time","We've seen a few failures recently due to the driver finishing the production of data and verifying the results before the whole cluster is even running.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-08-09T15:47:00Z","2021-08-09T19:19:38Z"
"","10485","MINOR: Enable scala/java joint compilation consistently for `core` module","We were doing it only for test files previously.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-06T04:47:08Z","2021-04-06T19:10:58Z"
"","10665","KAFKA-9009: increase replica.lag.time.max.ms to make the test reliable","We used to set a low `replica.lag.time.max.ms` value (2 sec) to speed up the test, but the 2 sec is not long enough in slow Jenkins env, and caused the follower got kicked out from ISR, so the `UnderReplicatedPartitions` count will be added. Increase the `replica.lag.time.max.ms` value to make this test more reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-05-11T03:35:41Z","2021-06-10T09:39:05Z"
"","10574","KAFKA-12700: override toString method to show correct value in doc","We use customized validator here, but forgot to override the `toString` method to show correct value in doc.  before: `Valid Values:  org.apache.kafka.connect.runtime.WorkerConfig$AdminListenersValidator@383534aa` after: `Valid Values:  List of comma-separated URIs, ex: http://localhost:8080,https://localhost:8443.`  ![image](https://user-images.githubusercontent.com/43372967/115515020-48430a00-a2b7-11eb-947e-02b15473427e.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-21T07:30:07Z","2021-04-22T03:41:10Z"
"","11404","MINOR: update Kafka Streams standby task config","We see many real-time deployments that benefit highly from standby tasks. We should bump it's priority to `HIGH` and add as recommended configuration.  Call for review @ableegoldman @astubbs","closed","docs,","mjsax","2021-10-15T18:17:19Z","2021-11-17T01:36:17Z"
"","10584","KAFKA-12701: NPE in MetadataRequest when using topic IDs","We prevent handling MetadataRequests where the topic name is null (to prevent NPE) as well as prevent requests that set topic IDs since this functionality has not yet been implemented. When we do implement in  in https://github.com/apache/kafka/pull/9769, we should bump the request/response version.  Should also cherry-pick these changes to 2.8 for the next release.   Added tests to ensure the error is thrown.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-04-22T02:54:31Z","2021-06-15T16:56:07Z"
"","11362","KAFKA-13319: Do not commit empty offsets on producer","We observed on the broker side that txn-offset-commit request with empty topics are received. After checking the source code I found there's on place on Streams which is unnecessarily sending empty offsets. This PR cleans up the streams layer logic a bit to not send empty offsets, and at the same time also guard against empty offsets at the producer layer as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-09-27T20:37:38Z","2021-10-12T20:33:24Z"
"","10522","MINOR: Remove `setupGradle()` from Jenkinsfile","We no longer need this since:  1. The PR and branch jobs are configured to `clean before checkout`. 2. The Gradle build outputs the gradle version on start-up.  The description of `clean before checkout` is:  > Clean up the workspace before every checkout by deleting all untracked files and directories, including those which are specified in .gitignore. It also resets all tracked files to their versioned state. This ensures that the workspace is in the same state as if you cloned and checked out in a brand-new empty directory, and ensures that your build is not affected by the files generated by the previous build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-11T03:46:23Z","2021-04-12T05:49:36Z"
"","10660","MINOR: Updating files with release 2.7.1","We need to wait for the artifact to show up in Maven before merging  Also I don't have permissions to push the artifacts to our S3 bucket. Can someone do it for me? Thanks  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-05-10T10:48:11Z","2021-05-20T09:43:18Z"
"","11236","MINOR: Set session timeout back to 10s for Streams system tests","We increased the default session timeout to 30s in KIP-735: https://cwiki.apache.org/confluence/display/KAFKA/KIP-735%3A+Increase+default+consumer+session+timeout  Since then, we are observing sporadic system test failures due to rebalances taking longer than the test timeout. Rather than increase the test wait times, we can  just override the session timeout to a value more appropriate in the testing domain.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-08-19T17:06:23Z","2021-08-20T16:28:03Z"
"","10571","HOTFIX: remove unimplemented SPILL_TO_DISK buffer full strategy","We haven't yet found the time to implement spill-to-disk for the suppression buffer, so we should remove the  `SPILL_TO_DISK` enum to avoid confusion. Technically this enum is in an internal package, and a user would have to be invoking the `StrictBufferConfigImpl` constructor (which is also internal) to have reason to access this enum at all. But users don't always know/notice/care that APIs are internal, or they may just be browsing the source code, so we may as well remove this source of confusion. It's always kind of confusing to see this as a dev.  At the moment if a user does choose this strategy, an UnsupportedOperationException claiming that this is a bug will be thrown when the buffer is full, so it's not like anything too bad would happen...it's just awkward.","closed","","ableegoldman","2021-04-20T23:20:22Z","2021-04-21T02:14:26Z"
"","11293","MINOR: defineInternal for KIP-405 configs","We haven't finished implementing KIP-405, therefore we should make KIP-405 configs as defineInternal.  ~~We may also want to port this change to 3.0 to avoid leaking these configs to the doc.~~  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-09-02T21:35:53Z","2021-09-20T17:53:20Z"
"","11344","KAFKA-13312; 'NetworkDegradeTest#test_rate' should wait until iperf server is listening","We have seen multiple failures with the following logs:  ``` [DEBUG - 2021-09-17 09:57:58,603 - remoteaccount - _log - lineno:160]: ubuntu@worker26: Running ssh command: iperf -i 1 -t 20 -f k -c worker25 [INFO - 2021-09-17 09:57:58,611 - network_degrade_test - test_rate - lineno:114]: iperf output connect failed: Connection refused ```  The iperf client can not connect to the iperf server. The test launches the server and then immediately launches the client but there is not guarantee that the server listens when the client is started.  It seems that we should add a condition to wait until the server prints `Server listening` before starting the client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-20T11:32:03Z","2021-09-21T08:26:49Z"
"","10880","KAFKA-12870; Flush in progress not cleared after transaction completion","We had been using `RecordAccumulator.beginFlush` in order to force the `RecordAccumulator` to flush pending batches when a transaction was being completed. Internally, `RecordAccumulator` has a simple counter for the number of flushes in progress. The count gets incremented in `beginFlush` and it is expected to be decremented by `awaitFlushCompletion`. The second call to decrement the counter never happened in the transactional path, so the counter could get stuck at a positive value, which means that the linger time would effectively be ignored.  The patch here fixes the problem by removing the use of `beginFlush` in `Sender`. Instead, we now add an additional condition in `RecordAccumulator` to explicitly check when a transaction is being completed.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-06-14T17:05:09Z","2021-06-18T22:50:49Z"
"","11372","[DO NOT MERGE] MINOR: Do not send data on abortable error too","We forbid sending produce request before adding partitions in the txn in the following way:  * In `Sender#runOnce`, we would not continue to `sendProducerData` as long as `maybeSendAndPollTransactionalRequest()` returns true. * Inside `maybeSendAndPollTransactionalRequest` as long as `!newPartitionsInTransaction.isEmpty()` we would enqueue an `AddPartitionsToTxn` request to the queue, so in normal cases `maybeSendAndPollTransactionalRequest` would return `true` which would forbid us to send produce request in that iteration. * One thing that caught my eyes however, is that inside `transactionManager.nextRequest(accumulator.hasIncomplete())` where we call `maybeTerminateRequestWithError(nextRequestHandler)`, if the state is already `hasAbortableError()` we would fail the request immediately and hence the caller would return `null`, in which case `maybeSendAndPollTransactionalRequest` would return false. * At the `Sender#runOnce()`, we only check `if (transactionManager.hasFatalError())` and if yes we would not proceed to send produce requests.  So I think there's a possible trace where we would send produce request before sending addPartitionsToTxn requests:  1) First of all, the state of the transaction is already in `abortable error`. 2) In `Sender#runOnce`, we would first pass `transactionManager.hasFatalError()` since we are not in `fatal error` 3) And then in `maybeSendAndPollTransactionalRequest`, although we would generate an `addPartitionsToTxn` request, that request would fail immediately due to the `abortable error` state. 4) And then we would pass `maybeSendAndPollTransactionalRequest` and continue to `sendProducerData` assuming the metadata for the destination brokers are available.  In this case, a producer request could be sent out while the addPartitionsToTxn request is failed. The effect of this happening is that the txn would ""hang forever"" since the txn marker may not be written to it ever while the txn is eventually aborted, but with the txn data appended to the partitions, it means LSO would not advance and all consumers / log compactors etc would be blocked.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2021-09-30T17:50:29Z","2021-10-08T00:11:51Z"
"","10631","MINOR: Stop using hamcrest in system tests","We currently use `hamcrest` imports to check the outputs of the `RelationalSmokeTest`, but with the new gradle updates, the proper hamcrest imports are no longer included in the test jar.  This is a bit of a workaround to remove the hamcrest usage so we can get system tests up and running again. Potential follow-up could be to update the way we create the test-jar to pull in the proper dependencies  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lct45","2021-05-04T23:21:06Z","2021-05-10T16:56:27Z"
"","11161","MINOR: Remove node from API versions cache on NetworkClient.close(nodeId)","We clear the two collections that track API version requests when server disconnects, but not for local close. The PR clears them for local close as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rajinisivaram","2021-08-02T09:14:32Z","2022-02-05T05:47:25Z"
"","10861","KAFKA-12909: disable spurious left/outer stream-stream join fix for old JoinWindows API","We changed the behavior of left/outer stream-stream join via KAFKA-10847. To avoid a breaking change during an upgrade, we need to disable this fix by default.  We only enable the fix if users opt-in expliclity by changing their code. We leverage KIP-633 (KAFKA-8613) that offers a new JoinWindows API with mandatory grace-period to enable the fix.  Call for review @guozhangwang @spena @ableegoldman @izzyacademy","closed","streams,","mjsax","2021-06-10T09:49:44Z","2021-06-16T16:31:33Z"
"","10900","KAFKA-12967; KRaft broker should forward DescribeQuorum to controller","We added the DescribeQuorum API in KIP-595. This patch adds the logic to forward DescribeQuorum requests to the controller when KRaft is enabled.  Note that the KRaft broker listener has already been enabled in DescribeQuorumRequest.json. The zk broker is not enabled, however, so DescribeQuorum requests will not be advertised and will be rejected at the network layer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-06-17T21:39:58Z","2021-08-11T13:14:03Z"
"","10879","KAFKA-12991; Fix unsafe access to `AbstractCoordinator.state`","We access the `state` from the `HeartbeatResponseHandler#handle` without synchronizing it. This seems unsafe to me as the `state` is not `volatile`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-06-14T16:31:59Z","2021-06-24T18:52:58Z"
"","11477","MINOR: Adding a constant to denote UNKNOWN leader in LeaderAndEpoch","Verified that the KafkaRaftClientTest passes with the change.","closed","","niket-goel","2021-11-08T21:08:21Z","2021-11-09T17:07:06Z"
"","10531","KAFKA-12658: Include kafka-shell jar and dependencies in release tar","Verified that `./bin/kafka-metadata-shell.sh --help` on the release tarball works as expected. It failed with a `ClassNotFoundException` before this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-13T04:37:12Z","2021-04-13T13:18:03Z"
"","10769","MINOR: fix code listings connect.html","Uses the right tool to show code listings, so it is consistent with the rest of the examples   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-05-26T13:12:08Z","2021-05-27T07:01:23Z"
"","10767","MINOR: Fix code listings in quickstart.html","Uses the right character for shell scripts (`>` instead of `$`) Uses the right syntax highlighting  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-05-26T13:04:47Z","2021-05-27T06:52:59Z"
"","10506","MINOR: Improve description of `max.poll.records` config","Users often think that changing `max.poll.records` would impact network traffic with regard to fetching behavior. Seems worth to clarify that it does not.  Call for review: @guozhangwang @hachikuji","closed","consumer,","mjsax","2021-04-08T21:06:51Z","2021-04-12T22:16:50Z"
"","11212","KAFKA-13200: Fix MirrorMaker2 connector version","Use the Kafka version instead of hardcoding it to 1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-08-13T15:30:33Z","2021-11-29T15:37:57Z"
"","10927","MINOR: add MockConfigRepository","Use MockConfigRepository rather than CachedConfigRepository in unit tests. This is useful for an upcoming change that will remove CachedConfigRepository.","closed","kip-500,","cmccabe","2021-06-24T21:41:38Z","2021-06-25T23:40:45Z"
"","10898","MINOR: Use MessageDigest equals when comparing signature","Use MessageDigest.isEquals for comparing the signature in Connect","closed","","rhauch","2021-06-17T19:46:39Z","2021-06-18T14:53:24Z"
"","11176","KAFKA-12935: use relative counts for shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore","Use a relative count from using 0  for `totalNumbRestores` to prevent flakiness.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-08-04T19:32:47Z","2021-08-05T14:47:42Z"
"","11479","KAFKA-12648: Make changing the named topologies have a blocking option","Use a Kafka future to be able to add, remove then add back the same topology  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-11-09T23:33:18Z","2021-12-09T21:57:54Z"
"","10754","KAFKA-12835: Topic IDs can mismatch on brokers (after interbroker protocol version update)","Upon upgrading to IBP 2.8, topic ID can end up getting reassigned which can cause errors in LeaderAndIsr handling when the partition metadata files from the previous ID are still on the broker.   Topic IDs are stored in the TopicZNode. The behavior of the code before this fix is as follows: Consider we had a controller with IBP 2.8+. Each topic will be assigned topic IDs and LeaderAndIsr requests will write partition.metadata files to the brokers. If we re-elect the controller and end up with a controller with an older IBP version and we reassign partitions, the TopicZNode is overwritten and we lose the topic ID. Upon electing a 2.8+ IBP controller, we will see the TopicZNode is missing a topic ID and will generate a new one. If the broker still has the old partition metadata file, we will see an ID mismatch that causes the error.  This PR changes controller logic so that we maintain the topic ID in the controller and the ZNode even when IBP < 2.8. This means that in the scenario above, reassigning partitions will not result in losing the topic ID and reassignment.  Topic IDs may be lost when downgrading the code below version 2.8, but upon re-upgrading to code version 2.8+, before bumping the IBP, all partition metadata files will be deleted to prevent any errors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-05-24T21:12:14Z","2021-06-18T07:26:47Z"
"","11224","KAFKA-13209: Upgrade jetty-server to fix CVE-2021-34429","Upgrading to 9.4.43.v20210629 Release notes: https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.43.v20210629  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-08-17T16:32:23Z","2021-08-17T18:26:17Z"
"","11343","MINOR: Add missing upgrade doc for 2.8","Upgrade doc in trunk as per https://github.com/apache/kafka/pull/11318.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-20T08:21:11Z","2021-09-20T08:27:39Z"
"","10918","KAFKA-12756: Update ZooKeeper to v3.6.3","Updates ZooKeeper to v3.6.3 and adds some additional exercising of ZooKeeper in the corresponding upgrade system tests.  The ZooKeeper v3.6.3 dependency on `io.dropwizard.metrics:metrics-core` cannot be avoided as per https://issues.apache.org/jira/browse/ZOOKEEPER-4324.  All integration tests based on `ZooKeeperTestHarness` fail without the dropwizard metrics jar being on the CLASSPATH; system tests fail as well.  This patch adds the library as a dependency.  This patch also adds 2.8.0 to the system test docker and Vagrant images (it was missing) and fixes `KafkaVersionTest.test_multi_version()` due to sending `kafka-topics --zookeeper` to the wrong node in a 2-broker cluster where one broker is v0.8.2 (which can use `kafka-topics --zookeeper`) and the other is the latest version (which no longer can due to https://github.com/apache/kafka/pull/10457).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-06-22T22:14:27Z","2021-07-06T19:35:35Z"
"","10784","KAFKA-12862: Update Scala fmt library and apply fixes","Updates the scala fmt to the latest stable version Applies all the style fixes (all source code changes are done by scala fmt) Removes setting about dangling parentheses as `true` is already the default  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-28T16:27:06Z","2021-08-09T10:13:12Z"
"","11451","KAFKA-13419: Only reset generation ID when ILLEGAL_GENERATION error","Updated: This PR will reset generation ID when `ILLEGAL_GENERATION` error since the member ID is still valid.  ===== `resetStateAndRejoin` when `REBALANCE_IN_PROGRESS` error in sync group, to avoid out-of-date `ownedPartition`  == JIRA description == In KAFKA-13406, we found there's user got stuck when in rebalancing with cooperative sticky assignor. The reason is the ""ownedPartition"" is out-of-date, and it failed the cooperative assignment validation.  Investigate deeper, I found the root cause is we didn't reset generation and state after sync group fail. In KAFKA-12983, we fixed the issue that the onJoinPrepare is not called in resetStateAndRejoin method. And it causes the ownedPartition not get cleared. But there's another case that the ownedPartition will be out-of-date. Here's the example:    1. consumer A joined and synced group successfully with generation 1   2. New rebalance started with generation 2, consumer A joined successfully, but somehow, consumer A doesn't send out sync group immediately   3.  other consumer completed sync group successfully in generation 2, except consumer A.   4.  After consumer A send out sync group, the new rebalance start, with generation 3. So consumer A got REBALANCE_IN_PROGRESS error with sync group response   5.  When receiving REBALANCE_IN_PROGRESS, we re-join the group, with generation 3, with the assignment (ownedPartition) in generation 1.   6.  So, now, we have out-of-date ownedPartition sent, with unexpected results happened     We might want to do resetStateAndRejoin when RebalanceInProgressException errors happend in sync group. Because when we got sync group error, it means, join group passed, and other consumers (and the leader) might already completed this round of rebalance. The assignment distribution this consumer have is already out-of-date.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-10-29T12:36:27Z","2022-05-09T14:12:12Z"
"","10736","KAFKA-9295: revert session timeout to default value","Updated: Since https://github.com/apache/kafka/pull/10803 is opened to increase default session timeout to 45 seconds, I reverted my previous change to use default session timeout setting. Thanks.  =============== Increase session timeout to 30 secs to make the test reliable.  I had a PR, to duplicate multiple `KTableKTableForeignKeyInnerJoinMultiIntegrationTest` tests and duplicate another `KTableKTableForeignKeyInnerJoinMultiIntegrationTest` tests to increase the session timeout to 30 seconds as comparison. After 3 runs, I found all 30 seconds tests passed, but the original 20 seconds session timeout failed sometimes. That's why this PR to increase session timeout to 30 seconds is created.  But, there's also 1 more thing I noticed. Not sure if it's a bug or not.   We have this declared: ``` table1.join(table2, tableOneKeyExtractor, joiner, materialized)             .join(table3, joinedTableKeyExtractor, joinerTwo, materializedTwo)             .toStream()             .to(OUTPUT,                 Produced.with(serdeScope.decorateSerde(Serdes.Integer(), streamsConfig, true),                               serdeScope.decorateSerde(Serdes.String(), streamsConfig, false))); ```  So, I think (and checked the code), we should run `tableOneKeyExtractor` first, then `joiner`, and then `joinedTableKeyExtractor` if we needed, then `joinerTwo`. But in the failed case:  ``` Did not receive all 1 records from topic output- within 60000 ms ``` I found we got all 4 data and entered `tableOneKeyExtractor` ```  tableOneKeyExtractor:-1 tableOneKeyExtractor:2 tableOneKeyExtractor:-2 tableOneKeyExtractor:1  joiner: value1 2.22,20 joinedTableKeyExtractor:value1=2.22,value2=20 ```  The expected result (run from my local env) ``` tableOneKeyExtractor:1 tableOneKeyExtractor:2 tableOneKeyExtractor:-1 tableOneKeyExtractor:-2 joiner: value1 1.33,10 joiner: value1 2.22,20 joinedTableKeyExtractor:value1=1.33,value2=10 joinedTableKeyExtractor:value1=2.22,value2=20 joiner2: value1 value1=1.33,value2=10,waffle ``` You can notice the `joiner: value1 1.33,10` doesn't get run in the failed case, and that's why the following `joinedTableKeyExtractor` and `joiner2` not run. Do you have any thought about that? I'm thinking it might because the 3 streams are still under rebalancing, so the streaming processing might get terminated if the stream drop suddenly. And that's why the data lost. Do you think it's possible? Any thought about it?  You can check this jenkins log [here](https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-10773/2/testReport/junit/org.apache.kafka.streams.integration/KTableKTableForeignKeyInnerJoinMultiIntegration132Test/Build___JDK_15_and_Scala_2_13___shouldInnerJoinMultiPartitionQueryable/) (search ""standard error""). Some keywords are:  `starting` is starting 3 streams `all running` is that 3 streams reach running state `tableOneKeyExtractor`, `joinedTableKeyExtractor`, `joiner2` should be clear `value1` is `joiner`   Thank you very much.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-05-20T07:47:54Z","2021-06-10T20:57:25Z"
"","11438","KAFKA-13403 Fix KafkaServer crashes when deleting topics due to the race in log deletion","Update the walkFileTree override implementation to handle parallel file deletion. So as to prevent crashing of Kafka broker process itself when logs are deleted by other threads due to retention expiry.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","arunmathew88","2021-10-27T02:47:44Z","2022-04-26T07:10:33Z"
"","11206","MINOR: Update streams doc to close KeyValueIterator in example code","Update the streams doc: 1. close `KeyValueIterator` in example code (add a comment to remind user: `Note: the KeyValueIterator instance should be closed explicitly to avoid resource leakage`)   2. Fix the example code indent    3. Add missing comma between key/value     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-08-12T13:03:49Z","2021-09-27T07:20:10Z"
"","11059","KAFKA-12930,KAFKA-12929: Deprecate Java 8 and Scala 2.12","Update the readme to note the deprecation. We will also mention the deprecation in the downloads page when the release is done.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-15T14:52:23Z","2021-07-15T22:30:33Z"
"","11003","KAFKA-12360: Document new time semantics","Update the docs for task idling, since the semantics have changed in 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vvcephei","2021-07-08T20:28:18Z","2021-07-12T21:18:03Z"
"","11009","MINOR: update doc for default assignor change","Update the doc and upgrade doc for default assignor change.  REF: https://github.com/apache/kafka/pull/10903  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-09T13:11:08Z","2021-07-13T20:08:47Z"
"","10722","MINOR: update java doc for deprecated methods","Update deprecated methods:  1. `KStream#through(String, Produced)` 2. `KafkaConsumer#poll(long)`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","showuon","2021-05-19T03:27:37Z","2021-05-24T01:33:14Z"
"","11179","KAFKA-13165: Validate node id, process role and quorum voters","Under certain configuration is possible for the Kafka Server to boot up as a broker only but be the cluster metadata quorum leader. We should validate the configuration to avoid this case.  https://issues.apache.org/jira/browse/KAFKA-13165  Tested manually by starting up a broker and a controller both with valid/invalid configurations","closed","","dielhennr","2021-08-05T03:06:26Z","2021-08-19T05:27:37Z"
"","10503","KAFKA-9988: Suppress uncaught exceptions in log messages during Connect task shutdown","Uncaught exceptions logged during task stop were misleading because the task is already on its way of being shutdown.  The suppression of exception causes a change in behavior as the caller method now calls `statusListener.onShutdown` instead of `statusListener.onFailure` which is the right behavior. A new test was added to test the right behavior for uncaught exception during shutdown and existing test was modified to test uncaught exception during normal execution  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kpatelatwork","2021-04-08T01:18:55Z","2021-04-12T15:06:48Z"
"","11458","MINOR: Bump trunk to 3.2.0-SNAPSHOT","Typical version bumps on trunk following the creation of the 3.1 release branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-11-02T09:02:27Z","2021-11-02T12:38:57Z"
"","10981","Bump trunk to 3.1.0-SNAPSHOT","Typical version bumps on `trunk` following the creation of the `3.0` release branch.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2021-07-06T19:05:15Z","2021-07-06T21:28:29Z"
"","11004","KAFKA-12257: Consumer mishandles topics deleted and recreated with the same name (trunk)","Trunk version of https://github.com/apache/kafka/pull/10952  This PR slightly cleans up some of the changes made in https://github.com/apache/kafka/pull/9944  Store topic ID info in consumer metadata. We will always take the topic ID from the latest metadata response and remove any topic IDs from the cache if the metadata response did not return a topic ID for the topic.  With the addition of topic IDs, when we encounter a new topic ID (recreated topic) we can choose to get the topic's metadata even if the epoch is lower than the deleted topic.  The idea is that when we update from no topic IDs to using topic IDs, we will not count the topic as new (It could be the same topic but with a new ID). We will only take the update if the topic ID changed.  Added tests for this scenario as well as some tests for storing the topic IDs. Also added tests for topic IDs in metadata cache.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-08T21:37:32Z","2021-11-17T17:57:17Z"
"","10915","KAFKA-13041: Enable connecting VS Code remote debugger","To kafka folks - Would it be possible to also merge it to 2.7? Thanks!  The changes in this PR enable connecting VS Code's remote debugger to a system test running locally with ducker-ak. Changes include: - added zip_safe=False to setup.py - this enables installing kafkatest module together with source code when running `python setup.py  develop/install`. - install [debugpy](https://github.com/microsoft/debugpy) on ducker nodes - expose 5678 (default debugpy port) on ducker01 node - ducker01 is the one that actually executes tests, so that's where you'd connect to. - added `-d|--debug` option to `ducker-ak test` command - if used, tests will run via `python3.7 -m debugpy` command, which would listen on 5678 and pause until debugger is connected. - changed the logic of the `ducker-ak test` command so that ducktape args are collected separately after `--` - otherwise any argument we add to the `test` command in the future might potentially shadow a similar ducktape argument.  	- we don't really check that `ducktape_args` are args while `test_name_args` are actual test names, so the difference between the two is minimal actually - most importantly we do check that `test_name_args` is not empty, but we are ok if `ducktape_args` is.  Tested: wrote a small test extending `KafkaTest` and verified it is executed with and without debug flag (to be absolutely fair, the test failed in both cases, but I was looking at whether the debugger connects correctly or not - it does). Ran it with `-- --debug` too to make sure ducktape args are passed correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stan-confluent","2021-06-22T02:17:17Z","2021-07-08T15:05:15Z"
"","10570","MINOR: Bump to latest version 2.6.2","To be merged once the release artifacts for 2.6.2 show up in Maven central","closed","","ableegoldman","2021-04-20T20:34:38Z","2021-04-21T18:34:50Z"
"","11340","KAFKA-13310 : KafkaConsumer cannot jump out of the poll method, and the…","Title: KafkaConsumer cannot jump out of the poll method, and cpu and traffic on the broker side increase sharply description:  The local test has been passed, the problem described by jira can be solved  JIRA link : https://issues.apache.org/jira/browse/KAFKA-13310  Author: RivenSun2   Reviewers: Luke Chen","closed","","RivenSun2","2021-09-19T10:53:10Z","2022-03-20T00:57:36Z"
"","10780","KAFKA-12782: Fix Javadocs generation by upgrading JDK","This, upgrades JDK to version 15 for the docs generation, this way we can circumvent bug https://bugs.openjdk.java.net/browse/JDK-8215291 present in JDK11  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-27T17:45:46Z","2021-05-28T19:01:53Z"
"","10845","KAFKA-12916: Add new AUTO_CREATE ACL for auto topic creation","This will authorizing a user to auto create a topic with cluster defaults but prevent manual creation with overriden settings. The change is backwards compatible as being granted CREATE also implies AUTO_CREATE.  Ran through tests and things seem ok. Updated AclAuthorizerTest to test new Acl Inheritance. Some more tests may need to be added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","cshannon","2021-06-08T18:07:55Z","2021-06-08T19:55:21Z"
"","11103","HOTFIX: Set session interval back to 10s for StreamsCooperativeRebalanceUpgradeTest","This test is hitting pretty frequent timeouts after bouncing a node and waiting for it to come back and fully rejoin the group. It seems to now take 45s for the initial JoinGroup to succeed, which I suspect is due to the new default session.interval.ms (which was recently changed to 45s). Let's try fixing this config to the old value of 10s and see if that helps it rejoin in time","closed","","ableegoldman","2021-07-21T22:28:23Z","2021-07-22T17:28:50Z"
"","11113","KAFKA-13128: wait for all keys to be fully processed in #shouldQueryStoresAfterAddingAndRemovingStreamThread","This test is flaky due to waiting on all records to be processed for only a single key before issuing IQ lookups and asserting whether data was found. See [this comment](https://issues.apache.org/jira/browse/KAFKA-13128?focusedCommentId=17385841&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17385841) for full analysis on how this happened  Should be cherrypicked to 3.0 (test fix to help stabilize the build @kkonstantine ) and to 2.8","closed","","ableegoldman","2021-07-23T02:00:26Z","2021-07-23T22:55:58Z"
"","11097","KAFKA-8529: Flakey test ConsumerBounceTest#testCloseDuringRebalance","This test became more flaky due to 2b8aff5 One of the changes of adding topic IDs to fetch requests involved returning a top-level error when topic IDs were not yet found on the receiving broker. In this case, I thought it would be good to delay partitions so that brokers can receive the metadata updates they need. However, it seems like this approach actually introduces a double delay that slows things down.   In general, the top-level error approach is being reconsidered here: https://issues.apache.org/jira/browse/KAFKA-13111 In the meantime, to fix the flakiness, we can remove this extra delay.  There are a few other issues related to the topic ID Fetch commit and I will follow up on those errors as well. Tickets are https://issues.apache.org/jira/browse/KAFKA-13079 and  https://issues.apache.org/jira/browse/KAFKA-13102  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-20T23:31:19Z","2021-07-23T16:30:56Z"
"","11284","KAFKA-13202: KIP-768: Extend SASL/OAUTHBEARER with Support for OIDC","This task is to provide a concrete implementation of the interfaces defined in KIP-255 to allow Kafka to connect to an OAuth/OIDC identity provider for authentication and token retrieval. While KIP-255 provides an unsecured JWT example for development, this will fill in the gap and provide a production-grade implementation.  The OAuth/OIDC work will allow out-of-the-box configuration by any Apache Kafka users to connect to an external identity provider service (e.g. Okta, Auth0, Azure, etc.). The code will implement the standard OAuth clientcredentials grant type.  The proposed change is largely composed of a pair of AuthenticateCallbackHandler implementations: one to login on the client and one to validate on the broker.  See the following for more detail:  * [KIP-768](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575) * [KAFKA-13202](https://issues.apache.org/jira/browse/KAFKA-13202)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-08-30T18:50:00Z","2021-10-28T18:36:53Z"
"","10568","KAFKA-8897: Upgrade RocksDB to 6.19.3","This PR upgrades RocksDB to 6.19.3. After the upgrade the Gradle build exited with code 134 due to SIGABRT signals (""Pure virtual function called!"") coming from the C++ part of RocksDB. This error was caused by RocksDB state stores not properly closed in Streams' code. This PR adds the missing closings and updates the RocksDB option adapter.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-04-20T09:08:46Z","2021-08-04T01:05:52Z"
"","10482","KAFKA-12499: add transaction timeout verification","This PR tries to add the check for transaction timeout for a comparison against commit interval of streams. If transaction timeout is smaller than commit interval, stream should crash and inform user to update their commit interval to match the given transaction timeout, or vise versa.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2021-04-06T00:16:01Z","2021-05-21T22:05:40Z"
"","11432","KAFKA-13399 towards scala3","This PR takes all changes in https://github.com/apache/kafka/pull/11350 that are needed for a Scala 3 compatibility except those with an open bug report or already solved one.  The PR is separated in 5 commits, one for each type of change performed: - Avoid Shadowing: Scala 3 compiler is not so friendly with shadowing and reports errors where Scala 2 wasn't. - Not rely on automatic widening in numeric types: Scala 3 doesn't automatically convert `Short` to `Int` or `Int` to `Long` at will as in Scala 2. Changes in here help the typer by manually forcing the conversions. - Remove redundant parenthesis: Scala 2 was more lax about calling with empty parenthesis a method without parenthesis. This became stricter in Scala 3. - Explicit type declaration: Scala 3 changed a bit in the area of type inference and in some cases it picks the most general type, causing in our case some trouble as this is not public but package protected. In other cases, the typer wasn't able to infer the proper one, forcing it to be manually set. - Workaround for SAM conversion with overloading: This is a reported bug that unfortunately can't be fixed easily without breakage on Scala's side. For further information check lampepfl/dotty#13549  The resulting code is still Scala 2 valid code and arguably more correct.  Instead of doing all the changes in an enormous PR with all the changes at once, we can already perform the changes we know for a fact that are going to be needed for Scala 3.  If desired I can provide 1 PR per commit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-10-25T15:40:02Z","2022-05-26T20:41:08Z"
"","11102","MINOR: Rename the @metadata topic to __cluster_metadata","This PR renames the internal metadata topic to ""__cluster_metadata"". The topic was previously known as ""@metadata""","closed","","niket-goel","2021-07-21T22:03:52Z","2021-07-22T00:30:35Z"
"","11135","KAFKA-13143 Remove HandleMetadata from ControllerAPis as metadata is not completely implemented on KRaft controllers","This PR removes the `METADATA` API from the Kraft controller as the controller does not yet implement the metadata fetch functionality completely.  Without the change (as per the JIRA https://issues.apache.org/jira/browse/KAFKA-13143), the API would return an empty list of topics making the caller incorrectly think that there were no topics in the cluster which could be confusing. After this change the describe and list topic APIs timeout on the controller endpoint when using the `kafka-topics` CLI (which is the same behavior as create_topic).  Post 3.0 we want to follow up and align the controller's APIs with the bigger picture on how we want users to interact with it. The JIRA https://issues.apache.org/jira/browse/KAFKA-13146 describes the follow up needed.","closed","","niket-goel","2021-07-27T21:31:50Z","2021-07-29T16:23:48Z"
"","10960","KAFKA-12981 Ensure LogSegment.maxTimestampSoFar and LogSegment.offsetOfMaxTimestampSoFar are read/updated in sync","This PR refactors LogSegment.offsetOfMaxTimestampSoFar and LogSegment.maxTimestampSoFar to single tuple to ensure consistent update/read  This ties in with: https://cwiki.apache.org/confluence/display/KAFKA/KIP-734%3A+Improve+AdminClient.listOffsets+to+return+timestamp+and+offset+for+the+record+with+the+largest+timestamp  as we can now use the adminClient to fetch timestamps/offsets with the largest timestamp in a partition. This refactor avoids a possible race condition where one of timestamp/offset us updated before the other is read (e.g. the dminclinet fetches the highest timestamp and then the offset changes before it is read)  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thomaskwscott","2021-07-02T12:23:15Z","2021-07-06T17:28:09Z"
"","10959","KAFKA-12981 Ensure LogSegment.maxTimestampSoFar and LogSegment.offsetOfMaxTimestampSoFar are read/updated in sync","This PR refactors LogSegment.offsetOfMaxTimestampSoFar and LogSegment.maxTimestampSoFar to single tuple to ensure consistent update/read  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thomaskwscott","2021-07-02T12:08:28Z","2021-07-02T12:22:18Z"
"","10691","KAFKA-12779: KIP-740, Introduce public TaskId interface and TaskMetadata.id() API that returns it","This PR moves the existing `TaskId` class to the `internals` package, and has it implement a new `TaskId` interface which just exposes the metadata fields. The interface will be part of the public API, and is returned by the new `TaskMetadata.Id()` API. The old `TaskMetadata.taskId()` method that returns a String is also deprecated in this PR  The KIP is currently under voting so this PR can't be merged yet, but it is ready to be reviewed: [KIP-740](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181306557)","closed","streams,","ableegoldman","2021-05-14T01:53:32Z","2021-05-20T02:54:08Z"
"","11008","KAFKA-10588 Rename kafka-console-consumer CLI command line arguments for KIP-629","This PR marks `--whitelist` as deprecated argument and introduce `--include` for `kafka-console-consumer`  Note: There is no test using `--whitelist` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","OmniaGM","2021-07-09T10:04:11Z","2021-07-14T06:28:40Z"
"","11007","KAFKA-10589 Rename kafka-replica-verification CLI command line arguments for KIP-629","This PR marks `--topic-white-list` as deprecated argument and introduce `--topics-include` for `kafka-replica-verification` Note  There's no test or any other usage for `--topic-white-list` in other places in the repo.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","OmniaGM","2021-07-09T09:55:40Z","2021-07-14T06:54:54Z"
"","10839","KAFKA-12913: Make case class's final","This PR makes all of the Scala's `case class`'s final to ensure correctness of Kafka's Scala code that uses these case classes. In Scala its best practice to make `case class`'s final since `case class` automatically generates critical methods such as `hashcode`/`equals`/`unapply` which can break code if user's override these methods by subclassing.  Please see the [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-754%3A+Make+Scala+case+class%27s+final) for more info.  Changes of note have been annotated in the PR.  ### Committer Checklist (excluded from commit message) - [X] Verify design and implementation  - [X] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","mdedetrich","2021-06-08T11:42:36Z","2021-09-01T13:55:07Z"
"","11350","Scala3 migration","This PR lays the groundwork for the Scala3 migration of the code base. Gradle's Scala3 plugin is merged but not released yet (see [PR](https://github.com/gradle/gradle/pull/18001)). During the migration I encountered 2 different bugs in Scala3, they are filed [here](https://github.com/lampepfl/dotty/issues/13549) and [here](https://github.com/lampepfl/dotty/issues/13572).  # Types of changes done in this PR: * Shadowing variables or packages are now errors instead of Scala doing guess work. A fair amount of the changes done in this PR was disambiguation. These include:   * Parameter shadowing   * Shadowing among imports * Parenthesis-less methods called with parameters is not possible any more in Scala3 * `SockerServer` extra classes were split into different files as Scala3 compiler was failing to find them when referenced in other classes in this same file. * `foreach` java collections method was not directly usable under Scala3, so I transformed it to the Scala collection's one (this might be another bug in Scala3, I need to investigate) * Scala3 fails to determine the right overload method when SAM is involved if types don't match exactly (a.k.a. bug https://github.com/lampepfl/dotty/issues/13549) * Scala3 companion object of a trait doesn't have the static forwarder methods, needed to reference the companion object ""old style"" (a.k.a. bug https://github.com/lampepfl/dotty/issues/13572) * Extra manual typing was needed occasionally as Scala3 is probably stricter than Scala 2.13  # Compiling with Scala3 In order to test this locally one can run the following:  `./gradlew wrapper --gradle-distribution-url=https://services.gradle.org/distributions-snapshots/gradle-7.3-20210906222431+0000-bin.zip` And then the usual `./gradlew compileTestScala -PscalaVersion=3.0`  # Notes Jackson is using ""2.13.0-rc2"" version as it's the one that contains Scala3 improvements, it's not really needed to successfully compile though.  Extra information, Scala3 is compiling in ""Migration Mode"", meaning it outputs some warnings about deprecated and dropped features. See [Migration Mode](https://docs.scala-lang.org/scala3/guides/migration/tooling-migration-mode.html) for further info. All these warnings can be automatically fixed by the Scala compiler itself.  # Current Problems Spotbugs is currently detecting 30 problems with Scala3, it works fine when compiling with Scala 2.13. This currently blocks the execution of core and streams tests. By excluding `spotbugs` tests can be run and some tests are still failing, I need to find out why is this. Tests run successfully in Scala 2.13. To exclude `spotbugs` run the following: `./gradlew test -x spotbugsMain -PscalaVersion=3.0`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-09-21T21:09:38Z","2022-04-05T20:01:43Z"
"","10802","KAFKA-6718 / Update SubscriptionInfoData with clientTags","This PR is Part of [KIP-708](https://cwiki.apache.org/confluence/display/KAFKA/KIP-708%3A+Rack+awareness+for+Kafka+Streams), which adds `ClientTags` to `SubscriptionInfoData`.   Splitting PRs into three smaller PRs to make the review process easier to follow. Overall plan is the following:  ⏭️  Rack aware standby task assignment logic. https://github.com/apache/kafka/pull/10851 👉  Protocol change, add `clientTags` to `SubscriptionInfoData` ⏭️  Add required configurations to `StreamsConfig` (public API change, at this point we should have full functionality) https://github.com/apache/kafka/pull/11837  This PR adds `clientTags` to `SubscriptionInfoData` and implements second point of above mentioned plan.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2021-06-01T14:48:21Z","2022-03-11T08:30:17Z"
"","10851","KAFKA-6718 / Rack aware standby task assignor","This PR is part of [KIP-708](https://cwiki.apache.org/confluence/display/KAFKA/KIP-708%3A+Rack+awareness+for+Kafka+Streams) and adds rack aware standby task assignment logic.  Rack aware standby task assignment won't be functional until all parts of this KIP gets merged.  Splitting PRs into three smaller PRs to make the review process easier to follow. Overall plan is the following:  👉  Rack aware standby task assignment logic. ⏭️  Protocol change, add clientTags to SubscriptionInfoData https://github.com/apache/kafka/pull/10802 ⏭️  Add required configurations to StreamsConfig (public API change, at this point we should have full functionality) https://github.com/apache/kafka/pull/11837  This PR implements first point of the above mentioned plan.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","lkokhreidze","2021-06-09T10:02:49Z","2022-03-03T10:28:49Z"
"","10950","Test https://github.com/warrenzhu25/kafka/tree/10619","This PR is only used to test https://github.com/warrenzhu25/kafka/tree/10619","closed","","ctan888","2021-06-30T21:53:40Z","2021-07-01T15:13:07Z"
"","10525","KAFKA-7572: Producer should not send requests with negative partition id","This PR is for [KAFKA-7572](https://issues.apache.org/jira/browse/KAFKA-7572), which fixes the issue that producers will throw confusing exceptions when a custom Partitioner returns a negative partition. Since the PR #5858 is not followed by anyone currently, I reopen this one to continue the work.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","predatorray","2021-04-12T06:23:03Z","2022-02-08T03:12:35Z"
"","10518","KAFKA-7572: Producer should not send requests with negative partition id","This PR is for [KAFKA-7572](https://issues.apache.org/jira/browse/KAFKA-7572), which fixes the issue that producers will throw confusing exceptions when a custom Partitioner returns a negative partition. Since the PR #5858 is not followed by anyone currently, I reopen this one to continue the work.","closed","","predatorray","2021-04-10T12:01:01Z","2021-04-12T06:23:36Z"
"","11424","KAFKA-13152: Replace ""buffered.records.per.partition"" with ""input.buffer.max.bytes""","This PR is an implementation of: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186878390. The following changes have been made:  - Adding a new config input.buffer.max.bytes applicable at a topology level.  - Adding new config statestore.cache.max.bytes.  - Adding new metric called total-bytes . - The per partition config buffered.records.per.partition is deprecated. - The config  cache.max.bytes.buffering is deprecated.","closed","","vamossagar12","2021-10-21T17:25:42Z","2022-02-11T01:52:40Z"
"","11285","KAFKA-10548: Implement topic deletion logic with the LeaderAndIsr in KIP-516","This PR includes the following changes 1. Adding the type field to the LeaderAndIsr request as proposed in KIP-516 2. Letting the controller set the type of LeaderAndIsr requests to be either FULL or INCREMENTAL 3. Allowing topic deletion to complete even with offline brokers 4. Schedule the deletion of replicas with inconsistent topic IDs or not present in the full LeaderAndIsr request  Testing Strategy: This PR added two tests  1. testTopicDeletionWithOfflineBrokers: to ensure that topic deletion can proceed even with offline brokers  2. testDeletionOfStrayPartitions: to ensure stray replicas whose topic has been deleted will be removed upon broker startup  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","gitlw","2021-08-30T23:15:04Z","2021-10-05T12:53:57Z"
"","10864","KAFKA-12155 Metadata log and snapshot cleaning","This PR includes changes to KafkaRaftClient and KafkaMetadataLog to support periodic cleaning of old log segments and snapshots.   Four new public config keys are introduced  * metadata.log.segment.bytes * metadata.log.segment.ms * metadata.max.retention.bytes * metadata.max.retention.ms  These are used to configure the log layer as well as the snapshot cleaning logic. Snapshot and log cleaning is performed based on two criteria: total metadata log + snapshot size (metadata.max.retention.bytes), and max age of a snapshot (metadata.max.retention.ms). Since we have a requirement that the log start offset must always align with a snapshot, we perform the cleaning on snapshots first and then clean what logs we can. The cleaning algorithm follows:  * Delete the oldest snapshot * Advance the log start offset to the new oldest snapshot * Request that the log layer clean any segments prior to the new log start offset * Repeat this until the retention size or time is no longer violated, or only a single snapshot remains  The cleaning process is triggered every 60 seconds from the KafkaRaftClient polling thread.","closed","kip-500,","mumrah","2021-06-10T19:50:06Z","2021-07-06T21:21:23Z"
"","11390","[KAFKA-13369] Follower fetch protocol changes for tiered storage.","This PR implements the follower fetch protocol as mentioned in KIP-405.  Added a new version for `ListOffsets` protocol to receive local log start offset on the leader replica. This is used by follower replicas to find the local log star offset on the leader.   Added a new version for `FetchRequest` protocol to receive OffsetMovedToTieredStorageException error. This is part of the enhanced fetch protocol as described in KIP-405.  We introduced a new field `locaLogStartOffset` to maintain the log start offset in the local logs. Existing logStartOffset will continue to be the log start offset of the effective log that includes the segments in remote storage.  When a follower receives OffsetMovedToTieredStorage, then it tries to build the required state from the leader and remote storage so that it can be ready to move to fetch state.   Introduced `RemoteLogManager` which is responsible for     - initializing `RemoteStorageManager` and `RemoteLogMetadataManager` instances.    - receives any leader and follower replica events and partition stop events and act on them    - also provides APIs to fetch indexes, metadata about remote log segments.  Followup PRs will add more functionality like copying segments to tiered storage, retention checks to clean local and remote log segments. This will change the local log start offset and make sure the follower fetch protocol works fine for several cases.  You can look at the detailed protocol changes in KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage#KIP405:KafkaTieredStorage-FollowerReplication  Authors: satishd@apache.org, kamal.chandraprakash@gmail.com, yingz@uber.com  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","satishd","2021-10-12T10:19:44Z","2022-07-29T17:42:30Z"
"","11053","KAFKA-13015 Ducktape System Tests for Metadata Snapshots","This PR implements system tests in ducktape to test the ability of brokers and controllers to generate and consume snapshots and catch up with the metadata log.  Test Output: ``` ================================================================================ SESSION REPORT (ALL TESTS) ducktape version: 0.8.1 session_id:       2021-07-15--017 run time:         6 minutes 46.061 seconds tests run:        2 passed:           2 failed:           0 ignored:          0 ================================================================================ test_id:    kafkatest.tests.core.snapshot_test.TestSnapshots.test_broker.metadata_quorum=REMOTE_KRAFT status:     PASS run time:   4 minutes 3.388 seconds -------------------------------------------------------------------------------- test_id:    kafkatest.tests.core.snapshot_test.TestSnapshots.test_controller.metadata_quorum=REMOTE_KRAFT status:     PASS run time:   2 minutes 42.434 seconds --------------------------------------------------------------------------------  ```","closed","","niket-goel","2021-07-14T21:41:11Z","2021-07-23T23:28:21Z"
"","10899","KAFKA-12952 Adding Delimiters to Metadata Snapshot","This PR implements part of the changes discussed in https://issues.apache.org/jira/browse/KAFKA-12952.  The following high level changes are made: * Add a header record to the metadata snapshot   * The header contains a Version and the append time             for the last log in the snapshot * Add a footer record to the metadata snapshot   * The footer contains a Version * Add a version to the leader change control message  [NOTE] Added a field lastContainedLogTimestamp to the SnapshotHeader  The field is currently defaulted to 0 in all callers. KAFKA-12997 will add in the necessary wiring to use the correct timestamp","closed","","niket-goel","2021-06-17T21:30:55Z","2021-06-29T16:37:21Z"
"","10931","KAFKA-12998: Implement broker-side KRaft snapshots","This PR implements broker-side KRaft snapshots, including both saving and loading. The code for triggering a periodic broker-side snapshot will come in a follow-on PR. Loading should work with just this PR.  It also implements reloading broker snapshots after initialization.  In order to facilitate snapshots, this PR introduces the concept of MetadataImage and MetadataDelta.  MetadataImage represents the metadata state retained in memory. It is basically a generalization of MetadataCache that includes a few things that MetadataCache does not (such as features and client quotas.) KRaftMetadataCache is now an accessor for the data stored in this object. Similarly, MetadataImage replaces CacheConfigRespository and ClientQuotaCache. It also subsumes kafka.server.metadata.MetadataImage and related classes.  MetadataDelta represents a change to a MetadataImage. When a KRaft snapshot is loaded, we will accumulate all the changes into a MetadataDelta first, prior to applying it. If we must reload a snapshot because we fell too far behind while consuming metadata, the resulting MetadataDelta will contain all the changes needed to catch us up.  During normal operation, MetadataDelta is also used to accumulate the changes of each incoming batch of metadata records. These incremental deltas should be relatively small.  I have removed the logic for updating the various manager objects from BrokerMetadataListener and placed it into BrokerMetadataPublisher.  This makes it easier to unit test BrokerMetadataListener.","closed","kip-500,","cmccabe","2021-06-26T00:59:30Z","2021-07-06T23:37:01Z"
"","10743","KIP-699: Update FindCoordinator to resolve multiple Coordinators at a time","This PR implements [KIP-699](https://cwiki.apache.org/confluence/display/KAFKA/KIP-699%3A+Update+FindCoordinator+to+resolve+multiple+Coordinators+at+a+time)  It updates FindCoordinator request and response to support resolving multiple coordinators at a time. If a broker does not support the new FindCoordinator version, clients can revert to the previous behaviour and use a request for each coordinator.  All methods in Admin that require looking up coordinators have been updated to use the new AdminApiDriver logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-05-21T17:23:26Z","2021-07-01T21:05:07Z"
"","11000","KAFKA-13045: Adding a test for batched offsetFetch requests with one group repeating","This PR follows up on the implementation in https://github.com/apache/kafka/pull/10962 to add an additional test for batched `offsetFetch` requests where we have a group that is repeating with a different topic partition list in the request. The expected behavior here is that the response will return whatever the last mapping of group to `List` was put in the request.   This path should be merged to both `trunk` and `3.0`.","closed","","skaundinya15","2021-07-08T18:34:31Z","2021-07-10T07:43:56Z"
"","11129","MINOR: Fix for flaky test in StoreQueryIntegrationTest","This PR fixes a bug in StoreQueryIntegrationTest::shouldQueryOnlyActivePartitionStoresByDefault that causes the test to fail in the case of a client rebalancing. The changes in this PR make sure the test keeps re-trying after a rebalancing operation, instead of failing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","patrickstuedi","2021-07-26T09:27:33Z","2021-07-29T21:27:18Z"
"","10762","KAFKA-12819: Add assert messages to MirrorMaker tests plus other qual…","This PR does various QoL improvements for the MM tests, mainly some basic refactoring to remove some boilerplate as well as adding messages to all of the assert statements so that if they fail there is more context  Some improvements to the assert failure messages may need to be done to make it more clear whats actually going on, @ryannedolan you may have more to add on this point. - [X] Verify design and implementation  - [X] Verify test coverage and CI build status","closed","","mdedetrich","2021-05-25T14:11:04Z","2021-05-27T06:21:46Z"
"","10537","KAFKA-10847: Delete Time-ordered duplicated records using deleteRange() internally","This PR changes the `TimeOrderedKeySchema` composite key from `time-seq-key` -> `time-key-seq` to allow deletion of duplicated time-key records using the RocksDB `deleteRange`  API. It also removes all duplicates when `put(key, null)` is called. Currently, the `put(key, null)` was a no-op, which was causing problems because there was no way to delete any keys when duplicates are allowed.  The RocksDB `deleteRange(keyFrom, keyTo)` deletes a range of keys from `keyFrom` (inclusive) to `keyTo` (exclusive). To make `keyTo` inclusive, I incremented the end key by one when calling the `RocksDBAccessor`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","spena","2021-04-14T19:49:40Z","2021-04-18T18:18:19Z"
"","11076","KAFKA-12486: Enforce Rebalance when a TaskCorruptedException is thrown","This PR aims to utilise HighAvailabilityTaskAssignor to avoid downtime on corrupted tasks. The idea is that, when we hit TaskCorruptedException on an active task, a rebalance is triggered after we've wiped out the corrupted state stores. This will allow the assignor to temporarily redirect this task to another client who can resume work on the task while the original owner works on restoring the state from scratch.","closed","","vamossagar12","2021-07-18T11:39:55Z","2021-09-28T23:50:28Z"
"","10847","KAFKA-12921: Upgrade ZSTD JNI to 1.5.0-2","This PR aims to upgrade `zstd-jni` from `1.4.9-1` to `1.5.0-2`.  This change will incorporate a number of bug fixes and performance improvements made in `1.5.0` of `zstd`: - https://github.com/facebook/zstd/releases/tag/v1.5.0 - https://github.com/luben/zstd-jni/releases/tag/v1.5.0-1 - https://github.com/luben/zstd-jni/releases/tag/v1.5.0-2  The most recent `1.5.0` release offers +25%-140% (compression) and +15% (decompression) performance improvements under certain conditions. Those conditions are unlikely to apply to Kafka with the default configuration, however.  Since this is a dependency change, this should pass all the existing CIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dchristle","2021-06-09T03:20:12Z","2021-06-13T16:14:25Z"
"","10542","KAFKA-12313: Streamling windowed Deserialiser configs.","This PR aims to streamline the configurations for WindowedDeserialisers. It deprecates default.windowed.key.serde.inner and default.windowed.value.serde.inner configs in StreamConfig and adds window.inner.class.deserialiser. Details described here: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177047930","closed","","vamossagar12","2021-04-15T15:01:41Z","2021-05-18T04:01:32Z"
"","11262","KAFKA-12963: Add processor name to error","This PR adds the processor name to the `ClassCastException` exception text in `process()`  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","alapidas","2021-08-26T03:24:07Z","2021-08-28T03:07:50Z"
"","10738","KAFKA-6945: KIP-373, allow users to create delegation token for others","This PR adds the capability of users to create delegation token to other users.  KIP-373: https://cwiki.apache.org/confluence/display/KAFKA/KIP-373%3A+Allow+users+to+create+delegation+tokens+for+other+users  In this use case, a superuser with username ‘superuser’ wants to run kafka clients on behalf of a user 'joe'. The 'superuser' has secure authentication credentials (kerberos, SSL, SCRAM) but user 'joe' doesn’t have any. The clients are required to run as user 'joe' and authorizations are required to be done as user 'joe.' In this case, 'superuser' can get a delegation token for user 'joe', and use the generated token to run the Kafka clients. This will mimic the impersonation functionality. This will help the stream processing frameworks/libs (Apache Spark, Storm, Kafka Streams) to run the jobs (Kafka clients) as submitted users.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2021-05-20T13:07:59Z","2022-06-21T14:07:37Z"
"","11273","KAFKA-13237; Add ActiveBrokerCount and FencedBrokerCount metrics to the ZK controller (KIP-748)","This PR adds the `ActiveBrokerCount` and the `FencedBrokerCount` metrics to the ZK controller. Note that `FencedBrokerCount` is always set to zero in the ZK controller.  `testControllerMetrics` has been extended to ensure that the metrics are exposed. However, I was not able to build a test with multiple brokers to verify the counts due to the shared common yammer metrics registry which is used. I have tested the metrics manually as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-08-27T07:23:43Z","2021-09-08T16:44:32Z"
"","11025","Scala 3 Compilation with 2.12 support","This PR adds support to Scala 3.0 without the need to drop support for Scala 2.12 Did my best as well to add some new stages in Jenkis for Scala 3.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-07-12T12:12:19Z","2021-09-04T09:54:06Z"
"","11120","KAFKA-4064: Add support for infinite endpoints for range queries","This PR adds support to open endpoint queries in the state store.   KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-763%3A+Range+queries+with+open+endpoints  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","patrickstuedi","2021-07-23T20:52:39Z","2021-07-30T02:52:16Z"
"","11441","KAFKA-8941: Add RocksDB Metrics that Could not be Added due to RocksD…","This PR adds some RocksDB metrics that could not be added in KIP-471 due to RocksDB version. The new metrics are extracted using Histogram data provided by RocksDB API, and the old ones were extracted using Tickers. The new metrics added are _memtable-flush-time-(avg|min|max)_ and _compaction-time-(avg|min|max)_.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Luizfrf3","2021-10-27T16:58:42Z","2021-11-03T11:18:29Z"
"","11375","KAFKA-10865: Log transformed record in WorkerSinkTask","This PR adds a log message with the **topic**, **key** and **value** to the transformed record in **WorkerSinkTask** similar to was already being done in **WorkerSinkTask**.  Also fixed a small typo in javadocs where there was a double period.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","upsidedownsmile","2021-10-03T16:43:09Z","2022-04-20T00:31:17Z"
"","11020","KAFKA-12937; mm2 can start from the ending of a topic","this patch resolve the KAFKA-12937 issue: mm2 can not start from the ending of a topic. When starting with previously uncommitted partitions, the MirrorSourceTask always set offset to 0.  solution:  filter the uncommitted partitions and call seekToEnd or seekToBeginning to reset offset according to auto.offset.reset config. @halorgium @astubbs  @alexism  @glasser  @rhardouin  please check","open","","yanspirit","2021-07-12T08:27:24Z","2021-07-20T02:26:14Z"
"","10497","KAFKA-12342; Merge RaftClient and MetaLogManager interfaces and remove shim","This patch removes the temporary shim layer we added to bridge the interface differences between `MetaLogManager` and `RaftClient`.   - Reverse dependency between :raft and :metadata modules. - Consolidate `handleResign` and `handleNewLeader` APIs into single `handleLeaderChange` API - Move `MetadataRecordSerde` into :metadata  - Update listeners to use `BatchReader` which takes disk reads out of the Raft IO thread - Delete `MetaLogRaftShim`, `MetaLogManager`, and `MetaLogListener`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-04-07T03:19:14Z","2021-05-24T22:45:11Z"
"","10705","KAFKA-12342: Reverse module dependency between Raft and  Metadata","This patch removes the temporary shim layer we added to bridge the interface differences between `MetaLogManager` and `RaftClient`.  1. Reverse dependency between :raft and :metadata modules. 2. Consolidate `handleResign` and `handleNewLeader` APIs into single `handleLeaderChange` API 3. Move `MetadataRecordSerde` into :metadata 4. Update listeners to use `BatchReader` which takes disk reads out of the Raft IO thread 5. Delete `MetaLogRaftShim`, `MetaLogManager`, and `MetaLogListener` 6. Remove code for loading snapshots on the controller  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-05-15T19:54:55Z","2021-07-14T19:51:14Z"
"","10757","MINOR: Log more information when producer snapshot is written","This patch logs more information when a producer snapshot is written to the disk.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-05-25T11:50:52Z","2021-05-26T07:21:25Z"
"","10895","KAFKA-12890; Consumer group stuck in `CompletingRebalance` (2.8)","This patch introduces a new delayed operation which effectively ensures that a SyncGroup request is received from all the stable members in the groups within the rebalance timeout. The timer starts when the group transitions to the `CompletingRebalance` state. The previous mechanism based on `DelayedHeartbeat` did not work anymore because of https://github.com/apache/kafka/pull/8834 which allows heartbeats while the group is in the `CompletingRebalance`.  Reviewers: Luke Chen , Jason Gustafson   (cherry-picked from commit d294b946ca)  Conflicts: 	core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala 	core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala 	core/src/test/scala/unit/kafka/coordinator/group/GroupMetadataTest.scala  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-06-17T12:19:36Z","2021-08-10T17:50:28Z"
"","10803","KAFKA-12874; Increase default consumer session timeout to 45s","This patch increases the default consumer session timeout to 45s as documented in KIP-735: https://cwiki.apache.org/confluence/display/KAFKA/KIP-735%3A+Increase+default+consumer+session+timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-06-01T19:18:48Z","2022-02-21T09:00:59Z"
"","10974","KAFKA-12979; Implement command to find hanging transactions","This patch implements the `find-hanging` command described in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions#KIP664:Providetoolingtodetectandaborthangingtransactions-FindingHangingTransactions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-07-05T07:53:43Z","2021-07-06T17:39:59Z"
"","11268","MINOR: Ensure transactional message copier failures are logged","This patch has a couple small improvements to `TransactionalMessageCopier` logging:  - Log all fatal exceptions which cause the copier to shutdown unexpectedly - Log all non-fatal exceptions which cause the copier to abort a transaction  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-08-26T18:38:49Z","2021-08-27T18:02:48Z"
"","11095","MINOR: Fix ZooKeeperAuthorizerTest for KRaft","This patch fixes the ZooKeeperAuthorizerTest for KRaft.  The system test was not configuring/reconfiguring/restarting the remote controller quorum with the correct security settings.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-20T19:59:07Z","2021-07-20T23:35:15Z"
"","11186","KAFKA-13162: Ensure ElectLeaders is properly handled in KRaft","This patch fixes several problems with the `ElectLeaders` API in KRaft:  - `KafkaApis` did not properly forward this request type to the controller. - `ControllerApis` did not handle the request type. - `ElectLeadersRequest.getErrorResponse` may raise NPE when `TopicPartitions` is null. - Controller should not do preferred election if `ElectLeaders` specifies `UNCLEAN` election. - Controller should not do unclean election if `ElectLeaders` specifies `PREFERRED` election. - Controller should use proper error codes to handle cases when desired leader is unavailable or when no election is needed because a desired leader is already elected. - When election for all partitions is requested (indicated with null `TopicPartitions` field), the response should not return partitions for which no election was necessary.  In addition to extending the unit test coverage in `ReplicationControlManagerTest`, I have also converted `LeaderElectionCommandTest` to use KRaft.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-08-06T19:07:12Z","2021-09-15T15:52:45Z"
"","10901","MINOR: Fix javadoc errors in `RaftClient`","This patch fixes a few minor javadoc issues in `RaftClient`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-06-17T21:58:41Z","2021-08-11T13:14:04Z"
"","11289","KAFKA-13254: Fix deadlock when AlterIsr response returns","This patch fixes a deadlock when incrementing the high watermark after the synchronous zk ISR modification happens. The main difference is that we prevent the callback from executing while under the leader and ISR lock. The deadlock bug was introduced in https://github.com/apache/kafka/pull/11245.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-09-01T17:31:45Z","2021-09-20T19:15:23Z"
"","11448","KAFKA-13417; Ensure dynamic reconfigurations set old config properly","This patch fixes a bug in `DynamicBrokerConfig` which causes some configuration changes to be ignored. In particular, the bug is the result of the reference to the old configuration getting indirectly mutated prior to the call to `BrokerReconfigurable.reconfigure`. This causes the first dynamic configuration update to pass effectively the same configuration as both `oldConfig` and `newConfig`. In cases such as in `DynamicThreadPool`, the update is ignored because the old configuration value matches the new configuration value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-10-28T21:07:44Z","2021-11-09T21:42:54Z"
"","11118","KAFKA-13127; Fix stray topic partition deletion for kraft","This patch fixes `BrokerMetadataPublisher.findGhostReplicas` (renamed to `findStrayPartitions`) so that it returns the stray partitions. Previously it was returning the non-stray partitions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-07-23T18:31:50Z","2021-08-11T13:13:15Z"
"","11319","KAFKA-13288; Include internal topics when searching hanging transactions","This patch ensures that internal topics are included when searching for hanging transactions with the `--broker-id` argument in `kafka-transactions.sh`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-09-10T18:27:01Z","2021-09-10T21:33:37Z"
"","10483","KAFKA-12586; Add `DescribeTransactions` Admin API","This patch contains the `Admin` implementation of the `DescribeTransactions` APIs described in KIP-664: KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-04-06T01:54:57Z","2021-04-22T16:34:11Z"
"","11222","MINOR: Test ReplicaManager MBean names","This patch closes a testing gap by confirming that ReplicaManager metrics are exposed with the expected MBean names.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rondagostino","2021-08-16T19:26:42Z","2021-08-23T17:44:59Z"
"","11066","MINOR: Test ReplicaManager Metric Names","This patch closes a test gap where we do not check ReplicaManager metrics remain as expected.  There was a bug in 2.8 where the metrics moved under a different class name for the KRaft case.  Having such tests would have helped identify the bug.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-15T21:38:03Z","2021-07-20T23:37:27Z"
"","11010","KAFKA-13053; Bump kraft frame version for incompatible changes from 2.8","This patch bumps the default frame version for kraft records from 0 to 1. At the same time, we reset all records versions back to 0 and we enable flexible version support for `UnregisterBrokerRecord`, which was missed previously. Note that the frame version bump also affects the KIP-405 records since they are sharing `AbstractApiMessageSerde`. Since these records were not part of any previous releases, I did not see a problem with this.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-07-09T17:18:07Z","2021-08-11T13:13:14Z"
"","10814","KAFKA-12888; Add transaction tool from KIP-664","This patch adds the transaction tool specified in KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. This includes all of the logic for describing transactional state and for aborting transactions. The only thing that is left out is the `--find-hanging` implementation, which will be left for a subsequent patch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-06-04T01:19:46Z","2021-06-22T16:47:31Z"
"","10599","KAFKA-12716; Add `Admin` API to abort transactions","This patch adds the Admin API to abort transactions from KIP-664: https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions. The `WriteTxnMarker` API needs to be sent to partition leaders, so we are able to reuse `PartitionLeaderStrategy`, which was introduced when support for `DescribeProducers` was added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-04-26T20:55:52Z","2021-04-28T15:43:51Z"
"","10699","MINOR: Add support for ZK Authorizer with KRaft","This patch adds support for running the ZooKeeper-based kafka.security.authorizer.AclAuthorizer with KRaft clusters. Set the authorizer.class.name config as well as the zookeeper.connect config while also setting the typical KRaft configs (node.id, process.roles, etc.), and the cluster will use KRaft for metadata and ZooKeeper for ACL storage. A system test that exercises the authorizer is included.  This patch also changes ""Raft"" to ""KRaft"" in several system test files. It also fixes a bug where system test admin clients were unable to connect to a cluster with broker credentials via the SSL security protocol when the broker was using that for inter-broker communication and SASL for client communication.  Co-authored-by: Ron Dagostino","closed","kip-500,","cmccabe","2021-05-14T23:12:59Z","2021-05-19T17:33:46Z"
"","10550","MINOR: Add support for ZK Authorizer with KRaft","This patch adds support for running the ZooKeeper-based `kafka.security.authorizer.AclAuthorizer` with KRaft clusters.  Set the `authorizer.class.name` config as well as the `zookeeper.connect` config while also setting the typical KRaft configs (`node.id`, `process.roles`, etc.), and the cluster will use KRaft for metadata and ZooKeeper for ACL storage.  A system test that exercises the authorizer is included.  This patch also changes ""Raft"" to ""KRaft"" in several system test files.  It also fixes a bug where system test admin clients were unable to connect to a cluster with broker credentials via the SSL security protocol when the broker was using that for inter-broker communication and SASL for client communication.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-04-17T00:28:11Z","2022-02-11T17:16:58Z"
"","11333","KAFKA-13306: Null connector config value passes validation, but fails creation","This patch adds null value check to the connector config validation, and extends unit tests to cover this functionality.","closed","connect,","lhunyady","2021-09-17T11:29:43Z","2022-02-11T15:14:06Z"
"","10913","KAFKA-12631; Implement `resign` API in `KafkaRaftClient`","This patch adds an implementation of the `resign()` API which allows the controller to proactively resign leadership in case it encounters an unrecoverable situation. There was not a lot to do here because we already supported a `Resigned` state to facilitate graceful shutdown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-06-21T23:07:39Z","2021-06-29T01:00:19Z"
"","10991","MINOR: system test fix for 3 co-located KRaft controllers","This patch adds a sanity-check bounce system test for the case where we have 3 co-located KRaft controllers and fixes the system test code so that this case will pass by starting brokers in parallel by default instead of serially.  We now also send SIGKILL to any running KRaft broker or controller nodes for the co-located case when a majority of co-located controllers have been stopped -- otherwise they do not shutdown, and we spin for the 60 second timeout.  Finally, this patch adds the ability to specify that certain brokers should not be started when starting the cluster, and then we can start those nodes at a later time via the `add_broker()` method call; this is going to be helpful for KRaft snapshot system testing.  We were not testing the 3 co-located KRaft controller case previously, and it would not pass because the first Kafka node would never be considered started.  We were starting the Kafka nodes serially, and we decide that a node has successfully started when it logs a particular message.  This message is not logged until the broker has identified the controller (i.e. the leader of the KRaft quorum).  There cannot be a leader until a majority of the KRaft quorum has started, so with 3 co-located controllers the first node could never be considered ""started"" by the system test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-07T18:49:58Z","2021-07-16T23:28:09Z"
"","11395","MINOR; Add a replication system test which simulates a slow replica","This patch adds a new system test which exercises the shrining/expansion process of the leader. It does so by introducing a network partition which isolate a broker from the other brokers in the cluster but not from KRaft Controller/ZK.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-10-14T08:47:23Z","2021-10-20T06:19:39Z"
"","10616","KAFKA-12709; Add Admin API for `ListTransactions`","This patch adds `Admin` support for the `listTransactions` API, which was added by [KIP-664](https://cwiki.apache.org/confluence/display/KAFKA/KIP-664%3A+Provide+tooling+to+detect+and+abort+hanging+transactions). Similar to `listConsumerGroups`, the new `listTransactions` API is intended to be sent to all brokers. This did not fit very well with the new `AdminApiDriver` that was used for the other KIP-664 APIs, so I had to refactor a bit. The way I ultimately handled it is to treat the brokerId as the key that needs to be mapped. Unlike with the other cases, the set of brokerIds is not known ahead of time and has to be discovered through a `Metadata` lookup (just as with `listConsumerGroups`). Because the future associated with `ListTransactionsResult` is more complex, I ended up externalizing future completion in a new `AdminApiFuture` object which is passed to `AdminApiDriver` during construction.  One minor change that is also worth calling out is the removal of `AdminApiHandler.Keys`. This was previously used to indicate the sets of statically and dynamically mapped keys. Instead, I now handle this with a new `StaticBrokerStrategy`. The static scope is indicated by returning an `ApiRequestScope` object which has a specific destination broker set.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-04-30T03:14:11Z","2021-06-02T00:58:47Z"
"","10539","Minor: Move trogdor out of tools and into its own project","This moves Trogdor out of tools/ and into its own project. This would allow Trogdor to be linked against other libraries (for example, you could build a worker that links against k8s to trigger issues through it), without bringing those dependencies into Kafka itself through the tools project.  It still keeps Trogdor in the CLASSPATH created in kafka-run-class.sh.  Tested by building, and running Trogdor in the resulting build.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shayelkin","2021-04-14T22:22:53Z","2021-04-15T19:07:17Z"
"","10870","MINOR: enable EOS during smoke test IT","This IT has been failing on trunk recently. Enabling EOS during the integration test makes it easier to be sure that the test's assumptions are really true during verification and should make the test more reliable.  I also noticed that in the actual system test file, we are using the deprecated property name ""beta"" instead of ""v2"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-06-12T20:03:53Z","2021-06-14T02:35:19Z"
"","10614","MINOR: Upgrade jersey to 2.34","This is to resolve CVE: CVE-2021-28168   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xjin-Confluent","2021-04-30T00:20:22Z","2021-05-20T03:25:28Z"
"","11335","MINOR: Let the list-store return null in putifabsent","This is to make sure that even if logging is disabled, we would still return null in order to workaround the deserialization issue for stream-stream left/outer joins.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-09-17T18:47:56Z","2021-09-17T19:05:22Z"
"","10674","KAFKA-12713: Report the real fetch latency by removing the wait-time in purgatory.","This is to help monitor the 'real' fetch latency by removing the waitTime when FetchRequest is in purgatory. The changes include: 1. Add waitTimeMs in FetchResponse() 2. In Kafka API handler (in handleFetchRequest() function),  when creating FetchResponse(),  set the waitTimeMs as the time spent in purgatory 3. In Follower broker processFetchRequest(),  it tracks the real latency of fetch requests by minus the waitTimeMs from FetchResponse. 4. In FetcherStats, we will add a new histogram to track this calculated ""true"" fetch latency.","open","","mingaliu","2021-05-11T17:54:48Z","2021-05-11T17:54:48Z"
"","10645","KAFKA-12464: follow up PR to refactor codes and add logs","This is the follow up PR to address the remaining comments in https://github.com/apache/kafka/pull/10509.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-05-07T03:42:22Z","2021-05-09T01:11:40Z"
"","11068","KAFKA-13081: detect doubly assigned parition (for v2.8)","This is the fix 1 and fix 2 in https://github.com/apache/kafka/pull/10985 for v2.8, including the tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-16T07:55:52Z","2021-08-19T00:03:23Z"
"","11247","HOTFIX: (doc) Disable spurious left/outer stream-stream join fix","This is the doc rollback for https://github.com/apache/kafka/pull/11233  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-21T06:34:19Z","2021-08-23T19:00:01Z"
"","10583","KAFKA-12701: NPE in MetadataRequest when using topic IDs","This is the change for 2.8 that should be used in 2.8.1. We prevent handling MetadataRequests where the topic name is null (to prevent NPE) as well as prevent requests that set topic IDs since this functionality has not yet been implemented.   Will cherry-pick these changes to trunk and bump the request/response version when we implement metadata requests using topic IDs.  Added tests to ensure the error is thrown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-04-22T01:38:35Z","2021-04-22T02:39:23Z"
"","10914","[KAFKA-8522] Streamline tombstone and transaction marker removal","This is rebased PR for #7884 and #9915.  This PR aims to remove tombstones that persist indefinitely due to low throughput. Previously, deleteHorizon was calculated from the segment's last modified time.  In this PR, the deleteHorizon will now be tracked in the baseTimestamp of RecordBatches. After the first cleaning pass that finds a record batch with tombstones, the record batch is recopied with deleteHorizon flag and a new baseTimestamp that is the deleteHorizonMs. The records in the batch are rebuilt with relative timestamps based on the deleteHorizonMs that is recorded. Later cleaning passes will be able to remove tombstones more accurately on their deleteHorizon due to the individual time tracking on record batches.  KIP 534: https://cwiki.apache.org/confluence/display/KAFKA/KIP-534%3A+Retain+tombstones+and+transaction+markers+for+approximately+delete.retention.ms+milliseconds  co author: @ConcurrencyPractitioner   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mattwong949","2021-06-22T01:21:31Z","2021-09-16T16:17:16Z"
"","10504","KAFKA-12620 Allocate producer ids on the controller","This is part of the implementation for [KIP-730](https://cwiki.apache.org/confluence/display/KAFKA/KIP-730%3A+Producer+ID+generation+in+KRaft+mode)   This change adds a new AllocateProducerIds RPC which is used by the broker to ask for a block of producer IDs from the controller. In particular, this PR only includes changes for the broker and zookeeper-based controller.  In [ProducerIdManager.scala](https://github.com/apache/kafka/pull/10504/files#diff-ef2ac0c9264a461805e6c5ca5624428755a31239edf4057ae7a1c561bbec12c1), I've added a prefetch to avoid the case of an InitProducerId request being blocked waiting on an AllocateProducerIds request. Once the generated producer ID has reached 90% of the current block, the next block is fetched and stored locally until it is needed. This could lead to blocks being needlessly consumed in the case of a broker shutdown before it block is needed, but that's probably not a big concern.  Another change was to factor out the data class ProducerIsBlock into the shiny and new ✨server-common✨ module. The class is currently used by the broker and controller, and in the next PR it will be used by the KRaft controller.  Some of the ZK-related code was reorganized, but the format of the ZK data is unchanged for backwards compatibility.  In [KafkaServer.scala](https://github.com/apache/kafka/pull/10504/files#diff-3638ff970bc6766f9e570f76a6440966930b990af33e53e5b3db9d65f90264e9), I am reusing the same network channel as the forwarding manager. This was done to avoid proliferation of these broker to controller channels.","closed","kip-500,","mumrah","2021-04-08T18:02:30Z","2021-05-21T19:58:50Z"
"","10752","KAFKA-12620 Allocate Producer IDs in KRaft controller","This is part 2 of [KIP-730](https://cwiki.apache.org/confluence/display/KAFKA/KIP-730%3A+Producer+ID+generation+in+KRaft+mode), part 1 was in #10504.  This PR adds support on the KRaft controller for handling AllocateProducerIDs requests and managing the state of the latest producer ID block in the controller and committing this state to the metadata log.","closed","","mumrah","2021-05-24T18:25:47Z","2021-06-03T23:23:32Z"
"","11191","KAFKA-13173 Making the controller fence stale brokers one at a time if multiple stale brokers are discovered at the same time","This is necessary as the effect of each fencing is visible to the controller one the corresponding record for it has been generated and applied. Processing stale brokers one at a time ensures that subsequent  fencing records are generated correctly with the correct view of the system.","closed","","niket-goel","2021-08-09T15:53:38Z","2021-08-12T17:00:41Z"
"","10758","KAFKA-12782: Javadocs search sends you to a non-existent URL","This is just a workaround to solve this problem while we are still using JDK11. Once moving to, presumably, JDK17 this change won't be needed anymore and could be deleted safely. See https://bugs.openjdk.java.net/browse/JDK-8215291  This change includes a snippet of code copied from JDK 12+  I'm not sure if an extra header needs to be added for the piece of code I copied over, or if I would need to implement it from scratch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-25T12:46:58Z","2021-05-27T17:46:08Z"
"","11057","KAFKA-13008: Try to refresh end offset when partitionLag returns empty","This is an idea I had for attacking this on the consumer client level:  1. When listOffset result is retrieved inside Fetcher, check if the partitions are part of the subscriptions of the consumer; if yes update the corresponding LSO or HW based on the isolation level. 2. When partitionLag cannot return result since the log end offset (LSO/HW) is not known, send an async list offset which would be completed by other calls polling (also the hb thread may complete it as well), and hope the next partitionLag would get the result.  Then on the streams side, the first partitionLag would still return empty, but soon enough the subsequent partitionLag should return data and we would not wait for the fetch response to update fetched state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-07-15T05:52:22Z","2021-07-23T23:47:33Z"
"","11252","KAFKA-13216: Use a KV with list serde for the shared store","This is an alternative approach in parallel to #11235. After several unsuccessful trials to improve its efficiency i've come up with a larger approach, which is to use a kv-store instead as the shared store, which would store the value as list. The benefits of this approach are:  1) Only serde once that compose , at the outer metered stores, with less byte array copies. 2) Deletes are straight-forward with no scan reads, just a single call to delete all duplicated  values. 3) Using a KV store has less space amplification than a segmented window store.  The cons though:  1) Each put call would be a get-then-write to append to the list; also we would spend a few more bytes to store the list (most likely a singleton list, and hence just 4 more bytes). 2) It's more complicated definitely.. :)  The main idea is that since the shared store is actively GC'ed by the expiration logic, not based on time retention, and since that the key format is in , the range expiration query is quite efficient as well.  Added testing covering for the list stores (since we are still use kv-store interface, we cannot leverage on the get() calls in the stream-stream join, instead we use putIfAbsent and range only). Another minor factoring piggy-backed is to let `toList` to always close iterator to avoid leaking.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2021-08-24T04:25:59Z","2021-09-17T17:38:38Z"
"","11096","Adding reviewers.py to help tag reviewers in commit message","This is a simple python script which looks at the commit log and lets you easily find names + emails for the ""Reviewers"" line.  You can search by first name or email. Here's a sample session:  ``` $ ./reviewers.py  Utility to help generate 'Reviewers' string for Pull Requests. Use Ctrl+D or Ctrl+C to exit  Name or email (case insensitive): Dav <-- Just need a few letters  Possible matches (in order of most recent): [1] David Jacot djacot@confluent.io (141)  <-- Shows a count of occurrences in the commit log [2] David Arthur mumrah@gmail.com (102) [3] David Arthur david.arthur@confluent.io (5) [4] David Jacot david.jacot@gmail.com (8)  Make a selection: 2 Reviewers so far: [('David Arthur', 'mumrah@gmail.com', 102)]  Name or email (case insensitive): chia  Possible matches (in order of most recent): [1] Chia-Ping Tsai chia7712@gmail.com (358) [2] Chia-Ping Tsai chia7712@apache.org (3)  Make a selection: 1 Reviewers so far: [('David Arthur', 'mumrah@gmail.com', 102), ('Chia-Ping Tsai', 'chia7712@gmail.com', 358)]  Name or email (case insensitive): ism  Possible matches (in order of most recent): [1] Ismael Juma ismael@juma.me.uk (1514) [2] Ismael Juma ijuma@apache.org (3) [3] Ismael Juma mlists@juma.me.uk (4) [4] Ismael Juma ismael@confluent.io (19) [5] Ismael Juma github@juma.me.uk (7)  Make a selection: 1 Reviewers so far: [('David Arthur', 'mumrah@gmail.com', 102), ('Chia-Ping Tsai', 'chia7712@gmail.com', 358), ('Ismael Juma', 'ismael@juma.me.uk', 1514)]  Name or email (case insensitive): ^C  Reviewers: David Arthur , Chia-Ping Tsai , Ismael Juma  ```  ctrl+d or ctrl+c will exit the program and print out the ""Reviewers"" line based on the selection.","open","","mumrah","2021-07-20T20:59:21Z","2022-07-13T18:47:33Z"
"","11163","MINOR: doc change for minisr to clarify replicas in Kafka Config","This is a minor doc change to MinInSyncReplicasDoc to clarify what is a replica and further help users understand the importance of this configuration in relation to their producer configuration.  It didn't seem like a Jira was necessary but I am open to creating one if needed. I did not compile from source to test, no other code was changed.  This contribution is my original work and I license the work to the project under the project's open source license.  Thanks!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","pdruley","2021-08-02T18:57:53Z","2021-08-04T20:07:00Z"
"","10934","[DO NOT MERGE] Scala3 test","This is a draft PR to showcase how complicated is to migrate to Scala3.  It includes a necessary change as well (removal of Scala Collections Compat and Java8 Compat libraries). This is needed as those libraries' only purpose was to cross-compile to Scala 2.11 and 2.12.  After this is done, only a bit of workaround gradle was needed (library names for Scala 3 compiler changed).  The only code change needed is the renaming of methods like `asJava`, `asScala`, `until`, `from`, and `to` which  were since Scala 2.13.0 (Compat libraries were shadowing those). And this change is pretty automatic. On the test file (only 1 didn't really compile) only the right new import needed to be brought in.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-06-28T14:11:47Z","2021-07-13T13:53:38Z"
"","11472","TRIVIAL: Remove unused parameters, exceptions, comments, etc.","This is a bulk of trivial glitches I found while working on the other issues.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-11-06T09:04:33Z","2021-11-18T10:52:11Z"
"","11014","MINOR: Add default for EndOffset and Epoch in FetchSnapshotResponse.","This is *extremely, extremely* minor.  Our Rust protocol impl is using default values for some serialization logic, and I noticed in doing a review for https://github.com/0x1991babe/kafka-protocol-rs/issues/1 that there's an inconsistency between `FetchSnapshotResponse` and `FetchSnapshot`, namely, `FetchSnapshot` [provides default values for these fields](https://github.com/apache/kafka/blob/trunk/clients/src/main/resources/common/message/FetchResponse.json#L73-L74).  Without defaults, we're constructing the default with `0`, which I'm concerned may be a valid state for the struct (as opposed to `-1`).  It looks like this is for Raft (??) so may actually be totally and completely irrelevant for end users of our lib, in which case feel free to close. :)","closed","","tychedelia","2021-07-09T22:51:16Z","2021-08-26T14:18:08Z"
"","10711","MINOR: Update Scala to 2.13.6","This includes TASTy Reader support for Scala 3.0.0. This makes it easier for Kafka libraries to be used in Scala 3.0 projects  Release notes: https://github.com/scala/scala/releases/tag/v2.13.6  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-17T13:02:46Z","2021-05-19T12:27:08Z"
"","10962","KAFKA-12234: Implement request/response for offsetFetch batching (KIP-709)","This implements the request and response portion of KIP-709: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=173084258. The Admin APIs will be implemented in https://github.com/apache/kafka/pull/10964.  It updates the OffsetFetch request and response to support fetching offsets for multiple consumer groups at a time. If the broker does not support the new OffsetFetch version, clients can revert to the previous behaviour and use a request for each coordinator.  We are trying to get this change into the 3.0 release. We believe this change will be safe to check in as we are changing the request protocol, and not the underlying Admin APIs. As a result, we are not changing any major client side code, just updating the protocol for the brokers to support multiple groups for a `offsetFetch` request. We have also added substantial request/response testing to ensure we are having backward compatibility.","closed","","skaundinya15","2021-07-02T19:28:03Z","2021-07-08T18:35:20Z"
"","10964","KAFKA-13043: Implement Admin APIs for offsetFetch batching","This implements the AdminAPI portion of KIP-709: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=173084258. The request/response changes was implemented in https://github.com/apache/kafka/pull/10962.  It updates the Admin APIs associated with the `OffsetFetch` API. This PR deprecates the old single group ID API and adds new APIs to take in multiple groups at a time for fetching consumer group offsets.","closed","","skaundinya15","2021-07-02T23:20:20Z","2022-07-14T12:47:34Z"
"","10954","WIP KIP-709: Implement batching for fetchOffsets","This implements the AdminAPI portion of KIP-709: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=173084258. The request/response changes was implemented in https://github.com/apache/kafka/pull/10954.  It updates the Admin APIs associated with the `OffsetFetch` API. This PR deprecates the old single group ID API and adds new APIs to take in multiple groups at a time for fetching consumer group offsets.","closed","","skaundinya15","2021-07-02T00:44:34Z","2021-07-02T23:22:17Z"
"","11091","MINOR: Fix `testResolveDnsLookup` by using a mocked dns resolver","This focuses on the currently failing test, #9315 is a more complete fix that we should also review and merge.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-20T13:03:04Z","2021-07-29T12:14:13Z"
"","11439","KAFKA-13406: skip assignment validation for built-in cooperativeStickyAssignor","This fix is trying to skip the assignment validation for built-in cooperative sticky assignor. And also add tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-10-27T09:25:43Z","2021-11-16T09:28:47Z"
"","11142","kafka-13148 Renounce leadership of the controller quorum when encountering a failure to append to metadata log.","This could happen either because a new leader was elected or because the current leader was unable to allocate memory for the incoming records","closed","","niket-goel","2021-07-28T21:16:37Z","2021-08-10T23:18:06Z"
"","10536","KAFKA-12667: Fix incorrect error log on StateDirectory close","This condition was fixed on the side in a PR against trunk, but unfortunately missed the ongoing 2.8.0, 2.7.1, and 2.6.2 releases. This is a quick hotfix we should merge once those releases are finally out the door, and backport to 2.7 at the least","closed","streams,","ableegoldman","2021-04-14T18:03:41Z","2021-04-14T20:12:10Z"
"","11084","KAFKA-13100: Create a snapshot during leadership promotion","This commit includes a few changes:  1. The leader assumes that there is always an in-memory snapshot at the last committed offset. This means that the controller needs to generate an in-memory snapshot when getting promoted from inactive to active.  2. Delete all in-memory snapshots less that the last committed offset when the on-disk snapshot is canceled or it completes.  3. The controller always starts inactive. When loading an on-disk snapshot the controller is always inactive. This means that we don't need to generate an in-memory snapshot at the offset -1 because there is no requirement that there exists an in-memory snapshot at the last committed offset when the controller is inactive.  4. SnapshotRegistry's createSnapshot should allow the creating of a snapshot if the last snapshot's offset is the given offset. This allows for simpler client code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-20T00:04:18Z","2021-07-20T17:15:48Z"
"","11080","KAFKA-13149: fix NPE for record==null when handling a produce request","This code https://github.com/apache/kafka/blob/bfc57aa4ddcd719fc4a646c2ac09d4979c076455/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L294-L296 returns record=null, and can subsequently cause a null pointer exception in https://github.com/apache/kafka/blob/bfc57aa4ddcd719fc4a646c2ac09d4979c076455/core/src/main/scala/kafka/log/LogValidator.scala#L191  This PR lets the broker throw an invalid record exception and notify clients. The fix is similar to https://github.com/apache/kafka/blob/bfc57aa4ddcd719fc4a646c2ac09d4979c076455/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L340-L358 where we throw an invalid record exception when the record's integrity is broken.","closed","","ccding","2021-07-19T17:23:42Z","2021-09-16T14:22:25Z"
"","10992","MINOR: Remove nullable from addingReplicas and removingReplicas","This change was discussed in KIP-746 and are actually included in #10753. I'm pulling them out here since there are NPEs on trunk as a result of the nullable fields in the PartitionRecord.","closed","kip-500,","mumrah","2021-07-07T18:57:33Z","2021-07-07T22:39:43Z"
"","11382","KAFKA-13348: Allow Source Tasks to Handle Producer Exceptions","This change allows Source Connectors the option to set ""error.tolerance"" to ""all"" to allow them to handle/ignore producer exceptions. In the event the producer cannot write to Kafka, the connector commitRecord() callback is invoked with null RecordMetadata. This is new behavior for the errors.tolerance setting. Default behavior is still to kill the task unconditionally if errors.tolerance is ""none"".  A unit test has been added to validate the producer callback for failure being invoked. The sourceTask will ignore the exception and the task will not be killed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","TheKnowles","2021-10-05T20:17:48Z","2022-01-27T18:19:51Z"
"","11351","KAFKA-13315: log layer exception during shutdown that caused an unclean shutdown","This also fixes KAFKA-13070.  We have seen a problem caused by shutting down the scheduler before shutting down LogManager.  When LogManager was closing partitions one by one, the scheduler called to delete old segments due to retention. However, the old segments could have been closed by the LogManager, which caused an exception and subsequently marked logdir as offline. As a result, the broker didn't flush the remaining partitions and didn't write the clean shutdown marker. Ultimately the broker took hours to recover the log during restart.  This PR essentially reverts https://github.com/apache/kafka/pull/10538  I believe the exception https://github.com/apache/kafka/pull/10538 saw is at https://github.com/apache/kafka/blob/5a6f19b2a1ff72c52ad627230ffdf464456104ee/core/src/main/scala/kafka/log/LocalLog.scala#L895-L903 which called the scheduler and crashed the compaction thread. The effect of this exception has been mitigated by https://github.com/apache/kafka/pull/10763  cc @rondagostino @ijuma @cmccabe @junrao @dhruvilshah3 as authors/reviewers of the PRs mentioned above to make sure this change look okay.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-09-21T21:34:08Z","2021-09-23T23:37:22Z"
"","10569","KAFKA-12696: Adds standardized getters to LagInfo class","This allows Jackson and other serialization libraries to serialize this object without any additional annotation, wrapping, injection etc.  Existing methods are now marked as deprecated. Tests have been  updated to test both code paths.  I've run the tests locally twice, with some failures both times. However, different tests failed each time, and they were in parts of the code that, to my reading, have no relation to the code being changed. I suspect they are just flakey tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","mihasya","2021-04-20T19:17:24Z","2021-04-21T01:42:24Z"
"","10808","KAFKA-12880: Remove deprecated `Count` and `SampledTotal` in 3.0","They were both deprecated in Apache Kafka 2.4 and it's a straightforward change to use the non deprecated variants.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-06-02T13:33:18Z","2021-06-02T18:31:10Z"
"","10872","KAFKA-12945: Remove port, host.name and related configs in 3.0","They have been deprecated since 0.10.0. Full list of removes configs: * port * host.name * advertised.port * advertised.host.name  Also adjust tests to take the removals into account. Some tests were no longer relevant and have been removed.  Finally, took the chance to: * Clean up unnecessary usage of `KafkaConfig$.MODULE$` in related files. * Add missing `Test` annotations to `AdvertiseBrokerTest` and make necessary changes for the tests to pass.  Co-authored-by: David Jacot   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-06-13T05:49:22Z","2021-06-17T12:35:47Z"
"","11108","KAFKA-13116: Fix message_format_change_test and compatibility_test_new_broker_test failures","These failures were caused by a46b82bea9abbd08e5. Details for each test:  * message_format_change_test: use IBP 2.8 so that we can write in older message formats. * compatibility_test_new_broker_test_failures: fix down-conversion path to handle empty record batches correctly. The record scan in the old code ensured that empty record batches were never down-converted, which hid this bug. * upgrade_test: set the IBP 2.8 when message format is < 0.11 to ensure we are actually writing with the old message format even though the test was passing without the change.  Verified with ducker that some variants of these tests failed without these changes and passed with them. Also added a unit test for the down-conversion bug fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-22T15:11:46Z","2021-07-23T20:43:51Z"
"","11193","MINOR: Delete temporary directories after using them in RaftManagerTest","These directories should be deleted after every test or else they will not be deleted until the tests finish running.","open","","dielhennr","2021-08-09T22:21:06Z","2021-08-11T02:09:38Z"
"","11114","KAFKA-13021: clarify KIP-633 javadocs and address remaining feedback","There were a few followup things to address from [#10926](https://github.com/apache/kafka/pull/10926), most importantly a number of fixes needed for the javadocs. Beyond that it's mostly just adding a few missing verification checks.  Given the whole point of this KIP was to help reduce a major source of confusion, the meaning and usage of grace period within Streams, it's critical that we have clear and correct javadocs accompanying the new APIs. For that reason I think it's very important to get this into 3.0 @kkonstantine -- it's also very low-risk, as the only non-docs changes are adding a handful of checks that already exist in the old APIs and were just missed in the new APIs","closed","","ableegoldman","2021-07-23T03:59:22Z","2021-07-23T23:17:57Z"
"","11386","MINOR: Fix highest offset when loading KRaft metadata snapshots","There are a few fixes included in this commit. 1. When loading a snapshot the broker `BrokerMetadataListener` was using the batch's append time, offset and epoch. These are not the same as the append time, offset and epoch from the log. We must instead use the `lastContainedLogTimeStamp`, `lastContainedLogOffset` and `lastContainedLogEpoch` from the `SnapshotReader`. 2. Include the highest offset and epoch into the `MetadataImage` and `MetadataDelta`. Adding the offset and epoch to `MetadataImage` is useful to version the image and to simplify the API. Adding the offset and epoch to `MetadataDelta` is needed to generate the `MetadataImage`. 3. Swapped the order of the arguments for `ReplicaManager.applyDelta` for consistency to match the order of the arguments for `MetadataPublisher.publish`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-10-08T22:14:15Z","2021-10-13T16:19:17Z"
"","10774","KAFKA-12380 Executor in Connect's Worker is not shut down when the worker is","The Worker class has an executor field that the public constructor initializes with a new cached thread pool (https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127.]).  When the worker is stopped, it does not shutdown this executor. This is normally okay in the Connect runtime and MirrorMaker 2 runtimes, because the worker is stopped only when the JVM is stopped (via the shutdown hook in the herders).  However, we instantiate and stop the herder many times in our integration tests, and this means we're not necessarily shutting down the herder's executor. Normally this won't hurt, as long as all of the runnables that the executor threads run actually do terminate. But it's possible those threads might not terminate in all tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","dgd-contributor","2021-05-27T10:32:42Z","2021-07-29T02:40:57Z"
"","10636","MINOR: Bump Jersey deps to 2.34 due to CVE-2021-28168","The version of the Eclipse Jersey library brought as dependences, 2.31, has a known vulnerability, CVE-2021-28168 (https://github.com/advisories/GHSA-c43q-5hpj-4crv).  This replaces it with 2.34, which is fully compatible with 2.31, except for bugs and vulnerabilities.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shayelkin","2021-05-06T01:37:39Z","2021-05-06T14:44:04Z"
"","11349","MINOR: Fix use of ConfigException in AbstractConfig class","The two-arg variant is intended to take a property name and value, not an exception message and a cause.  As-is, this leads to confusing log messages like: ``` org.apache.kafka.common.config.ConfigException: Invalid value java.lang.ClassNotFoundException: io.strimzi.kafka.KubernetesConfigMapConfigProvider for configuration Invalid config:io.strimzi.kafka.KubernetesConfigMapConfigProvider ClassNotFoundException exception occurred ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-09-21T15:28:25Z","2021-11-30T17:11:42Z"
"","10697","MINOR: Add @cluster annotation to StreamsNamedRepartitionTopicTest","The StreamsNamedRepartitionTopicTest system tests did not have the `@cluster` annotation and was therefore taking up the entire cluster.  For example, we see this in the log output:  `kafkatest.tests.streams.streams_named_repartition_topic_test.StreamsNamedRepartitionTopicTest.test_upgrade_topology_with_named_repartition_topic is using entire cluster. It's possible this test has no associated cluster metadata.`  This PR adds the missing annotation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-05-14T19:13:09Z","2021-05-17T21:34:11Z"
"","11183","MINOR: Increase the Kafka shutdown timeout to 120","The streams static membership test has failed several times due to hitting the Kafka shutdown timeout, but the logs were showing that the shutdown did actually succeed after the 60 second timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jzaralim","2021-08-05T19:48:28Z","2021-08-05T22:31:08Z"
"","10932","KAFKA-12958: add an invariant that notified leaders are never asked to load snapshot","The state machine always sees the following sequence of callback calls:  Leaders see: ... handleLeaderChange state machine is notify of leadership handleSnapshot is never called  Non-leader see: ... handleLeaderChange state machine is notify that is not leader handleSnapshot is called 0 or more times  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","zhaohaidao","2021-06-26T18:22:41Z","2021-07-04T15:32:12Z"
"","10997","MINOR: Fix flaky deleteSnapshots test","The sliding window + takeWhile behavior over a sequence seems somewhat different between Scala 2.12 and Scala 2.13. Worked around the each by using foreach with an early return.","closed","kip-500,","mumrah","2021-07-08T15:11:35Z","2021-07-09T15:15:25Z"
"","10709","KAFKA-12794: Fix trailing json tokens in DescribeProducersRequest.json","The schema definition for the DescribeProducersRequest see [here](https://github.com/apache/kafka/blob/3b6599c600f6e7fbeb000a088591f1cf9aba107d/clients/src/main/resources/common/message/DescribeProducersRequest.json) has trailing tokens - specifically, the last two lines in the commit in that link.  This does not cause problems for the generator, because Jackson will ignore trailing input by default.  However, some JSON parsers cannot be configured to ignore trailing characters, and so they fail on that file. This can cause problems for users wishing to use the official schema definitions to generate clients in other languages.  The fix here is pretty simple - just remove the trailing tokens, and optionally configure jackson to fail on trailing tokens. This patch is for the former, and I'll be happy to submit a patch that configures jackson so this won't happen again :).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","NLincoln","2021-05-17T02:24:02Z","2021-05-17T15:44:11Z"
"","11320","MINOR: Make ReplicaManager, LogManager, KafkaApis easier to construct","The ReplicaManager, LogManager, and KafkaApis class all have many constructor parameters. It is often difficult to add or remove a parameter, since there are so many locations that need to be updated. In order to address this problem, we should use named parameters when constructing these objects from Scala code. This will make it easy to add new optional parameters without modifying many test cases.  It will also make it easier to read git diffs and PRs, since the parameters will have names next to them.  For this reason, this PR updates the Scala code to use named parameters. Since Java does not support named parameters, this PR adds several Builder classes which can be used to achieve the same effect from Java code.  ReplicaManager also had a secondary constructor, which this PR removes. The function of the secondary constructor was just to provide some default parameters for the main constructor. However, it is simpler to just use default parameters.","closed","","cmccabe","2021-09-10T22:38:20Z","2021-09-17T21:12:35Z"
"","10749","KAFKA-12773: Use UncheckedIOException when wrapping IOException","The raft module may not be fully consistent on this but in general in that module we have decided to not throw the checked IOException. We have been avoiding checked IOException exceptions by wrapping them in RuntimeException. The raft module should instead wrap IOException in UncheckedIOException. This change should be limited to the raft module.","closed","","socutes","2021-05-24T07:02:12Z","2021-11-22T06:33:21Z"
"","10969","KAFKA-10884: Limit the size of Fetch and FetchSnapshot response based on broker configuration","The Raft Client implementation for Fetch and FetchSnapshot do not limit the size of the response sent based on the local configuration of replica.fetch.response.max.bytes. Fix this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","zhaohaidao","2021-07-04T03:06:20Z","2021-12-21T21:16:32Z"
"","10985","KAFKA-12984: make AbstractStickyAssignor resilient to invalid input, utilize generation in cooperative, and fix assignment bug","The primary goal of this PR is to address the problem we've seen in the wild in which the ConsumerCoordinator fails to update its SubscriptionState and ultimately feeds invalid `ownedPartitions` data as input to the assignor. Previously the assignor would detect that something was wrong and just throw an exception, now we make several efforts to detect this earlier in the assignment process and then fix it if possible, and work around it if not.  Specifically, this PR does a few things: 1) Bring the `generation` field back to the CooperativeStickyAssignor so we don't need to rely so heavily on the ConsumerCoordinator properly updating its SubscriptionState after eg falling out of the group. The plain StickyAssignor always used the generation since it had to, so we just make sure the CooperativeStickyAssignor has this tool as well 2) In case of unforeseen problems or further bugs that slip past the `generation` field safety net, the assignor will now explicitly look out for partitions that are being claimed by multiple consumers as owned in the same generation. Such a case should never occur, but if it does, we have to invalidate this partition from the `ownedPartitions` of both consumers, since we can't tell who, if anyone, has the valid claim to this partition. 3) Fix a subtle bug that I discovered while writing tests for the above two fixes: in the constrained algorithm, we compute the exact number of partitions each consumer should end up with, and keep track of the ""unfilled"" members who must -- or _might_ -- require more partitions to hit their quota. The problem was that members at the `minQuota` were being considered as ""unfilled"" even after we had already hit the maximum number of consumers allowed to go up to the `maxQuota`, meaning those `minQuota` members could/should not accept any more partitions beyond that. I believe this was introduced in [#10509](https://github.com/apache/kafka/pull/10509), so it shouldn't be in any released versions and does not need to be backported.","closed","","ableegoldman","2021-07-07T04:21:10Z","2021-07-14T01:58:06Z"
"","10516","KAFKA-12555 - Log all reason for rolling a segment","The original ticket was to debug log the reason for rolling a new log segment. based on LogSegment.shouldRoll, there are 5 conditions for rolling:  1. Segment exceeds max byte size 2. Segment exceeds time limit 3. Offset index is full 4. Timestamp index is full 5. The offsets can no longer be converted to relative offsets  And the first 4 were already trackable from the existing debug log message, but 5 was missing so I'm suggesting to add it in the existing one.  There was also an extra `}`  that's a typo in the log string  that I've removed.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","Comonut","2021-04-09T18:02:31Z","2022-03-01T15:30:43Z"
"","11015","KAFKA-12333:KafkaMetadataLog and MockLog should validate that appended epochs are monotonically","The original issue said both the MockLog and KafkaMetadataLog should only allow appendAsLeader and appendAsFollower with monotonically increasing epochs. However, It seems no need to validate epoch for appendAsFollower as far as I understand through comments. So I'm not sure  which one is correct.  @jsancio Could you please take a look and give some advice.        /**      * Append a set of records that were replicated from the leader. The main      * difference from appendAsLeader is that we do not need to assign the epoch      * or do additional validation.      *      * @return the metadata information of the appended batch      * @throws IllegalArgumentException if the record set is empty      * @throws RuntimeException if the batch base offset doesn't match the log end offset      */     LogAppendInfo appendAsFollower(Records records);   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","zhaohaidao","2021-07-10T08:30:02Z","2021-12-18T12:06:59Z"
"","11110","MINOR: move tiered storage related configs to a separate class within LogConfig","The original code uses a `RemoteLogManagerConfig` class to store KIP-405 configs and adds three configs to `LogConfig`. This makes the code complicated and developers may be confused.  This PR allows us to access `RemoteLogManagerConfig` from `KafkaConfig` and do the same for `LogConfig`. Kafka developers will see the same interface for the KIP-405 configs. After this change, if we want to read `remoteStorageEnable` we should use `LogConfig.tieredLogConfig.remoteStorageEnable` instead of `LogConfig.remoteStorageEnable`. The same for `localRetentionMs` and `localRetentionBytes`. If we want to read configs in `RemoteLogManagerConfig`, we should use `KafkaConfig.tieredKafkaConfig.xxx`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-07-22T19:30:37Z","2021-08-27T19:00:13Z"
"","10529","KAFKA-12650: fix NPE in InternalTopicManagerTest","The NPE arrises, if we cannot setup topics quickly enough, and run into a timeout -- for this case, we try to call `admit.delete` (note that `admin` is mocked) that returns `null`.  However, we should never actually ""abort"" the internal topic creation. -- Using `MockTime` instead of `SystemTime` should avoid this issue, as time won't advance at all. Or would there be any concern with regard to running forever? An alternative approach could be, to increase `max.poll.interval.ms` that defines the timeout.  Call for review @bbejeck @ableegoldman","closed","tests,","mjsax","2021-04-12T17:21:04Z","2021-04-14T18:45:48Z"
"","10572","KAFKA-12697: Add OfflinePartitionCount and PreferredReplicaImbalanceCount metrics to Quorum Controller","The metrics are calculated by counting records as they are replayed e.g. replay(TopicRecord), replay(RemoveTopicRecord)  This was unit tested using MockControllerMetrics.  https://issues.apache.org/jira/browse/KAFKA-12697","closed","","dielhennr","2021-04-21T00:53:42Z","2021-05-20T23:28:33Z"
"","11092","MINOR: Fix testTlsDefaults failure due to TLS 1.0/1.1 being disabled","The latest JDKs no longer support TLS 1.0/1.1 causing the test to fail. We have already fixed this in trunk and 3.0, so this is for 2.8 and older branches.  The relevant trunk commit is 530224e4fe. We had another test with the same issue and it was fixed for all branches via #10922.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-20T13:17:59Z","2021-07-20T21:18:43Z"
"","11168","KAFKA-13160: Fix BrokerMetadataPublisher to pass the correct resource name to the config handler when processing config updates","The KRaft brokers are throwing NumberFormatException when processing dynamic default broker config updates because they expect the default entity name that was used in zookeeper to be the resource name for dynamic default broker configs instead of empty string.   https://issues.apache.org/jira/browse/KAFKA-13160","closed","","dielhennr","2021-08-03T18:53:31Z","2021-08-19T05:29:52Z"
"","11243","MINOR: Fix minor typo in the Admin API Javadoc example","The Javadoc of the Admin API (`Admin` interface) has an example of how to use the Admin API to create a topic. The example does not work because it is missing one `)`. This small PR fixes it.","closed","","scholzj","2021-08-20T13:25:25Z","2021-08-23T09:11:04Z"
"","11419","MINOR: Javadoc formatting for MetricsContext","The Javadoc for MetricsContext wasn't correctly formatted for nice/readable HTML output.","closed","","tombentley","2021-10-20T13:01:28Z","2021-10-20T13:14:48Z"
"","10939","Avoid increasing app ID when test is executed multiple times","The integration test TaskMetadataIntegrationTest will increase the length of the app ID when its test methods are called multiple times in one execution. This is for example the case if you repeatedly run the test until failure in IntelliJ IDEA. This might also lead to exceptions because the state directory depends on the app ID and directory names have a length limit.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-06-29T11:53:42Z","2021-06-30T07:55:27Z"
"","10491","MINOR: Switch to using the Gradle RAT plugin","The Gradle RAT plugin properly declares inputs and outputs and is also cachable. This also relieves the Kafka developers from maintaining the build integration with RAT.  The generated RAT report is identical to the one generated previously. The only difference is the RAT report name: the RAT plugin sets the HTML report name to `index.html` (still under `build/rat`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jthurne","2021-04-06T19:19:03Z","2021-04-13T04:05:50Z"
"","10622","MINOR: Remove unused Utils.delete","The function was only used to walk around a Windows ""feature"", and now is no longer used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-05-01T03:24:10Z","2021-05-01T15:32:27Z"
"","10650","KAFKA-12763 NoSuchElementException during checkpointLogStartOffsets","The following exception shows that `log.logSegments` may be empty during `checkpointLogStartOffsets`. `logSegments.headOption` should be used instead of `logSegments.head` to prevent this exception.  ```json {   ""class"": ""java.util.NoSuchElementException"",   ""msg"": null,   ""stack"": [     ""java.util.concurrent.ConcurrentSkipListMap$ValueIterator.next(ConcurrentSkipListMap.java:2123)"",     ""scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.next(JavaCollectionWrappers.scala:38)"",     ""scala.collection.IterableOps.head(Iterable.scala:218)"",     ""scala.collection.IterableOps.head$(Iterable.scala:218)"",     ""scala.collection.AbstractIterable.head(Iterable.scala:920)"",     ""kafka.log.LogManager$$anonfun$4.applyOrElse(LogManager.scala:640)"",     ""kafka.log.LogManager$$anonfun$4.applyOrElse(LogManager.scala:639)"",     ""scala.collection.Iterator$$anon$7.hasNext(Iterator.scala:516)"",     ""scala.collection.mutable.Growable.addAll(Growable.scala:61)"",     ""scala.collection.mutable.Growable.addAll$(Growable.scala:59)"",     ""scala.collection.mutable.HashMap.addAll(HashMap.scala:111)"",     ""scala.collection.mutable.HashMap$.from(HashMap.scala:549)"",     ""scala.collection.mutable.HashMap$.from(HashMap.scala:542)"",     ""scala.collection.MapFactory$Delegate.from(Factory.scala:425)"",     ""scala.collection.MapOps.collect(Map.scala:283)"",     ""scala.collection.MapOps.collect$(Map.scala:282)"",     ""scala.collection.AbstractMap.collect(Map.scala:375)"",     ""kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$2(LogManager.scala:639)"",     ""kafka.log.LogManager.$anonfun$checkpointLogStartOffsetsInDir$1(LogManager.scala:636)"",     ""kafka.log.LogManager.checkpointLogStartOffsetsInDir(LogManager.scala:635)"",     ""kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1(LogManager.scala:600)"",     ""kafka.log.LogManager.$anonfun$checkpointLogStartOffsets$1$adapted(LogManager.scala:600)"",     ""scala.collection.IterableOnceOps.foreach(IterableOnce.scala:553)"",     ""scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:551)"",     ""scala.collection.AbstractIterable.foreach(Iterable.scala:920)"",     ""kafka.log.LogManager.checkpointLogStartOffsets(LogManager.scala:600)"",     ""kafka.log.LogManager.$anonfun$startup$6(LogManager.scala:426)"",     ""kafka.utils.KafkaScheduler.$anonfun$schedule$2(KafkaScheduler.scala:114)"",     ""java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)"",     ""java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)"",     ""java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)"",     ""java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)"",     ""java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)"",     ""java.lang.Thread.run(Thread.java:834)""   ] } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","soarez","2021-05-07T16:04:59Z","2021-05-22T12:30:50Z"
"","10661","MINOR: upgrade pip from 20.2.2 to 21.1.1","The following error happens on my mac m1 when building docker image for system tests.  ``` Collecting pynacl   Using cached PyNaCl-1.4.0.tar.gz (3.4 MB)   Installing build dependencies ... error   ERROR: Command errored out with exit status 1:    command: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-k867aac0/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools>=40.8.0' wheel 'cffi>=1.4.1; python_implementation != '""'""'PyPy'""'""''        cwd: None   Complete output (14 lines):   Traceback (most recent call last):     File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main       return _run_code(code, main_globals, None,     File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code       exec(code, run_globals)     File ""/usr/local/lib/python3.8/dist-packages/pip/__main__.py"", line 23, in        from pip._internal.cli.main import main as _main  # isort:skip # noqa     File ""/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py"", line 5, in        import locale     File ""/usr/lib/python3.8/locale.py"", line 16, in        import re     File ""/usr/lib/python3.8/re.py"", line 145, in        class RegexFlag(enum.IntFlag):   AttributeError: module 'enum' has no attribute 'IntFlag'   ---------------------------------------- ERROR: Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.8/dist-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-k867aac0/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'setuptools>=40.8.0' wheel 'cffi>=1.4.1; python_implementation != '""'""'PyPy'""'""'' Check the logs for full command output. ```  There was a related issue: https://github.com/pypa/pip/pull/9689 and it is already fixed by https://github.com/pypa/pip/pull/9689 (included by pip 21.1.1). I test the pip 21.1.1 and it works well on mac m1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-10T11:05:02Z","2021-07-08T22:23:09Z"
"","10841","KAFKA-12482 Remove deprecated rest.host.name and rest.port configs","The following Connect worker configuration properties were deprecated  3 years ago and 3.0.0 seems like a good major release to remove them as part of this PR:  - rest.host.name (deprecated in KIP-208) - rest.port (deprecated in KIP-208)  Ran connect unit and integration tests locally  @rhauch @kkonstantine @C0urante could you please review and see if this looks good?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kpatelatwork","2021-06-08T15:15:24Z","2021-06-23T14:20:04Z"
"","10917","KAFKA-10847: improve throughput of stream-stream join with spurious left/outer join fix","The fix to avoid spurious left/outer stream-stream join results, showed very low throughput for RocksDB, due to excessive creation of iterators. Instead of trying to emit left/outer stream-stream join result for every input record, this PR adds tracking of the lower timestamp bound of left/outer join candidates, and only tries to emit them (and create an iterator) if they are potentially old enough.  In our benchmarks, we use a 1-sec join window for inner/left/outer join. Inner join is always stable at 20K rec/sec using RocksDB and 35K rec/sec using in-memory store. Without this fix, throughput for left/outer join with RocksDB drops to 500 rec/sec. Inner join, and in-memory store are not affected. With this fix, we get left/outer join with RocksDB back up to 12K rec/sec.  Call for review @guozhangwang @spena @vcrfxia","closed","streams,","mjsax","2021-06-22T21:43:38Z","2021-07-01T22:46:45Z"
"","10862","KAFKA-12928: Add a check whether the Task's statestore is actually a directory","The first commit shows how to reproduce the problem, and the second commit is the fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-06-10T09:50:53Z","2021-06-22T23:35:31Z"
"","11104","KAFKA-13079: Forgotten Topics in Fetch Requests may incorrectly use topic IDs","The FetchSessionHandler had a small bug in the session build method where we did not consider building a session where no partitions were added and the session previously did not use topic IDs. (ie, it was relying on at least one partition being added to signify whether topic IDs were present)  Due to this, we could send forgotten partitions with the zero UUID. This would always result in an exception and closed session.  This PR fixes the logic to check that any forgotten partitions have topic IDs. There is also a test added for the empty session situation when topic IDs are used and when topic names are used.  I also fixed a few small things in some of the previous tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-21T22:30:04Z","2021-08-27T16:28:22Z"
"","10690","MINOR: clarify message ordering with max in-flight requests and idempotent producer","The docs for the `max.in.flight.requests.per.connection` and `enable.idempotence` configs currently imply that setting the max in-flight request greater than 1 will break the message ordering guarantee, but that is only true if `enable.idempotence` is false. When using an idempotent producer, the max in-flight request can be up to 5 without re-ordering messages.  See [this discussion](https://issues.apache.org/jira/browse/KAFKA-12776?focusedCommentId=17344185&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17344185)","closed","","ableegoldman","2021-05-14T01:17:31Z","2021-05-24T23:14:58Z"
"","10617","KAFKA-12734: LazyTimeIndex & LazyOffsetIndex may cause niobufferoverflow when skip activeSegment sanityCheck","The detailed stack information is linked to this jiraId [KAFKA-12734](https://issues.apache.org/jira/browse/KAFKA-12734)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-30T03:15:33Z","2021-05-03T17:08:24Z"
"","10966","MINOR: fix comments on deleteTopics method","The deleteTopics call with topic IDs is actually able to be handled by brokers 2.8 or higher. Of course, the call will only be successful if topic IDs are present which requires IBP 2.8 or higher. I've adjusted the comment to reflect this.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-03T16:00:48Z","2021-07-05T07:25:18Z"
"","11182","KAFKA-13074: Implement mayClean for MockLog","The current implement of MockLog doesn't implement maybeClean. It is expected that MockLog has the same semantic as KafkaMetadataLog. This is assumed to be true for a few of the tests suite like the raft simulation and the kafka raft client test context.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","zhaohaidao","2021-08-05T15:37:32Z","2021-12-18T12:22:17Z"
"","10834","KAFKA-12904: Corrected the timeout for config validation REST API resource","The constant is specified in milliseconds, and so the MILLISECOND time unit should be used instead of SECONDS. See #8069/[KAFKA-9374](https://issues.apache.org/jira/browse/KAFKA-9374).  Inspected other uses of the same constant and other changes in the original PR, and found no other errors. This kind of change is actually difficult to test for, so this relies upon existing unit and integration tests.  Users may run into this whenever validating a connector configuration where the connector implementation takes more than the 90 seconds to actually validate the configuration.  * Without this fix, the `PUT /connector-plugins/(string:name)/config/validate` REST requests might **_not_** return `500 Internal Server Error` and may block (the request thread) for a long period of time.  * With this fix, the `PUT /connector-plugins/(string:name)/config/validate` REST requests might **_not_** return `500 Internal Server Error` if the connector does not complete the validation of a connector configuration within 90 seconds.  The user will not see a difference between the behavior before or after this fix if/when the connectors complete validation of connector configurations before 90 seconds, since the method will return those results to the client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-06-07T15:50:09Z","2021-06-22T14:15:14Z"
"","10833","Corrected the timeout for config validation REST API resource","The constant is specified in milliseconds, and so the MILLISECOND time unit should be used instead of SECONDS. See #8069.  Inspected other uses of the same constant and other changes in the original PR, and found no other errors. This kind of change is actually difficult to test for, so this relies upon existing unit and integration tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2021-06-07T15:42:34Z","2021-06-07T16:33:44Z"
"","10585","MINOR: cleanTest ought to remove output of unitTest task and integrat…","The command used by our private CI is `./gradlew cleanTest xxx:test`. It does not re-run test when we use `unitTest` and `integrationTest` to replace `test`. The root cause is that we don't offer test output (`unitTest` and `integrationTest`) to `cleanTest` task and so it does not delete related test output.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-04-22T04:11:16Z","2021-06-01T04:31:10Z"
"","10791","MINOR: replace by org.junit.jupiter.api.Tag by net.jqwik.api.Tag for …","The command `./gradlew raft:integrationTest`  can't run any integration test since `org.junit.jupiter.api.Tag` does not work for jqwik engine (see https://github.com/jlink/jqwik/issues/36#issuecomment-436535760).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-29T08:16:58Z","2021-06-01T05:03:26Z"
"","11239","KAFKA-13219: BrokerState metric not working for KRaft clusters","The BrokerState metric always has a value of 0, for NOT_RUNNING, in KRaft clusters. This patch fixes it and adds a test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-08-19T21:57:50Z","2021-08-23T21:22:18Z"
"","10990","MINOR: the broker should use metadata.log.max.record.bytes.between.snapshots","The broker should trigger a snapshot once metadata.log.max.record.bytes.between.snapshots has been exceeded.","closed","","cmccabe","2021-07-07T17:30:30Z","2021-07-09T19:00:29Z"
"","10908","MINOR: fix round_trip_fault_test.py - don't assign replicas to nonexi…","The broker id starts with 1 (https://github.com/apache/kafka/blob/trunk/tests/kafkatest/services/kafka/kafka.py#L207) so `round_trip_fault_test.py` fails because it assigns replica to nonexistent broker.  The interesting story is the failure happens only on KRaft only. KRaft mode checks the existent ids (https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java#L950). By contrast, ZK mode has no such check and the `min.insync.replicas` is set to `1` so this test works with ZK mode even though there is one replica is always off-line.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-06-19T13:53:32Z","2021-06-21T16:28:23Z"
"","11248","HOTFIX: Fix null pointer when getting metric value in MetricsReporter","The alive stream threads metric relies on the threads field as a monitor object for its synchronized block. When the alive stream threads metric is registered it isn't initialised so any call to get the metric value before it is initialised will result in a null pointer exception.  This is tested in a minimal integration test where the KafkaStreams object is created  but not started and the test loops through all metric values to check they're not null.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","PhilHardwick","2021-08-21T10:04:12Z","2021-08-25T01:28:23Z"
"","10677","KAFKA-12380 Executor in Connect's Worker is not shut down when the worker is","The `Worker` class has an `executor` field that the public constructor initializes with a new cached thread pool (https://github.com/apache/kafka/blob/02226fa090513882b9229ac834fd493d71ae6d96/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L127.]).  When the worker is stopped, it does not shutdown this executor. This is normally okay in the Connect runtime and MirrorMaker 2 runtimes, because the worker is stopped only when the JVM is stopped (via the shutdown hook in the herders).  However, we instantiate and stop the herder many times in our integration tests, and this means we're not necessarily shutting down the herder's executor. Normally this won't hurt, as long as all of the runnables that the executor threads run actually do terminate. But it's possible those threads might not terminate in all tests.  @kkonstantine can you review this? many thanks!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","DuongPTIT","2021-05-12T08:10:39Z","2021-06-23T05:55:55Z"
"","11164","KAFKA-13155; Fix concurrent modification in consumer shutdown","The `TransactionalMessageCopier` tool, which is used in system tests attempts to close the consumer as part of a shutdown hook. Although the access is synchronized, there is no guarantee that the consumer has finished polling when shutdown is invoked. The patch fixes the problem by call `wakeup()` from the shutdown hook and pushing the call to `close()` to the main thread.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-08-02T23:23:00Z","2021-08-11T13:13:15Z"
"","10886","MINOR: TestSecurityRollingUpgrade system test fixes","The `TestSecurityRollingUpgrade. test_disable_separate_interbroker_listener()` system test had a design flaw: it was migrating inter-broker communication from a SASL_SSL listener to an SSL listener in one roll while immediately removing the SASL_SSL listener in that roll.  This requires two rolls because the existing SASL_SSL listener must remain available throughout the first roll so that unrolled brokers can continue to communicate with rolled brokers throughout.  This patch adds the second roll to this test and removes the original SASL_SSL listener on that second roll instead of the first one.  The test was not failing all the time -- it was flaky.  The `TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two()` system test was not explicitly identifying the SASL mechanism to enable on a third port when that port was using SASL but the client security protocol was not SASL-based.  This was resulting in an empty `sasl.enabled.mechanisms` config, which applied to that third port, and then when the cluster was rolled to take advantage of this third port for inter-broker communication the potential for an inability to communicate with other, unrolled brokers existed (similar to above, this resulted in a flaky test).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-06-15T17:09:05Z","2021-06-18T07:50:21Z"
"","10547","KAFKA-12284: increase request timeout to make tests reliable","The `MirrorConnectorsIntegrationTests` recently failed with  ``` TimeoutException: The request timed out. ``` quite frequently. After investigation, the reason is the `createTopic` in the server doesn't complete within `request.timeout.ms` (default to 30 seconds). It makes sense that the server is pretty slow during 2 connector cluster (primary and backup) started at the same time. What I did are: 1. increase the timeout value to 60 seconds to make the tests reliable. 2. wait for internal topics created after connector cluster started, and then try to create our test topics.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-16T06:27:13Z","2021-04-28T15:56:49Z"
"","10541","KAFKA-12284: increase request timeout to make tests reliable","The `MirrorConnectorsIntegrationTests` recently failed with  ``` TimeoutException: The request timed out. ``` quite frequently. After investigation, the reason is the `createTopic` in the server doesn't complete within `request.timeout.ms` (default to 30 seconds). It makes sense that the server is pretty slow during 2 connector cluster (primary and backup) started at the same time. We increase the timeout value to 60 seconds to make the tests reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-15T07:06:40Z","2021-04-16T10:08:47Z"
"","11354","MINOR: Print lastTimestamp when dumping producer snapshots","The `LastTimestamp` field is useful because its value is present even when there are no data batches written by a given producerId.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-09-22T23:27:56Z","2021-09-23T17:07:33Z"
"","10868","MINOR: make sure alterAclsPurgatory is closed when controller server …","The `alterAclsPurgatory` can get closed by kafka broker so Kraft should close it as well.  related to #10550  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-06-11T19:34:12Z","2021-07-17T07:00:04Z"
"","10986","KAFKA-12983: reset needsJoinPrepare flag before rejoining the group","The `#onJoinPrepare` callback is not always invoked before a member (re)joins the group, but only once when it first enters the rebalance. This means that any updates or events that occur during the join phase can be lost in the internal state: for example, clearing the SubscriptionState (and thus the ""ownedPartitions"" that are used for cooperative rebalancing) after losing its memberId during a rebalance.  We should reset the `needsJoinPrepare` flag inside the resetStateAndRejoin() method. Should be cherrypicked back to 2.8 at least","closed","","ableegoldman","2021-07-07T04:33:45Z","2021-07-13T19:29:07Z"
"","11180","MINOR: Fix testResolveDnsLookupResolveCanonicalBootstrapServers","testResolveDnsLookupResolveCanonicalBootstrapServers added in https://github.com/apache/kafka/pull/11091 treats the result from ClientUtils.resolve() when using specifying RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY as if it should differ from when specifying DEFAULT, yet the branching in resolve() is on ClientDnsLookup.USE_ALL_DNS_IPS == clientDnsLookup -- everything else gets the result squashed to a singletonList.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","shayelkin","2021-08-05T08:15:01Z","2021-08-08T06:29:19Z"
"","11240","KAFKA-13175; test","test  Signed-off-by: yangdaixai   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangdaixai","2021-08-20T04:39:24Z","2021-08-20T06:44:06Z"
"","10816","[WIP] MINOR: acl tests","test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-04T03:53:55Z","2021-06-29T12:19:39Z"
"","11392","freeze requirements for system-tests running with python2","system tests fail to run as they are still ran with python2, and pip installs newer dependancies.  By freezing the dependancies we should continue to be able to run python2 tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify test coverage and CI build status - [x] Verify system test run successfully","open","","imcdo","2021-10-12T17:33:40Z","2021-10-12T23:56:16Z"
"","11109","KAFKA-13113: Support unregistering Raft listeners","Support unregistering by returning a ListenerContext on registration and exposing a close method on the returned ListenerContext. To allow the user to use the same Listener on different registrations the associated ListenerContext is sent through all of the methods described by the Raft Listener.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-22T17:26:32Z","2021-07-24T17:27:39Z"
"","10753","KAFKA-12803: Support reassigning partitions when in KRaft mode","Support the KIP-455 reassignment API when in KRaft mode. Reassignments which merely rearrange partitions complete immediately. Those that only remove a partition complete immediately if the ISR would be non-empty after the specified removals. Reassignments that add one or more partitions follow the KIP-455 pattern of adding all the adding replicas to the replica set, and then waiting for the ISR to include all the new partitions before completing. Changes to the partition sets are accomplished via PartitionChangeRecord.","closed","kip-500,","cmccabe","2021-05-24T18:45:58Z","2021-08-09T19:08:39Z"
"","10922","KAFKA-12790: Remove SslTransportLayerTest.testUnsupportedTlsVersion","Support for TLS 1.0/1.1 was disabled in recent versions of Java 8/11 and all versions of 16 causing this test to fail.  It is possible to make it work by updating the relevant security property, but it has to be done before the affected classes are loaded and it can not be disabled after that. Given the low value of the test, we remove it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-06-23T13:22:51Z","2021-07-03T16:19:00Z"
"","10952","KAFKA-12257:  Consumer mishandles topics deleted and recreated with the same name","Store topic ID info in consumer metadata. We will always take the topic ID from the latest metadata response and remove any topic IDs from the cache if the metadata response did not return a topic ID for the topic.   With the addition of topic IDs, when we encounter a new topic ID (recreated topic) we can choose to get the topic's metadata even if the epoch is lower than the deleted topic.  The idea is that when we update from no topic IDs to using topic IDs, we will not count the topic as new (It could be the same topic but with a new ID). We will only take the update if the topic ID changed.  Added tests for this scenario as well as some tests for storing the topic IDs. Also added tests for topic IDs in metadata cache.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-01T19:41:14Z","2021-07-15T23:31:43Z"
"","11216","KAFKA-13198: Stop replicas when reassigned","Stop the replica and resign the coordinators when a replica gets reassigned away from a topic partition.  1. Implement `localChanges` in `TopicsDelta` and `TopicDelta` to return all of the partitions that were deleted, became leader and became follower for the given broker id. 2. Add tests for `TopicsDelta::localChanges`  3. Resign coordinators that were moved away from the consumer offset and transaction topic partitions. 4. Add replica manager tests for testing reassignment of replicas and removal of topic. 5. Add a new type `LocalReplicaChanges` that encapsulates topic partitions deleted, became leader and became follower.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-08-16T02:41:48Z","2021-08-17T20:31:06Z"
"","11309","MINOR: Remove unsupported rsync and ssh commands from release.py","ssh and rsync access has been removed from home.apache.org. Removing the commands from release.py and replacing them with a note to make sure they are manually uploaded with an sftp client instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2021-09-08T17:32:10Z","2021-09-09T05:50:47Z"
"","11069","KAFKA-12777 auto topic creation manager npe","Split off from #10696, but only contains the bug fix.  There are some cases where a response will include a `null` body, so we need to check for those cases before printing this log statement.","closed","","mumrah","2021-07-16T17:32:15Z","2021-07-16T20:17:32Z"
"","11152","MINOR: allow for reties of other InvalidStateStoreExceptions","Sometimes the test would hit a different InvalidStateStoreExcpetion but if retied it would still work. I think we can remove the check and log the exception. If the test recovers then it is fine. But if it times-out the exception will be in the logs.   The other exception is:  java.lang.AssertionError: org.apache.kafka.streams.errors.InvalidStateStoreException: The state store, source-table, may have migrated to another instance.  I ran the test 300 times failing on any exception but checking if it would timeout or have recovered. 13 times it hit an exception 10 of the one check for 3 of the one failed. It never timed-out and in all cases recovered after a retry.   I then ran the test 3117 times and it did not fail once where it would have failed about 1:100 times previously.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-07-31T00:21:34Z","2021-07-31T00:24:37Z"
"","10838","MINOR: small fix and clean up for DynamicConfigManager","Some refactor to DynamicConfigManager class. And add tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-06-08T03:45:13Z","2021-06-30T01:26:40Z"
"","11312","KAFKA-13224: Ensure that broker.id is set in KafkaConfig#originals","Some plugins make use of KafkaConfig#originals rather than the KafkaConfig object. We should ensure that these plugins see the correct value for broker.id if the broker is running in KRaft mode and node.id has been configured, but not broker.id.  This PR does this by ensuring that both node.id and broker.id are set in the orignals map if either one is set.  We also check that they are set to the same value in KafkaConfig#validateValues.  Co-author: Ron Dagostino","closed","kip-500,","cmccabe","2021-09-08T22:34:39Z","2021-09-13T17:14:05Z"
"","11298","MINOR: ""partition"" typos and method doc arg fix","Some minor typos that have annoyed me along with a fix to a docstring.","closed","","lbradstreet","2021-09-06T01:29:31Z","2021-10-18T08:44:12Z"
"","11019","KAFKA-13059: Make DeleteConsumerGroupOffsetsHandler unmap for COORDINATOR_NOT_AVAILABLE error and fix issue","Some issues found in the `DeleteConsumerGroupOffsetsHandler`: 1. if `coordinator errors` is put in the topic partition, plus a Errors.NONE, we'll failed with `IllegalArgumentException: Partition foo was not included in the original request`. This is the new added test case scenario: `testDeleteConsumerGroupOffsetsResponseIncludeCoordinatorErrorAndNoneError` 2. Didn't handle all possible exceptions, so there will be ""expected"" exception, but be logged as ""unexpected exception"" 4. In `DeleteConsumerGroupOffsetsHandlerTest`, we build all errors in partition result, including group error. Split group error tests and partition error tests.  This is the old handle response logic. FYR: ```java void handleResponse(AbstractResponse abstractResponse) {     final OffsetDeleteResponse response = (OffsetDeleteResponse) abstractResponse;      // If coordinator changed since we fetched it, retry     // note: we use `errorCounts` to collect all errors in the response, including partition errors.     if (ConsumerGroupOperationContext.hasCoordinatorMoved(response)) {         Call call = getDeleteConsumerGroupOffsetsCall(context, partitions);         rescheduleFindCoordinatorTask(context, () -> call, this);         return;     }      // If the error is an error at the group level, the future is failed with it     final Errors groupError = Errors.forCode(response.data().errorCode());     if (handleGroupRequestError(groupError, context.future()))         return;      final Map partitions = new HashMap<>();     response.data().topics().forEach(topic -> topic.partitions().forEach(partition -> partitions.put(         new TopicPartition(topic.name(), partition.partitionIndex()),         Errors.forCode(partition.errorCode())))     );      context.future().complete(partitions); } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-12T06:56:43Z","2021-07-15T12:19:46Z"
"","11208","MINOR: Refactored BrokerHeartbeatManager::findOneStaleBroker to not use iterator","Some code refactoring based on https://github.com/apache/kafka/pull/11191 feedback","open","","niket-goel","2021-08-12T17:16:38Z","2021-08-13T16:22:51Z"
"","11322","smt to rename schemas based on regex","smt to rename schemas based on regex  example configuration:  ```     ""transforms.regexSchema.regex"": "".*\\.([^.]*)\\.(Value|Key)"",     ""transforms.regexSchema.replacement"": ""com.company.schema.$1.$2"", ```  We need this as our debezium connector creates schemas with the name of the host and database schema  When we move from dev to production these names change Using Avro the the name in the schema is used to pick the class and thus will fail when we move between environments","open","kip,","msillence","2021-09-13T10:16:24Z","2022-04-20T00:35:49Z"
"","11081","MINOR: Typo in RaftClient Javadoc","Small typo in RaftClient javadoc.","closed","","dielhennr","2021-07-19T20:40:26Z","2021-07-19T21:14:34Z"
"","11079","MINOR: Small refactoring in admin group handlers","Small refactoring to make the code uniform across the newly introduced admin group handlers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-07-19T15:36:46Z","2021-07-22T08:28:56Z"
"","11308","KAFKA-13277; Fix size calculation for tagged string fields in message generator","Size calculation for tagged fields is currently incorrect and works only for small strings. This results in BufferOverflowException when serializing requests with large strings in tagged fields. The PR fixes size calculation to match the bytes written.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-09-07T16:28:14Z","2021-09-08T07:58:15Z"
"","10635","KAFKA-9295: increase start stream timeout","Since we need to wait for 3 streams reach `RUNNING` state, it makes sense to increase the waiting time to make the test reliable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-05-06T00:53:57Z","2021-05-06T05:00:44Z"
"","10994","KAFKA-8410: Update the docs to reference the new PAPI","Since the old Processor API is now deprecated, we need to update the documentation to steer people to the new API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vvcephei","2021-07-07T22:17:45Z","2021-07-14T20:55:15Z"
"","11259","HOTFIX: decrease session timeout in flaky NamedTopologyIntegrationTest","Since the default session timeout was bumped to 45s a number of our integration tests have begun failing. Let's reset it back to 10s for these NamedTopologyIntegrationTests which have been a bit flaky to help parse out whether it's just environmental, or possibly something more...sinister 😈","closed","","ableegoldman","2021-08-25T23:41:02Z","2021-08-26T04:52:33Z"
"","11112","MINOR: only request rejoin and log if necessary for metadata snapshot and subscription checks","Since now we call do not necessarily complete the rebalance within a poll call, we may keep checking the `rejoinNeededOrPending` which hits either of the conditions and returns true, but then returns early, resulting in flooding log entries. This PR would only log/set the flag when it was not set yet, effectively only logging for the first time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-07-23T01:06:55Z","2021-07-23T05:47:19Z"
"","10826","KAFKA-7632: Support Compression Level","Since I reworked [KIP-390](https://cwiki.apache.org/confluence/display/KAFKA/KIP-390%3A+Support+Compression+Level) from scratch, here I open a new PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-06-05T16:32:13Z","2022-07-08T20:13:46Z"
"","11210","KAFKA-13199: Make Task extends Versioned","Since `Task` is versioned, we can make it extends `Versioned` directly, no need to introduce `String version()` again  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wangxianghu","2021-08-13T05:50:04Z","2021-08-30T08:37:14Z"
"","11336","DRAFT: Add currentPosition and times to ProcessorContext","Similar to the currentStreamTime/currentSystemTime methods in KIP-622, this PR demonstrates adding a currentPositions call to the ProcessorContext, which returns the task's current offset position in each of its input topic partitions.  This is a fairly advanced requirement, which most applications would never need, but if an application does need to know the current position, there is no other way they could compute it.  If the initial reception to this POC is positive, I'll follow up with a KIP.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-09-17T21:10:39Z","2022-03-31T18:02:39Z"
"","11133","KAFKA-13140: KRaft brokers do not expose kafka.controller metrics","Several controller metrics are exposed on every broker in a ZooKeeper-based (i.e. non-KRaft) cluster regardless of whether the broker is the active controller or not, but these metrics are not exposed on KRaft nodes that have process.roles=broker (i.e. KRaft nodes that do not implement the controller role). For backwards compatibility, KRaft nodes that are just brokers should expose these metrics with values all equal to 0: just like ZooKeeper-based brokers do when they are not the active controller.  This patch adds these metrics and an associated test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-27T15:09:33Z","2021-11-15T14:22:40Z"
"","10824","KAFKA-12718: SessionWindows are closed too early","Session windows should be closed only after `streamTime` reaches `windowEnd + gap + grace`.","closed","streams,","gonzur","2021-06-05T02:09:37Z","2021-07-26T23:38:38Z"
"","10896","KAFKA-12964: Collect and rename snapshot files prior to async deletion.","Segment and index files are currently renamed with a .deleted suffix prior to async deletion. This serves two purposes, to resume deletion on broker failure and also protect against deletion of new segments during truncation (due to deletion being async).  We should do the same for snapshot files. While they are not subject to issues around resuming deletion due to the stray snapshot scanning which is performed on log initialization, we can end up with situations where truncation queues snapshots for deletion, but prior to deletion new segments with the same snapshot file name are created. Async deletion can then delete these new snapshots.  This patch offers a two-stage snapshot deletion which first renames and removes the segments in question from the ProducerStateManager, allowing the Log to asynchronously delete them.  Credit to Kowshik Prakasam  for finding this issue and creating the test demonstrating the failure.  Co-authored-by: Kowshik Prakasam","closed","","gardnervickers","2021-06-17T15:04:37Z","2021-07-01T21:23:59Z"
"","10581","HOTFIX: kafka streams lib missing in dependencies.gradle","Seems this was missed to add during the original 2.6.0 release","closed","","ableegoldman","2021-04-21T19:38:54Z","2021-04-21T22:35:05Z"
"","11400","HOTFIX: suppress deprecation warnings to fix compilation errors","Seems `trunk` was broken via https://github.com/apache/kafka/pull/11188  Unclear why Jenkins passed on #11188","closed","streams,","mjsax","2021-10-14T20:55:10Z","2021-10-15T18:36:03Z"
"","11388","KAFKA-13361: Support fine-grained compression options","see: [KIP-780: Support fine-grained compression options](https://cwiki.apache.org/confluence/display/KAFKA/KIP-780%3A+Support+fine-grained+compression+options)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-10-10T13:28:12Z","2021-10-10T13:28:12Z"
"","10876","KAFKA-12843: KIP-740 follow up: clean up TaskMetadata","See KIP-740 – for the TaskMetadata class, we need to:  Deprecate the TaskMetadata#getTaskId method ""Remove"" the deprecated TaskMetadata#taskId method, then re-add a taskId() API that returns a TaskId instead of a String Remove the deprecated constructor","closed","","socutes","2021-06-13T08:05:48Z","2021-11-22T06:33:04Z"
"","10875","KAFKA-12844: KIP-740 follow up: clean up TaskId","See KIP-740 – for the TaskId class, we need to remove the following deprecated APIs:  The public partition and topicGroupId fields should be ""removed"", ie made private (can also now rename topicGroupId to subtopology to match the getter) The two #readFrom and two #writeTo methods can be removed (they have already been converted to internal utility methods we now use instead, so just remove them)","closed","","socutes","2021-06-13T07:53:36Z","2021-11-22T06:32:59Z"
"","10874","KAFKA-12844: KIP-740 follow up: clean up TaskId","See KIP-740 – for the TaskId class, we need to remove the following deprecated APIs:  The public partition and topicGroupId fields should be ""removed"", ie made private (can also now rename topicGroupId to subtopology to match the getter) The two #readFrom and two #writeTo methods can be removed (they have already been converted to internal utility methods we now use instead, so just remove them)","closed","","socutes","2021-06-13T07:32:09Z","2021-11-22T06:32:59Z"
"","11397","KAFKA-13377: Close `Stream` to avoid resource leak","see https://issues.apache.org/jira/browse/KAFKA-13377","closed","","lujiefsi","2021-10-14T10:12:48Z","2021-10-15T16:38:05Z"
"","11136","KAFKA-13141; Skip follower fetch offset update in leader if diverging epoch is present","See https://issues.apache.org/jira/browse/KAFKA-13141 for the failing scenario. The PR skips follower fetch state for diverging epochs similar to the error case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-07-28T10:16:35Z","2021-07-28T16:27:27Z"
"","11128","MINOR: remove partition-level error from MetadataResponse#errorCounts","see https://github.com/apache/kafka/pull/9433#discussion_r676083224  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-07-25T23:29:01Z","2021-07-26T03:06:13Z"
"","10760","KAFKA-12541; Extend ListOffset to fetch offset with max timestamp (KIP-734)","See https://cwiki.apache.org/confluence/display/KAFKA/KIP-734%3A+Improve+AdminClient.listOffsets+to+return+timestamp+and+offset+for+the+record+with+the+largest+timestamp  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Tested with new Integration test  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thomaskwscott","2021-05-25T13:28:49Z","2022-03-24T08:26:40Z"
"","10796","HOTFIX: fix build error","see CI (https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-10585/5/pipeline/12/) and the error is related to 6b005b2b4eece81a5500fb0080ef5354b4240681  This error happens only in scala 2.12  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-31T07:51:30Z","2021-05-31T14:46:15Z"
"","11006","KAFKA-13049: Name the threads used for log recovery","See [KAFKA-13049](https://issues.apache.org/jira/browse/KAFKA-13049)","closed","","tombentley","2021-07-09T07:38:42Z","2021-07-14T05:14:38Z"
"","10717","KAFKA-12800: Configure generator to fail on trailing JSON tokens","See #10709 for an example of this happening.  The tl;dr is that Jackson will ignore trailing tokens by default, but other Json parsers cannot be configured to ignore them. This makes sure we don't regress :).  # Testing  I ran `./gradlew processMessages` and saw that everything completed succesfully. I then put a trailing `}` into one of the files and saw that `processMessages` failed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","NLincoln","2021-05-18T12:30:38Z","2021-05-25T13:19:07Z"
"","10694","MINOR: fix system test TestSecurityRollingUpgrade","SecurityConfig for a Kafka cluster in a system test is cached due to https://github.com/apache/kafka/pull/8917, but we mutate the security config during some system tests, and those mutations were not being passed through after-the-fact.  These system tests were not testing what they were supposed to be testing.  This patch passes through the potential changes so that we again test what we are supposed to be testing.  Also, since we became very specific about what SASL mechanisms to enable when updating the system tests for KRaft, we need to explicitly indicate to the SecurityConfig any additional SASL mechanisms that we want to enable.  This was always necessary once we made the KRaft changes, but it was not apparent due to the above bug (where mutations were not being passed through).  This patch provides a way to pass additional SASL mechanisms to the SecurityConfig by adding an optional `sasl_mechanism` to KafkaListener -- this is what gets passed into the SecurityConfig when we enable a new security protocol in the middle of a system test.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-05-14T16:45:59Z","2021-05-17T17:46:45Z"
"","11188","KAFKA-13021: disallow grace called after grace set via new API","Saw this `TODO` comment while reading codes.  `//TODO KAFKA-13021: disallow calling grace() if it was already set via ofTimeDifferenceAndGrace/WithNoGrace()`  Add the check to disallow grace called after grace set via new API, and add tests for them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-08T08:20:52Z","2021-10-12T23:05:22Z"
"","11326","KAFKA-9965/KAFKA-13303: RoundRobinPartitioner broken by KIP-480","RoundRobinPartitioner behaviour was broken by sticky partitioning (KIP-480).  This patch addresses the behavioural issue caused by the second call to `partition()` after `onNewBatch()`, in a predicatable and thread-safe manner.    Unit tested by simulation of multiple threads producing to two topics with race conditions.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jonmcewen","2021-09-15T11:44:32Z","2021-11-01T22:35:26Z"
"","10587","KAFKA-8897: Upgrade RocksDB to 6.8.1","RocksDB 6.8.1 is the newest version I could upgrade without running into a SIGABRT issue with error message ""Pure virtual function called!"" during Gradle builds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-04-22T11:58:47Z","2021-05-05T11:04:22Z"
"","10663","KAFKA-12708 Rewrite org.apache.kafka.test.Microbenchmarks by JMH","Rewrite org.apache.kafka.test.Microbenchmarks by JMH","open","","g1geordie","2021-05-10T19:10:12Z","2021-05-10T19:11:48Z"
"","11115","KAFKA-13129: replace describe topic via zk with describe users","Replace the unsupported describe topic via zk with describe users to fix the system tests. For the `upgrade_test` case where TLS support is not required, use `list_acls` instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-23T06:33:04Z","2021-07-23T20:48:13Z"
"","10904","KAFKA-13060: Replace EasyMock and PowerMock with Mockito in WorkerGroupMemberTest","Replace EasyMock and PowerMock with Mockito in WorkerGroupMemberTest https://issues.apache.org/jira/browse/KAFKA-13060  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","tang7526","2021-06-18T08:21:06Z","2022-07-29T17:21:45Z"
"","10884","MINOR: replace deprecated exactly_once_beta into exactly_once_v2","replace deprecated exactly_once_beta into exactly_once_v2 in system tests.  Follow up for https://github.com/apache/kafka/pull/10870, found out there are still some system tests using the deprecated `exactly_once_beta`. Update them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-15T07:27:23Z","2021-09-27T15:02:49Z"
"","10610","MINOR: replace deprecated Class.newInstance() to new one","replace deprecated `Clazz.newInstance()` to `clazz.getDeclaredConstructor().newInstance()` as described in official java doc [here](https://docs.oracle.com/javase/9/docs/api/java/lang/Class.html#newInstance--):  > The call  clazz.newInstance() can be replaced by  clazz.getDeclaredConstructor().newInstance()     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-29T08:45:14Z","2021-05-07T13:16:58Z"
"","10676","KAFKA-12648: MINOR - Add TopologyMetadata.Subtopology class for subtopology metadata","Renames the existing `Subtopology` class + interface to `SubtopologyDescription`, since that more closely matches what it is/is used for. Then introduces a new `Subtopology` class which includes basic metadata such as the topic group id and the NamedTopology that this subtopology belongs to, if any.  Also adds an internal `NamedTaskId` class to expose the `namedTopology` of a TaskId outside the package. I realized TaskId is part of the public API, so we can't just add a public `namedTopology()` getter method. I made the `namedTopology` field protected and removed the getter/moved it to the NamedTaskId class.  There are no actual logical changes or features in this PR, it's just a refactoring. Split these changes out to reduce the LOC in the main PRs.","closed","streams,","ableegoldman","2021-05-12T03:41:29Z","2021-05-17T16:40:32Z"
"","11194","KAFKA-12779: rename namedTopology in TaskId to topologyName","rename namedTopology is taskID to topologyName to clear up confusion    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-08-09T22:30:11Z","2021-08-10T01:29:21Z"
"","11192","KAFKA-12779: rename namedTopology in TaskId to topologyName","rename namedTopology is taks ID to topologyName to clear up confusion  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-08-09T17:51:00Z","2021-08-09T22:36:39Z"
"","10730","KAFKA-12813: Remove Deprecated schedule method in ProcessorContext","Removes the schedule method in ProcessorContext and all its implementations  Method was deprecated back in 2.1  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jlprat","2021-05-19T13:10:11Z","2021-05-20T05:30:15Z"
"","10710","KAFKA-12796: Removal of deprecated classes under `streams-scala`","Removes previously deprecated methods in older KIPs   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-17T10:24:32Z","2021-05-27T09:32:27Z"
"","10737","KAFKA-12814: Remove Deprecated Method StreamsConfig getConsumerConfigs","Removes previously deprecated method getConsumerConfigs under StreamsConfig, deprecated since 2.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jlprat","2021-05-20T08:22:20Z","2021-05-20T21:52:23Z"
"","10729","KAFKA-12809: Remove Deprecated methods under Stores","Removes deprecated methods since 2.1 Moves needed implementation from deprecated method to the right new one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jlprat","2021-05-19T12:09:23Z","2021-05-20T05:29:30Z"
"","10923","KAFKA-12976: Remove UNSUPPORTED_VERSION error from delete topics call","Removed the condition to throw the error. Now we only throw when we didn't find the topic ID.  Updated the test for IBP < 2.8 that tries to delete topics using ID.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-06-23T20:23:32Z","2021-06-28T17:15:50Z"
"","10792","MINOR: Small refactor of tests","Removed redundant method parameter.  Replaced Collections.singleton with Utils.mkSet for consistency.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","VJvaLbhYbfr","2021-05-29T21:48:44Z","2022-04-02T06:09:07Z"
"","10706","MINOR: Fix typo in `ClusterTool`","Removed double dot in command description.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gunnarmorling","2021-05-15T20:41:53Z","2021-05-17T07:34:44Z"
"","10972","remove useless code that assign null value to instance field  during …","remove useless code that assign null value to instance field in construting NetworkReceive instance, becase the default value of instance field is null, there is no need to assign null value again","open","","yws-tracy","2021-07-04T04:46:57Z","2021-07-06T08:20:06Z"
"","10970","remove useless code that assign null value to instance field in durin…","remove useless code that assign null value to instance field in construting NetworkReceive instance, becase the default value of instance field is null,  there is no need to assign null value again","open","","yws-tracy","2021-07-04T03:34:04Z","2021-07-04T03:45:41Z"
"","10856","MINOR: Small optimizations and removal of unused code in Streams","Remove unused methods in internal classes Mark fields that can be final as final Remove unneeded generic type annotation Convert single use fields to local final variables Use method reference in lambdas when it's more readable  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-06-09T16:06:47Z","2021-06-10T20:32:21Z"
"","10855","MINOR: clean up unneeded `@SuppressWarnings` on Streams module","Remove unneeded `@SuppressWarnings(""unchecked"")` in source and test Remove unneeded `@SuppressWarnings(""deprecated"")` in source and test  Several of those annotations were either never needed, or the code that force its introduction is already gone. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-06-09T15:57:02Z","2021-06-13T11:03:12Z"
"","11089","MINOR: remove unnecessary judgment in AdminUtils::assignReplicasToBrokersRackAware","remove unnecessary judgment in method AdminUtils::assignReplicasToBrokersRackAware because replicationFactor <= numBrokers , so the judgment is not necessary    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoeCqupt","2021-07-20T06:55:17Z","2022-06-29T13:05:52Z"
"","11088","MINOR: remove unnecessary judgment in method: assignReplicasToBrokersRackAware","remove unnecessary judgment in method AdminUtils::assignReplicasToBrokersRackAware  because replicationFactor <= numBrokers , so the judgment is not unnecessary","closed","","JoeCqupt","2021-07-20T05:58:42Z","2021-07-20T06:42:59Z"
"","10801","MINOR: remove unneccessary public keyword from ProducerInterceptor/ConsumerInterceptor interface","remove unneccessary public keyword from ProducerInterceptor/ConsumerInterceptor interface  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","KahnCheny","2021-06-01T12:23:21Z","2021-06-02T09:10:16Z"
"","10746","MINOR: remove unneccessary public keyword from ProducerInterceptor/ConsumerInterceptor interface","remove unneccessary public keyword from ProducerInterceptor interface","closed","","KahnCheny","2021-05-23T09:09:30Z","2021-06-01T12:27:37Z"
"","10556","MINOR: Remove redundant code from BrokerApiVersionsCommand","Remove some redundant parameters and configurations  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-18T11:19:19Z","2021-04-20T02:36:26Z"
"","10501","MINOR: Remove redundant test code","Remove redundant test code  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-04-07T17:05:14Z","2021-04-08T08:35:15Z"
"","10887","MINOR: Refactor the MetadataCache interface","Remove getNonExistingTopics, which was not necessary. MetadataCache already lets callers check for the existence of topics by calling MetadataCache#contains.  Add MetadataCache#getAliveBrokerNode and getAliveBrokerNodes.  This simplifies the calling code, which always wants a Node.  Add ScalaDoc for MetadataCache#numPartitions.  MetadataCache#numPartitions should return a simple int rather than an Option[Int]. A topic which exists can never have 0 partitions, so a return of 0 is not ambiguous.  Fix a case where we were calling getAliveBrokers and filtering by id, rather than simply calling getAliveBroker(id) and making use of the hash map.  Put ZkMetadataCache into its own file, like the other MetadataCache subclass.  Move some more test code from EasyMock to Mockito.","closed","kip-500,","cmccabe","2021-06-16T00:26:18Z","2021-06-29T22:59:25Z"
"","10515","MINOR: Remove deprecated checksum method","Remove deprecated checksum method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-04-09T16:57:39Z","2021-04-10T01:15:04Z"
"","10514","MINOR: Remove deprecated alterConfigs method","Remove deprecated alterConfigs method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tang7526","2021-04-09T16:42:36Z","2021-04-10T16:38:20Z"
"","10727","KAFKA-12810: Remove deprecated TopologyDescription.Source#topics","Remove already deprecated method that was already not in use internally  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jlprat","2021-05-19T11:43:53Z","2021-05-19T18:50:51Z"
"","10724","KAFKA-12808: Remove Deprecated Methods under StreamsMetrics","Removal of methods already deprecated since 2.5. Adapt test to use the new alternative method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-19T09:13:10Z","2021-05-21T10:40:01Z"
"","10783","MINOR: Dependency updates for Scala libraries for improved Scala 3.0 support","Release notes: * Scala 2.12.14: https://github.com/scala/scala/releases/tag/v2.12.14 * Scala Logging: https://github.com/lightbend/scala-logging/releases/tag/v3.9.3 * Scala Collection Compat:   *  https://github.com/scala/scala-collection-compat/releases/tag/v2.3.1   * https://github.com/scala/scala-collection-compat/releases/tag/v2.3.2   * https://github.com/scala/scala-collection-compat/releases/tag/v2.4.0   * https://github.com/scala/scala-collection-compat/releases/tag/v2.4.1   * https://github.com/scala/scala-collection-compat/releases/tag/v2.4.2   * https://github.com/scala/scala-collection-compat/releases/tag/v2.4.3   * https://github.com/scala/scala-collection-compat/releases/tag/v2.4.4 * Scala Java8 Compat:   * https://github.com/scala/scala-java8-compat/releases/tag/v1.0.0-RC1   * https://github.com/scala/scala-java8-compat/releases/tag/v1.0.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-28T16:08:00Z","2021-05-31T19:13:32Z"
"","11363","MINOR: remove note on future multi-cluster support of Kafka Streams","Related to https://issues.apache.org/jira/browse/KAFKA-13326  There are no plan to support cross-cluster processing atm. We should remove this sentence.  We should cherry-pick to 3.0 (maybe even further) and also update the web-page directly.","closed","docs,","mjsax","2021-09-27T21:00:42Z","2021-09-29T06:16:10Z"
"","10577","MINOR: check duplicate advertised listeners based on resolved host","related to #4897  I noticed this issue when reviewing #10575. With this patch, the listener `:12345` gets fast failure if its resolved host and port is registered already.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","chia7712","2021-04-21T10:21:58Z","2021-04-29T04:29:42Z"
"","10659","MINOR: remove unnecessary placeholder from WorkerSourceTask#recordSent","related to #10630  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-10T09:17:53Z","2021-05-10T15:42:12Z"
"","10632","MINOR: fix streams_broker_compatibility_test.py","related to #10573   the log message was changed and so the system test can't capture expected message.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-05T08:36:25Z","2021-05-05T18:13:47Z"
"","10673","MINOR: set replication.factor to 1 to make StreamsBrokerCompatibility…","related to #10532  the default value of `replication.factor` was changed from `1` to `-1`. The old broker (< 2.4) does not support such configuration so this PR sets the `replication.factor` to `1` to fix `streams_broker_compatibility_test.py`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-11T14:38:34Z","2021-05-14T05:51:32Z"
"","10637","MINOR: remove storage/src/generated from tracked files","related to #10271  The generated code from other modules have beed excluded from tracked files.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-06T06:17:25Z","2021-05-11T07:34:46Z"
"","10498","MINOR: remove `checksumOrNull` and `isValid` from Record","Reference: https://github.com/apache/kafka/pull/10470#discussion_r608347559  This PR includes following task  1. rewrite the checksum of `DumpLogSegments` 2. remove `checksumOrNull` and `isValid` from `Record` (blocked by #10470)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-04-07T05:30:17Z","2021-04-19T13:40:07Z"
"","10664","KAFKA-12749: Changelog topic config on suppressed KTable lost","Refactored `logConfig` to be passed appropriately when using `shutDownWhenFull` or `emitEarlyWhenFull`. Removed the constructor that doesn't accept a `logConfig` parameter so you're forced to specify it explicitly, whether it's empty/unspecified or not.  Ticket: https://issues.apache.org/jira/browse/KAFKA-12749  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vichu","2021-05-11T01:17:26Z","2021-06-03T18:00:20Z"
"","11035","KAFKA-13072: Make RemoveMembersFromConsumerGroupHandler unmap for COORDINATOR_NOT_AVAILABLE error","refactor RemoveMembersFromConsumerGroupHandler and tests. Also, put COORDINATOR_NOT_AVAILABLE as unmap retry.  This is the old handle response logic. FYR:  ```java void handleResponse(AbstractResponse abstractResponse) {       final LeaveGroupResponse response = (LeaveGroupResponse) abstractResponse;        // If coordinator changed since we fetched it, retry       // note here: we'll collect all errors in group error and member errors, to check if if we need to retry       if (ConsumerGroupOperationContext.hasCoordinatorMoved(response)) {           Call call = getRemoveMembersFromGroupCall(context, members);           rescheduleFindCoordinatorTask(context, () -> call, this);           return;       }        if (handleGroupRequestError(response.topLevelError(), context.future()))           return;        final Map memberErrors = new HashMap<>();       for (MemberResponse memberResponse : response.memberResponses()) {           memberErrors.put(new MemberIdentity()                                .setMemberId(memberResponse.memberId())                                .setGroupInstanceId(memberResponse.groupInstanceId()),                            Errors.forCode(memberResponse.errorCode()));       }       context.future().complete(memberErrors);   } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-13T09:42:25Z","2021-07-15T16:05:18Z"
"","11022","KAFKA-13063: Make DescribeConsumerGroupsHandler unmap for COORDINATOR_NOT_AVAILABLE error","refactor DescribeConsumerGroupsHandler and tests. Also, put `COORDINATOR_NOT_AVAILABLE` as unmap retry.  the old handleResponse for `DescribeConsumerGroups` request: ```java void handleResponse(AbstractResponse abstractResponse) {       final DescribeGroupsResponse response = (DescribeGroupsResponse) abstractResponse;        List describedGroups = response.data().groups();       if (describedGroups.isEmpty()) {           context.future().completeExceptionally(                   new InvalidGroupIdException(""No consumer group found for GroupId: "" + context.groupId()));           return;       }        if (describedGroups.size() > 1 ||               !describedGroups.get(0).groupId().equals(context.groupId())) {           String ids = Arrays.toString(describedGroups.stream().map(DescribedGroup::groupId).toArray());           context.future().completeExceptionally(new InvalidGroupIdException(                   ""DescribeConsumerGroup request for GroupId: "" + context.groupId() + "" returned "" + ids));           return;       }        final DescribedGroup describedGroup = describedGroups.get(0);        // If coordinator changed since we fetched it, retry       if (ConsumerGroupOperationContext.hasCoordinatorMoved(response)) {           Call call = getDescribeConsumerGroupsCall(context);           rescheduleFindCoordinatorTask(context, () -> call, this);           return;       }        final Errors groupError = Errors.forCode(describedGroup.errorCode());       if (handleGroupRequestError(groupError, context.future()))           return;        final String protocolType = describedGroup.protocolType();       if (protocolType.equals(ConsumerProtocol.PROTOCOL_TYPE) || protocolType.isEmpty()) {           final List members = describedGroup.members();           final List memberDescriptions = new ArrayList<>(members.size());           final Set authorizedOperations = validAclOperations(describedGroup.authorizedOperations());           for (DescribedGroupMember groupMember : members) {               Set partitions = Collections.emptySet();               if (groupMember.memberAssignment().length > 0) {                   final Assignment assignment = ConsumerProtocol.                       deserializeAssignment(ByteBuffer.wrap(groupMember.memberAssignment()));                   partitions = new HashSet<>(assignment.partitions());               }               final MemberDescription memberDescription = new MemberDescription(                       groupMember.memberId(),                       Optional.ofNullable(groupMember.groupInstanceId()),                       groupMember.clientId(),                       groupMember.clientHost(),                       new MemberAssignment(partitions));               memberDescriptions.add(memberDescription);           }           final ConsumerGroupDescription consumerGroupDescription =               new ConsumerGroupDescription(context.groupId(), protocolType.isEmpty(),                   memberDescriptions,                   describedGroup.protocolData(),                   ConsumerGroupState.parse(describedGroup.groupState()),                   context.node().get(),                   authorizedOperations);           context.future().complete(consumerGroupDescription);       } else {           context.future().completeExceptionally(new IllegalArgumentException(               String.format(""GroupId %s is not a consumer group (%s)."",                   context.groupId(), protocolType)));       }   } ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-12T10:12:42Z","2021-07-15T12:26:37Z"
"","10938","Fix StringSerializer could not be found when it not in ContextClassLoader","ref: https://stackoverflow.com/q/37363119/1120863       https://github.com/holgerbrandl/kscript/issues/131  `org.apache.kafka.common.serialization.StringSerializer` may be not in ContextClassLoader, when using some custom classloader. So try load class from kafkaClassLoader itself while failed load class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","eshizhan","2021-06-29T10:01:54Z","2021-07-01T14:41:30Z"
"","10897","MINOR: Reduced severity for ""skipping records"" falling out of time windows","Reduced severity for ""skipping records"" falling out of time windows/segments from ""warn"" to ""debug"" since this is, as mjsax called it on Slack, actually ""regular processing"", and we have metrics for skipped records anyway since Kafka Streams 2.0 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-274%3A+Kafka+Streams+Skipped+Records+Metrics).  Using severity ""warn"" or ""info"" results in lots of unnecessarily cluttered logs like this - even though the skipping is actually the intended processing behavior: {""@timestamp"":""2021-06-17T19:04:44.194Z"",""@version"":""1"",""message"":""Skipping record for expired segment."",""logger_name"":""org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore"",""thread_name"":""prod.devices.manufacturing.pt.streamingetl.v3-5f937ad9-7f89-41c5-b4ba-591dabe838fd-StreamThread-1"",""level"":""WARN"",""level_value"":30000,""dd.service"":""streaming-etl"",""dd.env"":""prod"",""dd.span_id"":""3790604313083202822"",""dd.trace_id"":""2039260532201528974"",""dd.version"":""1.0.0-20210616-master.4""} {""@timestamp"":""2021-06-17T19:04:44.194Z"",""@version"":""1"",""message"":""Skipping record for expired segment."",""logger_name"":""org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore"",""thread_name"":""prod.devices.manufacturing.pt.streamingetl.v3-5f937ad9-7f89-41c5-b4ba-591dabe838fd-StreamThread-1"",""level"":""WARN"",""level_value"":30000,""dd.service"":""streaming-etl"",""dd.env"":""prod"",""dd.span_id"":""3790604313083202822"",""dd.trace_id"":""2039260532201528974"",""dd.version"":""1.0.0-20210616-master.4""} {""@timestamp"":""2021-06-17T19:04:44.194Z"",""@version"":""1"",""message"":""Skipping record for expired segment."",""logger_name"":""org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore"",""thread_name"":""prod.devices.manufacturing.pt.streamingetl.v3-5f937ad9-7f89-41c5-b4ba-591dabe838fd-StreamThread-1"",""level"":""WARN"",""level_value"":30000,""dd.service"":""streaming-etl"",""dd.env"":""prod"",""dd.span_id"":""3790604313083202822"",""dd.trace_id"":""2039260532201528974"",""dd.version"":""1.0.0-20210616-master.4""} {""@timestamp"":""2021-06-17T19:04:44.194Z"",""@version"":""1"",""message"":""Skipping record for expired segment."",""logger_name"":""org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore"",""thread_name"":""prod.devices.manufacturing.pt.streamingetl.v3-5f937ad9-7f89-41c5-b4ba-591dabe838fd-StreamThread-1"",""level"":""WARN"",""level_value"":30000,""dd.service"":""streaming-etl"",""dd.env"":""prod"",""dd.span_id"":""3790604313083202822"",""dd.trace_id"":""2039260532201528974"",""dd.version"":""1.0.0-20210616-master.4""} {""@timestamp"":""2021-06-17T19:04:44.194Z"",""@version"":""1"",""message"":""Skipping record for expired segment."",""logger_name"":""org.apache.kafka.streams.state.internals.AbstractRocksDBSegmentedBytesStore"",""thread_name"":""prod.devices.manufacturing.pt.streamingetl.v3-5f937ad9-7f89-41c5-b4ba-591dabe838fd-StreamThread-1"",""level"":""WARN"",""level_value"":30000,""dd.service"":""streaming-etl"",""dd.env"":""prod"",""dd.span_id"":""3790604313083202822"",""dd.trace_id"":""2039260532201528974"",""dd.version"":""1.0.0-20210616-master.4""}","closed","","xdgrulez","2021-06-17T19:07:05Z","2022-02-08T02:35:08Z"
"","10704","KAFKA-12791: ConcurrentModificationException in AbstractConfig use by KafkaProducer","Recently we have noticed multiple instances where KafkaProducers have failed to constructer due to the following exception:  ``` org.apache.kafka.common.KafkaException: Failed to construct kafka producer at  org.apache.kafka.clients.producer.KafkaProducer.(KafkaProducer.java:440) at  org.apache.kafka.clients.producer.KafkaProducer.(KafkaProducer.java:291) at  org.apache.kafka.clients.producer.KafkaProducer.(KafkaProducer.java:318)  java.base/java.lang.Thread.run(Thread.java:832) Caused by: java.util.ConcurrentModificationException at  java.base/java.util.HashMap$HashIterator.nextNode(HashMap.java:1584) at  java.base/java.util.HashMap$KeyIterator.next(HashMap.java:1607) at  java.base/java.util.AbstractSet.removeAll(AbstractSet.java:171) at  org.apache.kafka.common.config.AbstractConfig.unused(AbstractConfig.java:221) at  org.apache.kafka.common.config.AbstractConfig.logUnused(AbstractConfig.java:379) at  org.apache.kafka.clients.producer.KafkaProducer.(KafkaProducer.java:433) ... 9 more  exception.class:org.apache.kafka.common.KafkaException exception.message:Failed to construct kafka producer ```  This is due to the fact that `used` below is a synchronized set. `used` is being modified while removeAll is being called. This is due to the use of RecordingMap in the Sender thread (see below). Switching to a ConcurrentHashSet avoids this issue as it support concurrent iteration.  ``` 	at org.apache.kafka.clients.producer.ProducerConfig.ignore(ProducerConfig.java:569) 	at org.apache.kafka.common.config.AbstractConfig$RecordingMap.get(AbstractConfig.java:638) 	at org.apache.kafka.common.network.ChannelBuilders.createPrincipalBuilder(ChannelBuilders.java:242) 	at org.apache.kafka.common.network.PlaintextChannelBuilder$PlaintextAuthenticator.(PlaintextChannelBuilder.java:96) 	at org.apache.kafka.common.network.PlaintextChannelBuilder$PlaintextAuthenticator.(PlaintextChannelBuilder.java:89) 	at org.apache.kafka.common.network.PlaintextChannelBuilder.lambda$buildChannel$0(PlaintextChannelBuilder.java:66) 	at org.apache.kafka.common.network.KafkaChannel.(KafkaChannel.java:174) 	at org.apache.kafka.common.network.KafkaChannel.(KafkaChannel.java:164) 	at org.apache.kafka.common.network.PlaintextChannelBuilder.buildChannel(PlaintextChannelBuilder.java:79) 	at org.apache.kafka.common.network.PlaintextChannelBuilder.buildChannel(PlaintextChannelBuilder.java:67) 	at org.apache.kafka.common.network.Selector.buildAndAttachKafkaChannel(Selector.java:356) 	at org.apache.kafka.common.network.Selector.registerChannel(Selector.java:347) 	at org.apache.kafka.common.network.Selector.connect(Selector.java:274) 	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:1097) 	at org.apache.kafka.clients.NetworkClient.access$700(NetworkClient.java:87) 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1276) 	at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1164) 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:637) 	at org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:327) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:242) ```","closed","","lbradstreet","2021-05-15T16:05:30Z","2021-05-27T20:54:55Z"
"","11364","MINOR: expand logging and improve error message during partition count resolution","Recently a user hit this TaskAssignmentException due to a bug in their regex that meant no topics matched the pattern subscription, which in turn meant that it was impossible to resolve the number of partitions of the downstream repartition since there was no upstream topic to get the partition count for. Debugging this was pretty difficult and ultimately came down to stepping through the code line by line, since even with TRACE logging we only got a partial picture.   We should expand the logging to make sure the TRACE logging hits both conditional branches, and improve the error message with a suggestion for what to look for should someone hit this in the future","closed","","ableegoldman","2021-09-28T22:17:48Z","2021-09-30T01:19:43Z"
"","10696","KAFKA-12777 Refactor and cleanup AutoTopicCreationManager","Rather than using multiple optional class members to determine if we are in ZK or KRaft mode, use inheritance. The factory method in AutoTopicCreationManager now makes the decision about which mode we're in and provides only the needed dependencies to the concrete classes (i.e., no optionals).  ```scala object AutoTopicCreationManager {   def apply(     config: KafkaConfig,     channelManager: BrokerToControllerChannelManager,     metadataSupport: MetadataSupport,     groupCoordinator: GroupCoordinator,     txnCoordinator: TransactionCoordinator,   ): AutoTopicCreationManager = {     metadataSupport match {       case zk: ZkSupport => new ZkAutoTopicCreationManager(config, zk, groupCoordinator, txnCoordinator)       case _: RaftSupport => new DefaultAutoTopicCreationManager(config, channelManager, groupCoordinator, txnCoordinator)     }   } } ```  This also adds some error handling to the response handler when running in KRaft mode (KAFKA-12777).","open","","mumrah","2021-05-14T17:54:45Z","2021-05-18T16:56:14Z"
"","10681","KIP-731 Connect per-task record rate limiting","RateLimiter impl for KIP-731: https://cwiki.apache.org/confluence/display/KAFKA/KIP-731%3A+Record+Rate+Limiting+for+Kafka+Connect","open","","ryannedolan","2021-05-12T22:33:33Z","2021-05-19T01:06:59Z"
"","11116","KAFKA-13114: Revert state and reregister raft listener","RaftClient's scheduleAppend may split the list of records into multiple batches. This means that it is possible for the active controller to see a committed offset for which it doesn't have an in-memory snapshot.  If the active controller needs to renounce and it is missing an in-memory snapshot, then revert the state and reregister the Raft listener. This will cause the controller to replay the entire metadata partition.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-23T15:53:45Z","2021-08-11T17:24:12Z"
"","10755","MINOR: deprecate TaskMetadata constructor and add KIP-740 notes to upgrade guide","Quick followup to KIP-740 to actually deprecate this constructor, and update the upgrade guide with what we changed in KIP-740. I also noticed the TaskId#parse method had been modified previously, and should be re-added to the public TaskId class. It had no tests, so now it does","closed","","ableegoldman","2021-05-24T22:37:49Z","2021-05-26T17:35:13Z"
"","10609","KAFKA-12648: Pt. 1 - Add NamedTopology to protocol and state directory structure","Pt. 1: [#10609](https://github.com/apache/kafka/pull/10609) Pt. 2: [#10683](https://github.com/apache/kafka/pull/10683) Pt. 3: [#10788](https://github.com/apache/kafka/pull/10788)  This PR includes adding the NamedTopology to the Subscription/AssignmentInfo, and to the StateDirectory so it can place NamedTopology tasks within the hierarchical structure with task directories under the NamedTopology parent dir.","closed","streams,","ableegoldman","2021-04-29T07:39:46Z","2021-06-07T22:39:05Z"
"","10683","KAFKA-12648: Pt. 2 - Introduce TopologyMetadata to wrap InternalTopologyBuilders of named topologies","Pt. 1: [#10609](https://github.com/apache/kafka/pull/10609) Pt. 2: [#10683](https://github.com/apache/kafka/pull/10683) Pt. 3: [#10788](https://github.com/apache/kafka/pull/10788)  The TopologyMetadata is next up after [Pt. 1 #10609](https://github.com/apache/kafka/pull/10609). This PR sets up the basic architecture for running an app with multiple NamedTopologies, though the APIs to add/remove them dynamically are not implemented until Pt. 3  (Apologies for the length of this PR -- the vast majority of it is just refactoring/moving methods from the InternalTopologyBuilder to the new TopologyMetadata wrapper that now wraps all the individual builders. You should focus the review on the TopologyMetadata and how it interacts with the other classes)","closed","","ableegoldman","2021-05-12T23:55:05Z","2021-07-30T01:41:35Z"
"","10788","KAFKA-12648: Pt. 3 - addNamedTopology API","Pt. 1: [#10609](https://github.com/apache/kafka/pull/10609) Pt. 2: [#10683](https://github.com/apache/kafka/pull/10683) Pt. 3: [#10788](https://github.com/apache/kafka/pull/10788)  In Pt. 3 we implement the `addNamedTopology` API. This can be used to update the processing topology of a running Kafka Streams application without resetting the app, or even pausing/restarting the process. It's up to the user to ensure that this API is called on every instance of an application to ensure all clients are able to run the newly added NamedTopology. This should not be too much of a burden as it only requires that each client eventually be updated by the user -- under the covers, Streams will take care of keeping the internal state consistent while various clients wait to converge on the latest view of the full topology.  Internally, when a new NamedTopology is added a rebalance will be triggered to distribute the tasks that correspond to it. To minimize disruption and wasted work, the assignor just computes the desired eventual assignment of these new tasks to clients regardless of whether the target client has been issued the `addNamedTopology` request yet. If a client receives tasks for a NamedTopology it does not yet recognize, it stashes them away and continues to process its other topologies. Once it receives this new NamedTopology, those tasks will be created and begin processing without triggering a new rebalance. If the new NamedTopology does not match any unknown tasks it has received, then the client must trigger a fresh rebalance for this new NamedTopology.","closed","","ableegoldman","2021-05-29T01:53:24Z","2021-08-06T07:19:26Z"
"","11421","KAFKA-12648: Pt. 4 - return Add/RemoveNamedTopologyResult so callers can wait on topology changes","Pt. 1: #10609 Pt. 2: #10683 Pt. 3: #10788 Pt. 4: #11421   WIP","open","","ableegoldman","2021-10-21T05:53:12Z","2021-10-21T05:53:38Z"
"","10902","KAFKA-12837: Process entire batch reader in the commit handler","Process entire batch and make sure that `hasNext` is called before calling `next` on the iterator.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-06-18T02:35:15Z","2021-06-19T19:10:51Z"
"","10682","MINOR: Refactored test using parameterization","Problem: (1) Conditions or full repetitions within a test method may produce a false test outcome. Once a condition is not met, and the code block inside the condition is not executed, a false 'passed' outcome can be produced without the test ever being executed. (2) A test method with many individual assertions stops being executed on the first failed assertion, which prevents the remaining ones' execution. The difference between these assertions lies in different arguments only.  Solution: Parameterized tests make it possible to run a test multiple times with different arguments. Using JUnit's parameterized tests feature instead of a repetition structure enables the repeated block to function as an independent test. This way, we could make the original tests become as many independent ones as there are versions to be tested. In this refactoring, no original assertion parameter was changed.","open","","eas5","2021-05-12T23:31:32Z","2022-02-17T19:10:35Z"
"","11028","MINOR: change log.dirs for config/kraft/controller.properties","Prior to this change the logs for the example controller were in raft-controller-logs which wasn't consistent (raft vs kraft) with kraft-broker-logs or kraft-combined-logs used for the broker or combined server respectively:    $ grep log.dirs config/kraft/*.properties   config/kraft/broker.properties:log.dirs=/tmp/kraft-broker-logs   config/kraft/controller.properties:log.dirs=/tmp/raft-controller-logs   config/kraft/server.properties:log.dirs=/tmp/kraft-combined-logs","closed","","tombentley","2021-07-12T14:02:58Z","2021-10-26T21:30:40Z"
"","10857","MINOR: Create SnapshotWriter and SnapshotReader interfaces","Previously, we had an interface named org.apache.kafka.controller.SnapshotWriter and a concrete implementation class called org.apache.kafka.snapshot.SnapshotWriter.  As part of unit tests, it is often helpful to be able to use an interface to mock snapshot writing and reading. Therefore, we should make the interface class part of the Raft package, not the controller package. This PR moves SnapshotWriter into that package and renames the concrete class to RaftSnapshotWriter.  This PR also harmonizes the two classes. For example, ""completeSnapshot"" becomes ""freeze"" (since that's what it was in the raft package). Similarly, ""writeBatch"" becomes ""append"".  SnapshotWriter is now templated on the record type, in order to be generic, rather than specific to metadata.  The controller code sometimes refers to the snapshot's end offset as its ""epoch"". This is confusing since there is also the concept of a raft log epoch. I have tried to remove most of these uses and replace them with ""endOffset"", which is more descriptive.  I also fixed an off-by-one error where we advanced lastCommittedOffset to be just before a snapshot's endOffset, rather than at a snapshot end offset.  Finally, I removed snapshotId from the reader and writer interface. Since snapshots are only ever taken (or indeed read) from committed offsets, there is no need for the controller or broker to supply this information to the raft layer. The end offset is sufficient. The raft layer can look up the relevant information if there is any need for it.","closed","kip-500,","cmccabe","2021-06-09T23:58:58Z","2021-06-15T23:40:24Z"
"","11353","KAFKA-13322: Reducing amount of garbage that gets generated during a poll operation","Presently KafkaConsumer creates an enourmous amount of garbage during a poll. Polls are generaly very frequent and so allocations during a poll decrease efficiency of the overall solution. This commit removes a large (and unnecessary allocation).  This is a non functional change and so has been tested by existing tests.","closed","","mprusakov","2021-09-22T14:54:24Z","2022-02-04T20:52:53Z"
"","11279","KAFKA-13236: TopologyTestDriver should not crash for EOS-beta config","Port of #11271 for 2.7 branch.  Call for review @abbccdda","closed","streams,","mjsax","2021-08-27T19:30:29Z","2021-08-28T03:15:36Z"
"","11256","KAFKA-13224: Expose consistent broker.id and node.id in config values/originals maps","Plugins may expect `broker.id` to exist as a key in the config's various originals()-related maps, but with KRaft we rely solely on `node.id` for the broker's ID, and with the Zk-based brokers we provide the option to specify `node.id` in addition to (or as a full replacement for) `broker.id`.  There are multiple problems related to this switch to `node.id`:  - We do not enforce consistency between explicitly-specified `broker.id` and `node.id` properties in the config – it is entirely possible right now that we could set `broker.id=0` and also set `node.id=1`, and the broker will use 1 for it's ID. This is confusing at best; the broker should detect this inconsistency and fail to start with a ConfigException. - When `node.id` is set, both that value and any explicitly-set `broker.id` value will exist in the config's **originals()-related maps**. Downstream components are often configured based on these maps, and they may ask for the `broker.id`, so downstream components may be misconfigured if the values differ, or they may fail during configuration if no `broker.id` key exists in the map at all. - The config's **values()-related maps** will contain either the explicitly-specified `broker.id` value or the default value of -1. When `node.id` is set, both that value (which cannot be negative) and the (potentially -1) `broker.id` value will exist in the config's values()-related maps. Downstream components are often configured based on these maps, and they may ask for the `broker.id`, so downstream components may be misconfigured if the `broker.id` value differs from the broker's true ID.  The broker should detect inconsistency between explicitly-specified `broker.id` and `node.id` values and fail startup accordingly. It should also ensures that the config's originals()- and values()-related maps contain the same mapped values for both `broker.id` and `node.id` keys when at least one is specified.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-08-24T20:23:09Z","2021-11-16T21:44:10Z"
"","10805","KAFKA-12436 KIP-720 Deprecate MirrorMaker v1","Per KIP-720, add deprecation warnings to legacy mirror maker.","closed","","ryannedolan","2021-06-02T04:54:07Z","2021-07-04T14:17:32Z"
"","10487","Kafka producer Partitioner add canAbortOnNewBatch","Partitioner add canAbortOnNewBatch method to reduce unnecessary abort new batch","open","","yunwan","2021-04-06T08:34:10Z","2021-04-06T12:30:33Z"
"","11159","MINOR: Fix logging in ClusterControlManager","Out of order put/get is affecting the logging in `ClusterControlManager`.","closed","","dielhennr","2021-07-31T19:31:14Z","2022-02-07T01:25:27Z"
"","11338","MINOR: improve error logging in List De/Serializer classes","Our soak test application ran into an error constructing the ListDeserializer and crashed, but unfortunately the actual causing exception was not logged anywhere. This PR adds logging for this case, as well as a few others around this class and the corresponding serializer where additional logging would be helpful","closed","streams,","ableegoldman","2021-09-18T04:13:13Z","2021-09-27T20:48:07Z"
"","11052","Use ByteBuffers for LZ4 OutputStream","Our current LZ4 OutputStream implementation allocates compression buffers internally and relies on intermediate byte arrays for input and output buffers.  With this change we now use ByteBuffers internally, and as a result: * we write directly to the target ByteBuffer, avoiding an additional copy * we no longer allocate an output compression buffer, reducing allocations by half * we pave the way to make compression buffers reusable, similar to what we do   for decompression","open","","xvrl","2021-07-14T20:34:50Z","2021-07-14T20:34:50Z"
"","11470","MINOR: Update docs for producer callbacks to reflect current behaviour","Originally, Callback would return a null metadata value when an error occurred.  This was partially changed by [KAFKA-3303](https://issues.apache.org/jira/browse/KAFKA-3303), where in some cases Callback would return an 'empty' metadata. In this empty metadata TopicPartition is set correctly but all other fields are set as `-1`.  The docs were later updated by [KAFKA-7412](https://issues.apache.org/jira/browse/KAFKA-7412), but it incorrectly states that Callback will always return this 'empty' metadata when an error occurs. However in the case of any exceptions that are a subclass of ApiException, Callback will still return a null value (see [here](https://github.com/apache/kafka/blob/3.1/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L1002)).  This change aims to clarify the docs to accurately reflect the behaviour of producer callbacks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soceanainn","2021-11-05T13:47:25Z","2022-02-08T15:35:28Z"
"","11242","[WIP] MINOR: POC for KIP-591: Add config to set default store impl class","original KIP link: https://cwiki.apache.org/confluence/display/KAFKA/KIP-591%3A+Add+Kafka+Streams+config+to+set+default+store+type KIP discussion link: https://lists.apache.org/list.html?dev@kafka.apache.org:gte=1d:KIP-591 (You might need to read this discussion thread to re-cap, since it's a discussion 1 year ago :) )  Discussed with @mjsax , we think this KIP should be back to live since this is a feature that many users requested for.   What I've done: 1. create a stream config: `default.store.impl.class` to store the default store implementation class, and default to RocksDB 2. create `StoreImplementation` interface, and 2 default implementation: RocksDB and In-memory 3. add a new method: `Materialized#as(storeImplementation)` to pass custom store implementation 4. **this is the place different from what described in KIP-591**: add a new constructor for `StreamsBuilder`, to accept a `properties` instance. (detailed below) 5. add a new constructor for `MaterializedInternal`, to accept `StreamsConfig`, so that we can store the storeImplementation provided, to create storeBuilder for the graph node 6. When creating store builder for a node, before, we check if the `storeSuppier` in materialized is null or not, if null, we use `RockDB store supplier`. Now, we'll get the default store supplier via store implementation 7. Add a `KeyValueStoreBuilder` constructor to accept a `StoreImplementation`, so that the processor API users can also benefit from it.  For (4), in KIP-591, we wanted to get the `Stream props` during `StreamBuilder.build(props)`, but I found it's too late to get the props. During `StreamBuilder#build`, we already built all the Stream graph nodes, including `StoreBuilder` in each node. So, I think we should get the props when creating `StreamBuilder`, so that when each node creation, we can initialize the store supplier via store implementation via config. This way, we can also adopt more optimization and customization in the future during building the stream graph nodes in `StreamBuilder`.  The problem I can think of, is that if the props passed to `StreamBuilder` constructor is different from the one passed into `build(props)`. We can actually just output a warning message and honor the one passed in that method, and `deprecate` the `build(props)` method in this KIP.  See if you have any thoughts about it. Thanks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-20T12:53:33Z","2022-01-25T07:29:10Z"
"","11282","MINOR: Fix redundant static modifier for Ops enum","Ops enum in KafkaRaftClient.Registration has redundant static modifier.","closed","","il-kyun","2021-08-29T17:02:33Z","2021-10-18T08:40:49Z"
"","10999","KAFKA-12257: Consumer mishandles topics deleted and recreated with the same name (for 3.0)","Opening a version of https://github.com/apache/kafka/pull/10952 for the 3.0 branch  Store topic ID info in consumer metadata. We will always take the topic ID from the latest metadata response and remove any topic IDs from the cache if the metadata response did not return a topic ID for the topic.  With the addition of topic IDs, when we encounter a new topic ID (recreated topic) we can choose to get the topic's metadata even if the epoch is lower than the deleted topic.  The idea is that when we update from no topic IDs to using topic IDs, we will not count the topic as new (It could be the same topic but with a new ID). We will only take the update if the topic ID changed.  Added tests for this scenario as well as some tests for storing the topic IDs. Also added tests for topic IDs in metadata cache.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-08T17:45:16Z","2021-07-08T17:50:28Z"
"","11001","MINOR: fix typo in config for eos-v2 docs of the upgrade guide","One of the configs has a typo and should have been `exactly_once _v2` rather than just `exactly_once`","closed","","ableegoldman","2021-07-08T18:56:18Z","2021-07-08T21:42:18Z"
"","11232","MINOR: Add missing licenses and update versions in LICENSE-binary for 3.0","One new dependency was missing a license entry ([jline](https://github.com/jline/jline3)) The rest of the changes correspond to updated package versions.  No functional changes in the code   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2021-08-19T00:40:30Z","2021-08-19T04:58:39Z"
"","10929","KAFKA-12995 [WIP] Allow old Broker compatibility for Metadata calls","Old Brokers prior to Metadata V4 don't support the **allowAutoTopicCreation** field.  The implementation of #3098 introduced a check for old Brokers compatibility falling back to _true_ in _MetadataRequest_'s **allowAutoTopicCreation** field for some operations like **describeTopics**.  Then, after that backward-compatibility changes, more methods were added to _AdminClient_ like **listOffsets** in #7296 which unfortunately don't have compatibility with old Brokers into account. One might argue that, given that new methods were not present previously, it's not a compatibility break. While this is strictly true, I believe that one of Kafka Clients' most appreciated feature is the ability to use old Brokers with new Clients.  I've prepared this small PR to fix for this, following the same approach used in #3098.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aartigao","2021-06-25T13:55:24Z","2022-06-03T19:11:32Z"
"","10996","MINOR: StreamsPartitionAssignor should log the individual members of each client","Often when debugging things like continuous rebalancing in Streams, we want to check whether the input conditions to the assignor are stable and if not, why. One of the common scenarios is regular changes to the group membership resulting in an unstable/non-deterministic assignment, so it's very useful to be able to tell at a glance how many members are participating in the rebalance, and which ones are missing, if any. This is currently a pretty terrible experience to tease out of the existing logs, so let's just make our lives easier by logging the info we want directly.   Also fixes some warnings I saw when compiling. I'd like to get this into 3.0 cc @kkonstantine","closed","","ableegoldman","2021-07-08T02:22:55Z","2021-07-08T18:23:50Z"
"","10493","MINOR: cleanup Jenkins workspace before build","Observed a failing build with ``` Failed to execute goal org.apache.maven.plugins:maven-archetype-plugin:3.2.0:generate (default-cli) on project standalone-pom: A Maven project already exists in the directory /home/jenkins/workspace/Kafka_kafka-pr_PR-10131/streams/quickstart/test-streams-archetype/streams.examples -> [Help 1] ```  We should clean the workspace before we start the build to avoid this issue. \cc @ijuma   Using Workspace Cleanup: https://plugins.jenkins.io/ws-cleanup/","closed","","mjsax","2021-04-06T21:02:43Z","2021-04-12T20:12:38Z"
"","11328","KAFKA-13255 Fixed code so user can use config.properties.exclude to exclude prope…","Objective - Use MM2 (kafka connect in distributed cluster) for data migration between cluster hosted in private data center and aws msk cluster.  Steps performed -  Started kafka-connect service. Created 3 MM2 connectors (i.e. source connector, checkpoint connector and heartbeat connector). Curl commands used to create connectors are in the attached file.  To exclude certain config properties while topic replication, we are using the 'config.properties.exclude' property in the MM2 source connector. Expected -  Source topic 'dev.portlandDc.anamika.helloMsk' should be successfully created in destination cluster.  Actual -  Creation of the source topic 'dev.portlandDc.anamika.helloMsk' in destination cluster fails with an error. Error is  [2021-08-06 06:13:40,944] WARN [mm2-msc|worker] Could not create topic dev.portlandDc.anamika.helloMsk. (org.apache.kafka.connect.mirror.MirrorSourceConnector:371) org.apache.kafka.common.errors.InvalidConfigurationException: Unknown topic config name: confluent.value.schema.validation","open","","AnamikaN","2021-09-16T00:29:25Z","2021-09-23T15:11:10Z"
"","11367","MINOR: Do not copy on range for in-memory shared store in stream stream left/out joins","Note this is a bit hacky solution to improve the performance of stream-stream join for in-memory shared stores: theoretically deleting while iterating is not a good pattern and hence have undefined behavior. After looking through the source code I think the only reason it works here is because we only delete keys that we have iterated over.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2021-09-29T02:49:37Z","2021-10-08T16:49:11Z"
"","10688","KAFKA-12778: Fix QuorumController request timeouts and electLeaders","Not all RPC requests to the quorum controller include a timeout, but we should honor the timeouts that do exist.  For electLeaders, attempt to trigger a leader election for all partitions when the request specifies null for the topics argument.","closed","kip-500,","cmccabe","2021-05-13T19:47:15Z","2021-05-14T19:44:20Z"
"","10777","MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout`","New parameters in overloaded methods should appear later apart from lambdas that should always be last.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-27T13:37:31Z","2021-05-27T16:58:52Z"
"","10776","MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout`","New parameters in overloaded methods should appear later apart from lambdas that should always be last.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-27T13:34:24Z","2021-05-27T16:57:57Z"
"","10759","MINOR: Adjust parameter ordering of `waitForCondition` and `retryOnExceptionWithTimeout`","New parameters in overloaded methods should appear later apart from lambdas that should always be last.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-25T13:18:04Z","2021-05-27T13:38:14Z"
"","10955","MINOR: Add `KafkaAdminClient.getListOffsetsCalls` benchmark","new benchmark to test KafkaAdminClient.getListOffsetsCalls originated from https://github.com/apache/kafka/pull/10940  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2021-07-02T02:16:24Z","2021-07-07T06:58:56Z"
"","10882","KAFKA-12948: Remove node from ClusterConnectionStates.connectingNodes when node is removed","NetworkClient.poll() throws IllegalStateException when checking `isConnectionSetupTimeout` if all nodes in `ClusterConnectionStates.connectingNodes` aren't present in `ClusterConnectionStates.nodeState`. When we remove a node from `nodeState`, we should also remove from `connectingNodes`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-06-14T20:47:24Z","2021-06-17T11:13:54Z"
"","10837","MINOR: Merge trunk","Need to resolve a merge-conflict in `gradle/dependencies.gradle` due to version bump of `scalaLogging` (cf https://github.com/apache/kafka/commit/274eccf922bd05f0979a649f1b466a569f197009#diff-02b139e755ab0e27e0b0a6f9845843ef189116955cc340c49a4022587dbb2b52)","closed","","mjsax","2021-06-07T17:17:34Z","2021-06-07T21:24:07Z"
"","10787","KAFKA-12864: Move KafkaEventQueue into server-common","Move the KafkaEventQueue into server-common. Since it is a generic data structure not specific to metadata, it fits better into the server-common gradle module than into the metadata module.","closed","kip-500,","cmccabe","2021-05-28T21:59:45Z","2021-06-02T23:11:01Z"
"","10865","KAFKA-12934: Move some controller classes to the metadata package","Move some controller classes to the metadata package so that they can be used with broker snapshots. Rename ControllerTestUtils to RecordTestUtils. Move PartitionInfo to PartitionRegistration.","closed","kip-500,","cmccabe","2021-06-10T22:17:05Z","2021-06-11T00:34:18Z"
"","10496","MINOR: move NoOpSnapshotWriter to main","Move NoOpSnapshotWriter and NoOpSnapshotWriterBuilder out of the test directory and into the main directory, until we implement the KRaft integration.","closed","kip-500,","cmccabe","2021-04-07T02:47:00Z","2021-04-07T05:30:00Z"
"","11171","KAFKA-13132: Upgrading to topic IDs in LISR requests has gaps introduced in 3.0 (part 2)","Most of [KAFKA-13132](https://issues.apache.org/jira/browse/KAFKA-13132) has been resolved, but there is one part of one case not covered. From the ticket: `2. We only assign the topic ID when we are associating the log with the partition in replicamanager for the first time`  We covered the case where the log is already existing when the leader epoch is _equal_ (ie, no updates besides the topic ID), but we don't cover the update case where the leader epoch is bumped and we already have the log associated to the partition.   This PR ensures we correctly assign topic ID in the makeLeaders/Followers path when the log already exists. I've also added a test for the bumped leader epoch scenario.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-08-03T22:54:44Z","2021-08-06T01:10:48Z"
"","11272","KAFKA-12648: introduce TopologyConfig and TaskConfig for topology-level overrides","Most configs that are read and used by Streams today originate from the properties passed in to the KafkaStreams constructor, which means they get applied universally across all threads, tasks, subtopologies, and so on. The only current exception to this is the `topology.optimization` config which is parsed from the properties that get passed in to `StreamsBuilder#build`. However there are a handful of configs that could also be scoped to the topology level, allowing users to configure each NamedTopology independently of the others, where it makes sense to do so.  This PR refactors the handling of these configs by interpreting the values passed in via KafkaStreams constructor as the global defaults, which can then be overridden for individual topologies via the properties passed in when building the NamedTopology. More topology-level configs may be added in the future, but this PR covers the following:  1. `max.task.idle.ms` 2. `task.timeout.ms` 3. `buffered.records.per.partition` 4. `default.timestamp.extractor.class` 5. `default.deserialization.exception.handler`","closed","","ableegoldman","2021-08-27T04:38:35Z","2021-11-10T19:27:59Z"
"","10818","KAFKA-12889: log clean relative index range check of group consider empty log segment to avoid too many empty log segment left","more detail see jira [KAFKA-12889](https://issues.apache.org/jira/browse/KAFKA-12889)  to avoid log index 4 byte relative offset overflow, log cleaner group check log segments offset to make sure group offset range not exceed Int.MaxValue.  this offset check currentlly not cosider next is next log segment is empty, so there will left empty log files every about 2^31 messages.  the left empty logs will be reprocessed every clean cycle, which will rewrite it with same empty content, witch cause little no need io.  for __consumer_offsets topic, normally we can set cleanup.policy to compact,delete to get rid of this.  my cluster is 0.10.1.1, but after aylize trunk code, it should has same problem too.     some of my left empty logs,(run ls -l)  rw-r---- 1 u g 0 Dec 16 2017 00000000000000000000.index rw-r---- 1 u g 0 Dec 16 2017 00000000000000000000.log rw-r---- 1 u g 0 Dec 16 2017 00000000000000000000.timeindex rw-r---- 1 u g 0 Jan  15 2018 00000000002148249632.index rw-r---- 1 u g 0 Jan  15 2018 00000000002148249632.log rw-r---- 1 u g 0 Jan  15 2018 00000000002148249632.timeindex rw-r---- 1 u g 0 Jan  27 2018 00000000004295766494.index rw-r---- 1 u g 0 Jan  27 2018 00000000004295766494.log rw-r---- 1 u g 0 Jan  27 2018 00000000004295766494.timeindex","closed","","iamgd67","2021-06-04T07:09:36Z","2021-06-19T22:33:52Z"
"","11401","KAFKA-13255: use exclude filter for new topics","MM2 needs to honor `config.properties.exclude` property when it replicates initial/new topics. Updates to `MirrorSourceConnector` in 2.8 break exclude filter. Prior to 2.8, MM2 would create a topic in target cluster without any config applied. The TopicSync would apply the configuration, honoring exclude filter.  But in 2.8, the change has been made to create a topic on target with all the configurations from the source cluster. While doing that, no filters applied at all.  This fix ensures that the same working logic filtering excluded properties is applied while creating new topics on target cluster.  Testing Strategy: **Reproduce:** 1. On Source cluster create topic ""Topic_Short_Retention"" 2. Alter topic Topic_Short_Retention: `bin/kafka-configs.sh --bootstrap-server  --alter --entity-type topics --entity-name Topic_Short_Retention --add-config retention.ms=300000` 3. include following line in MM2 config: `config.properties.exclude=follower.replication.throttled.replicas, leader.replication.throttled.replicas, message.timestamp.difference.max.ms, message.timestamp.type, unclean.leader.election.enable, min.insync.replicas, retention.ms`. *Note, I copied default exclude filter and added* `retention.ms` to it 4. Start MM2 connector. *Note, the topic on a target cluster will have* `retention.ms` *copied from source cluster*.  **Test the Fix:** 1. Stop MM2 connector 2. Apply patch 3. Delete the topic from target cluster 4. Start MM2 with configuration as described in step 3 of reproduction 5. Check topic's retention config - it should be preserved as cluster's default and not copied from source cluster.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdesert","2021-10-14T21:52:24Z","2021-11-15T10:45:49Z"
"","11466",":memo: Update Docs","MINOR: update doc for changes in Upgrading, 1.0.0 section, 4th point.","closed","","yasar03","2021-11-04T05:04:47Z","2021-11-15T16:00:50Z"
"","11414","MINOR: Renamed a few record definition files with the existing convention.","MINOR: Renamed a few record definition files with the existing convention.  - Throwing an error message while reading LeaderEpochCheckpoint file with an unsupported version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-10-19T11:54:16Z","2021-10-21T20:44:08Z"
"","11427","MINOR: fix partition state change error msg","MINOR: fix partition state change error msg  previous error msg replicas always empty. it is useless for troubleshooting  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoeCqupt","2021-10-22T09:55:40Z","2021-10-28T07:01:47Z"
"","10829","MINOR Removed unused ConfigProvider from raft resources module.","MINOR Removed unused ConfigProvider from raft resources module.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-06-07T01:22:39Z","2021-06-09T21:22:19Z"
"","11060","MINOR Refactored the existing CheckpointFile in core module, moved to server-common module and introduced it as SnapshotFile.","MINOR Refactored the existing CheckpointFile in core module, moved to server-common module.   - Refactored CheckpointFile to server-common module as a Java class and it is reused by LeaderCheckpointFile, OffsetCheckpointFile.  - This will be used by CommittedOffsetsFile which checkpoints remote log metadata partitions with respective offsets in the default RemoteLogMetadataManager implementation.  - Existing tests are available for LeaderCheckpointFile, OffsetCheckpointFile.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-07-15T16:10:41Z","2021-08-30T15:43:26Z"
"","10489","MINOR Moved tiered storage API classes from clients module to a new storage-api module.","MINOR Moved tiered storage API classes from `clients` module to a new `storage-api` module.  Created `storage` and `storage-api` modules. All the remote storage API classes are moved to `storage-api` module. All the remote storage implementation classes will be added to `storage` module.     ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-04-06T17:35:59Z","2021-04-17T00:06:19Z"
"","10578","MINOR Moved ApiMessageAndVersion and AbstractApiMessageAndVersionSerde to clients module.","MINOR Moved ApiMessageAndVersion and AbstractApiMessageAndVersionSerde to clients module.  Existing unit tests would be sufficient as this change is more about moving classes into `client` module.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-04-21T12:31:58Z","2021-05-06T05:38:29Z"
"","10513","MINOR : Downgrade log level for safe catch","Minor log level downgrade to avoid alarm on ERROR log for a safe catch that could be a WARN","open","","nicolasguyomar","2021-04-09T13:58:19Z","2021-04-09T13:58:19Z"
"","10608","MINOR: clean up some remaining locking stuff in StateDirectory","Minor followup to https://github.com/apache/kafka/pull/10342 that I noticed while working on the NamedTopology stuff. Cleans up a few things:  1. We no longer need locking for the global state directory either, since it's contained within the top-level state directory lock. Definitely less critical than the task directory locking, since it's less vulnerable to IOExceptions given that it's just locked and unlocked once during the application lifetime, but nice to have nonetheless 2. Clears out misc. usages of the LOCK_FILE_NAME that no longer apply. This has the awesome side effect of finally being able to actually delete obsolete task directories, whereas previously we had to leave behind the empty directory due to a ridiculous Windows bug (though I'm sure they would claim ""it's not a bug it's a feature"" 😉 ) 3. Lazily delete old-and-now-unused lock files in the `StateDirectory#taskDirIsEmpty` method to clean up the state directory for applications that upgraded from an older version that still used task locking","closed","","ableegoldman","2021-04-29T03:37:05Z","2021-05-01T03:11:49Z"
"","11436","KAFKA-12648: fill in javadocs for the StreamsException class with new guarantees","Minor followup to [#11405](https://github.com/apache/kafka/pull/11405) / [KIP-783](https://cwiki.apache.org/confluence/display/KAFKA/KIP-783%3A+Add+TaskId+field+to+StreamsException) to write down the new guarantees we're providing about the meaning of a StreamsException in the javadocs of that class","closed","","ableegoldman","2021-10-26T20:44:17Z","2021-10-27T17:56:18Z"
"","10675","KAFKA-12574: remove internal Producer config and auto downgrade logic","Minor followup to [#10573](https://github.com/apache/kafka/pull/10573), see in particular [this comment thread](https://github.com/apache/kafka/pull/10573#discussion_r628777848). Removes this internal Producer config which was only ever used to avoid a very minor amount of work to downgrade the consumer group metadata in the txn commit request.  Also replaces deprecation warning suppression with missing `@Deprecated` annotation on the MockProducer's deprecated method.","closed","producer,","ableegoldman","2021-05-11T23:21:43Z","2021-05-17T17:25:36Z"
"","11355","KAFKA-13298: Improve documentation on EOS KStream requirements","Minor documentation fix to address: https://issues.apache.org/jira/browse/KAFKA-13298  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cddr","2021-09-23T12:17:11Z","2021-10-06T02:07:29Z"
"","11201","MINOR: fix mbean tag name ordering in JMX reporter","Metric tag maps do not offer any ordering guarantees. We should always sort tags when generating mbean names, or we could end up with duplicate means with different tag order or orphaned mbeans that were not properly removed when metrics are updated / deleted","open","","xvrl","2021-08-11T21:58:01Z","2021-08-13T18:36:17Z"
"","10517","HOTFIX: delete removed WindowedStore.put() method","Merging https://github.com/apache/kafka/pull/10293 (that was not rebased) broke `trunk`.  Unresolved overlap with https://github.com/apache/kafka/pull/10331","closed","","mjsax","2021-04-09T21:14:56Z","2021-04-09T21:58:47Z"
"","10527","Merge trunk 04/12","Merge in AK/trunk.  Conflicts: * Jenkinsfile (just ignored AK change)","closed","","vvcephei","2021-04-12T14:31:40Z","2021-04-12T14:36:36Z"
"","10594","Merge pull request #1 from apache/trunk","merge  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","socutes","2021-04-25T13:56:48Z","2021-04-26T00:24:17Z"
"","10804","KAFKA-12877: Make flexibleVersions mandatory","Many Kafka protocol JSON files were accidentally configured to not use flexible versions, since it was not on by default.  This PR requires JSON files to specify a flexibleVersions value. If the JSON file does not specify the flexibleVersions value, display an error message suggesting the correct value to use for new messages.","closed","kip-500,","cmccabe","2021-06-01T23:34:45Z","2021-06-15T23:04:34Z"
"","11410","MINOR: Make TestUtils usable for KRaft mode","Make TestUtils usable for KRaft mode by using KafkaBroker instead of KafkaServer where appropriate, and adding some alternate functions that use AdminClient instead of ZooKeeper.","closed","kip-500,","cmccabe","2021-10-18T18:31:20Z","2021-10-19T22:48:29Z"
"","11278","KAFKA-12648: Enforce size limits for each task's cache","make max buffer cache settable for a name topology  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wcarlson5","2021-08-27T18:58:48Z","2021-09-15T22:55:07Z"
"","11026","KAFKA-13064: Make ListConsumerGroupOffsetsHandler unmap for COORDINATOR_NOT_AVAILABLE error","Make ListConsumerGroupOffsetsHandler unmap for COORDINATOR_NOT_AVAILABLE error  This is the old handle response logic. FYR: ```java void handleResponse(AbstractResponse abstractResponse) {       final OffsetFetchResponse response = (OffsetFetchResponse) abstractResponse;       final Map groupOffsetsListing = new HashMap<>();        // If coordinator changed since we fetched it, retry       // here, we'll check all errors, including partition errors, to see if we need to retry       if (ConsumerGroupOperationContext.hasCoordinatorMoved(response)) {           Call call = getListConsumerGroupOffsetsCall(context);           rescheduleFindCoordinatorTask(context, () -> call, this);           return;       }        if (handleGroupRequestError(response.error(), context.future()))           return;        for (Map.Entry entry :           response.responseData().entrySet()) {           final TopicPartition topicPartition = entry.getKey();           OffsetFetchResponse.PartitionData partitionData = entry.getValue();           final Errors error = partitionData.error;            if (error == Errors.NONE) {               final Long offset = partitionData.offset;               final String metadata = partitionData.metadata;               final Optional leaderEpoch = partitionData.leaderEpoch;               // Negative offset indicates that the group has no committed offset for this partition               if (offset < 0) {                   groupOffsetsListing.put(topicPartition, null);               } else {                   groupOffsetsListing.put(topicPartition, new OffsetAndMetadata(offset, leaderEpoch, metadata));               }           } else {               log.warn(""Skipping return offset for {} due to error {}."", topicPartition, error);           }       }       context.future().complete(groupOffsetsListing);   } ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-12T12:51:14Z","2021-07-16T08:05:38Z"
"","11021","KAFKA-13062: Make DeleteConsumerGroupsHandler unmap for COORDINATOR_NOT_AVAILABLE error","Make DeleteConsumerGroupsHandler unmap for COORDINATOR_NOT_AVAILABLE error  old handlResponse logic: ```java void handleResponse(AbstractResponse abstractResponse) {       final DeleteGroupsResponse response = (DeleteGroupsResponse) abstractResponse;        // If coordinator changed since we fetched it, retry       if (ConsumerGroupOperationContext.hasCoordinatorMoved(response)) {           Call call = getDeleteConsumerGroupsCall(context);           rescheduleFindCoordinatorTask(context, () -> call, this);           return;       }        final Errors groupError = response.get(context.groupId());       if (handleGroupRequestError(groupError, context.future()))           return;        context.future().complete(null);   } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-12T08:31:06Z","2021-07-15T12:41:12Z"
"","11303","MINOR: Upgrade compression libraries (Q3 2021)","lz4-java: 1.7.1 -> 1.8.0  The most noteworthy change is the upgrade of the underlying C library to 1.9.3. Details:  * https://github.com/lz4/lz4-java/releases/tag/1.8.0 * https://github.com/lz4/lz4/releases/tag/v1.9.3  snappy-java: 1.1.8.1 -> 1.1.8.4  The most noteworthy change is support for Apple M1. Details:  * https://github.com/xerial/snappy-java/releases/tag/1.1.8.2  * https://github.com/xerial/snappy-java/releases/tag/1.1.8.3 * https://github.com/xerial/snappy-java/releases/tag/1.1.8.4  zstd-jni: 1.5.0-2 -> 1.5.0-4  Minor fixes, details:  * https://github.com/luben/zstd-jni/releases/tag/v1.5.0-3 * https://github.com/luben/zstd-jni/releases/tag/v1.5.0-4  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-09-07T04:03:28Z","2021-09-07T15:46:25Z"
"","11478","KAFKA-13299: Accept duplicate listener on port for IPv4/IPv6","Loosens the validation so that we accept duplicate listeners on the same port but if and only if the listeners are valid IP addresses with one address being an IPv4 address and the other being an IPv6 address. The detection of whether an address is a valid IP/IPv4/IPv6 address is done using Apache's commons-validator. Outside of this specific case the validations are kept the same albeit error messages have been updated to reflect that there is this new exception case of accepting IPv4/IPv6 on the same port.  This PR tests different corner cases by checking whether the changed `listenerListToEndPoints` either throws an exception (and its message or not). The test follows the exact same pattern as the `testDuplicateListeners` test. Locally on my machine the PR passes both `testIPv4AndIPv6SamePortListeners` (the new test) and `testDuplicateListeners` pass but I will wait for CI to finish.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","mdedetrich","2021-11-09T13:20:25Z","2022-08-03T07:08:27Z"
"","10989","MINOR: Fix a startup NPE in BrokerServer","LogManager was getting a `null` MetadataCache","closed","kip-500,","mumrah","2021-07-07T17:15:15Z","2021-07-09T14:42:17Z"
"","10761","MINOR: Don't ignore deletion of partition metadata file and log topic id clean-ups","Log if deletion fails and don't expose log topic id for mutability outside of `assignTopicId()`.  Also remove an unnecessary parameter in `PartitionTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-25T13:41:55Z","2021-05-27T20:50:52Z"
"","10775","KAFKA-12668: Making MockScheduler.schedule safe to use in concurrent code","Link to JIRA-issue: https://issues.apache.org/jira/browse/KAFKA-12668  Making MockScheduler.schedule safe to use in concurrent code by removing `tick()` call inside MockScheduler.schedule.  To reproduce a bug I wrote a unit-test MockSchedulerTest.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","iakunin","2021-05-27T13:26:54Z","2021-10-06T15:03:48Z"
"","10586","KAFKA-12702: Fix NPE in networkListeners from BrokerServer in 2.8","Link to [KAFKA-12702](https://issues.apache.org/jira/browse/KAFKA-12702) , [10575](https://github.com/apache/kafka/pull/10575)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-22T04:23:02Z","2021-04-25T13:29:45Z"
"","11158","KAFKA-13153: Disallow KRaft broker registration if the node.id is already in use by a controller","Kraft brokers were previously able to register with the same `node.id` as a controller without failing startup but they would not unfence themselves because they never catch up to the last committed metadata offset. This PR explicitly disallows controllers and brokers from sharing `node.id`s and it also changes the `node.id` in the default config file for the Kraft broker from `node.id=1` to `node.id=2`.  https://issues.apache.org/jira/browse/KAFKA-13153","closed","","dielhennr","2021-07-31T07:30:40Z","2021-07-31T08:05:32Z"
"","10860","MINOR: fix client_compatibility_features_test.py - DescribeAcls is al…","Kraft already supports `DescribeAcls` (see 5b0c58ed53c420e93957369516f34346580dac95). Hence, the flag `describe-acls-supported` should be `True` rather than `False`   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-06-10T08:46:20Z","2021-06-10T14:02:17Z"
"","10965","KAFKA-13029; Set appropriate fields for FindCoordinatorRequest based on version","KIP-699 added support for batching in FindCoordinatorRequest using a new protocol that changes the wire format for both batched and unbatched requests. Clients were updated to try the new format first and switch irreversibly to the old format if the new format is not supported on one broker. During rolling upgrade (or a downgrade), it is possible that a broker doesn't support new format at some point while other brokers do at a later point. Clients end up in a bad state until restarted since use new version with old format. This PR changes FindCoordinatorRequest to set data based on actual version when a single group is used. This is always the case for consumer coordinator and transaction coordinator. For admin API, we still switch to unbatched mode on failure, but the data is set based on actual version, so we never fail even if brokers are upgraded/downgraded.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-07-03T10:33:23Z","2021-07-05T13:26:04Z"
"","10924","KAFKA-8613: KIP-633 New APIs for Controlling Grace Period for Windowed Operations","KIP-633 New APIs for Controlling Grace Period for Windowed Operations  - Added API changes by KIP-633 for JoinWindows, SessionWindows, TimeWindows and SlidingWindows - Renamed Windows.DEFAULT_GRACE_PERIOD_MS to DEPRECATED_OLD_24_HR_GRACE_PERIOD - Added new constant Windows.NO_GRACE_PERIOD to avoid magic constants when 0 is specified as grace Period - Added preliminary Java unit test cases for new API methods - Replaced Deprecated calls with equivalent in Examples - Replaced Deprecated API calls in Scala tests with updated API method calls - Added Deprecation suppression in Tests for derecated API method calls in Java and Scala Tests  modified:   streams/src/main/java/org/apache/kafka/streams/kstream/JoinWindows.java modified:   streams/src/main/java/org/apache/kafka/streams/kstream/SessionWindows.java modified:   streams/src/main/java/org/apache/kafka/streams/kstream/SlidingWindows.java modified:   streams/src/main/java/org/apache/kafka/streams/kstream/TimeWindows.java modified:   streams/src/main/java/org/apache/kafka/streams/kstream/Windows.java  modified:   streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewTypedDemo.java modified:   streams/examples/src/main/java/org/apache/kafka/streams/examples/pageview/PageViewUntypedDemo.java modified:   streams/examples/src/main/java/org/apache/kafka/streams/examples/temperature/TemperatureDemo.java  modified:   streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/kstream/KStream.scala modified:   streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/TopologyTest.scala modified:   streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KStreamTest.scala modified:   streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/kstream/KTableTest.scala  modified:   streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java modified:   streams/src/test/java/org/apache/kafka/streams/TopologyTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/AbstractResetIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/InternalTopicIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/JoinStoreIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationDedupIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/KStreamAggregationIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/KStreamRepartitionIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java modified:   streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java  modified:   streams/src/test/java/org/apache/kafka/streams/kstream/JoinWindowsTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/RepartitionTopicNamingTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowsTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/SlidingWindowsTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowsTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamJoinTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamLeftJoinTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamKStreamOuterJoinTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamRepartitionTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregateProcessorTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamSlidingWindowAggregateTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregateTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedCogroupedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SessionWindowedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedCogroupedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SlidingWindowedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressScenarioTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/SuppressTopologyTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedCogroupedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImplTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/GraphGraceSearchUtilTest.java modified:   streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/StreamsGraphTest.java modified:   streams/src/test/java/org/apache/kafka/streams/processor/internals/RepartitionOptimizingTest.java modified:   streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java modified:   streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java modified:   streams/src/test/java/org/apache/kafka/streams/tests/StreamsOptimizedTest.java modified:   streams/src/test/java/org/apache/kafka/test/GenericInMemoryKeyValueStore.java modified:   streams/src/test/java/org/apache/kafka/test/GenericInMemoryTimestampedKeyValueStore.java  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","izzyacademy","2021-06-23T21:56:26Z","2021-06-24T19:29:14Z"
"","11207","KAFKA-10900: Add metrics enumerated in KIP-630","KIP-630 enumerates a few metrics. Makes sure that those metrics are implemented. Add the following Metrics.  kafka.controller:type=KafkaController,name=GenSnapshotLatencyMs , A histogram of the amount of time it took to generate a snapshot. kafka.controller:type=KafkaController,name=LoadSnapshotLatencyMs , A histogram of the amount of time it took to load the snapshot. kafka.controller:type=KafkaController,name=SnapshotLag ,The number of offsets between the largest snapshot offset and the high-watermark. kafka.controller:type=KafkaController,name=SnapshotSizeBytes , Size of the latest snapshot in bytes.","open","","socutes","2021-08-12T16:44:30Z","2021-11-30T07:26:06Z"
"","11205","KAFKA-10900: Add metrics enumerated in KIP-630","KIP-630 enumerates a few metrics. Makes sure that those metrics are implemented. Add the following Metrics.   kafka.controller:type=KafkaController,name=GenSnapshotLatencyMs | A histogram of the amount of time it took to generate a snapshot. kafka.controller:type=KafkaController,name=LoadSnapshotLatencyMs | A histogram of the amount of time it took to load the snapshot. kafka.controller:type=KafkaController,name=SnapshotLag | The number of offsets between the largest snapshot offset and the high-watermark. kafka.controller:type=KafkaController,name=SnapshotSizeBytes | Size of the latest snapshot in bytes.","closed","","socutes","2021-08-12T12:31:32Z","2021-08-12T15:58:33Z"
"","10492","KAFKA-12457: Add sentinel ID to metadata topic","KIP-516 introduces topic IDs to topics, but there is a small issue with how the metadata topic will interact with topic IDs.   For example, https://github.com/apache/kafka/pull/9944 aims to replace topic names in the Fetch request with topic IDs. In order to get these IDs, brokers must fetch from the metadata topic. This leads to a sort of ""chicken and the egg"" problem concerning how we find out the metadata topic's topic ID.   One solution proposed in KIP-516 was to introduce a ""sentinel ID"" for the metadata topic. This is a reserved ID for the metadata topic only.  This PR adds the sentinel ID when creating the metadata log. More information can be found in the [JIRA](https://issues.apache.org/jira/browse/KAFKA-12457) and in [KIP-516](https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-04-06T20:06:21Z","2021-04-08T17:24:23Z"
"","10579","KAFKA-9555 Added default RLMM implementation based on internal topic storage.","KAFKA-9555 Added default RLMM implementation based on internal topic storage. - This is the initial version of the default RLMM implementation.  - This includes changes containing default RLMM configs, RLMM implementation, producer/consumer managers. - Introduced `TopicBasedRemoteLogMetadataManagerHarness` which takes care of bringing up a Kafka cluster and create remote log metadata topic and initializes `TopicBasedRemoteLogMetadataManager`. - Refactored existing `RemoteLogMetadataCacheTest` to `RemoteLogSegmentLifecycleTest` to have parameterized tests to run both `RemoteLogMetadataCache` and also `TopicBasedRemoteLogMetadataManager`. - Refactored existing `InmemoryRemoteLogMetadataManagerTest`,  `RemoteLogMetadataManagerTest` to have parameterized tests to run both `InmemoryRemoteLogMetadataManager` and also `TopicBasedRemoteLogMetadataManager`.  We will have a followup PR adding file based cache for storing the remote log state for each partition to avoid consuming from the remote log metadata topic from the earliest whenever a broker is restarted.  This is part of tiered storage KIP-405 efforts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-04-21T12:42:05Z","2021-07-19T16:05:46Z"
"","11334","KAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAn…","KAFKA-13246: StoreQueryIntegrationTest#shouldQueryStoresAfterAddingAndRemovingStreamThread does not gate on stream state well  The test now waits for the client to transition to REBALANCING/RUNNING after adding/removing a thread as well as to transition to RUNNING before querying the state store.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","andre0x01","2021-09-17T15:08:08Z","2021-09-21T16:48:33Z"
"","10853","KAFKA-12811: kafka-topics.sh should let the user know they cannot adj…ust the replication factor for a topic using the --alter flag and not warn about missing the --partition flag","KAFKA-12811: kafka-topics.sh should let the user know they cannot adj…ust the replication factor for a topic using the --alter flag and not warn about missing the --partition flag  This work is my own and I license it to the project.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Moovlin","2021-06-09T13:33:11Z","2021-06-09T18:07:16Z"
"","10936","KAFKA-13002: listOffsets must downgrade immediately for non MAX_TIMESTAMP specs","KAFKA-12541 introduced a regression for listOffsets requests for non maxtimestamp specs. when communicating with old brokers. This PR addresss this case.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Tested with new unit test for regression case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","thomaskwscott","2021-06-28T17:32:53Z","2021-07-01T06:35:56Z"
"","11233","HOTFIX: Disable spurious left/outer stream-stream join fix","KAFKA-10847 improves stream-stream left/outer joins to avoid spurious left/outer join results. However, it introduces regression bug KAFKA-13216.  This PR disables KAFKA-10847 by partially rolling back KIP-633 changes.","closed","streams,","mjsax","2021-08-19T02:35:42Z","2021-08-26T04:46:26Z"
"","11456","KAFKA-13351: Add possibility to write kafka headers in Kafka Console Producer","Just like with the ConsoleConsumer, if headers and/or key enabled then the order is ""headers key value"". The separator for the header and headerKey are configurable. The defaults are the same as in the ConsoleConsumer.  In short,  Default parsing pattern   when: parse.headers=true and parse.key=true:     ""h1:v1,h2...\tkey\tvalue""  when: parse.headers=false and parse.key=true:     ""key\tvalue""  when: parse.headers=true and parse.key=false:     ""h1:v1,h2...\tvalue""   Testing strategy: The testing is constrained to testing LineMessageReader as this is the only affected class. Different combinations of parse.headers=true|false and parse.key=true|false are tested. Different string patterns are tested. Including input that does not fit the expected Pattern. LineMessageReaderTest.testLineReader suggests that the change is backward compatible.  This contribution is my original work and I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","florin-akermann","2021-10-31T17:51:26Z","2022-01-19T21:54:34Z"
"","10591","Fix minor bugs in the existing documentation","Just finished reading the official docs, found some inconsistencies, here they are","open","","konarskis","2021-04-24T10:49:03Z","2022-02-25T18:35:22Z"
"","11123","MINOR: factor state checks into descriptive methods and clarify javadocs","Just a bit of minor cleanup that (a) does some prepwork for another PR I'm working on, (b) updates the javadocs & exception messages to report a more useful error to the user and describe what they actually need to do, and (c) hopefully makes these state checks more future-proof by defining methods for each kind of check in one place that can be easily updated instead of tracking down every individual check.","closed","","ableegoldman","2021-07-24T03:07:21Z","2021-07-26T22:53:31Z"
"","11187","[WIP] KAFKA-5966: let CacheFunction#key return ByteBuffer","jira: https://issues.apache.org/jira/browse/KAFKA-5966  This PR doesn't meet what KAFKA-5966 expected, just makes `CacheFunction#key` return ByteBuffer, to avoid unnecessary array copy. But I want to make sure this is the right way to do.   Although we can't avoid ALL array copy after this PR, we did reduce the array copy amounts after this change. The reason we can't avoid all array copy is that in many places, we still expect the `Bytes` and `byte array` type to be provided, ex: `ThreadCache`, `NamedCache`.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-07T08:53:41Z","2021-08-23T01:40:23Z"
"","10951","KAFKA-12841: NPE from the provided metadata in client callback in case of ApiException","Jira: https://issues.apache.org/jira/browse/KAFKA-12841  Using the `InterceptorCallback` wrapper in the case of `ApiException` so that we will adhere correctly to the `Callback` contract for `onCompletion` specifying a valid (dummy) `TopicPartition`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)  I'm also supposed to add:  > The contribution is my original work and that I license the work to the project under the project's open source license.","closed","","kirktrue","2021-06-30T23:05:19Z","2022-02-02T00:51:04Z"
"","10509","KAFKA-12464: enhance constrained sticky Assign algorithm","JIRA: https://issues.apache.org/jira/browse/KAFKA-12464  This improvement achieved: 1. Make code simpler and cleaner 2. After the PR: the `testLargeAssignmentAndGroupWithUniformSubscription` (1 million partitions) will run **from ~2600 ms down to ~1400 ms**, improves **46%** of performance, almost **2x faster**!! 3.  jenkins test results after my change: **(avg. 2 sec)** ``` Build / JDK 8 and Scala 2.12 / testLargeAssignmentAndGroupWithUniformSubscription() | 1.8 sec | Passed Build / JDK 11 and Scala 2.13 / testLargeAssignmentAndGroupWithUniformSubscription() | 2.2 sec | Passed Build / JDK 15 and Scala 2.13 / testLargeAssignmentAndGroupWithUniformSubscription() | 2.2 sec | Passed ``` the latest trunk test results: **(avg. 5.4 sec)** ``` Build / JDK 15 and Scala 2.12 / testLargeAssignmentAndGroupWithUniformSubscription() | 6.3 sec | Passed Build / JDK 11 and Scala 2.13 / testLargeAssignmentAndGroupWithUniformSubscription() | 4.4 sec | Passed Build / JDK 11 and Scala 2.12 / testLargeAssignmentAndGroupWithUniformSubscription() | 5.7 sec | Passed Build / JDK 15 and Scala 2.13 / testLargeAssignmentAndGroupWithUniformSubscription() | 6.5 sec | Passed Build / JDK 8 and Scala 2.13 / testLargeAssignmentAndGroupWithUniformSubscription() | 6.3 sec | Passed Build / JDK 8 and Scala 2.12 / testLargeAssignmentAndGroupWithUniformSubscription() | 3.6 sec | Passed ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-09T03:41:49Z","2021-05-06T04:02:52Z"
"","10782","Fix issue with 'UID' string in Dockerfile used for System Test","Jira issue: https://issues.apache.org/jira/browse/KAFKA-12847 Fixes #12847  Signed-off-by: Abhijit Mane    *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Abhijit-Mane","2021-05-28T10:29:10Z","2022-01-07T05:24:16Z"
"","10933","Fix NPE on concurrent RecordHeader.key() access","Jira issue (detailed description): https://issues.apache.org/jira/browse/KAFKA-12999  ## Summary After upgrading clients to `2.8.0`, reading `ConsumerRecord`'s header keys started resulting in occasional `java.lang.NullPointerException` in case of concurrent access from multiple(2) threads.  ### NPE location NPE happens here RecordHeader.java:45: ```java public String key() {     if (key == null) {         key = Utils.utf8(keyBuffer, keyBuffer.remaining()); // NPE here          keyBuffer = null;     }     return key; } ```  ### What introduced issue Cause of issue is introduced by changes of apache#9223  ### Consequences Current implementation renders RecordHeader not thread-safe for read-only access.  ### Reproducibility Since issue s concurrent race condition, reproducibility is non-deterministic but easy to reproduce with enough re-attempts This PR has test which almost always reproduces issue (wiithout `synchronized`)  ### Fix strategy This PR avoids race-condition by having `synchronized` on `key()` and `value()` methods  ### Benchmark Comparison between with and without `synchronized` on `RecordHeader.key()` method ``` Benchmark                              Mode  Cnt   Score   Error  Units RecordHeaderBenchmark.key              avgt    15  31.308 ± 7.862  ns/op RecordHeaderBenchmark.synchronizedKey  avgt    15  31.853 ± 7.096  ns/op ````  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","antonio-tomac","2021-06-27T01:04:25Z","2021-06-27T01:04:25Z"
"","11337","KAFKA-13309: fix InMemorySessionStore#fetch/backwardFetch order issue","JIRA is [here](https://issues.apache.org/jira/browse/KAFKA-13309). In https://github.com/apache/kafka/pull/9139, we added backward iterator on SessionStore. But there is a bug that  when fetch/backwardFetch the key range, if there are multiple records in the same session window, we can't return the data in the correct order.  For example: We have a session window inactivity gap with 10 ms, and the records:  key: ""A"", value: ""AA"", timestamp: 0 --> with SessionWindow(0, 0) key: ""B"", value: ""BB"", timestamp: 0 --> with SessionWindow(0, 0) key: ""C"", value: ""CC"", timestamp: 0 --> with SessionWindow(0, 0) key: ""D"" value: ""DD"", timestamp: 100 --> with SessionWindow(100, 100)  So, when fetch(""A"" /*key from*/, ""D"" /*key to*/), we expected to have [A, B, C, D], but we'll have [C, B A, D ]  And the reason is that we pass ""false"" in the ""is forward"" parameter for `fetch` method, and ""true"" for ""backwardFetch"" method, which obviously is wrong.  So, why does the tests can't find this issue?  It's because the test data we provided doesn't have multiple data in the same session window.   In this PR, I fixed the issue, and add tests to improve the test coverage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-09-18T02:46:30Z","2021-09-29T00:54:02Z"
"","10702","KAFKA-12940: Enable JDK 16 builds in Jenkins","JDK 15 no longer receives updates, so we want to switch from JDK 15 to JDK 16. However, we have a number of tests that don't yet pass with JDK 16.  Instead of replacing JDK 15 with JDK 16, we have both for now and we either disable (via annotations) or exclude (via gradle) the tests that don't pass with JDK 16 yet. The annotations approach is better, but it doesn't work for tests that rely on the PowerMock JUnit 4 runner.  Also add `--illegal-access=permit` when building with JDK 16 to make MiniKdc work for now. This has been removed in JDK 17, so we'll have to figure out another solution when we migrate to that.  Relevant JIRAs for the disabled tests: KAFKA-12790, KAFKA-12941, KAFKA-12942.  Moved some assertions from `testTlsDefaults` to `testUnsupportedTlsVersion` since the former claims to test the success case while the former tests the failure case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-15T13:23:08Z","2021-06-13T15:14:40Z"
"","11071","KAFKA-13098: Fix NoSuchFileException during snapshot recovery","Java's FileTreeIterator throws an NoSuchFileException when visting @metadata-0/partition.metadata.tmp. This is most like do to the fact that the Log type asynchronously creates and delete that file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-16T18:18:58Z","2021-07-16T21:45:03Z"
"","11296","KAFKA-13273: Add support for Java 17","Java 17 is at release candidate stage and it will be a LTS release once it's out (previous LTS release was Java 11).  Details: * Replace Java 16 with Java 17 in Jenkins and Readme. * Replace `--illegal-access=permit` (which was removed from Java 17)    with  `--add-opens` for the packages we require internal access to.    Filed KAFKA-13275 for updating the tests not to require `--add-opens`    (where possible). * Update `release.py` to use JDK8. and JDK 17 (instead of JDK 8 and JDK 15). * Removed all but one Streams test from `testsToExclude`. The    Connect test exclusion list remains the same. * Add notable change to upgrade.html * Upgrade to Gradle 7.2 as it's required for proper Java 17 support. * Upgrade mockito to 3.12.4 for better Java 17 support. * Adjusted `KafkaRaftClientTest` and `QuorumStateTest` not to require    private access to `jdk.internal.util.random`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-09-04T15:29:33Z","2021-09-06T15:55:58Z"
"","11409","MINOR: Rename LeaderChangeBatch in test source","Jacoco plugin in our CI fails because LeaderChangeBatch is present in both the main and test sources so this commit renames it to TestLeaderChangeBatch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","viktorsomogyi","2021-10-18T14:08:31Z","2021-10-19T15:06:49Z"
"","10828","MINOR: Only log overridden topic configs during topic creation","It's quite verbose to include all configs for every partition loaded/created. Also make sure to redact sensitive and unknown config values.  Unit test included.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-06-05T21:18:41Z","2021-06-08T13:45:31Z"
"","10790","KAFKA-12865","It's a small fix for documentation bug.  I don't think any test case is required for that.   closes KAFKA-12865  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","rohit-sachan","2021-05-29T07:34:26Z","2021-05-29T11:31:35Z"
"","10728","KAFKA-12841-from the provided metadata in client callback in case of ApiException.","It uses the client callback and not handling it as InterceptorCallback. causing NPE. --P.S : Might need also to handle the TopicPartition that also might throw NPE  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ayoukhananov","2021-05-19T11:59:28Z","2021-07-01T07:24:17Z"
"","11480","MINOR: fix comment in TimingWheel","It seems that there something error in the comment of TimingWheel  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoeCqupt","2021-11-10T03:21:25Z","2021-11-15T19:04:53Z"
"","11377","MINOR: Use try-with-resource to close the stream opened by Files.list()","It may cause the file handle not to be released.","closed","","wenzelcheng","2021-10-04T09:58:40Z","2022-02-05T05:42:15Z"
"","11253","MINOR: Improve local variable name in UnifiedLog.maybeIncrementFirstUnstableOffset","It looked odd that the code has a local variable named `updatedFirstStableOffset` while it is used to update `UnifiedLog.firstUnstableOffsetMetadata`. This PR improves the local variable name to be `updatedFirstUnstableOffset` instead which is more aligned with the `UnifiedLog` attribute being updated.  **Tests:** Relying on existing unit & integration tests.","closed","","kowshik","2021-08-24T08:22:15Z","2021-08-26T20:27:52Z"
"","11468","MINOR: Introduce `ApiKeyVersionsSource` annotation for `ParameterizedTest`","It is common in our code base to have unit tests which must be run for all the versions of a given request. Most of the time, we do so by iterating over all the versions in the test itself which is error prone.  With JUnit5 and ParameterizedTest, we can now use a custom arguments source for this case, which is way more convenient. It looks likes this:  ``` @ParameterizedTest @ApiKeyVersionsSource(apiKey = ApiKeys.ADD_PARTITIONS_TO_TXN) public void mytest(short version) {   // do smth based on version... } ```  This patch introduces the new annotation and updates `AddPartitionsToTxnRequestTest` test as a first example. I will migrate all the other cases in subsequent PRs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-11-04T18:24:17Z","2021-11-11T15:41:12Z"
"","10963","KAFKA-13030 FindCoordinators batching causes slow poll when requestin…","issue: https://issues.apache.org/jira/browse/KAFKA-13030  related: https://github.com/apache/kafka/pull/10743  https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L888  ```java             if (e instanceof NoBatchedFindCoordinatorsException) {                 batchFindCoordinator = false;                 clearFindCoordinatorFuture();                 lookupCoordinator();                 return;             }  ```  The current request future is NOT updated so it can't be completed until timeout. It causes a slow poll when user poll data from older broker the first time.  ### Broken Behavior  ```java   var consumer = new KafkaConsumer<>();   var records = consumer.poll(Duration.ofSeconds(30));   // there are producers which are pushing data to topic.   // [BEFORE PR-10743] the return data from fist poll could NOT be empty   // [AFTER PR-10743] the return data from first poll is always empty. To make matters worst, it returns only if duration is expired (if users set a large duration ...)   assert records.count() > 0; ```   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-07-02T22:13:48Z","2021-07-04T13:52:10Z"
"","10910","KAFKA-12965 - Graceful clean up of task error metrics","Issue:  We noticed that the Error metrics reported in Kafka Connect worker continues to stay even after the task is re distributed to another worker. As a result you would notice over a period of time the task_error_metrics  of a worker would contain the errors of all the tasks  that it had ever come across.  This is an anti pattern to what other metrics are reported by Kafka Connect worker. The Kafka Connect worker should only report the error metrics of the present task and leave the persistence of the previous tasks to the metrics storage system that is consuming these metrics.  In the below example we notice that there is only 2 active tasks that are running , but we have more than 20+ tasks error metrics that are available.    Task counter mbean: {""request"":\{""mbean"":""kafka.connect:type=connect-worker-metrics"",""type"":""read""} ,""value"":{""connector-startup-failure-percentage"":0.0,""task-startup-attempts-total"":90.0,""connector-startup-success-total"":1.0,""connector-startup-failure-total"":0.0,""task-startup-success-percentage"":0.0,""connector-startup-attempts-total"":1.0,""connector-count"":0.0,""connector-startup-success-percentage"":0.0,""task-startup-success-total"":90.0,""task-startup-failure-percentage"":0.0,""task-count"":2.0,""task-startup-failure-total"":0.0},""timestamp"":1623852927,""status"":200}   Task Error metrics mbean:  {""request"":\{""mbean"":""kafka.connect:connector=*,task=*,type=task-error-metrics"",""type"":""read""} ,""value"":{""kafka.connect:connector=***********,task=35,type=task-error-metrics"": {""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0} ,""kafka.connect:connector=**********,task=38,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=14,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=5,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=0,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=29,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=37,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=28,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=25,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=91,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=31,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=7,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=74,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=2,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=26,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=30,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=*********,task=53,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0,""total-records-skipped"":0.0,""total-record-errors"":0.0,""total-retries"":0.0},""kafka.connect:connector=**********,task=16,type=task-error-metrics"":{""last-error-timestamp"":0,""total-errors-logged"":0.0,""deadletterqueue-produce-requests"":0.0,""deadletterqueue-produce-failures"":0.0,""total-record-failures"":0.0, .....  Solution:  As part of the bug fix to KAFKA-12965 introducing code changes to gracefully cleanup the error handling metrics associated with a task. This is required to avoid duplicate metrics of task being reported from a worker that had the same task in the past.  UT - Not yet covered, in progress","open","connect,","ramesh-muthusamy","2021-06-21T11:01:34Z","2021-12-08T15:06:53Z"
"","11270","MINOR: print debug log for InvalidRequestException","InvalidRequestException is not an error case","closed","","ccding","2021-08-26T22:08:57Z","2021-09-22T18:04:53Z"
"","11254","KAFKA-2424: Introduce Scalafix linter","Introduces Scalafix, a linter with some rewrite capabilities on top (https://scalacenter.github.io/scalafix/)  By running the `checkScalafix` gradle tasks a report is printed with all broken rules.  Running `scalafix` gradle task will apply some rewrites on the files, for example: import ordering, and annotating return types. This change uses the official gradle plugin for Scalafix, and Scalafix itself is maintained by the Scala Center.  Current rules checked: - Do not allow `final var` - Explicit return types for public and protected methods (for public   `val` and `var` the change was too big. This can be done at a later time) - Avoid procedure syntax in Scala - Avoid use of val in for comprehensions (it's a deprecated feature)  *Known limitations*: when automatically inferring the return types, Scalafix might fail and pick a ""too wide"" type, i.e. `Any` instead of the correct one. After letting Scalafix fix the files, contributors should still look at the changes done and validate they are correct.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-08-24T11:46:39Z","2021-09-22T19:41:40Z"
"","10707","KAFKA-12792: Fix metrics bug and introduce TimelineInteger","Introduce a TimelineInteger class which represents a single integer value which can be changed while maintaining snapshot consistency. Fix a case where a metric value would be corrupted after a snapshot restore.","closed","kip-500,","cmccabe","2021-05-16T06:08:22Z","2021-05-17T17:21:30Z"
"","11134","KAFKA-12851: Fix Raft partition simulation","Instead of waiting for a high-watermark of 20 after the partition, the test should wait for the high-watermark to reach an offset greater than the largest log end offset at the time of the partition. Only that offset is guarantee to be reached as the high-watermark by the new majority.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-27T16:37:37Z","2021-07-28T16:28:57Z"
"","11228","KAFKA-12887 Skip some RuntimeExceptions from exception handler","Instead of letting all `RuntimeException`s go through and be processed by the uncaught exception handler, `IllegalStateException` and `IllegalArgumentException` are not passed through and fail fast.  Added test checking this new case. For the test I use an existing test as a baseline that was checking the uncaught exception handler was called and checked that it wasn't called for this new particular case.  Possible extensions would be to add more types of `RuntimeException`s on the new `catch` clause.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-08-18T10:59:05Z","2021-09-01T17:08:33Z"
"","10715","KAFKA-9295: increase heartbeat and session timeout","increase heartbeat and session timeout to make the test reliable. session timeout -> 20 sec heartbeat timeout -> 20/3 = 7 sec  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-05-18T02:46:43Z","2021-05-19T06:01:38Z"
"","11378","MINOR: The message of BufferExhaustedException should include pool st…","Including the pool status can help us to understand what happens in the moment. The new message is shown below. ` Failed to allocate 200 bytes within the configured max blocking time 10 ms. Total memory: 300 bytes. Available memory: 100 bytes. Poolable size: 100 bytes `  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-10-04T13:42:32Z","2021-10-13T14:52:36Z"
"","11154","KAFKA-13068: Rename Log to UnifiedLog","In this PR, I've renamed `kafka.log.Log` to `kafka.log.UnifiedLog`. With the advent of KIP-405, going forward the existing `Log` class would present a unified view of local and tiered log segments, so we rename it to `UnifiedLog`. The motivation for this PR is also the same as outlined in this design document: https://docs.google.com/document/d/1dQJL4MCwqQJSPmZkVmVzshFZKuFy_bCPtubav4wBfHQ/edit.  This PR is a follow-up to https://github.com/apache/kafka/pull/10280 where we had refactored the Log layer introducing a new `kafka.log.LocalLog` class.  **Note:** the `Log` class name had to be hardcoded to ensure metrics are defined under the `Log` class (for backwards compatibility). Please refer to the newly introduced `UnifiedLog.metricName()` method.  **Tests:** Relying on existing unit & integration tests to surface any regressions.","closed","","kowshik","2021-07-31T01:11:21Z","2021-08-12T23:10:19Z"
"","10732","MINOR: Eliminate redundant functions in LogTest suite","In this PR, I have eliminated the following 6 redundant static functions in test code: ``` LogTest.createLogConfig() LogTest.createLog() LogTest.hasOffsetOverflow() LogTest.firstOverflowSegment() LogTest.rawSegment() LogTest.initializeLogDirWithOverflowedSegment() ```  The above ones are already defined in `LogTestUtils.scala`, so we can just reuse those instead.  **Tests:** Relying on existing tests.","closed","","kowshik","2021-05-19T23:52:29Z","2021-05-20T15:46:35Z"
"","10723","MINOR: Remove unused maxProducerIdExpirationMs parameter in Log constructor","In this PR, I have cleaned up the unused `maxProducerIdExpirationMs` parameter in the `Log` constructor. https://github.com/apache/kafka/pull/10478 had moved the `maxProducerIdExpirationMs` parameter into `LogLoader`, so this is no longer required to be passed into the `Log` constructor.   **Tests:** Relying on existing tests.","closed","","kowshik","2021-05-19T07:28:17Z","2021-05-20T15:06:10Z"
"","11425","KAFKA-13393 Documentation - Add missing arguments to create a topic","In the quickstart documentation (quickstart_createtopic) , there is a command specified to create a topic:   `$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092`  However, it is no longer working due to missing arguments:   - partitions   - replications-factor  The previous command should be replaced with this one:   `$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`","closed","","FlorianLehmann","2021-10-21T22:00:34Z","2021-10-25T22:04:47Z"
"","10668","MINOR: Apply try-with-resource to KafkaStreamsTest","In the PR #9821, @mjsax 's [comment](https://github.com/apache/kafka/pull/9821#discussion_r556200365)  > We should use a try-with-resources clause to make sure close() is called. > > Seems, other tests in this class have a similar issue. Would be good to fix all test accordingly (if you want you can also to a separate PR for it).  Let me address this before the KAFKA-5876(KIP-216) part 5.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vitojeng","2021-05-11T07:25:11Z","2021-06-03T04:35:17Z"
"","11420","KAFKA-12648: extend IQ APIs to work with named topologies","In the new NamedTopology API being worked on, state store names and their uniqueness requirement is going to be scoped only to the owning topology, rather than to the entire app. In other words, two different named topologies can have different state stores with the same name.  This is going to cause problems for some of the existing IQ APIs which require only a name to resolve the underlying state store. We're now going to need to take in the topology name in addition to the state store name to be able to locate the specific store a user wants to query","open","","ableegoldman","2021-10-21T04:38:37Z","2021-10-21T20:08:05Z"
"","11381","KAFKA-12648: allow users to set a StreamsUncaughtExceptionHandler on individual named topologies","In the current exception handler, there's not much granularity and all exceptions from all sources are treated equally. With the introduction of independent named topologies, it would be nice to have some way to differentiate which topology an exception was thrown from during processing.  Currently we only consider exceptions thrown from the `process()` method of a task in that named topology, but eventually we can expand this to other exceptions that are tied to a particular topology, for example errors pertaining to the topics of that query","closed","","ableegoldman","2021-10-05T08:07:13Z","2021-10-18T19:47:37Z"
"","11376","KAFKA-13342: LeaderAndIsrRequest should not be sent for topic queued for deletion","In some cases a broker may be lost during a topic deletion and before its replica has moved to OfflineReplica state. When the broker comes back up the controller will send it a LeaderAndIsrRequest containing the partition even though it is already in the deleting state in the controller.","open","","lbradstreet","2021-10-04T05:48:10Z","2022-03-21T02:59:14Z"
"","11031","KAFKA-13067 Add internal config to lower the metadata log segment size","In order to facilitate system and integration tests that use a smaller log segment size, we are adding this internal config to lower the minimum. During normal operation, this config will use the default size of 8Mb (as defined by KafkaRaftClient).   The new config is named `metadata.log.segment.min.bytes`. If it is set to a non-default value, and ERROR level log will be printed","closed","","mumrah","2021-07-12T20:50:56Z","2021-07-13T23:23:32Z"
"","11111","KAFKA-13126: guard against overflow when computing `joinGroupTimeoutMs`","In older versions of Kafka Streams, the `max.poll.interval.ms` config was overridden by default to `Integer.MAX_VALUE`. Even after we removed this override, users of both the plain consumer client and kafka streams still set the poll interval to MAX_VALUE somewhat often. Unfortunately, this causes an overflow when computing the `joinGroupTimeoutMs` and results in it being set to the `request.timeout.ms` instead, which is much lower.  This can easily make consumers drop out of the group, since they must rejoin now within 30s (by default) yet have no obligation to almost ever call poll() given the high `max.poll.interval.ms`. We just need to check for overflow and fix it to `Integer.MAX_VALUE` when it occurs.  Also fixes a few other misc. possible overflows on the side (from a ticket I came across while searching for existing tickets on the joinGroupTimeout bug: [KAFKA-6948](https://issues.apache.org/jira/browse/KAFKA-6948))","closed","","ableegoldman","2021-07-22T21:58:52Z","2021-07-23T23:22:47Z"
"","10935","KAFKA-13003: In kraft mode also advertise configured advertised port instead of socket port","In Kraft mode Apache Kafka 2.8.0 does advertise the socket port instead of the configured advertised port.  A broker given with the following configuration ``` listeners=PUBLIC://0.0.0.0:19092,REPLICATION://0.0.0.0:9091 advertised.listeners=PUBLIC://envoy-kafka-broker:9091,REPLICATION://kafka-broker1:9091 ``` advertises on the _PUBLIC_ listener _envoy-kafka-broker:19092_, however I would expect that _envoy-kafka-broker:9091_ is advertised. In ZooKeeper mode it works as expected.  The reason is that in the BrokerServer at the moment the socket server port is used for registration at the controller: https://github.com/apache/kafka/blob/2beaf9a720330615bc5474ec079f8b4b105eff91/core/src/main/scala/kafka/server/BrokerServer.scala#L286  In KafkaServer class which is used in ZooKeeper mode the configured advertised port is used: https://github.com/apache/kafka/blob/2beaf9a720330615bc5474ec079f8b4b105eff91/core/src/main/scala/kafka/server/KafkaServer.scala#L462  I changed the BrokerServer class, so that in Kraft mode like in ZooKeeper mode also the configured advertised port is registered.  I manually tested it with a Docker-Compose setup. It basically runs 3 Kafka Broker with Apache Kafka 2.8 in Kraft mode and an Envoy proxy in front of them. With Apache Kafka 2.8.0 it does not work, because Kafka does not advertise the configured advertised port. For more details about the setup see: https://github.com/ueisele/kafka/tree/fix/kraft-advertisedlisteners-build/proxy-examples/proxyl4-kafkakraft-bug-2.8  The same Docker-Compose setup with the fix (proposed in this pull request) works and advertises the configured advertised port. For more details see: https://github.com/ueisele/kafka/tree/fix/kraft-advertisedlisteners-build/proxy-examples/proxyl4-kafkakraft-fix-2.8  At the moment there is no dedicated test for BrokerServer class. Therefore I did not create a test so far. Where such a test should be added? Is https://github.com/apache/kafka/blob/trunk/core/src/test/scala/integration/kafka/server/RaftClusterTest.scala a good place?","closed","kip-500,","ueisele","2021-06-28T14:29:28Z","2021-07-12T20:40:10Z"
"","10903","KAFKA-13023: make ""range, cooperative-sticky"" as the default assignor in V3.0","In KIP-726, we want to make ""cooperative-sticky, range"" as default assignor. But before this can be applied, we need to handle some important cooperative-sticky assignor bugs first. We plan to implement it in early V3.1. In V3.0, we plan to make ""range, cooperative-sticky"" as the default assignor, so that if user wants to use cooperative-sticky assignor from old version byte-code, they just need 1 rolling bounce, to set the cooperative-sticky assignor in the `PARTITION_ASSIGNMENT_STRATEGY_CONFIG` config.  This PR also adds test to make sure the default assignor is still **range** assignor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-18T07:43:59Z","2021-07-08T05:27:31Z"
"","10973","KAFKA-13033: COORDINATOR_NOT_AVAILABLE should be unmapped","In KIP-699, we add some handler to handle different types of operation. In the `handleError`, we didn't make the `COORDINATOR_NOT_AVAILABLE` as unmapped, to do a re-lookup. In [DescribeTransactionsHandler](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/DescribeTransactionsHandler.java#L172-L186), there's already explained by Jason Gustafson why `COORDINATOR_NOT_AVAILABLE` and `NOT_COORDINATOR` should be listed in unmapped, and `COORDINATOR_LOAD_IN_PROGRESS` should not.  ```java case COORDINATOR_LOAD_IN_PROGRESS:     // If the coordinator is in the middle of loading, then we just need to retry     log.debug(""DescribeTransactions request for transactionalId `{}` failed because the "" +             ""coordinator is still in the process of loading state. Will retry"",         transactionalIdKey.idValue);     break;  case NOT_COORDINATOR: case COORDINATOR_NOT_AVAILABLE:     // If the coordinator is unavailable or there was a coordinator change, then we unmap     // the key so that we retry the `FindCoordinator` request     unmapped.add(transactionalIdKey);     log.debug(""DescribeTransactions request for transactionalId `{}` returned error {}. Will attempt "" +             ""to find the coordinator again and retry"", transactionalIdKey.idValue, error);     break; ```  We should be consistent with it. Fix it, add logs and comments, and also update the tests.   The changed handler list: ``` AlterConsumerGroupOffsetsHandler DeleteConsumerGroupOffsetsHandler DeleteConsumerGroupsHandler DescribeConsumerGroupsHandler ListConsumerGroupOffsetsHandler RemoveMembersFromConsumerGroupHandler ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-04T10:18:14Z","2021-07-22T10:58:54Z"
"","11475","KAFKA-7077: Use default producer settings in Connect Worker","In KAFKA-7077 and the associated KIP-318, there's a desire to a) enable producer idempotence and b) set max in flight requests to 5 to improve throughput, as opposed to the previous hardwired 1.   With the change of producer defaults in Kafka 3.0.0 (acks = all, enable.idempotence = true, max.in.flight.requests.per.connection=5) all this behaviour comes for free if the explicit producer configuration in the Worker is removed.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","LiamClarkeNZ","2021-11-08T03:09:43Z","2022-03-23T01:31:08Z"
"","10794","KAFKA-12677: return not_controller error in envelope response itself","In Kafka Raft mode, the flow sending request from client to controller is like this: 1. client send reqesut to a random controller (ex: A-controller) 2. A-controller will forward the request to active controller (ex: B-controller) to handle the request 3. After active B-controller completed the request, the A-controller will receive the response, and do a check:   3.1. if the response has ""disconnected"" or ""NOT_CONTROLLER"" error, which means the cached active controller is changed. So, clear the cached active controller, and wait for next retry to get the updated active controller from `controllerNodeProvider`   3.2. else, complete the request and respond back to client  In this bug, we have 2 issues existed: 1. ""NOT_CONTROLLER"" exception won't be correctly send back to the requester, instead, `UNKNOWN_SERVER_ERROR` will be returned. The reason is the `NotControllerException` is wrapped by a `CompletionException` when the `Future` completeExceptionally. And the `CompletionException` will not match any Errors we defined, so the `UNKNOWN_SERVER_ERROR` will be returned. Even if we don't want the `NotControllerException` return back to client, we need to know it to do some check.  fix 1: unwrap the `CompletionException` before encoding the exception to error.  2. Even if we fixed 1st bug, we still haven't fixed this issue. After the 1st bug fixed, the client can successfully get `NotControllerException` now, and keep retrying... until timeout. So, why won't it meet the flow `3.1` mentioned above, since it has `NotControllerException`? The reason is, we wrapped the original request with `EnvelopeRequest` and forwarded to active controller. So, after the active controller completed the request, responded with `NotControllerException`, and then, wrapped into an `EnvelopeResponse` **with no error**, and then send the `EnvelopeResponse` back. That is, in the flow `3.1`, we only got ""no error"" from `EnvelopeResponse`, not the `NotControllerException` inside.  new: fix 2: Make the envelope response return `NotControllerException` if the controller response has `NotControllerException`. So that we can catch the `NotControllerException` on envelopeResponse to update the active controller.  old: fix 2: in flow `3.1`, parse the `EnvelopeResponse` to check if there's `NotControllerException` inside.  Note: in the jira ticket we think there's `Recorded new controller` log, which should already changed to new active controller: ``` [broker0_:BrokerToControllerChannelManager broker=0 name=heartbeat]: Recorded new controller, from now on will use broker localhost:54229 (id: 3000 rack: null)  ``` It's is true, but after further investigation, I found it only changed for this thread: `broker0_:BrokerToControllerChannelManager broker=0 name=heartbeat`. The topic creation thread is forwarding thread: `broker0_:BrokerToControllerChannelManager broker=0 name=forwarding`, which still using old  active controller.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","showuon","2021-05-30T11:53:11Z","2021-07-12T16:17:47Z"
"","11413","KAFKA-13370: add errors when commit offsets failed and add tests","In https://github.com/apache/kafka/pull/9642, we removed the unnecessary `success` parameter, and use the `error` as the key to identify if the commit successfully or failed. That's a good improvement. However, there are some cases we passed `success` with `false`, but without `error` value. I think we should always pass the `error` value when failed. Fix it and add tests. After this fix, all `recordCommitFailure` call will always pass with an error.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","showuon","2021-10-19T08:57:33Z","2021-11-16T15:38:25Z"
"","11370","MINOR: remove unneeded size and add lock coarsening to inMemoryKeyValueStore","In https://github.com/apache/kafka/pull/7212, we reverted the change to use `TreeMap` instead of `ConcurrentSkipListMap`, but we forgot to remove the `size` computing codes.  Also, I found the `putAll` method might be slow because the method itself is not locked, but it calls a locked method inside a loop. That is, if we elements we put has 1000 size, we might do lock and unlock for 1000 times. Although the JVM ""might"" do ""lock coarsening"" for us, it'd better we make sure it works as what we expected.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","showuon","2021-09-30T08:10:56Z","2021-10-07T01:32:23Z"
"","11426","KAFKA-13391: don't fsync directory on Windows OS","In https://github.com/apache/kafka/pull/10680, we added `fysnc` on dir to maintain crash consistency. But, it looks like Windows OS doesn't support `fsync` on directory. The same issues also happen on [LUCENE](https://issues.apache.org/jira/browse/LUCENE-5588) and [HDFS](https://issues.apache.org/jira/browse/HDFS-13586) projects. And the way they fix it is pretty much the same: to skip fsync directory on Windows OS. Here are the patches for both [LUCENE-5588](https://patch-diff.githubusercontent.com/raw/apache/lucenenet/pull/43.patch) and [HDFS-13586](https://issues.apache.org/jira/secure/attachment/12924032/HDFS-13586.001.patch).   I followed their way to fix this issue. No tests added since it's just an OS check added, no logic change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-10-22T06:47:29Z","2022-02-16T08:46:43Z"
"","10713","HOTFIX: undo renaming of public part of Subtopology API","In https://github.com/apache/kafka/pull/10676 we renamed the internal `Subtopology` class that implemented the `TopologyDescription.Subtopology` interface. By mistake, we also renamed the interface itself, which is a public API. This wasn't really the intended point of that PR, so rather than do a retroactive KIP, let's just reverse the renaming.","closed","","ableegoldman","2021-05-17T16:40:20Z","2021-05-18T19:59:21Z"
"","11429","KAFKA-13396: allow create topic without partition/replicaFactor","In https://github.com/apache/kafka/pull/10457/files#r635196307, the reviewer asked if there's a bug that we only check for zookeeper case, I didn't do a well check and directly confirm it. Now, after checking the change history, I found the change is because the [KIP-464](https://cwiki.apache.org/confluence/display/KAFKA/KIP-464%3A+Defaults+for+AdminClient%23createTopic) (PR: https://github.com/apache/kafka/pull/6728) added default values for partition count and replica factor for createTopic in AdminClient. So it didn't apply for zookeeper option case.   Fix this bug and add tests for the command lines in quick start (i.e. create topic and describe topic), to make sure it won't be broken in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-10-24T07:51:23Z","2021-11-04T19:51:43Z"
"","10940","KAFKA-13007: KafkaAdminClient getListOffsetsCalls reuse cluster snapshot","in getListOffsetsCalls, we rebuild the cluster snapshot for every topic partition. instead, we should reuse a snapshot. this will reduce the time complexity from O(n^2) to O(n).  for manual testing (used AK 2.8), i've passed in a map of 6K topic partitions to listOffsets  without snapshot reuse: duration of building futures from metadata response: **15582** milliseconds total duration of listOffsets: **15743** milliseconds  with reuse: duration of building futures from metadata response: **24** milliseconds total duration of listOffsets: **235** milliseconds  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeffkbkim","2021-06-29T16:20:26Z","2021-07-15T19:06:20Z"
"","10593","KAFKA-10800 Enhance the test for validation when the state machine creates a snapshot","In general, there are two ways of creating a snapshot. One is by the state machine through `RaftClient::createSnapshot` and `SnapshotWriter`, this PR mainly adds validation for this case. Another way is by the `KafkaRaftClient` itself downloading the snapshot from the quorum leader. In the second case, we want to trust the leader's snapshot and not perform the validation.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","feyman2016","2021-04-25T02:37:58Z","2021-11-14T14:22:31Z"
"","11094","MINOR: Improve usage of LogManager.currentDefaultConfig","In `deleteLogs`, we use a consistent value for `fileDeleteDelayMs` for the whole method. In `DynamicBrokerConfig.reconfigure`, it's a minor readability improvement, but there should be no change in behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-20T14:11:28Z","2021-07-21T13:12:06Z"
"","11332","MINOR: re-add removed test coverage for 'KAFKA-12983: reset needsJoinPrepare flag'","In [11231](https://github.com/apache/kafka/pull/11231) we fixed a bug in which the consumer would reset its state unnecessarily, and fixed up the tests accordingly. Unfortunately this also wiped out the test coverage for https://issues.apache.org/jira/browse/KAFKA-12983 that was added in [10986](https://github.com/apache/kafka/pull/10986).  I noticed this when cherrypicking that fix back to the 2.7 branch and hitting merge conflicts in the test. I moved/replaced the test coverage for KAFKA-12983 in the cherrypicked fix, so we should now port this forward to the newer branches.  Verified that without the fix (resetting the flag), this does indeed cause the test to fail. Should be cherrypicked back to 2.8","closed","","ableegoldman","2021-09-17T05:53:44Z","2021-10-07T02:09:36Z"
"","10484","MINOR: un-deprecate StreamsConfig overloads to support dependency injection","In [#5344](https://github.com/apache/kafka/pull/5344#issuecomment-413350338) it came to our attention that the StreamsConfig overloads of the KafkaStreams constructors are actually quite useful for dependency injection, providing a cleaner way to configure dependencies and better type safety.   We considered removing these deprecated overloads in the upcoming 3.0 release, but decided against it for the above reasons. Since we no longer intend to remove these APIs it makes sense to drop the Deprecation entirely, so users can start or continue to use them without worry.","closed","streams,","ableegoldman","2021-04-06T02:46:00Z","2021-04-08T04:13:33Z"
"","11101","MINOR: Remove redundant fields in dump log record output","In 2.8, the dump log output regressed to print batch level information for each record, which makes the output considerably noisier. This patch changes the output to what it was in 2.7 and previous versions. We only print batch metadata at the batch level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-07-21T18:26:38Z","2021-07-23T22:56:41Z"
"","11330","Make test-utils MockTime more flexible","Improve test-utils by making `TopologyTestDriver` more flexible in regard to `MockTime`.  Existing unit tests were adapted to make use of the new (Mock)Time call chain, specific tests were added and coverage improved.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","reneleonhardt","2021-09-16T07:34:06Z","2021-09-16T07:34:06Z"
"","10634","KAFKA-12754: Improve endOffsets for TaskMetadata","Improve endOffsets for TaskMetadata also add an int test for TaskMetadata offset collections  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-05-05T16:25:50Z","2021-05-14T19:57:18Z"
"","10822","KAFKA-4793: Connect API to restart connector and tasks (KIP-745)","Implements [KIP-745](https://cwiki.apache.org/confluence/display/KAFKA/KIP-745%3A+Connect+API+to+restart+connector+and+tasks) to change connector restart API to also restart tasks.  Testing strategy  - [x]  Unit tests added for all possible combinations of onlyFailed and includeTasks - [x]  Integration tests added for all possible combinations of onlyFailed and includeTasks - [x]  System tests for happy path   @rhauch @kkonstantine @tombentley @ryannedolan @dongjinleekr  Could you please check if this looks good?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","kpatelatwork","2021-06-04T18:35:20Z","2021-07-01T04:13:08Z"
"","10907","KAFKA-10000: Exactly-once support for source connectors (KIP-618)","Implements [KIP-618](https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Newmetrics).  Depends on changes from https://github.com/apache/kafka/pull/11046  There are several changes here that can be reviewed fairly independently of each other: - Support for transactional source tasks, which is largely implemented in the `ExactlyOnceWorkerSourceTask` class, its newly-introduced `AbstractWorkerSourceTask` superclass, the `Worker` class (whose API for task starts has been split up from the existing `startTask` method into separate `startSinkTask`, `startSourceTask`, and `startExactlyOnceSourceTask` methods), and the `WorkerTransactionContext` class (which is used to allow connectors to define their own transaction boundaries) - Zombie fencing logic and the use of a transactional producer for some writes to the config topic, which are done by the leader of the cluster and are largely implemented in the `DistributedHerder`, `ConfigBackingStore`, `KafkaConfigBackingStore`, and `ClusterConfigState` classes - A new method in the `Admin` API for fencing out transactional producers by ID, which is done with changes to the `Admin` interface (unsurprisingly) and the `KafkaAdminClient` class - Support for per-connector offsets topics, which touches on the `Worker`, `OffsetStorageReaderImpl`, and `OffsetStorageWriter` classes - A few new `SourceConnector` methods for communicating support for exactly-once guarantees and connector-defined transactions; these take place in the `SourceConnector` class (also unsurprisingly) and the `AbstractHerder` class  Existing unit tests are expanded where applicable, and new ones have been introduced where necessary.  Eight new integration tests are added, which cover scenarios including preflight validation checks, all three types of transaction boundary, graceful recovery of the leader when fenced out from the config topic by another worker, ensuring that the correct number of task producers are fenced out across generations, accurate reporting of failure to bring up tasks when fencing does not succeed (includes an ACL-secured embedded Kafka cluster to simulate one of the most likely potential causes of this issue--insufficient permissions on the targeted Kafka cluster), and the use of a custom offsets topic.  Many but not all existing system tests are modified to add cases involving exactly-once source support, which helps give us reasonable confidence that the feature is agnostic with regards to rebalance protocol. A new test is added that is based on the existing bounce test, but with no sink connector and with stricter expectations for delivery guarantees (no duplicates are permitted).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","C0urante","2021-06-19T09:16:22Z","2022-02-17T17:04:57Z"
"","11435","KAFKA-13395: Emit Client Quota Values as Metric","Implementation of proposed changes outlined in KIP 786.  - Optionally emits client quota values attached to the `byte-rate` and   `throttle-time` under the `kafka.server` MBean.   - This is done by the creation of new attributes `byte-quota-value`     and `request-quota-value` respectively. - Enabling the additional metrics is done through the boolean broker   config `client.quota.value.metric.value`  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","MasonLegere","2021-10-26T18:43:50Z","2021-10-28T07:48:08Z"
"","11302","KAFKA-13243: KIP-773 Differentiate metric latency measured in ms and ns","Implementation of [KIP-773](https://cwiki.apache.org/confluence/x/ZwNACw)  - Deprecates inconsistent metrics `bufferpool-wait-time-total`, `io-waittime-total`, and `iotime-total`. - Introduces new metrics `bufferpool-wait-time-ns-total`, `io-wait-time-ns-total`, and `io-time-ns-total` with the same semantics as before. - Adds metrics (old and new) in ops.html. - Adds upgrade guide for these metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-09-06T19:16:18Z","2021-09-08T17:05:41Z"
"","10889","KAFKA-12891: KIP-749 Add --files and --file-separator options to the ConsoleProducer (WIP)","Implementation of [KIP-749](https://cwiki.apache.org/confluence/display/KAFKA/KIP-749%3A+Add+--files+and+--file-separator+options+to+the+ConsoleProducer)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-06-16T10:27:32Z","2021-06-17T01:26:01Z"
"","10840","KAFKA-12849: KIP-744 TaskMetadata ThreadMetadata StreamsMetadata as API","Implementation of [KIP-744](https://cwiki.apache.org/confluence/x/XIrOCg).  Creates new Interfaces for TaskMetadata, ThreadMetadata, and StreamsMetadata, providing internal implementations for each of them.  Deprecates current TaskMetadata, ThreadMetadata under o.a.k.s.processor, and SreamsMetadata under a.o.k.s.state.  Updates references on internal classes from deprecated classes to new interfaces.  Deprecates methods on KStreams returning deprecated ThreadMeatada and StreamsMetadta, and provides new ones returning the new interfaces.  Update Javadocs referencing to deprecated classes and methods to point to the right ones.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-06-08T12:26:43Z","2021-06-25T16:43:26Z"
"","10813","KAFKA-9559: Change default serde to be `null`","Implementation of [KIP-741](https://cwiki.apache.org/confluence/display/KAFKA/KIP-741%3A+Change+default+serde+to+be+null) per [KAFKA-9559](https://issues.apache.org/jira/browse/KAFKA-9559). Changes the default serde from `byteArray` to `null`. This allows us to throw `ConfigExceptions` instead of NPEs which will give users a better idea of where their serdes are misconfigured.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","lct45","2021-06-03T22:51:19Z","2021-07-01T21:58:29Z"
"","11464","KAFKA-13431: Sink Connectors: Add support for topic-mutating SMTs to async users","Implementation for proposal described in [KIP-793](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=191336830).  It mainly consists of adding a new method in SinkRecord to get the original topic and partition, before applying transformations.  Unit and integration tests are included.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","erdody","2021-11-03T19:28:13Z","2022-04-20T00:29:36Z"
"","10867","KAFKA-12931: KIP-746: Revise KRaft Metadata Records","Implement the metadata records changes described in KIP-746.","closed","","cmccabe","2021-06-11T15:04:09Z","2021-06-15T23:39:21Z"
"","10494","KAFKA-12788: improve KRaft replica placement","Implement the existing Kafka replica placement algorithm for KRaft. This also means implementing rack awareness.  Previously, we just chose replicas randomly in a non-rack-aware fashion.  Also, allow replicas to be placed on fenced brokers if there are no other choices.  This was specified in KIP-631 but previously not implemented.","closed","kip-500,","cmccabe","2021-04-06T22:43:23Z","2021-05-17T23:49:51Z"
"","11083","KAFKA-13010: Retry getting tasks incase of rebalance for TaskMetadata tests","If there is a cooperative the tasks might not be assigned to a thread so retrying should work in a very short timeframe  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-07-19T23:08:01Z","2021-08-24T16:16:11Z"
"","11452","KAFKA-13412; Ensure initTransactions() safe for retry after timeout","If the user's `initTransactions` call times out, the user is expected to retry. However, the producer will continue retrying the `InitProducerId` request in the background. If it happens to return before the user retry of `initTransactions`, then the producer will raise an exception about an invalid state transition.   The patch fixes the issue by checking both the current state as well as the prior state in order to validate whether the retry is expected.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-10-29T22:12:29Z","2022-01-19T21:20:41Z"
"","10712","KAFKA-12798: Fixing MM2 rebalance timeout issue when source cluster is not available","If the network configuration of a source cluster which is taking part in a replication flow is changed (change of port number, if, for instance TLS is enabled or disabled) MirrorMaker2 won't update its internal configuration even after a reconfiguration followed by a restart.  What happens in MirrorMaker2 after a cluster ""identity"" (i.e. connectivity config) changes:  1. MM2 driver (MirrorMaker class) starts up with the new config. 2. DistributedHerder joins a dedicated consumer group that decides which driver instance has control over the assignments and the configuration topic. 3. The driver caches the consumer group assignment, which indicates that it is the leader of the group. 4. The driver reads the configuration topic (which is still not containing the new config), and starts the mm connectors. 5. Since the old config is invalid, the connectors cannot connect to the cluster anymore - MirrorSourceConnector tries to query the cluster through the admin client, but the queries time out after 2 minutes (it contains 2 tasks affecting the source cluster, the timeout is 1 minute for both). In the meantime, the background heartbeat thread checks on the state of the herder consumer membership. There is a default rebalance timeout of 1 minute. Since the herder thread was blocked due to the connector query timeouts, it wasn't able to call poll on the consumer. Heartbeat thread invalidates the consumer membership and triggers a new consumer creation. 6. The herder thread finishes the connector startup, and after realizing that the configuration has changed, tries to update the config topic.  The config topic can only be updated by the leader herder. The driver checks the group assignment to see if it is the leader. In the local cache, the old assignment is present, in which the leader is the previous consumer with its own ID. The current consumer ID of the driver does not match the cached leader ID.  7. The driver refuses to update the config topic.  **The proposed fix for this:** The rebalance issue can be fixed by decreasing the time that we wait for tasks that affects the source cluster at the start of MM2. By decreasing the timeout (from 1 minute to 15 seconds by default), if the kafka config is old, the tasks affecting the source cluster won't block for too long. With this the herder will be able to update the config topic. This timout is configurable now and defaults to 15 seconds.  Also needed to increase the number of threads in the scheduler so that other tasks won't be blocked.  **Testing done:**  -  configure replication between source->target -  checked that the replication is working -  change source kafka cluster broker port -  restart kafka/mirrormaker2, produced new messages in the replicated topic -  after the restart mm2 was trying to use the old kafka configs, and even after a long time, it couldn't replicate. After applying the fix, the issue was solved, replication worked.  Also tested with the same scenario, but instead of changing the port, ssl was turned on the source kafka cluster.","open","","bmaidics","2021-05-17T13:13:07Z","2021-05-17T20:58:40Z"
"","11347","KAFKA-13296: warn if previous assignment has duplicate partitions","If for some reason partition revoked did not get called due to some bug it would be good to log a warning that the previous assignment was not clean. This could affect eos and it might be good to be able to revoke a partition instead of using partitions lost ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","wcarlson5","2021-09-20T21:52:09Z","2021-09-27T14:14:35Z"
"","10930","KAFKA-12996; Return OFFSET_OUT_OF_RANGE for fetchOffset < startOffset even for diverging epochs","If fetchOffset < startOffset, we currently throw OffsetOutOfRangeException when attempting to read from the log in the regular case. But for diverging epochs, we return Errors.NONE with the new leader start offset, hwm etc.. ReplicaFetcherThread throws OffsetOutOfRangeException when processing responses with Errors.NONE if the leader's offsets in the response are out of range and this moves the partition to failed state. The PR adds a check for this case when processing fetch requests and throws OffsetOutOfRangeException regardless of epoch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-06-25T17:53:39Z","2021-06-29T17:18:47Z"
"","11200","KAFKA-13192: Prevent inconsistent broker.id/node.id values","If both `broker.id` and `node.id` are set, and they are set inconsistently (e.g.`broker.id=0`, `node.id=1`) then currently the value of `node.id` is used and the `broker.id` value is left at the original value. The server should detect this inconsistency, throw a ConfigException, and fail to start.  This patch adds the check and a test for it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rondagostino","2021-08-11T20:34:27Z","2021-08-25T13:45:07Z"
"","10983","mention IdentityReplicationPolicy in ops docs","IdentityReplicationPolicy was added to MM2 in 3.0.0, so we note it in the docs.","open","","ryannedolan","2021-07-06T19:46:53Z","2021-09-09T12:39:12Z"
"","10916","KAFKA-12938: fix and reenable testChrootExistsAndRootIsLocked test","I've tried many solutions and found the addAuthInfo and set Acl to a new digest ID would fail the build, somehow. So, I changed the test method, to use the acl `ZooDefs.Ids.READ_ACL_UNSAFE` to simulate the root is locked case. Because root is read-only, so the chroot can't have permission to create node. This test can test what the original issue/fix in KAFKA-12866.   The latest 5 builds are all in yellow light, which is after I changed the solution to use `ZooDefs.Ids.READ_ACL_UNSAFE` to simulate it. ![image](https://user-images.githubusercontent.com/43372967/123353600-703f3a00-d594-11eb-9305-599b294a1b37.png)   The failed tests are unrelated. ```     Build / JDK 15 and Scala 2.13 / kafka.server.RaftClusterTest.testCreateClusterAndCreateAndManyTopics()     Build / JDK 15 and Scala 2.13 / kafka.server.RaftClusterTest.testCreateClusterAndCreateListDeleteTopic()     Build / JDK 8 and Scala 2.12 / kafka.server.RaftClusterTest.testCreateClusterAndCreateAndManyTopicsWithManyPartitions()     Build / JDK 8 and Scala 2.12 / org.apache.kafka.common.network.SslTransportLayerTest.[1] tlsProtocol=TLSv1.2, useInlinePem=false     Build / JDK 8 and Scala 2.12 / org.apache.kafka.common.network.SslTransportLayerTest.[2] tlsProtocol=TLSv1.2, useInlinePem=true     Build / JDK 8 and Scala 2.12 / org.apache.kafka.common.network.SslTransportLayerTest.[1] tlsProtocol=TLSv1.2, useInlinePem=false     Build / JDK 8 and Scala 2.12 / org.apache.kafka.common.network.SslTransportLayerTest.[2] tlsProtocol=TLSv1.2, useInlinePem=true     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[1] tlsProtocol=TLSv1.2, useInlinePem=false     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[2] tlsProtocol=TLSv1.2, useInlinePem=true     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[3] tlsProtocol=TLSv1.3, useInlinePem=false     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[1] tlsProtocol=TLSv1.2, useInlinePem=false     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[2] tlsProtocol=TLSv1.2, useInlinePem=true     Build / JDK 11 and Scala 2.13 / org.apache.kafka.common.network.SslTransportLayerTest.[3] tlsProtocol=TLSv1.3, useInlinePem=false ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-22T03:22:42Z","2021-06-28T10:52:00Z"
"","11450","KAFKA-13414: Replace Powermock/EasyMock by Mockito in connect.storage","I've skipped the following classes as they use powermock to stub/access private and static fields/methods: - KafkaConfigBackingStoreTest - KafkaOffsetBackingStoreTest  Those will require some refactoring and will be updated in a separate PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-29T08:57:15Z","2022-07-20T19:13:49Z"
"","10756","MINOR: Log constructor: Flip logical NOT for readability","I've done a small improvement in this PR by flipping logical NOT for readability in the `Log` constructor. Basically, the following code:  ``` if (A) {  if (B) {  } else {  } } else if (B) { } ```  is a bit more readable than:  ``` if (A) {  if (!B) {  } else {  } } else if (B) { } ```  **Tests:**  Relying on existing tests.","closed","","kowshik","2021-05-25T07:28:15Z","2021-05-26T20:22:33Z"
"","11406","POC: Drafting improvements for Interactive Query","I'm starting to sketch out some new APIs I'm planning to propose to improve the Interactive Query experience and performance.  This PR is really just a brain dump right now. I'll refine it significantly before moving to a KIP.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2021-10-15T21:11:27Z","2021-11-18T05:21:17Z"
"","10764","MINOR: make sure all fiedls of o.p.k.s.a.Action are NOT null","I'm migrating Ranger's kafka plugin from deprecated Authorizer (this is already removed by 976e78e405d57943b989ac487b7f49119b0f4af4)  to new API (see https://issues.apache.org/jira/browse/RANGER-3231). The kafka plugin needs to take something from field `resourcePattern` but it does not know whether the field is nullable (or users need to add null check). I check all usages and I don't observe any null case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-26T05:55:16Z","2021-05-28T05:41:52Z"
"","10607","MINOR: bump version to 2.6.3-SNAPSHOT in missing files","I was just editing something on the release process wiki and noticed that under the ""Cut Branch"" section, which it says to skip for bugfix releases, there's actually a list of files that it then says to update for both feature and bugfix releases. Luckily most of those files are actually covered later in the wiki, but two were not.  Also updated the release process wiki to clear this up for future RMs","closed","","ableegoldman","2021-04-28T21:51:22Z","2021-04-28T21:52:12Z"
"","10600","[WIP]MINOR: add abstract keywords to util classes","I think that Util classes are intended for using the static method without instantiating the class. So in order to be clear, I add abstract keywords to some Util classes which have no private constructor.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Mia-jeong","2021-04-27T13:57:56Z","2021-04-27T14:55:18Z"
"","11418","MINOR: Write log differently according to the size of missingListenerPartitions","I think it may seem a little awkward when I got a log like below,  ``` 1 partitions have leader brokers without a matching listener, including [...] ```  So I divided it to 2 types:  ``` // When more than one partition (* Same as the original) 2 partitions have leader brokers without a matching listener, including [...] ```  ``` // When only one partition 1 partition has a leader broker without a matching listener, which is ... ```      ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","HyunSangHan","2021-10-20T10:16:21Z","2022-02-02T10:06:57Z"
"","11139","MINOR: add helpful error msg","I noticed that replace thread actions would not be logged unless the user added a log in the handler. I think it would be very useful for debugging  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-07-28T18:35:51Z","2021-07-28T18:40:50Z"
"","11238","MINOR: Fix force kill of KRaft colocated controllers in system tests","I noticed that a system test using a KRaft cluster with 3 brokers but only 1 co-located controller did not force-kill the second and third broker after shutting down the first broker (the one with the controller).  The issue was a floating point rounding error.  This patch adjusts for the rounding error and also makes the logic work for an even number of controllers.  A local run of `tests/kafkatest/sanity_checks/test_bounce.py` succeeded (and I manually increased the cluster size for the 1 co-located controller case and observed the correct kill behavior: the second and third brokers were force-killed as expected).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-08-19T19:34:31Z","2022-06-15T14:45:00Z"
"","10947","KAFKA-13018: test_multi_version failures","I noticed test_mult_version was consistently failing. The issue was with the zookeeper option in the create topic call from a dev version node. I switched this so the 0 node was the old version, since the default in kafka.py is for node 0 to make the create topics call. I've confirmed the test now passes.   Going forward, we may want to consider how we are choosing to use the ZK option in the future. Right now it checks if all nodes support the bootstrap server, rather than the node making the call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-06-30T17:37:09Z","2021-06-30T21:19:17Z"
"","11454","KAFKA-13341: Quotas are not applied to requests with null clientId","I inspected this issue a little bit.  As shown in the first, proof-of-problem commit, the quota limit does not work correctly when the user is `""""` or `ANONYMOUS` and the client id is `""""` or `null`. We can fix it by treating `ANONYMOUS` as a default user and using client quota (i.e., `/config/clients/`) when the client id is `null`.  As far as I understood, this issue is just an edge case in the current implementation, not a regression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-10-30T10:59:13Z","2021-12-25T04:11:16Z"
"","11474","KAFKA-13354: Topic metrics count request rate inconsistently with other request metrics","I inspected this issue a little bit and found the following:  1. There are 18 metrics in `kafka.server:type=BrokerTopicMetrics` and 4 of them should be counted per request, but actually counted per `TopicPartition`:      - `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec`  2. 5 of them are omitted in the documentation (see: #11473)      - `kafka.server:type=BrokerTopicMetrics,name=TotalProduceRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=TotalFetchRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=FailedProduceRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=FailedFetchRequestsPerSec`     - `kafka.server:type=BrokerTopicMetrics,name=BytesRejectedPerSec`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-11-07T07:10:58Z","2021-12-25T04:10:02Z"
"","11431","KAFKA-13397: Honor 'replication.policy.separator' configuration when creating MirrorMaker2 internal topics","I found this issue while working on [KAFKA-13365](https://issues.apache.org/jira/browse/KAFKA-13365). Since [KAFKA-10777](https://issues.apache.org/jira/browse/KAFKA-10777) is scheduled to be shipped with 3.1.0, this issue would be better to be fixed.  cc/ @ryannedolan @mimaison @OmniaGM  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-10-24T12:34:38Z","2021-11-24T10:34:38Z"
"","11473","KAFKA-13436: Omitted BrokerTopicMetrics metrics in the documentation","I found this issue while working on [KAFKA-13354](https://issues.apache.org/jira/browse/KAFKA-13354).  As soon as this PR is approved, I will open a PR on the kafka-site also.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-11-07T07:05:25Z","2022-06-13T14:46:20Z"
"","10678","TRIVIAL: Fix type inconsistencies, unthrown exceptions, etc","I found these glitches while working on [KAFKA-12768](https://issues.apache.org/jira/browse/KAFKA-12768), etc.  Here are some explanations on the last commit:  As of present, `ConsoleConsumer` is taking timeout ms parameter as `Integer`. (see `ConsumerConfig#timeoutMsOpt`) For this reason, `ConsumerConfig#timeoutMs` is `Integer` and in turn, `timeoutMs` variable in `ConsoleConsumer#run` becomes `Any` - since it can either of `Integer` or `Long`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-05-12T09:01:38Z","2021-11-03T14:39:46Z"
"","10552","KAFKA-12675: improve the sticky general assignor scalability and performance","I did code refactor/optimization, keep the same algorithm in this PR. I've achieved: 1. Originally, With this setting: ``` topicCount = 50; partitionCount = 800; consumerCount = 800; ``` We complete in 10 seconds, after my code refactor, the time down to 100~200 ms  2. With the 1 million partitions setting: ``` topicCount = 500; partitionCount = 2000; consumerCount = 2000; ``` No OutOfMemory will be thrown anymore. The time will take 4~5 seconds.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-17T08:32:24Z","2021-06-02T22:01:19Z"
"","10648","KAFKA-9726: Add IdentityReplicationPolicy for MM2","I am taking over the PR of https://github.com/apache/kafka/pull/9395 which has been rebased to work with the latest `trunk` of Kafka. As stated in the original PR, it adds a `IdentityReplicationPolicy` for MirrorMaker2 (known as `LegacyReplicationPolicy`) which imitates the behavior of MirrorMaker1 (i.e. it doesn't rename the the topics apart from `heartbeat`). The behavior of this PR is exactly the same as the stated original.  Here are some additional notes/changes that are different from the original PR. * An additional constructor for the `startClusters()` has been added which allows you to supply test specific MirrorMaker configs. This allows the `IdentityReplicationPolicy` tests to use as much more code from the base `MirrorConnectorsIntegrationBaseTest`, heavily reducing boilerplate when compared to the original PR. Although It can be argued that you can just do `.put` directly on the `mm2Props` `HashMap`, this method is cleaner and more importantly better handles the case where you need to override an existing key (since `super` needs to be called last in `startClusters` it will override any existing keys which you may have configured in your own test) * Certain methods/fields of `MirrorConnectorsIntegrationBaseTest` have been changed from `private` to `protected` so that `IdentityReplicationPolicy` specific tests can access them.  @ryannedolan You reviewed the original PR and in that PR you stated that a KIP is needed since we are adding a public method to the `ReplicationPolicy` interface, is this still necessary? You also had issues with the `canTrackSource` method name, do you have a better alternative?  ## Committer Checklist (excluded from commit message) * [ ] Verify design and implementation * [X] Verify test coverage and CI build status * [ ] Verify documentation (including upgrade notes)  EDIT: Renamed `LegacyReplicationPolicy` to `IdentityReplicationPolicy`","closed","","mdedetrich","2021-05-07T09:05:56Z","2021-06-03T07:51:29Z"
"","11307","KAFKA-13262: Remove final from `MockConsumer.close()` and delegate implementation","I added the final via 2f3600198722 to catch overriding mistakes since the implementation was moved from the deprecated and overloaded `close` with two parameters to the no-arg `close`.  I didn't realize then that `MockConsumer` is a public API (seems like a bit of a mistake since we tweak the implementation and sometimes adds methods without a KIP).  Given that this is a public API, I have also moved the implementation of `close` to the one arg overload. This makes it easier for a subclass to have specific overriding behavior depending on the timeout.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-09-07T14:04:29Z","2021-09-07T19:06:18Z"
"","11422","KAFKA-9648: Add configuration to adjust listen backlog size for Acceptor","https://issues.apache.org/jira/browse/KAFKA-9648  * This PR implements [KIP-764](https://cwiki.apache.org/confluence/display/KAFKA/KIP-764%3A+Configurable+backlog+size+for+creating+Acceptor)     - Add new KafkaConfig `socket.listen.backlog.size` which is passed to [ServerSocket#bind](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/net/ServerSocket.html#bind(java.net.SocketAddress,int)) to adjust the max length of the queue of incoming connections. * Please refer the wiki page for the motivation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","ocadaruma","2021-10-21T15:36:44Z","2021-11-19T22:08:57Z"
"","10815","KAFKA-12885: Add the --timeout property to kafka-leader-election.sh","https://issues.apache.org/jira/browse/KAFKA-9220 mentions kafka-preferred-replica-election.sh script hard-coded timeout problems. I see a similar problem with kafka-leader-election.sh.  I would like to add a --timeout parameter to kafka-leader-election.sh to control the request timeout. To solve similar problems.","open","","socutes","2021-06-04T01:31:51Z","2021-06-06T14:40:28Z"
"","10534","KAFKA-806: Index may not always observe log.index.interval.bytes","https://issues.apache.org/jira/browse/KAFKA-806  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tang7526","2021-04-14T04:09:20Z","2021-05-27T09:58:09Z"
"","11368","KAFKA-13261: Add support for custom partitioners in foreign key joins","https://issues.apache.org/jira/browse/KAFKA-13261  Implements KIP-775 for adding Kafka Streams support for foreign key joins on tables with custom partitioners: https://cwiki.apache.org/confluence/display/KAFKA/KIP-775%3A+Custom+partitioners+in+foreign+key+joins  Co-authored-by: Tomas Forsman \  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vcrfxia","2021-09-29T07:14:00Z","2021-11-03T20:40:31Z"
"","11074","KAFKA-13101: Replace EasyMock and PowerMock with Mockito for RestServerTest","https://issues.apache.org/jira/browse/KAFKA-13101  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-17T19:12:07Z","2021-09-06T18:21:45Z"
"","11062","KAFKA-13094: Session windows do not consider user-specified grace when computing retention time for changelog","https://issues.apache.org/jira/browse/KAFKA-13094  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tang7526","2021-07-15T18:26:47Z","2021-07-23T03:43:32Z"
"","11051","KAFKA-13088: Replace EasyMock with Mockito for ForwardingDisabledProcessorContextTest","https://issues.apache.org/jira/browse/KAFKA-13088  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-14T16:17:29Z","2021-09-06T18:22:40Z"
"","11045","KAFKA-13082: Replace EasyMock with Mockito for ProcessorContextTest","https://issues.apache.org/jira/browse/KAFKA-13082  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","tang7526","2021-07-14T03:56:05Z","2021-07-20T18:28:26Z"
"","11034","KAFKA-13075: Consolidate RocksDBStoreTest and RocksDBKeyValueStoreTest","https://issues.apache.org/jira/browse/KAFKA-13075  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-13T05:59:38Z","2021-07-13T20:34:46Z"
"","11027","KAFKA-13066: Replace EasyMock with Mockito for FileStreamSinkConnectorTest","https://issues.apache.org/jira/browse/KAFKA-13066  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-12T13:30:32Z","2021-09-06T18:22:28Z"
"","11024","KAFKA-13065: Replace EasyMock with Mockito for BasicAuthSecurityRestExtensionTest","https://issues.apache.org/jira/browse/KAFKA-13065  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-12T11:33:26Z","2021-10-11T11:14:02Z"
"","10993","KAFKA-13037: ""Thread state is already PENDING_SHUTDOWN"" log spam","https://issues.apache.org/jira/browse/KAFKA-13037  Hey doesn't get much easier than this change.","closed","","jgray1206","2021-07-07T19:04:51Z","2021-07-14T12:13:17Z"
"","10795","KAFKA-12866: Avoid root access to Zookeeper","https://issues.apache.org/jira/browse/KAFKA-12866  The broker shouldn't assume create access to the chroot. There are deployement scenarios where the chroot is already created is the only znode which the broker can access.  To test this, we can use a ZK integration test, and configure zookeeper in the same way the issue is reproduced.  1. Create the chroot 2. Set free access to the chroot 3. Lock down access to the root znode 4. Try to connect the KafkaZkClient  It should be a separate `ZooKeeperTestHarness` to avoid leaving the ACL changes made to ZK root visible to other tests.   #### Rejected alternatives  * Expect `NoAuth` in `KafkaZkClient.createRecursive` and assume `NoAuth` as success. * Create new configuration to set `createChrootIfNecessary = false` instead of the current non configurable default value of true.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soarez","2021-05-30T14:50:44Z","2021-06-01T13:40:08Z"
"","10588","KAFKA-12662: add unit test for ProducerPerformance","https://issues.apache.org/jira/browse/KAFKA-12662  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-04-22T16:04:58Z","2021-06-17T12:07:32Z"
"","10644","[KAFKA-12635] auto sync consumer offset 0 when translated offset larger than partition end offset","https://issues.apache.org/jira/browse/KAFKA-12635  This issue has been verified and reproduced by some users. Here are the reproduce steps:  1. Create a topic with 1 partition on the source cluster, with `retention.ms` set to something short like 10 seconds. ```./kafka-topics.sh --bootstrap-server $SOURCE --create --topic myTopic --config 'retention.ms=10000'``` 2. Create a consumer that consumes from the topic ```./kafka-console-consumer.sh --bootstrap-server $SOURCE --group myConsumer --topic myTopic``` 3. Send 100 messages to the topic. These should get consumed by the consumer. Offset for this consumer on source cluster should be 100. ```for i in $(seq 1 100); do echo $i; done | ./kafka-console-producer.sh --bootstrap-server $SOURCE --topic myTopic``` 4. Wait until the retention policy deletes the records 5. Start MM2 with `source->target.sync.group.offsets.enabled = true` 6. Observe on the target cluster that log-end-offset is 0, offset is 100, and lag is -100. ```./kafka-consumer-groups.sh --bootstrap-server $TARGET --describe --group myConsumer```","closed","","ning2008wisc","2021-05-07T03:02:21Z","2022-05-16T15:46:00Z"
"","11153","MINOR: Port fix to other StoreQueryIntegrationTests","https://github.com/apache/kafka/pull/11129 fixed one of the tests, This ports the fix to the others as well  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-07-31T00:45:44Z","2021-08-05T21:53:13Z"
"","10988","KAFKA-9559: add docs for changing default serde to null","https://github.com/apache/kafka/pull/10813 changed the default serde from ByteArraySerde as discussed in [KIP-741](https://cwiki.apache.org/confluence/display/KAFKA/KIP-741%3A+Change+default+serde+to+be+null). This adds proper documentation so users know to set a serde through the configs or explicitly pass one in  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lct45","2021-07-07T13:03:04Z","2021-07-08T19:12:26Z"
"","10823","KAFKA-12897: KRaft multi-partition placement on single broker","https://github.com/apache/kafka/pull/10494 introduced a bug in the KRaft controller where the controller will loop forever in `StripedReplicaPlacer` trying to identify the racks on which to place partition replicas if there is a single unfenced broker in the cluster and the number of requested partitions in a CREATE_TOPICS request is greater than 1.  This patch refactors out some argument sanity checks and invokes those checks in both `RackList` and `StripedReplicaPlacer`, and it adds tests for this as well as the single broker placement issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-06-04T21:26:36Z","2021-06-07T21:13:07Z"
"","10998","MINOR: fix quoted boolean in FetchRequest.json","Hi, I'm working on maintaining an implementation of the protocol written in Rust: https://github.com/0x1991babe/kafka-protocol-rs. A recent change in 2b8aff58b5 breaks my deserialization logic for the `ignorable` field. I could work around this, but hope this minor change is acceptable. :)","closed","","tychedelia","2021-07-08T16:52:02Z","2021-07-09T19:33:13Z"
"","10821","KAFKA-12892: Use dedicated root in ZK ACL test","Having the `testChrootExistsAndRootIsLocked` test in a separate `ZookeeperTestHarness` isn't enough to prevent the ACL changes to the ZK root from affecting other integration tests. So instead, let's use a dedicated znode for this test. It still works because `makeSurePersistentPathExists` uses `createRecursive`, which will recurse and act the same for the root or a given znode.   Changes:  - Use a dedicated znode for the test - Move the test to `KafkaZkClientTest` - there is no longer a need to keep it in a separate `ZooKeeperTestHarness`  Related:  #10820  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soarez","2021-06-04T16:56:43Z","2021-06-05T12:14:48Z"
"","10980","KAFKA-12989: MockClient should respect the request matcher passed to prepareUnsupportedVersionResponse","Handle the case where `matches` returns `false` and throw the `InvalidStateException` as stated by the JavaDoc.  We need to guard against this unexpected runtime error in the `KafkaAdminClient`'s `sendEligibleCalls` method with a try/catch. Not 100% sure if that's kosher or not.  Included a targeted unit test for this case. The remaining tests in `KafkaAdminTestClient` continue to pass.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kirktrue","2021-07-06T18:39:31Z","2021-07-27T16:02:28Z"
"","10603","WIP: handle missing sink topics in a separate way","handle all missing sink topic separately  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-04-28T14:45:29Z","2021-04-28T20:49:50Z"
"","10968","MINOR: Upgrade Gradle to 7.1.1 and remove JDK 15 build","Gradle 7.1 improves Java incremental compilation: https://docs.gradle.org/7.1.1/release-notes.html  We previously kept the JDK 15 build because some tests didn't work with JDK 16. Since then, a number of PRs were submitted to fix this so it's best to remove the JDK 15 build before we create the 3.0 release branch.  Finally bump `test-retry` gradle plugin version too.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-03T21:56:19Z","2021-07-04T17:55:16Z"
"","10750","KAFKA-8120 Getting NegativeArraySizeException when using Kafka Connect to send data to Kafka","Getting NegativeArraySizeException when using Kafka Connect to send data to Kafka on Kafka version 2.5  PTAL @huxihx @kkonstantine . Many thanks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","DuongPTIT","2021-05-24T10:24:57Z","2021-05-25T10:56:31Z"
"","11249","Fix wrong link to ""Guarantees"" part in introduction.html documentation","From Kafka 2.5 release, the id of html tag `Guarantees` part has changed from `#intro_guarantees` to `#impl_guarantees` But, the links in introduction.html file are still linking to the `#intro_guarantees` So, the link is not working. (do nothing) -> Needed fix.  I fixed link, but previous versions of the documentation (~ 2.4 releases) are still not working. To fix the links of documentations of prior releases, we can change back the id from `#impl_guarantees` to `#intro_guarantees`. (not fix links, but change the id of the html tag) But I'm not sure Which way is better. please comment :)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","hojongs","2021-08-23T08:29:53Z","2021-10-11T14:45:36Z"
"","10611","KAFKA-12730; Avoid duplicate logout if Kerberos login fails to prevent NPE","From Java 9 onwards, `LoginContext#logout()` throws an NPE if invoked multiple times due to https://bugs.openjdk.java.net/browse/JDK-8173069. KerberosLogin currently attempts logout followed by login in a background refresh thread. If login fails we retry the same sequence. As a result, a single login failure prevents subsequent re-login. And clients will never be able to authenticate successfully after the first failure, until the process is restarted.  The PR checks if logout is necessary before invoking LoginContext#logout(). Also adds a test for this case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-04-29T09:23:42Z","2021-04-29T13:32:50Z"
"","10546","MINOR: remove redundant parentheses from ControllerApis","from https://github.com/apache/kafka/pull/10505#discussion_r614264455  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-04-16T06:15:54Z","2021-04-16T14:59:23Z"
"","11160","MINOR: update batch.size doc","Found we didn't mention anything about the `linger.ms` and upper the bound batch size concept in the `batch.size`, which will make users believe we'll send every batch with this `batch.size` setting value.  **before**: ![image](https://user-images.githubusercontent.com/43372967/127757761-b36de3b6-3f05-4d35-b9a6-be9f39d2a05f.png)   **after**: ![image](https://user-images.githubusercontent.com/43372967/127757750-ed894ff5-509b-4a60-9bc9-9591c41a9196.png)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-01T03:07:58Z","2021-08-01T23:59:28Z"
"","11218","MINOR: optimize performAssignment to skip unnecessary check","Found this while reading the code. We did a ""a little heavy"" check each time after performing assignment, which is to compare the ""assigned topics"" set and the ""subscribed topics"" set, to see if there's any topics not existed in another set. Also, the ""assigned topics"" set is created by traversing all the assigned partitions, which will be a little heavy if partition numbers are large.   However, as the comments described, it's a safe-guard for user-customized assignor, which might do assignment that we don't expected. In most cases, user will just use the in-product assignor, which guarantee that we only assign the topics from subscribed topics. Therefore, no need this check for in-product assignors.  In this PR, I added an ""in-product assignor names"" list, and we'll in `consumerCoordinator` check if the assignor is one of in-product assignors, to decide if we need to do the additional check. Also add test for it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-16T09:12:27Z","2021-09-29T00:54:10Z"
"","11122","MINOR: remove unneeded comments to avoid misleading message","Found this comments left while we updating the PR: https://github.com/apache/kafka/pull/11057. We don't do deduplicate in Fetcher anymore, this comment should be removed to avoid misleading other people.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-24T02:52:49Z","2021-07-24T03:23:08Z"
"","11100","MINOR: update doc to reflect the grace period change","Found the issue while reading stream doc. We removed default 24 hours grace period in KIP-633, and deprecate some grace methods, but we forgot to update the stream docs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-21T07:30:18Z","2021-07-21T11:28:03Z"
"","11029","MINOR: Handle some null cases in BrokerMetadataPublisher","Found a few NPEs when running system tests. Added coverage for a few null cases in BrokerMetadataPublisher as well as a unit test","closed","kip-500,","mumrah","2021-07-12T16:51:24Z","2021-07-20T18:38:30Z"
"","10843","MINOR: Log formatting for exceptions during configuration related operations","Format configuration logging during exceptions or errors. Also make sure it redacts sensitive information or unknown values.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","YiDing-Duke","2021-06-08T17:45:34Z","2021-06-15T06:11:19Z"
"","10671","MINOR: exclude all `src/generated` and `src/generated-test`","for https://github.com/apache/kafka/pull/10637#issuecomment-837691327  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-11T07:34:25Z","2021-05-12T07:27:50Z"
"","11416","KAFKA-13490: Fix createTopics and incrementalAlterConfigs for KRaft mode","For CreateTopics, fix a bug where if one createTopics in a batch failed, they would all fail with the same error code.  Make the error message for TOPIC_ALREADY_EXISTS consistent with the ZK-based code by including the topic name.  For IncrementalAlterConfigs, before we allow topic configurations to be set, we should check that they are valid. (This also applies to newly created topics.) IncrementalAlterConfigs should ignore non-null payloads for DELETE operations. Previously we would return an error in these cases. However, this is not compatible with the old ZK-based code, which ignores the payload in these cases.","closed","","cmccabe","2021-10-19T15:24:36Z","2021-12-06T00:33:22Z"
"","10747","KAFKA-12446: Define KGroupedTable#aggregate subtractor + adder order of execution","For context, this issue was initially raised in the following [thread](https://lists.apache.org/thread.html/rc3c1d07375e25341ab0467e4f8526a88f9fcc825a766d15922a9ed7d%40%3Cusers.kafka.apache.org%3E ) on the Kafka users mailing list.   # Problem During `KTable.groupBy`, we write into a repartition topic. Since the grouping key can change, we need to send separate events for the oldValue and the newValue to downstream nodes (where they will be “subtracted” and “added” respectively from/to the aggregate for the old key and the new key respectively).   However, sending the oldValue and the newValue as separate events is not strictly necessary when the grouping key does not change and doing so poses two challenges for users:  1. Firstly, the resulting KTable (i.e. the result of `KTable.groupBy(???).aggregate(???)`) can briefly be in an “inconsistent” state where the oldValue has been “subtracted” from the aggregate for the key but the newValue has not yet been “added” to the aggregate of the key because each event (oldValue, newValue) is processed separately.   2. Secondly, if users fail to correctly configure their producers correctly to avoid re-ordering during `send()`, it’s possible the newValue may be sent (and therefore processed by the aggregator) before the oldValue. If the user’s`adder` and `subtractor` functions are non-commutative, this would put the aggregate in a permanently “inconsistent” state.   Whilst there are ways to get around this issue by dropping down to the Processor API level, it would be nicer if this was handled by Kafka Streams more seamlessly.   # Proposed solution If the grouping key has not changed, the oldValue and newValue events are guaranteed to be processed by the same processor.  As such, we should be able to send them as a single `Change` event. The subtractor and adder functions can then be executed (in that order) and the KTable can be updated in a single “atomic” operation. In this way, we are able to remove any possibility of ending up in an “inconsistent” state. Also, note that sending the oldValue and newValue in the same event ensures that they can’t be re-ordered relative to each other irrespective of how a user has configured the producer settings. This PR is an implementation of this idea.   # Why is the linked ticket KAFKA-12446? I’ve chosen KAFKA-12446 as the ticket number because it’s highly related but to be clear, this PR is doing much more than what the ticket is actually proposing. I can create a separate ticket for this but wanted to first see if there is any appetite for these stronger guarantees I'm proposing.  If not, I’m happy to cut this PR down to just what is being asked for in the ticket (which is basically to just publicly document the existing behaviour).  Please feel free to let me know if I’m going about this the wrong way.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","fqaiser94","2021-05-24T02:49:03Z","2021-05-25T02:43:09Z"
"","11352","Add DirectoryConfigProvider to the service provider list","Following the discussion in confluentinc/kafka-images#102, it seems that the `DirectoryConfigProvider` should be also listed in the list of the service providers.  CC: @C0urante  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","scholzj","2021-09-22T14:17:54Z","2021-09-28T09:57:28Z"
"","11040","KAFKA-13078: Closing FileRawSnapshotWriter too early","FollowerState state should close the store fetchingSnapshot if it exists instead of the new snapshot begin set.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-13T18:01:12Z","2021-07-14T19:51:14Z"
"","10825","KAFKA-5876: Add `streamsState()` method to StateStoreProvider","follow-up #8200  KAFKA-5876's PR break into multiple parts, this PR is part 5.  In KIP-216, the following exceptions is currently not completed: StreamsRebalancingException, StreamsRebalancingException, StateStoreNotAvailableException. In the CompositeReadOnlyXXXXStore class, we need using streams state to decide which exception should be thrown.  This PR add a new method `streams()` to StateStoreProvider interface. In the next PR, we can get streams state in the CompositeReadOnlyXXXXStore class to determine which exception should be thrown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vitojeng","2021-06-05T11:03:46Z","2021-06-23T12:34:17Z"
"","10657","KAFKA-5876: Apply InvalidStateStorePartitionException for Interactive Queries","follow-up #8200  KAFKA-5876's PR break into multiple parts, this PR is part 4 - apply InvalidStateStorePartitionException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vitojeng","2021-05-09T12:48:32Z","2021-05-26T23:18:51Z"
"","10597","KAFKA-5876: Apply StreamsNotStartedException for Interactive Queries","follow-up #8200  KAFKA-5876's PR break into multiple parts, this PR is part 3 - apply StreamsNotStartedException  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vitojeng","2021-04-26T07:28:00Z","2021-05-04T01:27:10Z"
"","11055","HOTFIX: Init stream-stream left/outer join emit interval correctly","Follow up to KAFKA-10847 (https://github.com/apache/kafka/pull/10917).  The above fix intended to reduce the emit frequency to save the creation cost of RocksDB iterators. However, we incorrectly initialized the ""timer"" with timestamp zero, and thus, the timer was always in the past and we did try to emit left/outer join result too often and got a throughput of only 500 record/sec.  This PR fixes the initialization of the emit interval timer. After re-running the benchmark, we determined that a default emit interval of 1000ms provides the best performance of 12K rec/sec, so this PR also changes the emit frequency to 1000ms.","closed","streams,","mjsax","2021-07-14T23:57:39Z","2021-07-16T21:30:49Z"
"","10810","MINOR: Improve Kafka Streams JavaDocs with regard to record metadata","Follow up to #10731  I am not totally happy with this PR yet, and we might need to descope or split it up. While working on the PR, I figured that headers should actually never be `null` but I am not sure if we can easily change it. Especially for the public test helpers...","closed","streams,","mjsax","2021-06-03T06:03:56Z","2021-06-10T05:51:39Z"
"","10731","KAFKA-12815: Update JavaDocs of ValueTransformerWithKey","Follow up to #10720","closed","","mjsax","2021-05-19T22:10:02Z","2021-06-08T05:04:54Z"
"","10745","MINOR: add window verification to sliding-window co-group test","Follow up to #10703 \cc @ijuma","closed","tests,","mjsax","2021-05-22T22:53:41Z","2021-05-26T06:43:37Z"
"","11147","KAFKA-13108: improve ConfigCommandTest test coverage","Follow up PR for this: https://github.com/apache/kafka/pull/10811#issuecomment-883408956, to improve `ConfigCommandTest` test coverage.  Tests added: `shouldNotAllowAddEntityDefaultBrokerQuotaConfigWhileBrokerUpUsingZookeeper` `shouldNotAllowDescribeEntityDefaultBrokerWhileBrokerUpUsingZookeeper` `shouldSupportDescribeEntityDefaultBrokerBeforeBrokerUpUsingZookeeper`  The `EntityDefault` Broker config update test is already tested in `testDynamicBrokerConfigUpdateUsingZooKeeper`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-07-30T02:52:18Z","2021-09-06T23:23:37Z"
"","10628","MINOR: Mark case objects as final","follow scala best practices - Mark case objects as final (https://nrinaudo.github.io/scala-best-practices/adts/final_case_objects.html)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","chia7712","2021-05-03T06:17:41Z","2021-05-03T06:52:30Z"
"","11360","Floating point number comparison problem","Floating point number comparison problem, there may be comparison round-off errors. .","closed","","wenzelcheng","2021-09-26T16:04:40Z","2021-10-04T06:09:07Z"
"","10771","Update implementation.html","Fixing the link to a cited blog. The existing link now points to a steroid website so need to pull the blog from internet archives.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","prince-mahajan","2021-05-26T22:45:45Z","2021-05-27T00:30:35Z"
"","10766","MINOR: Fix indentation for several doc pages","Fixes the indentation of the code listings for: * api.html * configuration.html * design.html * implementation.html * toc.html  These changes consist of whitespaces added or removed for consistency. It also contains a couple of fixes on unbalanced html tags.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-26T12:31:42Z","2021-06-18T08:46:10Z"
"","10604","MINOR: system test spelling/pydoc/dead code fixes","Fixes some pydoc, corrects spelling, and removes dead code in system tests.  Renames `server_prop_overides` to `server_prop_overrides` to correct spelling.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-04-28T14:47:04Z","2021-05-01T15:22:46Z"
"","10768","MINOR: fix code listings for ops.html","Fixes indentation, uses the right character for bash (`>`), and fixes syntax highlighting for listings  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-05-26T13:09:47Z","2021-05-27T07:00:23Z"
"","11039","MINOR: Fix indentation for ProducerConfig.java","Fixes indentation for constant strings Makes more readable initializing CONFIG constant by changes indentations Changes Short.valueOf(acksStr) to Short.parseShort(acksStr);","closed","","ZuevKirill95","2021-07-13T17:15:16Z","2022-07-31T08:18:33Z"
"","11077","MINOR: Fix indentation for CommonClientConfigs.java","Fixes indentation for constant strings  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ZuevKirill95","2021-07-18T19:02:34Z","2022-07-31T08:18:33Z"
"","10719","Fix compile errors for KAFKA-12543","Fixes for two compiler errors which were introduced in https://github.com/apache/kafka/pull/10431","closed","","mumrah","2021-05-18T20:51:27Z","2021-05-18T21:09:52Z"
"","11050","KAFKA-13052: Replace SerDe with Serde","Fixes [KAFKA-13052](https://issues.apache.org/jira/browse/KAFKA-13052).","closed","","JimGalasyn","2021-07-14T14:22:03Z","2021-08-02T00:00:58Z"
"","10873","KAFKA-7360 Fixed code snippet","Fixed code snippet in documentation to make it syntactically correct.  Ref. https://kafka.apache.org/20/documentation/streams/developer-guide/processor-api.html#accessing-processor-context  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vijaykriishna","2021-06-13T06:49:30Z","2021-11-25T13:31:33Z"
"","11203","MINOR: Fixed ""snasphot"" naming issue","Fixed ""snasphot"" naming issue. Instead of ""snapshot"".","closed","","socutes","2021-08-12T03:01:19Z","2021-11-22T06:33:46Z"
"","10751","MINOR: update java doc for ConsumerCoordinator","fix typo","closed","","sasukerui","2021-05-24T15:10:49Z","2021-05-25T01:16:50Z"
"","10948","MINOR: Some broker code cleanups","Fix the JavaDoc for the ClientQuotaManagerConfig#throttle function to refer to the correct parameter name.  BrokerEndPointTest#testHashAndEquals should test the BrokerEndPoint class, rather than the MetadataBroker class.  TopicConfigHandler: make the kafkaController argument optional, since we won't have it when in KRaft mode.  Remove the unecessary ConfigRepository argument for the Partition class.  Remove the unused TestUtils#deleteBrokersInZk function.","closed","kip-500,","cmccabe","2021-06-30T18:15:08Z","2021-06-30T21:58:58Z"
"","10486","KAFKA-12492: Fix the formatting of example RocksDBConfigSetter","Fix the formatting of example RocksDBConfigSetter due to the un-arranged spaces within `` tag.","closed","","cc13ny","2021-04-06T06:27:49Z","2021-04-09T03:55:53Z"
"","11359","MINOR: fix CustomRequestLog ms formatting","Fix the format string supplied to CustomRequestLogger. It was previously missing the brackets required to delineate the unit of time being recorded. (See [https://www.eclipse.org/jetty/javadoc/jetty-9/org/eclipse/jetty/server/CustomRequestLog.html](https://www.eclipse.org/jetty/javadoc/jetty-9/org/eclipse/jetty/server/CustomRequestLog.html))","closed","","kzzhang","2021-09-24T21:00:31Z","2021-09-30T20:06:09Z"
"","11178","KAFKA-13168: KRaft observers should not have a replica id","Fix the `KafkaRaftClient` to use Optional.empty as its localId so that the sentinel `node.id`, `node.id=-1` is sent as the replicaId in the `FetchRequest`.  https://issues.apache.org/jira/browse/KAFKA-13168","closed","","dielhennr","2021-08-05T01:38:16Z","2022-06-01T10:12:38Z"
"","11286","add units to metrics descriptions + test fix","Fix some review feedback and test flakiness from #11149  - adds units to metrics descriptions for total blocked time metrics - fix producer metrics test checks","closed","","rodesai","2021-08-31T06:13:49Z","2021-08-31T18:44:46Z"
"","10505","MINOR: fix some bugs in ControllerApis.scala","Fix some cases where ControllerApis was blocking on the controller thread.  This should not be necessary, since the controller thread can just interface directly with the network threads.  alterClientQuotas and incrementalAlterConfigs were not doing authorization correctly in ControllerApis.scala.  Since the previous release of KRaft did not support authorizers, this bug is not as severe as it could have been, but it still needs to be fixed.  This PR also adds unit tests to verify that all of the controller operations return authorization failures when appropriate.  Fix how duplicate configuration resources are handled.  Add support for the ALTER_CONFIGS API, and stub functions for cluster reassignment, as specified in KIP-631.  Additionally, this PR fixes a comment in ControllerApis#deleteTopics that no longer reflects what the code is doing when we don't have ""describe"" permission.","closed","kip-500,","cmccabe","2021-04-08T19:35:13Z","2021-04-15T18:55:22Z"
"","10601","KAFKA-12723: Fix potential NPE in HashTier","Fix potential NPE ,  [kafka-12723](https://issues.apache.org/jira/browse/KAFKA-12723)","closed","","loyispa","2021-04-28T10:56:29Z","2021-12-08T02:25:53Z"
"","11445","createTopic need sort brokerMetadata","fix KAFKA-13226  *More detailed description of your change, if not sort the brokerMetaData  Partition expansion may cause uneven distribution  *Summary of testing strategy (including rationale) more detail please see  https://issues.apache.org/jira/browse/KAFKA-13226 or https://shirenchuang.blog.csdn.net/article/details/119903289  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","shirenchuang","2021-10-28T12:44:33Z","2021-10-28T12:44:33Z"
"","11453","createTopic need sort brokerMetadata","fix KAFKA-13226  *More detailed description of your change, if not sort the brokerMetaData Partition expansion may cause uneven distribution  *Summary of testing strategy (including rationale) more detail please see https://issues.apache.org/jira/browse/KAFKA-13226 or https://shirenchuang.blog.csdn.net/article/details/119903289    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","shirenchuang","2021-10-30T06:43:07Z","2021-11-03T18:20:26Z"
"","10770","MINOR: fix code listings security.html","Fix examples under security.html so they use the right bash icon (`>` instead of `$`) and also uses the right tool for showing code listings  Some of the diffs are caused also by the IDE aligning html code properly.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-26T13:16:06Z","2021-06-01T12:00:14Z"
"","10890","MINOR: fix broken JavaDoc links","Fix broken JavaDoc markup to resolve build warnings.","closed","","mjsax","2021-06-16T18:32:18Z","2021-06-17T17:30:47Z"
"","11407","MINOR: fix typo in README.md","Fix a typo   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","fishmandev","2021-10-17T05:59:40Z","2021-10-18T08:19:06Z"
"","11032","KAFKA-13073: Inconsistent MockLog implementation","Fix a simulation test failure by:  1. Relaxing the valiation of the snapshot id against the log start offset when the state machine attempts to create new snapshot. It is safe to just ignore the request instead of throwing an exception when the snapshot id is less that the log start offset.  2. Fixing the MockLog implementation so that it uses startOffset both externally and internally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-13T03:40:49Z","2021-07-14T19:51:14Z"
"","11358","MINOR: fix ClusterControlManager log message","Fix a ClusterControlManager log message that should distinguish between newly registered and re-registered brokers, but was not due to a bug.","closed","kip-500,","cmccabe","2021-09-24T16:02:27Z","2021-09-24T18:16:09Z"
"","11361","KAFKA-13324: KRaft: fix validateOnly in CreateTopics","Fix a bug where the validateOnly flag for createTopics was being ignored.","closed","kip-500,","cmccabe","2021-09-27T00:40:41Z","2021-09-27T23:00:44Z"
"","11305","Make classpath smaller","Fix `The input line is too long. The syntax of the command is incorrect`  error when running on a windows environment.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","apenam","2021-09-07T09:31:27Z","2021-09-07T09:31:47Z"
"","10946","KAFKA-12997: Expose the append time for batches from raft","Expose the append time for both batches exposed by raft and for the snapshot reader:  1. The type `Batch` is extended to include the append timestamp. 2. The type `SnapshotReader` is extended to expose the last contained log append time. This is the append time of the last batch from the log that was included in the snapshot. The full implementation will follow as it required extending `Batch` and `RecordsIterator` to support deserializing control messages.  3. Fix the `QuorumController` to remember the last committed append time and to store it in the generated snapshot.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-06-30T16:54:42Z","2021-07-05T14:56:50Z"
"","11290","KAFKA-13245: KIP-771 implementation","Expose 0 on standby KRaft controllers as per [KIP-771](https://cwiki.apache.org/confluence/display/KAFKA/KIP+771%3A+KRaft+brokers+should+not+expose+controller+metrics).  https://issues.apache.org/jira/browse/KAFKA-13245","closed","","dielhennr","2021-09-01T19:40:46Z","2021-09-21T00:03:16Z"
"","11140","MINOR: log epoch and offset truncation similarly to HWM truncation","Example log message: ``` Truncating partition topic1-0 to leader epoch and offset EpochEndOffset(errorCode=0, partition=0, leaderEpoch=3, endOffset=4) ```  This will mean we log similarly to the hwm truncation here https://github.com/apache/kafka/pull/11140/files#diff-508e9dc4d52744119dda36d69ce63a1901abfd3080ca72fc4554250b7e9f5242R250, though obviously with the additional epoch end offset information which is helpful in diagnosis.","closed","","lbradstreet","2021-07-28T18:56:43Z","2021-07-30T20:26:42Z"
"","11049","MINOR: Fix small warning on javadoc and scaladoc","Escape the `>` character in javadoc Escape the `$` character when part of `${}` in scaladoc as this is the way to reference a variable  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jlprat","2021-07-14T10:20:50Z","2021-07-14T10:38:37Z"
"","11121","MINOR: Enable KRaft in transactions_test.py","Enabling KRaft in this test.","closed","","dielhennr","2021-07-23T22:10:30Z","2021-07-23T23:01:55Z"
"","11042","KAFKA-12886: Enable request forwarding to the controller by default for zookeeper mutation protocols","Enabled request forwarding by default and addressed test failures.  The following tests are still flaky: * ConsumerBounceTest > testCloseDuringRebalance * See [this JIRA](https://issues.apache.org/jira/browse/KAFKA-8529?jql=summary%20~%20%22ConsumerBounceTest*%22%20OR%20description%20~%20%22ConsumerBounceTest*%22%20ORDER%20BY%20lastViewed%20DESC)  https://issues.apache.org/jira/browse/KAFKA-12886  See a rewrite of this PR here https://github.com/apache/kafka/pull/11281","open","","dielhennr","2021-07-13T23:42:47Z","2021-08-29T00:40:53Z"
"","10652","KAFKA-9726 IdentityReplicationPolicy","Enable active/passive, one-way replication without renaming topics, similar to MM1. This implementation is described in KIP-382 (adopted), originally as ""LegacyReplicationPolicy"".  This enables operators to migrate from MM1 to MM2 without re-architecting their replication flows, and enables some additional use-cases for MM2. For example, operators may wish to ""upgrade"" their Kafka clusters by mirroring everything to a completely new cluster. Such a migration would have been difficult with either MM1 or MM2 previously.  When using IdentityReplicationPolicy, operators should be aware that MM2 will not be able to detect cycles among replicated topics. A misconfigured topology may result in replicating the same records back-and-forth or in an infinite loop. However, we don't prevent this behavior, as some use-cases involve filtering records (via SMTs) to prevent cycles.  This PR includes major contributions from @mdedetrich and @ivanyu, so please include them in the commit log!","closed","","ryannedolan","2021-05-07T19:17:07Z","2021-08-18T03:08:00Z"
"","11280","MINOR:When rebalance times out, print the member's clientHost in the …","During the operation and maintenance of Kafka cluster, the rebalance timeout is a very serious case, hope to add a clientHost information to help quickly locate which member has a problem.","open","","keepal7","2021-08-28T16:14:41Z","2021-09-03T11:37:38Z"
"","11385","KAFKA-13385: In the KRPC request header, translate null clientID to empty","Due to a historical quirk, the clientId field is nullable in the request header schema. However, the code does not handle null clientIds. They should be treated as the empty string, as they were previously.","closed","","cmccabe","2021-10-08T14:46:14Z","2021-10-27T22:19:13Z"
"","11162","MINOR: Update KRaft README.md and upgrade.html for 3.0","Doc changes for the KRaft 3.0 Preview release  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-08-02T14:17:23Z","2021-08-02T17:39:33Z"
"","10559","MINOR: diable RaftClusterTest first","Disable the flaky RaftClusterTest tests before the root cause KAFKA-12677 got fixed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-04-19T03:38:07Z","2021-07-13T12:05:02Z"
"","10786","KAFKA-12787: Integrate controller snapshoting with raft client","Directly use `RaftClient.Listener`, `SnapshotWriter` and `SnapshotReader` in the quorum controller.  1. Allow `RaftClient` users to create snapshots by specifying the last committed offset and last committed epoch. These values are validated against the log and leader epoch cache. 2. Remove duplicate classes in the metadata module for writing and reading snapshots. 3. Changed the logic for comparing snapshots. The old logic was assuming a certain batch grouping. This didn't match the implementation of the snapshot writer. The snapshot writer is free to merge batches before writing them. 4. Improve `LocalLogManager` to keep track of multiple snapshots. 5. Improve the documentation and API for the snapshot classes to highlight the distinction between the offset of batches in the snapshot vs the offset of batches in the log. These two offsets are independent of one another. `SnapshotWriter` and `SnapshotReader` expose a method called `lastOffsetFromLog` which represents the last inclusive offset from the log that is represented in the snapshot. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-05-28T19:30:46Z","2021-06-15T17:44:37Z"
"","11137","KAFKA-13133 Replace EasyMock and PowerMock with Mockito for AbstractHerderTest","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to use.[KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","wycccccc","2021-07-28T10:24:24Z","2022-08-02T15:11:37Z"
"","10881","KAFKA-12947 Replace EasyMock and PowerMock with Mockito for Streams…","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to use.[KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wycccccc","2021-06-14T17:41:31Z","2022-07-28T04:20:53Z"
"","10835","KAFKA-12905: Replace EasyMock and PowerMock with Mockito for NamedCacheMetricsTest","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to [use.KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438?jql=project%20%3D%20KAFKA%20AND%20status%20in%20(Open%2C%20Reopened)%20AND%20assignee%20in%20(EMPTY)%20AND%20text%20~%20%22mockito%22)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wycccccc","2021-06-07T16:10:48Z","2021-06-10T10:18:20Z"
"","11167","Kafka-13158 Replace EasyMock and PowerMock with Mockito for ConnectClusterStateImpl Test and ConnectorPluginsResourceTest","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to [use.KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wycccccc","2021-08-03T18:51:41Z","2022-02-09T15:57:57Z"
"","10976","KAFKA-13036 Replace EasyMock and PowerMock with Mockito for RocksDBMetricsRecorderTest","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to [use.KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wycccccc","2021-07-06T08:18:25Z","2022-03-25T01:57:21Z"
"","10850","KAFKA-12924 Replace EasyMock and PowerMock with Mockito in streams (metrics)","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to [use.KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-7438)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wycccccc","2021-06-09T09:42:14Z","2021-06-11T16:56:32Z"
"","11017","KAFKA-12950 Replace EasyMock and PowerMock with Mockito for KafkaStream","Development of EasyMock and PowerMock has stagnated while Mockito continues to be actively developed. With the new Java cadence, it's a problem to depend on libraries that do bytecode generation and are not actively maintained. In addition, Mockito is also easier to [use.KAFKA-7438](https://issues.apache.org/jira/browse/KAFKA-12950)  `build.gradle `will update after other related issues are merged, avoid conflicts.Minor changes to the source code.I think this is inevitable.If there is a better solution to avoid changes, suggestions are welcome.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wycccccc","2021-07-10T16:55:30Z","2022-07-29T17:10:08Z"
"","10619","MINOR: Update test libraries and gradle plugins for better JDK 16/17 support","Details: * spotbugs gradle plugin from 4.6.0 to 4.7.1:   https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.1   https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.6.2   https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.7.0   https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/4.7.1 * spotless gradle plugin from 5.10.2 to 5.12.4:   https://github.com/diffplug/spotless/blob/gradle/5.12.4/CHANGES.md * test-retry gradle plugin from 1.2.0 to 1.2.1:   https://github.com/gradle/test-retry-gradle-plugin/releases/tag/v1.2.1 * dependency check gradle plugin from 6.1.1 to 6.1.6:   https://github.com/jeremylong/DependencyCheck/releases/tag/v6.1.2   https://github.com/jeremylong/DependencyCheck/releases/tag/v6.1.3   https://github.com/jeremylong/DependencyCheck/releases/tag/v6.1.4   https://github.com/jeremylong/DependencyCheck/releases/tag/v6.1.5   https://github.com/jeremylong/DependencyCheck/releases/tag/v6.1.6 * versions gradle plugin from 0.36.0 to 0.38.0: https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.37.0 https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.38.0 * easymock from 4.2 to 4.3:   https://github.com/easymock/easymock/releases/tag/easymock-4.3 * mockito from 3.6.0 to 3.9.0: https://github.com/mockito/mockito/releases (too many releases to list   them all individually) * spotbugs from 4.1.4 to 4.2.2:   https://github.com/spotbugs/spotbugs/blob/4.2.2/CHANGELOG.md   4.2.3 has a regression that causes spurious errors related to `Random`   usage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-30T13:57:56Z","2021-05-02T19:15:56Z"
"","10654","MINOR: Update jacoco to 0.8.7 for JDK 16 support","Details: * https://github.com/jacoco/jacoco/releases/tag/v0.8.6 * https://github.com/jacoco/jacoco/releases/tag/v0.8.7  Ran `./gradlew clients:reportCoverage -PenableTestCoverage=true -Dorg.gradle.parallel=false` successfully with Java 15 (see https://github.com/gradle/gradle/issues/15730 and https://github.com/scoverage/gradle-scoverage/issues/150 for the reason why  `-Dorg.gradle.parallel=false` is required).  Also updated `README.md` to include `-Dorg.gradle.parallel=false` alongside `reportCoverage`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-08T18:27:13Z","2021-05-12T15:13:06Z"
"","10656","MINOR: checkstyle version upgrade: 8.20 -->> 8.36.2","details:   * checkstyle: 8.20  -->> 8.36.2  https://checkstyle.org/releasenotes.html#Release_8.36.2   *  ~~spotbugs  : 4.2.2 -->> 4.2.3 https://github.com/spotbugs/spotbugs/blob/4.2.3/CHANGELOG.md~~  Rationale:  * Checktyle 8.20 is two years old; also, both Gradle 6.8 and 7.0 are using recent Checkstyle version (8.37) * Checkstyle 8.36.2 is geared towards recent Java versions (see 8.36 release notes: https://checkstyle.org/releasenotes.html#Release_8.36) * Checkstyle version 8.42 should be skipped (lots of false positives, see here: https://github.com/checkstyle/checkstyle/issues/9957) * more recent Checkstyle versions (i.e. 8.37 and above) are imposing more strict indentation rules and hence we can opt to:   * relax Checkstyle indentation rules **_OR_**   *  comply with these new rules and change affected classes (note: total of 50 violations in 18 classes is recorded when compiled with Checkstyle 8.41.1)    @ijuma Please review this.  Also, let me know what do you prefer: to merge this commit or wait for a full upgrade to a latest checkstyle version ? And in case that you opt for the latter: which strategy do you prefer (i.e. are we going to relax checkstyle rules or comply with them) ?","closed","","dejan2609","2021-05-09T11:00:35Z","2021-06-16T21:53:27Z"
"","10820","KAFKA-12892: disable testChrootExistsAndRootIsLocked","Detail is described in https://issues.apache.org/jira/browse/KAFKA-12892. Thanks.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-04T13:23:12Z","2021-06-12T01:46:48Z"
"","11348","MINOR: fix CreateTopic to return the same as DescribeTopic","DescribeTopic returns  https://github.com/ccding/kafka/blob/79788ca042330faea7dc736f1f6ceb75b3f4d1d9/core/src/main/scala/kafka/server/ConfigHelper.scala#L45 ``` config.originals.asScala.filter(_._2 != null) ++ config.nonInternalValues.asScala ``` so we should do the same for CreateTopic.  I am not able to come up with a unit test for this because we don't have a config that is defineInternal in LogConfig and would be used.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-09-21T03:09:16Z","2021-09-21T23:31:23Z"
"","10573","KAFKA-12574: KIP-732, Deprecate eos-alpha and replace eos-beta with eos-v2","Deprecates and logs a warning upon usage of the following: - StreamsConfig.EXACTLY_ONCE - StreamsConfig.EXACTLY_ONCE_BETA - Producer#sendOffsetsToTransaction(Map offsets, String consumerGroupId)  The deprecated eos configs are to be replaced by the new StreamsConfig.EXACTLY_ONCE_V2 config. Additionally, this PR replaces usages of the term ""eos-beta"" throughout the code with the term ""eos-v2""","closed","","ableegoldman","2021-04-21T02:12:45Z","2021-05-11T23:22:02Z"
"","10944","MINOR: Loose verification of startup in EOS system tests","Currently, we verify that a Streams client transitioned from REBALANCING to RUNNING and that it processes some records in the EOS system test. However, if the Streams client only has standby tasks assigned, the client will never process records. Hence, the test will fail although everything is fine.  This commit removes the verification that checks whether records are processed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-06-30T11:23:58Z","2021-07-21T08:56:11Z"
"","11144","MINOR: call the serialize method including headers from the MockProducer","Currently when using serializers like the Cloud Event Serializer, we need to do a work around so it doesn't throw an error. Using the method taking the headers would prevent this. Since the default implementation just calls the method without the headers, it's expected to be fully backwards compatible.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","gklijs","2021-07-29T12:05:00Z","2022-07-12T07:53:02Z"
"","11231","KAFKA-13214; Consumer should not reset state after retriable error in rebalance","Currently the consumer will reset state after any retriable error during a rebalance. This includes coordinator disconnects as well as coordinator changes. The impact of this is that rebalances get delayed since they will be blocked until the session timeout of the old memberId expires.   The patch here fixes the problem by not resetting the member state after a retriable error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-08-18T18:41:56Z","2021-08-24T18:59:39Z"
"","11132","KAFKA-13139: Empty response after requesting to restart a connector without the tasks results in NPE","Currently tested via the system test `connect_distributed_test.py::ConnectDistributedTest.test_restart_failed_connector`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2021-07-26T20:03:18Z","2021-07-27T20:47:20Z"
"","10565","KAFKA-12691: Add case where task can be considered idling","Currently task is reporting the time it started idling as when the task is suspended where it should also take into account enforced non processing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","wcarlson5","2021-04-19T23:16:33Z","2021-04-22T06:39:15Z"
"","11241","KAFKA-13032: add NPE checker for KeyValueMapper","Currently in both `KStreamMap` and `KStreamFlatMap` classes, they will throw NPE if the call to `KeyValueMapper#apply` return Null. We should check whether the result of that call is Null and throw a more meaningful error message for better debugging.  Two unit tests are also added to check if we successfully captured the Null.  @mjsax Please help review, thanks!  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jasonyanwenl","2021-08-20T05:57:05Z","2021-09-06T12:03:16Z"
"","11181","MINOR: optimize the OrderedBytes#upperRange for not all query cases","Currently in `OrderedBytes#upperRange` method, we'll check key bytes 1 by 1, to see if there's a byte value >= first timestamp byte value, so that we can skip the following key bytes, because we know `compareTo` will always return 0 or 1.   Take this test for example (in `WindowKeySchemaTest`) ```java public void testUpperBoundWithKeyBytesLargerAndSmallerThanFirstTimestampByte() {         // here, because the 3rd byte of the key (0x9) < the first timestamp byte (0xA), we'll skip this type, and appending the timestamp directly         final Bytes upper = windowKeySchema.upperRange(Bytes.wrap(new byte[] {0xC, 0xC, 0x9}), 0x0AffffffffffffffL);          // so the tested shorter key should be included in the upper range         assertThat(             ""shorter key with timestamp should be in range"",             upper.compareTo(                 WindowKeySchema.toStoreKeyBinary(                     new byte[] {0xC, 0xC},                     0x0AffffffffffffffL,                     Integer.MAX_VALUE                 )             ) >= 0         );          assertThat(upper, equalTo(WindowKeySchema.toStoreKeyBinary(new byte[] {0xC, 0xC}, 0x0AffffffffffffffL, Integer.MAX_VALUE)));     } ```  Furthermore, since we don't know how many bytes are skipped, we did a byteBuffer copy to another byte array in the end.  However, in not all query cases, the first timestamp byte is alwyas **0**, because when we use the current timestamp (i.e. `System.currentTimeMillis()`), the first timestamp byte is `0`.  This PR optimizes the not all query cases  by not checking the key byte 1 by 1 (because we know the unsigned integer will always be >= 0), instead, put all bytes and timestamp directly. So, we won't have byte array copy in the end either.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-05T10:20:27Z","2021-08-26T21:37:34Z"
"","11297","KAFKA-10038: Supports default client.id for ConsoleConsumer, ProducerPerformance, ConsumerPerformance","Currently in `ConsoleConsumer`/`ProducerPerformance`/`ConsolePerformance`, we will not set `client.id` if it is not provided by the user. Simiarly to what we already did for `ConsoleProducer`([code](https://github.com/apache/kafka/blob/99b9b3e84f4e98c3f07714e1de6a139a004cbc5b/core/src/main/scala/kafka/tools/ConsoleProducer.scala#L96)), this PR will set a default value of `client.id` so that we can better support quota testing. The default value is shown as follows: * `client.id` in `ConsoleConsumer`: `console-consumer` * `client.id` in `ConsumerPerformance`: `perf-consumer-client` * `client.id` in `ProducerPerformance`: `perf-producer-client`  New unit testings are added to test both default `client.id` and `client.id` provided by users  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jasonyanwenl","2021-09-05T04:11:02Z","2021-09-07T20:49:50Z"
"","10666","MINOR: prevent cleanup() from being called while Streams is still shutting down","Currently `KafkaStreams#cleanUp` only throw an IllegalStateException if the state is RUNNING or REBALANCING, however the application could be in the process of shutting down in which case StreamThreads may still be running. We should also throw if the state is PENDING_ERROR or PENDING_SHUTDOWN","closed","streams,","ableegoldman","2021-05-11T05:24:49Z","2021-05-13T23:16:35Z"
"","10949","KAFKA-13019: Add MetadataImage and MetadataDelta classes for KRaft Snapshots","Create the image/ module for storing, reading, and writing broker metadata images. Metadat images are immutable. New images are produced from existing images using delta classes. Delta classes are mutable, and represent changes to a base image.  MetadataImage objects can be converted to lists of KRaft metadata records. This is essentially writing a KRaft snapshot. The resulting snapshot can be read back into a MetadataDelta object. In practice, we will typically read the snapshot, and then read a few more records to get fully up to date. After that, the MetadataDelta can be converted to a MetadataImage as usual.  Sometimes, we have to load a snapshot even though we already have an existing non-empty MetadataImage. We would do this if the broker fell too far behind and needed to receive a snapshot to catch up. This is handled just like the normal snapshot loading process. Anything that is not in the snapshot will be marked as deleted in the MetadataDelta once finishSnapshot() is called.  In addition to being used for reading and writing snapshots, MetadataImage also serves as a cache for broker information in memory. A follow-up PR will replace MetadataCache, CachedConfigRepository, and the client quotas cache with the corresponding Image classes.  TopicsDelta also replaces the ""deferred partition"" state that the RaftReplicaManager currently implements. (That change is also in a follow-up PR.)","closed","kip-500,","cmccabe","2021-06-30T18:30:08Z","2021-07-01T07:09:28Z"
"","10626","KAFKA-12744: Breaking change dependency upgrade: ""argparse4j"" 0.7.0 -->> 0.9.0","Corresponding JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-12744   Migration guide: https://argparse4j.github.io/migration.html#to-0-8-0  Release notes:  * https://github.com/argparse4j/argparse4j/releases/tag/argparse4j-0.8.0  * https://github.com/argparse4j/argparse4j/releases/tag/argparse4j-0.8.1  * https://github.com/argparse4j/argparse4j/releases/tag/argparse4j-0.9.0  Related to this PR: #10094 and this commit: https://github.com/apache/kafka/commit/690f72dd69d31589655c84d3cc1a6eec006bcab5  @cmccabe, @hachikuji, @mumrah and @soarez: it would be nice if you could spare some time and review this (especially these two classes):  ``` shell/src/main/java/org/apache/kafka/shell/Commands.java shell/src/main/java/org/apache/kafka/shell/ManCommandHandler.java ```  FYI also @ijuma","open","","dejan2609","2021-05-02T13:08:43Z","2021-05-11T17:49:07Z"
"","10561","KAFKA-12686 AlterIsr and LeaderAndIsr race condition","Copied from the JIRA:  > In Partition.scala, there is a race condition between the handling of an AlterIsrResponse and a LeaderAndIsrRequest. This is a pretty rare scenario and would involve the AlterIsrResponse being delayed for some time, but it is possible. This was observed in a test environment when lots of ISR and leadership changes were happening due to broker restarts. >  > When the leader handles the LeaderAndIsr, it calls Partition#makeLeader which overrides the isrState variable and clears the pending ISR items via AlterIsrManager#clearPending(TopicPartition). >  > The bug is that AlterIsrManager does not check its inflight state before clearing pending items. The way AlterIsrManager is designed, it retains inflight items in the pending items collection until the response is processed (to allow for retries). The result is that an inflight item is inadvertently removed from this collection. >  > Since the inflight item is cleared from the collection, AlterIsrManager allows for new AlterIsrItem-s to be enqueued for this partition even though it has an inflight AlterIsrItem. By allowing an update to be enqueued, Partition will transition its isrState to one of the inflight states (PendingIsrExpand, PendingIsrShrink, etc). Once the inflight partition's response is handled, it will fail to update the isrState due to detecting changes since the request was sent (which is by design). However, after the response callback is run, AlterIsrManager will clear the partitions that it saw in the response from the unsent items collection. This includes the newly added (and unsent) update. >  > The result is that Partition has a ""inflight"" isrState but AlterIsrManager does not have an unsent item for this partition. This prevents any further ISR updates on the partition until the next leader election (when isrState is reset). >  > If this bug is encountered, the workaround is to force a leader election which will reset the partition's state.   This PR removes the clearPending call from AlterIsrManager. As seen with this bug, this method is not safe to call any time there is an AlterIsrRequest in-flight. We could add more protections around this call, but it is simpler (and safer) to just remove it. Clearing unsent ISR updates is not really necessary after a leader election since the updates will fail due to a stale leader epoch.","closed","","mumrah","2021-04-19T15:03:23Z","2021-05-18T13:56:37Z"
"","10812","KAFKA-12863: Configure controller snapshot generation","Controller snapshot generation will be triggered based on the number of new bytes in the log since the latest snapshot.  1. Add the property `metadata.log.snapshot.min.new_record.bytes` and disabled it by default by setting the default to `Int.MaxValue` 2. Expose the number of bytes in a batch to the quorum controller 3. Change the commit handling implementation for the quorum controller so that it generates a new snapshot after a configured number of bytes have been read. 4. Fix `LocalLogManager` so that snapshot loading in only triggered when the listener is not the leader.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-06-03T17:25:55Z","2021-07-01T20:58:12Z"
"","11131","KAFKA-13137: KRaft Controller Metric MBean names incorrectly quoted","Controller metric names that are in common between the ZooKeeper-based and KRaft-based controller must remain the same, but they were not in the AK 2.8 early access release of KRaft. For example, the non-KRaft MBean name `kafka.controller:type=KafkaController,name=OfflinePartitionsCount` incorrectly became `""kafka.controller"":type=""KafkaController"",name=""OfflinePartitionCount""` (note the added quotes and the lack of plural).  This patch fixes the issues, closes the test gap that allowed the divergence to occur, and adds deregistration logic to remove the metrics when the controller is closed (this logic was missing).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-07-26T19:14:13Z","2021-07-29T20:01:20Z"
"","11177","KAFKA-13167; KRaft broker should send heartbeat immediately after starting controlled shutdown","Controlled shutdown in KRaft is signaled through a heartbeat request with the `shouldShutDown` flag set to true. When we begin controlled shutdown, we should immediately schedule the next heartbeat instead of waiting for the next periodic heartbeat. This allows the broker to shutdown more quickly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","hachikuji","2021-08-04T20:39:01Z","2021-08-11T13:13:16Z"
"","10507","KAFKA-8410: Migrating stateful operators to new Processor API","Continuation of https://github.com/apache/kafka/pull/10381. Migration of Kafka Streams stateful operators (KTable, KStream aggregations, joins).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-04-08T22:54:58Z","2021-10-19T14:20:48Z"
"","10643","KAFKA-12747: Fix flakiness in shouldReturnUUIDsWithStringPrefix","Consecutive UUID generation could result in same prefix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-05-06T22:28:24Z","2021-05-10T19:33:40Z"
"","10651","MINOR: Kafka Streams code samples formating unification","Code samples are now correctly formatted. Samples under Streams use consistently the prism library to be displayed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-07T16:36:13Z","2021-05-21T15:33:17Z"
"","11287","KAFKA-13256 Fix possible NPE when ConfigDef renders RST or HTML and ConfigKey.documentation is unset/NULL","closes https://issues.apache.org/jira/browse/KAFKA-13256  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","rk3rn3r","2021-08-31T12:47:28Z","2021-09-16T13:36:00Z"
"","11107","KAFKA-13125: close KeyValueIterator instances in internals tests (part 2)","close KeyValueIterator instances in internals tests (part 2)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2021-07-22T07:48:00Z","2021-07-26T23:26:23Z"
"","11105","KAFKA-13123: close KeyValueIterator instances in example code and tests","Close KeyValueIterator instances in example code and tests  I split the originally huge PR into 3 sub PRs. Here's the other 2: https://github.com/apache/kafka/pull/11106, https://github.com/apache/kafka/pull/11107  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2021-07-22T07:45:33Z","2021-07-26T23:23:23Z"
"","11106","KAFKA-13124: close KeyValueIterator instance in internals tests (part 1)","Close KeyValueIterator instance in internals tests (part 1)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2021-07-22T07:46:38Z","2021-07-26T23:26:44Z"
"","11143","MINOR: close TopologyTestDriver to release resources","Close `TopologyTestDriver` to release resources  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","showuon","2021-07-29T08:00:50Z","2021-07-30T03:08:26Z"
"","11437","MINOR: fix the path metadata shell uses for client quotas","Client quotas should appear under /client-quotas rather than /configs, since client quotas are not configs. Additionally we should correctly handle the case where the entity name is null (aka ""default"" quotas.)","closed","kip-500,","cmccabe","2021-10-26T20:55:29Z","2021-10-26T22:27:36Z"
"","11449","MINOR: Log client disconnect events at INFO level","Client disconnects are crucial events for debugging. The fact that we only log them at DEBUG/TRACE means we rarely have them available outside of a testing context. This patch therefore increases verbosity to INFO level. In practice, we already have backoff configurations which should prevent these logs from getting too spammy.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-10-28T21:49:28Z","2021-11-10T19:54:39Z"
"","11267","KAFKA-13234; Transaction system test should clear URPs after broker restarts","Clearing under-replicated-partitions helps ensure that partitions do not become unavailable longer than necessary as brokers are rolled. This prevents flakiness due to transaction timeouts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-08-26T18:03:28Z","2021-09-01T15:37:05Z"
"","11411","MINOR: Clarify acceptable recovery lag config doc","Clarify a misleading config description.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-10-18T19:04:11Z","2022-03-08T16:43:43Z"
"","10885","KAFKA-12701: NPE in MetadataRequest when using topic IDs","cherry-pick of c16711cb8e0d1c03f123e3e9d7e3d810796bf315 Updated KafkaApisTest to use 2.8 code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-06-15T16:55:52Z","2021-06-15T21:09:28Z"
"","11065","KAFKA-13092: Perf regression in LISR requests","cherry-pick of 584213ed20d679b11206b67c5a65035347632f07  Only conflicts were in Log.scala where the new flush code was near some log refactor changes in trunk. I kept the code the same and only added the flush call.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-15T21:27:53Z","2021-07-15T23:51:07Z"
"","10906","KAFKA-12835: Topic IDs can mismatch on brokers (after interbroker protocol version update)","cherry-pick of 195a8b0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-06-18T21:33:25Z","2021-08-12T08:12:30Z"
"","11073","KAFKA-13092: Perf regression in LISR requests","Cherry pick of 584213ed20d679b11206b67c5a65035347632f07  Main difference is that we do not try to write to partition metadata file when initializing the Log. Since we only do so in checkOrSet topic ID and there is a small bug in that path, the benchmark attempts to run that method last -- so that the log is created and we can actually record the ID to write.   Some quick runs of the benchmarks -- Master ``` Benchmark                                   (numPartitions)  (useTopicIds)  Mode  Cnt     Score   Error  Units PartitionCreationBench.makeFollower                    2000          false  avgt    2  2576.278          ms/op PartitionCreationBench.makeFollower                    2000           true  avgt    2  5378.075          ms/op ```  PR ``` Benchmark                                   (numPartitions)  (useTopicIds)  Mode  Cnt     Score   Error  Units PartitionCreationBench.makeFollower                    2000          false  avgt    2  2678.498          ms/op PartitionCreationBench.makeFollower                    2000           true  avgt    2  2962.268          ms/op ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-17T16:11:11Z","2021-08-31T07:26:02Z"
"","10811","KAFKA-12598: ConfigCommand should only support communication via ZooKeeper for a reduced set of cases","Checked the documentation, we must use `--zookeeper` option in 3 places (alter and describe): 1. user configs where the config is a SCRAM mechanism name (i.e. a SCRAM credential for a user) 2. update broker configs for a particular broker when that broker is down 3. broker default configs when all brokers are down  REF: 1. [config SCRAM Credentials](https://kafka.apache.org/documentation/#security_sasl_scram_credentials) 2. [Update config before broker started](https://kafka.apache.org/documentation/#dynamicbrokerconfigs)  So, after this PR, we only support `--zookeeper` on `users` and `brokers` entity. Add some argument parse rules and tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","showuon","2021-06-03T12:14:55Z","2021-07-22T23:47:13Z"
"","11054","KAFKA-13090: Improve kraft snapshot integration test","Check and verify generated snapshots for the controllers and the brokers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-14T22:17:17Z","2021-07-16T21:20:26Z"
"","11412","KAFKA-10543: Convert KTable joins to new PAPI","Changes:  - Migrate KTable joins to new Processor API. - Migrate missing KTableProcessorSupplier implementations. - Replace KTableProcessorSupplier with new Processor API implementation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-10-18T23:03:39Z","2021-11-09T10:56:40Z"
"","10544","KAFKA-12648: minimum changes for error handling namedTopologies","changed the obvious ones to attribute to a named topology. This might be all we need, we can always add more if they come up.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-04-15T17:53:09Z","2022-02-18T15:19:52Z"
"","11417","KAFKA-13340: Change ZooKeeperTestHarness to QuorumTestHarness","Change ZooKeeperTestHarness to QuorumTestHarness so that integration tests which inherit from this class can test both ZK and KRaft mode. Test cases which do this can specify the modes they support by including a ParameterizedTest annotation before each test case, like the following:  @ParameterizedTest @valuesource(strings = Array(""zk"", ""kraft"")) def testValidCreateTopicsRequests(quorum: String): Unit = { ... }  For each value that is specified here (zk, kraft), the test case will be run once in the appropriate mode. So the test shown above is run twice. This allows integration tests to be incrementally converted over to support KRaft mode, rather than rewritten to support it. As you might expect, test cases which do not specify a quorum argument will continue to run only in ZK mode.  JUnit5 makes the quorum annotation visible in the TestInfo object which each @beforeeach function in a test can optionally take. Therefore, this PR converts over the setUp function of the quorum base class, plus every derived class, to take a TestInfo argument. The TestInfo object gets ""passed up the chain"" to the base class, where it determines which quorum type we create (zk or kraft).  The general approach taken here is to make as much as possible work with KRaft, but to leave some things as ZK-only when appropriate. For example, a test that explicitly requests an AdminZkClient object will get an exception if it is running in KRaft mode. Similarly, tests which explicitly request KafkaServer rather than KafkaBroker will get an exception when running in KRaft mode.  As a proof of concept, this PR converts over MetricsTest to support KRaft.","closed","kip-500,","cmccabe","2021-10-20T00:03:16Z","2021-10-30T15:00:39Z"
"","11383","KAFKA-13340: Change ZooKeeperTestHarness to QuorumTestHarness","Change ZooKeeperTestHarness to QuorumTestHarness so that integration tests which inherit from this class can test both ZK and KRaft mode. Test cases which do this can specify the modes they support by including a ParameterizedTest annotation before each test case, like the following:  >  @ParameterizedTest >  @ValueSource(strings = Array(""zk"", ""kraft"")) >  def testValidCreateTopicsRequests(quorum: String): Unit = { ... }  For each value that is specified here (zk, kraft), the test case will be run once in the appropriate mode. So the test shown above is run twice. This allows integration tests to be incrementally converted over to support KRaft mode, rather than rewritten to support it. As you might expect, test cases which do not specify a quorum argument will continue to run only in ZK mode.  JUnit5 makes the quorum annotation visible in the TestInfo object which each @BeforeEach function in a test can optionally take. Therefore, this PR converts over the setUp function of the quorum base class, plus every derived class, to take a TestInfo argument. The TestInfo object gets ""passed up the chain"" to the base class, where it determines which quorum type we create (zk or kraft).  The general approach taken here is to make as much as possible work with KRaft, but to leave some things as ZK-only when appropriate. For example, a test that explicitly requests an AdminZkClient object will get an exception if it is running in KRaft mode. Similarly, tests which explicitly request KafkaServer rather than KafkaBroker will get an exception when running in KRaft mode.  As a proof of concept, this PR converts over CreateTopicsRequestTest to support KRaft. This is a good proof of concept because it is a very ""deeply nested"" test class (it has 5 base classes).  Some notable changes in each file:  KafkaServerTestHarness: add a ""brokers"" method, which replaces ""servers"" in KRaft-compatible tests. Create KRaft brokers when configured to do so.  BaseRequestTest: add KRaft implementations for functions to fetch various socket server objects.  TestUtils: add KRaft-compatible versions of many utility functions.  ZooKeeperTestHarness: rename to QuorumTestHarness. Change the setUp function to take a TestInfo object and configure the test appropriately (for KRaft or ZK) based on this.  BrokerServer: lifecycleManager should be final and initialized in the constructor, so that the object state is always available.  KafkaBroker: add more accessor methods to this base class for the ZK and the KRaft broker.  IntegrationTestHarness: set the controller endpoint when in KRaft mode.  Create TestInfoUtils to house utility functions for reading and writing TestInfo objects.  ReplicationControlManager.java: harmonize TOPIC_ALREADY_EXISTS error text with the zk case.","closed","kip-500,","cmccabe","2021-10-05T21:20:50Z","2021-11-02T18:23:09Z"
"","11138","KAFKA-12932: Interfaces for SnapshotReader and SnapshotWriter","Change the snapshot API so that SnapshotWriter and SnapshotReader are interfaces. Change the existing types SnapshotWriter and SnapshotReader to use a different name and to implement the interfaces introduced by this issue.","closed","","socutes","2021-07-28T12:25:59Z","2021-11-22T06:29:52Z"
"","10543","KAFKA-12648: tag uses of StreamsException to add Named to","change each StreamsException so that it has the sub topology name in it  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-04-15T17:35:40Z","2022-02-18T15:19:41Z"
"","10564","MINOR: clean up some replication code","Centralize leader and ISR changes in generateLeaderAndIsrUpdates. Consolidate handleNodeDeactivated and handleNodeActivated into this function.  Rename BrokersToIsrs#noLeaderIterator to BrokersToIsrs#partitionsWithNoLeader. Create BrokersToIsrs#partitionsLedByBroker, BrokersToIsrs#partitionsWithBrokerInIsr  In ReplicationControlManagerTest, createTestTopic should be a member function of ReplicationControlTestContext.  It should invoke ReplicationControlTestContext#replay so that records are applied to all parts of the test context.","closed","kip-500,","cmccabe","2021-04-19T21:49:44Z","2021-04-29T18:20:31Z"
"","11172","MINOR: update stream-stream join docs","Call for review @JimGalasyn @spena @ableegoldman (cf https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+Semantics)  Must be cherry-picked to `3.0` branch.","closed","docs,","mjsax","2021-08-03T23:41:52Z","2021-08-05T18:34:37Z"
"","11380","KAFKA-13345: Use ""delete"" cleanup policy for windowed stores if duplicates are enabled","Call for review @guozhangwang @cadonna","closed","streams,","mjsax","2021-10-05T00:13:47Z","2021-10-15T05:06:34Z"
"","10495","MINOR: add missing TimeoutException to Producer.send JavaDocs","Call for review @guozhangwang @abbccdda @hachikuji","closed","producer,","mjsax","2021-04-06T22:51:31Z","2021-04-09T18:36:46Z"
"","10532","KAFKA-8531: Change default replication factor config","Call for review @cadonna @ableegoldman","closed","kip,","mjsax","2021-04-13T05:27:30Z","2021-05-05T23:16:11Z"
"","11455","KAFKA-13423: GlobalThread should not log ERROR on clean shutdown","Call for review @cadonna","closed","streams,","mjsax","2021-10-30T18:45:38Z","2022-02-06T19:56:27Z"
"","10846","KAFKA-12914: StreamSourceNode should return `null` topic name for pattern subscription","Call for review @cadonna","closed","","mjsax","2021-06-08T19:02:06Z","2021-06-15T08:15:25Z"
"","11075","MINOR: Move off deprecated APIs in StreamsResetter","Call for review @bbejeck","closed","tools,","mjsax","2021-07-17T22:51:44Z","2021-07-26T21:15:36Z"
"","11342","MINOR: Bump 2.7/2.8 to latest versions","Bump 2.7/2.8 versions to latest where necessary in 2.8 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-20T07:03:05Z","2021-09-20T07:49:47Z"
"","10625","MINOR: Fix error log for bounce broker","bounce leader broker for topic partition was mistakenly log as a follower broker  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-05-02T07:20:50Z","2021-05-02T07:23:40Z"
"","10538","MINOR: shutdown KafkaScheduler at appropriate time","Both the ZooKeeper-based and KRaft brokers invoke `KafkaScheduler.shutdown()` too early -- before `LogManager.shutdown()` is invoked.  So it is possible for `LogManager` to try to use the scheduler after the scheduler has been shutdown, which results in an exception.  This patch moves the shutdown of the scheduler to a point after the shutdown of `LogManager`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","rondagostino","2021-04-14T20:43:15Z","2021-04-15T17:13:41Z"
"","11170","KAFKA-13102: Topic IDs not propagated to metadata cache quickly enough for Fetch path","Before we used the metadata cache to determine whether or not to use topic IDs. Unfortunately, metadata cache updates with ZK controllers are in a separate request and may be too slow for the fetcher thread. This results in switching between topic names and topic IDs for topics that could just use IDs.   This change adds topic IDs to FetcherState created in LeaderAndIsr requests. It also supports updating this state for follower threads as soon as a LeaderAndIsr request provides a topic ID.   I've opted to only update replica fetcher threads. Alter Log Dir threads will use either topic name or topic ID depending on what was present when they were created.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-08-03T21:38:30Z","2021-09-24T08:51:08Z"
"","10987","MINOR: improve the partitioner.class doc","Before the PR, the `partitioner.class` config doesn't describe what the config is use for, and what options user can choose. ![image](https://user-images.githubusercontent.com/43372967/124742380-3afcf980-df4f-11eb-99fa-a384013cfe86.png)  **After:**      ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-07T10:18:01Z","2021-07-19T05:55:18Z"
"","10615","KAFKA-12648: basic skeleton API for NamedTopology","Basically just the API of https://github.com/apache/kafka/pull/10609, should not contain any logical changes in Streams at this point (for example I also stripped out the protocol change for now, even though it's more or less done)","closed","","ableegoldman","2021-04-30T01:36:46Z","2021-05-01T05:46:00Z"
"","11301","KAFKA-13276: Prefer KafkaFuture in admin Result constructors","Avoid using the non-public API KafkaFutureImpl in the Admin client's `*Result` class constructors. This is particularly problematic for `DescribeConsumerGroupsResult` which currently has a public constructor.  For the other classes the rationale is simply consistency with the majority of the `*Result` classes.","closed","","tombentley","2021-09-06T16:22:34Z","2021-09-09T08:34:48Z"
"","11311","KAFKA-13280: Avoid O(N) behavior in KRaftMetadataCache#topicNamesToIds","Avoid O(N) behavior in KRaftMetadataCache#topicNamesToIds and KRaftMetadataCache#topicIdsToNames by returning a map subclass that exposes the TopicsImage data structures without copying them.","closed","kip-500,","cmccabe","2021-09-08T20:36:21Z","2021-10-07T16:42:00Z"
"","10596","KAFKA-12715: ACL authentication, Host field support IP network segment","At present, ACL authentication, the Host field only supports equal matching of source IP, so we hope that the Host field can support matching of IP network segment.  ### Committer Checklist (excluded from commit message) - [ ] Modify the Host matching logic of the AclAuthorizer's function matchingAclExists - [ ] Add IP segment validation test cases","closed","","socutes","2021-04-26T00:31:05Z","2021-05-26T16:05:03Z"
"","10800","MINOR: Update jmh for async profiler 2.0 support","Async profiler 2.0 outputs html5 flame graph files and supports simultaneous collection of cpu, allocation and lock profiles in jfr format.  Updated the readme to include an example of the latter and verified that the Readme commands work with async profiler 2.0.  Release notes: * 1.28: https://mail.openjdk.java.net/pipermail/jmh-dev/2021-March/003171.html * 1.29: https://mail.openjdk.java.net/pipermail/jmh-dev/2021-March/003218.html * 1.30: https://mail.openjdk.java.net/pipermail/jmh-dev/2021-May/003237.html * 1.31: https://mail.openjdk.java.net/pipermail/jmh-dev/2021-May/003286.html * 1.32: https://mail.openjdk.java.net/pipermail/jmh-dev/2021-May/003307.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-31T18:41:16Z","2021-06-02T12:55:05Z"
"","10508","KAFKA-12633: Remove deprecated APIs in TopologyTestDriver","As well as related test classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-04-09T00:58:01Z","2021-04-18T17:46:07Z"
"","10809","MINOR: Style fixes to KafkaRaftClient","As title suggested, made some style fixes to the class `KafkaRaftClient`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2021-06-03T04:30:54Z","2021-06-04T19:23:45Z"
"","10806","MINOR: update kafka-topics.sh line command tool upgrade notes with removed option","As the title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-06-02T10:17:27Z","2021-06-02T12:57:04Z"
"","10595","MINOR: Add some metrics names","As the title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","wenbingshen","2021-04-25T18:00:28Z","2021-04-25T18:00:28Z"
"","10551","MINOR: Fix nonsense test line from TopicCommandTest","As the title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-17T03:14:31Z","2021-04-20T08:30:30Z"
"","10653","MINOR: Add missing parameter description from AdminZkClient","As the title.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-05-08T06:08:22Z","2021-06-10T16:42:03Z"
"","10765","KAFKA-12519: Remove built-in Streams metrics for versions 0.10.0-2.4","As specified in KIP-743, this PR removes the built-in metrics in Streams that are superseded by the refactoring proposed in KIP-444.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-05-26T09:41:58Z","2021-06-01T16:31:12Z"
"","11379","KAFKA-12994: Migrated SlidingWindowsTest to new API","As raised in KAFKA-12994, All tests that use the old API should be either eliminated or migrated to the new API in order to remove the @SuppressWarnings(""deprecation"") annotations.  This PR will migrate the SlidingWindowsTest to the new API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","officialpatterson","2021-10-04T15:20:16Z","2021-10-21T23:56:49Z"
"","11215","KAFKA-12994: Migrate TimeWindowsTest to new API","As raised in [KAFKA-12994](https://issues.apache.org/jira/browse/KAFKA-12994), All tests that use the old API should be either eliminated or migrated to the new API in order to remove the `@SuppressWarnings(""deprecation"")` annotations.  As a starting point, this PR will migrate over all the relevant tests in `TimeWindowsTests.java`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","officialpatterson","2021-08-14T16:49:07Z","2021-10-04T14:33:10Z"
"","11356","KAFKA-10539: Convert KStreamImpl joins to new PAPI","As part of the migration to new Processor API, this PR converts KStream to KStream joins.  Depends #11315   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jeqo","2021-09-23T17:35:40Z","2021-10-18T21:20:45Z"
"","11099","KAFKA-10542: Migrate KTable mapValues, passthrough, and source to new Processor API","As part of the migration of KStream/KTable operations to the new Processor API https://issues.apache.org/jira/browse/KAFKA-8410, this PR includes the migration of KTable:  - mapValues,  - passthrough, - and source operations.  Testing strategy: operations should keep the same tests as new processor should be compatible.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-07-21T00:12:19Z","2021-10-19T14:20:47Z"
"","11213","KAFKA-13201: Convert KTable suppress to new PAPI","As part of the migration of KStream/KTable operations to the new Processor API https://issues.apache.org/jira/browse/KAFKA-8410, this PR includes the migration of KTable suppress.  Testing strategy: operations should keep the same tests as new processor should be compatible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-08-13T19:03:57Z","2021-09-07T22:19:56Z"
"","11316","KAFKA-10544: Migrate KTable aggregate and reduce","As part of the migration of KStream/KTable operations to the new Processor API https://issues.apache.org/jira/browse/KAFKA-8410, this PR includes the migration of KTable aggregate/reduce operations.  Testing strategy: operations should keep the same tests as new processor should be compatible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-09-09T16:44:09Z","2021-09-22T19:28:13Z"
"","11315","KAFKA-10540: Migrate KStream aggregate operations","As part of the migration of KStream/KTable operations to the new Processor API https://issues.apache.org/jira/browse/KAFKA-8410, this PR includes the migration of KStream aggregate/reduce operations.  Testing strategy: operations should keep the same tests as new processor should be compatible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-09-09T16:33:51Z","2021-10-03T20:06:47Z"
"","11117","MINOR: Remove older brokers from upgrade test","As of version 2.2.1 , Kafka Streams uses message headers and thus requires broker version 0.11.0 or newer.","closed","tests,","mjsax","2021-07-23T16:41:38Z","2021-07-26T21:11:14Z"
"","10624","KAFKA-12605 - make GZIP decompression use BufferSupplier","as laid out in https://issues.apache.org/jira/browse/KAFKA-12605 kafka consumers decoding gzip'ed payloads currently do not re-use memory buffers because the JDK classes used have no support for it.  this PR adds buffer reuse support to gzip decoding.  unfortunately, since the JDK classes involved are not properly extensible I've had to make copies of them. modification to these copies are kept minimal:  1. buffers now come from, and are returned to, suppliers 2. some use of Unsafe has been replaced with more portable code 3. minor changes required to comply with kafka's checkstyle 4. KafkaBufferedInputStream does not fully support the mark() operation as that may involve buffer re-allocation. I have not found any usage of mark() in kafka code though.  so far only decompression is supported. compression may be added later.  a (randomized) test has been added that I have run on my machine for several hours with no issues.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","radai-rosenblatt","2021-05-02T00:24:15Z","2021-05-03T15:59:32Z"
"","10627","MINOR: dozen of version upgrades (for both dependencies and gradle plugins)","As in title; I can change git commit message and provide full list of version upgrades and/or create corresponding JIRA ticket.  Note: line `scalatest: ""org.scalatest:scalatest_$versions.baseScala:$versions.scalatest""` is removed out of `gradle/dependencies.gradle` as obsoleted (related PR: #9858)  FYI @ijuma","closed","","dejan2609","2021-05-02T18:20:10Z","2021-05-07T14:34:21Z"
"","11430","KAFKA-13352: Kafka Client does not support passwords starting with number in jaas config","As I left in the comments, the `StreamTokenizer` used by `org.apache.kafka.common.security.JaasConfig` recognizes alphabetical characters, dash, underscore, and dollar sign as a 'word' only. (see [here](https://github.com/AdoptOpenJDK/openjdk-jdk9u/blob/master/jdk/src/java.base/share/classes/java/io/StreamTokenizer.java#L188)) Because of that, a string that contains a number or a symbol like '^' (which are so common in the password) raises an error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-10-24T08:40:10Z","2022-03-17T01:27:50Z"
"","10836","KAFKA-12906 - Added RecordDeserializationException containing partition and offset","As documented in KIP-334 (https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=87297793), we should add a new RecordDeserializationException, which is raised by the consumer when failing to parse a record. This allows the consumer to decide to take an action such as to shut down or skip past the record.   JIRA: [12906](https://issues.apache.org/jira/browse/KAFKA-12906)  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sarwarbhuiyan","2021-06-07T16:16:35Z","2021-06-17T21:49:22Z"
"","11214","KAFKA-12994 Migrate JoinWindowsTest and SessionWindowsTest to new API","As detailed in [KAFKA-12994](https://issues.apache.org/jira/browse/KAFKA-12994), unit tests using the old API should be either removed or migrated to the new API.  This PR migrates relevant tests in `JoinWindowsTest.java` and `SessionWindowsTest.java`.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","clolov","2021-08-14T06:36:03Z","2021-09-12T18:14:24Z"
"","11126","KAFKA-13132: Upgrading to topic IDs in LISR requests has gaps introduced in 3.0","As described in the ticket, in 3.0 there was a change that resulted in no longer assigning topic IDs to the log and the partition.metadata file in certain upgrade scenarios, specifically when upgrading from IBP 2.7 or below to 3.0. This PR adds a check for whether we need to handle the LISR request given a new topic ID when one is not yet assigned in the log and code to assign a topic ID when the log is already associated to a partition in ReplicaManager.  The idea is that we may need to handle one extra LISR request per partition when we are upgrading to using topic IDs from an IBP less than 2.8 to 3.0+. After an upgrade we should not see this issue. We will also not see this issue when upgrading from < 2.8 to 2.8 or from 2.8 to 3.0.   I've added tests for the upgrade scenario that would have replicated the gap before this fix. (The test fails on master)  I've also revised some replicamanager tests that will now fail with inconsistent topic IDs unless we use the correct topic ID.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-25T04:08:43Z","2021-07-29T21:36:45Z"
"","10735","KAFKA-12779: KIP-740, Clean up public API in TaskId and fix TaskMetadata#taskId()","As described in [KIP-740](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=181306557), we clean up the public TaskId class and introduce new APIs to return it from TaskMetadata  The KIP is currently under voting so this PR can't be merged yet, but it is ready to be reviewed","closed","","ableegoldman","2021-05-20T02:54:01Z","2021-05-20T22:01:24Z"
"","10490","MINOR: Gradle build optimizations","Applies various build optimizations to take full advantage of Gradle's incremental build feature, and Gradle's local build caching feature. The optimizations will help to reduce build times for developers when running builds locally.  Specific optimizations:    - **Enable the local build cache**      The local build cache avoids re-running tasks whose inputs have not changed, even if the build output has been cleaned or if the build is executed in a different directory.    - **Instruct Gradle to ignore version files when doing up-to-date checks**      This can save a lot of developer time when working locally and creating a lot of commits. In particular, tests won't be re-executed simply because the commit ID changes (unless one of the other test inputs changes).    - **Update createVersionFile tasks support incremental builds**      The version files are no longer recreated unless one of their inputs changes (the commit ID or the version property).    - **Set the root project name in settings.gradle**      The root project name is used to name the build as a whole. It is used in reports and other types of artifacts. If a root project name is not set, then the containing directory's name is used as the root project name.       It is a best-practice to explicitly set the root project name. Otherwise, the root project name could change if the directory the project belongs to is different from the desired name (for example, of someone checks out the repository into a directory named differently from the default).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jthurne","2021-04-06T19:10:37Z","2021-04-12T14:12:03Z"
"","10977","MINOR: Reuse hasDefault instead of comparing with NO_DEFAULT_VALUE directly","Always use the `hasDefault` method to check if the default value is specified in `ConfigKey` object  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","wuYin","2021-07-06T09:59:18Z","2021-07-08T06:14:44Z"
"","11405","KAFKA-12648: Wrap all exceptions thrown to handler as StreamsException & add TaskId field","Alternative to https://github.com/apache/kafka/pull/11381  To help users distinguish which task an exception was thrown from, and which NamedTopology if it exists, we add a TaskId field to the StreamsException class. We then make sure that all exceptions thrown to the handler are wrapped as StreamsExceptions, to help the user simplify their handling code as they know they will always need to unwrap the thrown exception exactly once.","closed","","ableegoldman","2021-10-15T20:06:06Z","2021-10-21T23:21:27Z"
"","11119","MINOR: Add ""controller"" listener to AlterConfigsRequest","AlterConfigsRequest is handled in ControllerApis but does not have the ""controller"" listener. This PR adds the listener to the RPC.","closed","","dielhennr","2021-07-23T19:26:49Z","2021-07-23T20:26:55Z"
"","11288","KAFKA-13258/13259/13260: Fix error response generation","AlterClientQuotas, DescribeProducers and FindCoordinator have issues when building error responses. This can lead to brokers returning responses without errors even when some have happened.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-09-01T08:00:01Z","2021-09-08T09:16:26Z"
"","11300","KAFKA-13258/13259/13260: Fix error response generation","AlterClientQuotas, DescribeProducers and FindCoordinator have issues when building error responses. This can lead to brokers returning responses without errors even when some have happened.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-09-06T13:12:30Z","2021-09-08T09:12:59Z"
"","11299","KAFKA-13258/KAFKA-13259: Fix error response generation","AlterClientQuotas and  DescribeProducers have issues when building error responses. This can lead to brokers returning responses without errors even when some have happened.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-09-06T10:05:09Z","2021-09-08T09:10:38Z"
"","11145","KAFKA-13151: Disallow policy configs in KRaft","alter.config.policy.class.name and create.topic.policy.class.name are unsupported by KRaft. KRaft servers should fail startup if any of these are configured.  Tested this manually by enabling/disabling the configs statically in a KRaft cluster.  https://issues.apache.org/jira/browse/KAFKA-13151","closed","","dielhennr","2021-07-29T22:40:22Z","2021-07-30T19:46:25Z"
"","11036","KAFKA-12944: Assume message format version is 3.0 when inter-broker protocol is 3.0 or higher (KIP-724)","Also: * Deprecate `log.message.format.version` and `message.format.version`. * Log broker warning if the deprecated config values are ignored due to the inter-broker protocol version. * Log warning if `message.format.version` is set via `ConfigCommand`. * Always down-convert if fetch version is v3 or lower. * Add tests to verify new message format version based on the inter-broker protocol version. * Adjust existing tests that create topics with an older message format to have the inter-broker protocol set to 2.8. * Add upgrade note.  Note that the log compaction change to always write new segments with record format v2 if the IBP is 3.0 or higher will be done as part of KAFKA-13093 (with Kafka 3.1 as the target release version).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-13T14:52:12Z","2021-07-19T14:21:27Z"
"","11078","KAFKA-12944: Assume message format version is 3.0 when inter-broker protocol is 3.0 or higher (KIP-724)","Also: * Deprecate `log.message.format.version` and `message.format.version`. * Log broker warning if the deprecated config values are ignored due to the inter-broker protocol version. * Log warning if `message.format.version` is set via `ConfigCommand`. * Always down-convert if fetch version is v3 or lower. * Add tests to verify new message format version based on the inter-broker protocol version. * Adjust existing tests that create topics with an older message format to have the inter-broker protocol set to 2.8. * Add upgrade note.  Note that the log compaction change to always write new segments with record format v2 if the IBP is 3.0 or higher will be done as part of KAFKA-13093 (with Kafka 3.1 as the target release version).  (cherry picked from commit a46b82bea9abbd08e550d)  Reviewers: David Jacot , David Arthur , Jason Gustafson   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-19T13:22:17Z","2021-07-19T17:21:46Z"
"","10941","KAFKA-10847: Remove internal config for enabling the fix","Also update the upgrade guide indicating about the grace period KIP and its indication on the fix with throughput impact.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-06-29T21:05:52Z","2021-07-15T17:59:22Z"
"","11070","Validate the controllerListener config on startup","Also generate a better error if failing to startup controller due to an empty controllerListener config  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip-500,","niket-goel","2021-07-16T18:17:24Z","2021-07-21T17:41:42Z"
"","11271","KAFKA-13236: TopologyTestDriver should not crash for EOS-beta config","Already fixed in `trunk`/`3.0`. Should be cherry-picked to `2.6`, and `2.7`. -- Did not add a test, but we might want to add one for `trunk` ?  @dajac can we include this in 2.8.1 bug-fix release?  Call for review @ableegoldman","closed","streams,","mjsax","2021-08-27T01:03:27Z","2021-08-27T19:31:10Z"
"","10590","KAFKA-5761: support ByteBuffer as value in ProducerRecord and avoid redundant serialization when it's used","Allows to use pooled byte buffer implementations to serialize record value before calling KafkaProducer.send and reclaim the buffer afterwards, thus reducing unnecessary allocations and GC pressure  Is a new take on https://cwiki.apache.org/confluence/display/KAFKA/KIP-646+Serializer+API+should+support+ByteBuffer","open","","bruto1","2021-04-23T12:19:50Z","2021-06-25T09:43:56Z"
"","10891","MINOR: Add reset to SnapshotRegistry and Revertable","Allow Revertable types to reset to their initial values by calling SnapshotRegistry::reset. This is needed to be able to support re-loading snapshots in the inactive/follower controllers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-06-16T19:55:45Z","2021-06-19T18:50:59Z"
"","10971","KAFKA-12992 Make kraft configuration properties public","All of the Kraft configurations should be made public: `          /*          * KRaft mode configs. Note that these configs are defined as internal. We will make them public in the 3.0.0 release.          */         .defineInternal(ProcessRolesProp, LIST, Collections.emptyList(), ValidList.in(""broker"", ""controller""), HIGH, ProcessRolesDoc)         .defineInternal(NodeIdProp, INT, Defaults.EmptyNodeId, null, HIGH, NodeIdDoc)         .defineInternal(InitialBrokerRegistrationTimeoutMsProp, INT, Defaults.InitialBrokerRegistrationTimeoutMs, null, MEDIUM, InitialBrokerRegistrationTimeoutMsDoc)         .defineInternal(BrokerHeartbeatIntervalMsProp, INT, Defaults.BrokerHeartbeatIntervalMs, null, MEDIUM, BrokerHeartbeatIntervalMsDoc)         .defineInternal(BrokerSessionTimeoutMsProp, INT, Defaults.BrokerSessionTimeoutMs, null, MEDIUM, BrokerSessionTimeoutMsDoc)         .defineInternal(MetadataLogDirProp, STRING, null, null, HIGH, MetadataLogDirDoc)         .defineInternal(ControllerListenerNamesProp, STRING, null, null, HIGH, ControllerListenerNamesDoc)         .defineInternal(SaslMechanismControllerProtocolProp, STRING, SaslConfigs.DEFAULT_SASL_MECHANISM, null, HIGH, SaslMechanismControllerProtocolDoc)  `  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","zhaohaidao","2021-07-04T03:35:14Z","2021-07-06T00:21:40Z"
"","11245","KAFKA-13091; Ensure high watermark incremented after AlterIsr returns","After we have shrunk the ISR, we have an opportunity to advance the high watermark. We do this currently in `maybeShrinkIsr` after the synchronous update through ZK. For the `AlterIsr` path, however, we cannot rely on this call since the request is sent asynchronously. Instead we should attempt to advance the high watermark in the callback when the `AlterIsr` response returns successfully.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-08-20T21:39:00Z","2021-08-23T18:07:39Z"
"","11056","KAFKA-13092: Perf regression in LISR requests","After noticing increased LISR times, we discovered a lot of time was spent synchronously flushing the partition metadata file. This PR changes the code so we asynchronously flush the files.  We ensure files are flushed before appending, renaming or closing the log to ensure we have the partition metadata information on disk. Three new tests have been added to address these cases.  Benchmark by @lbradstreet is included to compare the times. We are looking for the mode that uses topic IDs to decrease compared to trunk. Here are the results comparing trunk to this pr: Trunk ``` Benchmark                            (numPartitions)  (useTopicIds)  Mode  Cnt     Score     Error  Units PartitionCreationBench.makeFollower             2000          false  avgt   15  2421.390 ± 120.005  ms/op PartitionCreationBench.makeFollower             2000           true  avgt   15  5197.590 ±  97.346  ms/op ```  PR ``` Benchmark                            (numPartitions)  (useTopicIds)  Mode  Cnt     Score     Error  Units PartitionCreationBench.makeFollower             2000          false  avgt   15  2451.289 ± 144.459  ms/op PartitionCreationBench.makeFollower             2000           true  avgt   15  2848.374 ± 177.074  ms/op ``` ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-07-15T03:33:46Z","2021-07-15T21:05:00Z"
"","11408","KAFKA-13374: update doc to allow read from leader/followers","After KIP-392, we allow consumers to fetch data from leader or followers. Update the doc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-10-18T12:41:48Z","2021-12-20T17:35:49Z"
"","11093","MINOR: add serde configs to properly set serdes in failing StreamsStaticMembershipTest","After changing the default serde to be null, some system tests started failing. This test didn't explicitly pass in a serde and didn't set the default config so when the test was trying to setup the source node it wasn't able to find any config to use and threw a config exception.  Ran the system test locally and it looked good  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lct45","2021-07-20T14:06:48Z","2021-07-21T20:15:02Z"
"","10693","KAFKA-12625: Fix the NOTICE file","Adds new NOTICE-binary file and packages it in the binary release This follows up the https://github.com/apache/kafka/pull/10474 pull where LICENSE was fix.  Similarly as in that PR, I do not know if this is correct, and I would add the same disclaimer @vvcephei did: ""Please make no assumption that I know what I'm doing and let me know if anything seems wrong.""  I went through all jar files within the distribution file and copied the content of any existing NOTICE file.  Notes: * For the cases where several dependencies including the same NOTICE, I added it only once. * There are cases where the jar included in Kafka's distribution file doesn't contain any NOTICE file, hence nothing was copied. These are:   * activation-1.1.1.jar   * argparse4j-0.7.0.jar   * jackson-annotations-2.10.5.jar   * jackson-dataformat-csv-2.10.5.jar   * jackson-datatype-jdk8-2.10.5.jar   * jackson-jaxrs-base-2.10.5.jar   * jackson-module-scala_2.13-2.10.5.jar   * jakarta.validation-api-2.0.2.jar   * javassist-3.27.0-GA.jar   * javax.servlet-api-3.1.0.jar   * javax.ws.rs-api-2.1.1.jar   * jaxb-api-2.3.0.jar   * jline-3.12.1.jar   * jopt-simple-5.0.4.jar   * lz4-java-1.7.1.jar   * metrics-core-2.2.0.jar   * netty-buffer-4.1.62.Final.jar   * netty-codec-4.1.62.Final.jar   * netty-common-4.1.62.Final.jar   * netty-handler-4.1.62.Final.jar   * netty-resolver-4.1.62.Final.jar   * netty-transport-4.1.62.Final.jar   * netty-transport-native-epoll-4.1.62.Final.jar   * netty-transport-native-unix-common-4.1.62.Final.jar   * reflections-0.9.12.jar   * rocksdbjni-6.19.3.jar   * scala-collection-compat_2.13-2.3.0.jar   * scala-java8-compat_2.13-0.9.1.jar   * scala-logging_2.13-3.9.2.jar   * slf4j-api-1.7.30.jar   * slf4j-log4j12-1.7.30.jar   * snappy-java-1.1.8.1.jar   * zstd-jni-1.4.9-1.jar * For some of those _NOTICE-missing-in-jar_ dependencies, there were other dependencies from the same project that included a NOTICE file, for example, the jackson ones. * For the Netty project, I manually copied their NOTICE file from github  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jlprat","2021-05-14T09:15:29Z","2021-07-29T19:14:33Z"
"","10612","KAFKA-10847: Add internal flag to disable KAFKA-10847 fix","Adds an internal flag that can be used to disable the fixes in KAFKA-10847. It defaults to `true` if the flag is not set or has an invalid boolean value.  The flag is named `__enable.kstreams.outer.join.spurious.results.fix__`. This flag is considered internal only. It is a temporary flag that will be used to help users to disable the join fixes while they do a transition from the previous semantics of left/outer joins. The flag may be removed in future releases.  How was it tested? - I added a few tests to in left and outer joins to verify the previous semantics work when the flag is disabled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","spena","2021-04-29T19:02:58Z","2021-05-03T21:10:05Z"
"","11146","KAFKA-12648: minor followup from Pt. 2 and some new tests","Addresses the handful of remaining feedback from [Pt. 2](https://github.com/apache/kafka/pull/10683), plus adds two new tests: one verifying a multi-topology application with a FKJ and its internal topics, another to make sure IQ works with named topologies (though note that there is a bit more work left for IQ to be fully supported, will be tackled after [Pt. 3](https://github.com/apache/kafka/pull/10788)","closed","","ableegoldman","2021-07-30T02:29:25Z","2021-07-30T17:57:07Z"
"","11260","MINOR: Add missing license entry for jline in LICENSE-binary","Adding the license for https://github.com/jline/jline3  This is a commit that was missed when https://github.com/apache/kafka/pull/11232 was merged.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2021-08-26T02:09:28Z","2021-08-26T02:13:26Z"
"","10772","KAFKA-12697: Add FencedBrokerCount and ActiveBrokerCount metrics to the QuorumController","Adding FencedBrokerCount and ActiveBrokerCount metrics to the QuorumController  Tested using MockControllerMetrics.  https://issues.apache.org/jira/browse/KAFKA-12882  https://cwiki.apache.org/confluence/display/KAFKA/KIP-748%3A+Add+Broker+Count+Metrics#KIP748:AddBrokerCountMetrics","closed","","dielhennr","2021-05-26T23:02:13Z","2021-10-22T19:57:38Z"
"","10545","KAFKA-12672: Added config for raft testing server","Adding a property to the `raft/config/kraft.properties` for running the raft test server in development.  For testing I ran `./bin/test-kraft-server-start.sh --config config/kraft.properties` and validated the test server started running with a throughput test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2021-04-15T18:50:28Z","2021-04-16T03:49:05Z"
"","11011","KAFKA-13051: Require Principal Serde and add default","Adding a default PrincipalBuilderClassProp and requiring that any custom config setting implements KafkaPrincipalSerde.  https://issues.apache.org/jira/browse/KAFKA-13051","closed","","dielhennr","2021-07-09T18:27:44Z","2021-07-19T20:55:53Z"
"","10892","KAFKA-13011: Update deleteTopics Admin API","Added two new apis to support deleteTopics using topic IDs or names. Added a new class TopicCollection to keep a collection of topics defined either by names or IDs. Modified DeleteTopicsResult to support both names and IDs and deprecated the old methods. Eventually we will want to deprecate the old deleteTopics apis as well.  Tested using the existing deleteTopics tests. Also created unit tests for new/updated classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jolshan","2021-06-17T02:43:40Z","2021-07-03T16:01:11Z"
"","10679","KAFKA-12697: Add Global Topic and Partition count metrics to the Quorum Controller","Added GlobalTopicCount and GlobalPartitionCount metrics to the QuorumControllerMetrics.  The metrics are calculated by counting records as they are replayed e.g. replay(TopicRecord), replay(RemoveTopicRecord)  This was unit tested using MockControllerMetrics.  https://issues.apache.org/jira/browse/KAFKA-12697","closed","","dielhennr","2021-05-12T17:54:04Z","2021-05-13T19:08:19Z"
"","10718","KAFKA-12811: kafka-topics.sh should let the user know they cannot adjust the replication factor for a topic using the --alter flag and not warn about missing the --partition flag","Added 1 new line to check that that ""replication factor"" & ""alter"" are not being used together. Uses the existing CommandLineUtils to do the check.  Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ]  Verify documentation (including upgrade notes)","closed","","Moovlin","2021-05-18T14:58:35Z","2021-06-01T20:10:08Z"
"","11461","KAFKA-13422: Add verification of duplicate configuration for each type of LoginModule in JaasConfigFile","Add verification of duplicate configuration for each type of LoginModule in JaasConfigFile.  Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13422  Author: RivenSun2 riven.sun@zoom.us","open","","RivenSun2","2021-11-03T06:56:55Z","2022-01-07T13:23:10Z"
"","11217","KAFKA-13204: assignor name conflict check","Add the partition assignor name conflicting check to avoid wrong assignor used. Also add tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-16T08:21:45Z","2021-08-19T00:23:35Z"
"","11149","KAFKA-13229: add total blocked time metric to streams (KIP-761)","add the following producer metrics: flush-time-total: cumulative sum of time elapsed during in flush. txn-init-time-total: cumulative sum of time elapsed during in initTransactions. txn-begin-time-total: cumulative sum of time elapsed during in beginTransaction. txn-send-offsets-time-total: cumulative sum of time elapsed during in sendOffsetsToTransaction. txn-commit-time-total: cumulative sum of time elapsed during in commitTransaction. txn-abort-time-total: cumulative sum of time elapsed during in abortTransaction.  add the following consumer metrics: commited-time-total: cumulative sum of time elapsed during in committed. commit-sync-time-total: cumulative sum of time elapsed during in commitSync.  add a total-blocked-time metric to streams that is the sum of: consumer’s io-waittime-total consumer’s iotime-total consumer’s committed-time-total consumer’s commit-sync-time-total restore consumer’s io-waittime-total restore consumer’s iotime-total admin client’s io-waittime-total admin client’s iotime-total producer’s bufferpool-wait-time-total producer's flush-time-total producer's txn-init-time-total producer's txn-begin-time-total producer's txn-send-offsets-time-total producer's txn-commit-time-total producer's txn-abort-time-total  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rodesai","2021-07-30T05:37:57Z","2021-08-30T22:39:26Z"
"","10562","MINOR: Update tests to include the 2.8.0 release","Add the 2.8 release to our tests.","closed","","vvcephei","2021-04-19T16:29:48Z","2022-02-04T20:11:43Z"
"","11227","KAFKA-13211: add support for infinite range query for WindowStore","Add support for infinite range query for WindowStore. Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13210   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-18T09:09:22Z","2021-09-22T16:15:06Z"
"","11234","KAFKA-13212: add support infinite query for session store","Add support for infinite range query for SessionStore. Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13210  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-08-19T05:56:33Z","2021-10-12T23:14:39Z"
"","10893","KAFKA-12909: add missing tests","Add missing unit tests for ""on the side"" bug fix from https://github.com/apache/kafka/pull/10861","closed","streams,","mjsax","2021-06-17T05:13:11Z","2021-06-28T22:32:20Z"
"","10589","MINOR: move topic configuration defaults","Add default values for the configurations to TopicConfig.java.  Eventually we want to move all of the log configurations into TopicConfig.java, and this is a good step along the way.","open","kip-500,","cmccabe","2021-04-23T01:57:01Z","2021-05-03T08:27:10Z"
"","11258","MINOR: add to empty, remove then add different test","add a test for named topology  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-08-25T18:29:58Z","2021-09-20T16:21:36Z"
"","11275","KAFKA-13128: extract retry checker","add a new case for the flaky tests to take care of threads starting up  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2021-08-27T15:05:42Z","2021-08-30T17:32:22Z"
"","11476","KAFKA-13407: Always start controller when broker wins election","Add a call to `onControllerFailover` into code path where `elect` is called, and the broker discovers it has already been elected. We found that by restarting the ZK leader we could occasionally trigger this code path, and prior to this change it would not start a controller failover. This left our Kafka cluster in a state where the `/controller` znode existed, and named the broker that had ""won"" the controller election, but in terms of runtime state: all the brokers had resigned from being the controller. Without a running controller, restarting brokers would typically cause partitions to become under-replicated as the restarted brokers never received the UpdateMetadata or LeaderAndISR requests required to correctly lead / follow any of their replicas.  Also add some info level logging and more descriptive log messages for the log lines that were helpful in tracking the controller failover.  proposed fix for https://issues.apache.org/jira/browse/KAFKA-13407  Co-authored-by: Tina Selenge  Co-authored-by: Adrian Preston  Co-authored-by: Edoardo Comar   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2021-11-08T16:59:34Z","2022-05-03T21:03:32Z"
"","11086","KAFKA-13103: add REBALANCE_IN_PROGRESS error as retriable error for AlterConsumerGroupOffsetsHandler","Add `REBALANCE_IN_PROGRESS` error as retriable error for `AlterConsumerGroupOffsetsHandler`, and tests for it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-20T03:23:34Z","2021-09-03T18:29:45Z"
"","10575","KAFKA-12702: Fix NPE in networkListeners from BrokerServer","According to the comments in the configuration file (config/kraft/server.properties), if listeners and advertised.listeners are not configured with an address, the program will automatically obtain java.net.InetAddress.getCanonicalHostName(), but this will actually cause the service to fail to start. Because the host parameter value of Listener in BrokerRegistrationRequestData will be null. For more information stack, can link to this jiraId to view: [KAFKA-12702](https://issues.apache.org/jira/browse/KAFKA-12702)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-04-21T09:21:11Z","2021-04-22T03:20:22Z"
"","10700","KAFKA-12789: Remove Stale comments for meta response handling logic","According to my understanding, the following paragraph looks like a stale comments.  > public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse response) {             ...             // Don't update the cluster if there are no valid nodes...the topic we want may still be in the process of being             // created which means we will get errors and no nodes until it exists             if (response.brokers().isEmpty()) {                 log.trace(""Ignoring empty metadata response with correlation id {}."", requestHeader.correlationId());                 this.metadata.failedUpdate(now);             } else {                 this.metadata.update(inProgress.requestVersion, response, inProgress.isPartialUpdate, now);             }             ...  The comments above mean we will may get errors and no nodes if the topic we want may still be in the process of being created. However, every meta request will return all brokers from the logic of the server side, just as followed  >   def handleTopicMetadataRequest(request: RequestChannel.Request): Unit = {     ...     val brokers = metadataCache.getAliveBrokers     ...   }   I studied the related git commit history and figured out why.  1. This comments was first introduced in KAFKA-642 (e11447650a). which means meta request only need brokers related to the topics we want. 2. KAFKA-1535 (commitId: 4ebcdfd51f) changed the server side logic. which has the metadata response contain all alive brokers rather than just the ones needed for the given topics. 3. However, this comments are retained till now. So According to my understanding, this comments looks like a stale one and can be removed.","closed","","zhaohaidao","2021-05-15T03:27:51Z","2021-05-19T16:38:42Z"
"","10630","MINOR: Stop logging raw record contents above TRACE level in WorkerSourceTask","Accidental logging of record contents is a security risk as they may contain sensitive information such as PII. This downgrades the level of log messages in the `WorkerSourceTask` class that contain raw record contents to `TRACE` level in order to make that scenario less likely.  As these changes are trivial no tests are added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-05-04T16:10:08Z","2021-05-10T15:42:22Z"
"","11281","KAFKA-12886: Enable request forwarding to the controller by default for zookeeper mutation protocols in IBP 3.1 and higher [REWRITE]","A rewrite of [KAFKA-12886](https://github.com/apache/kafka/pull/11042) and including [KAFKA-13142](https://github.com/apache/kafka/pull/11141) since it shares code with KAFKA-12886.   This PR does the following:  1. Enables request forwarding to the controller by default for zookeeper mutation protocols in `IBP >= 3.1`. 2. Only the broker that a dynamic config applies to can validate its config update. This pr validates the configs from {Alter, IncrementalAlter}ConfigsRequest on the broker that the configs are for before they are forwarded to the active controller. If the validation fails, the proxy broker will send a response to the client with an ApiError. Otherwise they are forwarded to the active controller or persisted if the broker that did the validation is a pre-KRaft broker and is also the active controller. This PR introduces this new dynamic broker config behavior to both pre-KRaft and KRaft clusters.  3. Refactor tests that did not expect pre-KRaft brokers to forward requests.  Tested new dynamic config behavior manually for both pre-KRaft and KRaft clusters.  STATUS: Design and major functionality done TODO:   - Write new tests. - Refactor remaining failing tests that are not expecting pre-KRaft brokers to forward requests. - Find out where to validate Config Policy? where to validate topic configs? on broker before forwarding or on the controller? The KRaft controller only authorizes and validates the ConfigResource but not the actual configs. This PR only validates broker configs before the request is forwarded. It may be good to also validate topic configs on the brokers so that api behavior is consistent with before (send both a TOPIC and BROKER ConfigResource.Type alteration in one {Alter,IncrementalAlter}ConfigsRequest and get api errors for both if both configs are invalid instead of just an error for the broker config)  https://issues.apache.org/jira/browse/KAFKA-12886","open","","dielhennr","2021-08-29T00:39:28Z","2021-09-18T03:21:21Z"
"","11141","KAFKA-13142: Validate dynamic config alterations prior to forwarding them to the KRaft controller.","A requirement of KRaft is that it is upgradable from v3.0. Dynamic configs were previously not being validated before being persisted to the KRaft metadata quorum. This means that subsequent upgrades from 3.0 could retain invalid dynamic configs in metadata.   This PR validates dynamic configs on the broker that the configs are for before they are forwarded to the KRaft controller and persisted in metadata. The proxy broker will validate the configs, then either send a response to the client if any of the configs were invalid or forward the configs to the controller to be persisted in metadata.  https://issues.apache.org/jira/browse/KAFKA-13142","open","","dielhennr","2021-07-28T21:03:49Z","2022-02-11T06:36:27Z"
"","10605","KAFKA-12726 prevent a stuck Task.stop() from blocking subsequent Task.stops()s","A misbehaving Task.stop() can prevent other Tasks from stopping, even when a graceful shutdown timeout is configured. We improve the situation as follows:  - prior to task.shutdown.graceful.timeout.ms expiring, the existing behavior is retained, except that Task.stop() is called in a new Thread. - after task.shutdown.graceful.timeout.ms expires, the Worker runs any remaining Task.stop()s concurrently.  Thus, the behavior doesn't change appreciably when Tasks behave normally; however, if any Task.stop() get stuck (e.g. in a retry loop) we continue with a best-effort shutdown.","closed","","ryannedolan","2021-04-28T17:40:43Z","2021-04-30T20:11:23Z"
"","11399","Code optimization: Supplement the missing Override tag on the override method","A large number of ‘@Override’ tags are missing, resulting in a lot of warnings in IDEA, and inconvenience to code reading","open","","lordcheng10","2021-10-14T15:26:17Z","2021-10-15T04:52:50Z"
"","11246","MINOR: Improve controlled shutdown logging","A few small logging improvements: - Only print error when it is not NONE - Full list of remaining partitions are printed only at debug level - Only backoff and print retry logging if there are remaining retries  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2021-08-20T22:22:24Z","2021-08-24T16:22:58Z"
"","10629","BlockingConnectorTest improvements to verify Connectors and Tasks are successfully deleted","A few extra checks and tests to verify that a blocking Connector or Task will not prevent Workers from properly stopping and cleaning up Tasks. Previously, tests existed to verify that Tasks were stopped, but not cleaned up.  A new check and assertion is added to EmbeddedConnectClusterAssertions to enable these tests.  Also, an extra pause is added to the blocking `poll()` in these tests, because I observed `poll()` being called in a tight loop during some tests.  Extra context:  In a previous PR (https://github.com/apache/kafka/pull/10605) I was confused by `WorkerTask.stop()` vs `Task.stop()`. My reading of the code was that `Task.stop()` was called sequentially, but in fact `WorkerTask.stop()` merely triggers the asynchronous invocation of `Task.stop()`. During my investigation of https://issues.apache.org/jira/browse/KAFKA-12726 I wrote tests to validate that the problem I observed in production was already fixed. This PR includes these extra tests.","open","connect,","ryannedolan","2021-05-03T19:07:27Z","2022-04-19T22:27:19Z"
"","10779","CONFLUENT: Complete version regex check in system tests","A couple of the sanity-check system tests confirm that the version of Kafka on the CLASSPATH during the test is the one that is expected.  These tests were failing for the 2.8 release due to the version `6.2.0-ccs` not being accounted for correctly in the various regex's.  This patch revamps the regex set to be more organized and complete with the various possibilities clearly identified.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rondagostino","2021-05-27T16:53:25Z","2021-05-27T16:53:45Z"
"","10689","KAFKA-12697: Add EventQueueSize metric to Quorum controller","A counter is kept in the event queue.  Tested using events that wait on a CountDownLatch before completing.  https://issues.apache.org/jira/browse/KAFKA-12697","open","","dielhennr","2021-05-13T23:10:44Z","2021-05-17T22:42:40Z"
"","11257","HOTFIX: fix backport of #11248 by future-proofing the EmbeddedKafkaCluster","A backport of #11248 broke the 2.8 build due to usage of the `EmbeddedKafkaCluster#stop` method, which used to be private. It seems we made this public when we upgraded to JUnit5 on the 3.0 branch and had to remove the ExternalResource that was previously responsible for calling `start()` and `stop()` for this class using the no-longer-available `@ClassRule` annotation.   Rather than adapt this test to the 2.8 style by migrating it to use `@ClassRule` as well, I opted to just make the `stop() method public as well (since its analogue `start()` has always been public anyways). This should hopefully prevent any future backports that include integration tests from having to manually go in and adapt the test, or accidentally break the build as happened here.","closed","","ableegoldman","2021-08-25T01:24:47Z","2021-08-25T03:21:13Z"
"","11403","MINOR: Remove topic null check from `TopicIdPartition` and adjust constructor order","`TopicPartition` allows a null `topic` and there are cases where we have a topic id, but no topic name. Even for `TopicIdPartition`, the non null topic name check was only enforced in one constructor.  Also adjust the constructor order to move the nullable parameter to the end, update tests and javadoc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-10-15T16:29:22Z","2021-11-10T13:47:08Z"
"","10524","MINOR: Use `testRuntimeOnly` instead of `testRuntime` in storage modules","`testRuntime` is deprecated in Gradle 6.x and has been removed in Gradle 7.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-04-11T17:03:11Z","2021-04-12T05:48:45Z"
"","10511","KAFKA-12584; Remove deprecated `Sum` and `Total` classes","`Sum` and `Total` classes were deprecated and replaced by `WindowedSum` and `CumulativeSum` in 2.4. As they are not really supposed to be used by external parties (even if they are part of AK's public API), it seems fine to remove them for 3.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-04-09T11:58:33Z","2021-04-13T09:48:21Z"
"","11324","KAFKA-13294: Upgrade Netty to 4.1.68 for CVE fixes","`netty-codec` `4.1.62.Final` has the following security vulnerabilities, which in turn effects `netty-transport-native-epoll` Apache Kafka depends on.  - [CVE-2021-37136](https://github.com/netty/netty/security/advisories/GHSA-grg4-wf29-r9vv) - [CVE-2021-37137](https://github.com/netty/netty/security/advisories/GHSA-9vjp-v76f-g363)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-09-14T06:37:14Z","2021-09-17T12:36:14Z"
"","11265","MINOR: Update `TransactionalMessageCopier` to use the latest transaction pattern","`ExactlyOnceMessageProcessor` is the reference so this patch updates `TransactionalMessageCopier` to be similar.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-08-26T10:08:53Z","2021-08-27T09:12:01Z"
"","10984","MINOR: fix typo in LogCleanerTest.scala","`are rename to` -> `are renamed to`","closed","","ccding","2021-07-07T03:08:21Z","2021-07-07T13:15:23Z"
"","11221","KAFKA-13207: Skip truncation on fetch response with diverging epoch if partition removed from fetcher","`AbstractFetcherThread#truncateOnFetchResponse` is used with IBP 2.7 and above to truncate partitions based on diverging epoch returned in fetch responses. Truncation should only be performed for partitions that are still owned by the fetcher and this check should be done while holding `partitionMapLock` to ensure that any partitions removed from the fetcher thread are not truncated. The PR adds this check. Truncation will be performed by any new fetcher that owns the partition when it restarts fetching.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2021-08-16T17:51:08Z","2021-08-17T18:59:32Z"
"","11396","HOTFIX: Compilation fails due to unhandled deprecation warnings in streams tests","``` > Task :streams:compileTestJava FAILED /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowsTest.java:65: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.TimeWindows has been deprecated         assertThrows(IllegalStateException.class, () -> TimeWindows.ofSizeAndGrace(ofMillis(10), ofMillis(10)).grace(ofMillis(10)));                                                                                                               ^ /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/TimeWindowsTest.java:66: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.TimeWindows has been deprecated         assertThrows(IllegalStateException.class, () -> TimeWindows.ofSizeWithNoGrace(ofMillis(10)).grace(ofMillis(10)));                                                                                                    ^ error: warnings found and -Werror specified /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/JoinWindowsTest.java:79: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.JoinWindows has been deprecated         assertThrows(IllegalStateException.class, () -> JoinWindows.ofTimeDifferenceAndGrace(ofMillis(10), ofMillis(10)).grace(ofMillis(10)));                                                                                                                         ^ /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/JoinWindowsTest.java:80: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.JoinWindows has been deprecated         assertThrows(IllegalStateException.class, () -> JoinWindows.ofTimeDifferenceWithNoGrace(ofMillis(10)).grace(ofMillis(10)));                                                                                                              ^ /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowsTest.java:92: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.SessionWindows has been deprecated         assertThrows(IllegalStateException.class, () -> SessionWindows.ofInactivityGapAndGrace(ofMillis(10), ofMillis(10)).grace(ofMillis(10)));                                                                                                                           ^ /Users/djacot/dev/src/github.com/dajac/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/SessionWindowsTest.java:93: warning: [deprecation] grace(java.time.Duration) in org.apache.kafka.streams.kstream.SessionWindows has been deprecated         assertThrows(IllegalStateException.class, () -> SessionWindows.ofInactivityGapWithNoGrace(ofMillis(10)).grace(ofMillis(10)));                                                                                                                ^ 1 error 6 warnings ```  It seems that those tests were introduced in d1415866cc3ed7eeb198df6477a09584b6d1f8a2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-10-14T09:55:46Z","2021-10-15T07:00:49Z"
"","10785","KAFKA-6718 / A Rack awareness for Kafka Streams","[KIP-708](https://cwiki.apache.org/confluence/display/KAFKA/KIP-708%3A+Rack+awareness+for+Kafka+Streams) implementation.  Notable changes: - Added required configuration options in `StreamsConfig`. Configurations are guarded by the validation rules and corresponding unit tests are present in `StreamsConfigTest`. - `SubscriptionInfo` version bump to `10` and added necessary fields for encoding client tags. - Moved standby task assignment logic behind `StandbyTaskAssignor` abstract class. - `StandbyTaskAssignorInitializer` decides which of the `StandbyTaskAssignor` implementations to use based on `AssignmentConfigs` - When `AssignmentConfigs#rackAwareAssignmentTags` is present, new `ClientTagAwareStandbyTaskAssignor` will be chosen which distributes the standby tasks based on client tags and configured `rackAwareAssignmentTags`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lkokhreidze","2021-05-28T17:59:03Z","2022-01-20T09:08:48Z"
"","11220","KAFKA-10777: Add additional configuration to control MirrorMaker 2 internal topics naming convention","[KIP-690](https://cwiki.apache.org/confluence/display/KAFKA/KIP-690%3A+Add+additional+configuration+to+control+MirrorMaker+2+internal+topics+naming+convention)    *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","OmniaGM","2021-08-16T16:17:10Z","2021-10-11T14:41:10Z"
"","10740","Kafka 8613 kip 633 drop default grace period streams","[KAFKA-8613] Set default grace period to 0 KIP-633: Drop 24 hour default of grace period in Streams  Added API implementation for specifying grace periods for TimeWindows, SessionWindows, JoinWindows and SlidingWindows Changed default grace period constant Windows.DEFAULT_GRACE_PERIOD_MS to 0 milliseconds from 24 hours Updated corresponding unit tests  Added deprecation suppression annotations for test classes using the newly deprecated APIs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","izzyacademy","2021-05-20T16:09:10Z","2021-06-25T00:44:31Z"
"","10926","KAFKA-8613: KIP-633 New APIs for Controlling Grace Period for Windowed Operations","[KAFKA-8613] Make grace period mandatory  KIP-633 New APIs for Controlling Grace Period for Windowed Operations  - Added API changes by KIP-633 for JoinWindows, SessionWindows, TimeWindows and SlidingWindows  - Renamed Windows.DEFAULT_GRACE_PERIOD_MS to DEPRECATED_OLD_24_HR_GRACE_PERIOD - Added new constant Windows.NO_GRACE_PERIOD to avoid magic constants when 0 is specified as grace Period - Added preliminary Java unit test cases for new API methods  - Replaced Deprecated calls with equivalent in Examples - Replaced Deprecated API calls in Scala tests with updated API method calls - Added Deprecation suppression in Tests for deprecated API method calls in Java and Scala Tests  @ableegoldman @mjsax @cadonna @showuon please review when you have a moment  I apologize in advance but I was unable to merge the new changes into the old PR  https://github.com/apache/kafka/pull/10740  I spent a lot of time trying to rebase and merge but it just did not work. So I had to create a new branch and a new PR without the conflicts.","closed","","izzyacademy","2021-06-24T19:43:53Z","2021-07-23T04:12:53Z"
"","11369","KAFKA-13327, KAFKA-13328, KAFKA-13329: Clean up preflight connector validation","[KAFKA-13327](https://issues.apache.org/jira/browse/KAFKA-13327): Modify preflight validation logic to prevent 500 responses from being returned instead of valid 200 responses with detailed error messages pertaining to the relevant configuration properties.  [KAFKA-13328](https://issues.apache.org/jira/browse/KAFKA-13328): Add preflight validation logic for per-connector header converters.  [KAFKA-13329](https://issues.apache.org/jira/browse/KAFKA-13329): Add preflight validation logic for per-connector key and value converter classes.  Additionally, a small bug in the logic for [KAFKA-3829](https://issues.apache.org/jira/browse/KAFKA-3829) introduced by [KIP-458](https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy) is fixed; the preflight check to ensure that a sink connector's group ID doesn't conflict with the Connect worker's group ID now takes into account overrides made by the `consumer.override.group.id` connector property.  A new integration test is added that covers a wide variety of cases for sink connector, key/value converter, and header converter validation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2021-09-29T14:02:32Z","2021-10-28T14:39:49Z"
"","10807","KAFKA-12797: Log the evictor of fetch sessions","[KAFKA-12797](https://issues.apache.org/jira/browse/KAFKA-12797) describes the difficulty in discovering bad clients that are responsible for evicting every other non-privileged session from the cache. While the ideal solution would be to have a quota (so a user ends up evicting only their own sessions once quota was reached), the fetch session cache is very performance sensitive, and this problem is very rare, so logging seems like a reasonable first step.","open","","tombentley","2021-06-02T12:05:02Z","2021-08-06T13:48:51Z"
"","10937","KAFKA-10587 Rename kafka-mirror-maker CLI command line arguments for KIP-629","[KAFKA-10587](https://issues.apache.org/jira/browse/KAFKA-10587) Rename kafka-mirror-maker CLI command line arguments for KIP-629 Replace ""whitelist"" argument in kafka-mirror-maker cli command with ""include""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","OmniaGM","2021-06-29T09:52:35Z","2021-07-02T01:56:48Z"
"","10945","KAFKA-13017: Remove excessive logging for sink task deserialization errors","[Jira](https://issues.apache.org/jira/browse/KAFKA-13017)  Reverts https://github.com/apache/kafka/pull/7496, which added `ERROR`-level logging for deserialization errors in sink tasks even when connectors had logging for these errors disabled.  No information is lost by this change that cannot be retained in an opt-in fashion by setting `errors.log.enable` and `errors.log.include.messages` to `true` in a connector config.  No testing is added. This commit was created via the GitHub UI; best to wait for a clean(ish) CI build before reviewing/merging.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-06-30T15:28:26Z","2021-07-16T14:57:37Z"
"","11046","KAFKA-12980: Return empty record batch from Consumer::poll when position advances due to aborted transactions","[Jira](https://issues.apache.org/jira/browse/KAFKA-12980)  This is useful for reads to the end of a topic that contains aborted or empty transactions. If an aborted transaction is at the end of the topic, the consumer can now be expected to return from `poll` if it advances past that aborted transaction, and users can query the consumer's latest `position` for the relevant topic partitions to see if it has managed to make it past the end of the topic (or rather, what was the end of the topic when the attempt to read to the end of the topic began).  For a concrete example of this logic, see the [`KafkaBasedLog::readToLogEnd`](https://github.com/apache/kafka/blob/5e5d5bff3bdaf807338ec9adeac982f8a5c98fbd/connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java#L322-L345) method that Connect employs to refresh its view of internal topics.  No new unit tests are added, but many existing ones are modified to ensure that aborted and empty transactions are detected and reported correctly by `Fetcher::collectFetch`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-07-14T05:01:42Z","2021-12-09T16:46:04Z"
"","10854","KAFKA-12717: Remove internal Connect converter properties (KIP-738)","[Jira](https://issues.apache.org/jira/browse/KAFKA-12717) / [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-738%3A+Removal+of+Connect%27s+internal+converter+properties)  Changes:  - Drop support for user-specified internal converters and their properties - Log a warning if users attempt to specify internal converter-related properties in their worker config - Remove all references to the internal converter properties from the code base aside from the above-described warning logic - Updated `Plugins` test for internal converter instantiation as a sanity check (though this is likely redundant given the immense coverage already provided by existing integration tests) - Add an item to the upgrade notes about the removal of these properties  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","C0urante","2021-06-09T14:29:51Z","2021-07-02T14:05:54Z"
"","10521","KAFKA-12651: Register Connect REST extensions before bringing up REST resources","[Jira](https://issues.apache.org/jira/browse/KAFKA-12651)  Copied from the Jira ticket:  > Connect currently registers custom REST extensions after REST resources. This can be problematic in security-conscious environments where REST extensions are used to lock down access to the Connect REST API, as it creates a window of opportunity for unauthenticated access to the REST API between the time the worker's REST resources are brought up and when its REST extensions are registered.  This change aims to address that vulnerability by registering REST extensions before REST resources.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-04-11T02:36:10Z","2021-04-11T14:25:27Z"
"","10528","KAFKA-12497: Skip periodic offset commits for failed source tasks","[Jira](https://issues.apache.org/jira/browse/KAFKA-12497)  This change serves two purposes: 1. Eliminate unnecessary log messages for offset commit of tasks that don't need to perform offset commits (e.g., a task that has failed and for which all data has been flushed and committed) 2. Stop blocking the offset commit thread unnecessarily for flushes that will never succeed because the task's producer has failed to send a record in the current batch with a non-retriable error  Existing unit tests for the `OffsetStorageWriter` are tweaked to verify the small change made to it. Several new unit tests are added for the `WorkerSourceTask` that cover various cases where offset commits should not be attempted, and some existing tests are modified to cover cases where offset commits either should or should not be attempted.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","C0urante","2021-04-12T15:08:24Z","2022-08-01T12:33:04Z"
"","10563","KAFKA-12487: Add support for cooperative consumer protocol with sink connectors","[Jira](https://issues.apache.org/jira/browse/KAFKA-12487)  Currently, the `WorkerSinkTask`'s consumer rebalance listener (and related logic) is hardcoded to assume eager rebalancing, which means that all partitions are revoked any time a rebalance occurs and then the set of partitions included in `onPartitionsAssigned` is assumed to be the complete assignment for the task. Not only does this cause failures when the cooperative consumer protocol is used, it fails to take advantage of the benefits provided by that protocol.  These changes alter framework logic to not only not break when the cooperative consumer protocol is used for a sink connector, but to reap the benefits of it as well, by not revoking partitions unnecessarily from tasks just to reopen them immediately after the rebalance has completed.  This change will be necessary in order to support [KIP-726](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=177048248), which currently proposes that the default consumer partition assignor be changed to the `CooperativeStickyAssignor`.  Two integration tests are added to verify sink task behavior with both eager and cooperative consumer protocols, and new and existing unit tests are adopted as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-04-19T18:00:23Z","2021-11-12T19:37:14Z"
"","11323","KAFKA-12226: Commit source task offsets without blocking on batch delivery","[Jira](https://issues.apache.org/jira/browse/KAFKA-12226)  Replaces https://github.com/apache/kafka/pull/10112  Replaces the current batch-based logic for offset commits with a dynamic, non-blocking approach outlined in discussion on #10112 [here](https://github.com/apache/kafka/pull/10112#issuecomment-910510910), [here](https://github.com/apache/kafka/pull/10112#issuecomment-910540773), [here](https://github.com/apache/kafka/pull/10112#issuecomment-914348989), [here](https://github.com/apache/kafka/pull/10112#issuecomment-914547745), and [here](https://github.com/apache/kafka/pull/10112#issuecomment-915350922).  Essentially, a deque is kept for every source partition that a source task produces records for, and each element in that deque is a `SubmittedRecord` with a flag to track whether the producer has ack'd the delivery of that source record to Kafka yet. Periodically, the worker (on the same thread that polls the source task for records and transforms, converts, and dispatches them to the producer) polls acknowledged elements from the beginning of each of these deques and collects the latest offsets from these elements, storing them in a snapshot that is then committed on the separate source task offset thread.  The behavior of the `offset.flush.timeout.ms` property is retained, but essentially now only applies to the actual writing of offset data to the internal offsets topic (if running in distributed mode) or the offsets file (if running in standalone mode). No time is spent during `WorkerSourceTask::commitOffsets` blocking on the acknowledgment of records by the producer.  It's possible that memory exhaustion may occur if, for example, a single Kafka partition is offline for an extended period. In cases like this, the collection of deques in the `SubmittedRecords` class may continue to grow indefinitely until the partition comes back online and the `SubmittedRecord`s in those deques that targeted the formerly-offline Kafka partition are acknowledged and can be removed. Although this may be suboptimal, it is no worse than the existing behavior of the framework in these cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2021-09-13T18:53:37Z","2021-11-09T20:58:30Z"
"","10520","KAFKA-10816: Use root resource as readiness and health probe for Connect distributed mode","[Jira](https://issues.apache.org/jira/browse/KAFKA-10816)   To address KAFKA-10816, the root resource (`GET /`) is modified to go through the worker's herder tick thread. This causes all requests to that resource to either not complete or be met with a 404 response before the worker has finished startup and is ready to handle requests.  In addition to being made a viable readiness probe, the root resource is also made into a health probe for the worker. If the worker is having trouble reading to the end of the config topic or (re-)joining the Connect cluster, requests to the root resource will not return and will eventually be met with a 500 timeout response.  The hacky workaround used to check for worker readiness in the `BlockingConnectorTest` is modified to use the root resource instead. No other tests are touched; existing coverage should be sufficient to vet this change.","open","connect,","C0urante","2021-04-11T02:33:54Z","2022-04-19T22:25:15Z"
"","10919","KAFKA-12985: CVE-2021-28169 - Upgrade jetty to 9.4.42","[Jetty 9.4.41.v20210516](https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.41.v20210516) resolves following security vulnerabilities.  - [CVE-2021-28169](https://nvd.nist.gov/vuln/detail/CVE-2021-28169) (described in the issue) - [CVE-2021-34428](https://nvd.nist.gov/vuln/detail/CVE-2021-34428)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-06-23T06:16:51Z","2021-07-04T14:50:38Z"
"","10526","KAFKA-12655: CVE-2021-28165 - Upgrade jetty to 9.4.39","[Jetty 9.4.39.v20210325](https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.39.v20210325) resolves following security vulnerabilities.  - [CVE-2021-28165](https://nvd.nist.gov/vuln/detail/CVE-2021-28165) (described in the issue) - [CVE-2021-28164](https://nvd.nist.gov/vuln/detail/CVE-2021-28164) - [CVE-2021-28163](https://nvd.nist.gov/vuln/detail/CVE-2021-28163)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-04-12T11:11:27Z","2021-04-13T09:40:20Z"
"","10641","KAFKA-12752: CVE-2021-28168 upgrade jersey to 2.34 or 3.02","[CVE-2021-28168](https://nvd.nist.gov/vuln/detail/CVE-2021-28168) in Jersey was fixed in [2.34](https://github.com/eclipse-ee4j/jersey/security/advisories/GHSA-c43q-5hpj-4crv).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-05-06T13:55:39Z","2021-05-07T12:43:11Z"
"","10739","KAFKA-12820: Upgrade maven-artifact dependency to resolve CVE-2021-26291","[CVE-2021-26291](https://nvd.nist.gov/vuln/detail/CVE-2021-26291), which makes Man-In-The-Middle-Attack possible, was fixed in maven [3.8.1](https://maven.apache.org/docs/3.8.1/release-notes.html).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-05-20T16:00:53Z","2021-05-21T07:07:08Z"
"","10642","KAFKA-12756: Update Zookeeper to 3.6.3 or higher","[CVE-2021-21409](https://nvd.nist.gov/vuln/detail/CVE-2021-21409) in Zookeeper was fixed in [3.6.3](https://zookeeper.apache.org/doc/r3.6.3/releasenotes.html).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-05-06T14:08:20Z","2021-05-31T07:22:30Z"
"","10670","KAFKA-10273 Connect Converters should produce actionable error messages","@rhauch Hi, please review this PR for me. Many thanks","open","connect,","DuongPTIT","2021-05-11T07:29:11Z","2022-04-19T22:54:44Z"
"","10967","KAFKA-12770: CheckStyle team needs this feature (in order to include Kafka into their regression suite)","@ijuma please review.  **_Rationale /related comment:_** https://github.com/apache/kafka/pull/10656#issuecomment-836074067  ![image](https://user-images.githubusercontent.com/19467899/124363918-84db9c00-dc3e-11eb-95cb-493173007a0e.png)  **_Description:_** `checkstyleVersion` build option is introduced (for overriding CheckStyle project-defined dependency version).  **_Note:_** previous PR #10698 (that contains identical commit) is closed: it somehow slipped through the cracks, so I opted to open a new one.","open","","dejan2609","2021-07-03T18:45:53Z","2022-07-16T06:03:43Z"
"","10698","KAFKA-12770: introduce `checkstyleVersion` build option (for overriding CheckStyle project-defined dependency version)","@ijuma please review this.  **Rationale**: Requested by CheckStyle team (@romani); they needs this in order to add Apache Kafka project into their regression suit here https://github.com/checkstyle/contribution/blob/master/checkstyle-tester/projects-to-test-on.properties  **JIRA ticket**: https://issues.apache.org/jira/browse/KAFKA-12770  **Related PR/comment:** https://github.com/apache/kafka/pull/10656#issuecomment-835809154  @romani how to use this (hopefully this will be merged into trunk at some point): - this command uses project-defined CheckStyle version: `./gradlew checkstyleMain checkstyleTest`  - while this one overrides it: `./gradlew checkstyleMain checkstyleTest -PcheckstyleVersion=8.41.1`","closed","","dejan2609","2021-05-14T20:58:29Z","2021-07-03T18:48:06Z"
"","11197","Tiered Storage on 2.8.x","28x TS changes  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-08-10T10:17:31Z","2022-01-13T22:32:32Z"
"","10778","KAFKA-12856: Upgrade Jackson to 2.12.3","2.10.x is no longer supported, so we should move to 2.12 for the 3.0 release.  ScalaObjectMapper has been deprecated and it looks like we don't actually need it, so remove its usage.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-05-27T14:20:44Z","2021-05-27T16:56:36Z"
"","10646","KAFKA-8897 Follow-up: Consolidate the global state stores","1. When register state stores, add the store to globalStateStores before calling any blocking calls that may throw errors, so that upon closing we would close the stores as well. 2. Remove the other list as a local field, and call topology.globalStateStores when needed to get the list.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-05-07T05:35:23Z","2021-06-04T15:30:18Z"
"","11195","MINOR: update the KStream#branch(split) doc and java doc and tests","1. update documentation to fix some wrong (out-dated) content After https://github.com/apache/kafka/pull/9107 (KIP-418), the returned content is not `kstream[]` anymore. Also, this description: `If no predicate matches, the the record is dropped.` is not right now. `It'll be routed to the default branch, or dropped if no default branch is created.` Fix them.   2. update java doc 3. add a test for `Branched.withFunction()` with non-null result test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","showuon","2021-08-10T08:50:57Z","2021-08-10T20:41:25Z"
"","11155","MINOR: Fix flaky shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound","1. This test is taking two iterations since the firs iteration is designed to fail due to unknow topic leader. However both the timeout and the backoff are set to 50ms, while the actual SYSTEM time is used. This means we are likely to timeout before executing the second iteration. I thought about using a mock time but dropped that idea as it may forgo the purpose of this test, instead I set the backoff time to 10ms so that we are almost unlikely to hit this error anymore.  2. Found a minor issue while fixing this which is that when we have non-empty not-ready topics, but the topics-to-create is empty (which is possible as the test shouldCreateTopicWhenTopicLeaderNotAvailableAndThenTopicNotFound itself illustrates), we still call an empty create-topic function. Though it does not incur any round-trip it would still waste some cycles, so I branched it off and hence also simplified some unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-07-31T01:33:39Z","2021-08-10T19:27:45Z"
"","10928","KAFKA-12993: fix memory-mgmt.html","1. remove redundant new line 2. tag closing is wrong  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-06-25T07:21:36Z","2021-06-25T22:55:31Z"
"","11374","MINOR: TopicIdPartition improvements","1. It should not require a TopicPartition during construction and normal usage. 2. Simplify `equals` since `topicId` and `topicPartition` are never null. 3. Inline `Objects.hash` to avoid array allocation. 4. Make `toString` more concise using a similar approach as `TopicPartition` since `TopicIdPartition` will replace `TopicPartition` in many places in the future. 5. Add unit tests for `TopicIdPartition`, it seems like we had none. 6. Minor clean-up in calling/called classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-10-02T14:04:04Z","2021-10-05T18:37:39Z"
"","10684","MINOR: Improve Log layer segment iteration logic and few other areas","1. In `Log.collectAbortedTransactions()` I've restored a previously used logic, such that it would handle the case where the starting segment could be null. This was the case [previously](https://github.com/apache/kafka/blob/8205051e90e3ea16165f8dc1f5c81af744bb1b9a/core/src/main/scala/kafka/log/Log.scala#L1658-L1661), but the PR https://github.com/apache/kafka/pull/10401 accidentally [changed the behavior](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/Log.scala#L1240-L1243) causing the code to assume that the starting segment won't be null. 2. In `Log.rebuildProducerState()` I've removed usage of the `allSegments` local variable. The logic looks a bit simpler after I removed it. 3. I've introduced a new `LogSegments.higherSegments()` API. This is now used to make the logic a bit more readable in `Log. collectAbortedTransactions()` and `Log.deletableSegments()` APIs. 4. I've removed the unnecessary use of `java.lang.Long` in `LogSegments` class' segments map definition. 5. I've converted a few `LogSegments` API from public to private, as they need not be public.  **Tests:** Relying on existing unit tests.","closed","","kowshik","2021-05-13T00:51:52Z","2021-05-27T15:56:01Z"
"","11460","KAFKA-13425:  Optimization of KafkaConsumer#pause semantics","1. Enhance the annotation of KafkaConsumer#pause(...) method 2. Add log output when clearing the paused mark of topicPartitions  Story JIRA: https://issues.apache.org/jira/browse/KAFKA-13425  Author: RivenSun2   Reviewers: Luke Chen","closed","","RivenSun2","2021-11-03T02:51:00Z","2022-01-04T23:47:04Z"
"","11124","KAFKA-12839: Let SlidingWindows aggregation support window size of 0","1. Create closed [start, end] time window: `SessionWindow` for slidingWindow aggregation. So that we can support time window size with value 0, and have correct aggregation results.  2. Make the `TimeWindow` as a time interval window container that holds the start and end time, so that we can support window store with window size of 0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","showuon","2021-07-24T08:45:46Z","2022-02-07T00:06:29Z"
"","10623","MINOR: Clean up some redundant code from ReplicaManager","1. brokerEpoch has been processed in handleStopReplicaRequest, and has no practical effect in ReplicaManager.stopReplicas. 2. responseMap.put(topicPartition, Errors.forException(e)) has a redundant put in stopReplicas.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenbingshen","2021-05-01T18:57:44Z","2021-05-02T16:44:32Z"
"","10726","KAFKA-12478: optimize offset reset strategy and fix lose consumer data when add pa…","1. besides latest and earliest, we also add nearest: reset to either latest or earliest depending on the current offset (i.e. this policy won't trigger under the scenario when we see a partition for the first time, without committed offsets; it will only trigger for out-of-range). 2. latest-on-start, earliest-on-start: reset to either latest or earliest only when we see the partition for the first time without committed offset; when out-of-range default to none, i.e. throw exception. 3. safe-latest: an additional limitTimeStamp limit is used when reset offset. it means we only reset to latest / earliest if its partition's first record timestamp is smaller / larger than the given limitTimeStamp parameter, otherwise, reset to earliest / latest. set the limitTimeStamp value to the consumer group started timestamp, when new partitions are added it would reset to earliest to avoid losing data.","open","","hudeqi","2021-05-19T10:56:18Z","2022-05-22T06:03:15Z"
"","10725","optimize offset reset strategy and fix lose consumer data when add pa…","1. besides latest and earliest, we also add nearest: reset to either latest or earliest depending on the current offset (i.e. this policy won't trigger under the scenario when we see a partition for the first time, without committed offsets; it will only trigger for out-of-range). 2. latest-on-start, earliest-on-start: reset to either latest or earliest only when we see the partition for the first time without committed offset; when out-of-range default to none, i.e. throw exception. 3. an additional limitTimeStamp limit used for latest/earliest/latest-on-start/earliest-on-start: it means we only reset to latest / earliest if its partition's first record timestamp is smaller / larger than the given limitTimeStamp parameter, otherwise, reset to earliest / latest. set the limitTimeStamp value to the consumer group started timestamp, when new partitions are added it would reset to earliest to avoid losing data.","closed","","hudeqi","2021-05-19T10:40:41Z","2021-05-19T10:48:21Z"
"","10716","optimize offset reset strategy and fix lose data when add partition","1. besides `latest` and `earliest`, we also add `nearest`: reset to either latest or earliest depending on the current offset (i.e. this policy won't trigger under the scenario when we see a partition for the first time, without committed offsets; it will only trigger for out-of-range). 2. `latest-on-start`, `earliest-on-start`: reset to either latest or earliest only when we see the partition for the first time without committed offset; when out-of-range default to `none`, i.e. throw exception. 3. an additional `limitTimeStamp` limit used for `latest/earliest/latest-on-start/earliest-on-start`: it means we only reset to latest / earliest if its partition's first record timestamp is smaller / larger than the given `limitTimeStamp` parameter, otherwise, reset to earliest / latest. set the `limitTimeStamp` value to the consumer group started timestamp, when new partitions are added it would reset to `earliest` to avoid losing data.","closed","","hudeqi","2021-05-18T07:38:50Z","2021-05-19T10:51:00Z"
"","11156","KAFKA-13046: add test coverage for AbstractStickyAssignorTest","1. Add tests for `partitionsTransferringOwnership`   a. it should include revoked partitions and partitions claimed by multiple consumers.    b. for non-equal assignment case (general case), it should be null so that the cooperative assignor knows to compute it from scratch    2. small optimization for `allPreviousPartitionsToOwner` check.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-31T01:59:56Z","2021-08-05T20:26:18Z"
"","10848","MINOR Updated transaction index as optional in LogSegmentData.","- Updated transaction index as optional in `LogSegmentData`.  - Added a unit test for the introduced change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-06-09T06:32:06Z","2021-06-15T20:56:22Z"
"","11306","MINOR: Update library dependencies (Q1 2022)","- scala 2.13: 2.13.6 -> 2.13.8   * Support Java 18 and improve Android compatibility   * https://www.scala-lang.org/news/2.13.7   * https://www.scala-lang.org/news/2.13.8 - scala 2.12: 2.12.14 -> 2.12.15.    * Improves compatibility with JDK 17, JDK 18 and Scala 3 (cross-building).   * https://www.scala-lang.org/news/2.12.15 - gradle versions plugin: 0.38.0 -> 0.42.0   * Minor fixes   * https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.40.0   * https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.41.0   * https://github.com/ben-manes/gradle-versions-plugin/releases/tag/v0.42.0 - gradle dependency check plugin: 6.1.6 -> 6.5.3   * Minor fixes - gradle spotbugs plugin: 4.7.1 -> 5.0.5   * Fixes and minor improvements   * There were too many releases to include all the links, include the major version bump   * https://github.com/spotbugs/spotbugs-gradle-plugin/releases/tag/5.0.0 - gradle scoverage plugin: 5.0.0 -> 7.0.0   * Support newer Gradle versions and other improvements   * https://github.com/scoverage/gradle-scoverage/releases/tag/6.0.0   * https://github.com/scoverage/gradle-scoverage/releases/tag/6.1.0   * https://github.com/scoverage/gradle-scoverage/releases/tag/7.0.0 - gradle shadow plugin: 7.0.0 -> 7.1.2   * Support gradle toolchains and security fixes   * https://github.com/johnrengelman/shadow/releases/tag/7.1.0   * https://github.com/johnrengelman/shadow/releases/tag/7.1.1   * https://github.com/johnrengelman/shadow/releases/tag/7.1.2 - bcpkix: 1.66 -> 1.70   * Several improvements and fixes   * https://www.bouncycastle.org/releasenotes.html - jline: 3.12.1 -> 3.21.0   * Various fixes and improvements - jmh: 1.32 -> 1.34   * Compiler blackhole enabled by default when using Java 17 and improved     gradle incremental compilation   * https://mail.openjdk.java.net/pipermail/jmh-dev/2021-August/003355.html   * https://mail.openjdk.java.net/pipermail/jmh-dev/2021-December/003406.html - scalaLogging: 3.9.3 -> 3.9.4   * Support for Scala 3.0 - jose4j: 0.7.8 -> 0.7.9   * Minor fixes - junit: 5.7.1 -> 5.8.2   * Minor improvements and fixes   * https://junit.org/junit5/docs/current/release-notes/index.html#release-notes-5.8.0   * https://junit.org/junit5/docs/current/release-notes/index.html#release-notes-5.8.1   * https://junit.org/junit5/docs/current/release-notes/index.html#release-notes-5.8.2 - jqwik: 1.5.0 -> 1.6.3   * Numerous improvements   * https://github.com/jlink/jqwik/releases/tag/1.6.0 - mavenArtifact: 3.8.1 -> 3.8.4 - mockito: 3.12.4 -> 4.3.1   * Removed deprecated methods, `DoNotMock` annotation and     minor fixes/improvements   * https://github.com/mockito/mockito/releases/tag/v4.0.0   * https://github.com/mockito/mockito/releases/tag/v4.1.0   * https://github.com/mockito/mockito/releases/tag/v4.2.0   * https://github.com/mockito/mockito/releases/tag/v4.3.0 - scalaCollectionCompat: 2.4.4 -> 2.6.0   * Minor fixes   * https://github.com/scala/scala-collection-compat/releases/tag/v2.5.0   * https://github.com/scala/scala-collection-compat/releases/tag/v2.6.0 - scalaJava8Compat: 1.0.0 -> 1.0.2   * Minor changes - scoverage: 1.4.1 -> 1.4.11   * Support for newer Scala versions - slf4j: 1.7.30 -> 1.7.32   * Minor fixes, 1.7.35 automatically uses reload4j and 1.7.33/1.7.34     cause build failures, so we stick with 1.7.32 for now. - zstd: 1.5.0-4 -> 1.5.2-1   * zstd 1.5.2   * Small refinements and performance improvements   * https://github.com/facebook/zstd/releases/tag/v1.5.1   * https://github.com/facebook/zstd/releases/tag/v1.5.2  Checkstyle, spotBugs and spotless will be upgraded separately as they either require non trivial code changes or they have regressions that affect us.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-09-07T13:52:05Z","2022-02-08T00:09:28Z"
"","10647","MINOR Removed copying storage libraries specifically as the task already copies them.","- Removed copying storage libraries specifically as the task already copies them as part of `configurations.runtimeClasspath` in `releaseTarGz`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-05-07T08:26:22Z","2021-05-11T16:15:34Z"
"","10499","MINOR: Use the correct label while inspecting the docker images.","- Get a valid return code from the PIPESTATUS command.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-04-07T07:28:42Z","2021-08-04T07:07:40Z"
"","11393","MINOR: Refactor RequestResponseTest","- Ensure we test all requests and responses - Code cleanups  Following up from KAFKA-13258/13259/13260 (https://github.com/apache/kafka/pull/11288)   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-13T14:34:15Z","2021-12-10T17:54:56Z"
"","10905","MINOR Addressed minor typos in READMEs.","- Corrected a few typos in READMEs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-06-18T17:44:45Z","2021-06-19T15:58:58Z"
"","11394","KAFKA-13284: Use sftp to upload artifacts in release.py","- automatically upload artifacts to Apache Home using sftp - fix the logic to find JDK 17  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-13T16:53:34Z","2021-10-18T08:11:50Z"
"","10733","KAFKA-12816 Added tiered storage related configs including remote log manager configs.","- Added tiered storage related configs including remote log manager configs. - Added local log retention configs to LogConfig. - Added tests for the added configs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-05-20T01:28:23Z","2021-07-13T17:44:47Z"
"","11058","KAFKA-12802 Added a file based cache for consumed remote log metadata for each partition to avoid consuming again incase of broker restarts.","- Added snapshots for consumed remote log metadata for each partition to avoid consuming again in case of broker restarts. These snapshots are stored in the respective topic partition log directories. - Added TopicBasedRemoteLogMetadataManagerRestartTest:   * loads the earlier saved snapshots after restart   * checks the entries are available   * starts the consumer and add more metadata entries   * checks the newly added entries and loaded entries are available   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-07-15T06:01:57Z","2021-10-11T17:24:56Z"
"","11033","KAFKA-12988 Asynchronous API support for RemoteLogMetadataManager add/update methods.","- Added asynchronous API support for RemoeLogMetadataManager add/update/put methods.  - Implemented the changes on default topic based RemoteLogMetadataManager.  - Refactored the respective tests to cover the introduced asynchronous APIs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-07-13T04:28:25Z","2021-09-09T17:06:26Z"
"","10638","KAFKA-12758 Added `server-common` module to have server side common classes.","- Added `server-common` module to have server side common classes.  - Moved `ApiMessageAndVersion`, `RecordSerde`, `AbstractApiMessageSerde`, and `BytesApiMessageSerde` to server-common module.  Existing tests are sufficient as this change is about moving files into a separate module.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","satishd","2021-05-06T06:23:42Z","2021-05-11T16:58:29Z"
"","11196","KAFKA-13185 Clear message batch in rewind","*WorkerSinkTask - clear message batch after rewind* Messages that are left in messageBatch after failed put due to RetriableException should be cleared after rewind caused by yet another RetriableException in flush/pre-commit. Message left in messageBatch will be read again in the next poll iteration.  *Test strategy* Current test suite still works.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","xjrk58","2021-08-10T09:05:04Z","2022-03-17T03:08:20Z"
"","11002","KAFKA-13026: Idempotent producer (KAFKA-10619) follow-up testings","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* We modified the following producer config defaults: acks=all, enable.idempotence=true. This PR is thus to test the non-default config behavior when acks != all or enable.idempotence != false  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* 1. Add unit tests for testing the non-default behaviors, and the restrictions between ack, idempotence, and transaction. 2. Update the verifiable producer system test for testing the non-default overrides.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ctan888","2021-07-08T19:11:31Z","2021-07-26T20:45:59Z"
"","10830","KAFKA-12902: Add unit32 type in generator","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* We already defined `org.apache.kafka.common.protocol.types.Type.UNSIGNED_INT32`, which means we support unit32 in RPC protocol types, so also add unit32 in generator.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Unit test.","closed","","dengziming","2021-06-07T09:59:46Z","2022-05-25T23:25:29Z"
"","10831","KAFKA-12903: Replace producer state entry with auto-generated protocol","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* This is based on #10830  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Existed  unit test","open","","dengziming","2021-06-07T10:57:26Z","2021-06-07T11:03:38Z"
"","10958","MINOR: Add default serde in stream test to fix QA ERROR","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  We change default serde to be `null` in #10813 , but forgot to add some in tests, for example `TestTopicsTest` and `TopologyTestDriverTest`  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-07-02T05:30:12Z","2021-07-02T18:02:21Z"
"","10957","MINOR: Delete unused DeleteTopicsWithIdsResult.java","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  We can remove `DeleteTopicsWithIdsResult` since we use `DeleteTopicsResult`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-07-02T03:55:04Z","2021-07-05T07:26:59Z"
"","10956","MINOR: fix ZkMetadataCache package name","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  The `ZkMetadataCache` package name is not correct.","closed","","dengziming","2021-07-02T03:40:35Z","2021-10-26T21:17:21Z"
"","11481","KAFKA-13117: migrate TupleForwarder and CacheFlushListener to new Record API","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-11-10T04:34:38Z","2021-11-23T19:28:55Z"
"","11444","[WIP] Kafka 13406: fix for v2.8","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-10-28T08:08:31Z","2021-10-28T08:08:31Z"
"","11373","MINOR: Add shebang to gradlewAll","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","akatona84","2021-10-01T07:16:51Z","2021-12-03T14:03:40Z"
"","11357","KAFKA-10639: ConfigProvider for environment variables","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","akatona84","2021-09-24T12:41:13Z","2021-09-28T11:00:40Z"
"","11329","KAFKA-13301;  Optimized the interpretation of the relationship between 'request.timeout. ms' and 'max.poll.interval.ms' in the document.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","yangdaixai","2021-09-16T02:57:28Z","2021-09-19T05:19:02Z"
"","11251","KIP-768 WIP","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-08-23T19:08:13Z","2021-08-23T19:10:02Z"
"","11223","Upgrade zookeeper version to 3.6.3","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edipesh19","2021-08-17T07:14:22Z","2021-08-17T07:14:59Z"
"","11148","[WIP] MINOR: test if breaking build","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-30T03:37:41Z","2021-07-30T06:29:10Z"
"","11072","[WIP] MINOR: stream stuck fix test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","showuon","2021-07-17T02:47:32Z","2021-07-24T02:26:11Z"
"","10979","WIP","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-07-06T17:06:03Z","2021-07-06T18:43:49Z"
"","10894","KAFKA-12951: restore must terminate for tx global topic","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2021-06-17T06:32:55Z","2021-06-28T22:31:35Z"
"","10877","KAFKA-12925: adding presfixScan operation for missed implementations","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vamossagar12","2021-06-13T18:46:26Z","2021-07-15T02:17:27Z"
"","10859","Controller源码分析，对Contorller核心组件做了详细的注解","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Clarencezero","2021-06-10T04:06:47Z","2021-06-10T04:09:00Z"
"","10842","KAFKA-12848: kafka streams jmh benchmarks","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vamossagar12","2021-06-08T15:43:24Z","2021-09-06T15:15:39Z"
"","10799","WIP","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dajac","2021-05-31T16:08:22Z","2021-05-31T16:08:22Z"
"","10798","KAFKA-9168: Adding direct byte buffer support to rocksdb state store","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vamossagar12","2021-05-31T14:43:17Z","2022-02-11T01:48:53Z"
"","10773","[WIP] MINOR: Working in progress","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","showuon","2021-05-27T02:52:42Z","2021-07-19T10:02:59Z"
"","10734","Feature/kafka 12380","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ViE-17-tuoi","2021-05-20T02:48:23Z","2021-05-20T03:01:12Z"
"","10721","Feature/kafka 12380","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ViE-17-tuoi","2021-05-19T02:18:14Z","2021-05-19T02:23:36Z"
"","10692","Feature/kafka 12380","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ViE-17-tuoi","2021-05-14T02:28:21Z","2021-05-14T02:32:33Z"
"","10687","KAFKA-12380 Executor in Connect's Worker is not shut down when the worker is","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ViE-17-tuoi","2021-05-13T09:29:55Z","2021-05-13T15:23:31Z"
"","10685","KAFKA-12380 Executor in Connect's Worker is not shut down when the worker is","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","DuongPTIT","2021-05-13T03:05:40Z","2021-05-19T03:13:10Z"
"","10554","KAFKA-12506: Expand AdjustStreamThreadCountTest by writing some data to Kafka topics","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kebab-mai-haddi","2021-04-17T18:35:48Z","2021-04-27T17:19:12Z"
"","10576","KAFKA-12701: Remove topicId from MetadataReq since it was not supported in 2.8.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  #9622 add topicId in MetadataResponse and MetadataRequest, but describe topic using topicId is supported by #9769   sadly #9622 PR was merged but #9769 PR was held off, for more details: https://github.com/apache/kafka/pull/9769#issuecomment-772830472 I think this is a bug so we should fix this for 2.8.0.  This PR should be cherry-picked to 2.8.0.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-21T09:44:28Z","2021-04-23T01:40:55Z"
"","11169","KAFKA-10859: Add test annotation to FileStreamSourceTaskTest.testInvalidFile and speed up the test","*More detailed description of your change, Added the missing @Test annotation to a test in FileStreamSourceTaskTest. The test used to loop 100 times, each time blocking for 1 second. Checking the assertion more than once is unnecessary for this test.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","connect,","Borzoo","2021-08-03T20:37:36Z","2022-04-20T00:29:07Z"
"","11398","[Code optimization] Missing override annotation","*More detailed description of your change, A large number of ‘@Override’ tags are missing, resulting in a lot of warnings in IDEA, and inconvenience to code reading","closed","","lordcheng10","2021-10-14T15:07:54Z","2021-10-14T15:11:58Z"
"","10781","MINOR: Reduce duplicate authentication check","*More detailed description of your change,  Reduce repeat authentication check for same broker.  *Summary of testing strategy (including rationale)  The requests belong to same broker, it's unnecessary to check every request , although the check is cheap.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","loyispa","2021-05-28T08:54:42Z","2021-12-08T02:26:28Z"
"","11209","KAFKA-12465: Logic about inconsistent cluster id","*More detailed description of your change* When handling a response, we treat  INCONSISTENT_CLUSTER_ID as a fatal error unless a previous response contained a valid cluster id, this solution can catch misconfiguration as early as possible and also avoid cases when a misconfigured node kills a stable cluster. However, the node will continue executing with the misconfiguration in some edge cases, for example, 3 nodes with 3 different cluster ids.    *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-08-13T02:35:34Z","2022-02-07T13:48:08Z"
"","11219","MINOR: clarify assertion in handleListPartitionReassignmentsRequest","*More detailed description of your change* We already support list partition reassignments in KRAFT mode, so change `notYetSupported` to `shouldAlwaysForward`  *Summary of testing strategy (including rationale)* Unit test","closed","","dengziming","2021-08-16T13:00:57Z","2021-08-16T18:59:09Z"
"","10523","MINOR: Optimeze side effect brought by EpochState.canGrantVote","*More detailed description of your change* This fixes the discussion https://github.com/apache/kafka/pull/10393#discussion_r608289621  *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-04-11T04:07:59Z","2021-04-12T08:10:08Z"
"","11446","MINOR: Delete redudant tmp file","*More detailed description of your change* This file must be added by accident.","closed","","dengziming","2021-10-28T12:48:01Z","2021-12-06T02:49:31Z"
"","11198","MINOR: Use compressionType parameter in SnapshotWriter","*More detailed description of your change* The compression type of metadata topic is `CompressionType.NONE`, but we still need to get it from only parameter, see code for details.","closed","","dengziming","2021-08-10T10:44:51Z","2021-12-01T05:47:56Z"
"","10667","KAFKA-12772: Move all transaction state transition rules into their states","*More detailed description of your change* Similar to KAFKA-5258 which move all partition and replica state transition rules into their states, we move the transaction state transition rules into their states, this is a good develop habit.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-05-11T06:57:46Z","2021-05-12T01:14:50Z"
"","11157","MINOR: Replace EasyMock with Mockito in test-utils module","*More detailed description of your change* Replace EasyMock with Mockito in streams:test-utils","closed","tests,","dengziming","2021-07-31T06:17:42Z","2021-08-04T00:27:04Z"
"","11471","MINOR: Replace EasyMock with Mockito in connect:file","*More detailed description of your change* Replace EasyMock with Mockito in connect:file  *Summary of testing strategy (including rationale)* QA","closed","connect,","dengziming","2021-11-05T14:12:10Z","2022-03-17T16:30:38Z"
"","11321","MINOR: Replace EasyMock with Mockito in connect:basic-auth-extension","*More detailed description of your change* Replace EasyMock with Mockito in connect:basic-auth-extension","closed","","dengziming","2021-09-11T06:30:45Z","2021-09-23T12:08:07Z"
"","10500","MINOR: Move envelop body request serialize code to RequestContext","*More detailed description of your change* Move envelop request body serialize code to `RequestContext `, see https://github.com/apache/kafka/pull/10142#discussion_r606613914  *Summary of testing strategy (including rationale)* Unit test for the new method and `request.serializeWithHeader`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-04-07T08:48:05Z","2022-01-25T05:15:38Z"
"","10909","KAFKA-12158: Better return type of RaftClient.scheduleAppend","*More detailed description of your change* It's not very convenient to return Long for `RaftClient.scheduleAppend`, in this PR we add a `RaftAppendResult` class to replace it. I also found that we lack logic on append failure, for example, the `ControllerWriteEvent` won't check the result of `raftClient.scheduleAppend`, this should be fixed in a future PR.  *Summary of testing strategy (including rationale)* QA","closed","kip-500,","dengziming","2021-06-20T08:15:23Z","2021-08-02T21:47:03Z"
"","10701","KAFKA-10437: Update WordCount examples to use new PAPI","*More detailed description of your change* It seems that #9396 leaves out a TODO, just fix it.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dengziming","2021-05-15T03:51:22Z","2021-06-17T05:24:11Z"
"","11339","MINOR: ClientQuotaRecord is not handled in KRaft MetadataShell","*More detailed description of your change* In KRaft mode, I updated client quota and inspected using MetadataShell: ``` ConfigCommand --entity-type users --entity-name dengziming --bootstrap-server localhost:9092 --alter --add-config producer_byte_rate=1000  [ Kafka Metadata Shell ] >> ls /configs/user/ ls: /configs/user/: no such file or directory. ```  After this change, the config is rightly presented using MetadataShell: ``` cat /configs/user/dengziming/producer_byte_rate 1000.0 ```  Also add unit test for `MetadataNodeManager`","closed","","dengziming","2021-09-18T06:16:28Z","2021-10-26T20:56:27Z"
"","10789","KAFKA-6084: Propagate JSON parsing errors in ReassignPartitionsCommand","*More detailed description of your change* I'm verifying reassigning partitions in KRaft mode, and I accidentally find that #4090 only propagate errors when parsing reassignment json, we'd better propagate errors when parsing other json.  *Summary of testing strategy (including rationale)* unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dengziming","2021-05-29T03:13:03Z","2021-05-29T03:13:41Z"
"","10488","MINOR: Remove some unnecessary cyclomatic complexity suppressions","*More detailed description of your change* I find these classes don't violate cyclomatic complexity rule.  *Summary of testing strategy (including rationale)* QA  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-06T12:33:17Z","2021-04-12T09:49:40Z"
"","11274","KAFKA-13242: Handle UpdateFeatureRequest in kraft","*More detailed description of your change* Handle UpdateFeatureRequest in KRaft mode  *Summary of testing strategy (including rationale)* TODO","closed","","dengziming","2021-08-27T14:15:36Z","2021-11-22T11:21:33Z"
"","10852","MINOR: Replace easymock with mockito in log4j-appender","*More detailed description of your change* As the title  *Summary of testing strategy (including rationale)* QA","closed","","dengziming","2021-06-09T11:13:42Z","2022-04-28T03:17:19Z"
"","10832","MINOR: Use bootstrap-server instead of broker-list in doc","*More detailed description of your change* As the title  *Summary of testing strategy (including rationale)* QA","closed","","dengziming","2021-06-07T11:48:00Z","2022-02-14T12:24:21Z"
"","10510","KAFKA-12607: Test case for resigned state vote granting","*More detailed description of your change* As discussed in the Jira,  `ResignedState` will transition to `VotedState` and grant vote, just add some unit tests to verify this transition.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-09T06:30:42Z","2021-04-10T02:15:12Z"
"","10793","KAFKA-12338: Remove useless MetadataParser","*More detailed description of your change* `MetadataParser` is a duplication of `MetadataRecordSerde` and it's not used in any code, so we can remove it.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* QA","closed","","dengziming","2021-05-30T00:58:42Z","2021-06-07T17:52:16Z"
"","10618","MINOR: Fix wrong import control declaration","*More detailed description of your change* `IntegrationTestHelper` is not right.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-30T03:55:49Z","2021-04-30T06:51:48Z"
"","10512","KAFKA-12637: Remove deprecated PartitionAssignor interface","*More detailed description of your change* 1. Remove PartitionAssignor and related classes, update docs 2. Move unit test 3. fix some related typo  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-09T13:45:31Z","2021-04-13T01:37:13Z"
"","10535","MINOR: Remove duplicate method in test classes","*More detailed description of your change* 1. Remove duplicate serializing auto-generated data in `RequestConvertToJsonTest`, this is inspired by #9964 2. Remove `RequestTestUtils.serializeRequestWithHeader` since we added a `AbstractRequest.serializeWithHeader` in #10142  *Summary of testing strategy (including rationale)* Unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-14T05:08:16Z","2021-05-03T04:41:03Z"
"","10560","KAFKA-12660: Do not update offset commit sensor after append failure","*More detailed description of your change* 1. Do not update the commit-sensor if the commit failed and add test logic. 2. Also add 2 unit tests, the first for `OFFSET_METADATA_TOO_LARGE` error, the second is to cover circumstance when one offset is committed and the other is failed with `OFFSET_METADATA_TOO_LARGE`, these 2 are not covered currently.  *Summary of testing strategy (including rationale)* unit test  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-19T08:59:39Z","2021-07-08T17:13:24Z"
"","10748","MINOR: Add missing apiversion test for 3.0","*More detailed description of your change* #10504 bump apiversion to 3.0.0, we'd better also add test for it.  *Summary of testing strategy (including rationale)* Unit test","closed","","dengziming","2021-05-24T03:05:03Z","2021-06-28T08:36:16Z"
"","10598","MINOR: rename wrong topic id variable name and description","*More detailed description of your change* ![image](https://user-images.githubusercontent.com/26023240/116053055-d569d280-a6ac-11eb-84d1-3b6d8760214e.png) 1. The argument description ""The topic id to produce messages to"" is not right, currently we do not support topicId when producing messages, maybe 3.0.0. 2. also rename deprecaed `Class.newInstance` to `class.getDeclaredConstructor().newInstance()`  *Summary of testing strategy (including rationale)* Test locally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-26T08:36:21Z","2021-04-29T03:40:24Z"
"","10592","MINOR: Remove redudant test files and close LogSegment after test","*More detailed description of your change* ![image](https://user-images.githubusercontent.com/26023240/115959686-01b41080-a540-11eb-88b6-a7c668ddfa38.png)  1. `LogSegmentsTest` will create some redundant files, we should remove them after the test. 2. We also need to close LogSegment after the test.  *Summary of testing strategy (including rationale)* Test locally, will not generate any redundant files.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dengziming","2021-04-24T12:59:55Z","2021-04-27T09:53:47Z"
"","11261","KAFKA-13228: ApiVersionRequest is not properly handled in KRaft co-resident mode","*More detailed description of your change*  When I described quorum in Kraft mode I got `org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support DESCRIBE_QUORUM`. This happens because we only concerns `ApiKeys.zkBrokerApis()` when we call `NodeApiVersions.create()`, we should use `ApiKeys.controllerApiVersions` when in Kraft mode.  *Summary of testing strategy (including rationale)*  After this change, the DESCRIBE_QUORUM request was property handled and got a correct response: ``` TopicData(topicName='__cluster_metadata', partitions=[PartitionData(partitionIndex=0, errorCode=0, leaderId=1, leaderEpoch=30, highWatermark=141, currentVoters=[ReplicaState(replicaId=1, logEndOffset=141)], observers=[])]) ```","closed","","dengziming","2021-08-26T02:10:27Z","2022-06-13T15:21:01Z"
"","11173","KAFKA-13509: Support max timestamp in GetOffsetShell","*More detailed description of your change*  Support max timestamp in GetOffsetShell which we added in KIP-734.  *Summary of testing strategy (including rationale)* test locally:  bin/kafka-run-class.sh kafka.tools.GetOffsetShell --bootstrap-server localhost:9092 --topic topic1 --time -1 topic1:0:10000  bin/kafka-run-class.sh kafka.tools.GetOffsetShell --bootstrap-server localhost:9092 --topic topic1 --time -2 topic1:0:0  bin/kafka-run-class.sh kafka.tools.GetOffsetShell --bootstrap-server localhost:9092 --topic topic1 --time -3 topic1:0:9979","closed","","dengziming","2021-08-04T03:31:53Z","2022-03-23T05:47:38Z"
"","11291","MINOR: Fix broker registration log problem","*More detailed description of your change*  I found some error that says ""Re-registered broker"" but I didn't see ""Registered new broker"": ``` [2021-09-02 09:47:02,532] INFO [Controller 3000] Re-registered broker incarnation: RegisterBrokerRecord(brokerId=0, incarnationId=ScFuLmWTQ3WUF3P3XCkWEw, brokerEpoch=0, endPoints=[BrokerEndpoint(name='EXTERNAL', host='localhost', port=56409, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager:260) ``` After reviewing the code, I found we are doomed to get a not-null previous value if we put a value and get a value.  The log became normal after applying this change: ``` [2021-09-02 09:51:02,191] INFO [Controller 3000] Registered new broker: RegisterBrokerRecord(brokerId=0, incarnationId=Z398mgq4Q36J5DYoIrQW0g, brokerEpoch=0, endPoints=[BrokerEndpoint(name='EXTERNAL', host='localhost', port=57231, securityProtocol=0)], features=[], rack=null, fenced=true) (org.apache.kafka.controller.ClusterControlManager:259) ```","closed","","dengziming","2021-09-02T02:12:35Z","2021-09-25T00:55:08Z"
"","10580","KAFKA-12704: Improve cache access during connector class instantiation in config validations","*Concurrent requests to validate endpoint for the same connector type calls AbstractHerder::getConnector to get the cached connector instances  and if the connector hasn't been cached yet then there is a race condition in the AbstractHerder::getConnector method that potentially fails to detect that an instance of the connector has already been created and, as a result, can create another instance*  *Existing tests are present with enough coverage so no new tests are added.*  @C0urante  @rhauch  Could you please review if this fix is acceptable?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kpatelatwork","2021-04-21T18:23:46Z","2021-04-22T05:12:05Z"
"","10519","KAFKA-12344 Support SlidingWindows in the Scala API","*Added windowedBy(SlidingWindows) method to  CogroupedKStream and KGroupedStream scala wrappers also added test for KGroupedStream windowedBy(SlidingWindows) method in KTaableTests*   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ketulgupta1995","2021-04-10T19:13:13Z","2021-04-26T18:39:18Z"
"","10883","KAFKA-12949: TestRaftServer's scala.MatchError: null on test-kraft-server-start.sh","**Jira:** https://issues.apache.org/jira/browse/KAFKA-12949 Encounter the following exception when trying to run the TestRaftServer:  `bin/test-kraft-server-start.sh --config config/kraft.properties` ``` [2021-06-14 17:15:43,232] ERROR [raft-workload-generator]: Error due to (kafka.tools.TestRaftServer$RaftWorkloadGenerator)  scala.MatchError: null  at kafka.tools.TestRaftServer$RaftWorkloadGenerator.doWork(TestRaftServer.scala:220)  at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:96)  [2021-06-14 17:15:43,253] INFO [raft-workload-generator]: Stopped (kafka.tools.TestRaftServer$RaftWorkloadGenerator) ```  Caused because of a not contemplated *null* on eventQueue.poll:  ``` eventQueue.poll(eventTimeoutMs, TimeUnit.MILLISECONDS) match {     case HandleClaim(epoch) =>       claimedEpoch = Some(epoch)       throttler.reset()       pendingAppends.clear()       recordCount.set(0)      case HandleResign =>       claimedEpoch = None       pendingAppends.clear()      case HandleCommit(reader) =>       try {         while (reader.hasNext) {           val batch = reader.next()           claimedEpoch.foreach { leaderEpoch =>             handleLeaderCommit(leaderEpoch, batch)           }         }       } finally {         reader.close()       }      case HandleSnapshot(reader) =>       // Ignore snapshots; only interested in records appended by this leader       reader.close()      case Shutdown => // Ignore shutdown command   } ``` This makes raft-workload-generator's thread to stop.  Proposal:  Add a catch case on the match statement.   `case _ => // Ignore other events (such as null)`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","IgnacioAcunaF","2021-06-15T00:27:00Z","2021-06-22T19:15:33Z"
"","10858","KAFKA-12926: ConsumerGroupCommand's java.lang.NullPointerException at negative offsets while running kafka-consumer-groups.sh","**Jira**: https://issues.apache.org/jira/browse/KAFKA-12926  Instead of setting ""null"" to negative offsets partition (as in KAFKA-9507), this PR aims to skip those cases from the returned list, because setting them in ""null"" can cause java.lang.NullPointerExceptions on downstreams methods that tries to access the data on them, because they are expecting an _OffsetAndMetadata_ and they encouter null values.  For example, at ConsumerGroupCommand.scala at core: ```   val partitionOffsets = consumerSummary.assignment.topicPartitions.asScala             .map { topicPartition =>               topicPartition -> committedOffsets.get(topicPartition).map(_.offset)             }.toMap ``` If topicPartition has an negative offset, the committedOffsets.get(topicPartition) is null (as defined on KAFKA-9507), which translates into null.map(_.offset), which will lead to: _Error: Executing consumer group command failed due to null java.lang.NullPointerException_  Example: `./kafka-consumer-groups.sh --describe --group order-validations`  `Error: Executing consumer group command failed due to null java.lang.NullPointerException  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$6(ConsumerGroupCommand.scala:579)  at scala.collection.StrictOptimizedIterableOps.map(StrictOptimizedIterableOps.scala:99)  at scala.collection.StrictOptimizedIterableOps.map$(StrictOptimizedIterableOps.scala:86)  at scala.collection.convert.JavaCollectionWrappers$JSetWrapper.map(JavaCollectionWrappers.scala:180)  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$5(ConsumerGroupCommand.scala:578)  at scala.collection.immutable.List.flatMap(List.scala:293)  at scala.collection.immutable.List.flatMap(List.scala:79)  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$collectGroupsOffsets$2(ConsumerGroupCommand.scala:574)  at scala.collection.Iterator$$anon$9.next(Iterator.scala:575)  at scala.collection.mutable.Growable.addAll(Growable.scala:62)  at scala.collection.mutable.Growable.addAll$(Growable.scala:59)  at scala.collection.mutable.HashMap.addAll(HashMap.scala:117)  at scala.collection.mutable.HashMap$.from(HashMap.scala:570)  at scala.collection.mutable.HashMap$.from(HashMap.scala:563)  at scala.collection.MapOps$WithFilter.map(Map.scala:358)  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.collectGroupsOffsets(ConsumerGroupCommand.scala:569)  at kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeGroups(ConsumerGroupCommand.scala:369)  at kafka.admin.ConsumerGroupCommand$.run(ConsumerGroupCommand.scala:76)  at kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:63)  at kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)`  Unit tests added to assert that topics's partitions with an INVALID_OFFSET are not considered on the returned list of the consmer groups's offsets, so the downstream methods receive only valid _OffsetAndMetadata_ information.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","IgnacioAcunaF","2021-06-10T01:15:09Z","2021-06-29T12:46:18Z"
"","11204","KAFKA-13188 - Release the memory back into MemoryPool","**Forward porting** - https://github.com/linkedin/kafka/pull/186  **TICKET** = [KAFKA-13188](https://issues.apache.org/jira/browse/KAFKA-13188) **LI_DESCRIPTION** = The current KafkaConsumer code includes support for allocating buffers from MemoryPool when allocating bytes for requests to Brokers. However, the code doesn't release them back to the pool and hence, rendering pooling moot. Currently, it works because it uses MemoryPool.NONE which just mallocs buffer every time a buffer is requested. However, this won't work if a different memory pool implementation is used.  The consumer can't just release the pool back into the memory because the fetch requests keep on holding on to the references to the pool in CompletedFetch objects which live across multiple poll calls. The idea here is to use ref counting based approach, where the ClientResponse increments ref count every time a CompletedFetch object is created and decrement when the fetch is drained after a poll calls returning records.  For the rest of the things such as metadata, offset commit, list offsets it is somewhat easier as the client is done with the response bytes after response future callback is completed.  This PR also adds the ClientResponseWithFinalize class for debugging purposes as well protected by linkedin.enable.client.resonse.leakcheck flag. The class uses finalizer to check whether there is some issue in code due to which the buffer wasn't released back to the pool yet. However, during in an actual production setting, the finalizers are costly and hence, not enabled by default.  **EXIT_CRITERIA** = If and when the upstream ticket is merged, and the changes are pulled in  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","aloknnikhil","2021-08-12T05:33:12Z","2022-02-09T01:38:49Z"
"","10961","KAFKA-13028: Enable the AbstractConfig to resolve variables in ConfigProviders","**DO NOT MERGE UNTIL IT IS DETERMINED WHETHER THIS REQUIRED A KIP.**  See [KAFKA-13028](https://issues.apache.org/jira/browse/KAFKA-13028)  The AbstractConfig constructor is changed to instantiate and configure each ConfigProvider instance in the same order as defined by the 'config.providers' property. And when configuring a ConfigProvider, it uses any previously-configured ConfigProvider instances to potentially resolve any variables in that ConfigProvider's configuration.  New unit tests are added to verify this change in behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rhauch","2021-07-02T18:19:42Z","2021-07-02T18:19:42Z"
"","11447","KAFKA-13024","**Disclaimer**: This patch is WIP. Its purpose is to demonstrate a chosen approach and get feedback from committers.a   ### Summary   This patch solves [KAFKA-13024](https://issues.apache.org/jira/browse/KAFKA-13024) and[ KAFKA-13183](https://issues.apache.org/jira/browse/KAFKA-13183) by introducing a concept of `decorator nodes` in the topology. The idea behind decorator nodes is similar to a decorator pattern - to extract common added behaviour from different entities.   Specifically, this patch introduces 2 decorator nodes - `DropNullKeyNode` and `DropNullKeyValueNode`. These nodes have a single method `decorate` that accepts a `ProcessorSupplier` and returns another `ProcessorSupplier` that filters `null` keys/key-values respectively. Besides fixing 2 issues mentioned above (dropping `null` keys on repartition, missing metrics), there are 2 advantages to this approach: * It reduces code duplication around `if (record.key() == null...`.  * As @mjsax suggested in [this comment](https://issues.apache.org/jira/browse/KAFKA-13024?focusedCommentId=17427868&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17427868), It optimizes away duplicated null-key/value processing from the topology.  ### Implementation details  The decorators are set by nodes according to a new parameter in processor parameters:  https://github.com/apache/kafka/blob/b4c3adc3ce3f40ad96400b94379fa206d782922f/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/ProcessorParameters.java#L45-L52  Currently ± working example is `StatefulProcessorNode` where the original processor supplier is decorated:   https://github.com/apache/kafka/blob/b4c3adc3ce3f40ad96400b94379fa206d782922f/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/StatefulProcessorNode.java#L85-L92  Decorators are added to the topology together with the decorating node:  https://github.com/apache/kafka/blob/b4c3adc3ce3f40ad96400b94379fa206d782922f/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java#L229-L240  Then, `InternalStreamsBuilder#buildAndOptimizeTopology` removes duplicating downstream decorators and attaches the remaining ones to the decorated nodes. The decorator nodes are not present in the final topology.  Added tests in `InternalStreamBuilderTest` and `KStreamRepartitionIntegrationTest.java` describe how the decorator operations are done.   ---  The following description reflects the first take at this issue and is left here only for completeness.  ~Right now, the `repartition` operator always filters out `null` keys. This behavior is not correct and is a regression compared to the deprecated `through` operator.~  ~This patch fixes the issue by filtering `null` keys only for optimizable repartition nodes. First, it removes unnecessary processor node from the topology for `UnoptimizableRepartitionNode`. Second, it only adds the filtering processor only if the node is optimizable. I introduced an `isOptimizable` boolean method for repartition nodes to make this property more obvious in the code.~  ~To verify that change, I added a new test to the repartition integration suite that produces pairs with `null` keys and expects them to be present after the repartition.~  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","Gerrrr","2021-10-28T16:28:05Z","2022-02-07T09:49:36Z"
"","11250","Kafka 12766 - Disabling WAL-related Options in RocksDB","**description** Streams disables the write-ahead log (WAL) provided by RocksDB since it replicates the data in changelog topics. Hence, it does not make much sense to set WAL-related configs for RocksDB.   **Proposed solution** Ignore any WAL-related configuration and state in the log that we are ignoring them.    **Implementation details** See below In addition - I made very small refactoring - class RocksDBGenericOptionsToDbOptionsColumnFamilyOptionsAdapter is now package private, as this class is internal to Streams    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","TomerWizman","2021-08-23T19:03:37Z","2021-09-08T11:57:08Z"
"","10797","KAFKA-12867: Fix ConsumeBenchWorker exit behavior for maxMessages config","**Bug:** The trogdor `ConsumeBenchWorker` has a bug. It allows several consumption tasks to be run in parallel, the number is configurable using the `threadsPerWorker` config. If one of the consumption tasks completes executing successfully due to `maxMessages` being consumed, then, the consumption task prematurely notifies the `doneFuture` causing the entire  `ConsumeBenchWorker` to halt. This becomes a problem when more than 1 consumption task is running in parallel, because the successful completion of 1 of the tasks shuts down the entire worker while the other tasks are still running. When the worker is shut down, it kills all the active consumption tasks, though they have not consumed `maxMessages` yet. This is not the desired behavior.  **How to reproduce?:** The bug is easily reproducible by running a `ConsumeBenchSpec` task configured with `maxMessages` value and `threadsPerWorker` > 1. When a trogdor workload is started with such a spec, and when one of the threads (i.e. consumption task) has consumed `maxMessages`, you can notice that it prematurely shuts down the worker although the other threads have **not** yet consumed at least `maxMessages`.  **Fix:** The fix is to defer the notification of the `doneFuture` to the `CloseStatusUpdater` thread. This thread is already responsible for tracking the status of the tasks and updating their status when all of the tasks complete, so this seems like the right place to inject the `doneFuture` notification.","closed","","kowshik","2021-05-31T07:56:44Z","2021-06-03T07:30:21Z"
"","10655","MINOR: redundant dependency (scalatest) is removed out of libraries list","**_Notes_**: - related PR: #9858 - related commit/diff: https://github.com/apache/kafka/commit/9af81955c497b31b211b1e21d8323c875518df39#diff-02b139e755ab0e27e0b0a6f9845843ef189116955cc340c49a4022587dbb2b52L110  FYI @ijuma","closed","","dejan2609","2021-05-08T19:50:22Z","2021-05-10T02:22:29Z"
"","10606","KAFKA-12728: version upgrades: gradle (6.8.3 -->> 7.0.2) and gradle shadow plugin (6.1.0 -->> 7.0.0)","**_JIRA ticket:_** https://issues.apache.org/jira/browse/KAFKA-12728  **_Related pull requests:_**  - #10203  - #10466   Note: we will wait for a Gradle 7.0.1 patch ◀️","closed","","dejan2609","2021-04-28T19:16:14Z","2021-06-07T21:10:14Z"
"","11174","KAFKA-9747: Creating connect reconfiguration URL safely","* URL wasn't urlencoded when forwarded reconfiguration to leader connect worker * handling previously swallowed errors in connect RestClient  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","akatona84","2021-08-04T12:29:05Z","2021-10-04T16:17:14Z"
"","10555","MINOR: adjust HashMap init capacity to avoid unnecessary resizing","* The initial capacities for some `HashMap` objects are less than expected and result in resizing. * Increase the initial capacity, similar to what `java.util.HashSet` does.","open","","IUSR","2021-04-18T09:41:45Z","2021-04-18T09:41:45Z"
"","10602","KAFKA-12724: Add 2.8.0 to system tests and streams upgrade tests.","* Ran `./gradlew streams:testAll` to verify the streams upgrade test. * Ran individual system test in my local setup.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2021-04-28T13:46:49Z","2021-08-09T06:30:07Z"
"","10744","KAFKA-8410: KTableProcessor migration groundwork","* Lay the groundwork for migrating KTable Processors to the new PAPI. * Migrate the KTableFilter processor to prove that the groundwork works.  This is an effort to help break up https://github.com/apache/kafka/pull/10507 into multiple PRs.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-05-21T21:04:51Z","2021-05-28T19:59:54Z"
"","11327","KAFKA-13305: fix NullPointerException in LogCleanerManager ""uncleanable-bytes"" gauge","* Fix KAFKA-13305: NullPointerException in LogCleanerManager ""uncleanable-bytes"" gauge - Add a periodic task to remove deleted partitions from uncleanablePartitions   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vincent81jiang","2021-09-15T18:56:44Z","2021-09-27T16:49:21Z"
"","10869","KAFKA-10546: Deprecate old PAPI","* Deprecate the old Processor API * Suppress warnings on all internal usages of the old API   (which will be migrated in other child tickets of KAFKA-8410) * Add new KStream#process methods, since KAFKA-10603 has not seen any action.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2021-06-11T19:59:07Z","2021-06-22T14:17:17Z"
"","10621","MINOR: Improvements and fixes for Trogdor payload generators.","* Changes the new Throughput Generators to track messages per window instead of making per-second calculations which can have rounding errors. Also, one of these had a calculation error which prompted this change in the first place. * Fixes a couple typos. * Fixes an error where certain JSON fields were not exposed, causing the workloads to not behave as intended. * Fixes a bug where we use `wait` not in a loop, which exits too quickly. * Adds additional constant payload generators. * Fixes problems with an example spec. * Fixes several off-by-one comparisons.  This was built and tested extensively prior to the moving of Trogdor from `tools/` to `trogdor/`. Current update builds but fails in unrelated tests.","closed","","scott-hendricks","2021-04-30T19:38:40Z","2021-05-07T00:46:36Z"
"","11465","OAuth updates 1","* Adds defaults to `KafkaConfig` so that the broker doesn't immediately shut down if configs not explicitly provided 😱  * Adds SSL client options to `OAuthCompatibilityTool` and refactoring therefore  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kirktrue","2021-11-03T22:47:36Z","2021-11-10T23:47:36Z"
"","11166","KAFKA-13159 Enable additional transaction system tests in KRaft","* [x] group_mode_transactions_test.py  * [x] produce_bench_test.py  * [x] replication_test.py * [x] streams_smoke_test.py","closed","","mumrah","2021-08-03T18:26:10Z","2021-08-04T18:54:10Z"
"","10680","Rework on KAFKA-3968: fsync the parent directory of a segment file when the file is created","(reverted https://github.com/apache/kafka/pull/10405). https://github.com/apache/kafka/pull/10405 has several issues, for example: - It fails to create a topic with 9000 partitions. - It flushes in several unnecessary places. - If multiple segments of the same partition are flushed at roughly the same time, we may end up doing multiple unnecessary flushes: the logic of handling the flush in LogSegments.scala is weird.  Kafka does not call fsync() on the directory when a new log segment is created and flushed to disk.  The problem is that following sequence of calls doesn't guarantee file durability:  fd = open(""log"", O_RDWR | O_CREATE); // suppose open creates ""log"" write(fd); fsync(fd);  If the system crashes after fsync() but before the parent directory has been flushed to disk, the log file can disappear.  This PR is to flush the directory when flush() is called for the first time.  Did performance test which shows this PR has a minimal performance impact on Kafka clusters.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ccding","2021-05-12T20:00:45Z","2021-05-14T16:25:25Z"
"","10844","MINOR: rename parameter 'startOffset' in Log.read to 'fetchStartOffset'","'fetchStartOffset' is easier to read than 'startOffset' in Log.scala since 'startOffset' is easy to be confused with logStartOffset.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vincent81jiang","2021-06-08T17:46:26Z","2021-06-08T17:46:26Z"
"","11294","KAFKA-13266; `InitialFetchState` should be created after partition is removed from the fetchers"," `ReplicationTest.test_replication_with_broker_failure` in KRaft mode sometimes fails with the following error in the log:  ``` [2021-08-31 11:31:25,092] ERROR [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Unexpected error occurred while processing data for partition __consumer_offsets-1 at offset 31727 (kafka.server.ReplicaFetcherThread)java.lang.IllegalStateException: Offset mismatch for partition __consumer_offsets-1: fetched offset = 31727, log end offset = 31728. at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:194) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$8(AbstractFetcherThread.scala:545) at scala.Option.foreach(Option.scala:437) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7(AbstractFetcherThread.scala:533) at kafka.server.AbstractFetcherThread.$anonfun$processFetchRequest$7$adapted(AbstractFetcherThread.scala:532) at kafka.utils.Implicits$MapExtensionMethods$.$anonfun$forKeyValue$1(Implicits.scala:62) at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry(JavaCollectionWrappers.scala:359) at scala.collection.convert.JavaCollectionWrappers$JMapWrapperLike.foreachEntry$(JavaCollectionWrappers.scala:355) at scala.collection.convert.JavaCollectionWrappers$AbstractJMapWrapper.foreachEntry(JavaCollectionWrappers.scala:309) at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:532) at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:216) at kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:215) at scala.Option.foreach(Option.scala:437) at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:215) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:197) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:99)[2021-08-31 11:31:25,093] WARN [ReplicaFetcher replicaId=1, leaderId=2, fetcherId=0] Partition __consumer_offsets-1 marked as failed (kafka.server.ReplicaFetcherThread) ```  The issue is due to a race condition in `ReplicaManager#applyLocalFollowersDelta`. The `InitialFetchState` is created and populated before the partition is removed from the fetcher threads. This means that the fetch offset of the `InitialFetchState` could be outdated when the fetcher threads are re-started because the fetcher threads could have incremented the log end offset in between.  The patch fixes the issue by removing the partitions from the replica fetcher threads before creating the `InitialFetchState` for them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-03T12:27:50Z","2021-09-08T16:36:39Z"
"","11255","KAFKA-13225: Controller skips sending UpdateMetadataRequest when no change in partition state.","### What The controller can skip sending `updateMetadataRequest` during the broker failure callback if there are offline partitions and the deleted brokers don't host any partitions. Looking at the logic, I'm not sure why the if check is checking for partitionsWithOfflineLeader. This seems like a bug which may also result in sending additional `updateMetadataRequests` on broker shutdowns.  ### Testing Added an integration test for the failure scenario that fails without the code change. The controller integration test suite passes locally with my change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2021-08-24T16:11:49Z","2021-09-02T20:44:12Z"
"","11127","KAFKA-13134: Give up group metadata lock before sending heartbeat response","### What Small locking improvement to drop the group metadata lock before invoking the response callback.  ### Testing Existing GroupCoordinator tests pass.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2021-07-25T14:17:14Z","2021-08-24T10:07:12Z"
"","11244","KAFKA-12933: Flaky test ReassignPartitionsIntegrationTest.testReassignmentWithAlterIsrDisabled","### What Removes assertion added in #10471  It's unsafe to assert that there are partition movements ongoing for some of the tests in the suite because partitions in some of the tests have 0 data, which may complete reassignment before `verify` can run.  ### Testing Tests pass locally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","splett2","2021-08-20T18:53:31Z","2021-08-23T21:33:05Z"
"","10686","KAFKA-4083: prevent changing replication factor via partition reassignment","### Summary  `ReassignPartitionsCommand` does _not_ check whether the replication factor of a partition is the same as the existing replication factor, which may result that different partitions of a topic have different numbers of replicas.  A few years ago, I attempted a fix to `ReassignPartitionsCommand` within https://github.com/apache/kafka/pull/1779, but now I think it should be fixed on the broker side.  ### Tests  Besides the integration test I have changed, I built a 3-node Kafka cluster on my laptop with Docker containers and ran a few shell scripts to test.  Create a topic ``` bin/kafka-topics.sh --bootstrap-server localhost:29092 --create --topic test-reassign-partitions --partitions 3 --replication-factor 3 ```  reassignment-plan.json ``` {    ""version"":1,    ""partitions"":[       {          ""topic"": ""test-reassign-partitions"",          ""partition"": 0,          ""replicas"" :[1, 2]       }    ] } ```  Execute a partition reassignment ``` bin/kafka-reassign-partitions.sh --bootstrap-server localhost:29092 --reassignment-json-file reassignment-plan.json --execute ```  The output is ``` Current partition replica assignment  {""version"":1,""partitions"":[{""topic"":""test-reassign-partitions"",""partition"":0,""replicas"":[2,1,0],""log_dirs"":[""any"",""any"",""any""]}]}  Save this to use as the --reassignment-json-file option during rollback Error reassigning partition(s): test-reassign-partitions-0: Replica assignment on test-reassign-partitions-0: [1, 2] violates its replication factor ```","open","","wanwenli","2021-05-13T08:21:27Z","2021-05-20T10:32:37Z"
"","11023","Update doSend doc about possible blocking","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","open","","IvanVas","2021-07-12T10:42:18Z","2021-07-16T09:23:14Z"
"","11463","KAFKA-13430: Remove broker-wide quota properties from the documentation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2021-11-03T14:51:39Z","2021-11-04T09:05:40Z"
"","11434","MINOR: Fix `GroupCoordinator.onGroupLoaded` log","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-10-26T09:41:53Z","2021-10-27T09:02:51Z"
"","11428","KAFKA-13365: Improve MirrorMaker2's client configuration","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","kip,","dongjinleekr","2021-10-22T12:14:51Z","2022-02-09T14:12:21Z"
"","11415","KAFKA-13429: ignore bin on new modules","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jeqo","2021-10-19T12:56:14Z","2021-11-10T20:36:25Z"
"","11402","MINOR: Various small fixes in the configuration docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2021-10-15T08:28:24Z","2021-10-15T14:08:25Z"
"","11384","MINOR: Improve error message for scale mismatch in Connect logical Decimal types","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2021-10-06T14:52:50Z","2021-10-08T17:29:08Z"
"","11341","MINOR: Bump latest 2.8 version to 2.8.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-20T06:49:31Z","2021-09-20T07:23:18Z"
"","11325","MINOR: Update version in documentation to 2.8.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-14T09:16:40Z","2021-09-14T13:01:34Z"
"","11318","MINOR: Bump version in upgrade guide to 2.8.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-10T06:20:38Z","2021-09-10T06:43:26Z"
"","11317","KAFKA-13287: Upgrade RocksDB to 6.22.1.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-09-09T20:01:02Z","2021-09-13T07:30:47Z"
"","11314","MINOR: Update LICENSE for 2.8.1","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-09-09T06:40:45Z","2021-09-09T14:14:34Z"
"","11276","KAFKA-13240: Disable HTTP TRACE Method in Connect","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","viktorsomogyi","2021-08-27T15:58:49Z","2021-10-04T15:44:15Z"
"","11264","KAFKA-13231; `TransactionalMessageCopier.start_node` should wait until the process if fully started","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-08-26T09:52:17Z","2021-08-27T06:32:24Z"
"","11237","MINOR: Fix how the last borker id is computed","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-08-19T18:47:30Z","2021-08-19T21:54:57Z"
"","11229","KAFKA-12961; Verify group generation in `DelayedJoin`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dajac","2021-08-18T11:44:07Z","2022-02-05T05:51:03Z"
"","11225","MINOR; Small optimizations in `ReplicaManager#becomeLeaderOrFollower`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-08-18T08:51:10Z","2021-08-27T06:22:44Z"
"","11184","KAFKA-13172: Add downgrade guidance note for 3.0","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-08-05T23:06:48Z","2021-08-18T00:04:04Z"
"","11175","MINOR: Fix getting started documentation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jsancio","2021-08-04T16:43:15Z","2021-08-06T02:39:29Z"
"","11151","MINOR: Should commit a task if the consumer position advanced as well","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-07-30T17:52:14Z","2021-08-05T20:41:51Z"
"","11150","MINOR: Fix missing word in LogLoader logged warning","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2021-07-30T13:04:52Z","2021-08-06T23:05:56Z"
"","11125","MINOR: Update `./gradlew allDepInsight` example in README","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-07-24T09:46:50Z","2021-07-25T15:35:27Z"
"","11090","FIX: Modify test config to reduce time to task assignment w/o warm-up tasks","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-07-20T12:37:40Z","2021-07-21T06:00:14Z"
"","11087","MINOR: Add port in error message in method parseAndValidateAddresses","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ZuevKirill95","2021-07-20T05:40:27Z","2021-07-22T05:51:25Z"
"","11063","MINOR: Add test to verify behavior of grace API","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","tests,","cadonna","2021-07-15T20:09:28Z","2021-07-27T18:56:39Z"
"","11061","MINOR: Add test for verifying retention on changelog topics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-07-15T17:47:26Z","2021-07-21T15:31:04Z"
"","11047","MINOR: Remove unnecessary code for WindowStoreBuilder.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tang7526","2021-07-14T07:05:09Z","2021-10-11T11:48:21Z"
"","11044","MINOR: Fully specify CompletableFuture type","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-14T03:40:40Z","2021-07-14T19:51:14Z"
"","11041","KAFKA-13080: Direct fetch snapshot request to kraft","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2021-07-13T22:21:11Z","2021-07-14T03:40:24Z"
"","11037","MINOR: Replace unused variable with underscore","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tang7526","2021-07-13T16:01:43Z","2021-07-17T08:36:52Z"
"","11018","MINOR: Remove unused method and extract method for runtime#WorkerTest.java","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","tang7526","2021-07-11T18:54:24Z","2021-07-14T03:19:04Z"
"","10953","MINOR: Default GRACE with Old API should set as 24H minus window-size / inactivity-gap","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-07-02T00:02:21Z","2021-07-16T18:23:49Z"
"","10943","Fix verification of version probing","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-06-30T10:54:05Z","2021-07-12T17:54:33Z"
"","10920","MINOR: Improve test of log messages for dropped records","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-06-23T08:23:05Z","2021-06-29T11:00:36Z"
"","10912","MINOR: Remove obsolete variables for metric sensors","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2021-06-21T20:41:12Z","2021-06-23T15:50:48Z"
"","10911","MINOR: Remove log warning for RocksDB 6+ upgrade","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-06-21T14:07:12Z","2021-06-22T11:17:40Z"
"","10866","[Test ] debug zk acl errors","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2021-06-11T12:21:25Z","2021-06-29T16:15:34Z"
"","10849","KAFKA-12922: MirrorCheckpointTask should close topic filter","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2021-06-09T08:24:05Z","2021-06-10T16:17:43Z"
"","10827","[WIP] KAFKA-12899: Support --bootstrap-server in ReplicaVerificationTool","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2021-06-05T20:52:49Z","2022-07-09T00:31:22Z"
"","10817","MINOR: Log member id of the leader when assignment are received","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2021-06-04T07:08:17Z","2021-06-07T06:38:00Z"
"","10703","MINOR: Fix deprecation warnings in SlidingWindowedCogroupedKStreamImplTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","ijuma","2021-05-15T14:05:08Z","2021-05-22T22:53:58Z"
"","10639","MINOR: Fix formatting in RelationalSmokeTest","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2021-05-06T12:51:18Z","2021-05-06T15:25:02Z"
"","10566","KAFKA-12694 Avoid schema mismatch DataException when validating default values","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","Naros","2021-04-20T07:14:10Z","2022-04-20T00:33:23Z"
"","10557","KAFKA-12683: Remove deprecated UsePreviousTimeOnInvalidTimestamp","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-04-18T17:57:44Z","2021-05-01T15:31:46Z"
"","10502","KAFKA-12630: Remove deprecated KafkaClientSupplier#getAdminClient in Streams","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2021-04-08T00:50:21Z","2021-04-16T22:44:57Z"
"","11457","MINOR: guard against calls to exit in QuorumTestHarness tests","","closed","kip-500,","cmccabe","2021-11-01T03:16:59Z","2021-11-24T19:53:01Z"
"","11440","KAFKA-13409: Catch cases where we attempt to terminate the VM in a junit test","","closed","","cmccabe","2021-10-27T14:38:31Z","2021-11-29T19:07:37Z"
"","11346","HOTFIX: fix compilation of ReplicaManagerConcurrencyTest","","closed","","cmccabe","2021-09-20T19:45:24Z","2021-09-20T19:52:01Z"
"","11313","MINOR: GroupMetadataManager#shutdown should remove metrics","","closed","","cmccabe","2021-09-08T22:49:44Z","2021-09-10T23:47:54Z"
"","11310","KAFKA-13279: KRaft: add CreateTopicsPolicy + AlterConfigsPolicy","","closed","kip-500,","cmccabe","2021-09-08T17:54:59Z","2021-09-22T20:07:49Z"
"","11269","MINOR: rename variable sTime to currentTimeMs to follow convention in the codebase","","closed","","ccding","2021-08-26T22:08:38Z","2021-09-22T18:04:42Z"
"","11235","KAFKA-13216: write correct tombstones into stream-stream join store changelog","","closed","","mjsax","2021-08-19T08:25:56Z","2021-09-10T04:55:42Z"
"","11165","MINOR Enable transaction system tests for KRaft in 3.0","","closed","","mumrah","2021-08-03T13:33:32Z","2021-08-03T18:25:04Z"
"","11130","KAFKA-13138: FileConfigProvider#get should keep failure exception","","closed","","cmccabe","2021-07-26T18:22:26Z","2021-11-30T17:11:50Z"
"","11067","MINOR: log broker configs in KRaft mode","","closed","kip-500,","cmccabe","2021-07-15T23:35:47Z","2021-07-19T19:34:45Z"
"","11064","MINOR: enable reassign_partitions_test.py for kraft","","closed","kip-500,","cmccabe","2021-07-15T20:54:11Z","2021-07-19T16:08:55Z"
"","11043","MINOR: fix Scala 2.12 compile failure in ControllerApisTest","","closed","","cmccabe","2021-07-14T03:36:06Z","2021-07-14T06:19:19Z"
"","11030","MINOR: Unmarking raft quorum configs as internal","","closed","","dielhennr","2021-07-12T20:29:45Z","2021-07-12T21:53:59Z"
"","11012","Add KRaft ""broker"" to several RPC's listeners","","closed","kip-500,","mumrah","2021-07-09T19:34:11Z","2021-07-10T17:45:27Z"
"","10995","MINOR: Hint about ""docker system prune"" when ducker-ak build fails","","closed","","cmccabe","2021-07-08T00:26:26Z","2021-07-08T17:09:49Z"
"","10978","MINOR: Use time constant algorithms when comparing passwords or keys","","closed","","rhauch","2021-07-06T15:49:15Z","2021-07-30T22:48:04Z"
"","10942","MINOR: Move ZkMetadataCache into its own file.","","closed","kip-500,","cmccabe","2021-06-29T23:01:23Z","2021-06-29T23:03:07Z"
"","10714","MINOR: add ConfigUtils method for printing configurations","","closed","","cmccabe","2021-05-17T18:04:21Z","2021-05-19T18:03:53Z"
"","10708","MINOR: remove unnecessary `public` keyword from `Partitioner` interface","","closed","producer,","mjsax","2021-05-16T18:50:05Z","2021-05-16T21:22:53Z"
"","10695","KAFKA-12783: Remove the deprecated ZK-based partition reassignment API","","closed","kip-500,","cmccabe","2021-05-14T17:00:05Z","2021-07-21T17:32:17Z"
"","10582","MINOR: Bump latest 2.6 version to 2.6.2","","closed","","ableegoldman","2021-04-21T19:41:56Z","2021-04-22T23:26:02Z"
"","10567","MINOR: Ensure `ignorable` is a boolean value.","","closed","","alecthomas","2021-04-20T07:56:44Z","2021-04-21T09:35:18Z"
"","10548","KAFKA-12396 added a nullcheck before trying to retrieve a key","","closed","streams,","Nathan22177","2021-04-16T12:44:23Z","2021-04-30T02:47:41Z"