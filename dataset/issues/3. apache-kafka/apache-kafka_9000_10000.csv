"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","3219","KAFKA-5374. AdminClient gets ""server returned information about unkno…","…wn correlation ID"" when communicating with older brokers","closed","","cmccabe","2017-06-03T00:01:46Z","2019-05-20T19:02:32Z"
"","3003","KAFKA-5194: KIP-153: Include only client traffic in BytesOutPerSec me…","…tric  Also added 2 new metrics to account for incoming/outgoing traffic due to internal replication - ReplicationBytesInPerSec - ReplicationBytesOutPerSec  We had a look at creating an integration test to verify the Bytes/Replication In/Out metrics but had issues. When running using IntegrationTestHarness, all the brokers were using the same Yammer Metrics registry and metrics name collided.   Also although we only update the global value for the 2 new metrics, we can see an inner node for each topic (that stays 0). eg: kafka.server:type=BrokerTopicMetrics,name=ReplicationBytesOutPerSec,topic=testtopic   It looks like something forced by BrokerTopicMetrics. If there's no way to remove them, then we might want to update them accordingly to give them meaningful values","closed","","mimaison","2017-05-09T14:04:49Z","2018-04-18T13:31:40Z"
"","2755","KAFKA-4930: Added connector name validator …","…to check for empty connector name and illegal characters in connector name. This also fixes  KAFKA-4938 by removing the check for slashes in connector name from ConnectorsResource.","closed","connect,","soenkeliebau","2017-03-28T21:11:31Z","2020-10-16T06:29:08Z"
"","2976","KAFKA-5170. KafkaAdminClientIntegration test should wait until metada…","…ta is propagated to all brokers","closed","","cmccabe","2017-05-04T19:43:55Z","2019-05-20T18:42:03Z"
"","2754","KAFKA-4968. The NetworkClient should rate-limit its log messages abou…","…t protocol downgrades","closed","","cmccabe","2017-03-28T17:41:48Z","2019-01-23T18:09:12Z"
"","3277","KAFKA-5414: Revert ""KAFKA-5327: ConsoleConsumer should manually commi…","…t offsets for records that are returned in receive()""  This reverts commit d7d1196a0b542adb46d22eeb5b6d12af950b64c9.","closed","","hachikuji","2017-06-09T00:27:41Z","2017-06-09T16:59:51Z"
"","3461","MINOR: need to be backwards compatible with deprecated default config…","…s until removed","closed","","bbejeck","2017-06-29T22:57:54Z","2017-07-12T18:04:16Z"
"","2708","KAFKA-4914: Partition re-assignment tool should check types before pe…","…rsisting state in ZooKeeper  Prior to this, there have been instances where invalid data was allowed to be persisted in ZooKeeper, which causes ClassCastExceptions when a broker is restarted and reads this type-unsafe data.  Adds basic structural and type validation for the reassignment JSON via introduction of Scala case classes that map to the expected JSON structure.","closed","","nicktrav","2017-03-19T21:38:43Z","2018-03-31T21:33:45Z"
"","2977","KAFKA-5176: AdminClient: add controller and clusterId methods to Desc…","…ribeClusterResults","closed","","cmccabe","2017-05-04T22:21:25Z","2019-05-20T18:41:49Z"
"","2540","KAFKA-4756: The auto-generated broker id should be passed to MetricRe…","…porter.configure","closed","","cmccabe","2017-02-11T01:04:03Z","2017-02-13T21:47:32Z"
"","3250","KAFKA-5394. KafkaAdminClient#timeoutCallsInFlight does not work as ex…","…pected  * Rename KafkaClient#close to KafkaClient#forget to emphasize that it forgets the requests on a given connection. * Create KafkaClient#disconnect to tear down a connection and deliver disconnects to all the requests on it. * AdminClient.java: fix mismatched braces in JavaDoc. * Make the AdminClientConfig constructor visible for testing. * KafkaAdminClient: add TimeoutProcessorFactory to make the TimeoutProcessor swappable for testing. * Make TimeoutProcessor a static class rather than an inner class.","closed","","cmccabe","2017-06-06T21:27:32Z","2019-05-20T19:02:23Z"
"","2839","Fix typo - Acls Examples, Adding or removing a principal as producer …","…or consumer","closed","","sunnykrGupta","2017-04-11T09:00:34Z","2017-04-11T10:30:18Z"
"","3395","KAFKA-3575: Use console consumer access topic that does not exist, ca…","…n not use ""Control + C"" to exit process  A finally block is not guaranteed to execute in the event of Ctrl+C happening while in the try or catch blocks. Decrementing the latch in the finally block therefore made the shutdown hook hang waiting for something that would never happen and the JVM couldn't exit while it was running the shutdown hook.  Replacing the latch with an atomic flag to say whether we've run the cleanup code allows us to either run it from the shutdown hook, or the finally block. It should thus definitely run once. When run from the shutdown hook the main thread would no longer be running, so it should be threadsafe.  The contribution is my original work and I license the work to the project under the project's open source license.","open","","tombentley","2017-06-21T11:18:10Z","2018-03-02T19:30:18Z"
"","2729","KAFKA-4945: Suppress findbugs warnings about machine-generated code i…","…n jmh-benchmarks","closed","","cmccabe","2017-03-23T17:20:35Z","2019-05-20T18:39:32Z"
"","2558","KAFKA-4774. Inner classes which don't need a reference to the outer c…","…lass should be static","closed","","cmccabe","2017-02-16T21:10:44Z","2019-05-20T18:35:11Z"
"","3000","KAFKA-5198. RocksDbStore#openIterators should be synchronized, since …","…it is accessed from multiple threads","closed","","cmccabe","2017-05-08T21:54:47Z","2019-05-20T18:41:39Z"
"","2515","Use regex for including jars from libs folder instead of adding indiv…","…idual files  When we include individual files to classpath it may exceed the maximum length of command line which is 4096 bytes. This issue also breaks the stop script which will grep for ‘kafka.Kafka’ but when cmdline exceeds 4096 bytes, ‘kafka.Kafka’ text won’t be present in the process cmdline and hence cannot be grepped. Details here: http://unix.stackexchange.com/questions/343353/grep-only-prints-up-to-4096-characters-of-any-process  Using `CLASSPATH=$CLASSPATH:$base_dir/libs/*` limits the length of command line.","open","","zer0Id0l","2017-02-08T04:47:21Z","2017-06-12T21:39:27Z"
"","2654","MINOR: KAFKA-3989_follow_up_PR: update script to run from kafka root eg ./jm…","…h-benchmarks/jmh.sh","closed","","bbejeck","2017-03-08T01:59:00Z","2017-08-26T23:17:02Z"
"","3074","KAFKA-5036: hold onto the leader lock in Partition while serving an O…","…ffsetForLeaderEpoch request","closed","","junrao","2017-05-16T22:59:33Z","2017-05-18T02:07:12Z"
"","3296","KAFKA-3123: Follower Broker cannot start if offsets are already out o…","…f range  From https://github.com/apache/kafka/pull/1716#discussion_r112000498, ensure the cleaner is restarted if Log.truncateTo throws","closed","","mimaison","2017-06-10T20:43:33Z","2018-04-18T13:31:37Z"
"","3359","KAFKA-5062. Kafka brokers can accept malformed requests which allocat…","…e gigabytes of memory","open","","cmccabe","2017-06-16T22:15:14Z","2018-03-02T19:30:17Z"
"","2783","HOTFIX: fix potentially hanging test shouldAddStateStoreToRegexDefine…","…dSource","closed","","bbejeck","2017-04-01T11:42:38Z","2017-05-04T07:21:19Z"
"","2489","KAFKA-4708: Fix Transient Failure in BrokerApiVersionsCommandTest.che…","…ckBrokerApiVersionCommandOutput","closed","","cmccabe","2017-02-03T01:06:30Z","2019-05-20T18:35:05Z"
"","3105","KAFKA-5294: PlainSaslServerFactory should allow a null Map in getMech…","…anismNames  If props is null, use POLICY_NOPLAINTEXT default value: false  As far as I can tell, none of the other classes implementing SaslServerFactory use the properties Map","closed","","mimaison","2017-05-19T19:54:42Z","2018-04-18T13:31:39Z"
"","2602","KAFKA-4809: docker/run_tests.sh should set up /opt/kafka-dev to be the source directory","…0.x and not 0.8","closed","","cmccabe","2017-02-27T20:02:51Z","2019-05-20T18:36:08Z"
"","2800","added interface to allow producers to create a ProducerRecord without…","… specifying a partition, making it more obvious that the parameter partition can be null","closed","","simplesteph","2017-04-04T06:47:09Z","2017-04-19T22:36:21Z"
"","3174","KAFKA-5293. Do not apply exponential backoff if users have overridden…","… reconnect.backoff.ms","closed","","cmccabe","2017-05-31T02:16:31Z","2019-05-20T19:02:37Z"
"","2684","KAFKA-4895: Fix findbugs ""format string should use %n rather than \n""…","… in tools","closed","","cmccabe","2017-03-14T15:41:52Z","2019-05-20T18:36:52Z"
"","3282","[KAFKA-5417] Clients get inconsistent connection states when SASL/SSL…","… connection is marked CONECTED and DISCONNECTED at the same time  details are in: https://issues.apache.org/jira/browse/KAFKA-5417","closed","","dongeforever","2017-06-09T13:47:43Z","2017-08-24T02:44:26Z"
"","3227","MINOR: fix quotes for consistent rendering","“ as opposed to "" don't render consistently across browsers. On current Kafka website they render correctly in Firefox but not Chrome (â€œrunsâ€) - charset issue?","closed","","mihbor","2017-06-03T08:47:19Z","2017-06-16T20:10:46Z"
"","3249","MINOR: Fix docs; client no longer uses yammer metrics","Yammer metrics is no longer a dependency of the client, but the Monitoring section of the docs contradicts this. For quite some time, clients have been using an internal metrics system unrelated to Yammer/Dropwizard metrics.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","jklukas","2017-06-06T20:08:16Z","2017-07-20T11:33:11Z"
"","2926","KAFKA-5131: WriteTxnMarkers and complete commit/abort on partition immigration","write txn markers and complete the commit/abort for transactions in PrepareXX state during partition immigration.","closed","","dguy","2017-04-27T17:20:14Z","2017-05-16T14:04:46Z"
"","2872","MINOR: Update dependencies for 0.11","Worth special mention:  1. Update Scala to 2.11.11 and 2.12.2 2. Update Gradle to 3.5 3. Update ZooKeeper to 3.4.10 4. Update reflections to 0.9.11, which:     * Switches to jsr305 annotations with a provided scope     * Updates Guava from 18 to 20     * Updates javaassist from 3.18 to 3.21  There’s a separate PR for updating RocksDb, so I didn’t include that here.","closed","","ijuma","2017-04-19T07:59:49Z","2017-09-05T09:01:26Z"
"","3127","Add sleep between empty polls to avoid burning CPU","Workaround for  https://issues.apache.org/jira/browse/KAFKA-3159","closed","","felixgborrego","2017-05-23T12:36:03Z","2017-05-24T09:27:39Z"
"","2975","KIP-150 [WIP]: Kafka Streams Cogroup","Work in progress PR for KIP-150.","closed","streams,","ghost","2017-05-04T17:50:43Z","2019-09-24T18:43:39Z"
"","3184","KAFKA-5351: Reset pending state when returning an error in appendTransactionToLog","Without this patch, future client retries would get the `CONCURRENT_TRANSACTIONS` error code indefinitely, since the pending state wouldn't be cleared when the append to the log failed.","closed","","apurvam","2017-05-31T22:08:47Z","2017-06-01T06:13:58Z"
"","3002","KAFKA-5203: Metrics: fix resetting of histogram sample","Without the histogram cleanup, the percentiles are calculated incorrectly after purging of one or more samples: event counts go out of sync with counts in histogram buckets, and bucket with lower value gets chosen for the given quantile.  This change adds the necessary histogram cleanup.","closed","","iv-m","2017-05-09T12:27:43Z","2017-05-15T18:08:08Z"
"","3182","MINOR: Use `waitUntil` to fix transient failures of ControllerFailoverTest","Without it, it's possible that the assertion is checked before the exception is thrown in the callback.","closed","","ijuma","2017-05-31T18:26:16Z","2017-06-18T09:30:32Z"
"","3400","KAFKA-5491: Enable transactions in ProducerPerformance Tool","With this patch, the `ProducePerfomance` tool can create transactions of differing durations.  This patch was used to to collect the initial set of benchmarks for transaction performance, documented here: https://docs.google.com/spreadsheets/d/1dHY6M7qCiX-NFvsgvaE0YoVdNq26uA8608XIh_DUpI4/edit#gid=282787170","closed","","apurvam","2017-06-21T18:51:17Z","2017-06-21T21:47:41Z"
"","3108","KAFKA-5247: Materialize committed offsets in offset order","With this patch, offset commits are always materialized according to the order of the commit records in the offsets topic.   Before this patch, transactional offset commits were materialized in transaction order. However, the log cleaner will always preserve the record with the greatest offset. This meant that if there was a mix of offset commits from a consumer and a transactional producer, then it we would switch from transactional order to offset order after cleaning, resulting in an inconsistent state.","closed","","apurvam","2017-05-20T00:02:37Z","2017-05-23T16:48:38Z"
"","2665","MINOR: Let MirrorMaker pick up client.id if specified in consumer.properties","With this change, defining client.id in the consumer.properties used by mirror maker will no longer be ignored. If not specified, the default will still be the group id.  ----  Hi!  For reasons I won't get into here, we needed to change the client.id of our mirror maker consumers. We quickly realized that the config was not picked up, and looking at the code it became obvious why: it defaults to the groupId without checking for a clientId. Hence this small change.  PS: Ran unit tests successfully.","closed","","Xaelias","2017-03-09T21:59:41Z","2022-02-10T16:17:48Z"
"","3301","MINOR: Avoiding attempt to connect to Zookeeper on closing consumer for deleting random group.id with the new one","With the new consumer the ""/consumers"" path on Zookeeper isn't filled by consumer info. On closing the new consumer, there is some code that is useless to execute for trying to connect to Zookeeper (but the URL is null).","closed","","ppatierno","2017-06-12T09:07:09Z","2017-11-17T13:12:58Z"
"","2894","[KAFKA-5092] [WIP] changed ProducerRecord interface - KIP 141","WIP - need to update all deprecated calls if that's the way we go forward","closed","","simplesteph","2017-04-22T00:43:59Z","2017-12-14T05:40:18Z"
"","3260","Fixed path to log4j config file","While running the quickstart tutorial, I came across this exception due to bad file URI:  D:\gdrive\workspace\Pulse\kafka_2.11-0.10.2.0>bin\windows\kafka-server-start.bat config\server.properties log4j:ERROR Could not read configuration file from URL [file:D:/Userfiles/my_user_dir/Downloads/kafka_2.11-0.10.2.0/kafka_2.11-0.10.2.0/bin/windows/../../config/log4j.properties]. java.io.FileNotFoundException: D:\Userfiles\my_user_dir\Downloads\kafka_2.11-0.10.2.0\kafka_2.11-0. 10.2.0\bin\windows\..\..\config\log4j.properties (The system cannot find the path specified)         at java.io.FileInputStream.open0(Native Method)","open","","iotmani","2017-06-07T17:48:32Z","2018-03-02T19:30:13Z"
"","2775","MINOR: Reduce Fork Count in ITs","While looking into instabilities like https://issues.apache.org/jira/browse/KAFKA-4692 I noticed that we are running one JVM fork per core in the ITs.  This seems to me could be contributing to the instability of some ITs a lot. Turning down the number of forks to half the cores, the IT suits speed up measurably for me (example streams goes from ~4 minutes to 2.5 minutes). Also, case in point this is what 4 forks look like on an i7 (that would get 8 forks in trunk):  ![4forks](https://cloud.githubusercontent.com/assets/6490959/24557893/a7afa746-1639-11e7-94ff-6327695cf108.png)  Effectively there's still load on all 8 cores at 4 forks (which is expected looking at the fact that most ITs have 2 or more I/O threads).  => imo it's a good idea to turn this down to half the cores. Let's see how Jenkins behaves with this in terms of the build times :)","closed","","original-brownbear","2017-03-31T15:48:50Z","2017-04-22T20:24:39Z"
"","3256","MINOR: Updated .gitignore for excluding out directory","While building the project using IntelliJ IDEA, the ""out"" directory is generated but not excluded for commit by the .gitignore. Someone could accidentally commit it. This PR add the ""out"" directory into the .gitignore.","closed","","ppatierno","2017-06-07T12:13:35Z","2017-06-08T11:52:03Z"
"","2671","MINOR: incorrect javadoc formatting","When wrapped in a `{@code ...}` block, `<>` are not formatted as `<>`.   For instance, see the formatting of the example in https://kafka.apache.org/0102/javadoc/org/apache/kafka/streams/KafkaStreams.html","closed","","acoburn","2017-03-10T21:37:50Z","2017-03-11T01:07:11Z"
"","2649","KAFKA-4860: Allow spaces in paths on windows","When we install kafka on path with spaces, batch files were failing, this PR is trying to fix this issue.","closed","","klesta490","2017-03-07T07:51:46Z","2017-09-05T23:29:18Z"
"","3308","KAFKA-5437: Always send a sig_kill when cleaning the message copier","When the message copier hangs (like when there is a bug in the client), it ignores the sigterm and doesn't shut down. this leaves the cluster in an unclean state causing future tests to fail.   In this patch we always send SIGKILL when cleaning the node if the process isn't already dead. This is consistent with the other services.","closed","","apurvam","2017-06-12T22:54:09Z","2017-06-13T00:28:01Z"
"","3198","KAFKA-5164 Ensure SetSchemaMetadata updates key or value when Schema changes","When the `SetSchemaMetadata` SMT is used to change the name and/or version of the key or value’s schema, any references to the old schema in the key or value must be changed to reference the new schema. Only keys or values that are `Struct` have such references, and so currently only these are adjusted.  This is based on `trunk` since the fix is expected to be targeted to the 0.11.1 release.","closed","connect,","rhauch","2017-06-01T21:00:11Z","2020-10-16T06:29:09Z"
"","2898","kafka-5104: DumpLogSegments should not open index files with `rw`","When issuing DumpLogSegments command, the underlying index files(including timestamp index) will be opened with read-only mode.","closed","","huxihx","2017-04-23T11:49:21Z","2017-04-24T07:07:26Z"
"","2820","KAFKA-5039: Logging in BlockingChannel and SyncProducer connect","When an exception is thrown in BlockingChannel::connect, the connection is disconnected but the actual exception is not logged. This later manifests as ClosedChannelException when trying to send. Also the SyncProducer wrongfully logs ""Connected to host:port for producing"" even in case of exceptions.  The proposed patch logs the exception in BlockingChannel::connect in debug level and logs the ""Connected"" message in SyncProducer only on successful connection.","closed","","arunmahadevan","2017-04-07T06:20:55Z","2018-07-20T17:50:57Z"
"","3144","KAFKA-5323: AdminUtils.createTopic should check topic existence upfront","When a topic exists, AdminUtils.createTopic unnecessarily does N+2 zookeeper reads where N is the number of brokers. Here is the breakdown of the N+2 zookeeper reads: 1. reads the current list of brokers in zookeeper (1 zookeeper read) 2. reads metadata for each broker in zookeeper (N zookeeper reads where N is the number of brokers) 3. checks for topic existence in zookeeper (1 zookeeper read)  This can have a larger impact than one might initially suspect. For instance, a broker only populates its MetadataCache after it has joined the cluster and the controller sends it an UpdateMetadataRequest. But a broker can begin processing requests even before registering itself in zookeeper (before the controller even knows the broker is alive). In other words, a broker can begin processing MetadataRequests before processing the controller's UpdateMetadataRequest following broker registration.  Processing these MetadataRequests in this scenario leads to large local times and can cause substantial request queue backup, causing significant delays in the broker processing its initial UpdateMetadataRequest. Since the broker hasn't received any UpdateMetadataRequest from the controller yet, its MetadataCache is empty. So the topics from all the client MetadataRequests are treated as brand new topics, which means the broker tries to auto create these topics. For each pre-existing topic queried in the MetadataRequest, auto topic creation performs the N+2 zookeeper reads mentioned earlier.  In one bad production scenario (while recovering from KAFKA-4959), this caused a significant delay in bringing replicas online, as both the initial LeaderAndIsrRequest and UpdateMetadataRequest from the controller on broker startup was stuck behind these client MetadataRequests hammering zookeeper.  We can reduce the N+2 reads down to 1 by checking topic existence upfront.","closed","","onurkaraman","2017-05-25T18:15:44Z","2020-11-04T06:41:04Z"
"","2514","MINOR: Add logging when commitSync fails in StreamTask","When `consumer.commitSync` fails in `StreamTask`, the `CommitFailedException` bubbles up to [here](https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L780) and swallowed.  It'd be great if we knew which offsets failed to commit so that we may rewind our consumer.","closed","","jmoney","2017-02-08T02:05:55Z","2017-02-08T23:23:14Z"
"","3034","MINOR: Various small improvements to kafka.metrics.MetricsTest","We've had to tweak the other tests to make them pass.   `testBrokerTopicMetricsUnregisteredAfterDeletingTopic` seemed completely broken, as it's was creating a topic but producing/consuming to another one.   With @mpburg","closed","","mimaison","2017-05-12T14:44:54Z","2018-04-18T13:31:40Z"
"","3094","KAFKA-5269: Correct handling of UNKNOWN_TOPIC_OR_PARTITION error","We should retry AddPartitionsToTxnRequest and TxnOffsetCommitRequest when receiving an UNKNOWN_TOPIC_OR_PARTITION error.  As described in the JIRA: It turns out that the `UNKNOWN_TOPIC_OR_PARTITION` is returned from the request handler in KafkaAPis for the AddPartitionsToTxn and the TxnOffsetCommitRequest when the broker's metadata doesn't contain one or more partitions in the request. This can happen for instance when the broker is bounced and has not received the cluster metadata yet.   We should retry in these cases, as this is the model followed by the consumer when committing offsets, and by the producer with a ProduceRequest.","closed","","apurvam","2017-05-18T23:04:04Z","2017-05-20T01:53:04Z"
"","2747","KAFKA-5003: StreamThread should catch InvalidTopicException","We should catch `InvalidTopicException` and not just `NoOffsetForPartitionException`. Also, we need to step through all partitions that might be affected and reset those.","closed","","mjsax","2017-03-28T01:05:43Z","2017-04-07T03:18:19Z"
"","3261","MINOR: Set log level for producer internals to trace for transactions test","We need this to debug most issues with the transactions system test.","closed","","apurvam","2017-06-07T18:33:32Z","2017-06-08T16:47:45Z"
"","3426","MINOR: Fix race condition in KafkaConsumer close","We intended to make `KafkaConsumer.close()` idempotent, but due to the fact that the `closed` variable is checked without a lock prior to beginning close logic, it is possible for two or more threads to see `closed=false` and attempt to close.","closed","","hachikuji","2017-06-23T23:17:42Z","2017-06-27T15:38:34Z"
"","3205","KAFKA-5236; Increase the block/buffer size when compressing with Snappy and Gzip","We had originally increased Snappy’s block size as part of KAFKA-3704. However, we had some issues with excessive memory usage in the producer and we reverted it in 7c6ee8d5e.  After more investigation, we fixed the underlying reason why memory usage seemed to grow much more than expected via KAFKA-3747 (included in 0.10.0.1).  In 0.10.2, we changed the broker to use the same classes as the producer and the broker’s block size for Snappy was changed from 32 KB to 1KB. As reported in KAFKA-5236, the on disk size is, in some cases, 50% larger when the data is compressed with 1 KB instead of 32 KB as the block size.  As discussed in KAFKA-3704, it may be worth making this configurable and/or allocate the compression buffers from the producer pool. However, for 0.11.0.0, I think the simplest thing to do is to default to 32 KB for Snappy (the default if no block size is provided).  I also increased the Gzip buffer size. 1 KB is too small and the default is smaller still (512 bytes). 8 KB (which is the default buffer size for BufferedOutputStream) seemed like a reasonable default.","closed","","ijuma","2017-06-02T10:17:47Z","2017-08-22T06:40:00Z"
"","2757","KAFKA-4980: testReprocessingFromScratch unit test failure","We got test error `org.apache.kafka.common.errors.TopicExistsException: Topic 'inputTopic' already exists.` in some builds. Can reproduce reliably at local machine. Root cause it async ""topic delete"" that might not be finished before topic gets re-created.","closed","","mjsax","2017-03-29T06:31:33Z","2017-03-31T17:14:10Z"
"","2610","MINOR: Make asJsonSchema() and asConnectSchema() methods public","Want to use these methods in an external project.","closed","","C0urante","2017-02-28T17:38:24Z","2018-01-09T22:55:59Z"
"","3447","MINOR: Do not wait for first line of console consumer output since we now have a more reliable test using JMX","Waiting for the first line of output was added in KAFKA-2527 when JmxMixin was originally added as a heuristic to determine when the process was ready. We've since determined this is not good enough given JmxTool's limitations and now include a separate, more reliable check before starting JmxTool. This check is also dangerous since a consumer that is started before data is available in the topic, it won't output anything to stdout and only logs errors to a separate log file. This means we may have a long delay between starting the process and starting JMX monitoring.  Since we have a more reliable check for liveness via JMX now (and in cases that need it, partition assignment metrics via JMX), we should no longer need to wait for the first line of output.","closed","","ewencp","2017-06-27T23:00:57Z","2017-07-18T04:25:54Z"
"","2748","MINOR: remove misleading scala setter in ControllerContext","Using a scala setter `liveBrokers_` to update both `liveBrokersUnderlying` and `liveBrokerIdsUnderlying` in the ControllerContext was making the code hard to read.","closed","","onurkaraman","2017-03-28T08:04:26Z","2020-12-09T10:00:29Z"
"","3431","MINOR: Adjust checkstyle suppression paths to work on Windows","Use the file name whenever possible and replace / with [/\\] when it's not.  Also remove unnecessary suppresions.","closed","","ijuma","2017-06-26T10:45:30Z","2017-09-05T08:39:52Z"
"","2878","KAFKA-3070: SASL unit tests dont work with IBM JDK","Use IBM Kerberos module for SASL tests if running on IBM JDK  Developed with @edoardocomar Based on https://github.com/apache/kafka/pull/738 by @rajinisivaram","closed","","mimaison","2017-04-20T11:08:41Z","2018-04-18T13:31:43Z"
"","3213","KAFKA-5359: Use a suppressed exception when throwing`RequestFuture.exception()`","Use a suppressed exception when throwing `RequestFuture.exception()` on the client side to help with identifying where on the client side the exception is thrown.  For example, running the code inside `org.apache.kafka.clients.consumer.internals.ConsumerCoordinatorTest.testGroupDescribeUnauthorized` currently returns with ``` org.apache.kafka.common.errors.GroupAuthorizationException: Not authorized to access group: test-group ```  With this PR, the same code returns with the following failure trace ``` org.apache.kafka.common.errors.GroupAuthorizationException: Not authorized to access group: test-group 	Suppressed: org.apache.kafka.common.KafkaException: An error occurred in the broker when looking up the group coordinator. 		at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:226) 		at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:201) 		at org.apache.kafka.clients.consumer.internals.ConsumerCoordinatorTest.testGroupDescribeUnauthorizedOld(ConsumerCoordinatorTest.java:161) 		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 		at java.lang.reflect.Method.invoke(Method.java:497) 		at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 		at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 		at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 		at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 		at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 		at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 		at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 		at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 		at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 		at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 		at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 		at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 		at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 		at org.junit.runners.ParentRunner.run(ParentRunner.java:363) ```","open","","vahidhashemian","2017-06-02T21:04:22Z","2018-03-02T19:30:13Z"
"","2991","KAFKA-4208: Add Record Headers","Update upgrade.html  Raising this now, as KIP-118 is pulled from release as such submitting this without java 8 changes.  As per remaining review comment from https://github.com/apache/kafka/pull/2772, updating the upgrade notes.","closed","","michaelandrepearce","2017-05-07T16:33:33Z","2017-05-21T21:35:37Z"
"","3464","MINOR: Enable a number of xlint scalac warnings","Update the code where possible to fix the warnings. The unused warning introduced in Scala 2.12 is quite handy and provides a reason to compile with Scala 2.12.","closed","","ijuma","2017-06-30T05:45:46Z","2017-08-22T06:37:57Z"
"","3479","MINOR: compatibility tests for streams","Update system tests to make use of the newly released 0.11 version.  Add on to https://github.com/apache/kafka/pull/3454","closed","","enothereska","2017-07-03T09:24:34Z","2017-07-03T15:19:17Z"
"","2516","MINOR: add GlobalKTable doc to streams.html","Update streams.html with GlobalKTable docs","closed","","dguy","2017-02-08T10:17:22Z","2017-02-17T00:57:45Z"
"","2502","Merge pull request #1 from apache/trunk","Update forked repo","closed","","PierreCoquentin","2017-02-04T20:10:08Z","2017-02-04T23:11:53Z"
"","3082","KAFKA-5280: Protect txn metadata map with read-write lock","Two major changes plus one minor change:  0. change stateLock to a read-write lock.  1. Put the check of ""isCoordinator"" and ""coordinatorLoading"" together with the return of the metadata, under one read lock block, since otherwise we can get incorrect behavior if there is a change in the metadata cache after the check but before the accessing of the metadata.  2. Grab the read lock right before trying to append to local txn log, and until the local append returns; this is to avoid the scenario that the epoch has actually changed when we are appending to local log (e.g. emigration followed by immigration).  3. only watch on txnId instead of txnId and txnPartitionId in the txn marker purgatory, and disable reaper thread, as we can now safely clear all the delayed operations by traversing the marker queues.","closed","","guozhangwang","2017-05-18T01:07:18Z","2017-11-06T22:44:31Z"
"","2582","MINOR: Fixed Non-Final Close Method + its Duplication","Trivial but in my opinion it's well worth it making the `close` method `final` here and also deleting that duplication (especially since it's hiding the fact that the `close` is synchronized here).","closed","","original-brownbear","2017-02-22T05:52:06Z","2017-02-24T23:17:32Z"
"","3071","KAFKA-5258: move all partition and replica state transition rules into their states","Today the PartitionStateMachine and ReplicaStateMachine defines and asserts the valid state transitions inline for each state. It would be cleaner to move all partition and replica state transition rules into a map and simply do the assertion at the top of the handleStateChange.","closed","","onurkaraman","2017-05-16T21:00:24Z","2017-05-18T10:38:43Z"
"","2752","Using English characters to prevent garbled characters","to fix garbled characters      Stream as Table: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise, and it can be easily turned into a 鈥渞eal鈥� table by replaying the changelog from beginning to end to reconstruct the table.   Similarly, in a more general analogy, aggregating data records in a stream 鈥� such as computing the   total number of pageviews by user from a stream of pageview events 鈥� will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).     Table as Stream: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream's data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a 鈥渞eal鈥� stream by iterating over each key-value entry in the table.","closed","","tyronecai","2017-03-28T10:09:38Z","2018-01-30T18:23:59Z"
"","3476","Update KafkaLog4jAppender.java adding SaslMechanism as an option","to be able to use kafka with Sasl/Plain  we need to ba able to set saslMechanism from kafka log4j options","open","","jemaiah","2017-07-02T22:34:33Z","2018-03-02T19:30:23Z"
"","3410","KAFKA-4913: prevent creation of window stores with less than 2 segments","Throw IllegalArgumentException when attempting to create a `WindowStore` via `Stores` or directly with `RocksDBWindowStoreSupplier` when it has less than 2 segments.","closed","","dguy","2017-06-22T15:33:01Z","2017-08-16T13:23:17Z"
"","2676","MINOR: Suppress an inappropriate warning in MirrorMaker","Though MirrorMaker uses the `producer.type` value of the producer properties, ProducerConfig show the warning: `The configuration 'producer.type' was supplied but isn't a known config.`","closed","","takebayashi","2017-03-12T06:16:01Z","2017-04-03T23:18:43Z"
"","2695","KAFKA-4594: Annotate integration tests and provide gradle build targets to run subsets of tests","This uses junit Categories to identify integration tests. Adds 2 new build targets: `integrationTest` and `testAll`. The `test` target will just run the unit tests","closed","","dguy","2017-03-16T14:42:03Z","2017-03-30T11:10:26Z"
"","3122","KAFKA-5310: reset ControllerContext during resignation","This ticket is all about ControllerContext initialization and teardown. The key points are: 1. we should teardown ControllerContext during resignation instead of waiting on election to fix it up. A heapdump shows that the former controller keeps pretty much all of its ControllerContext state laying around. 2. we don't properly teardown/reset ControllerContext.partitionsBeingReassigned. This can cause problems when the former controller becomes re-elected as controller at a later point in time.  Suppose a partition assignment is initially R0. Now suppose a reassignment R1 gets stuck during controller C0 and an admin tries to ""undo"" R1 (by deleting /admin/partitions_reassigned, deleting /controller, and submitting another reassignment specifying R0). The new controller C1 may succeed with R0. If the controller moves back to C0, it will then reattempt R1 even though that partition reassignment has been cleared from zookeeper prior to shifting the controller back to C0. This results in the actual partition reassignment in zookeeper being unexpectedly changed back to R1.","closed","","onurkaraman","2017-05-23T06:40:52Z","2017-05-23T23:31:03Z"
"","2911","MINOR: Improve information in assert failure for testMetricCollectionAfterShutdown","This test is failing consistently in https://jenkins.confluent.io/job/kafka-trunk/, but nowhere else. I ran this branch in a clone of that job several times and this test didn't fail. I suggest we merge this PR, which improves the test, to help us gather more information about the test failure.","closed","","ijuma","2017-04-25T10:20:04Z","2017-04-26T23:19:07Z"
"","2734","KAFKA-4948: Failure in kafka.admin.DescribeConsumerGroupTest...","This test fails regularly when run on OSX (on my machine anyway). The fix decreases the number of partitions in the consumer offsets topic allows it to complete within the timeout.   As an aside I wondered why it was taking so long (regression??). This is why: - It takes 6 seconds to create the offsets topic (on OSX) - It takes ~200ms to create each log - It takes ~ 100ms to create each index file (time index + offset index) - This time is spent on the line [raf.setLength()](https://github.com/apache/kafka/blob/5fc530bc483db145e0cba3b63a57d6d6a7c547f2/core/src/main/scala/kafka/log/AbstractIndex.scala#L56)  This isn't an issue in practice, but at least we know this isn't a real regression.","closed","","benstopford","2017-03-24T19:12:48Z","2018-02-25T21:46:49Z"
"","3236","MINOR: Default GroupInitialRebalanceDelayMsProp to 0 and OffsetsTopicPartitionsProp to 5 in tests","This should make tests faster. Tests that require specific values can override these values.","closed","","ijuma","2017-06-05T11:03:05Z","2017-06-18T09:26:59Z"
"","3237","MINOR: Create offsets topic explicitly in DescribeConsumerGroupTest","This should fix transient failures due to timeouts caused by slow auto creation of the offsets topic. The one exception is `testDescribeGroupWithNewConsumerWithShortInitializationTimeout` where we want initialisation to take longer than the timeout so we let it be auto created. That test has also been a bit flaky and I reduced the timeout to 1 ms.  Also: - Simplified resource handling - Removed usage of EasyMock that didn't make sense. - `findCoordinator` should retry if `sendAnyNode` throws an exception","closed","","ijuma","2017-06-05T11:09:26Z","2017-06-18T09:26:55Z"
"","3178","MINOR: Use new consumer in ProducerCompressionTest","This should be less flaky as it has a higher timeout. I also increased the timeout in a couple of other tests that had a very low (100 ms) timeouts.  The failure would manifest itself as:  ```text java.net.SocketTimeoutException 	at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:229) 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) 	at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:85) 	at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) 	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) 	at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:100) 	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:84) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:133) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:133) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:133) 	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:132) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:132) 	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:132) 	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:31) 	at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:131) 	at kafka.api.test.ProducerCompressionTest.testCompression(ProducerCompressionTest.scala:97) ```","closed","","ijuma","2017-05-31T12:13:54Z","2017-06-18T09:31:17Z"
"","3021","KAFKA-5006: change exception path","This should be backported to 0.10.2 as well.","closed","","enothereska","2017-05-11T09:24:35Z","2017-06-27T12:20:02Z"
"","2549","MINOR: Add subtree .gitignore files themselves to gitignore","This seems like a silly rule, but for example the Eclipse Gradle integration will 'helpfully' generate .gitignore files in every project that then show as dirty.  For example, if you take a vanilla Eclipse install and import Kafka, this is the status you get:  ``` ??	clients/.gitignore ??	connect/api/.gitignore ??	connect/file/.gitignore ??	connect/json/.gitignore ??	connect/runtime/.gitignore ??	connect/transforms/.gitignore ??	examples/bin/.gitignore ??	log4j-appender/.gitignore ??	streams/.gitignore ??	streams/examples/.gitignore ??	tools/.gitignore ```  Since Kafka only uses the top level `/.gitignore` we can just ignore all the rest.","open","","stevenschlansker","2017-02-14T22:06:52Z","2018-03-02T19:29:55Z"
"","2590","KAFKA-4789: Added support to ProcessorTopologyTestDriver to forward timestamps to internal topics","This resolves the issue in the ProcessorTopologyTestDriver that the extracted timestamp is not forwarded with the produced record to the internal topics.  JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-4789  The contribution is my original work and I license the work to the project under the project's open source license.  @guozhangwang @dguy","closed","","hrafzali","2017-02-23T15:25:37Z","2017-02-28T21:32:12Z"
"","2587","KAFKA-4789: Added support to ProcessorTopologyTestDriver to forward timestamps to internal topics","This resolves the issue in the ProcessorTopologyTestDriver that the extracted timestamp is not forwarded with the produced record to the internal topics.  JIRA ticket: https://issues.apache.org/jira/browse/KAFKA-4789  The contribution is my original work and I license the work to the project under the project's open source license.  @guozhangwang @dguy","closed","","hrafzali","2017-02-23T11:37:16Z","2017-02-23T15:24:48Z"
"","2629","KAFKA-4828: ProcessorTopologyTestDriver does not work when using through","This resolves the following issues in the ProcessorTopologyTestDriver:  - It should not create an internal changelog topic when using `through()` and `table()` - It should forward the produced record back into the topology if it is to a source topic  Jira ticket: https://issues.apache.org/jira/browse/KAFKA-4828  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","hrafzali","2017-03-02T22:17:20Z","2017-03-08T02:18:56Z"
"","2499","KAFKA-4461 Added support to ProcessorTopologyTestDriver for internal topics.","This resolves an issue in driving tests using the ProcessorTopologyTestDriver when `groupBy()` is invoked downstream of a processor that flags repartitioning.  Ticket: https://issues.apache.org/jira/browse/KAFKA-4461 Discussion: http://search-hadoop.com/m/Kafka/uyzND1wbKeY1Q8nH1  @dguy @guozhangwang   The contribution is my original work and I license the work to the project under the project's open source license.","closed","","amccague","2017-02-04T02:17:23Z","2017-02-06T19:38:33Z"
"","3206","KAFKA-5368 Kafka Streams skipped-records-rate sensor bug","This resolved the issue with Kafka Streams skipped records sensor reporting wrong values.  Jira ticket: https://issues.apache.org/jira/browse/KAFKA-5368  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","hrafzali","2017-06-02T10:27:46Z","2017-06-02T19:22:23Z"
"","3059","KAFKA-5244: Refactor BrokerTopicStats and ControllerStats so that they are classes","This removes the need to force object initialisation via hacks to register the relevant Yammer metrics during start-up.  It also works around issues caused by tests that delete JVM-wide singleton metrics (like `MetricsDuringTopicCreationDeletionTest`). Without this change, they would never be registered again. After this change, they will be registered again during KafkaServer start-up.  It would be even better not to rely on JVM side singleton metrics (like we do for Kafka Metrics), but that's a bigger change that should be considered separately.","closed","","ijuma","2017-05-15T18:28:37Z","2017-09-05T08:43:44Z"
"","2939","MINOR: Fix some lgtm.com alerts, ensuring certain input/output resources are closed.","This PR uses the try-with-resources construct to ensure that certain input and output resources are always closed, preventing possible memory leaks. This addresses some of the alerts found on lgtm.com: see the ""Close input resource"" and ""Close output resource"" rules at https://lgtm.com/projects/g/apache/kafka/alerts/.  Type arguments have been added in one unrelated place to get one of the modified files to compile.","open","","adityasharad","2017-04-28T22:19:35Z","2018-03-02T19:30:06Z"
"","2808","KIP-101: Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation","This PR replaces https://github.com/apache/kafka/pull/2743 (just raising from Confluent repo)  This PR describes the addition of Partition Level Leader Epochs to messages in Kafka as a mechanism for fixing some known issues in the replication protocol. Full details can be found here:  [KIP-101 Reference](https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation)  *The key elements are*: - Epochs are stamped on messages as they enter the leader. - Epochs are tracked in both leader and follower in a new checkpoint file.  - A new API allows followers to retrieve the leader's latest offset for a particular epoch.  - The logic for truncating the log, when a replica becomes a follower, has been moved from Partition into the ReplicaFetcherThread - When partitions are added to the ReplicaFetcherThread they are added in an initialising state. Initialising partitions request leader epochs and then truncate their logs appropriately.   This test provides a good overview of the workflow `EpochDrivenReplicationProtocolAcceptanceTest.shouldFollowLeaderEpochBasicWorkflow()`  The corrupted log use case is covered by the test   `EpochDrivenReplicationProtocolAcceptanceTest.offsetsShouldNotGoBackwards()`  Remaining work: There is a do list here: https://docs.google.com/document/d/1edmMo70MfHEZH9x38OQfTWsHr7UGTvg-NOxeFhOeRew/edit?usp=sharing","closed","","benstopford","2017-04-04T23:09:08Z","2020-01-25T16:56:59Z"
"","3049","MINOR: removed code duplicates from several files (KafkaStreams)","This PR offers following to KafkaStreams project: * removed code duplication from **TopologyBuilder**'s function **makeNodeGroups** (extract function) * removed code duplication from **GlobalProcessorContextImpl** and from **ProcessorContextImpl** to parent class **AbstractProcessorContext** (move method to parent)","closed","","wlsc","2017-05-14T09:04:41Z","2017-12-28T09:01:42Z"
"","2857","KAFKA-5049 Chroot check should be done for each ZkUtils instance","This PR is for https://issues.apache.org/jira/browse/KAFKA-5049?jql=project%20%3D%20KAFKA%20AND%20labels%20%3D%20newbie%20AND%20status%20%3D%20Open This is for enabling check for chroot for each Zkutils instance.","closed","","anukin","2017-04-15T07:01:45Z","2017-04-19T15:34:53Z"
"","2932","KAFKA-5137 : Controlled shutdown timeout message improvement","This PR improves the warning message by adding correct config details.","closed","","umesh9794","2017-04-28T05:23:55Z","2017-05-05T04:13:07Z"
"","2946","KAFKA-4218: Enable access to key in ValueTransformer","This PR groups KAFKA-4218,  KAFKA-4726 and KAFKA-3745.","closed","","jeyhunkarimov","2017-04-30T17:17:38Z","2017-07-25T04:59:05Z"
"","2522","KAFKA-4716: Send request to controller","This PR fixes a blocker issue, where the streams client code cannot talk to the controller. It also enables a system test that was previously failing.   This PR is for trunk only. A separate PR with just the fix (but not the tests) will be created for 0.10.2.","closed","","enothereska","2017-02-09T10:44:21Z","2017-02-09T22:01:56Z"
"","2743","KIP-101: Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation","This PR describes the addition of Partition Level Leader Epochs to messages in Kafka as a mechanism for fixing some known issues in the replication protocol. Full details can be found here:  [KIP-101 Reference](https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation)  *The key elements are*: - Epochs are stamped on messages as they enter the leader. - Epochs are tracked in both leader and follower in a new checkpoint file.  - A new API allows followers to retrieve the leader's latest offset for a particular epoch.  - The logic for truncating the log, when a replica becomes a follower, has been moved from Partition into the ReplicaFetcherThread - When partitions are added to the ReplicaFetcherThread they are added in an initialising state. Initialising partitions request leader epochs and then truncate their logs appropriately.   This test provides a good overview of the workflow `EpochDrivenReplicationProtocolAcceptanceTest.shouldFollowLeaderEpochBasicWorkflow()`  The corrupted log use case is covered by the test   `EpochDrivenReplicationProtocolAcceptanceTest.offsetsShouldNotGoBackwards()`  Remaining work: The test `EpochDrivenReplicationProtocolAcceptanceTest.shouldSurviveFastLeaderChange()` doesn't correctly reproduce the underlying issue. This will be altered later to properly support this use case.","closed","","benstopford","2017-03-27T16:21:57Z","2017-06-29T22:27:39Z"
"","2831","KAFKA-5036 (Second part: Points 2 -> 5): Refactor caching of Latest Epoch","This PR covers point (2) and point (5) from KAFKA-5036:  **Commit 1:**  2. Currently, we update the leader epoch in epochCache after log append in the follower but before log append in the leader. It would be more consistent to always do this after log append. This also avoids issues related to failure in log append. 5. The constructor of LeaderEpochFileCache has the following: lock synchronized { ListBuffer(checkpoint.read(): _*) } But everywhere else uses a read or write lock. We should use consistent locking. This is a refactor to the way epochs are cached, replacing the code to cache the latest epoch in the LeaderEpochFileCache by reusing the cached value in Partition. There is no functional change.  **Commit 2:** Adds an assert(epoch >=0) as epochs are written. Refactors tests so they never hit this assert.","closed","","benstopford","2017-04-10T13:04:04Z","2017-04-18T00:25:48Z"
"","2550","KAFKA-4665: Normalize handling of non-existing topics/partitions in fetching offsets","This PR brings some consistency around how non-existing topics or partitions are handled when fetching offsets using different versions of offset fetch API. In particular, it now * returns `UNKNOWN_TOPIC_OR_PARTITION` for non-existing topics or partitions in versions 1 and later (similar to how it is done in version 0) * throws a `KafkaException` when offset of a non-existing topic or partition is fetched, or a topic to which they do not have Describe access.","open","","vahidhashemian","2017-02-14T22:32:08Z","2018-03-02T19:29:56Z"
"","3051","KAFKA-5235: GetOffsetShell: new KafkaConsumer API, support for multiple topics, minimized number of requests to server","This PR addresses two improvements:  [KAFKA-5235 GetOffsetShell: retrieve offsets for all given topics and partitions with single request to the broker](https://issues.apache.org/jira/browse/KAFKA-5235) [KAFKA-5234 GetOffsetShell: retrieve offsets for multiple topics with single request](https://issues.apache.org/jira/browse/KAFKA-5234)  1. Previous implementation used SimpleConsumer to get offsets and old Producer API to get topic/partition metadata. Previous implementation determined a leader broker for each partition and then requested the leader for offsets. In total, it did as many requests to the broker as the number of partitions (plus a request to Zookeeper for metadata). New implementation `kafka-get-offsets.sh` uses KafkaConsumer API. It makes at most two requests to the broker: 1) to query existing topics and partitions, 2) to grab all requested offsets. New implementation correctly handles non-existing topics and partitions asked by user:  > kafka-get-offsets.sh --bootstrap-servers vm:9092 --topics AAA,ZZZ --partitions 0,1 > AAA:0:7 > AAA:1:Partition not found > ZZZ:0:Topic not found  2. Previously, user could get offsets for one topic only. Now user can get offsets for many topics at once: `kafka-get-offsets.sh --bootstrap-servers vm:9092 --topics AAA,ZZZ` Moreover, now user is able to retrieve offsets for _all_ topics - this is the default when no topics specified: `kafka-get-offsets.sh --bootstrap-servers vm:9092` Thanks to this feature, there is no need anymore to retrieve all topics by means of `kafka-topics.sh`. When no topics specified, the new `kafka-get-offsets.sh` tool takes into account only user-level topics and ignores Kafka-internal topics (i.e. consumer offsets). This behavior can be altered via a special command line argument: `kafka-get-offsets.sh --bootstrap-servers vm:9092 --include-internal-topics`  3. New `kafka-get-offsets.sh` tool is consistent with other console tools with respect to command line argument names. In addition, `kafka-get-offsets.sh` tool gives the possibility to pass an arbitrary setting to KafkaConsumer via `--consumer-property` argument. Old command line arguments are marked as deprecated with appropriate warning messages.  I hope, now `kafka-get-offsets.sh` is easier in use and gives performance improvement. @junrao I suppose you may want to review.","closed","","tashoyan","2017-05-14T18:40:33Z","2020-10-24T06:31:16Z"
"","2697","MINOR: fixed memory pollution in KafkaConsumer","This PL fixes memory pollution in KafkaConsumer by removing creation of the unnecessary HashSet each time we need to create unmodifiableSet and using unmodifiableMap when we just need to commit offsets.","closed","","wlsc","2017-03-16T20:50:30Z","2017-03-19T09:42:42Z"
"","3218","KAFKA-5373 : Revert breaking change to console consumer","This patch reverts b63e41ea78a58bdea78be33f90bfcb61ce5988d3 since it broke the console consumer -- the consumer prints the addresses of the messages instead of the contents with that patch.","closed","","apurvam","2017-06-02T23:35:27Z","2017-06-03T01:21:52Z"
"","3377","KAFKA-5477: Lower retryBackoff for AddPartitionsRequest","This patch lowers the retry backoff when receiving a CONCURRENT_TRANSACTIONS error from an AddPartitions request. The default of 100ms would mean that back to back transactions would be 100ms long at minimum, making things to slow.","closed","","apurvam","2017-06-20T00:38:13Z","2017-06-21T18:01:57Z"
"","2970","Kafka-5160; KIP-98 Broker side support for TxnOffsetCommitRequest","This patch adds support for the `TxnOffsetCommitRequest` added in KIP-98. Desired handling for this request is [described here](https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#bookmark=id.55yzhvkppi6m) .   The functionality includes handling the stable state of receiving `TxnOffsetCommitRequests` and materializing results only when the commit marker for the transaction is received. It also handles partition emigration and immigration and rebuilds the required data structures on these events.  Tests are included for all the functionality.","closed","","apurvam","2017-05-04T01:21:34Z","2017-05-18T16:27:42Z"
"","2995","KAFKA-3763: Remove deprecated APIs for 0.11.0.0","This only removes deprecated methods, fields and constructors in a small number of classes.  Deprecated producer configs is tracked via KAFKA-3353 and the old clients and related (tools, etc.) won't be removed in 0.11.0.0.","closed","","ijuma","2017-05-08T14:15:26Z","2017-06-18T09:33:08Z"
"","3143","KAFKA-5326:When resize the index file, maybe cause the content disappear","This modify to resolved the [KAFKA-5326](https://issues.apache.org/jira/browse/KAFKA-5326).","open","","ZanderXu","2017-05-25T13:30:02Z","2017-07-12T23:27:15Z"
"","3150","KAFKA-5332:When resize the index file, maybe caused the content disappear","this modify is to fix [KAFKA-5332](https://issues.apache.org/jira/browse/KAFKA-5332)","open","","ZanderXu","2017-05-26T08:47:32Z","2018-03-02T19:30:12Z"
"","3151","KAFKA-5331:MirrorMaker can't guarantee data is consistent between clusters","this modify is to fix [KAFKA-5331](https://issues.apache.org/jira/browse/KAFKA-5331)","closed","","ZanderXu","2017-05-26T09:34:18Z","2022-02-10T16:18:23Z"
"","3177","KAFKA-5353: baseTimestamp should always have a create timestamp","This makes the case where we build the records from scratch consistent with the case where update the batch header ""in place"". Thanks to @edenhill who found the issue while testing librdkafka.  The reason our tests don’t catch this is that we rely on the maxTimestamp to compute the record level timestamps if log append time is used.","closed","","ijuma","2017-05-31T12:05:35Z","2017-09-05T08:39:39Z"
"","2992","KIP-147 WIP for discussion","This is WIP to help discussion, do not merge.","closed","","mihbor","2017-05-07T18:12:03Z","2018-01-26T19:13:53Z"
"","3451","MINOR: Implement `toString` in some Validator instances","This is used in the generated config table. Also fix a couple of typos in the process.","closed","","ijuma","2017-06-28T10:21:41Z","2017-09-05T08:38:20Z"
"","3423","KAFKA-5157: Options for handling corrupt data during deserialization","This is the implementation of KIP-161: https://cwiki.apache.org/confluence/display/KAFKA/KIP-161%3A+streams+deserialization+exception+handlers","closed","","enothereska","2017-06-23T13:35:00Z","2017-07-10T20:06:44Z"
"","2832","KAFKA-5045: KTable cleanup","This is the implementation of KIP-114: KTable state stores and improved semantics: - Allow for decoupling between querying and materialisation - consistent APIs, overloads with queryableName and without - depreciated several KTable calls  - new unit and integration tests  In this implementation, state stores are materialized if the user desires them to be queryable. In subsequent versions we can offer a second option, to have a view-like state store. The tradeoff then would be between storage space (materialize) and re-computation (view). That tradeoff can be exploited by later query optimizers.","closed","","enothereska","2017-04-10T14:07:45Z","2017-05-04T06:46:38Z"
"","3371","KAFKA-5470: Replace -XX:+DisableExplicitGC with -XX:+ExplicitGCInvokesConcurrent in kafka-run-class","This is important because Bits.reserveMemory calls System.gc() hoping to free native memory in order to avoid throwing an OutOfMemoryException. This call is currently a no-op due to -XX:+DisableExplicitGC.  It's worth mentioning that -XX:MaxDirectMemorySize can be used to increase the amount of native memory available for allocation of direct byte buffers.","closed","","ijuma","2017-06-19T15:18:44Z","2017-08-22T06:37:26Z"
"","2735","KAFKA-4817; Add idempotent producer semantics","This is from the KIP-98 proposal.   The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.  There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.  In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following in the error callback:  1. Close the producer so that no queued batches are sent. This is important for guaranteeing ordering. 2. Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.","closed","","apurvam","2017-03-24T21:26:41Z","2020-01-25T16:56:46Z"
"","3366","MINOR: Remove version in the uses page","This is contributed by Hao Chen.","closed","","guozhangwang","2017-06-19T00:11:08Z","2017-07-15T22:06:55Z"
"","2973","KAFKA-5171 : TC should not accept empty string transactional id","This is an initial PR. Changed the unit tests accordingly as per the expectation from TC.","closed","","umesh9794","2017-05-04T05:00:50Z","2017-05-27T04:06:22Z"
"","3086","KAFKA-5171 : TC should not accept empty string transactional id","This is a rebase version of [PR#2973](https://github.com/apache/kafka/pull/2973).   @guozhangwang , please review this updated PR.","closed","","umesh9794","2017-05-18T12:28:18Z","2017-05-18T18:41:22Z"
"","2709","MINOR: Map `mkString` format updated to default java format","This is a minor change but it helps to improve the log readability.","closed","","kamalcph","2017-03-20T12:56:55Z","2017-03-31T07:00:20Z"
"","3023","KAFKA-4317: Checkpoint StateStores on commit interval","This is a backport of: https://github.com/apache/kafka/pull/2471 from trunk","closed","","dguy","2017-05-11T11:52:10Z","2017-05-16T14:04:42Z"
"","3024","KAFKA-4317: Checkpoint state stores on commit interval","This is a backport of https://github.com/apache/kafka/pull/2471","closed","","dguy","2017-05-11T11:53:18Z","2017-05-16T14:04:42Z"
"","2781","MINOR: fix flaky StateDirectoryTest","This fixes: ``` java.lang.AssertionError: expected:<2> but was:<3> 	at org.junit.Assert.fail(Assert.java:88) 	at org.junit.Assert.failNotEquals(Assert.java:834) 	at org.junit.Assert.assertEquals(Assert.java:645) 	at org.junit.Assert.assertEquals(Assert.java:631) 	at org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldCleanUpTaskStateDirectoriesThatAreNotCurrentlyLocked(StateDirectoryTest.java:145) ```  While running test in infinite loop, hit other problems:  - fixed file management (release all locks and close everything)  - increased sleep time for `shouldCleanupStateDirectoriesWhenLastModifiedIsLessThanNowMinusCleanupDelay` too (was flaky as well)","closed","","mjsax","2017-03-31T22:23:12Z","2017-04-06T00:18:59Z"
"","2912","KAFKA-4942 Fix commitTimeoutMs being set before the commit actually started","This fixes KAFKA-4942  This supersededs #2730   /cc @simplesteph @gwenshap @ewencp","closed","connect,","56quarters","2017-04-25T15:11:47Z","2020-10-16T06:08:14Z"
"","2730","KAFKA-4942 fixed the commitTimeoutMs being set before the commit actually started","this fixes KAFKA-4942","closed","","simplesteph","2017-03-24T00:03:58Z","2017-06-06T22:32:35Z"
"","2680","KAFKA-4859: Raised Timeout","This fixes https://issues.apache.org/jira/browse/KAFKA-4859 for me over hundreds of iterations while I could easily reproduce it with less than ~30-40 iterations without the increased timeout.  Also raising the timeout (at least on my setup) looks like a valid approach looking at test runtimes being consistently slightly above those 30s default timeout coming from `org.apache.kafka.streams.integration.utils.IntegrationTestUtils#DEFAULT_TIMEOUT`.  ![byregion](https://cloud.githubusercontent.com/assets/6490959/23878174/1eac8154-0846-11e7-82a7-9e04f235630f.png)","closed","","original-brownbear","2017-03-13T22:40:33Z","2017-03-14T21:42:33Z"
"","2982","KAFKA-5174: Have at least 2 threads","This fix needs to be backported to 0.10.2 as well.","closed","","enothereska","2017-05-05T14:01:38Z","2017-05-09T06:53:06Z"
"","2833","KAFKA-5050 add ldap authentication support on PlainSaslServer","This feature will provide a way to add ldap authentication on PlainSaslServer.  Our Jaas Server configuration will be :  KafkaServer {             org.apache.kafka.common.security.plain.PlainLdapLoginModule REQUIRED             username=""cent""             password=""redhat""             source=""ldap""             ldap_user_template=""uid={0},ou=People,dc=srv,dc=world""             ldap_url=""ldap://192.168.15.247:389""; };  I tested my code with existing tests and everything looks good. I will create another ticket later to add new unit tests.","open","","nihed","2017-04-10T16:29:21Z","2018-03-02T19:30:03Z"
"","2639","KAFKA-3155: Avoid Long Overflow in org.apache.kafka.clients.producer.internals.RecordBatch#maybeExpire","This deals with https://issues.apache.org/jira/browse/KAFKA-3155  This is a really trivial one :) The problem is that `Long.MaxValue` for the linger overflows in `org.apache.kafka.clients.producer.internals.RecordBatch#maybeExpire` when added the current timestamp like so:  ![longoverflow](https://cloud.githubusercontent.com/assets/6490959/23581016/f1852054-010c-11e7-9a7f-574937b5060f.png)  Then causing an error to be set for the batch by `Sender` like so (not happening every time since this depends on the timing of `Sender`):  ![error](https://cloud.githubusercontent.com/assets/6490959/23581032/204725c2-010d-11e7-9553-0423e826e34c.png)  That error then causes a call to `org.apache.kafka.clients.producer.internals.ProduceRequestResult#done` on the batch, as can be seen in the second screenshot's stacktrace ... which then makes the check for ""not done"" fail.  ~~Long story short ... trivial to fix by using 1h as linger instead of `Long.MaxValue` prevents the overflow and fixes the instability.~~ As requested fixed by saturated long addition :)","closed","","original-brownbear","2017-03-04T18:04:28Z","2017-03-06T21:21:40Z"
"","3217","KAFKA-5366: Add concurrent reads to transactions system test","This currently fails in multiple ways. One of which is most likely KAFKA-5355, where the concurrent consumer reads duplicates.  During broker bounces, the concurrent consumer misses messages completely. This is another bug.","closed","","apurvam","2017-06-02T22:31:29Z","2017-06-07T00:24:27Z"
"","2785","KAFKA-2961: Add overload for subscribing to a single partition","This contribution is my original work and I license the work to the project under the project's open source license.","open","","TimoMeijer","2017-04-01T13:28:23Z","2018-03-02T19:30:01Z"
"","2784","KAFKA-4999: Add convenience overloads to seek* methods","This contribution is my original work and I license the work to the project under the project's open source license.","open","","TimoMeijer","2017-04-01T12:55:25Z","2018-03-02T19:30:01Z"
"","2672","KAFKA-4657: Improve test coverage of CompositeReadOnlyWindowStore","This commmit brings improved test coverage for window store fetch method and WindowStoreIterator","closed","","adyach","2017-03-10T21:54:11Z","2017-03-13T19:06:52Z"
"","2882","KAFKA-5008: Provide OSGi metadata for Kafka-Clients","This change uses the bnd-gradle-plugin for the kafka-clients module in order to generate OSGi metadata. The bnd.bnd file is used by the plugin for osgi-instructions. All packages from the clients-artifact are exported. Import-Package statements are automatically calculated by bnd.  Signed-off-by: Marc Schlegel","closed","","lostiniceland","2017-04-21T07:14:49Z","2021-03-04T20:48:39Z"
"","2807","KAFKA-5008: Provide OSGi metadata for Kafka-Clients","This change uses the bnd-gradle-plugin for the kafka-clients module in order to generate OSGi metadata. The bnd.bnd file is used by the plugin for instructions. For now, all packages are exported.  Signed-off-by: Marc Schlegel","closed","","lostiniceland","2017-04-04T21:10:41Z","2017-04-21T07:15:28Z"
"","3358","KAFKA-5463: Controller incorrectly logs rack information when new brokers are added","This change is in response to https://issues.apache.org/jira/browse/KAFKA-5463.  When a new broker is added, the `ControllerChannelManager` doesn't log rack information even if configured.  This happens because `ControllerChannelManager` always instantiates a `Node` using the same constructor whether or not rack-aware is configured and causes some confusion when running with rack-aware replica placement.  Before:  ``` pri=TRACE t=Controller-1-to-broker-0-send-thread at=logger Controller 1 epoch 1 received response {error_code=0} for a request sent to broker : (id: 0 rack: null) ```  After:  ``` pri=TRACE t=Controller-1-to-broker-0-send-thread at=logger Controller 1 epoch 1 received response {error_code=0} for a request sent to broker : (id: 0 rack: us-east-1d) ```","closed","","jeffchao","2017-06-16T22:02:47Z","2017-06-17T07:54:45Z"
"","2496","KAFKA-4725: Stop leaking messages in produce request body when requests are delayed","This change is in response to [KAFKA-4725](https://issues.apache.org/jira/browse/KAFKA-4725).   When a produce request is received, if the user/client is exceeding their produce quota, the response will be delayed until the quota is refilled appropriately.   Unfortunately, the request body is still referenced in the callback which in turn leaks the messages contained within the request.   This change allows the `KafkaApis` method to take ownership of the request body from the `RequestChannel.Request` object.   I am not sure whether this breaks other invariants which are assumed within other parts of Kafka.","closed","","halorgium","2017-02-03T21:57:44Z","2017-02-07T23:13:35Z"
"","2524","HOTFIX: default params missing","This caused the bounce and smoke tests to fail on trunk.","closed","","enothereska","2017-02-09T14:14:26Z","2017-02-09T15:38:41Z"
"","3139","MINOR: fix flakiness in testDeleteAcls","This call to isCompletedExceptionally introduced a race condition because the future might not have been completed.  assertFutureError checks that the exception is present and of the correct type in any case, so the call was not necessary.","closed","","cmccabe","2017-05-24T21:32:04Z","2019-05-20T18:41:10Z"
"","2746","KAFKA-4959: remove controller concurrent access to non-threadsafe NetworkClient, Selector, and SSLEngine","This brought down a cluster by causing continuous controller moves.  ZkClient's ZkEventThread and a RequestSendThread can concurrently use objects that aren't thread-safe: * Selector * NetworkClient * SSLEngine (this was the big one for us. We turn on SSL for interbroker communication).  As per the ""Concurrency Notes"" section from https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/SSLEngine.html: > two threads must not attempt to call the same method (either wrap() or unwrap()) concurrently  SSLEngine.wrap gets called in: * SslTransportLayer.write * SslTransportLayer.handshake * SslTransportLayer.close  It turns out that the ZkEventThread and RequestSendThread can concurrently call SSLEngine.wrap: * ZkEventThread calls SslTransportLayer.close from ControllerChannelManager.removeExistingBroker * RequestSendThread can call SslTransportLayer.write or SslTransportLayer.handshake from NetworkClient.poll  Suppose the controller moves for whatever reason. The former controller could have had a RequestSendThread who was in the middle of sending out messages to the cluster while the ZkEventThread began executing KafkaController.onControllerResignation, which calls ControllerChannelManager.shutdown, which sequentially cleans up the controller-to-broker queue and connection for every broker in the cluster. This cleanup includes the call to ControllerChannelManager.removeExistingBroker as mentioned earlier, causing the concurrent call to SSLEngine.wrap. This concurrent call throws a BufferOverflowException which ControllerChannelManager.removeExistingBroker catches so the ControllerChannelManager.shutdown moves onto cleaning up the next controller-to-broker queue and connection, skipping the cleanup steps such as clearing the queue, stopping the RequestSendThread, and removing the entry from its brokerStateInfo map.  By failing out of the Selector.close, the sensors corresponding to the broker connection has not been cleaned up. Any later attempt at initializing an identical Selector will result in a sensor collision and therefore cause Selector initialization to throw an exception. In other words, any later attempts by this broker to become controller again will fail on initialization. When controller initialization fails, the controller deletes the /controller znode and lets another broker take over.  Now suppose the controller moves enough times such that every broker hits the BufferOverflowException concurrency issue. We're now guaranteed to fail controller initialization due to the sensor collision on every controller transition, so the controller will move across brokers continuously.  This patch avoids the concurrent use of non-threadsafe classes in ControllerChannelManager.removeExistingBroker by shutting down the RequestSendThread before closing the NetworkClient.","closed","","onurkaraman","2017-03-27T22:05:44Z","2017-03-29T00:20:19Z"
"","3286","KAFKA-5415: Remove timestamp check in completeTransitionTo","This assertion is hard to get right because the system time can roll backward on a host due to NTP (as shown in the ticket), and also because a transaction can start on one host and complete on another. Getting precise clock times across hosts is virtually impossible, and this check makes things fragile.","closed","","apurvam","2017-06-09T22:53:53Z","2017-06-10T00:09:38Z"
"","2538","KAFKA-2857: Retry querying the consumer group while initializing","This applies to new-consumer based groups and would avoid scenarios in which user issues a `--describe` query while the group is initializing. Example: The following could occur for a newly created group. ``` kafka@kafka:~/workspace/kafka$ bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group g Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).  Error: Executing consumer group command failed due to The group coordinator is not available. ```  With this PR the group is queried repeatedly at specific intervals within a preset (and configurable) timeout `group-init-timeout` to circumvent unfortunate situations like above.","closed","","vahidhashemian","2017-02-10T20:08:55Z","2017-03-03T23:41:48Z"
"","2728","KAFKA 3514: alternate calculation for min timestamp [WiP]","This alternate calculation reduces the amount of buffering kept and is optimised for cases when out of order records are rare.","closed","streams,","enothereska","2017-03-23T16:43:35Z","2018-01-30T02:44:11Z"
"","3396","KAFKA-4931: stop script fails due 4096 ps output limit","This also fixes KAFKA-4389 and KAFKA-4297, which were exactly the same issue but for kafka-server-stop.sh.","open","","tombentley","2017-06-21T13:30:52Z","2018-03-02T19:30:18Z"
"","2677","MINOR: set trace logging for zookeeper upgrade test","This adds logging which will hopefully help root cause https://issues.apache.org/jira/browse/KAFKA-4574.","closed","","apurvam","2017-03-13T17:33:33Z","2017-10-09T21:03:06Z"
"","3372","Provide link to ZooKeeper within Quickstart","This adds a hyperlink to the ZooKeeper project within the quickstart tutorial for users who may be unfamiliar with ZooKeeper or who simply want a more-direct path to downloading the software.","closed","","restlessdesign","2017-06-19T15:40:00Z","2017-06-22T13:05:19Z"
"","2867","KAFKA-4755: Cherrypick streams tests from trunk","This addresses some tests' instabilities in 0.10.2 that have been fixed in trunk","closed","","enothereska","2017-04-18T13:39:45Z","2017-04-25T19:19:04Z"
"","2570","KAFKA-4196: Improved Test Stability","This addresses https://issues.apache.org/jira/browse/KAFKA-4196  What I found was below warning accompanying all failures I was seeing from this test (reproduced instability by putting system under load):  ```sh [2017-02-18 16:17:42,892] WARN fsync-ing the write ahead log in SyncThread:0 took 20632ms which will adversely effect operation latency. See the ZooKeeper troubleshooting guide (org.apache.zookeeper.server.persistence.FileTxnLog:338) ```  ZK at times keeps locking for multiple seconds in tests (not only this one, but it's very frequent in this one for some reason). In this case (20s) the ZK locking lasted longer than the test timeout waiting only 15s (`org.apache.kafka.test.TestUtils#DEFAULT_MAX_WAIT_MS`) for the path `/admin/delete_topic/topic` to be deleted. The only way to really fix this in a portable manner (should mainly hit ext3 users) is to turn off ZK fsyncing (not really needed in UTs anyways) as far as I know. Did that here as described in (https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html) by setting  ```scala   sys.props.put(""zookeeper.observer.syncEnabled"", ""false"") ```  This should also help general test performance in my opinion.  Edit: Adjustment to KafkaApi merged separately already :)","closed","","original-brownbear","2017-02-18T16:39:14Z","2017-02-26T11:27:43Z"
"","2604","KAFKA-4794: Add access to OffsetStorageReader from SourceConnector","This a first attempt to implement Add access to OffsetStorageReader from Source Connector. I am not sure if I did it right so I prefer to ask you your feedback. I still need to write some tests.","closed","connect,","fhussonnois","2017-02-27T22:40:28Z","2020-05-25T01:09:55Z"
"","2947","KAFKA-5144 added 2 test cases","These two newly added test cases are currently failing","closed","","mihbor","2017-04-30T18:26:24Z","2017-05-01T08:49:03Z"
"","3170","KAFKA-5054: ChangeLoggingKeyValueByteStore delete and putIfAbsent should be synchronized","These involve multiple operations on the underlying store. The underlying store access is synchronized, but these methods aren't. Which may result in inconsistent results when using IQ","closed","","dguy","2017-05-30T17:18:43Z","2017-08-16T13:23:25Z"
"","2987","KAFKA-3353: Remove deprecated producer configs","These configs have been deprecated since 0.9.0.0: block.on.buffer.full, metadata.fetch.timeout.ms and timeout.ms","closed","","ijuma","2017-05-06T18:11:07Z","2017-06-18T09:33:12Z"
"","3354","KAFKA-4829: Improve log4j on Streams thread / task-level","These are the following improvements I made:  1. On stream thread level, INFO will be demonstrating `Completed xx tasks in yy ms` or `Completed rebalance with xx state in yy ms`,  2. On Stream thread cache level, INFO on `Flushed xx records`. 3. On Stream thread level, DEBUG on internal batched operations like `created xx tasks`, and TRACE on individual operation like `created x task`. 4. Also using `isTraceEnabled` on the critical path to reduce overhead of creating `Object[]`. 5. Minor cleanups in the code.","closed","","guozhangwang","2017-06-16T00:23:51Z","2017-07-15T22:06:52Z"
"","3299","MINOR: Remove unused `AdminUtils.fetchTopicMetadataFromZk` methods","These are internal methods with no tests and since we now have an `AdminClient`, we should remove them.","closed","","ijuma","2017-06-12T00:10:15Z","2018-02-08T21:15:34Z"
"","2628","MINOR: Bump version to 0.11.0.0-SNAPSHOT","There won't be a 0.10.3.0.","closed","","ijuma","2017-03-02T14:07:21Z","2017-09-05T09:06:34Z"
"","2634","MINOR: Standardised benchmark params for consumer and streams","There were some minor differences in the basic consumer config and streams config that are now rectified. In addition, in AWS environments the socket size makes a big difference to performance and I've tuned it up accordingly. I've also increased the number of records now that perf is higher.","closed","","enothereska","2017-03-03T21:21:22Z","2017-03-05T07:22:12Z"
"","2801","MINOR: Fix multiple org.apache.kafka.streams.KafkaStreams.StreamStateListener being instantiated","There should only be a single `org.apache.kafka.streams.KafkaStreams.StreamStateListener` to ensure synchronization of operations on `org.apache.kafka.streams.KafkaStreams.StreamStateListener#threadState`.","closed","","original-brownbear","2017-04-04T07:43:56Z","2017-04-05T10:41:02Z"
"","3159","[KAFKA-5338]There is a Misspell in ResetIntegrationTest","There is a Misspell in Annotations of ResetIntegrationTest.","closed","","hejiefang","2017-05-27T09:26:56Z","2017-05-29T23:03:16Z"
"","2742","KAFKA-4574: Ignore test_zk_security_upgrade until KIP-101 lands","The transient failures make it harder to spot real failures and we can live without what is being tested (adding security to ZK via a rolling upgrade) until KIP-101 lands.","closed","","ijuma","2017-03-27T12:53:10Z","2017-09-05T09:06:05Z"
"","3067","KAFKA-5175: Fix transient failure in ControllerIntegrationTest.testPreferredReplicaLeaderElection","The transient failure came from the controller processing the preferred replica leader election before the restarted broker (the preferred replica leader) has joined isr, causing preferred replica leader election to fail and for the final zookeeper state validation to fail.","closed","","onurkaraman","2017-05-16T07:48:38Z","2017-05-16T09:48:05Z"
"","2567","MINOR: Increase consumer init timeout in throttling test","The throttling system test sometimes fail because it takes longer than the current 10 second time out for partitions to get assigned to the consumer.","closed","","apurvam","2017-02-18T01:33:13Z","2017-02-18T14:39:14Z"
"","3038","KAFKA-5180: fix transient failure in ControllerIntegrationTest.testControllerMoveIncrementsControllerEpoch","The tests previously ignored the fact that the controller does not atomically create the /controller znode and create/increment the /controller_epoch znode.","closed","","onurkaraman","2017-05-12T23:59:33Z","2017-05-13T02:25:01Z"
"","2657","KAFKA-4861; GroupMetadataManager record is rejected if broker configured with LogAppendTime","The record should be created with CreateTime (like in the producer). The conversion to LogAppendTime happens automatically (if necessary).","closed","","ijuma","2017-03-08T22:22:57Z","2017-09-05T09:06:22Z"
"","2999","KAFKA-5197: add a tool analyzing zookeeper client performance across its various apis","The raw zookeeper client offers various means of getting and setting znodes. It would be useful to have a tool that lets you analyze the performance of these apis.","open","","onurkaraman","2017-05-08T21:43:40Z","2018-03-02T19:30:07Z"
"","2589","KAFKA-4779: Fix security upgrade system test to be non-disruptive","The phase_two security upgrade test verifies upgrading inter-broker and client protocols to the same value as well as different values. The second case currently changes inter-broker protocol without first enabling the protocol, disrupting produce/consume until the whole cluster is updated. This commit changes the test to be a non-disruptive upgrade test that enables protocols first (simulating phase one of upgrade).","closed","","rajinisivaram","2017-02-23T15:25:37Z","2017-02-24T17:24:56Z"
"","3056","HOTFIX: AddOffsetsToTxnResponse using incorrect schema in parse","The parse method was incorrectly referring to `ApiKeys.ADD_PARTITIONS_TO_TXN`","closed","","dguy","2017-05-15T14:39:19Z","2017-05-16T14:04:33Z"
"","3346","MINOR: Cleanups for TransactionsTest","The motivation is that KAFKA-5449 seems to indicate that producer instances can be shared across tests, and that producers from one test seem to be hitting brokers in another test.  So this patch does two things:  1. Make transactionsTest use random ports in each test case.  2. Clear producers and consumers between tests.","closed","","apurvam","2017-06-15T00:38:10Z","2017-06-15T18:10:06Z"
"","2694","MINOR: FetchRequest.Builder maxBytes for version <3","The maxBytes field should be set to DEFAULT_RESPONSE_MAX_BYTES, the same way as the constructor using the Struct does.  codeveloped with @mimaison","closed","","edoardocomar","2017-03-16T11:29:50Z","2017-03-30T11:58:39Z"
"","3247","MINOR: log4j template should accept log_level","The log_level parameter is used in system tests in kafka.py. However the log4j template accepted that parameter in only one place. This led to a large number of DEBUG lines printed even when the intention was to capture only INFO lines. Led to huge log files. Thanks to @ijuma for noticing this.","closed","","enothereska","2017-06-06T16:31:13Z","2017-06-07T16:21:03Z"
"","2790","the kafka-server-stop.sh can not find the pids.","the kafka-server-stop.sh can not find the pids of kafka main process. use pgrep to fix it.","open","","houming818","2017-04-02T06:43:08Z","2018-01-08T12:15:44Z"
"","3233","MINOR: Remove redundant volatile write in RecordHeaders","The JMH benchmark included shows that the redundant volatile write causes the constructor of `ProducerRecord` to take more than 50% longer:  ProducerRecordBenchmark.constructorBenchmark  avgt   15  24.136 ± 1.458  ns/op (before) ProducerRecordBenchmark.constructorBenchmark  avgt   15  14.904 ± 0.231  ns/op (after)","closed","","ijuma","2017-06-04T14:02:09Z","2017-06-18T09:27:03Z"
"","2485","KAFKA-3896: Fix KStream-KStream leftJoin in RepartitionIntegrationTest","The issue of transiently having duplicates is due to the bad design of the left join itself: in order to ignore the partial joined results such as `A:null`, it lets the producer to potentially send twice to source stream one and rely on all the following conditions to be true in order to pass the test:  1. `receiveMessages` happen to have fetched all the produced results and have committed offsets. 2. streams app happen to have completed sending all result data. 3. consumer used in `receiveMessages` will complete getting all messages in a single poll().  If any of the above is not true, the test fails.  Fixed this test to add a filter right after left join to filter out partial joined results. Minor cleanup on integration test utils.","closed","","guozhangwang","2017-02-02T06:56:29Z","2017-07-15T22:07:24Z"
"","2588","KAFKA-4788: Revert ""KAFKA-4092: retention.bytes should not be allowed to be less than segment.bytes""","The intent is good, but it needs to take into account broker configs as well. See KAFKA-4788 for more details.  This reverts commit 4ca5abe8ee7578f602fb7653cb8a09640607ea85.","closed","","ijuma","2017-02-23T13:23:28Z","2017-09-05T09:28:39Z"
"","2816","KAFKA-5028: convert kafka controller to a single-threaded event queue model","The goal of this ticket is to improve controller maintainability by simplifying the controller's concurrency semantics. The controller code has a lot of shared state between several threads using several concurrency primitives. This makes the code hard to reason about.  This ticket proposes we convert the controller to a single-threaded event queue model. We add a new controller thread which processes events held in an event queue. Note that this does not mean we get rid of all threads used by the controller. We merely delegate all work that interacts with controller local state to this single thread. With only a single thread accessing and modifying the controller local state, we no longer need to worry about concurrent access, which means we can get rid of the various concurrency primitives used throughout the controller.  Performance is expected to match existing behavior since the bulk of the existing controller work today already happens sequentially in the ZkClient’s single ZkEventThread.","closed","","onurkaraman","2017-04-06T05:37:31Z","2017-05-11T05:50:33Z"
"","2948","KAFKA-5144 renamed and added comments to make it clear what's going on","The descendingSubsequence is a misnomer. The linked list is actually arranged so that the lowest timestamp is first and larger timestamps are added to the end, therefore renamed to ascendingSubsequence. The minElem variable was also misnamed. It's actually the current maximum element as it's taken from the end of the list. Added comment to get() to make it clear it's returning the lowest timestamp.","closed","","mihbor","2017-04-30T18:38:06Z","2017-05-04T07:11:26Z"
"","3315","MINOR: Generate deprecated warning for static default quota config","The default client-id bandwidth quota config properties have been marked deprecated in the doc, but a warning may be useful before the property is removed in a future release.","closed","","rajinisivaram","2017-06-13T13:33:54Z","2017-06-13T15:00:14Z"
"","3402","KAFKA-5486: org.apache.kafka logging should go to server.log","The current config sends org.apache.kafka and any unspecified logger to stdout. They should go to `server.log` instead.","closed","","ijuma","2017-06-21T20:32:51Z","2017-09-05T08:39:03Z"
"","3125","KAFKA-5311: Support ExtendedDeserializer in Kafka Streams.","The contribution is my original work and that I license the work to the project under the project's open source license.","closed","","subnova","2017-05-23T10:49:39Z","2017-06-16T12:07:36Z"
"","3199","KAFKA-5311: Support ExtendedDeserializer in Kafka Streams.","The contribution is my original work and I license the work to the project under the project's open source license.  This is a clone of the previous PR #3125 but now against trunk.","closed","","subnova","2017-06-01T22:05:30Z","2017-06-02T18:47:51Z"
"","3357","KAFKA-5413: Log cleaner fails due to large offset in segment file","the contribution is my original work and I license the work to the project under the project's open source license.  @junrao , I had already made the code change before your last comment.  I've done pretty much what you said, except that I've not used the current segment because I wasn't sure if it will always be available. I'm happy to change it if you prefer. I've run all the unit and integration tests which all passed.","closed","","kelvinrutt","2017-06-16T18:41:37Z","2017-06-21T16:14:12Z"
"","2600","KAFKA-4806: Prevent double logging of ConsumerConfig.","The consumer properties get logged twice since two instances of ConsumerConfig are created during creation of KafkaConsumer.  I added a constructor of ConsumerConfig accepting the boolean parameter doLog which is already passable in AbstractConfig and set it to false during the second ConsumerConfig creating in the KafkaConsumer constructor.","closed","","Gacko","2017-02-26T19:15:56Z","2017-02-27T11:50:14Z"
"","3167","MINOR: Remove unused configuration setting","The configuration setting zookeeper.sync.time.ms was not used for anything. Remove it to reduce confusion.  This changes rebalance.backoff.ms from defaulting to the value of zookeeper.sync.time.ms to default to 2000 (zookeeper.sync.time.ms's default). This means that if someone has not set rebalance.backoff.ms but has changed zookeeper.sync.time.ms from its default, then rebalance.backoff.ms will now instead be 2000ms. I'd be surprised if there were users who do this intentionally.","open","","reftel","2017-05-30T14:07:43Z","2018-03-02T19:30:12Z"
"","3043","KAFKA-5232 Fix Log.parseTopicPartitionName to take into account dot character in topic names of deleted topics","The commit here contains a fix and testcase for the issue reported in https://issues.apache.org/jira/browse/KAFKA-5232, where if a topic marked for deletion, has a `.` character in its name then the Kafka broker fails to start and keeps shutting down","closed","","jaikiran","2017-05-13T12:26:04Z","2017-05-17T14:38:54Z"
"","3050","(trunk) KAFKA-5232 Fix Log.parseTopicPartitionName to take into account dot character in topic names of deleted topics","The commit here contains a fix and a test case for the issue reported in https://issues.apache.org/jira/browse/KAFKA-5232. This PR is meant for `trunk` branch and a corresponding PR for `0.10.2` branch has been raised here https://github.com/apache/kafka/pull/3043","closed","","jaikiran","2017-05-14T14:18:25Z","2017-05-15T08:18:54Z"
"","2794","HOTFIX: Set `baseOffset` correctly in `RecordAccumulator`","The bug meant that the base offset was the same as the batch size instead of 0 so the broker would always recompress batches.","closed","","ijuma","2017-04-03T13:18:27Z","2017-09-05T09:05:11Z"
"","3132","KAFKA-5147: Add missing synchronization to TransactionManager","The basic idea is that exactly three collections, ie. `pendingRequests`, `newPartitionsToBeAddedToTransaction`, and `partitionsInTransaction` are accessed from the context of application threads. The first two are modified from the application threads, and the last is read from those threads.   So to make the `TransactionManager` truly thread safe, we have to ensure that all accesses to these three members are done in a synchronized block. I inspected the code, and I believe this patch puts the synchronization in all the correct places.","closed","","apurvam","2017-05-24T05:34:25Z","2017-05-25T23:26:48Z"
"","2809","CPKAFKA-465 : Exactly once transactional producer -- initial implementation","The basic flow works: find coordinator, get pid, add partitions to transaction, commit/abort.  Still to add: 1. 'sendOffsets' implementation. 2. error handling. 3. failure test cases.","closed","","apurvam","2017-04-05T06:37:13Z","2017-04-05T06:40:59Z"
"","2984","KIP-154 KAFKA-4667 Connect uses AdminClient to create internal topics when needed","The backing store for offsets, status, and configs now attempts to use the new AdminClient to look up the internal topics and create them if they don’t yet exist. If the necessary APIs are not available in the connected broker, the stores fall back to the old behavior of relying upon auto-created topics. Kafka Connect requires a minimum of Apache Kafka 0.10.0.1-cp1, and the AdminClient can work with all versions since 0.10.0.0.  All three of Connect’s internal topics are created as compacted topics, and new distributed worker configuration properties control the replication factor for all three topics and the number of partitions for the offsets and status topics; the config topic requires a single partition and does not allow it to be set via configuration. All of these new configuration properties have sensible defaults, meaning users can upgrade without having to change any of the existing configurations. In most situations, existing Connect deployments will have already created the storage topics prior to upgrading.  The replication factor defaults to 3, so anyone running Kafka clusters with fewer nodes than 3 will receive an error unless they explicitly set the replication factor for the three internal topics. This is actually desired behavior, since it signals the users that they should be aware they are not using sufficient replication for production use.  The integration tests use a cluster with a single broker, so they were changed to explicitly specify a replication factor of 1 and a single partition.  The `KafkaAdminClientTest` was refactored to extract a utility for setting up a `KafkaAdminClient` with a `MockClient` for unit tests.","closed","connect,","rhauch","2017-05-05T17:58:11Z","2020-10-16T06:29:09Z"
"","3477","KAFKA-1044: eliminating log4j from core","The aim of this PR to move kafka core/main away from log4j and introduce using slf4j only. To accomplish this task I: - refactored Log4jController into its own module as it is very tightly coupled with log4j (and removing these dependencies would have been impossible without feature loss). - as log4j supports FATAL level but slf4j doesn't, I introduced a FATAL marker similarly to the log4j-slf4j bridge","closed","","viktorsomogyi","2017-07-03T07:46:34Z","2018-08-23T13:09:55Z"
"","3408","KAFKA-5490: Skip empty record batches in the consumer","The actual fix for KAFKA-5490 is in https://github.com/apache/kafka/pull/3406.  This is just the consumer change that will allow the cleaner to use empty record batches without breaking 0.11.0.0 consumers (assuming that KAFKA-5490 does not make the cut). This is a safe change even if we decide to go with a different option for KAFKA-5490 and I'd like to include it in RC2.","closed","","ijuma","2017-06-22T12:01:43Z","2017-09-05T08:38:56Z"
"","3343","MINOR: Add transactionalId in more log lines in the producer.","The `kafka.api.TransactionsTest.testReadCommittedConsumerShouldNotSeeUndecidedData` very rarely sees the following.   I have run it 700 times locally without failure, so it only happens on jenkins.  this PR adds trace logging to the client. Will keep running the PR builder here and hope that the test fails again so that we can understand what's going on.  It is strange that we have an ongoing send when we are in `READY` state. It is even more strange that we see a `ProducerFencedException` in the log. Could it be that some other run is interfering with this one (since multiple test cases use the same producer ids) ?  ``` [2017-06-13 23:58:09,644] ERROR Aborting producer batches due to fatal error (org.apache.kafka.clients.producer.internals.Sender:381) org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker. [2017-06-13 23:58:10,177] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic2,3] to broker 0:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:10,177] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic2,0] to broker 0:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:10,178] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic1,2] to broker 0:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:12,128] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472) [2017-06-13 23:58:12,134] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472) [2017-06-13 23:58:12,310] ERROR [ReplicaFetcherThread-0-1]: Error for partition [topic1,0] to broker 1:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:12,311] ERROR [ReplicaFetcherThread-0-1]: Error for partition [topic1,3] to broker 1:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:15,998] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472) [2017-06-13 23:58:16,005] ERROR ZKShutdownHandler is not registered, so ZooKeeper server won't take any action on ERROR or SHUTDOWN server state changes (org.apache.zookeeper.server.ZooKeeperServer:472) [2017-06-13 23:58:16,177] ERROR [ReplicaFetcherThread-0-2]: Error for partition [topic1,2] to broker 2:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:16,177] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic1,3] to broker 0:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:16,178] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic1,0] to broker 0:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:28,177] ERROR Uncaught error in kafka producer I/O thread:  (org.apache.kafka.clients.producer.internals.Sender:164) org.apache.kafka.common.KafkaException: Invalid transition attempted from state READY to state ABORTABLE_ERROR 	at org.apache.kafka.clients.producer.internals.TransactionManager.transitionTo(TransactionManager.java:476) 	at org.apache.kafka.clients.producer.internals.TransactionManager.transitionToAbortableError(TransactionManager.java:289) 	at org.apache.kafka.clients.producer.internals.Sender.failBatch(Sender.java:601) 	at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:272) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:223) 	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:162) 	at java.lang.Thread.run(Thread.java:745) [2017-06-13 23:58:33,243] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic2,3] to broker 0:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:33,245] ERROR [ReplicaFetcherThread-0-0]: Error for partition [topic2,0] to broker 0:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread:99) [2017-06-13 23:58:35,665] WARN caught end of stream exception (org.apache.zookeeper.server.NIOServerCnxn:368) EndOfStreamException: Unable to read additional data from client sessionid 0x15ca3e4bdc30001, likely client has closed socket ```","closed","","apurvam","2017-06-14T21:21:53Z","2017-06-15T21:44:56Z"
"","2759","MINOR: Move `Os` class to utils package and rename it to OperatingSystem","The `common` package is public and this class is internal.","closed","","ijuma","2017-03-29T15:16:16Z","2017-09-05T09:05:55Z"
"","2737","MINOR: Fix deserialization of abortedTransactions and lastStableOffset in FetchResponse","Thanks to Dong Lin for finding the lastStableOffset issue.","closed","","ijuma","2017-03-25T16:45:42Z","2017-09-05T09:06:09Z"
"","2916","WIP: Avoid FileInputStream and FileOutputStream","Testing PR builder.","closed","","ijuma","2017-04-25T23:44:56Z","2017-04-28T01:24:48Z"
"","2853","KAFKA-5069: add controller integration tests","Test the various controller protocols by observing zookeeper and broker state.","closed","","onurkaraman","2017-04-14T01:56:50Z","2017-04-18T23:40:51Z"
"","3164","KAFKA-5150: Reduce lz4 decompression overhead (without thread local buffers)","Temporary PR that has additional changes over https://github.com/apache/kafka/pull/2967 for comparison.","closed","","ijuma","2017-05-29T18:44:20Z","2017-06-18T09:30:49Z"
"","3427","KAFKA-5501: introduce async ZookeeperClient","Synchronous zookeeper writes means that we wait an entire round trip before doing the next write. With respect to the controller, these synchronous writes are happening at a per-partition granularity in several places, so partition-heavy clusters suffer from the controller doing many sequential round trips to zookeeper. - PartitionStateMachine.electLeaderForPartition updates leaderAndIsr in zookeeper on transition to OnlinePartition. This gets triggered per-partition sequentially with synchronous writes during controlled shutdown of the shutting down broker's replicas for which it is the leader. - ReplicaStateMachine updates leaderAndIsr in zookeeper on transition to OfflineReplica when calling KafkaController.removeReplicaFromIsr. This gets triggered per-partition sequentially with synchronous writes for failed or controlled shutdown brokers.","closed","","onurkaraman","2017-06-24T09:50:46Z","2017-07-26T08:34:09Z"
"","2864","KAFKA-5078; PartitionRecords.fetchRecords(...) should defer exception to the next call if iterator has already moved across any valid record","Suppose there are two valid records followed by one invalid records in the FetchResponse.PartitionData(). As of current implementation, PartitionRecords.fetchRecords(...) will throw exception without returning the two valid records. The next call to PartitionRecords.fetchRecords(...) will not return that two valid records either because the iterator has already moved across them.  We can fix this problem by defering exception to the next call of PartitionRecords.fetchRecords(...) if iterator has already moved across any valid record.","closed","","lindong28","2017-04-17T19:57:49Z","2017-06-29T03:44:16Z"
"","3157","KAFKA-5226: Fixes issue where adding topics matching a regex","subscribed stream may not be detected by all followers until onJoinComplete returns.","closed","","bbejeck","2017-05-26T21:47:59Z","2017-06-01T02:37:13Z"
"","2673","KAFKA-4858: Broker should ignore long topic names","Starting from the 0.10.0.0 release topics names of over 249 characters are not allowed (`kafka-topics.sh` would disallow the creation). However, it is possible to use an older client version against the more recent brokers and attempt to create topics with longer names. This causes problems with the broker when trying to bring the corresponding partitions online (as described in the [JIRA](https://issues.apache.org/jira/browse/KAFKA-4858)). This PR makes the broker ignore those topics to avoid the reported failures.","open","","vahidhashemian","2017-03-10T23:00:14Z","2018-03-02T19:29:58Z"
"","2630","MINOR: improve hanging ResetIntegrationTest","Sometimes `ResetIntegrationTest` hangs and thus the build times out. We suspect, that this happens if no data is written into the input topics. Right now, input data is written once and reused for both test cases. If for some reason, the broker gets recreated (between both test cases), no data will be available for the second test method and thus the test hangs.  This change ensures, that input data is written for each test case individually.","closed","","mjsax","2017-03-03T00:02:40Z","2017-03-22T17:13:26Z"
"","2712","KAFKA-4921: AssignedPartition implements equals","Solves: [KAFKA-4921](https://issues.apache.org/jira/browse/KAFKA-4921)  Bug type EQ_COMPARETO_USE_OBJECT_EQUALS (click for details) In class org.apache.kafka.streams.processor.internals.StreamPartitionAssignor$AssignedPartition In method org.apache.kafka.streams.processor.internals.StreamPartitionAssignor$AssignedPartition.compareTo(StreamPartitionAssignor$AssignedPartition) At StreamPartitionAssignor.java:[line 75]","closed","streams,","mjuchli","2017-03-20T15:42:20Z","2018-02-26T18:40:09Z"
"","2711","KAFKA-4920: Stamped implements equals","Solves: [KAFKA-4920](https://issues.apache.org/jira/browse/KAFKA-4920) Bug type EQ_COMPARETO_USE_OBJECT_EQUALS (click for details) In class org.apache.kafka.streams.processor.internals.Stamped In method org.apache.kafka.streams.processor.internals.Stamped.compareTo(Object) At Stamped.java:[lines 31-35]","closed","","mjuchli","2017-03-20T15:33:41Z","2018-02-26T19:36:38Z"
"","3472","MINOR: Upgraded RocksDB to 5.3.6","Solves a problem as described here: https://groups.google.com/forum/#!topic/confluent-platform/cR02K2o6BIw","closed","","enothereska","2017-06-30T22:01:05Z","2017-07-11T20:26:21Z"
"","3009","MINOR: Missing logger argument","Small fix to add the missing argument in the logger message.","closed","","Mogztter","2017-05-10T09:48:39Z","2018-02-14T02:38:53Z"
"","3295","KAFKA-5418: ZkUtils.getAllPartitions() may fail if a topic is marked for deletion","Skip topics that don't have any partitions in zkUtils.getAllPartitions()","closed","","mimaison","2017-06-10T20:16:57Z","2018-04-18T13:31:37Z"
"","2834","KAFKA-5047: NullPointerException while using GlobalKTable in KafkaStreams","Skip null keys when initializing GlobalKTables. This is inline with what happens during normal processing.","closed","","dguy","2017-04-10T16:45:52Z","2017-05-16T14:05:02Z"
"","3266","WIP: KAFKA-5403: Transaction system test consumer should dedup messages by offset","Since the consumer can consume duplicate offsets due to rebalances, we should dedup consumed messages by offset in order to ensure that the test doesn't fail spuriously.  This achieved by transitioning to the VerifiableConsumer, which returns the offsets of the consumed records as well.","closed","","apurvam","2017-06-08T03:37:14Z","2017-10-09T21:01:06Z"
"","2612","KAFKA-4819: Expose states for active tasks to public API","Simple implementation of the feature : [KAFKA-4819](https://issues.apache.org/jira/browse/KAFKA-4819)  KAFKA-4819  This PR adds a new method `threadStates` to public API of `KafkaStreams` which returns all currently states of running threads and active tasks.  Below is a example for a simple topology consuming from topics; test-p2 and test-p4.  [{""name"":""StreamThread-1"",""state"":""RUNNING"",""activeTasks"":[{""id"":""0_0"", ""assignments"":[""test-p4-0"",""test-p2-0""], ""consumedOffsetsByPartition"":[{""topicPartition"":""test-p2-0"",""offset"":""test-p2-0""}]}, {""id"":""0_2"", ""assignments"":[""test-p4-2""], ""consumedOffsetsByPartition"":[]}]}, {""name"":""StreamThread-2"",""state"":""RUNNING"",""activeTasks"":[{""id"":""0_1"", ""assignments"":[""test-p4-1"",""test-p2-1""], ""consumedOffsetsByPartition"":[{""topicPartition"":""test-p2-1"",""offset"":""test-p2-1""}]}, {""id"":""0_3"", ""assignments"":[""test-p4-3""], ""consumedOffsetsByPartition"":[]}]}]","closed","","fhussonnois","2017-02-28T21:45:53Z","2017-09-05T21:13:32Z"
"","3270","Fixed file URI path to log4j in bin/windows batch files","Similar to what was done here, just more issues: https://github.com/apache/kafka/pull/3260","open","","iotmani","2017-06-08T12:37:52Z","2018-03-02T19:30:14Z"
"","2637","throw NoOffsetForPartitionException from poll once for all TopicPartitions affected","Signed-off-by: radai-rosenblatt","closed","","radai-rosenblatt","2017-03-04T01:48:25Z","2017-05-08T16:45:48Z"
"","3014","KAFKA-5182: Close txn coordinator threads during broker shutdown","Shutdown delayed delete purgatory thread, transaction marker purgatory thread and send thread in `TransactionMarkerChannelManager` during broker shutdown. Made `InterBrokerSendThread` interruptible so that it is shutdown.","closed","","rajinisivaram","2017-05-10T22:19:09Z","2017-05-11T12:38:12Z"
"","3011","KAFKA-5182: Close txn coordinator threads during broker shutdown","Shutdown delayed delete purgatory thread, transaction marker purgatory thread and send thread in `TransactionMarkerChannelManager`  during broker shutdown. Made `InterBrokerSendThread` interruptible so that it is shutdown.","closed","","rajinisivaram","2017-05-10T21:45:29Z","2017-05-10T23:37:53Z"
"","3432","KAFKA-5372: fixes to state transitions","Several fixes to state transition logic: - Kafka streams will now be in ERROR when all threads are DEAD or when global thread stops unexpectedly - Fixed transition logic in corner cases when thread is already dead or Kafka Streams is already closed - Fixed incorrect transition diagram in StreamThread - Unit tests to verify transitions  Also: - re-enabled throwing an exception when an unexpected state change happens - fixed a whole bunch of EoS tests that did not start a thread - added more comments.","closed","","enothereska","2017-06-26T12:12:13Z","2017-07-06T09:06:34Z"
"","2719","KAFKA-4916: test streams with brokers failing","Several fixes for handling broker failures: - default replication value for internal topics is now 3 in test itself (not in streams code, that will require a KIP.  - streams producer waits for acks from all replicas in test itself (not in streams code, that will require a KIP.  - backoff time for streams client to try again after a failure to contact controller.  - fix bug related to state store locks (this helps in multi-threaded scenarios) - fix related to catching exceptions property for network errors.  - system test for all the above","closed","","enothereska","2017-03-21T18:48:08Z","2017-04-05T05:40:38Z"
"","2750","KAFKA-4965: set internal.leave.group.on.close to false in StreamsConfig","Set the internal consumer config internal.leave.group.on.close in `StreamsConfig`. This is to reduce the number of rebalances we get during bounces","closed","","dguy","2017-03-28T08:41:47Z","2017-05-16T14:05:01Z"
"","2771","KAFKA-4689: Disable system tests for consumer hard failures","See the JIRA for the full details. Essentially the test assertions depend on receiving reliable events from the consumer processes, but this is not generally possible in the presence of a hard failure (i.e. `kill -9`). Until we solve this problem, the hard failure scenarios will be turned off.","closed","","hachikuji","2017-03-30T18:19:09Z","2017-03-30T23:16:23Z"
"","2744","KAFKA-4954: Request handler utilization quotas","See KIP-124 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-124+-+Request+rate+quotas) for details","closed","","rajinisivaram","2017-03-27T17:09:43Z","2017-05-01T16:14:39Z"
"","3380","MINOR: Add serialized vagrant rsync until upstream fixes broken parallelism","See https://github.com/mitchellh/vagrant/issues/7531. The core of the issue is that vagrant rsync uses a fixed set of 1000 possible temp file entries for SSH ControlMaster files to cache SSH connections for rsyncing. A few notes:  * We can't break down the steps further and maintain performance due to various limitations in vagrant/vagrant-aws (rsync is only executed on `vagrant up`/`vagrant reload`/`vagrant rsync`, you can't enable/disable and rsync shared folder only during some of those stages, and provisioning only runs in parallel with vagrant-aws during `vagrant up`). * We need to isolate each of the serialized rsync calls. (If we assumed `parallel` was available, we actually could get the parallelism back.) This is required because even across calls they could randomly choose the same temporary file. * If there's a chance multiple instances were running on the same server at the same or nearly the same time, they can conflict since the same temp file entries are used globally. This means anything running on shared CI servers might end up syncing data between different CI jobs (!!), which could lead to some very strange results. Especially weird if they aren't even for the same type of job. * Provisioning error check needs to be removed because it is catching rsync errors, but those can still happen in the initial `vagrant up` rsync step before the `vagrant up` provisioning step. It seems likely this bug was the cause of missing files anyway so this check might not be as valuable anymore.","closed","","ewencp","2017-06-20T06:23:18Z","2017-06-20T19:28:43Z"
"","3228","MINOR: (docs) in section, not at section","See https://english.stackexchange.com/questions/158981/at-this-section-vs-in-this-section and https://forum.wordreference.com/threads/at-in-within-the-section-we-can-find.374188/","closed","","mihbor","2017-06-03T09:20:07Z","2017-06-16T20:08:32Z"
"","3181","KAFKA-5154: Consumer fetches from revoked partitions when SyncGroup fails with disconnection [WIP]","Scenario is as follows: 1. Consumer subscribes to topic t1 and begins consuming 2. heartbeat fails as the group is rebalancing 3. ConsumerCoordinator.onJoinGroupPrepare is called    3.1 onPartitionsRevoked is called 4. consumer becomes the group leader 5. sends sync group request 6. sync group is cancelled due to disconnection 7. fetch request is sent for partitions that have previously been revoked","closed","","dguy","2017-05-31T16:50:09Z","2017-08-16T13:23:20Z"
"","2971","Kafka-5206: Remove use of aggSerde in RocksDBSessionStore.","RocksDBSessionStore wasn't properly using the default aggSerde if no Serde was supplied.","closed","","ghost","2017-05-04T01:28:55Z","2017-05-17T17:56:34Z"
"","2537","MINOR: Improve LogCleaner buffer size change logging","Right now it looks like the buffer size only grows if you are watching the cleaner logs. It would be nice if we also logged when the buffer size was restored to avoid confusion when the log just keeps saying the buffer has grown repeatedly to and from the same values.","closed","","cotedm","2017-02-10T19:40:34Z","2019-12-20T19:40:31Z"
"","2539","MINOR: Fix quickstart in docs","Reverting some of the recent changes to quickstart doc. Further explanation is provided inline.","closed","","vahidhashemian","2017-02-10T20:20:14Z","2017-02-11T04:37:17Z"
"","3152","KAFKA-5308: TC should handle UNSUPPORTED_FOR_MESSAGE_FORMAT in WriteTxnMarker response","Return UNSUPPORTED_MESSAGE_FORMAT in handleWriteTxnMarkers when a topic is not the correct message format. Remove any TopicPartitions that have same error from those waiting for markers","closed","","dguy","2017-05-26T09:52:39Z","2017-08-16T13:23:26Z"
"","2645","KAFKA-4851: only search available segments during Segments.segments(from, to)","restrict the locating of segments in `Segments#segments(..)` to only the segments that are currently available, i.e., rather than searching the hashmap for many segments that don't exist.","closed","","dguy","2017-03-06T17:38:53Z","2017-03-30T11:10:36Z"
"","2986","KAFKA-5099: Replica Deletion Regression from KIP-101","Replica deletion regressed from KIP-101. Replica deletion happens when a broker receives a StopReplicaRequest with delete=true. Ever since KAFKA-1911, replica deletion has been async, meaning the broker responds with a StopReplicaResponse simply after marking the replica directory as staged for deletion. This marking happens by moving a data log directory and its contents such as /tmp/kafka-logs1/t1-0 to a marked directory like /tmp/kafka-logs1/t1-0.8c9c4c0c61c44cc59ebeb00075a2a07f-delete, acting as a soft-delete. A scheduled thread later actually deletes the data. It appears that the regression occurs while the scheduled thread is actually trying to delete the data, which means the controller considers operations such as partition reassignment and topic deletion complete. But if you look at the log4j logs and data logs, you'll find that the soft-deleted data logs actually won't get deleted.  The bug is that upon log deletion, we attempt to flush the LeaderEpochFileCache to the original file location instead of the moved file location. Restarting the broker actually allows for the soft-deleted directories to get deleted.  This patch avoids the issue by simply not flushing the LeaderEpochFileCache upon log deletion.","closed","","onurkaraman","2017-05-06T03:46:10Z","2017-05-10T20:46:48Z"
"","3192","KAFKA-5357: StackOverFlow error in transaction coordinator","Replace recursion in `TransactionMarkerChannelManager#appendToLogCallback` with retryQueue. Retry the enqueued log appends each time the InterBrokerSendThread runs","closed","","dguy","2017-06-01T14:38:30Z","2017-08-16T13:23:19Z"
"","2560","KAFKA-4494: Reduce startup and rebalance time","Replace one-by-one initialization of state stores with bulk initialization.","closed","","dguy","2017-02-17T00:04:15Z","2017-03-30T11:10:48Z"
"","3061","KAFKA-5246: Remove backdoor that allows any client to produce to internal topics","removing unused `AdminUtils.AdminClientId`, as its a security hole.  @ijuma @guozhangwang would appreciate a review please =)  I state the contribution is my own work and I license the work to the project under the project's open source license.","closed","","big-andy-coates","2017-05-15T20:16:55Z","2017-06-08T16:46:00Z"
"","2507","HOTFIX: Do Not use unlimited num messages","Removed readKeyValues() that give UNLIMITED_MESSAGES which will doom to exhaust all wait time, as all its callers actually do provide the expected number of messages.","closed","","guozhangwang","2017-02-06T20:42:53Z","2017-07-15T22:07:21Z"
"","2717","MINOR: remove unused log field from KStreamTransformValuesProcessor","remove unused log field from KStreamTransformValuesProcessor","closed","","dguy","2017-03-21T16:31:00Z","2017-03-30T11:10:20Z"
"","3469","MINOR: remove unused eosEnabled field from ProcessorStateManager","remove unused eosEnabled field from ProcessorStateManager","closed","","dguy","2017-06-30T15:41:56Z","2017-08-16T13:23:12Z"
"","3101","KAFKA-5279: TransactionCoordinator must expire transactionalIds","remove transactions that have not been updated for at least `transactional.id.expiration.ms`","closed","","dguy","2017-05-19T14:13:44Z","2017-05-25T18:02:36Z"
"","2965","KAFKA-5168: Cleanup delayed produce purgatory during partition emmigration","remove operations from the replica manager's producer purgatory on transaction emmigration","closed","","dguy","2017-05-03T18:37:02Z","2017-05-16T14:04:45Z"
"","2605","Kafka 4738:Remove generic type of class ClientState","Remove generic type of class ClientState. Also removed Generic type from TaskAssignor as it referred to same generic type of ClientState","closed","","sharad-develop","2017-02-27T23:55:19Z","2017-03-01T00:36:46Z"
"","2616","KAFKA-4738: Remove generic type of class ClientState","Remove generic type of class ClientState and generic T inTaskAssignor.","closed","","sharad-develop","2017-03-01T00:39:18Z","2017-03-10T19:28:49Z"
"","2934","KAFKA-5059: [Follow Up] remove broken locking. Fix handleAddPartitions","remove broken locking. fix handleAddPartitions after complete commit/abort respond with CONCURRENT_TRANSACTIONS in initPid","closed","","dguy","2017-04-28T09:46:27Z","2017-05-16T14:04:50Z"
"","3326","MINOR: Add unit tests for PluginDesc in Connect.","Related to https://github.com/apache/kafka/pull/3321","closed","","kkonstantine","2017-06-13T22:29:46Z","2017-06-13T23:56:38Z"
"","2895","KAFKA-5111: Improve internal Task APIs","Refactors Task with proper interface methods `init()`, `resume()`, `commit()`, `suspend()`, and `close()`. All other methods for task handling are internal now. This allows to simplify `StreamThread` code, avoid code duplication and allows for easier reasoning of control flow.","closed","","mjsax","2017-04-22T05:01:08Z","2017-04-28T00:48:47Z"
"","2577","MINOR: add 2 failure test methods","Refactored test code. Added 2 methods for `manual` testing (not for nightly runs).","closed","","enothereska","2017-02-20T14:27:25Z","2018-01-17T00:04:55Z"
"","2583","MINOR: Reduce metrics overheads","Reduces overheads by avoiding re-calling time.milliseconds().","closed","","enothereska","2017-02-22T10:41:36Z","2017-02-24T10:40:41Z"
"","3057","KAFKA-5182: Reduce session timeout in request quota test","Reduce rebalance and session timeouts for join requests to trigger throttling in the request quota test.","closed","","rajinisivaram","2017-05-15T17:31:56Z","2017-05-17T13:42:57Z"
"","3265","KAFKA-5405: Request log should log throttle time","Record `apiThrottleTime` in RequestChannel. @junrao  A trivial change. Please review. Thanks.","closed","","huxihx","2017-06-08T01:47:50Z","2017-06-12T04:35:48Z"
"","2904","KAFKA-5091: ReassignPartitionsCommand should...","ReassignPartitionsCommand should protect against empty replica list assignment.","closed","","huxihx","2017-04-24T06:59:51Z","2017-05-01T19:25:17Z"
"","2786","MINOR: Removed dead field","Really trivial but can't hurt to clean this up I think.  Just saw this when trying out the Findbugs task randomly :)","closed","","original-brownbear","2017-04-01T19:45:05Z","2017-04-05T22:33:44Z"
"","2581","HOTFIX: Fix version in __init__.py","Quotes were missing.","closed","","ijuma","2017-02-21T22:22:49Z","2017-09-05T09:30:12Z"
"","3435","KAFKA-4388 Recommended values for converters from plugins","Questions to reviewers: 1. Should we cache `converterRecommenders.validValues()`, `SinkConnectorConfig.configDef()` and `SourceConnectorConfig.configDef()` results? 2. What is appropriate place for testing new `ConnectorConfig.configDef(plugins)` functionality?  cc @ewencp","closed","connect,","evis","2017-06-26T13:50:19Z","2019-10-30T07:26:28Z"
"","3088","0.10.2","pull","closed","","dwshmilyss","2017-05-18T16:36:08Z","2018-01-26T19:25:27Z"
"","3316","KAFKA-5274: AdminClient Javadoc improvements","Publish Javadoc for common.annotation package, which contains InterfaceStability.  Finally, mark AdminClient classes with `Evolving` instead of `Unstable`.","closed","","ijuma","2017-06-13T14:30:19Z","2017-06-18T09:26:08Z"
"","3041","MINOR: Eliminate PID terminology from non test code","Producer id is used instead.  Also refactored TransactionLog schema code to follow our naming convention and to have better structure.","closed","","ijuma","2017-05-13T08:37:13Z","2017-06-18T09:27:48Z"
"","2848","KAFKA-5038: Catch exception","Porting from 0.10.2 PR https://github.com/apache/kafka/pull/2841","closed","","enothereska","2017-04-12T14:37:29Z","2017-04-12T16:29:27Z"
"","3397","KAFKA-5413: Port fix to 0.10.2 branch","Port KAFKA-5413 to the 0.10.2 branch","closed","","kelvinrutt","2017-06-21T17:40:25Z","2017-08-25T20:00:05Z"
"","3068","KAFKA-5250: Do fetch down conversion after throttling","Perform down conversion after throttling to avoid retaining messages in memory during throttling since this could result in OOM. Also update bytesOut metrics after throttling when the response is about to be sent.","closed","","rajinisivaram","2017-05-16T13:30:19Z","2017-05-18T21:44:49Z"
"","2512","Removed obsolete parameter form example config in docs.","Parameter controller.message.queue.size was removed in 0.9 (KAFKA-2122) but is still listed in an example broker configuration in the documentation.","closed","","soenkeliebau","2017-02-07T14:47:59Z","2017-02-07T16:54:16Z"
"","3254","MINOR: set group initial rebalance delay to 0 in server.properties","override the setting of `group.initial.rebalance.delay` in server.properties","closed","","dguy","2017-06-07T09:32:13Z","2017-06-22T14:16:34Z"
"","3235","MINOR:  Running the ConsumerIteratorTest unit tests，TMP directory has not  been  deleted when an exception occurs","OS:Windows 7 After the running ConsumerIteratorTest unit tests, the TMP directory '..\AppData\Local\Temp\kafka-6579284700084877567\version-2' has not been deleted  Exception information: at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) 	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) 	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102) 	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269) 	at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103) 	at java.nio.file.Files.delete(Files.java:1126) 	at org.apache.kafka.common.utils.Utils$2.visitFile(Utils.java:575) 	at org.apache.kafka.common.utils.Utils$2.visitFile(Utils.java:564) 	at java.nio.file.Files.walkFileTree(Files.java:2670) 	at java.nio.file.Files.walkFileTree(Files.java:2742) 	at org.apache.kafka.common.utils.Utils.delete(Utils.java:564) 	at org.apache.kafka.test.TestUtils$1.run(TestUtils.java:182) [2017-06-05 11:03:32,552] ERROR Error deleting C:\Users\10110346\AppData\Local\Temp\kafka-5691323213778461274 (org.apache.kafka.test.TestUtils:184) java.nio.file.FileSystemException: C:\Users\10110346\AppData\Local\Temp\kafka-5691323213778461274\version-2\log.1: 另一个程序正在使用此文件，进程无法访问。","closed","","10110346","2017-06-05T03:23:28Z","2017-10-24T06:07:41Z"
"","3275","HOTFIX: use atomic boolean for inject errors in streams eos integration test","Originally we assume the task will be created exactly three times (twice upon starting up, once for each thread, and then one more time when rebalancing upon the thread failure). However there is a likelihood that upon starting up more than one rebalance will be triggered, and hence the tasks will be initialized more than 3 times, i.e. there will be more than three hashcodes of the `Transformer` object, causing the `errorInjected` to never be taken and exception never thrown.  The current fix is to use an atomic boolean instead and let threads compete on compare-and-set to make sure exactly one thread will throw exception, and will only throw once.  Without this patch I can reproduce the issue on my local machine with a single core ever 3-5 times; with this patch I have been running successfully for 10+ runs.  Ping @mjsax @ijuma for reviews.","closed","","guozhangwang","2017-06-08T23:41:00Z","2017-07-15T22:07:00Z"
"","3429","KAFKA-5508: Documentation for altering topics","Operations documentation should guide user to employ `kafka-configs.sh` to add/remove configs for a topic.","closed","","huxihx","2017-06-26T00:55:14Z","2017-07-06T09:11:16Z"
"","2950","enhance JmxTool usage and output format","one-time flag to only run the pull once instead of looping forever reporting-interval -1 equivalent to setting one-time reportFormatOpt supports 'original', 'properties', 'csv', 'tsv' formatting","closed","","scottf","2017-04-30T22:05:52Z","2018-03-22T23:47:19Z"
"","2913","KAFKA-4994: Fix findbug warnings about OffsetStorageWriter","OffsetStorageWriter is not a thread-safe class and should be accessed only from a Task's processing thread. The WorkerSourceTask class calls the different methods (offset, beginFlush, cancelFlush, handleFinishWrite) within a synchronized block. Hence the method definitions in  OffsetStorageWriter.java does not need to contain the keyword synchronized again.  In the OffsetStorageWriter.java class, the doFlush() method is not explicitely synchronized like the other methods in this class. Hence this can lead to inconsistent synchronization of variables like currentFlushId and toFlush which are set in the synchronized methods within this class.  - https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageWriter.java - https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L295   Closes bug: Kafka-4994","closed","","johnma14","2017-04-25T16:54:57Z","2017-04-25T20:14:07Z"
"","2914","KAFKA-4994: Fix findbug warnings about OffsetStorageWriter","OffsetStorageWriter is not a thread-safe class and should be accessed only from a Task's processing thread. The WorkerSourceTask class calls the different methods (offset, beginFlush, cancelFlush, handleFinishWrite) within a synchronized block. Hence the method definitions in OffsetStorageWriter.java does not need to contain the keyword synchronized again.  In the OffsetStorageWriter.java class, the doFlush() method is not explicitely synchronized like the other methods in this class. Hence this can lead to inconsistent synchronization of variables like currentFlushId and toFlush which are set in the synchronized methods within this class. - https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageWriter.java - https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L295","open","connect,","johnma14","2017-04-25T20:54:55Z","2018-03-02T19:30:06Z"
"","2847","MINOR: update num.x.threads server.properties comments","num.io.threads was described as doing disk-IO the description in KafkaConfig is more accurate","closed","","edoardocomar","2017-04-12T14:25:28Z","2017-05-15T10:00:10Z"
"","3307","KAFKA-4831: Extract WindowedSerde to public APIs","Now that we have augmented WindowSerde with non-arg parameters, extract it out as part of the public APIs so that users who want to I/O windowed streams can use it. This is originally introduced by @vitaly-pushkar   This PR grows out to be a much larger one, as I found a few tech debts and bugs while working on it. Here is a summary of the PR:  1. Public API changes (I will propose a KIP after a first round of reviews):  * Add TimeWindowedSerializer,  TimeWindowedDeserializer, SessionWindowedSerializer,  SessionWindowedDeserializer into o.a.k.streams.kstream. The serializers would implemented an internal `WindowedSerializer` interface for the serializeBaseKey function used in 3) below.  * Add WindowedSerdes into o.a.k.streams.kstream. The reason to now add them into o.a.k.clients's Serdes is that it then needs dependency of streams.  * Add ""default.windowed.key.serde.inner"" and ""default.windowed.value.serde.inner"" into StreamsConfig, used when ""default.key.serde"" is specified to use time or session windowed serde. Note this requires the serde class, not the type class.  2. Consolidated serde format from multiple classes, including SessionKeySerde.java for session, and WindowStoreUtils for time window, into SessionKeySchema and WindowKeySchema.  3. Bug fix: `WindowedStreamPartitioner` needs to consider both time window and session window serdes.  4. Removed `RocksDBWindowBytesStore` etc optimization since after KIP-182 all the serde know happens on metered store, hence this optimization is not worth.  5. Bug fix: for time window, the serdes used for store and the serdes used for piping (source and sink node) are different: the former needs to append sequence number but not for the later.  6. Other minor cleanups: remove unnecessary throws, etc.","closed","streams,","vitaly-pushkar","2017-06-12T20:05:13Z","2018-03-09T19:08:08Z"
"","2814","change language level from java 7 to 8","now that KIP-118 has passed, and there are no major releases coming before 0.11 ....  Signed-off-by: radai-rosenblatt","closed","","radai-rosenblatt","2017-04-05T14:19:56Z","2017-09-06T19:21:18Z"
"","2653","MINOR: Update possible errors in OffsetFetchResponse","Note: None of the use cases for offset fetch would lead to a `TOPIC_AUTHORIZATION_FAILED` error (fetching offset of an unauthorized partition would return an `UNKNOWN_TOPIC_OR_PARTITION` error). That is why it is being removed from the `PARTITION_ERRORS` list.","closed","","vahidhashemian","2017-03-07T23:34:16Z","2017-03-30T18:25:50Z"
"","3309","MINOR: NetworkClient#disconnect should not erase connection info","NetworkClient#disconnect should not erase the connection information.  This will allow exponential backoff to occur.","closed","","cmccabe","2017-06-12T23:43:36Z","2019-05-20T19:01:53Z"
"","2880","New transform: ExtractFields","Needed a transform that properly handled extracting fields from multiple different levels in  s nested records.","open","connect,","dbtucker","2017-04-20T23:45:43Z","2020-05-28T02:06:42Z"
"","2533","KAFKA-4758: Checking the TimestampType is insufficient","My understanding is that the record format allots 1 bit for the timestamp type field. As a result it can be either 1 or 0 and never -1. This means Connect's timestamp type validation is insufficient. Instead the records timestamp value itself should be checked.   Technically the timestamp value should never be another negative value but while we are being defensive we may as well go all the way.   Doing so inside Connect takes the burden off Connector developers which I think is a good thing.","closed","","rnpridgeon","2017-02-10T11:22:22Z","2017-02-13T00:08:48Z"
"","2925","KAFKA-5136: move coordinatorEpoch from WriteTxnMarkerRequest to TxnMarkerEntry","Moving the coordinatorEpoch from WriteTxnMarkerRequest to TxnMarkerEntry will generate fewer broker send requests","closed","","dguy","2017-04-27T15:00:17Z","2017-05-16T14:04:49Z"
"","3053","KAFKA-5239: Producer buffer pool allocates memory inside a lock","Move byte buffer allocation out of lock. Add unit test for restoring count when OOM is thrown from byte buffer allocation.","closed","","smccauliff","2017-05-15T04:32:23Z","2017-06-06T20:43:50Z"
"","3191","KAFKA-5360: Down-converted uncompressed batches should respect fetch offset","More specifically, V2 messages are always batched (whether compressed or not) while V0/V1 are only batched if they are compressed.  Clients like librdkafka expect to receive messages from the fetch offset when dealing with uncompressed V0/V1 messages. When converting from V2 to V0/1, we were returning all the messages in the V2 batch.","closed","","ijuma","2017-06-01T12:46:43Z","2017-06-18T09:30:23Z"
"","3474","KAFKA-5548 - Extended validation for SchemaBuilder methods.","More input validation for SchemaBuilder methods.","closed","","jcustenborder","2017-07-01T01:24:36Z","2017-07-01T09:48:15Z"
"","2513","MINOR: Remove Struct from Request/Response classes","More details: * Replaced `struct` field in Request/Response with a `toStruct` method. This makes the performance model (including memory usage) easier to understand. Note that requests have `toStruct()` while responses have `toStruct(version)`. * Replaced mutable `version` field in `Request.Builder` with an immutable field `desiredVersion` and a `version` parameter passed to the `build` method. * Optimised `handleFetchRequest` to avoid unnecessary creation of `Struct` instances (from 4 to 2 in the worst case and 2 to 1 in the best case). * Various clean-ups in request/response classes and their test. In particular, it is now clear what we are testing. Previously, it looked like we were testing more than we really were.  With this in place, we could remove `AbstractRequest.Builder` in the future by doing the following: * Change `AbstractRequest.toStruct` to accept a version (like responses). * Change `AbstractRequest.version` to be `desiredVersion` (like `Builder`). * Change `ClientRequest` to take `AbstractRequest`. * Move validation from the `build` methods to the request constructors or static factory methods. * Anything else required for the code to compile again.","closed","","ijuma","2017-02-07T17:29:19Z","2017-09-05T09:30:23Z"
"","2706","MINOR: fixed memory pollution by removing unnecessary creation of HashSet in KafkaConsumer","MINOR: fixed memory pollution by removing unnecessary creation of HashSet (assignment and subscription functions), changed modifiable to non-modifiable state while committing offsets (commitSync and commitAsync functions).  All changes made in in KafkaConsumer.","closed","","wlsc","2017-03-19T10:05:12Z","2017-06-19T18:25:55Z"
"","2710","KAFKA-4922: - Minor FindBugs warning fixes","Minor one line adaptations to one bad practice and three performance warnings https://issues.apache.org/jira/browse/KAFKA-4922","closed","","drennings","2017-03-20T14:05:07Z","2018-02-25T21:48:14Z"
"","2662","KAFKA-4863: Querying window store may return unwanted keys","Make sure that the iterator returned from `WindowStore.fetch(..)` only returns matching keys, rather than all keys that are a prefix match.","closed","","dguy","2017-03-09T14:56:24Z","2017-03-30T11:10:33Z"
"","2525","KAFKA-4484: Set more conservative default values on RocksDB for memory usage","Lowered the default RocksDB settings for the block cache and write buffers","closed","","dguy","2017-02-09T14:37:02Z","2017-02-17T00:58:01Z"
"","2900","KAFKA-5081: force version for 'jackson-annotations' in order to prevent redundant jars from being bundled into kafka distribution","Long story short: prevent redundant jars from being bundled into kafka distribution.  (Inspired by this JIRA ticket: [KAFKA-5081](https://issues.apache.org/jira/browse/KAFKA-5081) **_two versions of jackson-annotations-xxx.jar in distribution tgz_**)","closed","","dejan2609","2017-04-23T17:38:41Z","2017-05-22T16:54:20Z"
"","3274","KAFKA-3199 LoginManager should allow using an existing Subject","LoginManager or KerberosLogin (for > kafka 0.10) should allow using an existing Subject. If there's an existing subject, the Jaas configuration won't needed in getService()","open","","utenakr","2017-06-08T22:35:27Z","2018-03-02T19:30:14Z"
"","3362","MINOR: StreamThread standbyTask comment typo","log info typos. should be standby task, not active task.","closed","","zqhxuyuan","2017-06-17T01:42:27Z","2017-06-20T04:01:59Z"
"","2843","KAFKA-5056: Add shuffling of FetchRequest requestInfo back to old consumer","KIP-74 removed the shuffle of requestInfo from the FetchRequest constructor, moving the logic to the replica fetcher and new consumer API. This adds a shuffle back to the old consumer that uses the same logic as the deprecated constructor.","closed","","toddpalino","2017-04-12T00:30:05Z","2019-01-05T10:12:38Z"
"","3126","KAFKA-5260: Producer should not send AbortTxn unless transaction has actually begun","Keep track of when a transaction has begun by setting a flag, `transactionStarted` when a successfull `AddPartitionsToTxnResponse` or `AddOffsetsToTxnResponse` had been received. If an `AbortTxnRequest` about to be sent and `transactionStarted` is false, don't send the request and transition the state to `READY`","closed","","dguy","2017-05-23T11:36:32Z","2017-05-30T05:56:45Z"
"","3223","KAFKA-5098: KafkaProducer.send() should validate topic name before sending","KafkaProducer.send() should check topic name before sending the record.","closed","","huxihx","2017-06-03T06:51:56Z","2017-06-05T04:52:03Z"
"","2576","kafka-4767: KafkaProducer is not joining its IO thread properly","KafkaProducer#close swallows the InterruptedException which might be acceptable when it's invoked from within the main thread or user is extending Thread and therefore controls all the code higher up on the call stack. For other cases, it'd better retstore the interupted status after capturing the exception.","closed","","huxihx","2017-02-20T02:29:27Z","2017-05-31T01:06:53Z"
"","2518","KAFKA-4747: add metrics for KafkaConsumer.poll","KafkaConsumer heavily depends on KafkaConsumer.poll yet we don't have metrics directly associated with it.  We probably want to add two metrics: 1. time spent in KafkaConsumer.poll 2. time since last KafkaConsumer.poll (measured as now - endTimeOfLastPoll)","closed","","onurkaraman","2017-02-09T00:56:37Z","2017-09-11T18:15:37Z"
"","2631","KAFKA-4706: Unify StreamsKafkaClient instances","KAFKA:4706 - Unify StreamsKafkaClient instances","closed","","sharad-develop","2017-03-03T02:08:08Z","2018-01-02T18:42:17Z"
"","2625","KAFKA-4623: Change default unclean.leader.election.enabled from True to False","KAFKA:4623- Change default unclean.leader.election.enabled from True to False","closed","","sharad-develop","2017-03-01T20:04:11Z","2017-05-02T15:30:00Z"
"","2902","kafka-5901: ReassignPartitionsCommand should protect against empty re…","kafka-5901: ReassignPartitionsCommand should protect against empty replica list assignment  Add logic code to throw exception when replica list for the to-be-reassign partition is empty.","closed","","huxihx","2017-04-24T02:55:33Z","2017-04-24T06:56:03Z"
"","3404","KAFKA-5476: Implement a system test that creates network partitions","KAFKA-5476: Implement a system test that creates network partitions","closed","","cmccabe","2017-06-21T23:41:49Z","2019-05-20T19:00:53Z"
"","3148","KAFKA-5327: ConsoleConsumer should manually commit offsets...","KAFKA-5327: ConsoleConsumer should manually commit offsets for those records it really consumed. Currently it leaves this job to the automatic offset commit scheme where some unread messages will be passed if `--max-messages` is set.","closed","","huxihx","2017-05-26T03:14:16Z","2017-06-02T19:09:42Z"
"","2927","KAFKA-5107: remove preferred replica election state from ControllerContext","KAFKA-5028 moves the controller to a single-threaded model, so we would no longer have work interleaved between preferred replica leader election, meaning we don't need to keep its state.  This patch additionally addresses a bug from KAFKA-5028 where it made onPreferredReplicaElection keep the line calling topicDeletionManager.markTopicIneligibleForDeletion but removes the line calling topicDeletionManager.resumeDeletionForTopics","closed","","onurkaraman","2017-04-27T18:09:37Z","2017-05-02T15:42:20Z"
"","2768","KAFKA-4981 Add connection-accept-rate and connection-prepare-rate","KAFKA-4981 Add connection-accept-rate and connection-prepare-rate metrics  added metrics per network processor with the rates of connections accepted and the rate of connections ready for work  developed with @mimaison","closed","","edoardocomar","2017-03-30T13:10:56Z","2018-04-26T13:40:27Z"
"","2487","Kafka-4722 : Add application.id to StreamThread name","Kafka-4722 : Add application.id to StreamThread name","closed","","sharad-develop","2017-02-02T23:39:45Z","2017-03-01T01:32:12Z"
"","2511","KAFKA-4567 - Connect Producer and Consumer ignore ssl parameters…","KAFKA-4567 - Connect Producer and Consumer ignore ssl parameters configured for worker  Added brief explanation to the docs about parameter inheritance of Kafka consumers and producers from the worker config.","closed","","soenkeliebau","2017-02-07T13:00:59Z","2017-03-05T19:44:18Z"
"","2531","KAFKA-1610: Review usages of Map#mapValues.","KAFKA-1610: Local modifications to collections generated from mapValues will be lost.","closed","","jozanek","2017-02-10T10:41:51Z","2017-04-26T09:04:15Z"
"","2990","KAFKA-5096: Log invalid user configs and use defaults","Kafka Streams does not allow users to modify some consumer configurations. Currently, it does not allow modifying the value of 'enable.auto.commit'. If the user modifies this property, currently an exception is thrown. The following changes were made in this patch: - Defined a new array 'NON_CONFIGURABLE_CONSUMER_CONFIGS' to hold the names   of the configuration parameters that is not allowed to be modified - Defined a new method 'checkIfUnexpectedUserSpecifiedConsumerConfig' to   check if user overwrote the values of any of the non configurable configuration   parameters. If so, then log a warning message and reset the default values - Updated the javadoc to include the configuration parameters that cannot be   modified by users. - Updated the corresponding tests in StreamsConfigTest.java to reflect the changes   made in StreamsConfig.java","closed","","johnma14","2017-05-07T08:33:38Z","2017-08-02T13:12:38Z"
"","2938","KAFKA-5096: Log invalid user configs and use defaults","Kafka Streams do not allow users to modify some consumer configurations. If the user modifies this property, currently an exception is thrown. The following changes were made in this patch: - Defined a new array 'NON_CONFIGURABLE_CONSUMER_CONFIGS' to hold the names   of the configuration parameters that is not allowed to be modified. Currently, this   contains just 1 parameter - enable_auto_commit. When the 'exactly once'   feature is implemented ( KAFKA-4923), more parameters can be added to this   array. - Defined a new method 'checkIfUnexpectedUserSpecifiedConsumerConfig' to   check if user overwrote the values of any of the non configurable configuration   parameters. If so, then log a warning message and reset the default values - Updated the javadoc to include the configuration parameters that cannot be   modified by users. - Updated the corresponding tests in StreamsConfigTest.java to reflect the changes   made in StreamsConfig.java","closed","","johnma14","2017-04-28T19:38:35Z","2017-05-17T05:38:43Z"
"","3379","KAFKA-5472 Eliminated duplicate group names when validating connector results","Kafka Connect was adding duplicate group names in the response from the REST API's validation of connector configurations. This fixes the duplicates and maintains the order of the `ConfigDef` objects so that the `ConfigValue` results are in the same order.  This is a blocker and should be merged to 0.11.0.","closed","","rhauch","2017-06-20T00:58:25Z","2017-06-21T00:49:45Z"
"","2481","Kafka 4706 :Unify StreamsKafkaClient instances","Kafka 4706 :Unify StreamsKafkaClient instances","closed","","sharad-develop","2017-02-01T20:57:47Z","2017-03-02T23:19:39Z"
"","3478","KAFKA-3539: KafkaProducer.send() should not block","Just wrapped current `doSend()` method invocation into another `Future` to get rid of blocking while calling `KafkaProducer#send()`. I know, that a lot of tests will be failed (including one I added), just want to know, is it appropriate to do such thing? If it is, I will try to fix tests.","closed","","evis","2017-07-03T09:22:35Z","2017-07-16T06:29:46Z"
"","2573","MINOR: Cleanup org.apache.kafka.streams.kstream.internals.AbstractKTableKTableJoinValueGetterSupplier#storeNames (performance and clarity)","Just a trivial fix improving the performance and clarity of `org.apache.kafka.streams.kstream.internals.AbstractKTableKTableJoinValueGetterSupplier#storeNames`.  The method is essentially just an array concatenation. This shouldn't be done via an `ArrayList`, replaced it by direct instantiation + copy.","closed","","original-brownbear","2017-02-19T07:36:16Z","2017-03-27T04:47:36Z"
"","2584","MINOR: Fix transient failure of testCannotSendToInternalTopic","It’s a simple matter of creating the internal topic before trying to send to it. Otherwise, we could get an `UnknownTopicOrPartitionException` in some cases.  Without the change, I could reproduce a failure in less than 5 runs. With the change, 30 consecutive runs succeeded.","closed","","ijuma","2017-02-22T11:52:38Z","2017-09-05T09:31:39Z"
"","2658","MINOR: Introduce NetworkClient.hasInFlightRequests","It’s a minor optimisation, but simple enough.","closed","","ijuma","2017-03-08T22:35:33Z","2017-09-05T09:06:24Z"
"","2713","KAFKA-4863: [Follow Up] Querying window store may return unwanted keys","iterate over all keys returned from the rocksdb iterator so we don't miss any results","closed","","dguy","2017-03-20T16:31:47Z","2017-03-30T11:10:24Z"
"","3207","KAFKA-5492: Fix flaky testHWCheckpointWithFailuresSingleLogSegment","It was incorrectly passing `oldLeaderOpt` to `waitUntilLeaderIsElectedOrChanged` as can be seen by the subsequent assertion. This started failing recently because we fixed `waitUntilLeaderIsElectedOrChanged` to handle this case correctly.","closed","","ijuma","2017-06-02T11:38:35Z","2020-01-29T09:04:48Z"
"","3176","MINOR: Improve assert in `ControllerFailoverTest`","It sometimes fails in Jenkins like:  ```text java.lang.AssertionError: IllegalStateException was not thrown 	at org.junit.Assert.fail(Assert.java:88) 	at org.junit.Assert.assertTrue(Assert.java:41) 	at kafka.controller.ControllerFailoverTest.testHandleIllegalStateException(ControllerFailoverTest.scala:86) ```  I ran it locally 100 times with no failure.","closed","","ijuma","2017-05-31T07:56:25Z","2017-06-18T09:31:20Z"
"","3392","KAFKA-5319:Add a tool to balance the cluster","It is the code about [KIP-166](https://cwiki.apache.org/confluence/display/KAFKA/KIP-166+-+Add+a+tool+to+make+amounts+of+replicas+and+leaders+on+brokers+balancedl)","closed","","MarkTcMA","2017-06-21T10:07:21Z","2017-06-21T10:28:34Z"
"","3220","KAFKA-5374: Set allow auto topic creation to false when requesting node information only","It avoids the need to handle protocol downgrades and it's safe (i.e. it will never cause the auto creation of topics).","closed","","ijuma","2017-06-03T01:33:38Z","2017-06-18T09:27:10Z"
"","3196","KAFKA-5330: Use per-task converters in Connect","Instead of sharing the same converter instance within the worker, use a converter per task.  More details: - https://github.com/confluentinc/schema-registry/issues/514 - https://issues.apache.org/jira/browse/KAFKA-5330","closed","","tbcdns","2017-06-01T20:02:10Z","2017-09-22T03:13:50Z"
"","3448","KAFKA-5526: Report group consumers while offsets are being initialized (WIP)","Instead of reporting no rows in consumer group command's result for new consumers while group offsets are being initialized, report the consumers within the group and their assignment without reporting any specific offset or lag for them.","closed","","vahidhashemian","2017-06-28T00:13:23Z","2017-11-28T19:56:58Z"
"","2827","Fix KAFKA-5044.","InFlightRequests#canSendMore should not require that previous sends completed if we are under the maxInFlightRequestsPerConnection limit.","closed","","twbecker","2017-04-07T22:50:02Z","2017-04-10T11:46:06Z"
"","2826","Fix KAFKA-5044.","InFlightRequests#canSendMore should not require that previous sends completed if we are under the maxInFlightRequestsPerConnection limit.","closed","","twbecker","2017-04-07T22:44:40Z","2017-04-07T22:56:17Z"
"","3344","KAFKA-5450 Increased timeout of Connect system test utilities","Increased the timeout from 30sec to 60sec. When running the system tests with packaged Kafka, Connect workers can take about 30seconds to start.","closed","","rhauch","2017-06-14T21:41:54Z","2017-06-15T21:47:40Z"
"","2714","Fix ZKSec Migrate example","Incorrect option in example   https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala#L71","closed","","rnpridgeon","2017-03-20T17:08:21Z","2017-03-21T09:53:10Z"
"","3419","MINOR: Fixed way how logging methods are used for having a consistent one","In the stream library there are few cases where we don't leverage on logging methods features (i.e. using {} placeholder instead of string concatenation or passing the exception variable)","closed","","ppatierno","2017-06-23T10:09:47Z","2017-06-26T07:31:10Z"
"","3100","KAFKA-5278: ConsoleConsumer should honor  `--value-deserializer`","In the original implementation, console-consumer fails to honor `--value-deserializer` config.","closed","","huxihx","2017-05-19T11:41:32Z","2017-05-31T01:07:34Z"
"","3211","MINOR: make flush no-op as we don't need to call flush on commit.","In the event of a crash, we always restore the data from the backing changelog.  So it seems that we don't need to   persist all data to disk by calling flush when committing.  Frequent flushing leads to a large number of small files for compaction increasing  compaction pressure.   This PR will perform benchmarks to see if there is any performance gain in not calling `flush()` each time we commit.","closed","","bbejeck","2017-06-02T18:38:46Z","2017-06-15T20:56:46Z"
"","3204","KAFKA-5322:  Add an `OPERATION_NOT_ATTEMPTED` error code","In the `AddPartitionsToTxn` request handling, if even one partition fails authorization checks, the entire request is essentially failed. However, the `AddPartitionsToTxnResponse` today will only contain the error codes for the topics which failed authorization. It will have no error code for the topics which succeeded, making it inconsistent with other APIs.  This patch adds a new error code `OPERATION_NOT_ATTEMPTED` which is returned for the successful partitions to indicate that they were not added to the transaction.","closed","","apurvam","2017-06-02T05:43:03Z","2017-06-02T21:24:44Z"
"","2504","KAFKA-4735; Fix deadlock issue during MM shutdown","In https://issues.apache.org/jira/browse/KAFKA-4521 we fixed a potential message reorder bug in MM. However, the patch introduced another bug that can cause deadlock during MM shutdown. The deadlock will happen if zookeeper listener thread call requestAndWaitForCommit() after MirrorMaker thread has already exited loop of consuming and producing messages.  This patch fixes the problem by setting `iter` to `null` in `MirrorMakerOldConsumer.cleanup()`. If zookeeper listener thread calls `requestAndWaitForCommit()` after `cleanup()`, then it will not block waiting for commit notification since `iter == null`. If zookeeper listener thread calls `requestAndWaitForCommit()` before `cleanup()`, then `cleanup()` will call `notifyAll()` to unblock zookeeper listener thread.","closed","","lindong28","2017-02-05T10:19:58Z","2017-03-15T03:14:37Z"
"","3287","KAFKA-5416: Re-prepare transition to CompleteCommit/Abort upon retrying append to log","In `TransationStateManager`, we reset the pending state if an error occurred while appending to log; this is correct except that for the `TransactionMarkerChannelManager`, as it will retry appending to log and if eventually it succeeded, the transaction metadata's completing transition will throw an IllegalStateException since pending state is None, this will be thrown all the way to the `KafkaApis` and be swallowed.  1. When re-enqueueing to the retry append queue, re-prepare transition to set its pending state. 2. A bunch of log4j improvements based the debugging experience. The main principle is to make sure all error codes that is about to sent to the client will be logged, and unnecessary log4j entries to be removed. 3. Also moved some log entries in ReplicationUtils.scala to `trace`: this is rather orthogonal to this PR but I found it rather annoying while debugging the logs. 4. A couple of unrelated bug fixes as pointed by @hachikuji and @apurvam .","closed","","guozhangwang","2017-06-09T23:08:33Z","2017-07-15T22:06:56Z"
"","2829","HOTFIX: Use a true sentinel for `UseDefaultAcls`","In 67fc2a91a6, we are using an empty collection and comparing via value equality, so if a user passes an empty collection, they will get the default ACLs instead of no ACLs. We fix that issue here.","closed","","ijuma","2017-04-08T00:41:48Z","2017-09-05T09:51:11Z"
"","2698","MINOR: Increase Throttle lower bound assertion in throttling_test","Improves the reliability of this test by decreasing the lower bound (this is to be expected as throttling  takes the first fetch to stabilise and will ""over-fetch"" for this first request)","closed","","benstopford","2017-03-16T21:17:28Z","2017-03-17T12:40:55Z"
"","3027","KAFKA-5192 WindowStore range scan KIP-155","Implements range scan for keys in windowed and session stores  Modifies caching session and windowed stores to use segmented cache keys. Cache keys are internally prefixed with their segment id to ensure key ordering in the cache matches the ordering in the underlying store for keys spread across multiple segments. This should also result in fewer cache keys getting scanned for queries spanning only some segments.","closed","","xvrl","2017-05-11T23:32:05Z","2017-09-22T22:43:00Z"
"","3055","KAFKA-5233 KIP-138: Change punctuate semantics","Implementation for KIP-138: Change punctuate semantics","closed","","mihbor","2017-05-15T14:37:17Z","2017-08-31T08:12:38Z"
"","3102","Follow-up improvements for consumer offset reset tool (KIP-122)","Implement improvements defined here: https://issues.apache.org/jira/browse/KAFKA-5266","closed","","jeqo","2017-05-19T14:50:57Z","2017-12-13T09:12:08Z"
"","2953","handle 0 futures in all()","if we pass in 0 futures to an AllOfAdapter, we should complete immediately  @ijuma @cmccabe","closed","","norwood","2017-05-02T06:45:22Z","2017-05-03T01:29:04Z"
"","2689","KAFKA-4901: Make ProduceRequest thread-safe","If request logging is enabled, `ProduceRequest` can be accessed and mutated concurrently from a network thread (which calls `toString`) and a request handler thread (which calls `clearPartitionRecords()`).  That can lead to a `ConcurrentModificationException` when iterating the `partitionRecords` map.  The underlying thread-safety issue has existed since the server started using the Java implementation of ProduceRequest in 0.10.0. However, we were incorrectly not clearing the underlying struct until 0.10.2, so `toString` itself was thread-safe until that change. In 0.10.2, `toString` is no longer thread-safe and we could potentially see a `NullPointerException` given the right set of interleavings between `toString` and `clearPartitionRecords` although we haven't seen that happen yet.  In trunk, we changed the requests to have a `toStruct` method instead of creating a struct in the constructor and `toString` was no longer printing the contents of the `Struct`. This accidentally fixed the race condition, but it meant that request logging was less useful.  A couple of days ago, `AbstractRequest.toString` was changed to print the contents of the request by calling `toStruct().toString()` and reintroduced the race condition. The impact is more visible because we iterate over a `HashMap`, which proactively checks for concurrent modification (unlike arrays).  We will need a separate PR for 0.10.2.","closed","","ijuma","2017-03-15T13:25:43Z","2017-09-05T09:06:13Z"
"","3208","KAFKA-5325: Avoid and handle exceptions for Kerberos re-login","If producer creates a connection during Kerberos re-login (after logout, before login), there are no principals in the subject and `SaslClientAuthenticator.configure` may throw an exception while trying to determine the principal. A socket channel is created and its key registered with the selector, but the `RuntimeException` thrown leaves the key registered with the selector without adding the channel to the channel list. This results in an infinite loop of `NullPointerExceptions`. The PR applies two fixes: 1. Convert the `RuntimeException` to a meaningful `KafkaException` 2. Handle any exception in `buildChannel`, cleanup and throw `IOException`. Retries will take care of re-connections.","closed","","rajinisivaram","2017-06-02T14:07:33Z","2018-03-28T11:24:48Z"
"","2622","KAFKA-4631: Request metadata in consumer if topic/partitions unavailable","If leader node of one more more partitions in a consumer subscription are temporarily unavailable, request metadata refresh so that partitions skipped for assignment dont have to wait for metadata expiry before reassignment. Metadata refresh is also requested if a subscribe topic or assigned partition doesn't exist.","closed","","rajinisivaram","2017-03-01T14:05:32Z","2017-03-16T21:46:24Z"
"","2608","KAFKA-4779: Request metadata in consumer if partitions unavailable","If leader node of one more more partitions in a consumer subscription are temporarily unavailable, request metadata refresh so that partitions skipped for assignment dont have to wait for metadata expiry before reassignment.","closed","","rajinisivaram","2017-02-28T13:15:13Z","2017-03-01T13:55:59Z"
"","2924","change connection refused message from level DEBUG into WARN.","If broker is down, we supply logging message to let user know `java.net.ConnectException: Connection refused` but not should be debug message because in normal situation user may not set logging level with **INFO** or **WARN** but not **DEBUG**.  ``` 2017-04-27 15:54:47,759 DEBUG [org.apache.kafka.common.network.Selector] - Connection with localhost/127.0.0.1 disconnected java.net.ConnectException: Connection refused         at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)         at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)         at org.apache.kafka.common.network.PlaintextTransportLayer.finishConnect(PlaintextTransportLayer.java:50)         at org.apache.kafka.common.network.KafkaChannel.finishConnect(KafkaChannel.java:80)         at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:339)         at org.apache.kafka.common.network.Selector.poll(Selector.java:307)         at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:359)         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:230)         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:206)         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:197)         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:132)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:223)         at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:200)         at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:285)         at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1037)         at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1002)         at com.kafka.Test.main(Test.java:27) ```","closed","","jedichien","2017-04-27T08:02:31Z","2017-05-02T12:07:40Z"
"","2868","Minor fix record collector","If `partition==null` and `partitioner!=null` we should not fall back to default partitioner (as we do before the patch if `producer.partitionsFor(...)` returns empty list. Falling back to default partitioner might corrupt hash partitioning.","closed","","mjsax","2017-04-19T03:24:14Z","2017-04-21T18:42:56Z"
"","3441","MINOR: Enable the TransactionsBounceTest","I'll let this have multiple runs on the branch builder to see if it fails, and investigate if so.","closed","","apurvam","2017-06-27T05:24:47Z","2017-06-28T17:23:34Z"
"","2887","KAFKA-5097: Add testFetchAfterPartitionWithFetchedRecordsIsUnassigned","I verified that the test would trigger an `IllegalStateException` if the `position` call was added back.","closed","","ijuma","2017-04-21T13:54:57Z","2017-09-05T09:49:16Z"
"","2791","MINOR: Fix Deadlock in StreamThread","I think this may be the (or on of them) reason we see Jenkins jobs time out at times. At least I can reproduce this to cause tests to time out with a certain rate.  With current trunk there is a possibility to run into this:  ```sh ""kafka-streams-close-thread"" #585 daemon prio=5 os_prio=0 tid=0x00007f66d052d800 nid=0x7e02 waiting for monitor entry [0x00007f66ae2e5000]    java.lang.Thread.State: BLOCKED (on object monitor) 	at org.apache.kafka.streams.processor.internals.StreamThread.close(StreamThread.java:345) 	- waiting to lock <0x000000077d33c538> (a org.apache.kafka.streams.processor.internals.StreamThread) 	at org.apache.kafka.streams.KafkaStreams$1.run(KafkaStreams.java:474) 	at java.lang.Thread.run(Thread.java:745)  ""appId-bd262a91-5155-4a35-bc46-c6432552c2c5-StreamThread-97"" #583 prio=5 os_prio=0 tid=0x00007f66d052f000 nid=0x7e01 waiting for monitor entry [0x00007f66ae4e6000]    java.lang.Thread.State: BLOCKED (on object monitor) 	at org.apache.kafka.streams.KafkaStreams.setState(KafkaStreams.java:219) 	- waiting to lock <0x000000077d335760> (a org.apache.kafka.streams.KafkaStreams) 	at org.apache.kafka.streams.KafkaStreams.access$100(KafkaStreams.java:117) 	at org.apache.kafka.streams.KafkaStreams$StreamStateListener.onChange(KafkaStreams.java:259) 	- locked <0x000000077d42f138> (a org.apache.kafka.streams.KafkaStreams$StreamStateListener) 	at org.apache.kafka.streams.processor.internals.StreamThread.setState(StreamThread.java:168) 	- locked <0x000000077d33c538> (a org.apache.kafka.streams.processor.internals.StreamThread) 	at org.apache.kafka.streams.processor.internals.StreamThread.setStateWhenNotInPendingShutdown(StreamThread.java:176) 	- locked <0x000000077d33c538> (a org.apache.kafka.streams.processor.internals.StreamThread) 	at org.apache.kafka.streams.processor.internals.StreamThread.access$1600(StreamThread.java:70) 	at org.apache.kafka.streams.processor.internals.StreamThread$RebalanceListener.onPartitionsRevoked(StreamThread.java:1321) 	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinPrepare(ConsumerCoordinator.java:406) 	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:349) 	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:310) 	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:296) 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1037) 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1002) 	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:531) 	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:669) 	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:326)  ```  In a nutshell: `KafkaStreams` and `StreamThread` are both waiting for each other since another intermittend `close` (eg. from a test) comes along  also trying to lock on `KafkaStreams` :  ```sh ""main"" #1 prio=5 os_prio=0 tid=0x00007f66d000c800 nid=0x78bb in Object.wait() [0x00007f66d7a15000]    java.lang.Thread.State: WAITING (on object monitor) 	at java.lang.Object.wait(Native Method) 	at java.lang.Thread.join(Thread.java:1249) 	- locked <0x000000077d45a590> (a java.lang.Thread) 	at org.apache.kafka.streams.KafkaStreams.close(KafkaStreams.java:503) 	- locked <0x000000077d335760> (a org.apache.kafka.streams.KafkaStreams) 	at org.apache.kafka.streams.KafkaStreams.close(KafkaStreams.java:447) 	at org.apache.kafka.streams.KafkaStreamsTest.testCannotStartOnceClosed(KafkaStreamsTest.java:115) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.ExpectException.evaluate(ExpectException.java:19) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) 	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) 	at org.junit.rules.RunRules.evaluate(RunRules.java:20) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) 	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:71) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:237) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) ```  => causing a deadlock.  Fixed this by softer locking on the state change, that guarantees atomic changes to the state but does not lock on the whole object (I at least could not find another method that would require more than atomicly-locked access except for `setState`). Also qualified the state listeners with their outer-class to make the whole code-flow around this more readable (having two interfaces with the same naming for interface and method and then using them between their two outer classes is crazy hard to get imo :)).  Easy to reproduced yourself by running `org.apache.kafka.streams.KafkaStreamsTest` in a loop for a bit (save yourself some time by running 2-4 in parallel :)). Eventually it will lock on one of the tests (for me this takes less than 1 min with 4 parallel runs).","closed","","original-brownbear","2017-04-02T10:52:37Z","2017-04-03T18:58:50Z"
"","2940","don't call the zookeeper host strings URLs","I spent a few minutes trying to guess what the scheme part might be. Documentation would be better if it doesn't say it is expecting [a format it isn't expecting][url-syntax].  [url-syntax]: https://en.wikipedia.org/w/index.php?title=URL&oldid=777577354#Syntax","closed","","travisfw","2017-04-28T23:41:34Z","2018-01-26T19:17:53Z"
"","2955","KAFKA-4772: Exploit #peek to implement #print() and other methods","I remove `KeyValuePrinter` and `KStreamForeach` two class, then implements them by `KStreamPeek`. So, now `KStreamPeek` can do `KeyValuePrinter` and `KStreamForeach` job.","closed","","jedichien","2017-05-02T10:56:03Z","2017-05-18T04:25:13Z"
"","2739","KAFKA-1449: Use CRC32C for checksum of V2 message format","I manually tested that Crc32CTest and AbstractChecksums pass with JDK 9. I also verified that `Java9ChecksumFactory` is used in that case.","closed","","ijuma","2017-03-25T23:38:43Z","2017-09-05T09:06:03Z"
"","3234","MINOR: Optimise performance of `Topic.validate()`","I included a JMH benchmark and the results follow. The implementation in this PR takes no more than 1/10th of the time when compared to trunk. I also included results for an alternative implementation that is a little slower than the one in the PR.  Trunk: ```text TopicBenchmark.testValidate                                topic  avgt   15  134.107 ±  3.956  ns/op TopicBenchmark.testValidate                    longer-topic-name  avgt   15  316.241 ± 13.379  ns/op TopicBenchmark.testValidate  very-long-topic-name_with_more_text  avgt   15  636.026 ± 30.272  ns/op ```  Implementation in the PR: ```text TopicBenchmark.testValidate                                topic  avgt   15  13.153 ± 0.383  ns/op TopicBenchmark.testValidate                    longer-topic-name  avgt   15  26.139 ± 0.896  ns/op TopicBenchmark.testValidate  very-long-topic-name.with_more_text  avgt   15  44.829 ± 1.390  ns/op ```  Alternative implementation where boolean validChar = Character.isLetterOrDigit(c) || c == '.' || c == '_' || c == '-'; ```text TopicBenchmark.testValidate                                topic  avgt   15  18.883 ± 1.044  ns/op TopicBenchmark.testValidate                    longer-topic-name  avgt   15  36.696 ± 1.220  ns/op TopicBenchmark.testValidate  very-long-topic-name_with_more_text  avgt   15  65.956 ± 0.669  ns/op ```","closed","","ijuma","2017-06-04T16:09:51Z","2017-06-18T09:27:02Z"
"","2883","0.10.0 kafka how to delete a deprecated group","I have a group called HBASE1,but I can't find it by kafka-consumer-groups.sh shell。  I do it by call: `/usr/hdp/2.5.0.0-1245/kafka/bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --list` there is some consumer groups likes system produces, the follows: ``` console-consumer-52667 console-consumer-48120 console-consumer-17316 ```  and then I call: `/kafka-consumer-groups.sh --new-consumer --bootstrap-server dn5:6667  --list` there is some consumer groups is running by my named。 ``` PHOTOSEND HDFS REDIS ```  but I can't find the group is HBASE1,and it just a test group,now I want to delete it,I try called: `kafka-consumer-groups.sh --zookeeper localhost:2181 --delete -group HBASE1` system said: `Delete for group HBASE1 failed because group does not exist.`  But I can find it in my kafkaManager,And the group is my test group,how could I delete it!","closed","","linjikai","2017-04-21T07:36:05Z","2018-02-25T07:46:32Z"
"","3085","KAFKA-4830: Augment KStream.print() to allow users pass in extra parameters in the printed string","I extend `KStream#print()` to `KStream#print(KeyValueMapper)`. So I add the following methods : 1. `KStream#print(KeyValueMapper)` 2. `KStream#print(KeyValueMapper, String streamName)` 3. `KStream#print(KeyValueMapper, Serde, Serde)` 4. `KStream#print(KeyValueMapper, Serde, Serde, String streamName)`","closed","","jedichien","2017-05-18T09:21:14Z","2017-07-20T14:28:18Z"
"","2664","MINOR: Added additional -start/-stop files for consistency","I added \*-start.sh and \*-stop.sh for connect-\* and kafka-mirror-maker to follow the pattern used by the other services. The purpose of this is to make all services follow the pattern of ${COMPONENT_NAME}-start.sh and ${COMPONENT_NAME}-stop.sh.  I did a copy of the files into the -start.sh files instead of making one use the other because I was trying to be the most patch-friendly for who know what people did in their forks, but the idea would be to logically deprecate the non -start.sh files that should go away, update the documentation, etc., and eventually (with any luck) delete the originals on the next major version bump.  The contribution is my original work and that I license the work to the project under the project's open source license.","closed","connect,","nnordrum","2017-03-09T20:26:33Z","2021-06-23T16:05:23Z"
"","3156","KAFKA-5031: validate count of records for DefaultRecordBatch in validateMessagesAndAssignOffsets","https://issues.apache.org/jira/browse/KAFKA-5031  Implements additional check for `DefaultRecordBatch` that compares number of records declared in the header with actual number of records.","closed","","gosubpl","2017-05-26T20:20:23Z","2017-06-17T08:31:29Z"
"","2787","KAFKA-4810: Kafka Connect SchemaBuilder unset fields validation fix","https://issues.apache.org/jira/browse/KAFKA-4810 > Currently SchemaBuilder is strict when checking that certain fields have not been set yet (e.g. version, name, doc). It just checks that the field is null. This is intended to protect the user from buggy code that overwrites a field with different values, but it's a bit too strict currently. In generic code for converting schemas (e.g. Converters) you will sometimes initialize a builder with these values (e.g. because you get a SchemaBuilder for a logical type, which sets name & version), but then have generic code for setting name & version from the source schema.  Changed the validation method to not only check if a field is null but also to check if the new value that is being set is the same as the current value of the field.  @ewencp","closed","connect,","vitaly-pushkar","2017-04-01T20:42:26Z","2020-10-16T06:35:05Z"
"","2806","KAFKA-4810: Kafka Connect SchemaBuilder unset fields validation fix","https://issues.apache.org/jira/browse/KAFKA-4810   > Currently SchemaBuilder is strict when checking that certain fields have not been set yet (e.g. version, name, doc). It just checks that the field is null. This is intended to protect the user from buggy code that overwrites a field with different values, but it's a bit too strict currently. In generic code for converting schemas (e.g. Converters) you will sometimes initialize a builder with these values (e.g. because you get a SchemaBuilder for a logical type, which sets name & version), but then have generic code for setting name & version from the source schema.  Changed the validation method to not only check if a field is null but also to check if the new value that is being set is the same as the current value of the field. @ewencp","closed","","vitaly-pushkar","2017-04-04T20:20:04Z","2017-04-04T21:59:45Z"
"","2493","KAFKA-4720: add a KStream#peek(ForeachAction)","https://issues.apache.org/jira/browse/KAFKA-4720  Peek is a handy method to have to insert diagnostics that do not affect the stream itself, but some external state such as logging or metrics collection.","closed","","stevenschlansker","2017-02-03T20:49:20Z","2017-04-12T15:42:17Z"
"","2521","KAFKA-4709：Error message from Struct.validate() should include the name of the offending field.","https://issues.apache.org/jira/browse/KAFKA-4709","closed","","Aegeaner","2017-02-09T08:34:05Z","2017-02-16T21:48:19Z"
"","2761","KAFKA-4692: Make testNo thread safe","https://issues.apache.org/jira/browse/KAFKA-4692 should be resolved by this.  The problem here (or at least one problem, causing trouble) appears to have been that the `testNo` field is not thread-safe the way it is used here (`volatile` doesn't help anything really) to generate a unique test number (for creating non-conflicting topic names). If two instances of this class are set up inside the same fork and `testNo++;` is run in one just as the other runs e.g.   ```java     private void createTopics() throws InterruptedException {         streamOneInput = ""stream-one-"" + testNo;         outputTopic = ""output-"" + testNo;         CLUSTER.createTopic(streamOneInput, 3, 1);         CLUSTER.createTopic(outputTopic);     } ```  then both will have the same value for the topic names while sharing the same static `CLUSTER` field too. I fixed this by simply making the actually used `testNo` an instance variable that is derived from an increasing static counter for each test in an atomic way. I think this should resolve any naming collisions here.","closed","","original-brownbear","2017-03-29T19:38:24Z","2017-03-30T17:47:32Z"
"","2548","KAFKA-4159: allow consumer/specific configs at connector level","https://issues.apache.org/jira/browse/KAFKA-4159","closed","connect,","sjdurfey","2017-02-14T16:56:07Z","2020-10-16T06:08:13Z"
"","3241","KAFKA-5384: Enable topic deletion by default","https://cwiki.apache.org/confluence/display/KAFKA/KIP-162+-+Enable+topic+deletion+by+default","closed","","gwenshap","2017-06-06T04:34:38Z","2017-07-18T17:07:03Z"
"","2960","KAFKA-4343: Expose Connector type in REST API","https://cwiki.apache.org/confluence/display/KAFKA/KIP-151+Expose+Connector+type+in+REST+API","closed","connect,","norwood","2017-05-02T20:43:11Z","2020-10-16T06:08:14Z"
"","2811","MINOR: update javadoc on ReadOnlyWindowStore","Highlight that the range in `fetch` is inclusive of both `timeFrom` and `timeTo`","closed","","dguy","2017-04-05T10:45:25Z","2017-05-16T14:05:06Z"
"","2884","MINOR: fix javadoc comment","Here's where it's called: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamTransform.java#L64-L67","closed","","mihbor","2017-04-21T08:28:54Z","2017-04-21T20:27:59Z"
"","3130","KAFKA-5202: Handle topic deletion while trying to send txn markers","Here is the sketch of this proposal:  1. When it is time to send the txn markers, only look for the leader node of the partition once instead of retrying, and if that information is not available, it means the partition is highly likely been removed since it was in the cache before. In this case, we just remove the partition from the metadata object and skip putting into the corresponding queue, and if all partitions' leader broker are non-available, complete this delayed operation to proceed to write the complete txn log entry.  2. If the leader id is unknown from the cache but the corresponding node object with the listener name is not available, it means that the leader is likely unavailable right now. Put it into a separate queue and let sender thread retry fetching its metadata again each time upon draining the queue.  One caveat of this approach is the delete-and-recreate case, and the argument is that since all the messages are deleted anyways when deleting the topic-partition, it does not matter whether the markers are on the log partitions or not.","closed","","guozhangwang","2017-05-23T23:32:16Z","2017-11-06T22:44:32Z"
"","2606","kafka4811: ReplicaFetchThread may fail to create due to existing metric","Have fetcherThreadMap keyed off brokerId + fetcherId instead of broker + fetcherId, but did not consider the case where port is changed.","closed","","huxihx","2017-02-28T05:36:45Z","2017-03-02T22:44:20Z"
"","3155","KAKFA-5334: Allow rocksdb.config.setter to be specified as a String or Class instance.","Handle` rocksdb.config.setter` being set as a class name or class instance.","closed","","twbecker","2017-05-26T14:28:15Z","2017-06-05T17:57:58Z"
"","2520","KAFKA-4749: fix join-time-max and sync-time-max MeasurableStat type","GroupCoordinatorMetrics currently sets up join-time-max and sync-time-max incorrectly as a ""new Avg()"" MeasurableStat instead of ""new Max()""","closed","","onurkaraman","2017-02-09T01:10:36Z","2017-02-09T02:42:39Z"
"","3460","KAFKA-5534: `offsetForTimes` result should include partitions with no offset","For topics that support timestamp search, if no offset is found for a partition, the partition should still be included in the result with a `null` offset value. This `KafkaConsumer` method currently excludes such partitions from the result.","closed","","vahidhashemian","2017-06-29T22:18:01Z","2017-07-21T00:39:51Z"
"","2901","KAFKA-5018: LogCleaner tests to verify behaviour of message format v2","For https://issues.apache.org/jira/browse/KAFKA-5018:  * Added test for `baseOffset` behaviour after compaction   * Added helper method for writing a multi-record batch * Dried up handling of `LogConfig.SegmentIndexBytesProp` and added comments on the chosen magic values","closed","","original-brownbear","2017-04-23T20:22:53Z","2017-09-12T07:16:45Z"
"","3124","KAFKA-5263: Avoid tight polling loop in consumer with no ready nodes","For consumers with manual partition assignment, await metadata when there are no ready nodes to avoid busy polling.","closed","","rajinisivaram","2017-05-23T10:34:56Z","2017-05-25T10:24:56Z"
"","3458","KAFKA-5538: User-specified Producer/Consumer config doesn't effect wi…","For configuration of Kafka source and Kafka sink tasks, the same parameters can be used but need to be prefixed with consumer. and producer. The worker will take off the prefix and get user specified configurations, but the KafkaBackingStores will not. All the three KafkaBackingStores just took originals from the Kafka config without taking off the prefix, so the producer/consumer will ignore these configurations. (e.g. Kerberos configs)","closed","connect,","Aegeaner","2017-06-29T11:23:18Z","2020-03-29T01:48:25Z"
"","3180","MINOR: reuse decompression buffers in log cleaner","follow-up to KAFKA-5150, reuse decompression buffers in the log cleaner thread.  @ijuma @hachikuji","closed","","xvrl","2017-05-31T16:42:09Z","2017-06-03T02:31:13Z"
"","2778","MINOR: fix cleanup phase for KStreamWindowAggregateTest","fixes: ``` java.nio.file.NoSuchFileException: /tmp/test7863510415433793941/topic2-Canonized/topic2-Canonized-197001010000/000015.sst 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:55) 	at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144) 	at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:97) 	at java.nio.file.Files.readAttributes(Files.java:1686) 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:105) 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199) 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199) 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199) 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69) 	at java.nio.file.Files.walkFileTree(Files.java:2602) 	at java.nio.file.Files.walkFileTree(Files.java:2635) 	at org.apache.kafka.common.utils.Utils.delete(Utils.java:555) 	at org.apache.kafka.streams.kstream.internals.KStreamWindowAggregateTest.testJoin(KStreamWindowAggregateTest.java:320) ```","closed","","mjsax","2017-03-31T17:26:09Z","2017-04-03T17:05:07Z"
"","2903","MINOR: Fix needless GC + Result time unit in JMH","Fixes two issues with the JMH benchmark example: * Trivial: The output should be in `ops/ms` for readability reasons (it's in the millions of operations per second) * Important: The benchmark is not actually measuring the LRU-Cache performance as most of the time in each run is wasted on concatenating `key + counter` as well as `value + counter`. Fixed by pre-generating 10k K-V pairs (100x the cache capacity) and iterating over them. This brings the performance up by a factor of more than 5 on a standard 4 core i7 (`~6k/ms` before goes to `~35k/ms`).   * Also made static what could be made static in the benchmark class to lower the GC background noise","closed","","original-brownbear","2017-04-24T06:56:28Z","2017-09-18T10:11:37Z"
"","2568","Kafka 4198: Fix Race Condition in KafkaServer Shutdown","Fixes the initially reported issue in https://issues.apache.org/jira/browse/KAFKA-4198.  The relevant part (the other changes are just dead whitespaces that IDEA removed and one dead import fixed automatically by IDEA) in fixing the initial issue here is the change to `kafka.server.KafkaServer#shutdown`.  It contained this step:  ```java       val canShutdown = isShuttingDown.compareAndSet(false, true)       if (canShutdown && shutdownLatch.getCount > 0) { ```  without any fallback for the case of `shutdownLatch.getCount == 0`. So in the case of `shutdownLatch.getCount == 0`  (when a previous call to the shutdown method was right about to finish) you would set `isShuttingDown` to true again without any possibility of ever getting the server started (since `startup` will check `isShuttingDown` before setting up a new latch with count 1).  Long story short: concurrent calls to shutdown can get the server locked in a broken state.  This fixes the reported error:  ```sh java.lang.IllegalStateException: Kafka server is still shutting down, cannot re-start! 	at kafka.server.KafkaServer.startup(KafkaServer.scala:184) 	at kafka.integration.KafkaServerTestHarness$$anonfun$restartDeadBrokers$2.apply$mcVI$sp(KafkaServerTestHarness.scala:117) 	at kafka.integration.KafkaServerTestHarness$$anonfun$restartDeadBrokers$2.apply(KafkaServerTestHarness.scala:116) 	at kafka.integration.KafkaServerTestHarness$$anonfun$restartDeadBrokers$2.apply(KafkaServerTestHarness.scala:116) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.immutable.Range.foreach(Range.scala:160) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at kafka.integration.KafkaServerTestHarness$class.restartDeadBrokers(KafkaServerTestHarness.scala:116) 	at kafka.api.ConsumerBounceTest.restartDeadBrokers(ConsumerBounceTest.scala:34) 	at kafka.api.ConsumerBounceTest$BounceBrokerScheduler.doWork(ConsumerBounceTest.scala:158) ```  That said this error (reported in a comment to the JIRA)  is still left even with this fix:  ```sh kafka.api.ConsumerBounceTest > testConsumptionWithBrokerFailures FAILED     java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer.         at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1271)         at kafka.api.ConsumerBounceTest.consumeWithBrokerFailures(ConsumerBounceTest.scala:96)         at kafka.api.ConsumerBounceTest.testConsumptionWithBrokerFailures(ConsumerBounceTest.scala:69) ```  ... I think this one should get a separate JIRA though. It seems to me that the behaviour of the call to `partition` when a Broker just died is a separate issue from the one initially reported.","closed","","original-brownbear","2017-02-18T12:08:25Z","2017-02-24T16:36:38Z"
"","2642","KAFKA-4848: Fix retryWithBackoff deadlock issue","Fixes related to handling of MAX_POLL_INTERVAL_MS_CONFIG during deadlock and CommitFailedException on partition revoked.","closed","","sjmittal","2017-03-05T12:02:45Z","2017-03-21T04:58:36Z"
"","2640","0.10.2","Fixes related to handling of MAX_POLL_INTERVAL_MS_CONFIG deadlock and CommitFailedException","closed","","sjmittal","2017-03-05T09:09:27Z","2017-03-06T04:39:11Z"
"","2552","KAFKA-4765: Fixed Intentionally Broken Hosts Resolving to 127.0.53.53 in Tests","Fixes https://issues.apache.org/jira/browse/KAFKA-4765 by simply using artificially broken hosts that are not resolved as potential collisions (127.0.53.53s) by some DNS servers.  This change is the only way to build while using Google's `8.8.8.8` (at least in my network). I can't see a downside to making this change, simply makes the build more stable and portable :)  Edit: Also seems my IDEA removed some redundant whitespaces in empty lines unintentionally.","closed","","original-brownbear","2017-02-15T18:12:21Z","2017-02-18T07:55:00Z"
"","2632","KAFKA-3182: Fix Transient Test Failure (kafka.network.SocketServerTest.testSocketsCloseOnShutdown)","Fixes https://issues.apache.org/jira/browse/KAFKA-3182 by: * Turning off Nagle on the sending sockets to force the socket to physically acknowledge after the first write in `sendRequest` * (Still had to do this on Linux though not on Mac) Adding a `200ms` delay between write attempts (my guess would be [Slow Start](https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start) is to blame for this, but with Nagle off 200ms should more than suffice here imo)  ### Before: ![before](https://cloud.githubusercontent.com/assets/6490959/23545356/5015f6a6-fffb-11e6-88f2-418e286a920d.png)  ### After: ![after](https://cloud.githubusercontent.com/assets/6490959/23545331/32265096-fffb-11e6-8843-017580dacfc5.png)","closed","","original-brownbear","2017-03-03T09:22:56Z","2017-03-05T05:58:01Z"
"","2792","MINOR: Fix potential deadlock in consumer close test","Fixes deadlock scenario found during local test run: The main thread was waiting for the coordinator lock. The thread performing close() was holding the coordinator lock and polling to find coordinator. The test expected close() to timeout, but for timing out, the main thread had to update time, which it couldn't since it was waiting for the lock. This fix avoids using coordinator in the main thread during the close task.","closed","","rajinisivaram","2017-04-03T10:32:29Z","2017-04-03T11:59:03Z"
"","2921","KAFKA-5124: autocommit reset earliest fixes race condition","Fixes `org.apache.kafka.streams.integration.utils.IntegrationTestUtils#readKeyValues` potentially starting to `poll` for stream output after the stream finished sending the test data and hence missing it when working with `latest` offsets.","closed","","original-brownbear","2017-04-26T15:12:21Z","2017-04-26T20:07:53Z"
"","3338","KAFKA-5446: Annoying braces showed on log.error using streams","Fixed log.error usage with annoying braces","closed","","ppatierno","2017-06-14T13:40:53Z","2017-06-19T12:55:32Z"
"","3368","KAFKA-5469: Created state changelog topics not logged correctly","Fixed debug logging for the created state changelog topics Added toString() for InternalTopicMetadata and InternalTopicConfig for above debug logging","closed","","ppatierno","2017-06-19T10:59:25Z","2017-07-04T14:32:07Z"
"","3443","KAFKA-5525: Streams reset tool should have same console output with or without dry-run","Fixed console output to be consistent with/without dry-run option","closed","","ppatierno","2017-06-27T12:54:34Z","2017-07-06T07:52:00Z"
"","2613","MINOR. Fix tests/docker/Dockerfile","Fix tests/docker/Dockerfile to put the old Kafka distributions in the correct spot for tests.  Also, run_tests.sh should exit with an error code if image rebuilding fails, rather than silently falling back to an older image.","closed","","cmccabe","2017-02-28T22:30:50Z","2019-05-20T18:36:21Z"
"","2871","KAFKA-5088: some spelling error in code comment","fix some spelling errors","closed","","auroraxlh","2017-04-19T06:51:41Z","2017-05-18T07:15:45Z"
"","2753","KAFKA-4966：Producer throw a NullPointerException under a network environment where packet loss and error packets exist.","fix KAFKA-4966 NPE in sender complete batch","closed","","Aegeaner","2017-03-28T11:03:39Z","2022-04-21T05:38:56Z"
"","2618","KAFKA-4791: unable to add state store with regex matched topics","Fix for adding state stores with regex defined sources","closed","","bbejeck","2017-03-01T02:33:25Z","2017-03-31T17:00:05Z"
"","2949","KAFKA-5055: Kafka Streams skipped-records-rate sensor bug","Fix as described in the [KAFKA-5055 Jira comment](https://issues.apache.org/jira/browse/KAFKA-5055?focusedCommentId=15990086&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15990086).  @mjsax @guozhangwang take a look","closed","","dpoldrugo","2017-04-30T21:15:21Z","2017-05-03T21:20:29Z"
"","2546","KAFKA-4764: Improve diagnostics for SASL auth failures","First step towards improving handling of client SASL authentication failures.","closed","","rajinisivaram","2017-02-14T14:58:11Z","2017-09-14T09:26:57Z"
"","2615","Add a consumer offset migration tool","Extends #1715 to support renaming the consumer group as part of the migration.   I tested this pretty thoroughly and seems to be working perfectly.  This is particularly useful if your consumers aren't Java-based, as many 3rd-party clients don't support `dual.commit` forcing the use of a migration script like this in order to migrate.","closed","","jeffwidman","2017-02-28T23:54:04Z","2020-03-11T08:17:06Z"
"","3244","MINOR: expose commit interval config for SimpleBenchmark","Expose the streams commit interval config for different testing scenarios.","closed","","bbejeck","2017-06-06T13:20:27Z","2017-06-06T17:59:12Z"
"","2907","kafka-5118: Improve message for Kafka failed startup with non-Kafka data in data.dirs","Explicitly throwing clear exceptions when starting up a Kafka with some non-Kafka data in data.dirs.","closed","","huxihx","2017-04-25T03:42:52Z","2017-05-05T01:31:02Z"
"","3158","KAFKA-5337: LagBasedAssignor partition assignment strategy","Existing partition assignment strategies (RangeAssignor and RoundRobinAssignor) do not account for the current consumer group lag on each partition. This can result in sub-optimal assignments when the distribution of lags for a given topic and consumer group is skewed.  The LagBasedAssignor operates on a per-topic basis, and attempts to assign partitions such that lag is distributed as evenly across a consumer group.  ## Algorithm  For each topic, first obtain the lag on all partitions. Lag on a given partition is the difference between the end offset and the last offset committed by the consumer group. If no offsets have been committed for a partition we determine the lag based on the code auto.offset.reset property. If auto.offset.reset=latest, we assume a lag of 0. If auto.offset.reset=earliest (or any other value) we assume lag equal to the total number of message currently available in that partition.  Next, create a map storing the current total lag of all partitions assigned to each member of the consumer group. Partitions are assigned in decreasing order of lag, with each partition assigned to the consumer with least total number of assigned partitions, breaking ties by assigning to the consumer with the least total currently assigned lag.  Assigning partitions evenly across consumers (by partition count) ensures that the assignment is reasonably balanced (by partition count) when all partitions have a current lag of 0 or if the distribution of lags is heavily skewed. It also gives the consumer group the best possible chance of remaining balanced if the assignment is retained for a long period (assuming throughput is consistent across members of the consumer group).","closed","","grantneale","2017-05-27T05:39:53Z","2017-05-28T23:04:20Z"
"","3470","MINOR: Changed javadoc on KafkaConsumer#endOffsets to mention end offset for…","Existing javadoc fails to mention that endOffsets returns zero for empty TopicPartitions.  Just added that important little tidbit to save developers' time.","closed","","ekenny","2017-06-30T18:02:38Z","2018-02-07T19:12:07Z"
"","2918","0.10.2","essai","closed","","jpi-atos","2017-04-26T11:28:04Z","2017-04-27T12:19:06Z"
"","3054","KAFKA-5241: GlobalKTable does not checkpoint offsets after restoring state","Ensure checkpointable offsets for GlobalKTables are always written on close.","closed","","twbecker","2017-05-15T12:33:14Z","2018-03-26T18:30:41Z"
"","2773","KAFKA-4986: Add producer per task support","Enable producer per task if exactly-once config is enabled.","closed","","mjsax","2017-03-30T21:00:04Z","2017-04-28T00:00:41Z"
"","3046","MINOR: Consolidate Topic classes","During the 0.11.0.0 cycle, a Java version of the class was introduced so that Streams could use it. Given that it includes the bulk of the functionality of the Scala version of the class, it makes sense to consolidate them.  While doing this, I noticed that one of the tests for the Java class (`shouldThrowOnInvalidTopicNames`) was broken as it only checked if the first topic name in the list was invalid.","closed","","ijuma","2017-05-13T23:31:47Z","2017-06-18T09:27:46Z"
"","3283","KAFKA-2170 [WIP]: Updated Fixes For Windows Platform","During stress testing of kafka 0.10.2.1 on a Windows platform, our group has encountered some issues that appear to be known to the community but not fully addressed by kafka.  Using:  https://github.com/apache/kafka/pull/154  as a guide, we have made derived changes to the source code and automated tests such that the ""clients"" and ""core"" tests pass for us on Windows and Linux platforms.  Our stress tests succeed as well.  This pull request adapts those changes to merge and build with kafka/trunk.  The ""clients"" and ""core"" tests from kafka/trunk pass on Linux for us with these changes in place, and all tests pass on Windows except:  ConsumerBounceTest (intermittent failures) TransactionsTest DeleteTopicTest.testDeleteTopicWithCleaner EpochDrivenReplicationProtocolAcceptanceTest.offsetsShouldNotGoBackwards  Our intention is to help efforts to further kafka support for the Windows platform.  Our changes are the work of engineers from Nexidia building upon the work found in the aforementioned pull request link, and they are contributed to the community per kafka's open source license.  We welcome all feedback and look forward to working with the kafka community.  Matt Briggs Principal Software Engineer Nexidia, a NICE Analytics Company www.nexidia.com","open","","nxmbriggs404","2017-06-09T15:59:27Z","2018-07-03T09:30:26Z"
"","3418","HOTFIX: Don't check metadata unless you are creating topic","During a broker rolling upgrade, it's likely we don't have enough brokers ready yet. If streams does not need to create a topic it shouldn't check how many brokers are up.   The system test for this is in a separate PR: https://github.com/apache/kafka/pull/3411","closed","","enothereska","2017-06-23T08:46:13Z","2017-06-23T15:18:22Z"
"","3202","KAFKA-5364: Don't fail producer if drained partition is not yet in transaction","Due to the async nature of the producer, it is possible to attempt to drain a messages whose partition hasn't been added to the transaction yet. Before this patch, we considered this a fatal error. However, it is only in error if the partition isn't in the queue to be sent to the coordinator.   This patch updates the logic so that we only fail the producer if the partition would never be added to the transaction. If the partition of the batch is yet to be added, we will simply wait for the partition to be added to the transaction before sending the batch to the broker.","closed","","apurvam","2017-06-02T00:20:54Z","2017-06-02T07:54:47Z"
"","3413","KAFKA-5502: read current brokers from zookeeper upon processing broker change","Dong Lin's testing of the 0.11.0 release revealed a controller-side performance regression in clusters with many brokers and many partitions when bringing up many brokers simultaneously.  The regression is caused by KAFKA-5028: a Watcher receives WatchedEvent notifications from the raw ZooKeeper client EventThread. A WatchedEvent only contains the following information: - KeeperState - EventType - path  Note that it does not actually contain the current data or current set of children associated with the data/child change notification. It is up to the user to do this lookup to see the current data or set of children.  ZkClient is itself a Watcher. When it receives a WatchedEvent, it puts a ZkEvent into its own queue which its own ZkEventThread processes. Users of ZkClient interact with these notifications through listeners (IZkDataListener, IZkChildListener). IZkDataListener actually expects as input the current data of the watched znode, and likewise IZkChildListener actually expects as input the current set of children of the watched znode. In order to provide this information to the listeners, the ZkEventThread, when processing the ZkEvent in its queue, looks up the information (either the current data or current set of children) simultaneously sets up the next watch, and passes the result to the listener.  The regression introduced in KAFKA-5028 is the time at which we lookup the information needed for the event processing.  In the past, the lookup from the ZkEventThread during ZkEvent processing would be passed into the listener which is processed immediately after. For instance in ZkClient.fireChildChangedEvents: ``` List children = getChildren(path); listener.handleChildChange(path, children); ``` Now, however, there are multiple listeners that pass information looked up by the ZkEventThread into a ControllerEvent which gets processed potentially much later. For instance in BrokerChangeListener: ``` class BrokerChangeListener(controller: KafkaController) extends IZkChildListener with Logging {   override def handleChildChange(parentPath: String, currentChilds: java.util.List[String]): Unit = {     import JavaConverters._     controller.addToControllerEventQueue(controller.BrokerChange(currentChilds.asScala))   } } ```  In terms of impact, this: - increases the odds of working with stale information by the time the ControllerEvent gets processed. - can cause the cluster to take a long time to stabilize if you bring up many brokers simultaneously.  In terms of how to solve it: - (short term) just ignore the ZkClient's information lookup and repeat the lookup at the start of the ControllerEvent. This is the approach taken in this ticket. - (long term) try to remove a queue. This basically means getting rid of ZkClient. This is likely the approach that will be taken in KAFKA-5501.","closed","","onurkaraman","2017-06-22T20:09:48Z","2017-06-22T21:52:48Z"
"","3370","KAFKA-5772 Improve Util classes","Done for https://issues.apache.org/jira/browse/KAFKA-5772  Utils with static methods should not be instantiated, hence marking the classes `final` and adding a `private` constructor.  this is consistent w/ some of the Util classes, such as `ByteUtils`, see: https://github.com/apache/kafka/blob/d345d53e4e5e4f74707e2521aa635b93ba3f1e7b/clients/src/main/java/org/apache/kafka/common/utils/ByteUtils.java#L29-L31","closed","","matzew","2017-06-19T12:44:29Z","2017-12-22T20:09:44Z"
"","3080","KAFKA-4222: QueryableIntegrationTest.queryOnRebalance transient failure","Don't produce messages on a separate thread continuosly. Just produce one of each value and stop. Close the producer once finished.","closed","","dguy","2017-05-17T16:09:03Z","2017-05-18T10:30:02Z"
"","3386","KAFKA-4249: Document how to customize GC logging options for broker","Document the KAFKA_GC_LOG_OPTS environment variable as well as the common `kafka-run-class.sh` options.  The contribution is my original work and I license the work to the project under the project's open source license.","open","","tombentley","2017-06-20T17:14:47Z","2018-03-02T19:30:17Z"
"","2609","KAFKA-4677: [Follow Up] add optimization to StickyTaskAssignor for rolling rebounce","Detect when a rebalance has happened due to one or more existing nodes bouncing. Keep assignment of previous active tasks the same and only assign the tasks that were not active to the new clients.","closed","","dguy","2017-02-28T15:01:36Z","2017-03-30T11:10:42Z"
"","2915","KAFKA-5119: Ensure global metrics are empty before running testMetricCollectionAfterShutdown","Depending on the test execution order, the global registry would contain some metrics causing the test to fail.","closed","","ijuma","2017-04-25T23:18:57Z","2017-09-05T09:01:00Z"
"","2486","KAFKA-4724: Clean up of state directories can possibly remove stores that are about to be used by another thread","Delay the cleanup of state directories that are not locked and not owned by the current thread such that we only remove the directory if its last modified is < now - cleanupDelayMs. This also helps to avoid a race between threads on the same instance, where during rebalance, one thread releases the lock on the state directory, and before another thread can take the lock, the cleanup runs and removes the data.","closed","","dguy","2017-02-02T13:01:44Z","2017-02-17T00:58:07Z"
"","2724","MINOR: log state store restore offsets","Debug logging of the start and end offsets used during state store restoration","closed","","dguy","2017-03-22T12:32:34Z","2017-05-16T14:05:09Z"
"","2718","MINOR: log start and end offsets for state store restoration","Debug loggin of the start and end offsets used during state store restoration","closed","","dguy","2017-03-21T17:10:00Z","2017-03-30T11:10:16Z"
"","3417","MINOR: Correct the ConsumerPerformance print format","Currently, the output of `ConsumerPerformance` looks strange. The `header` format as follow: ``` ""time, threadId, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec"" ``` while the `body` as follow: ``` println(""%s, %d, %.4f, %.4f, %d, %.4f"".format(dateFormat.format(endMs), id, totalMBRead,         1000.0 * (mbRead / elapsedMs), messagesRead, ((messagesRead - lastMessagesRead) / elapsedMs) * 1000.0)) ```  So we get the follow result: ``` time, data.consumeed.in.MB, MB.sec, data.consumeed.in.nMsg, nMsg.Sec 09:52:00, 0, 1100.3086, 220.0177, 563358, 112649.0702 ``` So the `header` and `body` mismatching.  And also, this pr makes the functions more readable.","closed","","ConeyLiu","2017-06-23T02:16:44Z","2017-07-15T04:05:08Z"
"","2633","MINOR: Add simplified aggregate methods to KGroupedStream","Currently the KGroupedStream aggregate methods requires users to supply (Serde) null if they intend to use the default value serde. This pull request aligns the methods available in KGroupedStream with those in KGroupedTable allowing users to not supply a (Serde) null when using the default value serde. I added the same disclaimer in the javadocs as that found in the KGroupedTable informing the user to use the other available methods if the result value type doesn't match the default value serde. Unit tests were updated to go through these methods instead of supplying a Serdes.String() which doesn't get used.","closed","","ghost","2017-03-03T14:49:14Z","2017-05-04T17:29:18Z"
"","2741","KAFKA-4953: Global Store: cast exception when initialising with in-memory logged state store","Currently it is not possible to initialise a global store with an in-memory logged store via the TopologyBuilder. This results in the following exception: ``` java.lang.ClassCastException: org.apache.kafka.streams.processor.internals.GlobalProcessorContextImpl cannot be cast to org.apache.kafka.streams.processor.internals.RecordCollector$Supplier  	at org.apache.kafka.streams.state.internals.StoreChangeLogger.(StoreChangeLogger.java:52) 	at org.apache.kafka.streams.state.internals.StoreChangeLogger.(StoreChangeLogger.java:44) 	at org.apache.kafka.streams.state.internals.InMemoryKeyValueLoggedStore.init(InMemoryKeyValueLoggedStore.java:56) 	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore$7.run(MeteredKeyValueStore.java:99) 	at org.apache.kafka.streams.processor.internals.StreamsMetricsImpl.measureLatencyNs(StreamsMetricsImpl.java:187) 	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init(MeteredKeyValueStore.java:130) 	at org.apache.kafka.streams.processor.internals.GlobalStateManagerImpl.initialize(GlobalStateManagerImpl.java:97) 	at org.apache.kafka.streams.processor.internals.GlobalStateUpdateTask.initialize(GlobalStateUpdateTask.java:61) 	at org.apache.kafka.test.ProcessorTopologyTestDriver.(ProcessorTopologyTestDriver.java:215) 	at org.apache.kafka.streams.processor.internals.ProcessorTopologyTest.shouldDriveInMemoryLoggedGlobalStore(ProcessorTopologyTest.java:235) 	... ``` This PR includes a unit test to verify this behaviour.","closed","","SlevinBE","2017-03-27T10:42:54Z","2018-01-26T19:48:36Z"
"","2863","KAFKA-5077 fix GC logging arguments for Java 9","Current JDK-8 GC logs ``` Java HotSpot(TM) 64-Bit Server VM (25.101-b13) for bsd-amd64 JRE (1.8.0_101-b13), built on Jun 22 2016 02:42:15 by ""java_re"" with gcc 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2336.11.00) Memory: 4k page, physical 16777216k(264500k free)  /proc/meminfo:  CommandLine flags: -XX:+DisableExplicitGC -XX:GCLogFileSize=104857600 -XX:InitialHeapSize=1073741824 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ManagementServer -XX:MaxGCPauseMillis=20 -XX:MaxHeapSize=1073741824 -XX:NumberOfGCLogFiles=10 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC -XX:+UseGCLogFileRotation  2017-07-20T14:33:08.181+0800: 0.970: [GC pause (G1 Evacuation Pause) (young), 0.0083484 secs]    [Parallel Time: 3.6 ms, GC Workers: 8]       [GC Worker Start (ms): Min: 969.8, Avg: 969.8, Max: 969.9, Diff: 0.1]       [Ext Root Scanning (ms): Min: 0.4, Avg: 0.7, Max: 2.2, Diff: 1.8, Sum: 5.6]       [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0]       [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]       [Code Root Scanning (ms): Min: 0.0, Avg: 0.1, Max: 0.3, Diff: 0.3, Sum: 0.8]       [Object Copy (ms): Min: 1.3, Avg: 2.6, Max: 3.0, Diff: 1.7, Sum: 20.9]       [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Termination Attempts: Min: 1, Avg: 21.0, Max: 36, Diff: 35, Sum: 168]       [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.2]       [GC Worker Total (ms): Min: 3.4, Avg: 3.4, Max: 3.5, Diff: 0.1, Sum: 27.6]       [GC Worker End (ms): Min: 973.3, Avg: 973.3, Max: 973.3, Diff: 0.0]    [Code Root Fixup: 0.2 ms]    [Code Root Purge: 0.0 ms]    [Clear CT: 0.1 ms]    [Other: 4.5 ms]       [Choose CSet: 0.0 ms]       [Ref Proc: 4.0 ms]       [Ref Enq: 0.0 ms]       [Redirty Cards: 0.1 ms]       [Humongous Register: 0.1 ms]       [Humongous Reclaim: 0.0 ms]       [Free CSet: 0.1 ms]    [Eden: 51.0M(51.0M)->0.0B(44.0M) Survivors: 0.0B->7168.0K Heap: 179.0M(1024.0M)->137.6M(1024.0M)]  [Times: user=0.02 sys=0.01, real=0.01 secs]  2017-07-20T14:33:08.435+0800: 1.224: [GC pause (Metadata GC Threshold) (young) (initial-mark), 0.0066388 secs]    [Parallel Time: 3.9 ms, GC Workers: 8]       [GC Worker Start (ms): Min: 1223.9, Avg: 1223.9, Max: 1224.0, Diff: 0.1]       [Ext Root Scanning (ms): Min: 0.8, Avg: 1.0, Max: 1.1, Diff: 0.3, Sum: 8.0]       [Update RS (ms): Min: 0.0, Avg: 0.3, Max: 1.4, Diff: 1.4, Sum: 2.4]          [Processed Buffers: Min: 0, Avg: 0.4, Max: 1, Diff: 1, Sum: 3]       [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]       [Code Root Scanning (ms): Min: 0.0, Avg: 0.1, Max: 1.0, Diff: 1.0, Sum: 1.2]       [Object Copy (ms): Min: 1.3, Avg: 2.3, Max: 2.9, Diff: 1.6, Sum: 18.1]       [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0]          [Termination Attempts: Min: 1, Avg: 1.9, Max: 3, Diff: 2, Sum: 15]       [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1]       [GC Worker Total (ms): Min: 3.7, Avg: 3.7, Max: 3.8, Diff: 0.1, Sum: 29.8]       [GC Worker End (ms): Min: 1227.6, Avg: 1227.6, Max: 1227.7, Diff: 0.0]    [Code Root Fixup: 0.2 ms]    [Code Root Purge: 0.0 ms]    [Clear CT: 0.1 ms]    [Other: 2.4 ms]       [Choose CSet: 0.0 ms]       [Ref Proc: 2.1 ms]       [Ref Enq: 0.0 ms]       [Redirty Cards: 0.1 ms]       [Humongous Register: 0.0 ms]       [Humongous Reclaim: 0.0 ms]       [Free CSet: 0.1 ms]    [Eden: 36.0M(44.0M)->0.0B(44.0M) Survivors: 7168.0K->7168.0K Heap: 172.9M(1024.0M)->139.9M(1024.0M)]  [Times: user=0.02 sys=0.00, real=0.01 secs]  2017-07-20T14:33:08.442+0800: 1.231: [GC concurrent-root-region-scan-start] 2017-07-20T14:33:08.445+0800: 1.233: [GC concurrent-root-region-scan-end, 0.0026891 secs] 2017-07-20T14:33:08.445+0800: 1.233: [GC concurrent-mark-start] 2017-07-20T14:33:08.445+0800: 1.234: [GC concurrent-mark-end, 0.0005248 secs] 2017-07-20T14:33:08.445+0800: 1.234: [GC remark 2017-07-20T14:33:08.445+0800: 1.234: [Finalize Marking, 0.0001508 secs] 2017-07-20T14:33:08.446+0800: 1.234: [GC ref-proc, 0.0000591 secs] 2017-07-20T14:33:08.446+0800: 1.234: [Unloading, 0.0014629 secs], 0.0029893 secs]  [Times: user=0.01 sys=0.00, real=0.00 secs]  2017-07-20T14:33:08.449+0800: 1.237: [GC cleanup 140M->138M(1024M), 0.0007472 secs]  [Times: user=0.00 sys=0.00, real=0.00 secs]  2017-07-20T14:33:08.449+0800: 1.238: [GC concurrent-cleanup-start] 2017-07-20T14:33:08.449+0800: 1.238: [GC concurrent-cleanup-end, 0.0000250 secs] Heap  garbage-first heap   total 1048576K, used 162690K [0x0000000780000000, 0x0000000780102000, 0x00000007c0000000)   region size 1024K, 29 young (29696K), 7 survivors (7168K)  Metaspace       used 22198K, capacity 22340K, committed 22528K, reserved 1069056K   class space    used 3205K, capacity 3264K, committed 3328K, reserved 1048576K ```  JDK9 logs with equivalent settings proposed in this PR ``` [2017-07-20T14:35:37.812+0800][gc,heap] Heap region size: 1M [2017-07-20T14:35:37.822+0800][gc     ] Using G1 [2017-07-20T14:35:37.822+0800][gc,heap,coops] Heap address: 0x0000000780000000, size: 1024 MB, Compressed Oops mode: Zero based, Oop shift amount: 3 [2017-07-20T14:35:38.820+0800][gc,start     ] GC(0) Pause Initial Mark (Metadata GC Threshold) [2017-07-20T14:35:38.820+0800][gc,task      ] GC(0) Using 8 workers of 8 for evacuation [2017-07-20T14:35:38.832+0800][gc,phases    ] GC(0)   Pre Evacuate Collection Set: 0.0ms [2017-07-20T14:35:38.832+0800][gc,phases    ] GC(0)   Evacuate Collection Set: 6.7ms [2017-07-20T14:35:38.832+0800][gc,phases    ] GC(0)   Post Evacuate Collection Set: 4.7ms [2017-07-20T14:35:38.832+0800][gc,phases    ] GC(0)   Other: 0.2ms [2017-07-20T14:35:38.832+0800][gc,heap      ] GC(0) Eden regions: 48->0(44) [2017-07-20T14:35:38.832+0800][gc,heap      ] GC(0) Survivor regions: 0->7(7) [2017-07-20T14:35:38.832+0800][gc,heap      ] GC(0) Old regions: 0->6 [2017-07-20T14:35:38.832+0800][gc,heap      ] GC(0) Humongous regions: 0->0 [2017-07-20T14:35:38.832+0800][gc,metaspace ] GC(0) Metaspace: 20662K->20662K(1067008K) [2017-07-20T14:35:38.832+0800][gc           ] GC(0) Pause Initial Mark (Metadata GC Threshold) 48M->12M(1024M) 11.675ms [2017-07-20T14:35:38.832+0800][gc,cpu       ] GC(0) User=0.03s Sys=0.01s Real=0.01s [2017-07-20T14:35:38.832+0800][gc           ] GC(1) Concurrent Cycle [2017-07-20T14:35:38.832+0800][gc,marking   ] GC(1) Concurrent Clear Claimed Marks [2017-07-20T14:35:38.833+0800][gc,marking   ] GC(1) Concurrent Clear Claimed Marks 0.016ms [2017-07-20T14:35:38.833+0800][gc,marking   ] GC(1) Concurrent Scan Root Regions [2017-07-20T14:35:38.835+0800][gc,marking   ] GC(1) Concurrent Scan Root Regions 2.242ms [2017-07-20T14:35:38.835+0800][gc,marking   ] GC(1) Concurrent Mark (1.034s) [2017-07-20T14:35:38.835+0800][gc,marking   ] GC(1) Concurrent Mark From Roots [2017-07-20T14:35:38.835+0800][gc,task      ] GC(1) Using 2 workers of 2 for marking [2017-07-20T14:35:38.836+0800][gc,marking   ] GC(1) Concurrent Mark From Roots 0.461ms [2017-07-20T14:35:38.836+0800][gc,marking   ] GC(1) Concurrent Mark (1.034s, 1.034s) 0.494ms [2017-07-20T14:35:38.836+0800][gc,start     ] GC(1) Pause Remark [2017-07-20T14:35:38.837+0800][gc,stringtable] GC(1) Cleaned string and symbol table, strings: 8742 processed, 0 removed, symbols: 65774 processed, 32 removed [2017-07-20T14:35:38.837+0800][gc            ] GC(1) Pause Remark 12M->12M(1024M) 1.533ms [2017-07-20T14:35:38.837+0800][gc,cpu        ] GC(1) User=0.01s Sys=0.00s Real=0.00s [2017-07-20T14:35:38.837+0800][gc,marking    ] GC(1) Concurrent Create Live Data [2017-07-20T14:35:38.838+0800][gc,marking    ] GC(1) Concurrent Create Live Data 0.260ms [2017-07-20T14:35:38.838+0800][gc,start      ] GC(1) Pause Cleanup [2017-07-20T14:35:38.838+0800][gc            ] GC(1) Pause Cleanup 12M->12M(1024M) 0.419ms [2017-07-20T14:35:38.838+0800][gc,cpu        ] GC(1) User=0.00s Sys=0.00s Real=0.00s [2017-07-20T14:35:38.838+0800][gc,marking    ] GC(1) Concurrent Cleanup for Next Mark [2017-07-20T14:35:38.844+0800][gc,marking    ] GC(1) Concurrent Cleanup for Next Mark 5.611ms [2017-07-20T14:35:38.844+0800][gc            ] GC(1) Concurrent Cycle 11.624ms [2017-07-20T14:35:39.199+0800][gc,start      ] GC(2) Pause Young (G1 Evacuation Pause) [2017-07-20T14:35:39.199+0800][gc,task       ] GC(2) Using 8 workers of 8 for evacuation [2017-07-20T14:35:39.210+0800][gc,phases     ] GC(2)   Pre Evacuate Collection Set: 0.1ms [2017-07-20T14:35:39.210+0800][gc,phases     ] GC(2)   Evacuate Collection Set: 8.5ms [2017-07-20T14:35:39.210+0800][gc,phases     ] GC(2)   Post Evacuate Collection Set: 2.3ms [2017-07-20T14:35:39.210+0800][gc,phases     ] GC(2)   Other: 0.1ms [2017-07-20T14:35:39.210+0800][gc,heap       ] GC(2) Eden regions: 44->0(45) [2017-07-20T14:35:39.210+0800][gc,heap       ] GC(2) Survivor regions: 7->6(7) [2017-07-20T14:35:39.210+0800][gc,heap       ] GC(2) Old regions: 6->13 [2017-07-20T14:35:39.210+0800][gc,heap       ] GC(2) Humongous regions: 129->129 [2017-07-20T14:35:39.210+0800][gc,metaspace  ] GC(2) Metaspace: 25575K->25575K(1073152K) [2017-07-20T14:35:39.210+0800][gc            ] GC(2) Pause Young (G1 Evacuation Pause) 185M->146M(1024M) 11.025ms [2017-07-20T14:35:39.210+0800][gc,cpu        ] GC(2) User=0.05s Sys=0.00s Real=0.02s [2017-07-20T14:35:52.051+0800][gc,heap,exit  ] Heap [2017-07-20T14:35:52.051+0800][gc,heap,exit  ]  garbage-first heap   total 1048576K, used 167618K [0x0000000780000000, 0x0000000780102000, 0x00000007c0000000) [2017-07-20T14:35:52.051+0800][gc,heap,exit  ]   region size 1024K, 24 young (24576K), 6 survivors (6144K) [2017-07-20T14:35:52.051+0800][gc,heap,exit  ]  Metaspace       used 26440K, capacity 26966K, committed 27136K, reserved 1073152K [2017-07-20T14:35:52.051+0800][gc,heap,exit  ]   class space    used 3453K, capacity 3574K, committed 3584K, reserved 1048576K ```","closed","","xvrl","2017-04-17T17:55:03Z","2017-08-10T01:10:45Z"
"","2731","MINOR: Make LeaderAndIsr immutable case class.","Continuation of #2531. Asking for review @guozhangwang and @ijuma","closed","","jozanek","2017-03-24T11:30:20Z","2017-04-12T07:27:16Z"
"","3153","MINOR: clarify partition option behavior for console consumer","Console Consumer help doesn't say that ``--partition`` option needs ``--offset`` otherwise will consume from the end of the partition. This minor fix makes that happen.","closed","","cotedm","2017-05-26T12:35:59Z","2017-06-16T16:02:59Z"
"","2675","KAFKA-4882: Remove internal converter configuration from example property files","Connect works best with the default JSON converters, and exposing the internal converter config tempts users to change it (see [KAFKA-4882](https://issues.apache.org/jira/browse/KAFKA-4882) for more info).  Also, I don't really see how `internal.key.converter.schemas.enable` ever gets used. I see some tests specifying this value but don't see anything in `org.apache.kafka.connect.runtime.WorkerConfig` or anywhere else (maybe I'm just missing it?).  Anyways, this is my first commit, hope it's okay :)  This contribution is my original work and I license the work to the project under the project's open source license.  cc @gwenshap @ewencp","closed","connect,","mitch-seymour","2017-03-11T04:46:09Z","2018-04-30T03:06:44Z"
"","2571","HOTFIX: ClassCastException in Request logging","Comming from [here](https://github.com/apache/kafka/pull/2570#issuecomment-280859637)  Fixed ClassCastException resulting from missing type hint in request logging.","closed","","original-brownbear","2017-02-18T17:26:16Z","2017-02-18T18:43:02Z"
"","2534","MINOR: Update SimpleAclAuthorizer.scala","Comments had the wrong name for the Group znode, updating ConsumerGroup-->Group in the comments to match what's actually coded.  @ijuma if you can review, it's a trivial change.","closed","","cotedm","2017-02-10T15:34:57Z","2017-02-12T15:11:14Z"
"","3452","KAFKA-5483: Shutdown of scheduler should come after LogManager","Close the scheduler after all components using it have been shutdown  @hachikuji Can you have a look ?","closed","","mimaison","2017-06-28T14:44:33Z","2018-04-18T13:31:34Z"
"","3222","MINOR: Code Cleanup","Clean up includes:  - Switching try-catch-finally blocks to try-with-resources when possible - Removing some seemingly unnecessary `@SuppressWarnings` annotations - Resolving some Java warnings - Closing unclosed Closable objects - Removing unused code","closed","","vahidhashemian","2017-06-03T04:46:51Z","2017-07-19T17:54:44Z"
"","2804","MINOR: don't throw CommitFailedException during suspendTasksAndState","Cherrypicked from trunk https://github.com/apache/kafka/pull/2535","closed","","enothereska","2017-04-04T12:55:55Z","2017-04-05T05:43:04Z"
"","3253","KAFKA-5395: Remove synchronization from AbstractCoordinator.close, remove duplication","Cherry-picked from trunk.","closed","","rajinisivaram","2017-06-07T09:08:34Z","2017-06-07T14:33:02Z"
"","3209","KAFKA-5345; Close KafkaClient when streams client is closed","Cherry-pick KAFKA-5345 to 0.10.2","closed","","rajinisivaram","2017-06-02T14:33:13Z","2017-06-02T15:31:08Z"
"","2803","MINOR: improve MinTimestampTrackerTest and fix NPE when null element","Cherry picked from trunk fix https://github.com/apache/kafka/pull/2611","closed","","enothereska","2017-04-04T12:51:25Z","2017-04-05T05:44:45Z"
"","2817","MINOR: Improve log4j on stream thread and stream process","Cherry pick from https://github.com/apache/kafka/pull/2685","closed","","enothereska","2017-04-06T10:37:50Z","2017-04-06T11:31:02Z"
"","2720","KAFKA-4929: Transformation Key/Value type references should be to class name(), not canonicalName()","Changing getCanonicalName() references to getName() so that docs update with ""$"" instead of ""."".  Also added a connect-plugin-discovery.sh CLI to list all of the transformations available.","closed","","bruce-szalwinski","2017-03-21T21:44:14Z","2017-03-22T21:03:08Z"
"","3072","KAFKA-5229:Reflections logs excessive warnings when scanning classpaths","changed the reflections log level to ERROR. And tested it, now the warning logs are not shown up during the start of Kafka connect. @ewencp could you please review the changes.","closed","","bharatviswa504","2017-05-16T21:08:45Z","2019-02-07T18:59:47Z"
"","3367","KAFKA-5468 WorkerSourceTask offset commit loglevel changes","changed log level for source connector worker task when committing offsets","closed","","simplesteph","2017-06-19T04:34:53Z","2017-07-21T04:56:57Z"
"","3022","HOTFIX: change compression codec in TransactionStateManager to UncompressedCodec","Change the compression code used for the transaction log to UncompressedCoded as it fails during creation when the codec is set to NoCompressionCodec.","closed","","dguy","2017-05-11T10:39:41Z","2017-05-16T14:04:43Z"
"","2972","KAFKA-5172: Fix fetchPrevious to find the correct session.","Change fetchPrevious to use findSessions with the proper key and timestamps rather than using fetch.","closed","","ghost","2017-05-04T02:07:58Z","2017-05-15T13:19:12Z"
"","2769","KAFKA-4937: Batch offset fetches in the Consumer","change `consumer.position` so that it always updates any partitions that need an update. Keep track of partitions that `seekToBeginning` in `StoreChangeLogReader` and do the `consumer.position` call after all `seekToBeginning` calls.","closed","","dguy","2017-03-30T13:41:07Z","2017-05-16T14:04:58Z"
"","3001","KAFKA-5167: fix unreleased state lock due to uncaught exception when closing a task","Catch any exception throw from user code during `task.close()` and close state manger to release state lock even in case on an exception.","closed","","mjsax","2017-05-09T06:13:46Z","2017-07-06T05:15:31Z"
"","2963","Kafka-5205: Removed use of keySerde in CachingSessionStore.","CachingSessionStore wasn't properly using the default keySerde if no Serde was supplied. I saw the below error in the logs for one of my test cases.  ERROR stream-thread [cogroup-integration-test-3-5570fe48-d2a3-4271-80b1-81962295553d-StreamThread-6] Streams application error during processing:  (org.apache.kafka.streams.processor.internals.StreamThread:335) java.lang.NullPointerException 	at org.apache.kafka.streams.state.internals.CachingSessionStore.findSessions(CachingSessionStore.java:93) 	at org.apache.kafka.streams.kstream.internals.KStreamSessionWindowAggregate$KStreamSessionWindowAggregateProcessor.process(KStreamSessionWindowAggregate.java:94) 	at org.apache.kafka.streams.processor.internals.ProcessorNode$1.run(ProcessorNode.java:47) 	at org.apache.kafka.streams.processor.internals.StreamsMetricsImpl.measureLatencyNs(StreamsMetricsImpl.java:187) 	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:133) 	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:82) 	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:69) 	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:206) 	at org.apache.kafka.streams.processor.internals.StreamThread.processAndPunctuate(StreamThread.java:657) 	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:728) 	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:327)","closed","","ghost","2017-05-03T02:00:30Z","2017-05-17T17:56:16Z"
"","3166","KAFKA-5316: Follow-up with ByteBufferOutputStream and other misc improvements","ByteBufferOutputStream improvements: * Document pitfalls * Improve efficiency when dealing with direct byte buffers * Improve handling of buffer expansion * Be consistent about using `limit` instead of `capacity` * Add constructors that allocate the internal buffer  Other minor changes: * Fix log warning to specify correct Kafka version * Clean-ups","closed","","ijuma","2017-05-30T12:14:41Z","2017-09-05T08:42:09Z"
"","2569","MINOR: Add build_eclipse to .gitignore","build_eclipse is the configured output directory for eclipse when using the gradle eclipse plugin and should be ignored","closed","","cshannon","2017-02-18T14:30:14Z","2017-02-21T14:00:11Z"
"","2678","KAFKA-4891 kafka.request.logger TRACE regression","Both the headers and requests have regressed to just show object ids instead of their contents from their underlying structs. I'm guessing this regression came from commit [fc1cfe475e8ae8458d8ddf119ce18d0c64653a70](https://github.com/apache/kafka/commit/fc1cfe475e8ae8458d8ddf119ce18d0c64653a70)","closed","","onurkaraman","2017-03-13T17:52:43Z","2017-03-13T20:54:30Z"
"","3252","KAFKA-5385: ProducerBatch expiry should go through Sender.failBatch","Before this patch, we would call `producerBatch.done` directly from the accumulator when expiring batches. This meant that we would not transition to the `ABORTABLE_ERROR` state in the transaction manager, allowing other transactional requests (including Commits!) to go through, even though the produce failed.   This patch modifies the logic so that we call `Sender.failBatch` on every expired batch, thus ensuring that the transaction state is accurate.","closed","","apurvam","2017-06-07T05:04:19Z","2017-06-07T21:50:16Z"
"","3313","KAFKA-5438: Fix UnsupportedOperationException in writeTxnMarkersRequest","Before this patch, the `partitionErrors` was an immutable map. As a result if a single producer had a marker for multiple partitions, and if there were multiple response callbacks for a single append, we would get an `UnsupportedOperationException` in the `writeTxnMarker` handler.","closed","","apurvam","2017-06-13T04:51:18Z","2017-06-13T23:55:33Z"
"","3119","KAFKA-5273: Make KafkaConsumer.committed query the server for all partitions","Before this patch the consumer would return the cached offsets for partitions in its current assignment. This worked when all the offset commits went through the consumer.   With KIP-98, offsets can be committed transactionally through the producer. This means that relying on cached positions in the consumer returns incorrect information: since commits go through the producer, the cache is never updated.   Hence we need to update the `KafkaConsumer.committed` method to always lookup the server for the last committed offset to ensure it gets the correct information every time.","closed","","apurvam","2017-05-22T23:41:44Z","2017-05-24T06:10:01Z"
"","3374","KAFKA-5032: Update the docs for message size configs across the board","Before 0.11, we used to have limits for maximum message size on the producer, broker, and consumer side.  From 0.11 onward, these limits apply to record batches as a whole. This patch updates the documentation of the configs to make this explicit.   A separate patch will have more extensive upgrade notes to tie all the changes together in one narrative.","closed","","apurvam","2017-06-19T21:23:16Z","2017-06-27T14:12:41Z"
"","2707","MINOR: exchanged addAll/putAll with parametrized constructor","Basic static analysis was done here.  I've exchanged all addAll/putAll methods that were stand right after creation of each particular HashSet/HashMap with parametric constructor of HashSet/HashMap.","closed","","wlsc","2017-03-19T13:29:58Z","2018-02-03T21:13:55Z"
"","2906","Kafka-4994 Fix findbug warnings about OffsetStorageWriter#currentFlushId","Based on the description of the class OffsetStorageWriter, it is not a thread-safe class and should be accessed only from a Task's processing thread. Many methods within this class have been explicitly synchronized in their function definition. The doFlush() method is a non-blocking function and hasn't been synchronized but modifies the variables used within the synchronized methods in this class. This could lead to potential inconsistent synchronization of some variables within this class.  We can therefore remove the synchronized keyword from the method signatures within the OffsetStorageWriter class since the WorkerSourceTask class calls the different methods (offset, beginFlush,cancelFlush, handleFinishWrite) within a synchronized block. There is no need to synchronize calls to these methods more than once.","closed","","johnma14","2017-04-24T19:52:39Z","2017-04-25T20:13:10Z"
"","3083","KAFKA-1955: [WIP] Disk based buffer in Producer","Based on patch from @jkreps in [this JIRA ticket](https://issues.apache.org/jira/browse/KAFKA-1955).  - [ ] Get some unit tests that would cover disk-backed usage - [ ] Do some manual performance testing of this usage and understand the impact on throughput. - [ ] Do some manual testing of failure cases (i.e. if the broker goes down for 30 seconds we should be able to keep taking writes) and observe how well the producer handles the catch up time when it has a large backlog to get rid of. - [ ] Add a new configuration for the producer to enable this, something like use.file.buffers=true/false. - [ ] Add documentation that covers these new options.  I've brought the patch into sync with trunk. Testing is next, which I've started on. I am flexible on how this can be implemented.","closed","","blbradley","2017-05-18T01:30:07Z","2019-06-26T09:28:27Z"
"","3035","KAFKA-5290 docs need clarification on meaning of 'committed' to the log","based on conversations with @vahidhashemian @rajinisivaram @apurvam   The docs didn't make clear that what gets committed and what gets not may depend on the producer acks.","closed","","edoardocomar","2017-05-12T16:39:56Z","2017-06-22T12:10:30Z"
"","3090","KAFKA-5150 reduce lz4 decompression overhead - Backport to 0.10.2.x","Backport KAFKA-5150 (#2967) to 0.10.2  - reuse decompression buffers in consumer Fetcher - switch lz4 input stream to operate directly on ByteBuffers - avoids performance impact of catching exceptions when reaching the end of legacy record batches - more tests with both compressible / incompressible data, multiple   blocks, and various other combinations to increase code coverage - fixes bug that would cause exception instead of invalid block size   for invalid incompressible blocks - fixes bug if incompressible flag is set on end frame block size  cc @radai-rosenblatt in case you are interested in a 0.10 version of the patch","closed","","xvrl","2017-05-18T17:42:41Z","2017-06-01T19:15:22Z"
"","3032","KAFKA-4965: set internal.leave.group.on.close to false in StreamsConfig","Backport from trunk","closed","","dguy","2017-05-12T11:10:05Z","2017-05-16T14:04:35Z"
"","3025","KAFKA-4881: add internal.leave.group.config to consumer","Backport from https://github.com/apache/kafka/pull/2650","closed","","dguy","2017-05-11T12:08:12Z","2017-05-16T14:04:41Z"
"","3110","KAFKA-5297: Reduce time required for shutdown","Avoid reopening and recreating page mappings for index files during truncation unless it's required by the platform.  In my tests this increases throughput of Log#close by about 70%. See KAFKA-5297 for details.","closed","performance,","ambroff","2017-05-20T16:46:39Z","2021-11-02T04:15:13Z"
"","2603","MINOR: Minor reduce unnecessary calls to time.millisecond (part 2)","Avoid calling time.milliseconds more often than necessary. Cleaning and committing logic can use the timestamp at the start of the loop with minimal consequences. 5-10% improvements noticed with request rates of 450K records/second.  Also tidy up benchmark code a bit more.","closed","","enothereska","2017-02-27T20:13:39Z","2017-03-02T07:51:04Z"
"","2993","KAFKA-5191: Autogenerate Consumer Fetcher metrics","Autogenerate docs for the Consumer Fetcher's metrics. This is a smaller subset of the original PR https://github.com/apache/kafka/pull/1202.  CC @ijuma @benstopford @hachikuji","closed","","wushujames","2017-05-08T05:17:05Z","2017-05-31T01:01:32Z"
"","2861","KAFKA-4862: Kafka client connect to a shutdown node will block for a long time","Author: pengwei   Reviewers: Jiangjie Qin                  @becketqin   Modify: Add a connect timeout for the kafka client to avoid long blocking if network is down","closed","","pengwei-li","2017-04-17T14:34:37Z","2020-10-21T14:18:46Z"
"","3280","MINOR: Avoid calling interceptors.onSendError() with null TopicPartition","Assign non-null tp as soon as possible once we know the partition. This is so that if ensureValidRecordSize() throws, the interceptors.onSendError() call is made with a non-null tp.","closed","","tombentley","2017-06-09T11:11:49Z","2017-06-13T12:15:23Z"
"","2772","KAFKA-4208: Add Record Headers","As per KIP-82  Adding record headers api to ProducerRecord, ConsumerRecord Support to convert from protocol to api added Kafka Producer, Kafka Fetcher (Consumer) Updated MirrorMaker, ConsoleConsumer and scala BaseConsumer Add RecordHeaders and RecordHeader implementation of the interfaces Headers and Header  Some bits using are reverted to being Java 7 compatible, for the moment until KIP-118 is implemented.","closed","","michaelandrepearce","2017-03-30T19:27:47Z","2017-05-07T16:23:32Z"
"","2942","(WIP) KAFKA-5142: Expose Record Headers in Kafka Connect (DO NOT MERGE)","as per KIP-145  Add constructors to Connect/Source/SinkRecord to take `Iterable`  add accessor method to ConnectRecord `Headers headers()` Update WorkerSource/Sink to pass headers to/from Producer/ConsumerRecords","closed","","michaelandrepearce","2017-04-29T07:11:49Z","2018-02-25T07:52:53Z"
"","3382","KAFKA-4260: Check for nonroutable address in advertised.listeners","As described in KAFKA-4260, when `listeners=PLAINTEXT://0.0.0.0:9092` (note the 0.0.0.0 ""bind all interfaces"" IP address) and `advertised.listeners` is not specified it defaults to `listeners`, but it makes no sense to advertise 0.0.0.0 as it's not a routable IP address.  This patch checks for a 0.0.0.0 host in `advertised.listeners` (whether via default or not) and fails with a meaningful error if it's found.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","tombentley","2017-06-20T11:18:36Z","2017-09-08T12:12:49Z"
"","3136","KAFKA-5319 Add a tool to make cluster replica and leader balance","As [KAFKA-5319](https://issues.apache.org/jira/browse/KAFKA-5319) describes Kafka does not have tools to make replica or leader num of cluster balanced ,I write a tool to deal with it.","closed","","MarkTcMA","2017-05-24T10:47:34Z","2017-06-21T06:23:09Z"
"","3405","KAFKA-5495: Update docs to use `kafka-consumer-groups.sh` for checking consumer offsets","And remove the deprecated `ConsumerOffsetChecker` example.","closed","","vahidhashemian","2017-06-22T00:20:37Z","2017-10-02T19:47:59Z"
"","3330","MINOR: Javadoc for ExtendedSerializer and ExtendedDeserializer","And add warning about usage.","closed","","ijuma","2017-06-14T01:27:27Z","2017-06-18T09:26:06Z"
"","2699","KAFKA-4569: Make KafkaConsumer Trigger Wakeup before Updating Offsets","An attempt at resolving https://issues.apache.org/jira/browse/KAFKA-4569 using the @hachikuji suggestions from https://issues.apache.org/jira/browse/KAFKA-4569?focusedCommentId=15901556&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15901556.  I tried to use this:  > We could also add a hasPendingWakeup() or something to ConsumerNetworkClient. If a wakeup is expected, then we can just call poll(0) to trigger it.  but instead of adding a new method to `ConsumerNetworkClient` I simply exposed `org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient#maybeTriggerWakeup` publicly. Since we still have to synchronize to not run into issues with concurrent calls to `org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient#pollNoWakeup` by the heartbeat thread, this seemed to be the shortest route to triggering the wake up if one is pending before committing offsets.  Stabilizes the test in question over 8k+ iterations on my machine, where it failed after ~100 runs consistently before.","closed","","original-brownbear","2017-03-17T09:56:23Z","2017-03-27T23:06:14Z"
"","3347","MINOR: Upgrade to Gradle 4.0","Among other things, it includes performance and CLI improvements.","closed","","ijuma","2017-06-15T02:01:30Z","2017-08-22T06:39:52Z"
"","3320","KAFKA-5370: Replace uses of the old consumer with the new consumer when possible","Also, methods in `ClientUtils` that are called by server or tools code are introduced in `AdminUtils` with the implementation living in `AdminUtils`. All the existing callers apart from the old clients call the `AdminUtils` methods.","closed","","vahidhashemian","2017-06-13T18:46:17Z","2018-06-14T00:50:39Z"
"","2935","MINOR: onControllerResignation should be invoked if triggerControllerMove is called","Also update the test to be simpler since we can use a mock event to simulate the issue more easily (thanks Jun for the suggestion). This should fix two issues:  1. A transient test failure due to a NPE in ControllerFailoverTest.testMetadataUpdate:  ```text Caused by: java.lang.NullPointerException 	at kafka.controller.ControllerBrokerRequestBatch.addUpdateMetadataRequestForBrokers(ControllerChannelManager.scala:338) 	at kafka.controller.KafkaController.sendUpdateMetadataRequest(KafkaController.scala:975) 	at kafka.controller.ControllerFailoverTest.testMetadataUpdate(ControllerFailoverTest.scala:141) ```  The test was creating an additional thread and it does not seem like it was doing the appropriate synchronization (perhaps this became more of an issue after we changed the Controller to be single-threaded and changed the locking)  2. Setting `activeControllerId.set(-1)` in `triggerControllerMove` causes `Reelect` not to invoke `onControllerResignation`. Among other things, this causes an `IllegalStateException` to be thrown when `KafkaScheduler.startup` is invoked for the second time without the corresponding `shutdown`. We now simply call `onControllerResignation` as part of `triggerControllerMove`.  Finally, I included a few clean-ups:  1. No longer update the broker state in `onControllerFailover`. This is no longer needed since we removed the `RunningAsController` state (KAFKA-3761). 2. Trivial clean-ups in KafkaController 3. Removed unused parameter in `ZkUtils.getPartitionLeaderAndIsrForTopics`","closed","","ijuma","2017-04-28T12:39:14Z","2017-06-18T09:30:54Z"
"","3385","KAFKA-4059: Documentation still refers to AsyncProducer and SyncProducer","Also remove old code snippet which bears little resemblance to the current Producer API.  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","tombentley","2017-06-20T14:07:42Z","2017-06-22T12:46:02Z"
"","3341","MINOR: Update `TransactionMarkerChannelManager` metric name","Also remove broker-id tags as we generally use them to provide additional context.  Finally, do a few clean-ups (could not resist).","closed","","ijuma","2017-06-14T20:23:37Z","2017-06-18T09:26:10Z"
"","2580","MINOR: Move ProtoUtils methods to ApiKeys","Also move `requireTimestamp` to `minVersion` logic from `Fetcher` to `ListOffsetRequest.Builder.forConsumer()`.","closed","","ijuma","2017-02-21T17:39:48Z","2017-09-05T09:30:01Z"
"","3420","KAFKA-5506: Fix NPE in OffsetFetchRequest.toString","Also include a number of improvements in NetworkClient's logging: - Include correlation id in a number of log statements - Avoid eager toString call in parameter passed to log.debug - Use node.toString instead of passing a subset of fields to the logger - Use requestBuilder instead of clientRequest in one of the log statements","closed","","ijuma","2017-06-23T12:21:49Z","2017-09-05T08:40:44Z"
"","3473","MINOR: Follow-up Streams doc changes to break into sub-pages","Also fixed a bunch of broken links (details can be found in https://github.com/apache/kafka-site/commit/34f8ecea0db15523ce4b81e6b6bc4c5c2fabd603)","closed","","guozhangwang","2017-07-01T01:15:53Z","2017-07-15T22:06:47Z"
"","3031","MINOR: Fix bug in `waitUntilLeaderIsElectedOrChanged` and simplify result type","Also disable a couple of tests that were passing incorrectly until KAFKA-3096 is fixed.  The bug was for the following case:  `leader.isDefined && oldLeaderOpt.isEmpty && newLeaderOpt.isDefined && newLeaderOpt.get != leader.get`  We would consider it a successful election even though the new leader was not the expected leader.  I also changed the result type as we never return `None` (we throw an exception instead).","closed","","ijuma","2017-05-12T09:17:22Z","2017-06-18T09:33:29Z"
"","2937","MINOR: Fix consumer and producer to actually support metrics recording level","Also add tests and a few clean-ups.","closed","","ijuma","2017-04-28T15:51:46Z","2017-06-18T09:33:10Z"
"","2822","MINOR: Fix incorrect pattern matching on `version` in `CheckpointFile`","Also add test and refactor things a little to make testing easier.","closed","","ijuma","2017-04-07T12:05:02Z","2017-09-05T09:51:31Z"
"","2844","HOTFIX: HTML formatting error in upgrade docs from pr-2824","Already fixed in the website github","closed","","gwenshap","2017-04-12T03:05:50Z","2017-04-12T10:10:22Z"
"","3459","KAFKA-3741: allow users to specify default topic configs for internal topics","Allow users to specify default topic configs for streams internal topics by supplying properties from `TopicConfig` with a prefix. Supplied defaults are used when creating the internal topics. They are overridden by the configs supplied along with the `InternalTopicConfig`","closed","","dguy","2017-06-29T15:38:30Z","2017-08-16T13:23:06Z"
"","3255","KAFKA-3741: add min.insync.replicas config to Streams","Allow users to specify `min.insync.replicas` via StreamsConfig. Default to `null` so that the server settting will be used. If `replication.factor` is lower than `min.insync.replicas` then set `min.insync.replicas` to `replication.factor`","closed","","dguy","2017-06-07T11:10:24Z","2017-06-22T14:16:29Z"
"","3398","allow transactions in producer perf script","allow the transactional producer to be enabled in `producer-perf.sh`, with a new flag `--use-transactions`","closed","","tcrayford","2017-06-21T18:01:54Z","2017-06-21T22:02:51Z"
"","2766","KAFKA-4913: creating a window store with one segment throws division by zero error","Allow a window store with a single segment.","closed","","dguy","2017-03-30T10:12:54Z","2017-05-16T14:05:05Z"
"","3422","MINOR: SaslChannelBuilder should be idempotent","After we call `release`, we should null out the reference so that we neither use it or release it a second time.  This should fix the following exception that has been reported:  ```text [2017-06-23 03:24:02,485] ERROR stream-thread [...] Failed to close consumer:  (org.apache.kafka.streams.processor.internals.StreamThread:1054) org.apache.kafka.common.KafkaException: Failed to close kafka consumer         at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1623)         at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1573)         at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1549)         at org.apache.kafka.streams.processor.internals.StreamThread.shutdown(StreamThread.java:1052)         at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:538) Caused by: java.lang.IllegalStateException: release called on LoginManager with refCount == 0         at org.apache.kafka.common.security.authenticator.LoginManager.release(LoginManager.java:106)         at org.apache.kafka.common.network.SaslChannelBuilder.close(SaslChannelBuilder.java:125)         at org.apache.kafka.common.network.Selector.close(Selector.java:257)         at org.apache.kafka.clients.NetworkClient.close(NetworkClient.java:505)         at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.close(ConsumerNetworkClient.java:439)         at org.apache.kafka.clients.ClientUtils.closeQuietly(ClientUtils.java:71)         at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1613) ```  It's worth noting that it's not clear how `SaslChannelBuilder.close()` is called more than once and it would be good to understand that as well.","closed","","ijuma","2017-06-23T13:06:15Z","2017-09-05T08:38:39Z"
"","3440","MINOR: Close producer in all KafkaProducer tests","After running  KafkaProducer tests, we should close  producer for free resource","closed","","10110346","2017-06-27T02:58:47Z","2018-10-18T15:13:44Z"
"","2855","Minor improve streams config parameters","Adjust ""importance level"" and add explanation to the docs.","closed","","mjsax","2017-04-14T18:00:46Z","2017-06-01T23:20:57Z"
"","3077","KAFKA-4923: Add Streams EOS integration tests","Adds integration tests for exactly-once semantics with Streams API. Fixes Streams EOS bug: need to commit transaction on suspend","closed","","mjsax","2017-05-17T01:07:27Z","2017-06-01T23:21:07Z"
"","2722","KAFKA-4878: Improved Invalid Connect Config Error Message","Addresses for https://issues.apache.org/jira/browse/KAFKA-4878  * Adjusted the error message to explicitly state errors and their number * Dried up the logic for generating the message between standalone and distributed  Example  messed up two config keys in the file source config: ```` namse=local-file-source connector.class=FileStreamSource tasks.max=1 fisle=test.txt topic=connect-test ```  Produces:  ``` [2017-03-22 08:57:11,896] ERROR Stopping after connector error (org.apache.kafka.connect.cli.ConnectStandalone:99) java.util.concurrent.ExecutionException: org.apache.kafka.connect.runtime.rest.errors.BadRequestException: Connector configuration is invalid and contains the following 2 error(s): Missing required configuration ""file"" which has no default value. Missing required configuration ""name"" which has no default value. You can also find the above list of errors at the endpoint `/{connectorType}/config/validate` ```","closed","connect,","original-brownbear","2017-03-22T08:02:53Z","2020-10-16T06:08:13Z"
"","3005","KAFKA-5166: Add option ""dry run"" to Streams application reset tool","Addressed the below review comment from #PR #2998 from @mjsax   I am wondering if it would be better, to ""embed"" the dry-run into the actual code and branch on each place. Otherwise, if things get changed, we could easily introduce bugs (ie, dry run show something different than what the actual reset code does.  We could introduce methods like mabyeSeekToBeginning() that either does the seek or only prints to stdout. This would ensure that the main logic is used to ""feed"" into dry-run and we don't have code duplication.  WDYT?","closed","","bharatviswa504","2017-05-09T19:22:13Z","2017-05-16T21:25:55Z"
"","2505","KAFKA-4276: Add REST configuration in connector properties","Addition of REST configuration in connect-distributed.properties config file @gwenshap @ewencp - Please review.","closed","","akhilesh1194","2017-02-06T06:04:13Z","2017-03-05T18:21:52Z"
"","2889","KAFKA-4928: Add integration test for DumpLogSegments","Adding tests for `kafka.tools.DumpLogSegments`","closed","","original-brownbear","2017-04-21T14:45:22Z","2017-09-12T07:16:50Z"
"","3383","KAFKA-5481: ListOffsetResponse isn't logged in the right way with trace level enabled","Added toString() method to ListOffsetResponse for logging","closed","","ppatierno","2017-06-20T12:37:12Z","2017-07-24T12:05:38Z"
"","3004","KAFKA-4982: add listener tags to socket-server-metrics","Added tags","closed","","edoardocomar","2017-05-09T19:16:12Z","2017-05-15T10:00:07Z"
"","3018","KAFKA-5209 : Transient failure: kafka.server.MetadataRequestTest.testControllerId","Added sleep after creating the socket so that socket can be spawned properly and ready to send the request.  @guozhangwang may I request you to review it.","open","","umesh9794","2017-05-11T05:19:13Z","2018-03-02T19:30:08Z"
"","3425","Kafka 3368","Added Message set / Record set documentation to protocol.html. Also added short description of new types used in 0.11","closed","docs,","andrasbeni","2017-06-23T20:33:26Z","2018-03-12T22:13:35Z"
"","2943","KAFKA-5143: add kafka-broker-api-version.bat for Windows platform","Added kafka-broker-api-versions.bat for Windows platform","closed","","huxihx","2017-04-29T11:38:51Z","2017-05-06T01:34:25Z"
"","2503","KAFKA-4733: Improve Streams Reset Tool console output","Added general explanation of the tool and what it does. Also added few details to the arguments.","closed","","gwenshap","2017-02-05T04:28:09Z","2017-02-08T08:08:53Z"
"","2482","MINOR: Update Streams docs: quickstart and concepts","Added figures for topology and table-stream duality; added sections about aggregations; misc code fixes.","closed","","guozhangwang","2017-02-02T01:08:36Z","2017-07-15T22:07:23Z"
"","3117","KAFKA-5225:StreamsResetter doesn't allow custom Consumer properties","Added command-config option, as the client configuration is required for AdminClient and Embedded Consumer.  @mjsax @guozhangwang please review the changes.  @msjax from previous PR couple of questions. 1. Tests for Secure cluster, do you mean to add Integration Test? 2. --dry-run option should print user configs or not? This is just a thought. (Not got what do you mean here?","closed","","bharatviswa504","2017-05-22T21:15:50Z","2017-09-27T21:06:54Z"
"","2962","KAFKA-5161: add code in reassign-partitions to check broker existence","Added code to check existence of the brokers in the proposed plan.","closed","","huxihx","2017-05-03T01:58:40Z","2017-05-05T09:31:48Z"
"","3335","KAFKA-5434: Console consumer hangs if not existing partition is specified","Added checking partition exists before assign request","closed","","ppatierno","2017-06-14T08:24:47Z","2018-06-06T06:33:49Z"
"","3453","KAFKA-5532: Making bootstrap.servers property a first citizen option for the ProducerPerformance","Added bootstrap-server as option for specifying Kafka cluster to connect to","open","","ppatierno","2017-06-28T14:46:43Z","2018-03-02T19:30:21Z"
"","3048","KAFKA-4850:RocksDb cannot use Bloom Filters","Added BloomFilter to speedup rocksdb lookup.","closed","streams,","bharatviswa504","2017-05-14T05:50:43Z","2019-01-15T04:03:06Z"
"","2652","MINOR: add warning when offset topic is not replicated properly","Added a warning, otherwise this problem is hard to debug if you haven't followed the latest KIP closely.","closed","","enothereska","2017-03-07T16:44:41Z","2017-03-17T12:04:37Z"
"","3352","KAFKA-5454: Add a new Kafka Streams example IoT oriented","Added a Kafka Streams example (IoT oriented) using ""tumbling"" window","closed","","ppatierno","2017-06-15T15:03:43Z","2017-08-01T18:46:50Z"
"","2661","kafka-4866: Kafka console consumer property is ignored","Added `print.value` config in ConsoleConsumer to match what the quickstart document says.","closed","","huxihx","2017-03-09T01:23:07Z","2017-04-11T11:40:51Z"
"","2621","KAFKA-4800: Streams State transition ASCII diagrams need fixing and polishing","added \ tags to not break javadoc display of the ASCII diagrams. see broken ascii here: https://kafka.apache.org/0102/javadoc/org/apache/kafka/streams/KafkaStreams.State.html  fix can be checked with gradle :streams:javadoc and then checking streams/build/docs/javadoc/org/apache/kafka/streams/KafkaStreams.State.html  I also fixed the diagram in StreamThread.java however currently no javadoc is generated for that one (since it's internal)  @enothereska please have a look","closed","","cvaliente","2017-03-01T11:23:43Z","2017-03-02T21:27:41Z"
"","3188","KAFKA-5358: Consumer perf tool should count rebalance time.","Added 'join.group.ms' for new consumer to calculate the time of joining group.   @hachikuji  Please review the PR. Thanks.","closed","","huxihx","2017-06-01T09:18:30Z","2017-09-20T16:44:10Z"
"","3438","KAFKA-3465: Clarify warning message of ConsumerOffsetChecker","Add that the tool works with the old consumer only.","closed","","vahidhashemian","2017-06-26T20:33:51Z","2017-09-25T15:09:38Z"
"","3323","KAFKA-5354: MirrorMaker not preserving headers","Add test case to ensure fix and avoid regression Update mirror maker for new consumer to preserve headers","closed","","michaelandrepearce","2017-06-13T19:53:18Z","2017-06-14T15:18:02Z"
"","3322","KAFKA-5354: MirrorMaker not preserving headers","Add test case to ensure fix and avoid regression Update mirror maker for new consumer to preserve headers","closed","","michaelandrepearce","2017-06-13T19:38:12Z","2017-06-14T10:52:02Z"
"","2756","KAFKA-4818: Implement transactional producer","add public API for transactions  - add new Producer methods to interface  - add new Producer and Consumer config parameters  We want to add the API early to break the dependency for Streams EoS implementation and allow for parallel development.","closed","","mjsax","2017-03-29T01:33:01Z","2017-03-31T00:16:56Z"
"","2758","KAFKA-4925: delay initial rebalance of consumer group","Add new broker config, `group.initial.rebalance.delay.ms`, with a default of 3 seconds. When a consumer creates a new group, set the group's state to InitialRebalance and delay the rebalance until `min(group.initial.rebalance.delay.ms, rebalanceTimeout)`. As other members join the group further delay the rebalance by `min(group.initial.rebalance.delay.ms, remainingRebalanceTimeout)`. Once `rebalanceTimeout` is hit or no new members join the group within the delay, complete the rebalance.","closed","","dguy","2017-03-29T13:12:26Z","2017-05-16T14:04:45Z"
"","3391","Add increment number to clientId","Add increment number to clientId if the ConsumerConfig.CLIENT_ID_CONFIG has been setted. This modify could fix the ""WARN"" when I use spring-kafka that the same client.id can't regist to the mbean. Thanks.","closed","","ccandy413","2017-06-21T05:32:56Z","2018-02-24T23:19:28Z"
"","2594","MINOR: add code quality checks to checkstyle.xml. Also add suppressions","Add code quality/complexity checks to checkstyle","closed","","dguy","2017-02-24T15:48:06Z","2017-03-30T11:10:46Z"
"","3103","KAFKA-5128: check inter broker in transactional methods","Add check in `KafkaApis` that the inter broker protocol version is at least `KAFKA_0_11_0_IV0`, i.e., supporting transactions","closed","","dguy","2017-05-19T15:39:24Z","2017-05-26T16:55:28Z"
"","3026","KAFKA-5713: Shutdown brokers in tests","Add broker shutdown for `LeaderEpochIntegrationTest`. Move broker shutdown in other tests to `tearDown` to ensure brokers are shutdown even if tests fail. Also added assertion to `ZooKeeperTestHarness` to verify that controller event thread is not running since this thread may load JAAS configuration if ZK ports are reused.","closed","","rajinisivaram","2017-05-11T17:36:03Z","2017-05-12T13:12:44Z"
"","2617","KAFKA-4722: Add application.id to StreamThread name","Add application.id to StreamThread name","closed","","sharad-develop","2017-03-01T01:42:12Z","2017-03-08T19:41:33Z"
"","2998","KAFKA-5166: Add option dry run to Streams application reset tool","Add an option dry-run that only prints what topics would get modified/deleted without actually applying any actions.","closed","","bharatviswa504","2017-05-08T20:56:44Z","2017-05-09T19:27:35Z"
"","2979","KAFKA-5129: Add ACL checks for Transactional APIs","Add ACL checks for Transactional APIs","closed","","dguy","2017-05-05T11:07:24Z","2017-05-16T17:15:31Z"
"","3179","MINOR: update stream docs for kip-134","Add a section in the streams docs about the broker config introduced in KIP-134","closed","","dguy","2017-05-31T14:56:08Z","2017-08-16T13:23:23Z"
"","3070","AdminClient: add some more APIs","Add a public create API that takes an AdminClientConf object.  Make the constructors for TopicDescription, TopicListing, and TopicPartitionInfo public to enable AdminClient users to write better tests.","closed","","cmccabe","2017-05-16T20:45:18Z","2017-05-17T11:43:14Z"
"","2905","kafka-5104: DumpLogSegments should not open index files with `rw`","Add a parameter 'writable' for AbstractIndex and set its default value to true for its children classes.","closed","","huxihx","2017-04-24T07:08:03Z","2017-05-05T01:31:18Z"
"","2532","MINOR: update KafkaStreams.metadataForKey(...) javadoc","Add a note to `KafkaStreams.metadataForKey(String, K, Serializer)` to point out that in the case of a Window Store the Serializer should still be the record key serializer and not a window serializer","closed","","dguy","2017-02-10T10:47:45Z","2017-02-17T00:57:55Z"
"","3189","MINOR: add upgrade not for group.initial.rebalance.delay.ms","Add a new entry in upgrade.html for `group.initial.rebalance.delay.ms`","closed","","dguy","2017-06-01T09:26:54Z","2017-08-16T13:23:22Z"
"","2627","[WIP]: add a consumer leave group config","Add a new config to ConsumerConfig, `leave.group.on.close`. When set to `false` then the `AbstractCoordinator` will not send the `LeaveGroupRequest`. Default is `true`.","closed","","dguy","2017-03-02T10:39:57Z","2017-03-07T10:57:28Z"
"","2850","KAFKA-5065; AbstractCoordinator.ensureCoordinatorReady() stuck in loop if absent any bootstrap servers","add a consumer config: ""max.block.ms"" default to 60000 ms; when specified, the ensureCoordinatorReady check default call will be limited by ""max.block.ms""","closed","","porshkevich","2017-04-13T12:56:21Z","2019-01-05T10:08:28Z"
"","2957","KAFKA-5132: abort long running transactions","Abort any ongoing transactions that haven't been touched for longer than the transaction timeout","closed","","dguy","2017-05-02T15:12:13Z","2017-05-16T14:04:39Z"
"","2810","KAFKA-4901: Make ProduceRequest thread-safe","A more conservative version of the change for the 0.10.2 branch.  Trunk commit: 1659ca1773596b.","closed","","ijuma","2017-04-05T09:23:30Z","2017-09-05T09:05:06Z"
"","2716","KAFKA-4927 : KStreamsTestDriver fails with NPE when KStream.to() sinks are used","a KStream.to() sink is also a topic ... so the KStreamTestDriver to fetch it when required","closed","","wimvanleuven","2017-03-21T15:55:26Z","2017-05-11T20:04:44Z"
"","2936","MINOR: JoinGroupRequest V0 invalid rebalance timeout","A JoinGroupRequest V0 built with the Builder had  a rebalance timeout  = -1 rather than equal to session timeout as it would have been if coming from the wire and deserialized from a V0 Struct  fix developed with @mimaison","closed","","edoardocomar","2017-04-28T13:43:07Z","2017-05-12T10:28:26Z"
"","3109","KAFKA-4144: Allow per stream/table timestamp extractor","A follow up RP to fix [issue](https://github.com/confluentinc/examples/commit/2cd0b87bc8a7eab0e7199fa0079db6417f0e6b63#commitcomment-22200864)","closed","","jeyhunkarimov","2017-05-20T09:31:33Z","2017-05-25T02:01:48Z"
"","2635","MINOR: additional refactoring around the use of Errors","A couple of updates were missed in the [PR](https://github.com/apache/kafka/pull/2475) that replaced the use of error codes with Errors objects.","closed","","vahidhashemian","2017-03-03T22:31:39Z","2017-03-06T10:46:47Z"
"","3128","MINOR: remove TransactionCoordinatorIntegrationTest","`TransactionCoordinatorIntegrationTest` is not covering anything that isn't already covered by the more complete `TransactionsTest`","closed","","dguy","2017-05-23T15:13:22Z","2017-05-23T16:52:02Z"
"","3096","KAFKA-5289: handleStopReplica should not send a second response","`shutdownIdleFetcherThreads()` can throw InterruptedException for example.","closed","","ijuma","2017-05-19T09:13:16Z","2017-09-05T09:04:20Z"
"","3246","KAFKA-4948: Wait for offset commit in test to fix transient failure","`DescribeConsumerGroupTest#testDescribeExistingGroupWithNoMembersWithNewConsumer` shuts down the consumer executor thread and then checks that the assignments returned by `describeGroup` contain the consume group with no members. But if the executor thread is shut down before any offsets are committed, the assignments returned by `describeGroup` doesn't contain the group at all. This PR waits for an offset commit by waiting for the group to appear in `describeGroup` assignments prior to shutting down the executor.","closed","","rajinisivaram","2017-06-06T14:10:06Z","2017-06-06T16:09:10Z"
"","3131","org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition","``` [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,15] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,45] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,27] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,12] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,9] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,42] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) [2017-05-24 09:39:34,637] ERROR [ReplicaFetcherThread-0-175], Error for partition [__consumer_offsets,24] to broker 175:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread) ``` =================== I just use [bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker]  to get the offset info from this kafka cluster. Who can tell me why the server log always print this error?  Thanks!","closed","","linxiaobai","2017-05-24T01:47:00Z","2018-01-22T05:11:12Z"
"","3052","KAFKA-5237 - change logging of termination message to use stderr instead of stdout","[KAFKA-5237](https://issues.apache.org/jira/browse/KAFKA-5237) - change logging of termination message to use stderr instead of stdout","open","","deinspanjer","2017-05-14T22:09:56Z","2018-03-02T19:30:10Z"
"","2749","[KAFKA-4964]Delete the kafka to prefix the name of the keystore and truststore file will be more suitable","[https://issues.apache.org/jira/browse/KAFKA-4964](url) Kafka to prefix the name of the keystore and truststore file,will possible to cause misdirection, because of according to the previous steps to generate the file name without that prefix. Delete the prefix may helpful to the kafka SSL beginners.","closed","","zhengsg","2017-03-28T08:24:55Z","2017-04-14T01:32:27Z"
"","2981","KAFKA-5173: Add logging to dump JaasContext before writing it to file","@rajinisivaram plz review.","closed","","baluchicken","2017-05-05T13:28:21Z","2017-05-08T07:24:13Z"
"","3062","KAFKA-5225: StreamsResetter tool to allow custom consumer properties","@mjsax @guozhangwang Could you please review the changes.","closed","","bharatviswa504","2017-05-15T21:03:41Z","2017-05-22T21:16:30Z"
"","3073","KAFKA-5210:Application Reset Tool does not need to seek for internal topics","@mjsax @dguy @guozhangwang Could you please review the changes.","closed","","bharatviswa504","2017-05-16T22:51:52Z","2017-05-18T06:16:31Z"
"","2860","kafka-5068: Optionally print out metrics after running the perf tests","@junrao added a config `--print.metrics` to control whether ProducerPerformance prints out metrics at the end of the test. If its okay, will add the code counterpart for consumer.","closed","","huxihx","2017-04-17T02:53:23Z","2017-04-19T22:51:01Z"
"","2919","KAFKA-5127 Replace pattern matching with foreach where the case None …","@ijuma plz review. This one is not complete because KafkaController, AdminUtils and ReplicaStateMachine has some of these but for those I will wait until the KAFKA-5028 and KAFKA-5103 are merged.","closed","","baluchicken","2017-04-26T13:47:58Z","2017-07-14T17:04:25Z"
"","3097","KAFKA-5134 Replace zkClient.getChildren method with zkUtils.getChildren","@ijuma plz review. I also created a version where I didn't refactor the  getPartitionAssignmentForTopics and the getConsumersPerTopic methods, but I thought maybe it is a good idea to do and move the logic (where there is no ZK connection) to the ZkUtils Object. About the two new methods, I wanted to make them private and visible only for testing, but because the tests where I use them are not in the same package I couldn't do it.","closed","","baluchicken","2017-05-19T10:06:36Z","2017-07-20T11:49:34Z"
"","2506","KAFKA-4703 Test with two SASL_SSL listeners with different JAAS contexts","@ijuma plz review","closed","","baluchicken","2017-02-06T14:57:07Z","2017-05-03T20:10:08Z"
"","2886","KAFKA-5101 Remove KafkaController's incrementControllerEpoch method p…","@ijuma minor fix. can you please review.","closed","","baluchicken","2017-04-21T12:36:48Z","2017-04-27T23:09:11Z"
"","3245","KAFKA-5391 Replace zkClient.delete* method with an equivalent zkUtills","@ijuma can you please review.","closed","","baluchicken","2017-06-06T13:59:18Z","2017-06-08T16:00:21Z"
"","3281","KAFKA-5388 Replace zkClient.subscribe*Changes method with an equivalent zkUtils method","@ijuma can you please review if you have time:) ?","closed","","baluchicken","2017-06-09T11:36:35Z","2017-08-04T13:20:22Z"
"","3243","KAFKA-5389 Replace zkClient.exists method with zkUtils.pathExists","@ijuma can you please review","closed","","baluchicken","2017-06-06T12:52:52Z","2017-06-07T18:33:50Z"
"","2952","MINOR: fix some c/p error that was causing describe to delete","@ijuma @cmccabe","closed","","norwood","2017-05-02T06:45:05Z","2017-05-02T11:45:48Z"
"","2888","KAFKA-5103 Refactor AdminUtils to use zkUtils methods instad of zkUti…","@ijuma  plz review.","closed","","baluchicken","2017-04-21T14:08:48Z","2017-04-28T01:19:12Z"
"","2679","HOTFIX: Fix apache headers in float serde class files","@hachikuji","closed","","guozhangwang","2017-03-13T20:03:11Z","2017-07-15T22:07:08Z"
"","3016","KAFKA-5216 fix error on peekNextKey in cached window/session store iterators","@guozhangwang @mjsax @dguy","closed","","xvrl","2017-05-10T23:58:42Z","2017-05-12T23:51:42Z"
"","3430","KAFKA-4750: RocksDBStore always deletes null values","@guozhangwang @mjsax","closed","streams,","evis","2017-06-26T09:26:51Z","2018-02-01T20:47:07Z"
"","3466","MINOR: fix InsertSource transformation properties in doc","@guozhangwang","closed","","evis","2017-06-30T09:46:38Z","2019-10-30T07:27:38Z"
"","2732","KAFKA-4855 Struct SchemaBuilder should not allow duplicate fields","@ewencp can you please review.","closed","","baluchicken","2017-03-24T12:53:52Z","2017-04-04T03:10:42Z"
"","3099","KAFKA-4660 : Improve test coverage KafkaStreams","@dguy , @mjsax Please review the PR and let me know your comments.","closed","","umesh9794","2017-05-19T11:08:39Z","2017-09-14T06:27:58Z"
"","2873","KAFKA-5072[WIP]: Kafka topics should allow custom metadata configs within some config namespace","@benstopford @ijuma @granthenke @junrao   This change allows one to define any topic property within the namespace ""metadata.*"" - for e.g. metadata.description, metadata.project, metadata.contact.info, etc (More details on the JIRA)  Raising a PR with [WIP] tag since I am not sure how to add this to the documentation given that the list of topic properties is auto-generated for the documentation.  This contribution is my original work and I license the work to the Kafka project under the project's open source license","open","","soumabrata-chakraborty","2017-04-19T15:07:02Z","2018-03-02T19:30:04Z"
"","3225","KAFKA-5371: Increase request timeout for producer used by testReachableServer","500ms is low for a shared Jenkins environment.  Also removed the try/catch blocks that simply obscured the underlying error.","closed","","ijuma","2017-06-03T08:18:52Z","2017-06-18T09:27:06Z"
"","2740","MINOR: Use method handles instead of reflection for creating Snappy and LZ4 streams","1. Use Initialization-on-demand holder idiom that relies on JVM lazy-loading instead of explicit initialization check. 2. Method handles were designed to be faster than Core Reflection, particularly if the method handle can be stored in a static final field (the JVM can then optimise the call as if it was a regular method call). Since the code is of similar complexity (and simpler if we consider the whole PR), I am treating this as a clean-up instead of a performance improvement (which would require doing benchmarks). 3. Remove unused `ByteBufferReceive`. 4. I removed the snappy library from the classpath and verified that `CompressionTypeTest` (which uses LZ4) still passes. This shows that the right level of laziness is achieved even if we use one of the lazily loaded compression algorithms.","closed","","ijuma","2017-03-26T02:36:34Z","2017-09-05T09:06:07Z"
"","2490","kafka-4727: A Production server configuration needs to be updated","1. Update value for queued.max.requests to 500 2. Removed invalid config 'controller.message.queue.size' 3. Removed flush configs including 'log.flush.interval.messages', 'log.flush.interval.ms' and 'log.flush.scheduler.interval.ms'","closed","","huxihx","2017-02-03T01:06:42Z","2017-02-07T16:41:35Z"
"","3238","KAFKA-5380: Fix transient failure in KafkaConsumerTest, close consumers","1. Fix ordering of metadata update request for regex subscription to avoid timing issue when heartbeat thread updates metadata 2. Override metadata cluster in MockClient for `KafkaConsumer#testChangingRegexSubscription` to avoid timing issues during update 3. Close consumer in all KafkaConsumer tests since they leave behind heartbeat threads.","closed","","rajinisivaram","2017-06-05T18:03:16Z","2017-06-07T11:53:36Z"
"","2964","KAFKA-5130: Refactor TC In-memory Cache","1. Collapsed the `ownedPartitions`, `pendingTxnMap` and the `transactionMetadataCache` into a single in-memory structure, which is a two-layered map: first keyed by the transactionTxnLog, and then valued with the current coordinatorEpoch of that map plus another map keyed by the transactional id.  2. Use `transactionalId` across the modules in transactional coordinator, attach this id with the transactional marker entries.  3. Use two keys: `transactionalId` and `txnLogPartitionId` in the writeMarkerPurgatory as well as passing it along with the TxnMarkerEntry, so that `TransactionMarkerRequestCompletionHandler` can use it to access the two-layered map upon getting responses.  4. Use one queue per `broker-id` and `txnLogPartitionId`. Also when there is a possible update on the end point associated with the `broker-id`, update the Node without clearing the queue but relying on the requests to retry in the next round.  5. Centralize the error handling callback for appending-metadata-to-log and sending-markers-to-brokers in `TransactionStateManager#appendTransactionToLog`, and `TransactionMarkerChannelManager#addTxnMarkersToSend`.  6. Always update the in-memory transaction metadata AFTER the txn log has been appended and replicated, and then double check on the cache to make sure nothing has changed since log appending. The only exception is when initializing the pid for the first time, in which we will put a dummy into the cache but set its pendingState as `Empty` (so it will be valid to transit from `Empty` to `Empty`) so that it can be updated after the log append has completed.","closed","","guozhangwang","2017-05-03T02:37:56Z","2017-11-06T22:44:21Z"
"","3010","KAFKA-5184, KAFKA-5173; Various improvements to SASL tests","1. Call `closeSasl` in `MultipleListenersWithSameSecurityProtocolBaseTest` 2. Refactor the code to make it easier to reason about 3. Add an assert that may possibly help us narrow down how KAFKA-5184 can happen (it seems impossible). 4. Remove SaslTestHarness to make it easier to reason about setUp and tearDown methods. 5. Fix *AdminClientIntegrationTest to have a single `tearDown` 6. Remove a *ReplicaFetcherTest and *TopicMetadataTest secure variants. They are redundant from a security perspective given the consumer and producer tests.","closed","","ijuma","2017-05-10T11:58:05Z","2017-09-05T09:04:33Z"
"","2488","MINOR: add architecture section and configure / execution for streams","1. Added an architecture section. 2. Added a configuration / execution sub-section to developer guide.  Minor tweaks and a bunch of missing fixes from `kafka-site` repo.","closed","","guozhangwang","2017-02-03T00:37:27Z","2017-07-15T22:07:19Z"
"","2908","KAFKA-4379 Followup: Avoid sending to changelog while restoring InMemoryLRUCache","1. Added a flag to indicate if it is restoring or not in the LRU Store; since we only have a restore callback we have to set it each time applying the change. 2. Fixed the corresponding unit test, plus some minor cleaning up.","closed","","guozhangwang","2017-04-25T04:50:49Z","2017-07-15T22:08:13Z"
"","2702","MINOR: Improvements on log4j","1. add thread id as prefix in state directory classes; also added logs for lock activities. 2. add logging for task creation / suspension. 3. add more information in rebalance listener logging. 4. add restored number of records into changlog reader.","closed","","guozhangwang","2017-03-17T21:04:28Z","2017-07-15T22:07:59Z"
"","3092","KAFKA-5277: Sticky Assignor should not cache previous assignment (KIP-54 follow-up)","... plus some minor cleanup","closed","","vahidhashemian","2017-05-18T19:36:48Z","2018-06-08T21:36:28Z"
"","3411","KAFKA-5487: upgrade and downgrade streams app system test","-Tests for rolling upgrades for a streams app (keeping broker config fixed) -Tests for rolling upgrades of brokers (keeping streams app config fixed)","closed","","enothereska","2017-06-22T16:36:12Z","2017-06-24T07:43:09Z"
"","2528","KAFKA-4745 -Optimize close to remove unnecessary flush","-Remove unnecessary 'flush', the underlying stream should handle it on close, also most apps flush before close.","closed","","wdroste","2017-02-09T18:03:16Z","2017-03-09T18:30:30Z"
"","3465","MINOR: Fixed the invalid link to Log Compaction","- variable name updated in the doc example.","closed","","kamalcph","2017-06-30T07:32:50Z","2018-02-12T05:03:02Z"
"","3364","HOTFIX: Improve error handling for ACL requests","- Use ResourceType.toJava instead of ResourceType.fromString. The latter doesn't work for TransactionalId (or any type with two camel-case words). - Replace Throwable with ApiError in response classes. - Return InvalidRequest instead of Unknown error if ANY or UNKNOWN are provided during ACL creation. - Rename `unknown()` to `isUnknown()` in a few places that were missed previously. - Add tests.","closed","","ijuma","2017-06-17T05:08:36Z","2017-06-18T09:25:41Z"
"","2885","MINOR: Logger parameter corrected.","- typo error corrected (spelling)","closed","","kamalcph","2017-04-21T11:57:05Z","2017-06-20T04:33:53Z"
"","3271","KAFKA-5411: AdminClient javadoc and documentation improvements","- Show AdminClient configs in the docs. - Update Javadoc config so that public classes exposed by the AdminClient are included. - Version and table of contents fixes.","closed","","ijuma","2017-06-08T15:17:48Z","2017-06-18T09:29:43Z"
"","2967","KAFKA-5150 reduce lz4 decompression overhead","- reuse decompression buffers in consumer Fetcher - switch lz4 input stream to operate directly on ByteBuffers - avoids performance impact of catching exceptions when reaching the end of legacy record batches - more tests with both compressible / incompressible data, multiple   blocks, and various other combinations to increase code coverage - fixes bug that would cause exception instead of invalid block size   for invalid incompressible blocks - fixes bug if incompressible flag is set on end frame block size  Overall this improves LZ4 decompression performance by up to 40x for small batches. Most improvements are seen for batches of size 1 with messages on the order of ~100B. We see at least 2x improvements for for batch sizes of < 10 messages, containing messages < 10kB  This patch also yields 2-4x improvements on v1 small single message batches for other compression types.  Full benchmark results can be found here https://gist.github.com/xvrl/05132e0643513df4adf842288be86efd","closed","","xvrl","2017-05-03T21:00:48Z","2017-05-31T18:43:32Z"
"","2648","KAFKA-4849: Bug in KafkaStreams documentation","- removed ZK config from Streams cods  - removed deprecated ZK config parameter from configs table","closed","docs,","mjsax","2017-03-07T01:19:16Z","2018-05-11T17:26:03Z"
"","3079","MINOR: Removed the redundant code snippets.","- Removed the unwanted null check in the hashCode and equals method.","closed","","kamalcph","2017-05-17T12:54:05Z","2017-05-30T11:06:12Z"
"","3350","MINOR: Redirected the creation of new Thread to KafkaThread class.","- removed `newThread` & `daemonThread` methods from the Utils.","closed","","kamalcph","2017-06-15T11:56:53Z","2017-06-20T04:31:34Z"
"","3276","KAFKA-5361: Add more integration tests for Streams EOS","- multi-subtopology tests  - fencing test  - producer fenced bug fix: Streams did not recover correctly from ProducerFencedException","closed","","mjsax","2017-06-08T23:59:42Z","2017-06-14T17:25:19Z"
"","2917","KAFKA-5111: code cleanup follow up","- mainly moving methods  - also improved logging","closed","","mjsax","2017-04-26T03:29:56Z","2017-04-28T02:57:41Z"
"","2578","MINOR: ConfigKey variable renamed","- Java doc updated - Used final modifier where-ever it's applicable - Unnecessary type-cast removed - cleanup","closed","","kamalcph","2017-02-21T11:01:50Z","2017-03-14T12:48:43Z"
"","3140","KAFKA-5309: Stores not queryable after one thread died","- introduces a new thread state DEAD  - ignores DEAD threads when querying","closed","","mjsax","2017-05-24T21:54:19Z","2017-05-26T17:04:33Z"
"","3444","KAFKA-5006: Better exception messages when producing","- Instead of throwing exception inside record collector, throw it at a higher level in the processing task as desribed by @guozhangwang: ""After looking into your pointed code I feel we should not throw exception in RecordCollector.send(), instead the exception should be thrown from the coarsen-grained task processing logic itself."" in KAFKA-5006's comments.","closed","","enothereska","2017-06-27T12:54:42Z","2017-07-12T19:57:37Z"
"","3467","MINOR: InFlightRequests#isEmpty(node) method corrected.","- In clearAll method, get operation is removed. - variable name `requestTimeout` changed to `requestTimeoutMs` for clarity","closed","","kamalcph","2017-06-30T12:17:54Z","2017-08-25T12:49:31Z"
"","2643","KAFKA-4843: More efficient round-robin scheduler","- Improves streams efficiency by more than 200K requests/second (small 100 byte requests) - Gets streams efficiency very close to pure consumer (see results in https://jenkins.confluent.io/job/system-test-kafka-branch-builder/746/console)  - Maintains same fairness across tasks - Schedules all records in the queue in-between poll() calls, not just one per task.","closed","","enothereska","2017-03-05T15:33:50Z","2017-03-30T06:34:54Z"
"","2830","KAFKA-5046: Support file rotation in FileStreamSource Connector","- Detect file rotation/removal in FileStreamSource task. - Extend offset key to include creation time and fileKey attributes.","closed","connect,","kkonstantine","2017-04-09T18:00:35Z","2022-05-25T22:07:50Z"
"","2802","MINOR: Log append validation improvements","- Consistent validation across different code paths in LogValidator - Validate baseOffset for message format V2 - Flesh out LogValidatorTest to check producerId, baseSequence, producerEpoch and partitionLeaderEpoch.","closed","","ijuma","2017-04-04T10:18:16Z","2017-09-05T09:52:46Z"
"","2495","Adjustments to docs to make them work with latest template","- Change introduction.html to be a partial like all the other doc content sections - Fix typo with include statement for the footer of docs - Stop embedding the full Streams documentation inside of documentation.html now that we're treating it as a separate section.","open","","derrickdoo","2017-02-03T21:57:43Z","2018-03-02T19:29:54Z"
"","2788","MINOR: improve test cleanup","- call close() on Metrics to join created threads","closed","","mjsax","2017-04-02T02:01:25Z","2017-04-21T20:22:48Z"
"","3087","MINOR: Small refactor of request quotas handling in KafkaApis","- Avoid unnecessary inner methods - Remove redundant parameter in `sendResponseExemptThrottle` - Go through `sendResponseExemptThrottle` for produce requests with acks=0 - Tighten how we handle cases where there’s no response","closed","","ijuma","2017-05-18T14:10:47Z","2017-06-18T09:27:30Z"
"","2854","KAFKA-4986: Adding producer per task (follow-up)","- addressing open Github comments from #2773  - test clean-up","closed","","mjsax","2017-04-14T16:53:00Z","2017-04-28T16:24:34Z"
"","2988","Added the producer record metadata to the SourceTask commitRecord","- Added the Producers Record Metadata object to the commitRecord method on the SourceTask class so more data is provided to those who override and wish to hook into the recordCommit call from the producer.  -If its a transformation, it will return null which is explained in the javadoc.","closed","","GeoSmith","2017-05-07T03:54:27Z","2017-05-07T04:12:02Z"
"","3098","KAFKA-5291: AdminClient should not trigger auto creation of topics","- Added a boolean `allow_auto_topic_creation` to MetadataRequest and bumped the protocol version to V4.  - When connecting to brokers older than 0.11.0.0, the `allow_auto_topic_creation` field won't be considered, so we send a metadata request for all topics to keep the behavior consistent.  - Set `allow_auto_topic_creation` to false in the new AdminClient and StreamsKafkaClient (which exists for the purpose of creating topics manually); set it to true everywhere else for now. Other clients will eventually rely on client-side auto topic creation, but that’s not there yet.  - Add `allowAutoTopicCreation` field to `Metadata`, which is used by `DefaultMetadataUpdater`. This is not strictly needed for the new `AdminClient`, but it avoids surprises if it ever adds a topic to `Metadata` via `setTopics` or `addTopic`.","closed","","ijuma","2017-05-19T10:46:36Z","2017-06-18T09:30:38Z"
"","2974","KAFKA-4923: Add Exaclty-Once Semantics to Streams (testing)","- add broker compatibility system tests","closed","","mjsax","2017-05-04T05:26:41Z","2017-05-22T16:43:28Z"
"","2703","KAFKA-4772: Use peek to implement print","**Tackles [KAFKA-4772](https://issues.apache.org/jira/browse/KAFKA-4772) and was previously discussed in https://github.com/apache/kafka/pull/2669** Serves as an alternative to: https://github.com/apache/kafka/pull/2704 in case topic name is not a necessity for now.  The functionality of `KeyValuePrinter` is replaced with `PrintAction` which implements `ForeachAction` and can be passed to `KStreamPeek`. We therefore can get rid of `KeyValuePrinter`. My only concern is that `ForeachAction` does not have access to the `ProcessorContext`, which means, that the topic name cannot be used within the deserialization process. For now, the a null value is filled in as a placeholder for topic.","closed","streams,","mjuchli","2017-03-18T14:10:49Z","2018-01-30T04:41:28Z"
"","2669","KAFKA-4772: [WIP] Use peek to implement print","**PROPOSAL/DISCUSSION FOR KIP-132.** Tackles [KAFKA-4772](https://issues.apache.org/jira/browse/KAFKA-4772) and [KAFKA-4830](https://issues.apache.org/jira/browse/KAFKA-4830).  The functionaliy of KeyValuePrinter is replaced with a printAction that is being passed to KStreamPeek. We therefore can get rid of KeyValuePrinter.  KStream.print was extended to handle KeyValueMapper in order to provide users the option to change output of K and V. Therefore, constructors of KeyValuePrinter as well as KeyValuePrinterProcessor have to be adapted. The default case where no mapper will be passed, is still covered by the previous comma separated out of K, V.","closed","streams,","mjuchli","2017-03-10T14:36:20Z","2018-01-30T04:41:21Z"
"","3116","KAFKA-5081: force version for 'jackson-annotations' in order to prevent redundant jars from being bundled into kafka distribution (and to align with other jackson libraries)","**JIRA ticket:** [KAFKA-5081 two versions of jackson-annotations-xxx.jar in distribution tgz](https://issues.apache.org/jira/browse/KAFKA-5081)  **Solutions:** 1. accept this merge request **_OR_** 2. upgrade jackson libraries to version **_2.9.x_** (currently available as a pre-release only)  **Related jackson issue:** [Add explicit \`jackson-annotations\` dependency version for \`jackson-databind\`](https://github.com/FasterXML/jackson-databind/issues/1545)  **Note:** previous (equivalent) merge request #2900 ended up deep in the sand with swarm of messages due to flaky test, so I opted to close it and to open this one.  @ijuma: FYI","closed","","dejan2609","2017-05-22T17:02:07Z","2017-05-24T07:23:19Z"
"","2989","KAFKA-5185: Adding the RecordMetadata that is returned by the producer to the commitRecord method for SourceTask","**Included:** - Added the producers record metadata object to the commitRecord method on the SourceTask class so more data is provided from the producer and it allows anyone overriding and hooking into the commitRecord method to receive more information about where the record was produced to.  - If its a transformation, it will send in a null,  which is explained in the javadoc.","closed","","GeoSmith","2017-05-07T05:22:03Z","2018-01-03T21:38:48Z"
"","2704","KAFKA-4772: [WIP] Use KStreamPeek to replace KeyValuePrinter","**Alternative to: https://github.com/apache/kafka/pull/2703 and serves as a reference for discussion.** Tackles [KAFKA-4772](https://issues.apache.org/jira/browse/KAFKA-4772) and was previously discussed in https://github.com/apache/kafka/pull/2669  This PR contains only a slight improvement over the current `KeyValuePrinter`. The main functionaliy of the printer was refactored such that ForeachAction can be used. This would allow to further refactor KeyValuePrinter as soon as ForeachAction accepts a ProcessorContext as an argument, which is required to retrieve the topic name.","closed","streams,","mjuchli","2017-03-18T14:15:35Z","2018-01-30T04:42:18Z"
"","2574","MINOR: Fixed 3 inner classes without instance reference to be static","* Turned 3 inner classes that weren't static but could be into `static` ones. * Turned one `public` inner class that wasn't used publicly into a `private`.  Trivial but imo worthwhile to explicitly keep visibility and ""staticness"" correct in syntax (if only to be nice to the GC) :)","closed","","original-brownbear","2017-02-19T09:26:31Z","2017-02-20T00:44:44Z"
"","3111","HOTFIX: Increase timeouts in distributed connect tests.","* There's considerably increased logging in DEBUG mode due to the class scanning  performed with the new isolation scheme.","closed","","kkonstantine","2017-05-21T05:49:34Z","2018-01-15T23:54:02Z"
"","2862","KAFKA-5076: remove usage of java.xml.bind.* classes","* replaces base64 from DatatypeConverter with Base64 introduced in JDK 8 * this change enables running Kafka with Java 9  Given that we plan to stop supporting Java 7 in 0.11 this should be fine, but merging this would depend on when we are comfortable introducing backwards-incompatible changes.","closed","","xvrl","2017-04-17T16:44:30Z","2017-08-16T22:44:13Z"
"","2920","KAFKA-5005: IntegrationTestUtils Consumers need to run from earliest offset","* Producer and Consumer `close` calls were not handled via `try-with-resources` * `cleanRun` unused field removed * Refactored handling of Consumer configuration in `IntegrationTestUtils` to ensure auto-committing of offsets and starting from `earliest`   * As a result reverted https://github.com/apache/kafka/pull/2921 since it's redundant now","closed","","original-brownbear","2017-04-26T13:59:11Z","2017-04-28T00:52:37Z"
"","3240","KAFKA-5292. Fix authorization checks in AdminClient","* NetworkClient.java: when trace logging is enabled, show AbstractResponse Struct objects, rather than just a memory address of the AbstractResponse. * AclOperation.java: add documentation of what ACLs imply other ACLs. * Resource.java: add CLUSTER, CLUSTER_NAME constants. * Reconcile the Java and Scala classes for ResourceType, OperationType, etc.  Add unit tests to ensure they can be converted to each other. * AclCommand.scala: we should be able to apply ACLs containing Alter and Describe operations to Cluster resources. * SimpleAclAuthorizer: update the authorizer to handle the ACL inheritance rules described in AclOperation.java. * KafkaApis.scala: update createAcls and deleteAcls to use ALTER on CLUSTER, as described in the KIP.  describeAcls should use DESCRIBE on CLUSTER.  Use      fromJava methods instead of fromString methods to convert from Java objects to Scala ones. * SaslSslAdminClientIntegrationTest.scala: do not use AllowEveryoneIfNoAclIsFound.  Add a configureSecurityBeforeServerStart hook which installs the ACLs     necessary for the tests.  Add a test of ACL authorization ALLOW and DENY functionality.","closed","","cmccabe","2017-06-06T01:26:12Z","2019-05-20T19:02:15Z"
"","3381","KAFKA-5479: Tidy up Authorization section of Security docs","* Mention the authz is disabled by default and enabled via authorizer.class.name * ACL is an initialism, so spell it ACL not acl. * In examples standardize on ""we"" rather than having a mixture of ""the user"", ""you"" and ""we"" * Link to KIP-11 instead of just referencing it  This contribution is my original work and I license the work to the project under the project's open source license.","open","","tombentley","2017-06-20T09:44:10Z","2018-03-02T19:30:17Z"
"","3042","MINOR: Fix one flaky test in MetricsTest and improve checks for another","* Fix flakiness of `testBrokerTopicMetricsUnregisteredAfterDeletingTopic` by not consuming messages. Filed KAFKA-5238 to track the issue that metrics for a deleted topic may be re-created if there are fetch requests in the purgatory.  * Check the log size in `testBrokerTopicMetricsBytesInOut` before attempting to read the `replicationBytesIn` metric. This helps understand where things have gone wrong if if the metric has not increased (i.e. if it was an issue replicating or with the metric).  * Only remove the replication bytes in/out if the metrics are defined. This should not affect the behaviour due to the tags, but it makes the code clearer. We've seen some cases in Jenkins when the metric does not exist and it's still unclear how that can happen.","closed","","ijuma","2017-05-13T12:08:22Z","2017-09-05T08:43:41Z"
"","3078","MINOR: Use exceptions from clients jar if possible","* Avoid `import kafka.common._` * Delete now unused kafka.common.InvalidOffsetException * Change kafka.common.KafkaException to inherit from org.apache.kafka.common.KafkaException","closed","","ijuma","2017-05-17T11:10:44Z","2018-10-11T14:31:41Z"
"","2899","KAFKA-5081: use gradle resolution strategy 'failOnVersionConflict'","(in order to prevent redundant jars from being bundled into kafka distribution)","closed","","dejan2609","2017-04-23T17:31:41Z","2017-04-23T21:12:31Z"
"","2869","KAFKA-5086; Update topic expiry time in Metadata every time the topic metadata is requested","#As of current implementation, KafkaProducer.waitOnMetadata() will first reset topic expiry time of the topic before repeatedly sending TopicMetadataRequest and waiting for metadata response. However, if the metadata of the topic is not available within Metadata.TOPIC_EXPIRY_MS, which is set to 5 minutes, then the topic will be expired and removed from Metadata.topics. The TopicMetadataRequest will no longer include the topic and the KafkaProducer will never receive the metadata of this topic. It will enter an infinite loop of sending TopicMetadataRequest and waiting for metadata response.  This problem can be fixed by updating topic expiry time every time the topic metadata is requested.  Ping @becketqin for review.","closed","","lindong28","2017-04-19T04:09:06Z","2017-04-27T22:51:01Z"
"","2765","MINOR: Fix race condition in TestVerifiableProducer sanity test","## Fixes race condition in TestVerifiableProducer sanity test: The test starts a producer, waits for at least 5 acks, and then logs in to the worker to grep for the producer process to figure out what version it is running.  The problem was that the producer was set up to produce 1000 messages at a rate of 1000 msgs/s and then exit. This means it will have a typical runtime slightly above 1 second.  Logging in to the vagrant instance might take longer than that thus resulting in the process grep to fail, failing the test.  This commit doesn't really fix the issue - a proper fix would be to tell the producer to stick around until explicitly killed - but it increases the chances of the test passing, at the expense of a slightly longer runtime.  ## Improves error reporting when is_version() fails","closed","","edenhill","2017-03-30T09:52:32Z","2017-05-22T01:24:21Z"
"","3375","KAFKA-5474: Streams StandbyTask should no checkpoint on commit if EOS is enabled","- actual fix for `StandbyTask#commit()`   Additionally (for debugging):  - EOS test, does not report ""expected"" value correctly  - add `IntegerDecoder` (to be use with `kafka.tools.DumpLogSegments`)  - add test for `StreamTask` to not checkpoint on commit if EOS enabled","closed","","mjsax","2017-06-19T22:00:18Z","2017-06-22T17:05:59Z"
"","3475","MINOR: Unit test added for InFlightRequests class.","","closed","","kamalcph","2017-07-01T08:23:12Z","2018-03-16T07:00:28Z"
"","3471","MINOR: Add test brokers to list as they are created to ensure cleanup","","closed","","rajinisivaram","2017-06-30T18:54:23Z","2017-07-04T09:38:22Z"
"","3468","KAFKA-5542: Improve Java doc for LeaderEpochFileCache.endOffsetFor()","","closed","","benstopford","2017-06-30T13:02:59Z","2017-06-30T22:03:12Z"
"","3463","KAFKA-5544; The LastStableOffsetLag metric should be removed when partition is deleted","","closed","","lindong28","2017-06-30T03:16:46Z","2017-07-03T00:55:12Z"
"","3462","KAFKA-5543: Remove all partition metrics when a topic is deleted","","open","","apurvam","2017-06-29T23:42:44Z","2018-06-19T22:27:43Z"
"","3457","MINOR: Rename baseTimestamp to firstTimestamp to clarify usage","","closed","","hachikuji","2017-06-28T21:23:23Z","2017-06-30T05:49:57Z"
"","3456","KAFKA-5522: ListOffsets should bound timestamp search by LSO in read_committed","","closed","","hachikuji","2017-06-28T20:35:21Z","2017-06-30T22:41:07Z"
"","3455","KAFKA-5379 - ProcessorContext.appConfigs() should return parsed values.","","closed","","twbecker","2017-06-28T15:25:07Z","2017-08-29T23:07:54Z"
"","3454","MINOR: Compatibility and upgrade tests for 0.11.0.x","","closed","","ijuma","2017-06-28T14:56:00Z","2017-12-22T18:14:16Z"
"","3450","KAFKA-5529 Use only KafkaProducer in ConsoleProducer","","closed","","evis","2017-06-28T08:12:02Z","2018-10-18T15:15:05Z"
"","3449","KAFKA-5167: Release state locks in case of failure","","closed","","mjsax","2017-06-28T02:00:35Z","2017-07-06T05:15:15Z"
"","3446","MINOR: Improve TransactionIndex.sanityCheck() message","","closed","","ijuma","2017-06-27T22:09:05Z","2017-09-05T08:38:23Z"
"","3445","MINOR: remove unnecessary null check","","closed","","mjsax","2017-06-27T22:00:19Z","2017-06-27T23:06:23Z"
"","3442","KAFKA-5512:  Awake the heartbeat thread when timetoNextHeartbeat is equal to 0","","closed","","rosets","2017-06-27T10:24:07Z","2017-07-21T15:55:30Z"
"","3439","KAFKA-5464: StreamsKafkaClient should not use StreamsConfig.POLL_MS_CONFIG","","closed","","mjsax","2017-06-26T22:58:00Z","2017-07-06T17:59:48Z"
"","3437","MINOR: Make JmxMixin wait for the monitored process to be listening on the JMX port before launching JmxTool","","closed","","ewencp","2017-06-26T18:21:24Z","2017-06-27T00:08:14Z"
"","3436","KAFKA-5517: Add id to config HTML tables to allow linking","","closed","","tombentley","2017-06-26T15:54:11Z","2020-02-25T11:56:56Z"
"","3434","KAFKA-5516: Formatting verifiable producer/consumer output in a similar fashion","","closed","","ppatierno","2017-06-26T12:59:07Z","2017-08-16T11:17:33Z"
"","3433","MINOR: typo in variable name ""unkownInfo""","","closed","","evis","2017-06-26T12:22:22Z","2017-07-13T07:34:52Z"
"","3428","HOTFIX: Add back the copy-constructor of abstract stream","","closed","","guozhangwang","2017-06-25T17:55:08Z","2017-07-15T22:06:49Z"
"","3424","MINOR: Typo in comments makes debug confusing","","closed","","enothereska","2017-06-23T18:50:02Z","2017-06-24T06:35:29Z"
"","3421","KAFKA-5507: Check classpath empty in kafka-run-class.sh","","closed","","evis","2017-06-23T12:48:30Z","2017-08-09T23:00:38Z"
"","3416","MINOR: improve test README","","closed","","mjsax","2017-06-22T23:14:16Z","2017-06-23T00:16:35Z"
"","3415","MINOR: Make 'Topic-Level Configs' a doc section for easier access","","closed","","vahidhashemian","2017-06-22T21:18:23Z","2017-07-19T13:28:10Z"
"","3414","HOTFIX: reduce log verbosity on commit","","closed","","mjsax","2017-06-22T20:29:05Z","2017-06-26T18:23:10Z"
"","3412","KAFKA-5498: ConfigDef derived from another ConfigDef did not correctly compute parentless configs","","closed","","ewencp","2017-06-22T19:04:17Z","2017-06-22T20:02:17Z"
"","3409","MINOR: Switch ZK client logging to INFO","","closed","","ijuma","2017-06-22T14:55:54Z","2017-09-05T08:38:43Z"
"","3407","KAFKA-3881: Remove the replacing logic from ""."" to ""_"" in Fetcher","","closed","","tombentley","2017-06-22T10:33:29Z","2020-02-25T12:15:24Z"
"","3406","KAFKA-5490: Retain empty batch for last sequence of each producer","","closed","","hachikuji","2017-06-22T05:21:41Z","2017-07-06T09:57:02Z"
"","3403","MINOR: Turn off caching in demos for more understandable outputs","","closed","","guozhangwang","2017-06-21T21:24:10Z","2017-07-15T22:06:53Z"
"","3401","MINOR: explain producer naming within Streams","","closed","","mjsax","2017-06-21T18:52:16Z","2017-06-22T15:34:33Z"
"","3399","KAFKA-5475: Connector config validation should include fields for defined transformation aliases","","closed","connect,","ewencp","2017-06-21T18:04:09Z","2020-10-16T06:08:16Z"
"","3394","KAFKA-5475: Connector config validation needs to include tranformation types","","closed","connect,","kkonstantine","2017-06-21T10:38:17Z","2020-10-16T06:08:16Z"
"","3393","KAFKA-5319: Add a tool to balance replicas and leaders of cluster","","open","","MarkTcMA","2017-06-21T10:35:37Z","2017-09-28T09:59:55Z"
"","3390","KAFKA-5485: Streams should not suspend tasks twice","","closed","","mjsax","2017-06-21T02:13:17Z","2017-07-01T20:52:59Z"
"","3389","KAFKA-5484: Refactor kafkatest docker support","","closed","","cmccabe","2017-06-20T22:28:28Z","2019-05-20T18:58:53Z"
"","3388","KAFKA-5021: Update delivery semantics documentation for EoS (KIP-98)","","closed","","hachikuji","2017-06-20T21:35:36Z","2017-06-21T00:47:24Z"
"","3387","MINOR: Update documentation to use `kafka-consumer-groups.sh` as the main tool for checking consumer offsets","","closed","","vahidhashemian","2017-06-20T21:01:36Z","2017-06-22T00:21:04Z"
"","3384","MINOR: remove unused hitRatio field in NamedCache","","closed","","dguy","2017-06-20T13:25:42Z","2017-06-22T14:16:25Z"
"","3378","MINOR: explain producer naming within Streams","","closed","","mjsax","2017-06-20T00:44:00Z","2017-06-22T15:34:36Z"
"","3376","MINOR: MemoryRecordsBuilder.sizeInBytes should consider initial position","","closed","","hachikuji","2017-06-19T22:14:58Z","2017-06-20T21:29:20Z"
"","3373","MINOR: Detail message/batch size implications for conversion between old and new formats","","closed","","hachikuji","2017-06-19T20:53:38Z","2017-06-21T21:05:39Z"
"","3369","KAFKA-5479: Improve the documentation around ACLs","","closed","","tombentley","2017-06-19T11:11:36Z","2017-06-20T11:18:23Z"
"","3365","MINOR: Unused logger removed.","","closed","","kamalcph","2017-06-17T05:30:49Z","2017-06-20T04:31:17Z"
"","3363","MINOR: Some small cleanups/improvements to KAFKA-5031","","closed","","hachikuji","2017-06-17T05:02:19Z","2017-06-17T11:50:05Z"
"","3361","KAFKA-5435: Improve producer state loading after failure","","closed","","hachikuji","2017-06-17T00:35:00Z","2017-06-17T17:21:09Z"
"","3360","KAFKA-5020: Update message format in implementation docs","","closed","","apurvam","2017-06-17T00:11:29Z","2017-06-30T16:24:17Z"
"","3356","KAFKA-5456: Ensure producer handles old format large compressed messages","","closed","","hachikuji","2017-06-16T06:23:46Z","2017-06-16T19:40:43Z"
"","3355","KAFKA-5457: MemoryRecordsBuilder.hasRoomFor should account for record headers","","closed","","apurvam","2017-06-16T00:53:45Z","2017-06-16T13:43:01Z"
"","3353","KAFKA-5455 - Better Javadocs for the transactional producer and consumer","","closed","","apurvam","2017-06-15T21:48:45Z","2017-06-17T22:59:35Z"
"","3351","MINOR: Add --help to DumpLogSegments","","closed","","tombentley","2017-06-15T13:31:30Z","2017-06-16T11:33:47Z"
"","3349","MINOR: Used Kafka native constants in KafkaLog4jAppender","","closed","","kamalcph","2017-06-15T10:47:14Z","2018-02-12T05:00:47Z"
"","3348","KAFKA-5449: Fix race condition on producer dequeuing of EndTxn request","","closed","","hachikuji","2017-06-15T03:11:23Z","2017-06-16T00:19:17Z"
"","3345","MINOR: Add Processing Guarantees in Streams docs","","closed","","guozhangwang","2017-06-15T00:17:16Z","2017-07-15T22:06:54Z"
"","3342","KAFKA-5448: Change TimestampConverter configuration name to avoid conflicting with reserved 'type' configuration used by all Transformations","","closed","","ewencp","2017-06-14T21:06:22Z","2017-06-14T23:05:33Z"
"","3340","MINOR: Add random aborts to system test transactional copier service","","closed","","hachikuji","2017-06-14T17:14:50Z","2017-06-14T23:21:38Z"
"","3339","KAFKA-5275: AdminClient API consistency","","closed","","ijuma","2017-06-14T15:37:40Z","2017-06-18T09:26:21Z"
"","3337","MINOR: Delegated the creation of new Thread to KafkaThread class.","","closed","","kamalcph","2017-06-14T12:00:00Z","2017-06-20T04:36:08Z"
"","3336","MINOR: Fix typo at StreamsConfig javadoc","","closed","","ilayaperumalg","2017-06-14T11:44:39Z","2018-01-26T19:22:50Z"
"","3334","MINOR: Fix ConsumerBounceTest to wait for consumer close","","closed","","rajinisivaram","2017-06-14T07:12:14Z","2017-06-29T01:44:24Z"
"","3333","MINOR: Mark AbstractLogCleanerIntegrationTest as an IntegrationTest","","closed","","ewencp","2017-06-14T06:28:32Z","2017-06-20T23:55:17Z"
"","3332","MINOR: Enable request and other debug loggers during tests, directed at a NullAppender","","open","","ewencp","2017-06-14T06:23:24Z","2019-12-12T06:22:21Z"
"","3331","KAFKA-5443: Consumer should use last offset from batch to set next fetch offset","","closed","","hachikuji","2017-06-14T04:21:58Z","2017-06-14T17:20:00Z"
"","3329","KAFKA-5442: Streams producer client.id are not unique for EOS","","closed","","mjsax","2017-06-13T23:06:18Z","2017-06-20T00:44:40Z"
"","3328","KAFKA-5559: Metrics should throw if two client registers with same ID","","closed","","mjsax","2017-06-13T22:55:43Z","2017-09-05T20:04:17Z"
"","3327","HOTFIX: Introduce max wait time for retry-and-backoff while creating tasks","","closed","","mjsax","2017-06-13T22:51:01Z","2017-06-14T16:43:15Z"
"","3325","KAFKA-5363: KIP-167  implementing bulk load, restoration event notification","","closed","","bbejeck","2017-06-13T20:41:09Z","2017-07-28T18:32:13Z"
"","3324","Temporarily remove the apiVersions API for 0.11","","closed","","cmccabe","2017-06-13T20:34:49Z","2019-05-20T19:01:41Z"
"","3321","HOTFIX: Handle Connector version returning 'null' during plugin loading.","","closed","","kkonstantine","2017-06-13T19:03:52Z","2017-06-13T21:41:24Z"
"","3319","MINOR: Verify mocks in all WorkerTest tests and don't unnecessarily mockStatic the Plugins class.","","closed","connect,","ewencp","2017-06-13T18:34:14Z","2020-10-16T06:08:15Z"
"","3318","HOTFIX: Fix invalid long format conversion in request logger message","","closed","","hachikuji","2017-06-13T17:23:50Z","2017-06-13T18:39:47Z"
"","3317","MINOR: Clean up in DeleteConsumerGroupTest even if test fails","","closed","","rajinisivaram","2017-06-13T16:58:34Z","2017-06-13T18:27:34Z"
"","3314","KAFKA-5439: Verify that no unexpected threads are left behind in tests","","closed","","rajinisivaram","2017-06-13T08:21:57Z","2017-06-30T18:44:17Z"
"","3312","HOFIX: Introduce max wait time for retry-and-backoff while creating tasks","","closed","","mjsax","2017-06-13T01:00:04Z","2017-06-14T16:43:29Z"
"","3311","MINOR: fix local Docker setup","","closed","","mjsax","2017-06-13T00:50:22Z","2017-07-06T01:04:06Z"
"","3310","KAFKA-5362: Add Streams EOS system test with repartitioning topic","","closed","","mjsax","2017-06-13T00:46:39Z","2017-06-26T18:23:01Z"
"","3306","KAFKA-5435: Ensure producer snapshot retained after truncation","","closed","","hachikuji","2017-06-12T18:06:36Z","2017-06-17T00:23:47Z"
"","3305","MINOR: Specify keyalg RSA for SSL key generation commands","","closed","","omkreddy","2017-06-12T14:42:12Z","2018-07-03T15:46:27Z"
"","3304","KAFKA-5433: Close SimpleAclAuthorizer in test to close ZK client","","closed","","rajinisivaram","2017-06-12T14:22:54Z","2017-06-12T15:51:23Z"
"","3303","KAFKA-5402: Avoid creating quota related metrics if quotas not enabled","","closed","","rajinisivaram","2017-06-12T13:15:24Z","2017-06-29T12:03:08Z"
"","3302","KAFKA-5409: Providing a custom client-id to the ConsoleProducer tool","","open","","ppatierno","2017-06-12T10:50:56Z","2018-06-06T06:51:31Z"
"","3300","KAFKA-5429: Ignore produce response if batch was previously aborted","","closed","","hachikuji","2017-06-12T01:12:16Z","2017-06-12T23:33:27Z"
"","3298","KAFKA-5428: Transactional producer should only abort batches in fatal error state","","closed","","hachikuji","2017-06-11T07:21:10Z","2017-06-13T01:10:34Z"
"","3297","KAFKA-5427: Transactional producer should allow FindCoordinator in error state","","closed","","hachikuji","2017-06-10T23:34:09Z","2017-06-12T22:07:02Z"
"","3294","KAFKA-4653: Improve test coverage of RocksDBStore","","closed","","jeyhunkarimov","2017-06-10T17:56:05Z","2017-06-23T10:42:13Z"
"","3293","KAFKA-4658: Improve test coverage InMemoryKeyValueLoggedStore","","closed","","jeyhunkarimov","2017-06-10T16:25:39Z","2017-06-22T14:06:09Z"
"","3292","KAFKA-4656: Improve test coverage of CompositeReadOnlyKeyValueStore","","closed","","jeyhunkarimov","2017-06-10T15:38:11Z","2017-06-23T10:34:04Z"
"","3291","KAFKA-4659: Improve test coverage of CachingKeyValueStore","","closed","","jeyhunkarimov","2017-06-10T13:24:26Z","2017-06-22T07:42:09Z"
"","3290","KAFKA-4655: Improve test coverage of CompositeReadOnlySessionStore","","closed","","jeyhunkarimov","2017-06-10T12:14:56Z","2017-06-22T14:02:26Z"
"","3289","MINOR: add Yahoo benchmark to nightly runs","","closed","","enothereska","2017-06-10T09:38:48Z","2017-06-21T19:38:38Z"
"","3288","KAFKA-4661: Improve test coverage UsePreviousTimeOnInvalidTimestamp","","closed","","jeyhunkarimov","2017-06-10T00:57:40Z","2017-06-10T21:55:54Z"
"","3285","KAFKA-5422: Handle multiple transitions to ABORTABLE_ERROR correctly","","closed","","apurvam","2017-06-09T21:40:23Z","2017-06-09T22:55:30Z"
"","3284","MINOR: A few cleanups of usage of Either in TC","","closed","","hachikuji","2017-06-09T18:33:05Z","2017-06-12T21:58:38Z"
"","3279","KAFKA-5412: Using connect-console-sink/source.properties raises an exception related to ""file"" property not found","","closed","connect,","ppatierno","2017-06-09T09:15:12Z","2020-10-16T06:29:09Z"
"","3278","MINOR: Add some logging for the transaction coordinator","","closed","","apurvam","2017-06-09T00:29:28Z","2017-06-20T18:33:22Z"
"","3273","HOTFIX: for flaky Streams EOS integration tests","","closed","","mjsax","2017-06-08T17:38:40Z","2017-06-08T20:17:25Z"
"","3272","MINOR: disable flaky Streams EOS integration tests","","closed","","mjsax","2017-06-08T17:02:27Z","2017-06-14T16:44:13Z"
"","3269","KAFKA-5410: Fix taskClass() method name in Connector and flush() signature in SinkTask","","closed","connect,","ppatierno","2017-06-08T11:52:57Z","2020-10-16T06:35:05Z"
"","3268","KAFKA-5410: Fix taskClass() method name in Connector and flush() signature in SinkTask","","closed","","ppatierno","2017-06-08T10:52:13Z","2017-06-08T12:10:11Z"
"","3267","MINOR: more details in error message descriptions","","closed","","reftel","2017-06-08T08:29:52Z","2018-02-14T02:36:39Z"
"","3264","MINOR: improve JavaDocs for TimestampExtractor interface","","closed","","mjsax","2017-06-07T23:17:37Z","2017-06-08T17:44:01Z"
"","3263","KAFKA-5404: Add more AdminClient checks to ClientCompatibilityTest","","closed","","cmccabe","2017-06-07T21:50:05Z","2019-05-20T19:02:09Z"
"","3262","MINOR: Cache metrics were missing","","closed","","enothereska","2017-06-07T20:40:21Z","2017-06-08T08:35:55Z"
"","3259","MINOR: A few logging improvements in group coordinator","","closed","","hachikuji","2017-06-07T16:35:07Z","2017-06-07T20:22:31Z"
"","3258","KAFKA-5394; Fix disconnections due to timeouts in AdminClient","","closed","","ijuma","2017-06-07T14:46:43Z","2017-06-18T09:26:43Z"
"","3257","KAFKA-5329: Fix order of replica list in metadata cache","","closed","","ijuma","2017-06-07T14:06:29Z","2017-06-18T09:26:38Z"
"","3251","MINOR: remove ignore annotations in Streams EOS integration test","","closed","","guozhangwang","2017-06-06T21:31:02Z","2017-07-15T22:07:01Z"
"","3248","KAFKA-5378: Return LSO in FetchResponse plus some metrics","","closed","","hachikuji","2017-06-06T17:42:02Z","2017-06-07T15:47:38Z"
"","3242","KAFKA-5357 follow-up: Yammer metrics, not Kafka Metrics","","closed","","guozhangwang","2017-06-06T05:45:12Z","2017-11-06T22:44:54Z"
"","3239","KAFKA-5376: Ensure aborted transactions are propagated in DelayedFetch","","closed","","hachikuji","2017-06-05T19:06:15Z","2017-06-06T02:38:59Z"
"","3232","MINOR: update docs with regard to KIP-123","","closed","","mjsax","2017-06-04T03:39:38Z","2017-06-05T07:06:32Z"
"","3231","KAFKA-5364: ensurePartitionAdded does not handle pending partitions in abortable error state","","closed","","hachikuji","2017-06-04T01:30:47Z","2017-06-06T19:28:00Z"
"","3230","KAFKA-5355: Test cases to ensure isolation level propagated in delayed fetch","","closed","","hachikuji","2017-06-03T22:45:55Z","2017-06-04T23:38:03Z"
"","3229","MINOR: (docs) add missing word","","closed","","mihbor","2017-06-03T09:48:31Z","2018-01-26T19:23:56Z"
"","3226","MINOR: Fix grammar","","closed","","mihbor","2017-06-03T08:19:32Z","2018-01-26T19:25:05Z"
"","3224","MINOR: mention IQ use-case in the Overview","","closed","","mihbor","2017-06-03T08:13:01Z","2018-01-26T19:25:19Z"
"","3221","KAFKA-5355: DelayedFetch should propagate isolation level to log read","","closed","","hachikuji","2017-06-03T02:27:22Z","2017-06-03T05:25:32Z"
"","3216","HOTFIX: Reinstate the placeholder for logPrefix in TransactionManager","","closed","","apurvam","2017-06-02T22:26:58Z","2017-06-02T22:30:30Z"
"","3215","MINOR: Adding deprecated KTable methods to docs from KIP-114","","closed","","bbejeck","2017-06-02T22:18:54Z","2017-06-05T18:28:46Z"
"","3214","MINOR: syntax brush for java / bash / json / text","","closed","","guozhangwang","2017-06-02T21:51:19Z","2017-07-15T22:07:00Z"
"","3212","KAFKA-5019: Upgrades notes for idempotent/transactional features and new message format","","closed","","hachikuji","2017-06-02T20:50:03Z","2017-06-03T07:04:22Z"
"","3210","KAFKA-5272: Policy for Alter Configs (KIP-133)","","closed","","ijuma","2017-06-02T14:56:53Z","2017-09-05T08:42:05Z"
"","3203","KAFKA-5365: Fix regression in compressed message iteration affecting magic v0 and v1","","closed","","hachikuji","2017-06-02T01:00:33Z","2017-06-02T08:45:32Z"
"","3201","KAFKA-5362: Add EOS system tests for Streams API","","closed","","mjsax","2017-06-01T23:33:20Z","2017-06-08T21:10:25Z"
"","3200","MINOR: Add upgrade notes for KAFKA-2358","","closed","","guozhangwang","2017-06-01T22:40:32Z","2017-07-15T22:07:03Z"
"","3197","MINOR: Set log level for org.reflections to ERROR.","","closed","","kkonstantine","2017-06-01T20:33:08Z","2017-06-01T22:01:23Z"
"","3195","KAFKA-5345: Close KafkaClient when streams client is closed","","closed","","rajinisivaram","2017-06-01T19:08:12Z","2017-06-02T13:19:11Z"
"","3194","MINOR: update Streams docs for KIP-123","","closed","","mjsax","2017-06-01T17:53:40Z","2017-06-01T23:58:53Z"
"","3193","KAFKA-5361: Add EOS integration tests for Streams API","","closed","","mjsax","2017-06-01T16:23:53Z","2017-06-01T23:17:00Z"
"","3190","KAFKA-4956: Verify client-side throttle time metrics in quota test","","closed","","rajinisivaram","2017-06-01T12:42:39Z","2017-06-01T14:47:23Z"
"","3187","KAFKA-5316: LogCleaner should account for larger record sets after cleaning (0.10.2)","","closed","","ijuma","2017-06-01T08:36:53Z","2017-06-18T09:30:26Z"
"","3186","KAFKA-4459: Run rat checks in Jenkins script","","closed","","ewencp","2017-06-01T07:57:09Z","2017-06-01T21:09:05Z"
"","3185","MINOR: Logging/debugging improvements for transactions","","closed","","hachikuji","2017-06-01T02:33:17Z","2017-06-02T02:33:24Z"
"","3183","KAFKA-5283: Handle producer epoch/sequence overflow by wrapping around","","closed","","hachikuji","2017-05-31T20:19:29Z","2017-06-02T07:02:01Z"
"","3175","KAFKA-5349: Fix illegal state error in consumer's ListOffset handler","","closed","","hachikuji","2017-05-31T07:07:31Z","2017-05-31T16:10:38Z"
"","3173","MINOR: Traverse plugin path recursively in connect","","closed","connect,","kkonstantine","2017-05-31T02:05:45Z","2020-10-16T06:08:15Z"
"","3172","KAFKA-5350: Modify unstable annotations in Streams API","","closed","","guozhangwang","2017-05-31T00:00:40Z","2017-11-06T22:44:35Z"
"","3171","MINOR: A few cleanups in KafkaApis and TransactionMarkerChannelManager","","closed","","hachikuji","2017-05-30T23:43:15Z","2017-06-17T01:07:43Z"
"","3169","MINOR: Fix doc for producer throttle time metrics","","closed","","rajinisivaram","2017-05-30T14:36:16Z","2017-05-30T23:36:38Z"
"","3168","KAFKA-4956: System test for request quotas","","closed","","rajinisivaram","2017-05-30T14:16:18Z","2017-06-01T14:48:07Z"
"","3165","KAFKA-5316: LogCleaner should account for larger record sets after cleaning","","closed","","hachikuji","2017-05-30T05:51:13Z","2017-06-01T16:18:10Z"
"","3163","KAFKA-5344: set message.timestamp.difference.max.ms back to Long.MaxValue","","closed","","becketqin","2017-05-29T05:56:17Z","2017-05-30T14:55:15Z"
"","3162","KAFKA-5340: Batch splitting should preserve magic and transactional flag","","closed","","hachikuji","2017-05-29T03:29:11Z","2017-06-01T04:35:16Z"
"","3161","KAFKA-5251: Producer should cancel unsent AddPartitions and Produce requests on abort","","closed","","hachikuji","2017-05-28T22:05:43Z","2017-05-31T04:23:26Z"
"","3160","KAFKA-5093: Avoid loading full batch data when possible when iterating FileRecords","","closed","","hachikuji","2017-05-27T19:11:26Z","2017-05-31T21:15:34Z"
"","3154","KAFKA-5333; Remove Broker ACL resource type","","closed","","ijuma","2017-05-26T13:03:05Z","2017-09-05T08:43:12Z"
"","3149","KAFKA-5281: System tests for transactions","","closed","","apurvam","2017-05-26T06:11:49Z","2017-06-01T17:28:07Z"
"","3147","MINOR: Remove unused method parameter in `SimpleAclAuthorizer`","","closed","","vahidhashemian","2017-05-25T21:22:18Z","2017-05-27T09:20:22Z"
"","3146","MINOR: Cleanup in tests to avoid threads being left behind","","closed","","rajinisivaram","2017-05-25T19:02:00Z","2017-05-27T09:19:04Z"
"","3145","MINOR: Add test case for duplicate check after log cleaning","","closed","","hachikuji","2017-05-25T18:50:08Z","2017-05-26T08:51:58Z"
"","3142","KAFKA-5316: LogCleaner should account for larger record sets after cleaning","","closed","","hachikuji","2017-05-24T23:47:32Z","2017-05-28T17:02:45Z"
"","3141","KAFKA-5324: AdminClient: add close with timeout, fix some timeout bugs","","closed","","cmccabe","2017-05-24T23:11:16Z","2019-05-20T19:08:12Z"
"","3138","KAFKA-5017: Record batch first offset remains accurate after compaction","","closed","","hachikuji","2017-05-24T18:40:52Z","2017-05-25T18:06:26Z"
"","3137","KAFKA-5320: Include all request throttling in client throttle metrics","","closed","","rajinisivaram","2017-05-24T11:26:57Z","2017-05-25T19:30:00Z"
"","3135","KAFKA-5314: exception handling and cleanup for state stores","","closed","","enothereska","2017-05-24T09:05:51Z","2017-06-08T06:30:11Z"
"","3134","KAFKA-5315: should not subtract for empty key in KTable.aggregate/reduce","","open","streams,","mjsax","2017-05-24T08:20:36Z","2018-03-02T19:30:11Z"
"","3133","MINOR: GroupCoordinator can append with group lock","","closed","","hachikuji","2017-05-24T07:35:57Z","2017-05-25T04:03:40Z"
"","3129","KAFKA-5282: Use a factory method to create producers/consumers and close them in tearDown","","closed","","vahidhashemian","2017-05-23T18:09:29Z","2017-06-02T09:27:44Z"
"","3123","KAFKA-4935: Deprecate client checksum API and compute lazy partial checksum for magic v2","","closed","","hachikuji","2017-05-23T07:22:11Z","2017-05-25T07:22:18Z"
"","3121","HOTFIX: Replace JDK download and fix missing argument in Vagrant provisioning script","","closed","","ewencp","2017-05-23T00:40:39Z","2017-05-23T01:55:31Z"
"","3120","Kafka 5265","","closed","","cmccabe","2017-05-23T00:34:44Z","2019-05-20T19:02:42Z"
"","3118","MINOR: Broker should disallow downconversion of transactional/idempotent records","","closed","","hachikuji","2017-05-22T23:05:29Z","2017-05-23T03:02:40Z"
"","3115","KAFKA-5303, KAFKA-5305: Improve logging when fetches fail in ReplicaFetcherThread","","closed","","ijuma","2017-05-22T13:33:10Z","2017-09-05T09:02:41Z"
"","3114","KAFKA-5211: Do not skip a corrupted record in consumer","","closed","","becketqin","2017-05-22T06:27:52Z","2017-05-31T06:00:14Z"
"","3113","KAFKA-5186: Avoid expensive log scan to build producer state when upgrading","","closed","","hachikuji","2017-05-22T00:52:55Z","2017-05-22T22:42:39Z"
"","3112","HOTFIX: In Connect test with auto topic creation disabled, ensure precreated topic is always used","","closed","","ewencp","2017-05-22T00:48:11Z","2017-05-22T02:13:27Z"
"","3107","MINOR: improve descriptions of Streams reset tool options","","closed","","mjsax","2017-05-19T21:54:36Z","2017-05-22T16:45:36Z"
"","3106","KAFKA-4785: Records from internal repartitioning topics should always use RecordMetadataTimestampExtractor","","closed","","jeyhunkarimov","2017-05-19T21:44:04Z","2017-06-22T11:25:16Z"
"","3104","MINOR: improve EmbeddedKafkaCluster test utility for deleting topics","","closed","","mjsax","2017-05-19T18:39:03Z","2017-05-26T17:04:27Z"
"","3095","MINOR: Bump Kafka version to 0.11.1.0-SNAPSHOT","","closed","","ijuma","2017-05-19T00:03:29Z","2017-06-18T09:27:28Z"
"","3093","HOTFIX: Close transactional producers in all new tests","","closed","","apurvam","2017-05-18T20:51:06Z","2017-05-18T22:22:32Z"
"","3091","KAFKA-5033: Set default retries for the idempotent producer to be infinite.","","closed","","apurvam","2017-05-18T18:44:02Z","2017-05-18T20:08:28Z"
"","3089","KAFKA-5268: Fix bounce test transient failure by clearing partitions before writing Complete state to transaction log","","closed","","hachikuji","2017-05-18T16:50:50Z","2017-05-18T18:18:54Z"
"","3084","TRIVIAL: Remove redundant asMap utility in ConsumerProtocol","","closed","","hachikuji","2017-05-18T06:05:17Z","2017-05-18T10:29:07Z"
"","3081","MINOR: Log transaction metadata state transitions plus a few cleanups","","closed","","hachikuji","2017-05-18T00:48:08Z","2017-05-23T16:58:20Z"
"","3076","KAFKA-3267: Describe and Alter Configs Admin APIs (KIP-133)","","closed","","ijuma","2017-05-16T23:54:22Z","2017-09-05T09:04:19Z"
"","3075","KAFKA-5259: TransactionalId auth implies ProducerId auth","","closed","","hachikuji","2017-05-16T23:54:12Z","2017-05-24T22:30:11Z"
"","3069","KAFKA-4957: Request rate quotas documentation","","closed","","rajinisivaram","2017-05-16T18:26:29Z","2017-05-16T23:37:37Z"
"","3066","KAFKA-5231: Bump up producer epoch when sending abort txn markers on InitPid","","closed","","guozhangwang","2017-05-16T06:29:16Z","2017-11-06T22:44:25Z"
"","3065","KAFKA-4714: TimestampConverter transformation (KIP-66)","","closed","connect,","ewencp","2017-05-16T05:40:09Z","2020-10-16T06:08:15Z"
"","3064","KAFKA-5252: Fix transient failures LogCleanerTest testCommitMarkerRemoval and testAbortMarkerRemoval","","closed","","hachikuji","2017-05-16T02:16:54Z","2017-05-16T05:57:30Z"
"","3063","MINOR: Print offset and size in sendFetches","","closed","","guozhangwang","2017-05-16T01:32:39Z","2017-07-15T22:08:09Z"
"","3060","KAFKA-5249: Fix incorrect producer snapshot offsets when recovering segments","","closed","","hachikuji","2017-05-15T19:20:31Z","2017-05-15T23:09:06Z"
"","3058","KAFKA-5248: Remove unused/unneeded retention time in TxnOffsetCommitRequest","","closed","","hachikuji","2017-05-15T18:13:50Z","2017-05-15T20:00:18Z"
"","3047","KAFKA-5228: Revisit Streams DSL JavaDocs","","closed","","jeyhunkarimov","2017-05-13T23:41:41Z","2018-01-26T18:49:18Z"
"","3045","MINOR: Handle nulls in NonEmptyListValidator","","closed","","ewencp","2017-05-13T20:12:13Z","2017-05-14T17:34:17Z"
"","3044","KAFKA-5230: Fix conversion of Class configs to handle nested classes properly","","closed","","ewencp","2017-05-13T19:49:45Z","2017-05-14T23:40:29Z"
"","3040","MINOR: Some minor improvements to TxnOffsetCommit handling","","closed","","hachikuji","2017-05-13T05:49:44Z","2017-05-16T20:26:22Z"
"","3039","MINOR: Close ZooKeeper clients in tests","","closed","","rajinisivaram","2017-05-13T02:22:43Z","2017-05-13T07:08:39Z"
"","3037","KAFKA-5227: Remove unsafe assertion, make jaas config safer","","closed","","rajinisivaram","2017-05-12T22:43:02Z","2017-05-13T00:42:16Z"
"","3036","KAFKA-3356: Remove ConsumerOffsetChecker","","closed","","mimaison","2017-05-12T17:47:18Z","2018-04-18T13:31:20Z"
"","3033","MINOR: Set min isr to avoid race condition in ReplicationBytesIn initialisation","","closed","","ijuma","2017-05-12T14:44:07Z","2017-06-18T09:33:24Z"
"","3030","MINOR: Improve shutdown sequence","","closed","","ijuma","2017-05-12T07:48:00Z","2017-06-18T09:27:58Z"
"","3029","MINOR: Fix code example reference to SchemaBuilder call in Connect's documentation","","closed","connect,","smferguson","2017-05-12T00:49:57Z","2020-03-29T01:34:08Z"
"","3028","KAFKA-3487: Support classloading isolation in Connect.","","closed","connect,","kkonstantine","2017-05-12T00:31:18Z","2020-10-16T06:08:15Z"
"","3020","[WIP] Test PR builder for Scala 2.10 runs if target is 0.10.2","","closed","","ijuma","2017-05-11T08:19:25Z","2017-09-05T08:44:49Z"
"","3019","[WIP] Test that Scala PR builder for 2.10 doesn't trigger","","closed","","ijuma","2017-05-11T08:16:26Z","2017-06-18T09:33:00Z"
"","3017","KAFKA-5218: New Short serializer, deserializer, serde","","closed","","mmolimar","2017-05-11T01:19:01Z","2017-05-31T22:12:08Z"
"","3015","KAFKA-5213; Mark a MemoryRecordsBuilder as full as soon as the append stream is closed","","closed","","apurvam","2017-05-10T22:36:57Z","2017-05-11T01:19:59Z"
"","3013","KAFKA-5215. Small JavaDoc fix for AdminClient#describeTopics","","closed","","cmccabe","2017-05-10T21:59:59Z","2019-05-20T18:41:29Z"
"","3012","KAFKA-5214: KafkaAdminClient#apiVersions should return a public class","","open","","cmccabe","2017-05-10T21:54:55Z","2018-10-02T07:52:57Z"
"","3008","KAFKA-5196: Make LogCleaner transaction-aware","","closed","","hachikuji","2017-05-10T06:38:03Z","2017-05-12T19:34:06Z"
"","3007","HOTFIX: Increase kafkatest startup wait time on ConnectDistributed service in 0.10.2","","closed","","kkonstantine","2017-05-09T21:18:47Z","2017-05-09T22:10:36Z"
"","3006","HOTFIX: Increase kafkatest startup wait time on ConnectDistributed service","","closed","","kkonstantine","2017-05-09T20:28:20Z","2017-05-09T23:30:53Z"
"","2997","MINOR: Rename InitPidRequest/InitPidResponse to InitProducerIdRequest/InitProducerIdResponse","","closed","","hachikuji","2017-05-08T17:46:29Z","2017-05-13T08:37:51Z"
"","2996","KAFKA-4222: Adjust timeout in low resource environments","","closed","","enothereska","2017-05-08T16:32:06Z","2017-05-09T16:35:07Z"
"","2994","KAFKA-5188: Integration tests for transactions","","closed","","apurvam","2017-05-08T11:23:04Z","2017-05-17T23:23:08Z"
"","2985","KAFKA-5173: Log jaas.config if broker fails to start","","closed","","rajinisivaram","2017-05-05T20:49:22Z","2017-05-11T21:11:18Z"
"","2983","KAFKA-5135: Controller Health Metrics (KIP-143)","","closed","","ijuma","2017-05-05T16:30:14Z","2017-09-05T08:42:58Z"
"","2980","KAFKA-5179: Log connection termination during authentication","","closed","","rajinisivaram","2017-05-05T12:56:26Z","2017-05-15T22:14:50Z"
"","2978","MINOR: Temporarily disable a few flaky tests","","closed","","ijuma","2017-05-04T23:36:01Z","2017-06-18T09:33:15Z"
"","2969","doc typo","","closed","","smferguson","2017-05-04T00:45:06Z","2018-01-26T19:42:03Z"
"","2968","KAFKA-5169: KafkaConsumer.close should be idempotent","","closed","","mjsax","2017-05-03T21:43:13Z","2017-05-05T16:47:34Z"
"","2966","KAFKA-4996: Fix findbugs multithreaded correctness warnings for streams","","closed","","amitdaga","2017-05-03T18:54:11Z","2017-05-05T05:46:32Z"
"","2961","MINOR: Serialize the real isolationLevel in FetchRequest","","closed","","apurvam","2017-05-02T22:13:00Z","2017-05-03T00:20:41Z"
"","2959","Created convenience method to create ZkUtils","","closed","","bbaugher","2017-05-02T20:32:31Z","2017-09-11T19:24:37Z"
"","2958","KAFKA-5162: Add a reference to AdminClient to docs/api.html","","closed","","cmccabe","2017-05-02T19:26:57Z","2019-05-20T18:41:56Z"
"","2956","KAFKA-4422: Drop support for Scala 2.10 (KIP-119)","","closed","","ijuma","2017-05-02T14:26:39Z","2017-06-18T09:33:05Z"
"","2954","MINOR: Fix error logged if not enough alive brokers for transactions state topic","","closed","","ijuma","2017-05-02T10:53:12Z","2017-09-05T08:44:55Z"
"","2951","KAFKA-5126: Implement KIP-98 transactional methods in the MockProducer","","closed","","mjsax","2017-05-01T23:28:31Z","2017-05-09T18:50:01Z"
"","2945","KAFKA-4923: Add Exactly-Once Semantics to Streams","","closed","","mjsax","2017-04-30T09:16:57Z","2017-05-17T00:41:07Z"
"","2944","KAFKA-3940: Replaced File.mkdir/mkdirs/delete by their Files equivalent","","closed","","mimaison","2017-04-29T17:57:52Z","2018-04-18T13:31:45Z"
"","2941","KAFKA-3266: Implement KIP-140 RPCs and APIs for creating, altering, and listing ACLs","","closed","","cmccabe","2017-04-29T00:04:52Z","2019-05-20T18:41:15Z"
"","2933","MINOR: Some cleanup in the transactional producer","","closed","","hachikuji","2017-04-28T06:13:17Z","2017-05-02T00:12:43Z"
"","2931","KAFKA-5063: Flaky ResetIntegrationTest","","closed","","mjsax","2017-04-28T04:03:34Z","2017-05-15T17:39:08Z"
"","2930","MINOR: Make assignment expectation explicit in testConsumptionWithBrokerFailures","","closed","","hachikuji","2017-04-28T01:50:39Z","2017-04-28T10:06:51Z"
"","2929","KAFKA-4763; Handle disk failure for JBOD (KIP-112)","","closed","","lindong28","2017-04-27T23:16:22Z","2017-07-22T19:45:05Z"
"","2928","HOTFIX [WIP]: Check on not owned partitions","","closed","","guozhangwang","2017-04-27T18:56:40Z","2017-07-15T22:08:12Z"
"","2923","MINOR: Fix a few transaction coordinator issues","","closed","","hachikuji","2017-04-27T06:23:45Z","2017-04-27T21:03:23Z"
"","2922","KAFKA-5108: Add support for reading PID snapshot files to DumpLogSegments","","closed","","hachikuji","2017-04-26T17:33:32Z","2017-04-26T20:47:36Z"
"","2910","KAFKA-5121: Implement transaction index for KIP-98","","closed","","hachikuji","2017-04-25T06:23:15Z","2017-05-06T18:52:10Z"
"","2909","MINOR: adding global store must ensure unique names","","closed","","mjsax","2017-04-25T05:58:08Z","2017-04-28T02:05:08Z"
"","2897","KAFKA-4564: follow up hotfix for system test","","closed","","mjsax","2017-04-22T19:13:09Z","2017-04-23T17:53:59Z"
"","2896","Minor: Make sure ProducerPerformance can bind with older client jar","","closed","","junrao","2017-04-22T16:46:52Z","2017-04-23T15:30:36Z"
"","2893","KAFKA-5112: Trunk compatibility tests should test against 0.10.2","","closed","","cmccabe","2017-04-21T23:35:10Z","2019-05-20T18:42:18Z"
"","2892","KAFKA-5110: Check for errors when fetching the log end offset in ConsumerGroupCommand","","closed","","hachikuji","2017-04-21T23:34:01Z","2018-11-21T22:31:24Z"
"","2891","kafka 3355","","closed","","mgarmes","2017-04-21T19:58:26Z","2018-06-14T00:48:51Z"
"","2890","KAFKA-5100: ProducerPerformanceService failing due to parsing error","","closed","","junrao","2017-04-21T16:02:25Z","2017-04-21T19:53:30Z"
"","2881","MINOR: Remove redundant CRC validation for non-compressed records in older message formats","","closed","","hachikuji","2017-04-21T04:03:34Z","2017-04-21T17:38:30Z"
"","2879","KAFKA-5094: Replace SCRAM credentials in broker logs with tag hidden","","closed","","rajinisivaram","2017-04-20T11:30:55Z","2017-04-21T09:48:42Z"
"","2877","KAFKA-5095: Adjust accepted overhead","","closed","","enothereska","2017-04-20T09:35:12Z","2017-04-20T21:57:37Z"
"","2876","KAFKA-5097: guard against unassigned partitions","","closed","","enothereska","2017-04-20T09:26:37Z","2017-04-20T21:47:32Z"
"","2875","KAFKA-4808 : Send of null key to a compacted topic should throw non-retriable error back to user","","open","","MayureshGharat","2017-04-19T20:26:41Z","2018-03-02T19:30:04Z"
"","2874","KAFKA-5090 Kafka Streams SessionStore.findSessions javadoc broken","","closed","","mihbor","2017-04-19T15:18:36Z","2017-04-21T20:26:02Z"
"","2870","MINOR: Fix open file leak in log cleaner integration tests","","closed","","hachikuji","2017-04-19T06:06:27Z","2017-04-20T12:23:10Z"
"","2866","MINOR: Improvements to PID snapshot management","","closed","","hachikuji","2017-04-18T02:54:35Z","2017-04-20T20:02:42Z"
"","2865","KAFKA-4850: RocksDB using Bloomfilters","","closed","","bharatviswa504","2017-04-18T00:31:20Z","2017-05-14T05:48:58Z"
"","2859","KAFKA-5075; Defer exception to the next pollOnce() if consumer's fetch position has already increased","","closed","","lindong28","2017-04-16T01:15:47Z","2017-04-17T19:57:13Z"
"","2858","KAFKA-438: Code cleanup in MessageTest","","closed","","jozanek","2017-04-15T21:46:20Z","2017-04-25T19:39:59Z"
"","2856","KAFKA-5073: Kafka Streams stuck rebalancing after exception thrown in rebalance listener","","closed","","mjsax","2017-04-14T20:43:53Z","2017-04-20T23:09:37Z"
"","2852","MINOR: Fix some re-raising of exceptions in system tests","","closed","","ewencp","2017-04-13T22:52:06Z","2017-04-19T15:18:51Z"
"","2851","MINOR: Clarify wording","","closed","","jeffwidman","2017-04-13T21:36:49Z","2017-07-20T09:19:28Z"
"","2849","KAFKA-5059: Implement Transactional Coordinator","","closed","","dguy","2017-04-13T07:26:41Z","2020-01-25T16:57:05Z"
"","2846","KAFKA-5059: Exactly once transactional coordinator","","closed","","dguy","2017-04-12T13:50:00Z","2020-01-25T16:57:01Z"
"","2845","KAFKA-4814: Enable ZK ACLs only when zookeeper.set.acl is set","","closed","","rajinisivaram","2017-04-12T12:47:45Z","2017-04-20T11:05:58Z"
"","2842","MINOR: findbugs should generate XML reports","","closed","","cmccabe","2017-04-11T23:54:22Z","2019-05-20T18:42:20Z"
"","2841","KAFKA-5038: Catch exception","","closed","","enothereska","2017-04-11T19:51:24Z","2017-04-12T14:29:29Z"
"","2840","KAFKA-4818: Exactly once transactional clients","","closed","","apurvam","2017-04-11T18:19:10Z","2017-04-27T22:02:36Z"
"","2838","KAFKA-5052 : Don't send undelying exception to RetriableCommitFailedException","","closed","","apurvam","2017-04-10T21:03:30Z","2017-04-11T00:22:03Z"
"","2837","KAFKA-4564: Add system test for pre-0.10 brokers","","closed","","mjsax","2017-04-10T21:00:26Z","2017-04-21T18:45:33Z"
"","2836","MINOR: improve test stability for Streams broker-compatibility test","","closed","","mjsax","2017-04-10T19:03:52Z","2017-04-21T05:37:58Z"
"","2835","KAFKA-5051: Avoid reverse DNS lookup to obtain hostname for TLS","","closed","","rajinisivaram","2017-04-10T18:51:23Z","2017-06-06T11:36:19Z"
"","2828","KAFKA-5011: down convert the message on replication if necessary","","closed","","becketqin","2017-04-08T00:05:17Z","2017-07-07T13:54:17Z"
"","2825","KAFKA-5043 : Add FindCoordinatorRPC stub and update InitPidRequest for transactions in KIP-98.","","closed","","apurvam","2017-04-07T22:37:17Z","2017-04-11T08:42:55Z"
"","2824","MINOR: Added changes in 0.10.2.1","","closed","","enothereska","2017-04-07T21:36:49Z","2017-04-12T00:28:08Z"
"","2823","KAFKA-5042: InFlightRequests#isEmpty() always returns false","","closed","","ijuma","2017-04-07T16:17:47Z","2017-09-05T09:51:44Z"
"","2821","MINOR: CollectionUtils.groupDataByTopic in OffsetsForLeaderEpochRequest/Response","","closed","","benstopford","2017-04-07T09:33:53Z","2017-04-07T11:25:02Z"
"","2819","KAFKA-5040: Increase number of retries from the default of 0","","closed","","enothereska","2017-04-07T00:49:20Z","2017-04-20T20:51:12Z"
"","2818","HOTFIX: Remove duplicate entry from ApiVersion","","closed","","ijuma","2017-04-06T22:19:12Z","2017-09-05T09:53:21Z"
"","2815","WIP DO NOT MERGE -- KAFKA-5037: Infinite loop if all input topics are unknown at startup","","closed","streams,","mjsax","2017-04-06T00:03:07Z","2018-07-20T18:04:08Z"
"","2813","KAFKA-5014: NetworkClient.leastLoadedNode should check if channel is ready","","closed","","ijuma","2017-04-05T14:14:43Z","2017-09-05T09:29:58Z"
"","2812","MINOR: Improve topic management instructions for Kafka Streams examples","","closed","","miguno","2017-04-05T13:11:44Z","2021-02-09T10:53:40Z"
"","2805","KAFKA-5013. Fail the build when findbugs fails","","closed","","cmccabe","2017-04-04T18:45:33Z","2019-05-20T18:42:31Z"
"","2799","Kafka-4990: Add API stubs, config parameters, and request types","","closed","","mjsax","2017-04-03T22:17:32Z","2017-04-07T16:08:09Z"
"","2798","KAFKA-4837: Fix class name comparison in connector-plugins REST endpoint","","closed","connect,","kkonstantine","2017-04-03T20:45:04Z","2020-10-16T06:08:14Z"
"","2797","KAFKA-4990: Add API stubs, config parameters, and request types","","closed","","mjsax","2017-04-03T20:36:44Z","2017-04-04T01:02:15Z"
"","2796","MINOR: Close the producer batch data stream when the batch gets full to free up compression buffers, etc.","","closed","","apurvam","2017-04-03T17:35:02Z","2017-04-03T23:15:40Z"
"","2795","MINOR: Add a release script that helps generate release candidates.","","closed","","ewencp","2017-04-03T16:50:12Z","2017-05-10T20:11:01Z"
"","2793","KAFKA-4916: test streams with brokers failing (for 0.10.2)","","closed","","enothereska","2017-04-03T10:53:44Z","2017-04-06T22:06:21Z"
"","2789","MINOR: reduce commit interval and cache size for integration test","","closed","","mjsax","2017-04-02T03:00:49Z","2017-04-21T05:40:11Z"
"","2782","MINOR: add jenkins.sh to fix PR branch builder","","closed","","mjsax","2017-03-31T22:34:02Z","2017-04-07T03:17:59Z"
"","2780","KAFKA-4995. Fix remaining findbugs warnings in Kafka Streams","","closed","","cmccabe","2017-03-31T21:28:17Z","2019-05-20T18:42:49Z"
"","2779","KAFKA-4993: Fix findbugs warnings in kafka-clients","","closed","","cmccabe","2017-03-31T19:51:59Z","2019-05-20T18:42:57Z"
"","2777","HOTFIX: WindowedStreamPartitioner does not provide topic name to serializer","","closed","","mjsax","2017-03-31T16:49:21Z","2017-04-06T00:18:52Z"
"","2776","HOTFIX: WindowedStreamPartitioner does not provide topic name to serializer","","closed","","mjsax","2017-03-31T16:18:14Z","2017-07-10T16:38:44Z"
"","2774","KAFKA-5003: StreamThread should catch InvalidTopicException","","closed","","mjsax","2017-03-31T05:55:35Z","2017-04-07T03:18:20Z"
"","2770","MINOR: Increase max.poll time for streams consumers","","closed","","enothereska","2017-03-30T16:39:43Z","2017-03-31T05:52:58Z"
"","2767","Vagrant provisioning fixes","","closed","","edenhill","2017-03-30T13:00:02Z","2017-03-30T21:42:24Z"
"","2764","MINOR: reduce amount of verbose printing","","closed","","enothereska","2017-03-30T09:34:02Z","2017-03-30T21:38:45Z"
"","2763","Kafka 4977","","closed","connect,","cmccabe","2017-03-29T21:18:24Z","2020-10-16T06:08:13Z"
"","2762","MINOR: Ensure streaming iterator is closed by Fetcher","","closed","","hachikuji","2017-03-29T20:02:08Z","2017-03-30T21:40:16Z"
"","2760","KAFKA-4973; Fix transient failure of AdminClientTest.testDeleteRecordsWithException","","closed","","lindong28","2017-03-29T17:03:28Z","2017-06-29T03:57:19Z"
"","2751","MINOR: Document ordering contract of iterator for window stores and session stores","","closed","","miguno","2017-03-28T09:11:27Z","2017-04-12T10:25:17Z"
"","2745","MINOR: Replication system tests should cover compressed path","","closed","","hachikuji","2017-03-27T21:54:03Z","2017-04-24T16:51:20Z"
"","2738","MINOR: Support streaming decompression of fetched records for new format","","closed","","hachikuji","2017-03-25T23:01:58Z","2017-03-29T01:20:56Z"
"","2736","HOTFIX: Fix unsafe dependence on class name in VerifiableClientJava","","closed","","hachikuji","2017-03-25T02:29:14Z","2017-03-25T03:59:15Z"
"","2733","KAFKA-4943: Make /config/users with SCRAM credentials not world-readable","","closed","","rajinisivaram","2017-03-24T17:59:25Z","2017-04-08T00:42:38Z"
"","2727","KAFKA-4944. Fix an ""unread field"" findbugs warning in streams examples","","closed","","cmccabe","2017-03-23T15:40:28Z","2019-05-20T18:39:26Z"
"","2726","MINOR: only log first exception in RecordCollectorImpl producer callback","","closed","","dguy","2017-03-23T10:40:39Z","2017-03-30T11:10:12Z"
"","2725","KAFKA-4919: Document that stores must not be closed when Processors are closed","","closed","","dguy","2017-03-22T13:47:33Z","2017-03-30T11:09:50Z"
"","2723","MINOR: MyProcessor doc example should implement, not extend `Processor`","","closed","","miguno","2017-03-22T10:00:26Z","2021-02-09T10:53:40Z"
"","2721","Minor: Adding example to SMT documentation","","closed","","gwenshap","2017-03-22T00:31:09Z","2017-03-23T06:06:19Z"
"","2715","KAFKA-4924: Fix Kafka Connect API findbugs warnings","","closed","","cmccabe","2017-03-20T20:42:45Z","2019-05-20T18:37:22Z"
"","2705","KAFKA-4907: message.timestamp.difference.max.ms should not be applied to log compacted topic if user did not set the property.","","closed","","becketqin","2017-03-19T06:12:33Z","2018-02-25T21:49:26Z"
"","2701","KAFKA-5112: Update compatibility system tests to include 0.10.2.1","","closed","","ijuma","2017-03-17T13:19:30Z","2017-06-18T09:26:57Z"
"","2700","MINOR: Precondition check corrected","","closed","","kamalcph","2017-03-17T12:54:55Z","2017-03-31T07:00:35Z"
"","2696","MINOR: Guard against NPE when throwing StreamsException on serializer mismatch","","closed","","miguno","2017-03-16T15:43:14Z","2017-03-22T09:59:23Z"
"","2693","KAFKA-4885: Add client.close as exception handler in streams system tests","","closed","","guozhangwang","2017-03-15T22:10:05Z","2017-07-15T22:07:49Z"
"","2692","KAFKA-4903: Shell#runCommand does not clear the input buffer","","closed","","cmccabe","2017-03-15T15:53:42Z","2019-05-20T18:40:50Z"
"","2691","KAFKA-4902: Utils#delete should correctly handle I/O errors and symlinks","","closed","","cmccabe","2017-03-15T15:40:11Z","2019-05-20T18:40:57Z"
"","2690","HOTFIX: Fix header in ByteArrayConverter","","closed","","ijuma","2017-03-15T13:44:35Z","2017-09-05T09:52:23Z"
"","2688","KAFKA-4863: Querying window store may return unwanted keys. Backport to 0.10.2","","closed","","dguy","2017-03-15T13:06:53Z","2017-05-16T14:05:21Z"
"","2687","KAFKA-4899: Fix findbugs warnings in kafka-core","","closed","","cmccabe","2017-03-14T23:23:19Z","2019-05-20T18:42:40Z"
"","2686","KAFKA-4898: Add timeouts to streams integration tests","","closed","","cmccabe","2017-03-14T21:28:12Z","2019-02-18T19:06:45Z"
"","2685","MINOR: Improve log4j on stream thread and stream process","","closed","","guozhangwang","2017-03-14T19:38:59Z","2017-07-15T22:08:03Z"
"","2683","KAFKA-4894. Fix findbugs ""default character set in use"" warnings","","closed","","cmccabe","2017-03-14T15:35:20Z","2019-05-20T18:37:02Z"
"","2682","KAFKA-4859: Set shorter commit interval for integration tests with caching","","closed","","guozhangwang","2017-03-14T05:46:30Z","2017-07-15T22:09:16Z"
"","2681","KAFKA-4890: Improve log4j for debugging [WIP]","","closed","","guozhangwang","2017-03-14T01:21:43Z","2017-07-15T22:07:51Z"
"","2674","MINOR: Fix a documentation typo","","closed","","vahidhashemian","2017-03-10T23:19:59Z","2017-03-15T18:51:44Z"
"","2670","MINOR: updated incorrect JavaDocs for joins","","closed","","mjsax","2017-03-10T18:14:39Z","2017-03-22T17:13:40Z"
"","2668","MINOR: KStream: fix typo in javadoc","","closed","","miguno","2017-03-10T13:04:49Z","2021-02-09T10:53:39Z"
"","2667","MINOR: Used in-built TimeUnit conversion function","","closed","","kamalcph","2017-03-10T12:40:56Z","2017-03-14T12:48:34Z"
"","2666","MINOR: Used in-built TimeUnit conversion function","","closed","","kamalcph","2017-03-10T12:34:27Z","2017-03-10T12:38:48Z"
"","2663","MINOR: increase RocksDb parallelism","","closed","","enothereska","2017-03-09T19:36:07Z","2017-03-11T11:40:21Z"
"","2660","MINOR: Make ConfigDef safer by not using empty string for NO_DEFAULT_VALUE.","","closed","","ewencp","2017-03-09T00:39:49Z","2017-04-05T10:38:28Z"
"","2659","KAFKA-4840 : BufferPool errors can cause buffer pool to go into a bad state","","closed","","smccauliff","2017-03-08T22:51:33Z","2017-04-27T17:14:12Z"
"","2656","MINOR: Use ConcurrentMap for ConsumerNetworkClient UnsentRequests","","closed","","hachikuji","2017-03-08T18:44:01Z","2017-03-08T23:17:43Z"
"","2655","KAFKA-4864 added correct zookeeper nodes for security migrator","","closed","","simplesteph","2017-03-08T06:21:17Z","2017-03-09T05:42:22Z"
"","2651","MINOR: reduce() javadocs: clarify position of arguments","","closed","","miguno","2017-03-07T12:46:03Z","2021-02-09T10:53:40Z"
"","2650","KAFKA-4881: add internal leave.group.on.close config to consumer","","closed","","dguy","2017-03-07T11:26:57Z","2017-03-30T11:10:31Z"
"","2647","MINOR: Add varint serde utilities for new message format","","closed","","hachikuji","2017-03-06T22:38:57Z","2017-03-08T23:02:25Z"
"","2646","MINOR: Rename RecordBatch to ProducerBatch to allow reuse in KIP-98","","closed","","hachikuji","2017-03-06T20:53:10Z","2017-03-07T03:57:53Z"
"","2644","Hotfix: broken link","","closed","","mjsax","2017-03-06T17:31:37Z","2017-03-06T22:00:42Z"
"","2641","KAFKA-4841; NetworkClient should only consider a connection to have failed after attempt to connect","","closed","","lindong28","2017-03-05T10:19:20Z","2017-03-22T20:19:27Z"
"","2638","KAFKA-3995: fix compression ratio estimation.","","closed","","becketqin","2017-03-04T02:33:40Z","2017-05-24T16:51:54Z"
"","2636","MINOR: Follow up to KAFKA-2857","","closed","","vahidhashemian","2017-03-03T23:46:54Z","2017-03-04T02:47:27Z"
"","2626","MINOR: Fix typo in GlobalKTable javadocs","","closed","","miguno","2017-03-02T08:20:22Z","2021-02-09T10:53:39Z"
"","2624","KAFKA-4743: Add Reset Consumer Group Offsets tooling [KIP-122]","","closed","","jeqo","2017-03-01T19:16:02Z","2017-05-18T13:39:30Z"
"","2623","KAFKA-4826. Fix some findbugs warnings in Kafka Streams","","closed","","cmccabe","2017-03-01T18:14:52Z","2019-05-20T18:36:28Z"
"","2620","MINOR: Doc change related to ZK sasl configs","","closed","","omkreddy","2017-03-01T07:21:34Z","2018-07-03T15:46:25Z"
"","2619","KAFKA-4820; ConsumerNetworkClient.send() should not require global lock","","closed","","lindong28","2017-03-01T04:15:54Z","2017-03-06T13:23:27Z"
"","2614","KAFKA-4816: Message format changes for idempotent/transactional producer","","closed","","hachikuji","2017-02-28T23:45:08Z","2020-01-25T16:56:42Z"
"","2611","MINOR: improve MinTimestampTrackerTest and fix NPE when null element removed","","closed","","dguy","2017-02-28T17:55:41Z","2017-03-30T11:10:44Z"
"","2607","MINOR: Fix typo in javadoc of `flatMapValues`","","closed","","miguno","2017-02-28T11:48:58Z","2021-02-09T10:53:15Z"
"","2601","KAFKA-4744: Increased timeout for bounce test","","closed","","enothereska","2017-02-26T19:32:22Z","2017-02-27T22:35:17Z"
"","2599","KAFKA-4783: Add ByteArrayConverter (KIP-128)","","closed","","ewencp","2017-02-25T22:21:09Z","2017-07-06T16:44:17Z"
"","2598","KAFKA-4784: Add ByteArrayConverter (KIP-128)","","closed","","ewencp","2017-02-25T22:19:04Z","2017-02-25T23:31:56Z"
"","2597","KAFKA-4652: Added tests for KStreamBuilder","","closed","","bbejeck","2017-02-25T18:04:17Z","2017-03-02T21:19:54Z"
"","2596","MINOR: Ensure consumer calls poll() if requests are outstanding","","closed","","hachikuji","2017-02-24T22:45:34Z","2017-02-25T00:09:49Z"
"","2595","MINOR: Fix typos in javadoc and code comments","","closed","","vahidhashemian","2017-02-24T16:57:53Z","2017-03-30T12:53:11Z"
"","2593","KAFKA-4796: Fix some findbugs warnings in Kafka Java client","","closed","","cmccabe","2017-02-24T01:00:18Z","2019-05-20T18:36:40Z"
"","2592","MINOR: fixed javadoc typo in KafkaProducer::partitionsFor","","closed","","jpdaigle","2017-02-23T19:51:31Z","2017-02-24T15:32:23Z"
"","2591","HOTFIX: use explicit version in upgrade.html","","closed","","guozhangwang","2017-02-23T19:07:43Z","2017-07-15T22:07:09Z"
"","2586","KAFKA-4786: Wait for heartbeat thread to terminate in consumer close","","closed","","rajinisivaram","2017-02-22T16:15:03Z","2017-02-22T20:42:37Z"
"","2585","MINOR: Fix potential integer overflow and String.format issue","","closed","","ijuma","2017-02-22T13:29:46Z","2017-09-05T09:29:05Z"
"","2579","MINOR: Make it impossible to invoke `Request.body` without an explicit type parameter","","closed","","ijuma","2017-02-21T13:43:11Z","2017-09-05T09:32:24Z"
"","2575","MINOR: update AWS test setup guide","","closed","","mjsax","2017-02-20T01:34:16Z","2017-06-23T00:58:05Z"
"","2572","KAFKA-4776: Implement graceful handling for improperly formed compressed message sets","","closed","","hachikuji","2017-02-18T22:46:09Z","2017-02-21T15:07:40Z"
"","2566","HOTFIX: ClassCastException in request logging","","closed","","hachikuji","2017-02-18T00:06:30Z","2017-02-18T04:54:52Z"
"","2565","MINOR: Use scale of 3 in streams benchmark by default [DO NOT MERGE]","","closed","","guozhangwang","2017-02-17T22:18:11Z","2017-07-15T22:07:14Z"
"","2564","KAFKA-4777 fix client heartbeat non-stop retry issue.","","closed","","allenxiang","2017-02-17T20:05:43Z","2017-02-18T17:43:14Z"
"","2563","Kafka 4757: NetworkClient should log request details at trace level when a request is cancelled because of disconnection","","closed","","cmccabe","2017-02-17T19:38:34Z","2019-05-20T18:35:25Z"
"","2562","MINOR: Update docstring for ""offsets.retention.minutes"" config","","closed","","omkreddy","2017-02-17T06:05:36Z","2018-07-03T15:46:24Z"
"","2561","MINOR: Fix NPE handling unknown APIs in NodeApiVersions.toString","","closed","","hachikuji","2017-02-17T00:38:05Z","2017-02-21T13:53:12Z"
"","2559","KAFKA-4775. Fix findbugs warnings in kafka-tools","","closed","","cmccabe","2017-02-16T21:45:42Z","2019-05-20T18:34:42Z"
"","2557","KAFKA-4773: The Kafka build should run findbugs","","closed","","cmccabe","2017-02-16T19:50:46Z","2019-05-20T18:36:19Z"
"","2556","KAFKA-4771 - NPEs in KerberosLogin due to autoboxing","","open","","coheigea","2017-02-16T16:39:44Z","2018-03-02T19:29:56Z"
"","2555","MINOR: Javadoc typo","","closed","","astubbs","2017-02-16T12:00:51Z","2017-10-18T12:02:57Z"
"","2554","KAFKA-4769: Add Float serializer, deserializer, serde","","closed","","miguno","2017-02-16T05:54:29Z","2017-03-13T18:50:44Z"
"","2553","MINOR: fix indention in  tags","","closed","","mjsax","2017-02-16T00:51:50Z","2017-02-16T18:58:57Z"
"","2551","KAFKA-4752: Fixed bandwidth calculation","","closed","","enothereska","2017-02-15T13:45:26Z","2017-02-21T17:51:57Z"
"","2547","MINOR: add session windows doc to streams.html","","closed","","dguy","2017-02-14T16:20:53Z","2017-02-17T00:57:41Z"
"","2545","KAFKA-4761: Fix producer regression handling small or zero batch size","","closed","","hachikuji","2017-02-13T19:17:15Z","2017-02-14T17:43:37Z"
"","2544","KFKA-4340: follow up patch for KAFAK-4340","","closed","","becketqin","2017-02-13T10:13:02Z","2017-02-17T10:43:57Z"
"","2543","MINOR: Remove unused MessageWriter and CompressionFactory","","closed","","hachikuji","2017-02-13T02:02:26Z","2017-02-21T14:03:06Z"
"","2542","MINOR: Stream metrics documentation","","closed","","enothereska","2017-02-12T10:46:26Z","2017-02-16T10:55:23Z"
"","2541","KAFKA-4759: Add support for subnet masks in SimpleACLAuthorizer","","open","","takebayashi","2017-02-12T01:25:58Z","2020-10-06T21:06:34Z"
"","2536","MINOR: Move compression stream construction into CompressionType","","closed","","hachikuji","2017-02-10T18:46:08Z","2017-02-16T23:30:20Z"
"","2535","MINOR: don't throw CommitFailedException during suspendTasksAndState","","closed","","dguy","2017-02-10T17:55:58Z","2017-02-17T00:57:43Z"
"","2530","MINOR: Replacing for with foreach loop in common module","","closed","","PKOfficial","2017-02-10T08:10:58Z","2017-02-18T14:37:17Z"
"","2529","KAFKA-4754: Correctly parse '=' characters in command line overrides","","closed","","granthenke","2017-02-10T03:17:32Z","2018-10-18T15:03:50Z"
"","2527","MINOR: update README with how to run code coverage on module. add reportCoverage at subProject level","","closed","","dguy","2017-02-09T15:48:18Z","2017-03-30T11:10:52Z"
"","2526","KAFKA-4716: Fix case when controller cannot be reached","","closed","","enothereska","2017-02-09T14:48:21Z","2017-02-10T07:45:27Z"
"","2523","MINOR: fix integer overflow in simple benchmark MB/sec calculation","","closed","","dguy","2017-02-09T11:31:19Z","2017-02-17T00:58:03Z"
"","2519","MINOR: changes to the production broker configuration docs.","","closed","","alexlod","2017-02-09T00:56:58Z","2017-02-09T02:16:28Z"
"","2517","HOTFIX: renamed test so it is picked up by ducktape","","closed","","enothereska","2017-02-08T15:55:12Z","2017-02-08T21:21:26Z"
"","2510","0.9.0","","closed","","rajatraj07","2017-02-07T05:09:53Z","2018-01-26T19:11:39Z"
"","2509","KAFKA-4741 Fix for buffer leaks in RecordAccumulator","","closed","","satishd","2017-02-07T04:58:44Z","2017-02-08T13:39:06Z"
"","2508","MINOR: Fix import for streams broker compatibility test to use new DEV_BRANCH constant","","closed","","ewencp","2017-02-06T21:38:01Z","2017-02-06T23:20:21Z"
"","2501","KAFKA-4734; Trim the time index on old segments","","closed","","becketqin","2017-02-04T06:39:52Z","2017-02-07T16:43:50Z"
"","2500","KAFKA-4654 improve test coverage for MemoryLRUCacheStore","","closed","","bbejeck","2017-02-04T02:44:18Z","2017-02-10T01:21:09Z"
"","2498","MINOR: Make Kafka Connect step in quickstart more Windows friendly","","closed","","vahidhashemian","2017-02-03T23:52:23Z","2017-07-21T05:07:51Z"
"","2497","[MINOR] fixed some JavaDoc typos","","closed","","mjsax","2017-02-03T23:44:59Z","2017-02-07T18:35:29Z"
"","2494","HOTFIX: Verifiable producer request timeout needs conversion to milliseconds","","closed","","hachikuji","2017-02-03T21:37:49Z","2017-02-03T22:47:05Z"
"","2492","HOTFIX: fixed section incompatible Steams API changes","","closed","","mjsax","2017-02-03T18:18:11Z","2017-02-16T18:58:58Z"
"","2491","[KAFKA-4728] KafkaConsumer#commitSync should clone its input","","closed","","je-ik","2017-02-03T09:54:18Z","2017-02-06T19:48:41Z"
"","2484","KAFKA-3959: Follow-up; move upgrade notes to 0.10.3 upgrade section.","","closed","","ewencp","2017-02-02T06:30:10Z","2018-02-21T15:45:35Z"
"","2483","MINOR: Ensure timestamp type is provided when up-converting messages","","closed","","hachikuji","2017-02-02T04:41:58Z","2017-02-02T23:11:05Z"
"","2480","MINOR: Update docs for version 0.10.2.0","","closed","","ewencp","2017-02-01T18:18:26Z","2017-02-01T19:05:10Z"