"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","7302","KAFKA-8878: Fix flaky test AssignedStreamsTasksTest#shouldCloseCleanl…","…yWithSuspendedTaskAndEOS  The previous approach to testing KAFKA-8412 was to look at the logs and determine if an error occurred during close. There was no direct way to detect than an exception occurred because the exception was eaten in `AssignedTasks.close`. In the PR for that ticket (#7207) it was acknowledged that this was a brittle way to test for the exception. We now see occasional failures because an unrelated ERROR level log entry is made while closing the task.  This change eliminates the brittle log checking by rethrowing any time an exception occurs in close, even when a subsequent unclean close succeeds. This has the potential benefit of uncovering other supressed exceptions down the road.  I've verified that even with us rethrowing on `closeUnclean` that all tests pass.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cpettitt-confluent","2019-09-05T20:25:00Z","2019-09-10T20:53:06Z"
"","7022","KAFKA-8614: Rename the `responses` field of IncrementalAlterConfigsRe…","…sponse to match AlterConfigs  This patch changes the name of the `responses` field of IncrementalAlterConfigsResponse to `Resources`. This makes it consistent with AlterConfigsResponse, which has a differently-named but structurally-identical field.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-07-01T22:49:51Z","2019-07-13T00:23:59Z"
"","7300","KAFKA-8876:KafkaBasedLog does not throw exception when some partition…","…s of the topic is offline  https://issues.apache.org/jira/browse/KAFKA-8876  When starting up, KafkaBasedLog should throw ConnectException if any of the subscribed partitions has no leader.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","huxihx","2019-09-05T09:48:04Z","2020-03-22T06:18:16Z"
"","7164","KAFKA-8736: Streams performance improvement, use isEmpty() rather tha…","…n size() == 0  According to https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ConcurrentSkipListMap.html#size--, the size method has to traverse all elements to get a count. It looks like the count is being compared against 0 to determine if the map is empty; In this case, we don't need a full count. Instead, the isEmpty() method should be used, which just looks for one node.  No expected changes to unit or integration tests.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjarvie","2019-08-05T18:41:16Z","2019-08-06T18:47:34Z"
"","7336","KAFKA-8107:Flaky Test kafka.api.ClientIdQuotaTest.testQuotaOverrideDe…","…lete  https://issues.apache.org/jira/browse/KAFKA-8107  `removeQuotaOverrides` should await some time to ensure entity cnofig takes effect in ZooKeeper.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-09-16T12:15:49Z","2020-10-24T05:56:25Z"
"","7207","KAFKA-8412: Still a nullpointer exception thrown on shutdown whi…","…le flushing before closing producers  Prior to this change an NPE is raised when calling AssignedTasks.close under the following conditions:  1. EOS is enabled 2. The task was in a suspended state  The cause for the NPE is that when a clean close is requested for a StreamTask the StreamTask tries to commit. However, in the suspended state there is no producer so ultimately an NPE is thrown for the contained RecordCollector in flush.  It is my opinion that in the long term, this (and probably other surprising state interactions) could be cleaned up by consolidating state into one place instead of spreading it across AssignedTasks, StreamTask, and AbstractTask. However, that is a much larger, more risky change, and this issue is currently considered minor.  The fix put forth in this commit is to have AssignedTasks call closeSuspended when it knows the underlying StreamTask is suspended.  Currently the only externally visible way to detect this problem in test seems to be via logging. This is because the NPE is logged but then suppressed under the following sequence:  RecordCollectorImpl.flush:266     - throws NPE (producer is null)  StreamTask.suspend:578     - goes through the finally block and then reraises the NPE  StreamTask.close:706     - catches the NPE, calls closeSuspended with the NPE  StreamTask.closeSuspended:676     - rethrows the NPE after some cleanup  AssignedTasks.close:341     - catches and logs the exception     - tries a ""dirty"" close (clean = true) which succeeds     - firstException is NOT set because the test `!closeUnclean(task)`       does not hold.  It seems this is not the intended behavior? If so, I will happily correct that and stop using logging as a way to detect failure.  Otherwise this commit does not currently pass checkstyle because I'm using blacklisted imports: `LogCaptureAppender` and its various dependencies from `log4j`. I would appreciate guidance as to whether we should whitelist these or use another technique for detection.  Note also that this test is quite involved. I could have just tested that AssignedTasks calls closeSuspended when appropriate, but that is testing, IMO, a detail of the implementation and doesn't actually verify we reproduced the original problem as it was described. I feel much more confident that we are reproducing the behavior - and we can test exactly the conditions that lead to it - when testing across AssignedTasks and StreamTask. I believe this is an additional support for the argument of eventually consolidating the state split across classes.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cpettitt-confluent","2019-08-13T18:55:50Z","2019-08-26T20:38:52Z"
"","7466","KAFKA-8928: Logged producer config does not always match actual confi…","…g values  https://issues.apache.org/jira/browse/KAFKA-8928  Some logged producer configs(clientId, acks, retries) might not be reflected the actual values.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","huxihx","2019-10-08T10:28:38Z","2020-01-01T18:49:21Z"
"","7176","KAFKA-8325: Remove batch from in-flight requests when handling MESSAG…","…E_TOO_LARGE error  This patch fixes a bug in the handling of MESSAGE_TOO_LARGE errors. The large batch is split, the smaller batches are re-added to the accumulator, and the batch is deallocated, but it was not removed from the list of in-flight batches. When the batch was eventually expired from the in-flight batches, the producer would try to deallocate it a second time, causing an error. This patch changes the behavior to correctly remove the batch from the list of in-flight requests.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-08-07T20:35:15Z","2019-08-22T06:29:21Z"
"","7298","KAFKA-8875:CreateTopic API should check topic existence before replic…","…ation factor  https://issues.apache.org/jira/browse/KAFKA-8875  If the topic already exists, `handleCreateTopicsRequest` should return TopicExistsException even given an invalid config (replication factor for instance).  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-09-05T06:17:37Z","2019-09-11T21:24:28Z"
"","7115","KAFKA-8710: Update InitProducerId to allow transactional producers to…","… bump epoch and continue running after errors  This patch implements the broker-side changes for KIP-360. It adds two new fields to InitProducerId, lastEpoch and producerId. Passing these values allows the TransactionCoordinator to safely bump a producer's epoch after some failures (such as UNKNOWN_PRODUCER_ID and INVALID_PRODUCER_ID_MAPPING). When a producer calls InitProducerId after a failure, the coordinator first checks the producer ID from the request to make sure no other producer has been started using the same transactional ID. If it is safe to continue, the coordinator checks the epoch from the request; if it matches the existing epoch, the epoch is bumped and the producer can safely continue. If it matches the previous epoch, the the current epoch is returned without bumping. Otherwise, the producer is fenced.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","core,","bob-barrett","2019-07-25T18:18:46Z","2020-01-02T04:32:28Z"
"","6802","KAFKA-8634: Update ZooKeeper to 3.5.5","ZooKeeper 3.5.5 is the first stable release in the 3.5.x series. The key new feature in is TLS support, but there are a few more noteworthy features:  * Dynamic reconfiguration * Local sessions * New node types: Container, TTL * Ability to remove watchers * Multi-threaded commit processor * Upgraded to Netty 4.1  See the release notes for more detail: https://zookeeper.apache.org/doc/r3.5.5/releasenotes.html  In addition to the version bump, we:  * Add `commons-cli` dependency as it's required by `ZooKeeperMain`, but specified as `provided` in their pom. * Remove unnecessary `ZooKeeperMainWrapper`, the bug it worked around was fixed upstream a long time ago. * Ignore non zero exit in one system test invocation of `ZooKeeperMain`. `ZooKeeperMainWrapper` always returned `0` and `ZooKeeperService.query` relies on that for correct behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-23T22:00:04Z","2019-07-10T16:45:16Z"
"","7036","MINOR: Remove zkclient dependency","ZkUtils was removed so we don't need this anymore.  Also: * Fix ZkSecurityMigrator and ReplicaManagerTest not to reference ZkClient classes. * Remove references to zkclient in various `log4j.properties` and `import-control.xml`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-07-04T19:23:19Z","2019-07-05T14:50:36Z"
"","6948","KAFKA-8545: Remove legacy ZkUtils","ZkUtils is not used by the broker, has been deprecated since 2.0.0 and it was never intended as a public API. We should remove it along with `AdminUtils` methods that rely on it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-16T08:30:26Z","2019-06-23T03:44:44Z"
"","6675","KAFKA-8320 : fix retriable exception package for source connectors","WorkerSourceTask is catching the exception from wrong package `org.apache.kafka.common.errors`.  It is not clear from the API standpoint as to which package the connect framework supports - the one from common or connect. The safest thing would be to support both the packages even though it's less desirable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-05-03T22:22:38Z","2020-10-16T06:19:07Z"
"","7119","MINOR: Remove unused TopicAndPartition and remove unused symbols","With the removal of `ZkUtils` and `AdminUtils`, `TopicAndPartition` is finally unused.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-07-26T13:11:45Z","2019-07-28T16:38:30Z"
"","6488","MINOR: Remove line for testing repartition topic name","With KIP-307 `joined.name()` is deprecated plus we don't need to test for repartition topic names.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-22T14:29:54Z","2019-03-24T16:48:03Z"
"","6764","MINOR: fix Streams version-probing system test","With KAFKA-8284, the `AbstractCoordinator` does not allow to suppress sending `LeaveGroupRequest` on `Consumer#close()` any longer. Hence, we get a different rebalance pattern.  This implies, that the last rebalance, does not do any version probing any longer. When the last instance is bounced, an immediate rebalance ""stabilized the group"" and prepares it to upgrade to the new metadata version, because all members are on old version but support new version already. On restart of the last instance, we immediately switch to the new version instead of doing a version probing.","closed","tests,","mjsax","2019-05-19T18:06:24Z","2019-05-24T15:10:23Z"
"","6850","KAFKA-8449: Restart tasks on reconfiguration under incremental cooperative rebalancing","With incremental cooperative rebalancing tasks are not restarted unless they are reassigned to different worker. Therefore, to load config updates due to connector reconfiguration the currently running tasks that get new configs need to be recorded and considered for restart when connectors and tasks are started after a rebalance.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-05-31T07:09:34Z","2020-10-16T05:50:55Z"
"","6810","MINOR: Reset numIterations to 1 on txn timeouts","With EOS enabled, in the `runOnce` loop the `numIterations` can get incremented to a size that causes the transaction to timeout if the record processing is slow.  When the producer is fenced due to its epoch being no longer valid, we should reset `numIterations` back to 1, otherwise, we see a repeated behavior of transaction time outs as `numIterations` stays at the value where the timeout occurred in the first place   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-05-24T17:59:01Z","2019-05-28T13:50:29Z"
"","7105","KAFKA-7245 (Deprecate WindowStore#put(key, value)) :- The method in t…","WindowStore#put(key, value) has no timestamp as a parameter, so it causes inconsistency to identify to which window does the key belong.  The stream module of Kafka has a window state store that stores the aggregated values for a key in a given time frame. The window store is implemented as an interface, this interface has a strange method named put(key, value), this method has does not have a timestamp as a parameter which is important to determine that to which window frame does the key belongs. In this method, the current record timestamp is used for determining the window frame(as specified in the description of the method), this constraint makes WindowStore error-prone. It is also specified in the method description that method with a timestamp parameter should be used which already present in the interface which expects key, value, and start timestamp as well of the window to which the key belongs. Therefore by deprecating (and finally removing) the method put(key, value), we can prevent inconsistency.  - As the method is deprecated, the test classes which used them are needed to be changed when later the method is removed. Following is the list of the classes: 1) WindowStoreFacadeTest.java 2) WindowBytesStoreTest.java 3) SimpleBenchmark.java (benchmarking) 4) RocksDBWindowStoreTest.java 5) ProcessorContextImplTest.java 6) MeteredWindowStoreTest.java 7) InMemoryWindowStoreTest.java 8) ChangeLoggingWindowBytesStoreTest.java 9) CachingWindowStoreTest.java 10) ChangeLoggingTimestampedWindowBytesStoreTest.java   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","omanges","2019-07-22T17:44:53Z","2020-06-12T23:40:04Z"
"","6657","[HOT FIX] in-memory store behavior should match rocksDB","While working on consolidating the various store unit tests I uncovered some minor ""bugs"" in the in-memory stores (inconsistencies with the behavior as established by the RocksDB stores).  - open iterators should be properly closed in the case the store is closed - fetch/findSessions should *always* throw NPE if key is null - window end time should be truncated at Long.MAX_VALUE rather than throw exception  (Verified in-memory stores pass all applicable rocksDB tests now, unified unit tests coming in another PR)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-04-30T22:08:15Z","2019-05-06T20:36:31Z"
"","7348","KAFKA-8931: just restart current reconfigurated connectorTask, not all cached updateTasks","While testing Kafka Connect KIP-415, it don't work well under new connector commited, which will restart all tasks before the one current commited.  Filter `updatedTasks` with current reconfigurated `connectorName` ensure just restart current reconfigurated connectorTask, not all cached `updateTasks` ;","open","connect,","HeryLong","2019-09-17T00:58:42Z","2020-03-22T04:49:34Z"
"","6595","[MINOR] In-memory stores cleanup","While going through the review of InMemorySessionStore I realized there is also some minor cleanup to be done for the other in-memory stores. This includes trivial changes such as removing unnecessary references to 'this' and moving collection initialization to the declaration. It also fixes some unsafe behavior (registering an iterator from inside its own constructor). In-memory window store iterator classes were made static and some instances of KeyValueIterator missing types were fixed across a handful of tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-04-17T23:42:27Z","2020-06-26T22:39:06Z"
"","7311","MINOR: Add api version to uncaught exception message","When we have an unhandled exception in the request handler, we print some details about the request such as the api key and payload. It is also useful to see the version of the request which is not always apparent from the request payload.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-07T01:41:11Z","2019-09-11T02:08:42Z"
"","6636","KAFKA-8290: Close producer for zombie task","When we close a task and EOS is enabled we should always close the producer regardless if the task is in a zombie state (the broker fenced the producer) or not.  I've added tests that fail without this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-04-25T18:19:13Z","2019-05-20T13:54:55Z"
"","6754","MINOR: Add select changes from 3rd KIP-307 PR for incrementing name index counter","When users provide a name for operation via the Streams DSL, we need to increment the counter used for auto-generated names to make sure any operators downstream of a named operator still produce a compatible name.    This PR is a subset of #6411 by @fhussonnois.  We need to merge this PR now because it covers cases when users name repartition topics or state stores.    Updated tests to reflect the counter produces expected number even when the user provides a name.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","bbejeck","2019-05-17T14:52:13Z","2020-06-12T23:40:51Z"
"","7436","HOTFIX: don't throw if upgrading from very old versions","When upgrading from version before version probing, the `UPGRADE_FROM` parameter is set and forces the used subscription version to stay at the associated older version until everyone is on the new.  This means we might get a mix of new and old versions < `EARLIEST_PROBEABLE_VERSION` as we go through the second rolling bounce and remove the `UPGRADE_FROM` config. So we should just not do version probing if the version is too old, rather than throw an exception","closed","","ableegoldman","2019-10-02T20:48:04Z","2020-06-26T22:38:22Z"
"","7155","KAFKA-8746: Kibosh must handle an empty JSON string from Trogdor","When Trogdor wants to clear all the faults injected to Kibosh, it sends the empty JSON object {}. However, Kibosh expects {""faults"":[]} instead. Kibosh should handle the empty JSON object, since that's consistent with how Trogdor handles empty JSON fields in general (if they're empty, they can be omitted). We should also have a test for this.","closed","","cmccabe","2019-08-02T17:45:43Z","2019-11-15T23:13:33Z"
"","6842","KAFKA-8446: Kafka Streams restoration crashes with NPE when the record value is null","When the restored record value is null, we are in danger of NPE during restoration phase.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-05-30T02:24:24Z","2019-06-02T02:27:45Z"
"","6974","KAFKA-8570: Grow buffer to hold down converted records if it was insufficiently sized","When the log contains out of order message formats (for example v2 message followed by v1 message) and consists of compressed batches typically greater than 1kB in size, it is possible for down-conversion to fail. With compressed batches, we estimate the size of down-converted batches using:  ```     private static int estimateCompressedSizeInBytes(int size, CompressionType compressionType) {         return compressionType == CompressionType.NONE ? size : Math.min(Math.max(size / 2, 1024), 1","closed","","dhruvilshah3","2019-06-20T00:24:07Z","2019-07-03T16:41:12Z"
"","6580","KAFKA-8152; Controller should transition offline replicas on startup","When the controller starts up, we need to transition both online and offline replicas so that even offline state will be propagated to other brokers in the cluster. This patch also moves `ControllerChannelManager` out of `ControllerContext` since it was not really used there.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-14T07:42:45Z","2019-04-16T21:07:20Z"
"","6866","[KAFKA-8465]replication  strategy for the topic dimension","When some partiton's replication is assigned to a broker, which disks should these copies be placed on the broker? The original strategy is to allocate according to the number of partiitons, but this will cause a partiton with too many topics to be stored on a disk, which may cause disk hotspot problems.     In order to solve this problem, we propose an improved strategy: first ensure that the number of partitions of each disk in the topic dimension is even. If the number of partitions of a topic on two disks is equal, then sort according to the total number of partitions on the disk. Select a disk with the least number of partitions to store the current replication.","closed","","lordcheng10","2019-06-03T09:01:19Z","2019-08-01T15:19:50Z"
"","7254","MINOR: fix ProduceBenchWorker not to fail on final produce","When sending bad records, the Trogdor task will fail if the final record produced is bad.  Instead we should catch the exception to allow the task to finish since sending bad records is a valid use case.","closed","","rayokota","2019-08-26T22:05:14Z","2019-08-27T16:55:52Z"
"","6643","KAFKA-8298: Fix possible concurrent modification exception","When processing multiple key-changing operations during the optimization phase a `ConcurrentModificationException` is possible.  This PR fixes that situation.  I've added a test that fails without this fix.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-04-26T21:13:50Z","2019-05-01T16:45:05Z"
"","6563","KAFKA-8226: New MirrorMaker option partition.to.partition","When partition.to.partition=true MirrorMaker retains the partition number when mirroring records even without the keys. When partition.to.partition is disabled, records with null keys are shuffled between destination partitions. When using this option - source and destination topics are assumed to have the same number of partitions.","closed","","ernisv","2019-04-11T14:54:09Z","2022-02-10T16:18:35Z"
"","7076","HOT FIX: close RocksDB objects in correct order","When opening a rocksdb database with column families, things must be closed following a specific order as described [here](https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#opening-a-database-with-column-families)  Should be cherry-picked back to 2.2","closed","streams,","ableegoldman","2019-07-11T22:46:37Z","2019-07-15T16:13:45Z"
"","6577","MINOR: Add logging for the case when a topic is automatically created upon receiving a MetadataRequest","When Kafka receives a metadata request to get metadata for some non-existent topic and if `auto.create.topics.enable` is true, it automatically creates that non-existent topic with the default number of partitions and replication factor. The current logging provides minimum information when this automatic topic creation happens.  In some situations, knowing which application/client triggers the auto-topic creation would be very useful. For example, for a Kafka cluster with `auto.create.topics.enable` being true, some Kafka users might have their business log relies on the fact that this configuration property is true. However, `auto.create.topics.enable` is required to be set to false for some reason, knowing which applications/clients trigger the auto-topic creation would help cluster operators identify who are potentially going to be affected by this configuration change a so that some actions could be taken ahead of time.","closed","","Lincong","2019-04-13T00:48:27Z","2019-07-23T18:31:23Z"
"","6582","KAFKA-8052; Ensure fetch session epoch is updated before new request","When fetch response is processed by the heartbeat thread, polling thread may send new fetch request with the same epoch as the previous fetch request if heartbeat thread hasn't yet updated the epoch. This results in INVALID_FETCH_SESSION_EPOCH error. Even though the request is retried without any disconnections, it will be good to avoid this error. The PR tracks status of previous request in the session handler and sends next fetch request only after the response from the previous request is processed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-04-14T10:36:40Z","2019-05-22T17:16:13Z"
"","6690","[KAFKA-8328] Kafka smooth expansion","When expanding the kafka cluster, the new follower will read the data from the earliest offset. This can result in a large amount of historical data being read from the disk, putting a lot of pressure on the disk and affecting the performance of the kafka service, for example, the producer write latency will increase. In general, kafka's original expansion mechanism has the following problems:           1. The new follower will put a lot of pressure on the disk;           2. Causes the producer write latency to increase;           3. Causes the consumer read latency to increase;        In order to solve these problems, we have proposed a solution for smooth expansion. The main idea of ​​the scheme is that the newly added follower reads data from the HW position, and when the newly added follower reads the data to a certain time threshold or data size threshold, the follower enters the ISR queue. . Since the new follower reads data from the HW location, most of the data read is in the operating system's cache, so it does not put pressure on the disk and does not affect the performance of the kafka service, thus solving the above problems.","open","","lordcheng10","2019-05-07T06:52:28Z","2019-06-17T09:45:45Z"
"","6782","MINOR: Remove checking on original joined subscription within handleAssignmentMismatch","When consumer coordinator realize the subscription may have changed, today we check again against the `joinedSubscription` within `handleAssignmentMismatch`. This checking however is a bit fishy and over-kill as well. It's better just simplifying it to always request re-join.  The `joinedSubscription` object itself however still need to be maintained for potential augment to avoid extra re-joining the group.  Since `testOutdatedCoordinatorAssignment` already cover the normal case we also remove the other `invalidAssignment` test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-21T21:56:02Z","2020-04-24T23:53:21Z"
"","6722","KAFKA-8351; Cleaner should handle transactions spanning multiple segments","When cleaning transactional data, we need to keep track of which transactions still have data associated with them so that we do not remove the markers. We had logic to do this, but it was not being carried over when beginning cleaning for a new set of segments. This could cause the cleaner to incorrectly believe a transaction marker was no longer needed. The fix here carries the transactional state between groups of segments to be cleaned.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-13T16:20:56Z","2019-05-25T06:10:57Z"
"","6719","KAFKA-8347: Choose next record to process by timestamp","When choosing the next record to process, we should look at the head record's timestamp of each partition and choose the lowest rather than choosing the lowest of the partition's streamtime.  This change effectively makes RecordQueue return the timestamp of the head record rather than its streamtime. Streamtime is removed (replaced) from RecordQueue as it was only being tracked in order to choose the next partition to poll from.","closed","streams,","ableegoldman","2019-05-11T05:44:47Z","2020-06-26T22:38:55Z"
"","7163","MINOR: Fix ImplicitLinkedHashCollection.find()","When calling find() on an empty ImplicitLinkedHashCollection, it should return null instead of hitting a division by zero error.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-08-05T16:12:34Z","2019-08-16T08:45:25Z"
"","7137","MINOR: Fix wrong debug log message","When a `ProcessorStateManager` is constructed a debug message is written to the application logs that state the following:  ""Created state store manager for task {} with the acquired state dir lock""  This is wrong because the the state store manager is created during rebalancing but the the dir lock is acquired later on during execution of `runOnce()`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-07-30T11:43:01Z","2019-10-21T11:40:24Z"
"","6903","KAFKA-7800: Dynamic log levels admin API (KIP-412)","What ----   References ---------- [**KIP-412**](https://cwiki.apache.org/confluence/display/KAFKA/KIP-412%3A+Extend+Admin+API+to+support+dynamic+application+log+levels) [**KAFKA-7800**](https://issues.apache.org/jira/browse/KAFKA-7800) [**Discussion Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201901.mbox/%3CCANZZNGyeVw8q%3Dx9uOQS-18wL3FEmnOwpBnpJ9x3iMLdXY3gEug%40mail.gmail.com%3E) [**Vote Thread**](http://mail-archives.apache.org/mod_mbox/kafka-dev/201902.mbox/%3CCANZZNGzpTJg5YX1Gpe5S%3DHSr%3DXGvmxvYLTdA3jWq_qwH-UvorQ%40mail.gmail.com%3E)    Test&Review ------------ Test cases covered: * DescribeConfigs * Alter the log level with and without validateOnly, validate the results with DescribeConfigs  Open questions / Follow ups -------------------------- If you're a reviewer, I'd appreciate your thoughts on these questions I have open: 1. Should we add synchronization to the Log4jController methods? - Seems like we don't get much value from it 2. Should we instantiate a new Log4jController instead of it having static methods? - All operations are stateless, so I thought static methods would do well 3. A logger which does not have a set value returns ""null"" (as seen in the unit tests). Should we just return the Root logger's level?   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2019-06-07T10:28:48Z","2019-08-02T18:54:05Z"
"","6906","MINOR: Fix race condition on shutdown of verifiable producer","We've seen `ReplicaVerificationToolTest.test_replica_lags` fail occasionally due to errors such as the following: ``` RemoteCommandError: ubuntu@worker7: Command 'kill -15 2896' returned non-zero exit status 1. Remote error message: bash: line 0: kill: (2896) - No such process ``` The problem seems to be a shutdown race condition when using `max_messages` with the producer. The process may already be gone which will cause the signal to fail.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-07T19:08:33Z","2019-06-07T23:58:43Z"
"","6594","[KAFKA-6520] Add DISCONNECTED state to Kafka Streams","We wish to add a DISCONNECTED state to Kafka Streams. Please see KIP-457 for details. https://cwiki.apache.org/confluence/display/KAFKA/KIP-457%3A+Add+DISCONNECTED+status+to+Kafka+Streams  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ConcurrencyPractitioner","2019-04-17T20:46:46Z","2019-11-25T01:12:40Z"
"","7470","KAFKA-8122: Fix Kafka Streams EOS integration test","We tried to use `commitRequested` to synchronize the test progress, however, this mechanism seems to be broken. Note that `context.commit();` is a request that KS should commit asap -- however, after `context.commit()` returned only an internal flag is set and the commit is not executed yet.  Hence, after the counter is increased to 2, there is no guarantee what happens next: either we commit, or we might actually `poll()` for new data and if `writeInputData(uncommittedDataBeforeFailure);` executed before KS `poll()` again, we might process more than 10 records per partition before we actually commit, and hence the test fails.  A better way seems to be, to read the committed output data before writing new data to make sure the new data is not part of the first transactions and stays uncommitted before we inject the error.  Call for review @guozhangwang @ableegoldman @abbccdda @cpettitt-confluent","closed","tests,","mjsax","2019-10-08T21:41:43Z","2019-10-15T20:02:21Z"
"","6608","KAFKA-7965; Fix flaky test ConsumerBounceTest","We suspect the problem might be a race condition after broker startup where the consumer has yet to find the coordinator and rebalance. The fix here rolls all the brokers first and then waits for the expected exception.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-19T02:29:13Z","2019-04-19T23:45:27Z"
"","6699","KAFKA-8106:Reducing the allocation and copying of ByteBuffer when logValidator do validation(target trunk).","We suggest that reducing the allocation and copying of ByteBuffer when logValidator do validation when magic value to use is above 1 and no format conversion or value overwriting is required for compressed messages.And improved code is as follows. 1. Adding a class **SimplifiedDefaultRecord** implement class Record which define  various attributes of a message.  2. Adding Function **simplifiedreadFrom**() at class **DefaultRecord** .This function will not read data from DataInput to  ByteBuffer which need newly creating .**This will reduce the allocation and copying of ByteBuffer** when logValidator do validation .This will reduces GC frequency. We offer a simple read function to read data from **DataInput** whithout create ByteBuffer.Of course this opertion can not avoid deconmpression to data. 3. Adding Function **simplifiedIterator**() and **simplifiedCompressedIterator**() at class **DefaultRecordBatch**.This two functions will return iterator of instance belongs to class **SimplifiedDefaultRecord**. 4. Modify code of function **validateMessagesAndAssignOffsetsCompressed**() at class  LogValidator.      **After modifing code wich  reducing the allocation and copying of ByteBuffer**, the test performance is greatly improved, and the CPU's stable usage is below 60%. The following is a comparison of different code test performance under the same conditions. **Result of performance testing** Main config of Kafka: Single Message:1024B;TopicPartitions:200;linger.ms:1000ms, **1.Before modified code(Source code):** Network inflow rate:600M/s;CPU(%)(97%);production:25,000,000 messages/s **2.After modified code(remove allocation of ByteBuffer):** Network inflow rate:1G/s;CPU(%)(","closed","","Flowermin","2019-05-08T07:22:34Z","2019-07-04T08:06:49Z"
"","7252","KAFKA-8832:Limit the maximum size read by a fetch request on the kafka server.","We should limit the maximum size read by a fetch request on the kafka server.","open","","lordcheng10","2019-08-25T07:26:37Z","2019-08-29T03:11:45Z"
"","6591","MINOR: Ensure useful producer state validation exceptions","We should include partition/offset information when we raise exceptions during producer state validation. This saves a lot of the discovery work to figure out where the problem occurred. This patch also includes a new test case to verify additional coordinator fencing cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-17T19:00:46Z","2019-04-18T15:38:44Z"
"","6604","MINOR: Remove unused and unneeded Cluster object","We seem not to be using this class anymore and it is not a public API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-18T21:51:00Z","2019-04-18T23:13:07Z"
"","7102","MINOR: Tolerate limited data loss for upgrade tests with old message format","We see transient test failures in system tests for upgrade scenarios when older message formats prior to KIP-101 are used. Since we need to continue to test these versions, tolerate a small amount of data loss in those tests. We will still be testing that upgrade works and most messages are successfully delivered.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-07-19T13:59:53Z","2019-07-31T15:19:37Z"
"","6760","MINOR: Increase security test timeouts for transient failures","We see this test fail from time to time. After investigation, it seems the 5s timeouts are too tight. This patch increases them to 30s.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-18T23:15:39Z","2019-05-18T23:52:19Z"
"","6905","KAFKA-8003: Fix flaky testFencingOnTransactionExpiration","We see this failure from time to time: ``` java.lang.AssertionError: expected: but was: 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at kafka.api.TransactionsTest.testFencingOnTransactionExpiration(TransactionsTest.scala:512) ``` The cause is probably that we are using `consumeRecordsFor` which has no expectation on the number of records to fetch and a timeout of just 1s. This patch changes the code to use `consumeRecords` and the default 15s timeout.   Note we have also fixed a bug in the test case itself, which was using the wrong topic for the second write, which meant it could never have failed in the anticipated way anyway.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-07T16:04:20Z","2019-06-07T19:54:23Z"
"","6900","MINOR: Increase timeout for flaky ResetConsumerGroupOffsetTest","We see this failing from time to time. I think the default 5s timeout used by the reset tool may be a bit too tight. This patch increases the timeout to 15s.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-07T04:23:12Z","2019-06-07T07:48:19Z"
"","6907","MINOR: Lower producer throughput in flaky upgrade system test","We see the upgrade test failing from time to time. I looked into it and found that the root cause is basically that the test throughput can be too high for the 0.9 producer to make progress. Eventually it reaches a point where it has a huge backlog of timed out requests in the accumulator which all have to be expired. We see a long run of messages like this in the output:  ``` {""exception"":""class org.apache.kafka.common.errors.TimeoutException"",""time_ms"":1559907386132,""name"":""producer_send_error"",""topic"":""test_topic"",""message"":""Batch Expired"",""class"":""class org.apache.kafka.tools.VerifiableProducer"",""value"":""335160"",""key"":null} {""exception"":""class org.apache.kafka.common.errors.TimeoutException"",""time_ms"":1559907386132,""name"":""producer_send_error"",""topic"":""test_topic"",""message"":""Batch Expired"",""class"":""class org.apache.kafka.tools.VerifiableProducer"",""value"":""335163"",""key"":null} {""exception"":""class org.apache.kafka.common.errors.TimeoutException"",""time_ms"":1559907386133,""name"":""producer_send_error"",""topic"":""test_topic"",""message"":""Batch Expired"",""class"":""class org.apache.kafka.tools.VerifiableProducer"",""value"":""335166"",""key"":null} {""exception"":""class org.apache.kafka.common.errors.TimeoutException"",""time_ms"":1559907386133,""name"":""producer_send_error"",""topic"":""test_topic"",""message"":""Batch Expired"",""class"":""class org.apache.kafka.tools.VerifiableProducer"",""value"":""335169"",""key"":null} ``` This can continue for a long time (I have observed up to 1 min) and prevents the producer from successfully writing any new data. While it is busy expiring the batches, no data is getting delivered to the consumer, which causes it to eventually raise a timeout. ``` kafka.consumer.ConsumerTimeoutException at kafka.consumer.NewShinyConsumer.receive(BaseConsumer.scala:50) at kafka.tools.ConsoleConsumer$.process(ConsoleConsumer.scala:109) at kafka.tools.ConsoleConsumer$.run(ConsoleConsumer.scala:69) at kafka.tools.ConsoleConsumer$.main(ConsoleConsumer.scala:47) at kafka.tools.ConsoleConsumer.main(ConsoleConsumer.scala) ``` The fix here is to reduce the throughput, which seems reasonable since the purpose of the test is to verify the upgrade, which does not demand heavy load. Note that I investigated several failing instances of this test going back to 1.0 and saw a similar pattern, so there does not appear to be a regression.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-07T23:21:15Z","2019-06-08T16:32:43Z"
"","6908","MINOR: Fix transient failure in PreferredReplicaLeaderElectionCommandTest","We saw this failure: ``` java.util.NoSuchElementException: None.get 	at scala.None$.get(Option.scala:366) 	at scala.None$.get(Option.scala:364) 	at kafka.admin.PreferredReplicaLeaderElectionCommandTest.getLeader(PreferredReplicaLeaderElectionCommandTest.scala:101) 	at kafka.admin.PreferredReplicaLeaderElectionCommandTest.testNoopElection(PreferredReplicaLeaderElectionCommandTest.scala:240) ``` We need to wait for the leader to be available.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-08T00:40:43Z","2019-06-10T19:20:52Z"
"","7033","MINOR: Control plane listener tests should not use static port","We recently saw a few failing tests recently due to the static reliance on port 5000. For example: ``` org.apache.kafka.common.KafkaException: Socket server failed to bind to localhost:5000: Address already in use. 	at kafka.network.Acceptor.openServerSocket(SocketServer.scala:605) 	at kafka.network.Acceptor.(SocketServer.scala:481) 	at kafka.network.SocketServer.createAcceptor(SocketServer.scala:253) 	at kafka.network.SocketServer.$anonfun$createControlPlaneAcceptorAndProcessor$1(SocketServer.scala:234) 	at kafka.network.SocketServer.$anonfun$createControlPlaneAcceptorAndProcessor$1$adapted(SocketServer.scala:232) 	at scala.Option.foreach(Option.scala:438) 	at kafka.network.SocketServer.createControlPlaneAcceptorAndProcessor(SocketServer.scala:232) 	at kafka.network.SocketServer.startup(SocketServer.scala:119) 	at kafka.network.SocketServerTest.withTestableServer(SocketServerTest.scala:1139) 	at kafka.network.SocketServerTest.testControlPlaneRequest(SocketServerTest.scala:198) ``` This patch fixes the failing tests to dynamically select the port.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-04T06:59:19Z","2019-07-04T15:22:46Z"
"","6975","MINOR: Fix flaky test case for compact/delete topics","We recently observed a transient failure of this test case: ``` java.lang.AssertionError: Contents of the map shouldn't change expected: (340,340), 5 -> (345,345), 10 -> (350,350), 14 -> (354,354), 1 -> (341,341), 6 -> (346,346), 9 -> (349,349), 13 -> (353,353), 2 -> (342,342), 17 -> (357,357), 12 -> (352,352), 7 -> (347,347), 3 -> (343,343), 18 -> (358,358), 16 -> (356,356), 11 -> (351,351), 8 -> (348,348), 19 -> (359,359), 4 -> (344,344), 15 -> (355,355))> but was: (340,340), 5 -> (345,345), 10 -> (350,350), 14 -> (354,354), 1 -> (341,341), 6 -> (346,346), 97 -> (297,297), 9 -> (349,349), 96 -> (296,296), 13 -> (353,353), 2 -> (342,342), 17 -> (357,357), 12 -> (352,352), 7 -> (347,347), 98 -> (298,298), 3 -> (343,343), 18 -> (358,358), 95 -> (295,295), 16 -> (356,356), 11 -> (351,351), 99 -> (299,299), 8 -> (348,348), 19 -> (359,359), 4 -> (344,344), 15 -> (355,355))> ``` The presence of old keys implies that not all old segments had been deleted. The fix is to improve the wait condition. Rather than waiting for the number of segments to be 1, we wait for the log start offset to reach the end offset.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-20T16:35:28Z","2019-06-25T20:44:05Z"
"","7183","MINOR: Ignore dynamic log4j log level tests","We recently introduced a bunch of flaky tests in the AdminClientIntegrationTest. These tests are failing very frequently. We should ignore the tests in order to make the build stable until we have a fix.","closed","","stanislavkozlovski","2019-08-09T10:44:00Z","2019-08-09T15:11:39Z"
"","6855","MINOR: Reordering the props modification with configs construction","We need to only construct the StreamsConfig object after the props being fully populated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","guozhangwang","2019-05-31T21:16:46Z","2020-10-16T06:17:28Z"
"","7317","KAFKA-8889: Log errors properly","We need stacktrace of the error to understand the root cause and to trouble shoot the underlying problem.","closed","","qinghui-xu","2019-09-09T21:55:38Z","2019-09-10T05:26:54Z"
"","6925","MINOR: Reinstate info-level log for dynamic update of SSL keystores","We log all dynamic updates at info-level in DynamicBrokerConfig. In addition to this, we also used to have an info-level log entry in SslFactory for SSL keystore and truststore update including the modification time of the file, since we update these without an actual config change. We seem to have lost that entry when making updates to the factory recently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-06-12T15:59:38Z","2019-06-23T08:30:29Z"
"","6813","MINOR: Remove redundant hw checkpoint thread started field in ReplicaManager","We have two fields `highWatermarkCheckPointThreadStarted` and `hwThreadInitialized` which appear to be serving the same purpose. This patch gets rid of `hwThreadInitialized`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-25T00:01:32Z","2019-05-28T19:56:23Z"
"","7079","MINOR: Use dynamic port in RestServerTest","We have seen some failures recently in `RestServerTest`. It's the usual problem with reliance on static ports.  ``` Caused by: java.io.IOException: Failed to bind to 0.0.0.0/0.0.0.0:8083 	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:346) 	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:308) 	at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80) 	at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:236) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) 	at org.eclipse.jetty.server.Server.doStart(Server.java:396) 	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:68) 	at org.apache.kafka.connect.runtime.rest.RestServer.initializeServer(RestServer.java:178) 	... 56 more Caused by: java.net.BindException: Address already in use ``` This patch makes the chosen port dynamic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-12T05:27:00Z","2019-07-12T22:29:10Z"
"","7330","KAFKA-8894: Bump streams test topic deletion assertion timeout from 30s to 60s","We have seen rare flakiness in this assertion - all of streams' internal topics would not get deleted within the 30 second window. Increasing to 60 seconds should reduce the occurrence.","open","tests,","stanislavkozlovski","2019-09-13T19:44:59Z","2020-03-03T00:56:21Z"
"","6559","MINOR: Remove SubscriptionState.Listener and replace with assignmentId tracking","We have not had great experience with listeners. They make the code harder to understand because they result in indirectly maintained circular dependencies. Often this leads to tricky deadlocks when we try to introduce locking. We were able to remove the Metadata listener in KAFKA-7831. Here we do the same for the listener in SubscriptionState.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-11T01:12:13Z","2019-04-12T04:26:59Z"
"","7179","MINOR: improve StreamsBrokerDownResilienceTest debuggability","We have encountered some recent failure on this test and couldn't debug further due to the fact that we have no clue of the time when data flows in between different components. Adding the time when record gets processed on stream could help us better triage issue.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","abbccdda","2019-08-08T05:22:11Z","2020-05-14T02:55:39Z"
"","7021","KAFKA-8620: fix NPE due to race condition during shutdown while rebalancing","We have detected a race condition under system test failure. The problem was that the task manager internal active tasks should be guarded against state changes on the stream thread. Could definitely consider other fixes but this is currently the make-sense one.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-07-01T22:19:12Z","2019-07-15T23:13:35Z"
"","6956","MINOR: Fix GroupMaxSizeReachedException error message","We get the following confusing message in the log when we hit the GROUP_MAX_SIZE_REACHED error: ``` Attempt to join group failed due to fatal error: Consumer group The consumer group has reached its max size. already has the configured ... ``` The problem is a conflict with the single-arg constructor, which is expected to be both a groupId and an exception message. The patch makes the argument a message in order to resolve the inconsistency.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-17T21:10:54Z","2019-07-19T21:55:52Z"
"","7047","MINOR: Use Topic::isInternalTopic instead of directly checking","We don't allow chaning number of partitions for internal topics. To do so we check if the topic name belongs to the set of internal topics directly instead of using the ""isInternalTopic"" method. This breaks the encapsulation by making client aware of the fact that internal topics have special names.  This is a simple change to use the method ""Topic::isInternalTopic"" method instead of checking it directly in ""alterTopic"" command.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-07-08T19:01:06Z","2019-07-10T00:51:55Z"
"","6539","KAFKA-8190; Don't update keystore modification time during validation","We currently store keystore file modification time when loading keystores in a `SecurityStore` instance. When dynamically updating keystores without filename change, we compare the time at the last load against the current file modification time. But we load keystore for validation of dynamic configs and as a result, we dont recreate SSLContext when performing actual reconfiguration after the validation. We always create a new `SecurityStore` instance for reconfiguration of the store, so we only need to store file modification time when we construct the instance.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-04-04T22:01:24Z","2019-04-05T18:42:29Z"
"","6714","KAFKA-8221 & KIP-345 part-4: Add batch leave group request","We are aiming to support batch leave group request issued from admin client. This diff is the first effort to bump leave group request version.  Note that we relax the state check on broker side for restricting a request with both `member.id` and `group.instance.id` set. Although it's not possible at the moment, we don't want to restrict future development to potentially handle leave group from static member too.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-10T22:01:50Z","2019-08-06T17:28:21Z"
"","6984","KAFKA-8583: Optimization for SslTransportLayer#write(ByteBuffer)","Warp data as many as possible in SslTransportLayer#write(ByteBuffer) The change comes from Ambry, whose TransportLayer code is same with Kafka. https://github.com/linkedin/ambry/pull/1105  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","zzmao","2019-06-21T21:58:22Z","2019-09-25T19:02:39Z"
"","7413","KAFKA-8649: version probing with upgraded leader","Version probing is currently broken when the leader is chosen from one of the new (already upgraded) instances, as older members will blindly upgrade to a version they don't support (then throw an exception) while newer members will receive an assignment with the older version and trigger a new rebalance (leading to rebalance loop if the older members can't upgrade their subscription) because we always send assignments encoded using the min version seen by any client--see ticket for details.  Note that a real ""version probing"" rebalance is technically one where the leader is old and receives subscriptions it can't understand. In this case we send an assignment back with the old version, which the receiving consumer then knows to downgrade to and trigger another rebalance.   When you have a ""new"" leader however, I propose we:  - always send assignments back using the same version as the corresponding subscription, EXCEPT if we notice that everyone now supports the latest version but some are still using the older version. this signals the rolling upgrade is complete, so send everyone back the latest version - if you receive a version greater than the one you sent, it must mean the bounce is over and it is safe to now send new versions. upgrade your subscription version and trigger a final rebalance. this will actually only happen when the leader is the last to be bounced - if the leader is new, it can understand all subscriptions so we just wait for everyone to be bounced and allow new members to keep sending new version subscriptions. once the last member is upgraded everyone will already be using the new subscription and we don't need to trigger a second and final rebalance. - if you receive a version less than what you sent, this is version probing so downgrade your subscription and trigger another rebalance -- this will now only happen when you are actually on a higher version than the leader, so we know this is true version probing.","closed","","ableegoldman","2019-09-29T09:42:56Z","2020-06-26T22:38:28Z"
"","7286","[KAFKA-8820] [WIP] Use Admin API of Replica Reassignment in CLI tools","Utilize the new Admin API for replica reassignment in the command-line tools, as described in KIP-455: https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment  This particular PR is a first PR for Kafka and is only a change to the kafka-topics.sh code to print out  adding/removing replicas. It includes both a TopicCommandTest and a TopicCommandWithAdminClientTest. Since the new replica reassignment API isn't getting used, we  are expecting no replicas to actually be  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","steverod","2019-09-03T15:28:55Z","2020-05-12T21:06:23Z"
"","7068","KAFKA-8651: Add predicate map to #branch","Using #branch with an array is error prone and not readable, due to referencing the results by magic index offsets. So, we suggest to add an API that allows naming different branches, which is more natural.  See KIP (on it's way)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","astubbs","2019-07-10T17:36:03Z","2020-12-23T01:12:53Z"
"","6730","[HOT FIX] Leverage ConfigSetter#close method to properly open/close RocksObjects","Uses the close method added in KIP-453 to properly init & close RocksObjects in RocksDBStoreTest","closed","streams,","ableegoldman","2019-05-14T19:11:30Z","2020-06-26T22:38:57Z"
"","7009","MINOR: improve RocksDBConfigSetter docs","Users often use the RocksDBConfigSetter to modify parameters such as cache or block size, which must be set through the BlockBasedTableConfig object. Rather than creating a new object in the config setter, however, users should most likely retrieve a reference to the existing one so as to not lose the other defaults (eg the BloomFilter)  There have been notes from the community that it is not obvious this should be done, nor is it immediately clear how to do so. This PR updates the RocksDBConfigSetter docs to hopefully improve things.  I also piggybacked a few minor cleanups in the docs","closed","","ableegoldman","2019-06-27T20:50:13Z","2019-07-08T23:20:39Z"
"","7187","MINOR: always call onPartitionsLost in eager rebalancing","Users may be relying on the notification of a rebalance beginning, for example as Streams does with its FSM. If we don't call `onPartitionsLost` with empty partition set, users will only see `onPartitionsAssigned` as the first callback in a rebalance.  On the other hand, it does seem to make sense to reserve `onPartitionsLost` for ""abnormal"" conditions like missing a generation","closed","","ableegoldman","2019-08-09T18:36:13Z","2020-06-26T22:38:42Z"
"","6659","KAFKA-8215: Upgrade Rocks to v6.0.1","Users have indicated that the memory usage of RocksDB can be excessive, in particular as it scales with the number of rocksdb instances meaning it could cause OOM if a thread ends up with a large number of stores following a rebalance.   Upgrading Rocks exposes a number of new configs, including the WriteBufferManager which --along with the existing TableFormatConfigs -- allows users to limit the **total** memory allocated to Rocks across all instances in a single process  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-05-01T18:44:54Z","2020-06-26T22:39:03Z"
"","7283","KAFKA-8862: Use consistent exception messages for nonexistent partition","Use the same exception message for timeout via `metadata.awaitUpdate(version, remainingWaitMs)` as for TimeoutException originating directly in `KafkaProducer.waitOnMetadata()`.  The contribution is my original work and I license the work to the project under the project's open source license.  cc/ @hachikuji   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tombentley","2019-09-03T13:04:59Z","2022-04-14T20:56:10Z"
"","6987","MINOR: Pass byte buffer [DO NOT MERGE]","Use ByteBuffer.wrap() only once per batch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-06-23T22:17:46Z","2020-04-24T23:58:11Z"
"","7069","KAFKA-8571: Clean up purgatory when leader replica is kicked out of replica list.","Upon receiving `StopReplicaRequest`, broker should check its purgatory and finish related pending produce/consume requests if any; otherwise client will wait unnecessary long time before getting timeout response.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kidkun","2019-07-10T19:01:16Z","2019-11-18T23:13:10Z"
"","7333","MINOR: Update dependencies for Kafka 2.4 (part 2)","Upgrade to Gradle 5.6.2 as a step towards Gradle 6.0 (necessary for Java 13 support).  https://docs.gradle.org/5.5.1/release-notes.html https://docs.gradle.org/5.6.2/release-notes.html  The other updates are mostly bug fixes:  * Scala 2.13.1: https://github.com/scala/scala/releases/tag/v2.13.1 * Scala 2.12.10: https://github.com/scala/scala/releases/tag/v2.12.10 * Jetty 9.4.20: https://www.eclipse.org/lists/jetty-announce/msg00133.html * SLF4J 1.7.28: adds Automatic-Module-Name in MANIFEST.MF * Bouncy castle 1.63: https://www.bouncycastle.org/releasenotes.html * zstd 1.4.3: https://github.com/facebook/zstd/releases/tag/v1.4.3  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-14T20:44:05Z","2019-09-18T19:51:51Z"
"","7099","MINOR: Update documentation for enabling optimizations","Updated docs for enabling all optimizations as of `2.3`  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-07-17T16:13:30Z","2019-07-19T17:20:14Z"
"","7410","Merge pull request #1 from apache/trunk","update to newest  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","nackinggit","2019-09-29T03:30:57Z","2019-09-29T03:33:48Z"
"","7215","MINOR: Update the javadoc of SocketServer#startup()","Update the javadoc of SockerServer#startup(). SocketServer#startProcessors() does not exist any more and it has been replaced by SocketServer#startDataPlaneProcessors() and SocketServer#startControlPlaneProcessor().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-08-14T20:26:40Z","2019-08-15T16:57:05Z"
"","6487","Merge pull request #1 from apache/trunk","update from apache  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","Captainhzy","2019-03-22T13:41:00Z","2019-05-11T07:05:25Z"
"","7301","MINOR: Add cause to thrown exception when deleting topic in TopicCommand","Unexpected exceptions are caught during topic deletion in `TopicCommand`. The caught exception is currently lost and we raise `AdminOperationException`. This patch fixes the problem by chaining the caught exception.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-05T19:04:10Z","2019-09-06T16:44:16Z"
"","6990","MINOR: Fix scala 2.13 (attempt 2)","Trying to trigger Scala 2.13 Jenkins job.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-24T10:48:58Z","2019-06-24T10:51:25Z"
"","6489","KAFKA-8150: Fix bugs in handling null arrays in generated RPC code","ToString functions must not get a NullPointException.  read() functions must properly translate a negative array length to a null field.","closed","","cmccabe","2019-03-22T22:50:47Z","2019-05-20T18:54:09Z"
"","6666","KAFKA-8220 & KIP-345 part-3: Avoid kicking out members through rebalance timeout","To make consumer group members more persist, we want to avoid kick-out unjoined members through rebalance timeout. The only exception is when leader fails to join, because we will at risk of no assignment computed during sync stage. The choice will be kicking off non-responsive leader and choose a new leader if possible.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-02T19:01:16Z","2019-05-17T18:26:32Z"
"","7139","KAFKA-8735: Check properties file existence first","To make BrokerMetadataCheckpoint more robust, and avoid a leak of abstraction.  Details and rationales are in the jira ticket.","open","","qinghui-xu","2019-07-30T21:13:31Z","2019-07-31T16:51:37Z"
"","7111","Remove spaces from thread name","Thread name should not contains spaces, it looks a bit weird in logs and hard to parse, for example it breaks ""%{NOTSPACE:thread}"" with logstash grok filter.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","quaff","2019-07-25T03:31:58Z","2019-07-25T11:31:00Z"
"","6513","streams documentation input topic corrected","Though out the tutorial, the name of the input topic that was created is `streams-plaintext-input`. However, this was mistaken at some point in the tutorial and changed to `streams-wordcount-input`.  This patch is to adjust that. Thanks.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Tcheutchoua-Steve","2019-03-28T07:38:46Z","2019-03-29T04:02:42Z"
"","7464","KAFKA-8932; Add tag for CreateTopicsResponse.TopicConfigErrorCode","This was discussed under KIP-525, but tag was not added in the original commit since the PR with flexible versions hadn't yet been merged. Sets CreateTopics version to 5 with both KIP-525 and flexible versions change so that tags can be used with v5.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-10-08T09:23:35Z","2019-10-09T08:19:18Z"
"","6931","MINOR: Fix merge conflict in version.py","This was a regression in commit c450bfc2911fa121e18815a15b5fd7f087fd03ac  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","stanislavkozlovski","2019-06-13T09:44:02Z","2019-06-14T15:39:35Z"
"","6743","KAFKA-8215: Upgrade Rocks to v5.18.3","This upgrade exposes a number of new options, including the WriteBufferManager which -- along with existing TableConfig options -- allows users to limit the total memory used by RocksDB across instances. This can alleviate some cascading OOM potential when, for example, a large number of stateful tasks are suddenly migrated to the same host.  The RocksDB docs guarantee backwards format compatibility across versions","closed","streams,","ableegoldman","2019-05-15T22:02:33Z","2020-06-26T22:38:54Z"
"","7061","MINOR: Increase `awaitCommits` timeout in ExampleConnectIntegrationTest","This test usually fails in `awaitCommits`. This patch increases the timeout from 5s to 15s. It seemed odd that we were relying on `CONNECTOR_SETUP_DURATION_MS`, so I just had it use `RECORD_TRANSFER_DURATION_MS` instead.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","hachikuji","2019-07-10T00:17:38Z","2020-10-16T05:50:57Z"
"","6681","Improve high watermark checkpointing performance","This PR works to improve high watermark checkpointing performance.   `ReplicaManager.checkpointHighWatermarks()` was found to be a major contributor to GC pressure, especially on Kafka clusters with high partition counts.  #### Commit gardnervickers@4307a73  Adds a JMH benchmark for `checkpointHighWatermarks` which establishes a baseline for the commits which come after it.   The parameterized benchmark was run with 100, 1000 and 2000 topics.   #### Commit gardnervickers@ceb8272  Modifies `ReplicaManager.checkpointHighWatermarks()` to avoid extra copies, and caches the Log parent directory to avoid frequent allocations when calculating the `File.getParent()`. It appears `Log.dir` is accessed from `ReplicaManager.checkpointHighWatermarks` without taking the `Log.lock` currently, this PR does the same for the cached `Log.parentDir`. I considered making these values change atomically but I had some trouble thinking of a case where not taking the lock would be safe with the current implementation, but unsafe with these proposed changes.    The following improvements were measured off baseline - 100 topics   - +29% ops/ms   - -63% MB/sec alloc - 1000 topics   - +86% ops/ms   - -35% MB/sec alloc - 2000 topics   - +72% ops/ms   - -55% MB/sec alloc  #### Commit gardnervickers@c1830fe This is a pretty invasive/significant refactor of the `CheckpointFile` code and accompanying classes. The goal was to avoid unnecessary buffering and allocation when constructing and writing out the checkpoint file. Also, I attempted to encourage inlining as much as possible. The change set is pretty drastic but the improvements are attractive.   The following improvements were measured off baseline - 100 topics   - +45% ops/ms   - -679% MB/sec alloc - 1000 topics   - +222% ops/ms   - -256% MB/sec alloc - 2000 topics   - +186% ops/ms   - -339% MB/sec alloc   I think it would be reasonable if the project only wanted to take gardnervickers@ceb8272 (or a variant of it) for now, if the changes in gardnervickers@c1830fe seem too extreme.   #### JMH output [logs-0.8.0-beta1-5620-gc1830fec95.txt](https://github.com/apache/kafka/files/3146181/logs-0.8.0-beta1-5620-gc1830fec95.txt) [logs-0.8.0-beta1-5619-gceb8272a61.txt](https://github.com/apache/kafka/files/3146182/logs-0.8.0-beta1-5619-gceb8272a61.txt) [logs-0.8.0-beta1-5618-g4307a734ef.txt](https://github.com/apache/kafka/files/3146183/logs-0.8.0-beta1-5618-g4307a734ef.txt)","closed","","gardnervickers","2019-05-06T00:16:12Z","2019-05-07T21:20:21Z"
"","6741","Improve performance of checkpointHighWatermarks, patch 1/2","This PR works to improve high watermark checkpointing performance.  `ReplicaManager.checkpointHighWatermarks()` was found to be a major contributor to GC pressure, especially on Kafka clusters with high partition counts and low throughput.  #### Commit gardnervickers@4307a73  Adds a JMH benchmark for `checkpointHighWatermarks` which establishes a baseline for the commits which come after it.   The parameterized benchmark was run with 100, 1000 and 2000 topics.   #### Commit 27c1ba1 Modifies `ReplicaManager.checkpointHighWatermarks()` to avoid extra copies, and caches the Log parent directory to avoid frequent allocations when calculating the `File.getParent()`. It appears `Log.dir` is accessed from `ReplicaManager.checkpointHighWatermarks` without taking the `Log.lock` currently, this PR does the same for the cached `Log.parentDir`. I considered making these values change atomically but I had some trouble thinking of a case where not taking the lock would be safe with the current implementation, but unsafe with these proposed changes.    #### Improvements over gardnervickers@4307a73  (baseline)  | Topic Count | Ops/ms | MB/sec allocated | |-------------|---------|------------------| | 100               | + 51%    |  - 91% | | 1000             | + 143% |  - 49% | | 2000            | + 149% |   - 50% |  Also: * Changed all usages of Log.dir.getParent to Log.parentDir and Log.dir.getParentFile to Log.parentDirFile. * Only expose accessors for Log.dir and Log.parentDir. * Removed unused parameters in Partition.makeLeader, Partition.makeFollower and Partition.createLogIfNotExists.","closed","","gardnervickers","2019-05-15T21:56:35Z","2020-03-26T03:53:43Z"
"","6704","MINOR: docs typo in '--zookeeper myhost:2181--execute'","this PR will fix a typo related to docs: http://kafka.apache.org/21/documentation.html#rep-throttle  ```bash $ bin/kafka-reassign-partitions.sh --zookeeper myhost:2181--execute --reassignment-json-file bigger-cluster.json —throttle 50000000 ```  I think `myhost:2181` should be `localhost:2181` and followed by a `space`","closed","","opera443399","2019-05-09T14:56:37Z","2019-05-10T06:16:07Z"
"","6668","KAFKA-8316: Remove deprecated usage of Slf4jRequestLog, SslContextFactory","This PR resolves [KAFKA-8316](https://issues.apache.org/jira/browse/KAFKA-8316) following the method described [here](https://github.com/eclipse/jetty.project/issues/3502), along with [KAFKA-8308](https://issues.apache.org/jira/browse/KAFKA-8308).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-05-03T10:32:13Z","2019-05-20T17:15:16Z"
"","6701","KAFKA-8337: Fix tests/setup.cfg to work with pytest 4.x","This PR replaces [pytest] section in tests/setup.cfg with [tool:pytest] so that the unit tests for the system tests work with pytest 4.x. I ran `python setup.py test` and confirmed it succeeded.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sekikn","2019-05-08T15:27:09Z","2019-12-10T19:27:40Z"
"","6916","MINOR: Remove integration package prefix from ConsumerTopicCreationTest","This PR removes the `integration` package prefix from the `ConsumerTopicCreationTest`, bringing it back in line with the rest of the integration tests.","closed","","gardnervickers","2019-06-11T18:13:57Z","2019-06-12T05:34:13Z"
"","6976","KAFKA-8546: Call System#runFinalization to avoid memory leak caused by JDK-6293787","This PR prevents the surge of Finalizer objects from `[Inflater, Deflater]` instances, by calling `System#runFinalization` on closing `GZip[Input, Output]Stream`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2019-06-20T18:43:44Z","2020-09-13T08:30:50Z"
"","7324","KAFKA-8841: reduce overhead of ReplicaManager.updateFollowerFetchState","This PR makes two changes to code in the ReplicaManager.updateFollowerFetchState path, which is in the hot path for follower fetches. Although calling ReplicaManager.updateFollowerFetch state is inexpensive on its own, it is called once for each partition every time a follower fetch occurs.  1. updateFollowerFetchState no longer calls maybeExpandIsr when the follower is already in the ISR. This avoid repeated expansion checks.  2. Partition.maybeIncrementLeaderHW is also in the hot path for ReplicaManager.updateFollowerFetchState. Partition.maybeIncrementLeaderHW calls Partition.remoteReplicas four times each iteration, and it performs a toSet conversion. maybeIncrementLeaderHW now avoids generating any intermediate collections when updating the HWM.  **Benchmark results for Partition.updateFollowerFetchState on a r5.xlarge:** Old: ```   1288.633 ±(99.9%) 1.170 ns/op [Average]   (min, avg, max) = (1287.343, 1288.633, 1290.398), stdev = 1.037   CI (99.9%): [1287.463, 1289.802] (assumes normal distribution) ```  New (when follower fetch offset is updated): ```   261.727 ±(99.9%) 0.122 ns/op [Average]   (min, avg, max) = (261.565, 261.727, 261.937), stdev = 0.114   CI (99.9%): [261.605, 261.848] (assumes normal distribution) ```  New (when follower fetch offset is the same): ```   68.484 ±(99.9%) 0.025 ns/op [Average]   (min, avg, max) = (68.446, 68.484, 68.520), stdev = 0.023   CI (99.9%): [68.460, 68.509] (assumes normal distribution) ```","closed","","lbradstreet","2019-09-11T21:27:18Z","2019-09-18T19:49:12Z"
"","7026","KAFKA-5876: [WIP]IQ should throw different exceptions for different errors","This PR is used for KIP-216 discussion: [KIP-216: IQ should throw different exceptions for different errors](https://cwiki.apache.org/confluence/display/KAFKA/KIP-216%3A+IQ+should+throw+different+exceptions+for+different+errors)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","vitojeng","2019-07-02T10:05:02Z","2020-06-12T23:40:14Z"
"","6933","KAFKA-8450: Using KeyValueTimeStamp in MockProcessor","This PR is to use KeyValueTimeStamp Object in MockProcessor Test file instead of String and change all the dependency files with broken test cases.","closed","streams,","suryateja008","2019-06-13T11:21:16Z","2019-07-16T17:18:28Z"
"","6520","KAFKA-7502: Cleanup KTable materialization logic in a single place (doMapValues)","This PR is the final part of KAFKA-7502, which cleans up `KTableImpl#doMapValues` method. (follow up of #6174, #6453, and #6519)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-03-28T23:56:49Z","2019-03-29T23:54:35Z"
"","7172","(DO NOT MERGE) Kip 447 condensed POC","This PR is serving as a benchmarking sample to justify stream client performance using stream thread producer.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-08-06T21:30:01Z","2020-05-14T02:57:40Z"
"","6936","KAFKA-8539 (part of KIP-345): add group.instance.id to Subscription","This PR is part of KIP-345's effort to utilize this new field for more stable topic partition assignment. More details [here](https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances#KIP-345:Introducestaticmembershipprotocoltoreduceconsumerrebalances-ClientBehaviorChanges).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-13T22:06:30Z","2019-06-17T20:25:45Z"
"","7092","KAFKA-8602: Separate PR for 2.3 branch","This PR is from the original work by @cadonna in #7008.  Due to incompatible changes in trunk that should not get cherry-picked back, a separate PR is required for this bug fix  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-07-15T23:00:21Z","2019-08-01T18:03:54Z"
"","7217","TRIVIAL: Improve Admin Documentation","This PR is a follow-up of #7087, fixing typos, styles, etc.  cc/ @big-andy-coates @ijuma  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-08-15T10:40:43Z","2019-11-29T11:47:37Z"
"","6519","KAFKA-7502: Cleanup KTable materialization logic in a single place (doTransformValues)","This PR is a follow-up of #6174 and #6453, which cleans up `KTableImpl#doTransformValues` method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","dongjinleekr","2019-03-28T23:45:04Z","2019-03-29T23:55:58Z"
"","7377","KAFKA-8896: Check group state before completing delayed heartbeat","This PR is a defensive fix for reported bug 8896, which would cause group coordinator crash when the heartbeat member was not found.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-09-23T16:07:29Z","2019-10-01T14:18:27Z"
"","6998","MINOR: add serializeTo method for AbstractRequestResponse so it can be serialized to an existing ByteBuffer","This PR intends to add a new method so I can serialize request/response objects to an existing ByteBuffer in some scenes instead of creating a new ByteBuffer every time.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Wennn","2019-06-25T16:24:13Z","2019-06-26T15:15:16Z"
"","6961","MINOR: Fix for typos in processor-api.html","This PR intendent to address some typos in https://kafka.apache.org/documentation/streams/developer-guide/processor-api.html page.  - Invalid configuration option specified in the example. I've replaced with closest constant `TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG`, since `LogConfig.MinInSyncReplicasProp()` requires Scala stuff - Reference to `LogConfig` seems to be obsolete, I believe I've moved it to correct line - Apostrophe displayed incorrectly","closed","","seprokof","2019-06-18T16:56:22Z","2019-06-26T11:18:14Z"
"","6669","KAFKA-8238: Adding Number of messages/bytes read","This PR includes changes to count the number of messages/bytes read while loading offsets from TopicPartition.","closed","","vamossagar12","2019-05-03T14:07:46Z","2020-10-19T05:13:48Z"
"","7398","KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 2)","This PR implements the second part of KIP-511: * Connection Registry * Integration in the Processor and the KafkaApis * Integration with SaslServerAuthenticator * New Metrics * Enrichment of the RequestLog  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-09-26T15:58:50Z","2020-10-06T20:11:47Z"
"","7381","KAFKA-8855; Collect and Expose Client's Name and Version in the Brokers (KIP-511 Part 1)","This PR implements the first part of KIP-511, namely: - update the ApiVersionsRequest/Response to the auto-generated ones; - bump the versions of ApiVersionsRequest/Response; - add ClientSoftwareName and ClientSoftwareVersions in the ApiVersionsRequest; - add validation for those two fields and handle error in the client; - implement the proposed fail-back mechanism.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-09-24T16:33:32Z","2020-08-11T06:49:06Z"
"","7041","KAFKA-8345: Add an Admin API for partition reassignment (KIP-455)","This PR implements the changes needed for [KIP-455: Create an Administrative API for Replica Reassignment](https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment)  The changes here are heavily influenced by https://github.com/apache/kafka/pull/6955. We synced offline with Colin and decided I'll continue the work here. There were some missing stuff from the existing commits there, so I'll be re-creating them, instead of cherry-picking.  Colin's original implementation had a CallbackManager for managing callbacks from and into the controller. I've decided to not implement it here because: 1. I don't completely understand how it would work - Colin's implementation didn't have it plugged in to anything yet 2. It would essentially make the controller multi-threaded if I understand it correctly. I'd like to reduce the complexity and scope of this PR.  I believe that we can delegate it to a separate PR/JIRA, that way we could also move other parts of the controller to leverage it (e.g )","closed","","stanislavkozlovski","2019-07-08T10:41:22Z","2019-08-20T16:51:37Z"
"","7114","KAFKA-8345: KIP-455 Protocol changes (part 1)","This PR implements parts of the changes needed for [KIP-455: Create an Administrative API for Replica Reassignment](https://cwiki.apache.org/confluence/display/KAFKA/KIP-455%3A+Create+an+Administrative+API+for+Replica+Reassignment)  cc people who voted for the KIP - @viktorsomogyi @sql888 @bob-barrett @gwenshap @hachikuji @cmccabe","closed","","stanislavkozlovski","2019-07-25T15:14:39Z","2019-07-30T10:49:47Z"
"","7403","KAFKA-7772: Dynamically Adjust Log Levels in Connect","This PR implements changes proposed in [KIP-495](https://cwiki.apache.org/confluence/display/KAFKA/KIP-495%3A+Dynamically+Adjust+Log+Levels+in+Connect).  This change will introduce an `/admin/loggers` endpoint to the Connect worker that can be used to get/modify the log levels of any named logger in the Connect worker.  The following new configs are introduced:   * `admin.listeners` to control where the admin endpoint will be made available. The default value should bring the `/admin` endpoint along with the existing endpoints. Setting it to empty disables the /admin endpoint altogether. Any other host:port string create that listener and only bring the admin endpoints on it. * The `admin.listeners.https.` prefix is used to find SSL props for any HTTPS endpoints.   Multiple tests are added to verify adding admin endpoints with various listeners, and to check if the REST endpoints behave as intended.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2019-09-27T07:00:53Z","2020-10-16T06:17:31Z"
"","7478","KAFKA-8992: (2.4 blocker) Redefine RemoveMembersFromGroup interface on Admin Client","This PR fixes the inconsistency involved in the `removeMembersFromGroup` admin API calls. Essentially:  1. fail the `all()` request when there is sub level error (either partition or member) 2. change getMembers() to members() 3. Hide the actual Errors from user  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-10-09T18:38:00Z","2019-10-25T07:01:53Z"
"","6482","KAFKA-7989: RequestQuotaTest should wait for quota config change before running tests","This PR fixes KAFKA-7989 and KAFKA-8123: testResponseThrottleTimeWhenBothProduceAndRequestQuotasViolated and testResponseThrottleTimeWhenBothFetchAndRequestQuotasViolated.  The failure was due to throttle-time metric for produce/consume was not updated.   The root cause of the failure is that test's `setUp()` sets quota configs for various producers/consumers, which is done through admin client, but it waited for only default and unthrottled client quotas to be updated; it did not wait for quota of smallQuotaProducerClientId and smallQuotaConsumerClientId to be updated. The failed tests are those that verify throttle metrics of smallQuotaProducerClientId and smallQuotaConsumerClientId clients. One important details is that default request quota and request quota of smallQuotaConsumer/ProducerClientId are the same == 0.01. The test failure happens if the test runs before produce/consume quota gets updated (the default request quota is already small). In this case, the request gets throttled due to violating request quota, but not produce/consume quota, while the test expects that both types of quotas are violated. The fix is for the test to wait for all quotas to be updated in `setUp` before running tests.  I was not able to reproduce the test failure locally. However, I was able to ""reproduce"" this by adding a while loop in the beginning of ClientQuotaManager#updateQuota().   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","apovzner","2019-03-21T06:40:26Z","2019-03-21T10:15:44Z"
"","6901","MINOR: Fix a wrong description in PipeDemo's javadoc","This PR fixes a wrong input stream name in PipeDemo's javadoc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-06-07T04:42:43Z","2019-07-04T08:00:27Z"
"","6899","KAFKA-8500: static member rejoin should always update member.id","This PR fixes a blocker issue for static membership. Previously we limit the `member.id` replacement in JoinGroup only when the group is in Stable. This is error-prone and could potentially allow duplicate consumers reading from the same topic, imagine a case where two unknown members join in the `PrepareRebalance` stage at the same time. The PR fixes following things:  1. Replace `member.id` at any time we see a known static member rejoins group with unknown member.id 2. Immediately fence any ongoing join/sync group callback to early terminate duplicate member 3. Clearly handle Dead/Empty cases as exception 4. Return old leader id upon static member leader rejoin to avoid trivial member assignment being triggered.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-06T22:44:40Z","2019-06-12T15:41:59Z"
"","7460","KAFKA-8104: Consumer cannot rejoin to the group after rebalancing","This PR contains the fix of race condition bug between ""consumer thread"" and ""consumer coordinator heartbeat thread"". It reproduces in many production environments.  Condition for reproducing:  1. Consumer thread initiates rejoin to the group because of commit timeout. Call of `AbstractCoordinator#joinGroupIfNeeded` which leads to `sendJoinGroupRequest`. 2. `JoinGroupResponseHandler` writes to the `AbstractCoordinator.this.generation` new generation data and leaves the` synchronized` section. 3. Heartbeat thread executes `mabeLeaveGroup` and clears generation data via `resetGenerationOnLeaveGroup`. 4. Consumer thread executes `onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment);` with the cleared generation data. This leads to the corresponding exception.  The race fixed with the condition in `maybeLeaveGroup`: if we have ongoing rejoin process in consumer thread there is no reason to reset generation data and send `LeaveGroupRequest` in heartbeat thread.  This PR contains unfair ""reproducer"". It implemented with the `CountDownLatch` that imitates described race in `AbstractCoordinator` code.  I need assistance on how should be fair reproducer implemented.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2019-10-07T18:30:30Z","2019-10-17T06:01:49Z"
"","6751","KAFKA-6455: Update integration tests to verify result timestamps","This PR contains the changes from #6725 and must be rebased after #6725 is merged.  For review, only consider the second commit of this PR.","closed","streams,","mjsax","2019-05-17T01:49:56Z","2019-05-30T16:56:52Z"
"","6692","DOCS-1266 cleanup policy update","This PR clarifies  log.cleanup.policy impact on other topics in cluster.  @guozhangwang   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","JoyceFee1","2019-05-07T21:40:28Z","2019-05-09T01:38:29Z"
"","6994","KAFKA-8590: use automated protocol for txn commit and add test coverage for offset commit","This PR changes the txn commit protocol to auto generated, and add more unit test coverage to the plain OffsetCommit protocol.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-24T18:08:13Z","2019-09-06T06:07:43Z"
"","6742","Improve performance of checkpointHighWatermarks, patch 2/2","This PR builds on apache/kafka#6741 to improve the performance of `checkpointHighWatermarks()`.  #### Commit gardnervickers@387fce0 This is a pretty invasive/significant refactor of the `CheckpointFile` code and accompanying classes. The goal was to avoid unnecessary buffering and allocation when constructing and writing out the checkpoint file. Also, I attempted to encourage inlining as much as possible. The change set is pretty drastic but the improvements are attractive.   #### Improvements over gardnervickers@4307a73  (baseline)  | Topic Count | Ops/ms | MB/sec allocated | |-------------|---------|------------------| | 100               | + 61%    |  - 481% | | 1000             | + 251% |  - 177% | | 2000            | + 211% |   - 239% |","closed","","gardnervickers","2019-05-15T22:00:40Z","2021-02-19T19:46:36Z"
"","6638","Cyrus timeout","This PR applies to the `1.0` branch two commits that are already on the `1.1` branch and later branches.  These two together make it possible for for Connect tests to have more control on how long to wait for Connect worker services to finish loading.  Original PRs are: https://github.com/apache/kafka/pull/4423 https://github.com/apache/kafka/pull/5882","closed","","cyrusv","2019-04-25T22:48:37Z","2019-04-26T19:56:36Z"
"","6712","MINOR: Align KTableAgg and KTableReduce","This PR aligns the logic of `KTableAgg` and `KTableReduce` and makes the code more readable. It also fixes some bugs in both implementations:   - 'remove' should be done before 'add'  - 'remove' should only be done if an old value exists in the store  - init() should not be called for 'remove' case if old value does not exist in store","closed","streams,","mjsax","2019-05-10T16:34:16Z","2019-05-11T10:21:15Z"
"","7122","KAFKA-8222 & KIP-345 part 5: admin request to batch remove members based on instance id","This PR adds supporting features for static membership. It could batch remove consumers from the group with provided `group.instance.id` list.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-27T06:26:25Z","2019-09-09T20:07:05Z"
"","7371","KAFKA-6290: Support casting from logical types in cast transform","This PR adds support for cast transforms to cast from logical types Date, Time, and Timestamp to their internal int32 or int64 representations. Any valid cast on their internal representations would also be applicable. For instance, Time has internal int32 representation, but can be cast to int64 if desired.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ncliang","2019-09-19T17:15:24Z","2020-10-16T06:17:30Z"
"","6938","KAFKA-8557: system tests - add support for (optional) interbroker listener with the same security protocol as client listeners","This PR adds an option to `kafka.py` to start a separate interbroker listener using the same security protocol as one of the predefined listeners in 'port_mappings' dict. This is a first baby-step to properly support named listeners in system tests.  Kafka currently supports named listeners, where you can have two or more listeners with the same security protocol but different names. Current `KafkaService` implementation, however, wouldn't allow that, since listeners in `port_mappings` are keyed by `security_protocol`, so there's 1-1 relationship. Clients use `bootstrap_servers` method which also accepts security_protocol.  This PR adds a parameter `use_separate_interbroker_listener` to KafkaService constructor as well as adds a `setup_interbroker_listener()` method, which, if provided/invoked, will enable a separate listener in `port_mappings` map, keyed by `INTERBROKER_LISTENER_NAME`. This way client implementations don't need to change and can still pick a port based on security protocol, but underneath there will be an additional port open used exclusively for broker-to-broker communication. This opens up additional scenarios for testing.  This change should be backwards compatible - default value for `use_separate_interbroker_listener` is `False`, and I tried to make sure that behavior of `KafkaService` doesn't change when it's `False`. Even when it is `True`, it should not affect behavior of clients, which, as described above, would continue to pick ports based on security_protocol field.  I intentionally didn't change behavior more than this. We'll work on adding wider support for named listeners, but its better to make smaller incremental changes rather than trying to boil the ocean.  **Testing:** Updated `security_rolling_upgrade_test` to include scenarios when separate interbroker listener is present. Ran these subset of these scenarios locally (didn't run all security_protocol permutations, just some of them) using `ducker-ak`. Ran updated `security_rolling_upgrade_test` in branch builder, all passed: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2713/  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","stan-confluent","2019-06-13T23:14:07Z","2019-06-21T16:51:44Z"
"","7422","MINOR: avoid unnecessary gettimeofday in ReplicaFetcher","This PR adds a Replica Fetcher benchmark for use in improving the performance and CPU usage of the ReplicaFetcher. In this initial PR I chose to only address one minor performance issue - unnecessary gettimeofday calls in steady state (when partitions are not delayed). I realize there are ways to improve the performance of gettimeofday calls, but it does require action on the part of the user, so I believe this optimization is still worthwhile. I have further performance optimizations that I will address in future PRs once the benchmark has been added.  Benchmarked on r5.xlarge: ``` Before: Benchmark                                  (partitionCount)  Mode  Cnt        Score       Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    28406.454 ±   614.140  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   144999.326 ±  6774.986  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   316551.913 ±  3331.297  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  2507985.580 ± 12476.098  ns/op  After: Benchmark                                  (partitionCount)  Mode  Cnt        Score       Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    21877.395 ±   670.745  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   123234.307 ±  4083.449  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   297592.869 ±  2629.679  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  2258424.746 ± 10646.843  ns/op ```","closed","","lbradstreet","2019-09-30T17:57:39Z","2019-10-01T20:17:03Z"
"","7285","KAFKA-8558:  Add StreamJoined config object to join","This PR adds `StreamJoined` to a `Streams` - `Streams` join, allowing to separate the naming of the join state stores from the name of the join processor.  See [KIP-479 for more details](https://cwiki.apache.org/confluence/display/KAFKA/KIP-479%3A+Add+StreamJoin+config+object+to+Join).  For testing some of the existing tests were updated and additional tests added verifying the expected behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","bbejeck","2019-09-03T15:21:17Z","2020-06-12T23:35:32Z"
"","7186","MINOR: Use max retries for consumer group tests to avoid flakiness","This patch updates ConsumerGroupCommandTest.scala to use the maximum possible number of AdminClient retries. The test runs will still be bounded by the request timeout. This address flakiness in tests such as testResetOffsetsNotExistingGroup and testResetOffsetsExistingTopic, which was caused by group coordinators being intermittently unavailable.   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-08-09T18:12:51Z","2019-08-15T20:47:57Z"
"","6944","MINOR: Simplify controller election utilities","This patch simplifies the controller election API. We were passing `LeaderIsrAndControllerEpoch` into the election utilities even though we just needed `LeaderAndIsr`. We also remove some unneeded collection copies `doElectLeaderForPartitions`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-15T07:58:55Z","2019-06-17T21:30:06Z"
"","7121","KAFKA-8721: Kafka metrics improvements","This patch includes the upgrade to a new version of the metrics library used in Kafka so that we are able to add new features on its metrics.  All tests metrics-related in the project have been updated.  Notice that: * ``yammer-metrics-count`` metric in ``KafkaServer`` has been renamed to ``dropwizard-metrics-count``. * All meters and timers have SECONDS as a rate-unit. Previously, all meters in Kafka had the rate unit in seconds except the meter ``*RequestHandlerAvgIdlePercent`` (in ``KafkaRequestHandlerPool``) which now is in seconds as well (before it was in nanosecs). * The property ``eventType`` in JMXReporter MBean doesn't exist anymore. So, all meters don't have this MBean property. * I've done a refactor in the metrics trying to encapsulate a little bit the metrics inside the ``kafka.metrics`` package. * Currently, the PR just has 2 types of reservoirs for the histogram metric. This might be extended to support the other reservoirs included in this metrics library version.  This PR implements KIP-510  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mmolimar","2019-07-26T23:53:03Z","2019-10-30T16:31:04Z"
"","6731","KAFKA-8365 Consumer support for follower fetch","This patch includes API changes for follower fetching per [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) as well as the consumer implementation. After this patch, consumers will continue to fetch only from the leader, since the broker implementation to select an alternate read replica is not included here.  Adds new `client.rack` consumer configuration property is added which allows the consumer to indicate its rack. This is just an arbitrary string to indicate some relative location, it doesn't have to actually represent a physical rack. We are keeping the naming consistent with the broker property (`broker.rack`).  FetchRequest now includes `rack_id` which can optionally be specified by the consumer. FetchResponse includes an optional `preferred_read_replica` field for each partition in the response. OffsetForLeaderEpochRequest also adds new `replica_id` field which is similar to the same field in FetchRequest.  When the consumer sees a `preferred_read_replica` in a fetch response, it will use the Node with that ID for the next fetch.","closed","","mumrah","2019-05-14T19:39:53Z","2019-05-18T05:45:46Z"
"","7198","MINOR: Greedily initialize allTopicStats BrokerTopicMetrics","This patch fixes the quota system test whose JMX tool relies on the metrics existing.","closed","","stanislavkozlovski","2019-08-12T15:04:19Z","2019-08-28T10:40:57Z"
"","7290","MINOR: Use toString in Records implementations only for summary metadata","This patch fixes a couple problems with logging of request/response objects which include records data. First, it adds a missing `toString` to `LazyDownConversionRecords`. Second, it changes the `toString` of `MemoryRecords` to not print record-level information. This was always a dangerous practice, but it was especially bad when these objects ended up in request logs. With this patch, implementations use `toString` only to print summary details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-03T21:37:40Z","2019-09-06T07:54:54Z"
"","6613","KAFKA-8248; Ensure time updated before sending transactional request","This patch fixes a bug in the sending of transactional requests. We need to call `KafkaClient.send` with an updated current time. Failing to do so can result in an `IllegalStateExcepton` which leaves the producer effectively dead since the in-flight correlation id has been set, but no request has been sent. To avoid the same problem in the future, we update the in flight correlationId only after sending the request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-20T07:59:23Z","2019-05-02T16:29:22Z"
"","6570","KAFKA-7866; Ensure no duplicate offsets after txn index append failure","This patch fixes a bug in the append logic which can cause duplicate offsets to be appended to the log when the append to the transaction index fails. Rather than incrementing the log end offset after the index append, we do it immediately after the records are written to the log. If the index append later fails, we do two things:  1) We ensure that the last stable offset cannot advance. This guarantees that the aborted data will not be returned to the user until the transaction index contains the corresponding entry. 2) We skip updating the end offset of the producer state. When recovering the log, we will have to reprocess the log and write the index entries.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-12T09:19:48Z","2019-04-18T15:41:33Z"
"","6626","KAFKA-8332: Preserve order of protocols when handling JOIN_GROUP requests","This patch fixes a bug in JOIN_GROUP handling. Currently, because ImplicitLinkedHashSet implements Set, invoking `joinGroupRequest.data().protocols().asScala.map` returns a Set and can cause the protocols to lose their ordering. By changing ImplicitLinkedHashSet to implement Collection but not Set, the ordering is preserved when invoking `map`. Tested with unit tests and ConsumerRollingUpgradeTest, which intermittently fails under the old behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-04-24T07:35:51Z","2019-05-09T18:08:23Z"
"","7280","MINOR: Add system configuration to zk security exception messages","This patch ensures that relevant system configurations are included in exception messages when zk security validation fails.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-02T18:25:33Z","2019-09-04T06:16:34Z"
"","7332","KAFKA-8908; Use ApiVersion request for inter-broker requests","This patch enables the usage of version discovery for inter-broker requests using the ApiVersions API. This is only used when `inter.broker.protocol.version` is high enough to support this. Otherwise, we use the existing static version determination based on `inter.broker.protocol.version`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","hachikuji","2019-09-14T18:38:55Z","2019-12-07T23:35:16Z"
"","7136","MINOR: Small cleanups in TopicCommand describe handling","This patch contains a few small cleanups to make topic describe logic a little clearer. It also fixes a minor inconsistency in the output between the --zookeeper and --bootstrap-server behavior when the leader is unknown. Previously we printed -1 when --zookeeper was used, and now we print ""none."" The patch consolidates the output logic for describing topics and partitions in order to avoid inconsistencies like this in the future.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-30T08:05:45Z","2019-07-31T15:27:39Z"
"","7264","KAFKA-8724; Improve range checking when computing cleanable partitions","This patch contains a few improvements on the offset range handling when computing the cleanable range of offsets.  1. It adds bounds checking to ensure the dirty offset cannot be larger than the log end offset. If it is, we reset to the log start offset. 2. It adds a method to get the non-active segments in the log while holding the lock. This ensures that a truncation cannot lead to an invalid segment range. 3. It improves exception messages in the case that an inconsistent segment range is provided so that we have more information to find the root cause.  The patch also fixes a few problems in `LogCleanerManagerTest` due to unintended reuse of the underlying log directory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-08-28T17:12:02Z","2019-09-05T16:10:53Z"
"","6773","MINOR: A few logging improvements in the broker","This patch contains a few fixes for log messages which tend to cause a lot of confusion/noise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-20T16:18:23Z","2019-05-21T21:50:25Z"
"","6814","KAFKA-8400; Do not update follower replica state if the log read failed","This patch checks for errors handling a fetch request before updating follower state. Previously we were unsafely passing the failed `LogReadResult` with most fields set to -1 into `Replica` to update follower state. Additionally, this patch attempts to improve the test coverage for ISR shrinking and expansion logic in `Partition`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-25T01:03:24Z","2019-06-05T21:36:45Z"
"","7406","KAFKA-8954; Topic existence check is wrongly implemented in the DeleteOffset API (KIP-496)","This patch changes the way topic existence is checked in the DeleteOffset API. Previously, it was relying on the committed offsets. Now, it relies on the metadata cache which is better.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-09-27T14:22:58Z","2020-08-11T06:49:52Z"
"","7085","KAFKA-8635: Skip client poll in Sender loop when no request is sent","This patch changes maybeSendTransactionalRequest to handle both sending and polling transactional requests (and renames it to maybeSendAndPollTransactionalRequest), and skips the call to poll if no request is actually sent. It also removes the inner loop inside maybeSendAndPollTransactionalRequest and relies on the main Sender loop for retries.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-07-12T22:01:43Z","2019-07-18T22:07:53Z"
"","6705","KAFKA-8371: Remove dependence on ReplicaManager from Partition","This patch attempts to simplify the interaction between Partition and the various components from `ReplicaManager`. This is primarily to make unit testing easier. I have also tried to eliminate the OfflinePartition sentinel which has always been unsafe.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-09T20:57:54Z","2019-05-23T20:20:40Z"
"","7435","Minor: Add JUnit Foundation for setting default junit timeouts","This patch adds the JUnit Foundation library which does annotation munging to set a default timeout value for JUnit tests.","closed","","mumrah","2019-10-02T19:33:11Z","2019-10-03T20:44:50Z"
"","7453","KAFKA-8985; Add flexible version support to inter-broker APIs","This patch adds flexible version support for the following inter-broker APIs: ControlledShutdown, LeaderAndIsr, UpdateMetadata, and StopReplica.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-07T04:24:03Z","2019-10-07T16:21:15Z"
"","7320","KAFKA-8747: Fix the flaky ControllerEventManagerTest#testEventQueueTimeQuery test","This patch adds an atomic counter in the test to ensure we have processed all the events before we assert the metrics. There was a race condition with the previous assertion, which asserted that the event queue is empty before checking the metrics.","closed","","stanislavkozlovski","2019-09-10T17:44:37Z","2019-09-11T00:29:15Z"
"","7433","KAFKA-8264: Log more information on ConsumerTest consume timeout","This patch adds additional logging information on timeout of the AbstractConsumerTest#consumeRecords method. It is difficult to pinpoint the exact issue of timeouts in this method with flaky tests due to the difficulty in reproducing it. Adding more information will help us diagnose whether the consumer was constantly polling for records or whether it was stuck rebalancing.","open","","stanislavkozlovski","2019-10-02T13:37:23Z","2019-10-02T13:37:23Z"
"","7318","KAFKA-8264: Log more information on ConsumerTest consume timeout","This patch adds additional logging information on timeout of the `AbstractConsumerTest#consumeRecords` method. It is difficult to pinpoint the exact issue of timeouts in this method with flaky tests due to the difficulty in reproducing it. Adding more information will help us diagnose whether the consumer was constantly polling for records or whether it was stuck rebalancing.","closed","","stanislavkozlovski","2019-09-09T23:52:59Z","2019-10-02T13:38:19Z"
"","7339","MINOR: Refactor controller partition reassignment logic into separate class","This patch adds a ReassignmentManager class which encapsulates most of the nitty-gritty details of reassigning a partition. Splitting the logic helps with testability and this patch leverages that to add unit tests for partition reassignments","open","","stanislavkozlovski","2019-09-16T17:30:53Z","2019-11-27T08:45:52Z"
"","7181","MINOR: Add log message when deleting topics","This patch adds a log message that indicates what topics are attempting deletion","closed","","stanislavkozlovski","2019-08-08T21:15:40Z","2020-06-08T07:26:39Z"
"","6569","KAFKA-6647 KafkaStreams.cleanUp creates .lock file in directory its trying to clean (Windows OS)","This KAFKA-6647 KafkaStreams.cleanUp creates .lock file in directory its trying to clean (Windows OS) has been open for a while, and long conversation in previous Pull Requests. Anyway this problems is causing the TopologyTestDriver driver based unit test failing in Windows if not adding extra exception handling in there.  This PR version is trying to keep the general functionality as similar as earlier. Only add one extra retry of delete, if first failed due to DirectoryNotExmptyException. When added the retry logic only at the end of finally, caused checkstyle CyclomaticComplexity and NPathComplexity to go above threshold. After it extracted cleanRemovedTaskDir and deleteTaskDir methods to avoid complexity. Also time condition evaluation changed to be first before locking, so no need to lock if inner block is doing nothing.   No external functionality changed, so no additional test cases added.  Following five test in StateDirectoryTest failed earlier in Windows. StateDirectoryTest.shouldCleanUpTaskStateDirectoriesThatAreNotCurrentlyLocked StateDirectoryTest.shouldCleanupStateDirectoriesWhenLastModifiedIsLessThanNowMinusCleanupDelay StateDirectoryTest.shouldNotLockStateDirLockedByAnotherThread StateDirectoryTest.shouldNotUnLockStateDirLockedByAnotherThread StateDirectoryTest.shouldCleanupAllTaskDirectoriesIncludingGlobalOne  Test shouldNotLockStateDirLockedByAnotherThread is still failing due to there is no way to unlock task of already dead thread without adding extra code to StateDirectory class. The test left as is, but explanatory comment added. The rest four of those five tests are now successful also in Windows.   Also shouldUseSerdesDefinedInMaterializedToConsumeGlobalTable test in StreamsBuilderTest failed in Windows before this change. After the change all the tests in StreamsBuilderTest are succesful.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","jukkakarvanen","2019-04-12T08:54:11Z","2019-05-14T06:34:23Z"
"","7078","(DO NOT MERGE) Kip 447 POC","This is the POC diff for overall KIP-447 implementation. Domains of changes are: Server side: - Implement consumer back-off logic with pending offsets (GroupMetadataManager.scala) - Implement generation fencing for txn offset commit (GroupCoordinator.scala)  Client side: Init stage: - decide the type of producer to be initialized (StreamThread.java) - Setup task producer new initTransaction(consumer) API  (KafkaProducer.java) - Initialize thread txn producer before run loop  (StreamThread.java#maybeInitializeTransactions) - Initialize transactional.id with consumer group states (KafkaProducer.java)  Running Stage: - add handle for thread producer to get necessary offsets to commit (Task.java#getPendingOffsets) - Refactored maybeCommitPerUserRequested to share the same commit template with super class (AssignedStreamTasks.java) - Augment thread commit logic to be able to aggregate offsets, batch commit txn offsets, and restart transaction session (AssignedTasks.java#commit) - Replace `eosEnabled` logic with new flag `isTaskProducer` as necessary (search isTaskProducer in codebase)  External API: - Add exposure for Consumer group metadata (Consumer/ConsumerCoordinator/AbstractCoordinator.java) - Some automated protocol dependencies for OffsetFetch and TxnOffsetCommit   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-12T02:02:53Z","2020-05-08T22:21:53Z"
"","7156","KAFKA-8753: expose the number of topics marked for deletion","This is the implementation for [KIP-503](https://cwiki.apache.org/confluence/display/KAFKA/KIP-503%3A+Add+metric+for+number+of+topics+marked+for+deletion)  When deleting a large number of topics, the Controller can get quite bogged down. One problem with this is the lack of visibility into the progress of the Controller. We can look into the ZK path for topics marked for deletion, but in a production environment this is inconvenient. This PR adds a JMX metric `kafka.controller:type=KafkaController,name=TopicsToDeleteCount` to make it easier to see how many topics are being deleted.","closed","","mumrah","2019-08-02T19:05:57Z","2019-08-23T20:22:42Z"
"","7293","[DO NOT MERGE]  Make new Authorizer API asynchronous","This is still being discussed under KIP-504, so not ready for merge yet.  The PR is currently just to help with review of the API.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-04T10:28:40Z","2019-09-08T17:52:43Z"
"","7195","MINOR: Remove throttling logic from RecordAccumulator","This is redundant since `Sender` and `NetworkClient` handle throttling. It's also confusing since the `RecordAccumulator` logic only applies when `max.in.flight.requests.per.connection=1`.  In `Sender.sendProducerData`, the following code handles throttling:  ```java while (iter.hasNext()) {     Node node = iter.next();     if (!this.client.ready(node, now)) {         iter.remove();         notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));     } } ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-11T16:15:30Z","2020-03-09T18:05:20Z"
"","7378","KAFKA-8233: KIP-470: TopologyTestDriver test input and output usability improvements","This is PR for KIP-470: TopologyTestDriver test input and output usability improvements: https://cwiki.apache.org/confluence/display/KAFKA/KIP-470%3A+TopologyTestDriver+test+input+and+output+usability+improvements  A couple question added as Review comment.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","kip,","jukkakarvanen","2019-09-24T02:52:37Z","2020-06-12T23:34:25Z"
"","7261","KAFKA-6460: Add mocks for state stores used in Streams unit testing.","This is more of a POC PR. Added a MockStoreFactory. Added an implementation for MockKeyValueStoreBuilder. Added an implementation for a MockKeyValueStore (skeleton codes from previous MockKeyValueStore)  KIP-448: Add State Stores Unit Test Support to Kafka Streams Test Utils https://cwiki.apache.org/confluence/display/KAFKA/KIP-448%3A+Add+State+Stores+Unit+Test+Support+to+Kafka+Streams+Test+Utils  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","shunge","2019-08-27T23:03:15Z","2019-10-14T18:05:10Z"
"","6927","POC: use Materialized as an adapter","This is just a POC...  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2019-06-12T19:38:46Z","2019-06-13T21:18:57Z"
"","7095","KAFKA-8179: Minor, add ownedPartitions to PartitionAssignor#subscription","This is just a just a minor PR for KIP-429, which needs `ConsumerCoordinator` to pass the current assignment to the assignor when building the subscription.  Since this is in the internal package and a cursory search of github suggests not too many people have implemented this method, we just change the signature rather than add a new method and deprecate the old one.","closed","","ableegoldman","2019-07-16T00:36:21Z","2019-07-24T17:53:23Z"
"","6785","KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator","This is inspired by the original work of https://github.com/apache/kafka/pull/6699 by @Flowermin. It tries to achieve the same CPU savings by reducing unnecessary byte allocation and corresponding GC. Unlike #6699 though, which depends on `skipBytes` of LZ4 which used a shared byte array, in this PR we create a `skip buffer` outside of the compressed input stream. The reason is that not all compressed inputstream's implementation is optimized, more specifically:  1. GZIP used BufferedInputStream, which has a shared buffer, sized 16KB 2. SNAPPY used its own SnappyInputStream -> InputStream, which dynamically allocate 3. LZ4 used its own KafkaLZ4BlockInputStream, which has a shared buffer of 64KB 4. ZSTD used its own ZstdInputStream, but it's own overriden `skip` also dynamically allocate  The detailed implementation can be summarized as follows:  1. Add skipKeyValueIterator() into DefaultRecordBatch, used in LogValidator; also added PartialDefaultRecord which extends DefaultRecord.  1.a. In order make this optimization really effective, we also need to refactor the LogValidator to refactor part of the validation per record into the outer loop so that we do not need to update inPlaceAssigment inside the loop any more. And then based on this boolean we can decide whether or not to use skipKeyValueIterator or not before the loop.  1.b. Also used streaming iterator instead when skip-iterator cannot be used.  2. With SkipKeyValueIterator, pre-allocate a skip byte array with fixed size (2KB), and use this array to take the decompressed bytes through each record, validating metadata and key / value / header size, while skipping the key / value bytes.  3. Also tighten the unit tests of LogValidator to make sure scenarios like mismatched magic bytes / multiple batches per partition / discontinuous offsets / etc are indeed validated.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-22T04:17:45Z","2020-04-24T23:50:01Z"
"","7070","KAFKA-7157: Fix handling of nulls in TimestampConverter","This is based on the PR from @Nimfadora, with some minor cleanup: https://github.com/apache/kafka/pull/6446  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rayokota","2019-07-10T21:39:20Z","2020-10-16T06:17:29Z"
"","6988","KAFKA-7548: KafkaConsumer should not throw away already fetched data for paused partitions (v2)","This is an updated implementation of #5844 by @MayureshGharat (with Mayuresh's permission).  > Today when we call KafkaConsumer.poll(), it will fetch data from Kafka asynchronously and is put in to a local buffer (completedFetches). > > If now we pause some TopicPartitions and call KafkaConsumer.poll(), we might throw away any buffered data that we might have in the local buffer for these TopicPartitions. Generally, if an application is calling pause on some TopicPartitions, it is likely to resume those TopicPartitions in near future, which would require KafkaConsumer to re-issue a fetch for the same data that it had buffered earlier for these TopicPartitions. This is a wasted effort from the application's point of view.  I've reviewed the original PR's feedback from @hachikuji and reimplemented this solution to add completed fetches that belong to paused partitions back to the queue.  I also rebased against the latest trunk which caused more changes as a result of subscription event handlers being removed from the fetcher class.  You can find more details in my updated notes in the original Jira issue [KAFKA-7548](https://issues.apache.org/jira/browse/KAFKA-7548).","closed","","seglo","2019-06-23T23:52:22Z","2019-08-02T00:10:20Z"
"","7465","KAFKA-5050: SASL/PLAIN server callback handler which authenticates against LDAP","This is an implementation of PlainServerCallbackHandler which allows Kafka clients to authenticate against LDAP through SASL/PLAIN.  Following is a sample Kafka client properties which will allow LDAP authentication through SASL/PLAIN: **sasl.mechanism=PLAIN security.protocol=SASL_SSL sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""ldap_uid""  password=ldap_password""""; ssl.truststore.location=......**  The following is a sample KafkaServer jaas config:  **org.apache.kafka.common.security.plain.PlainLoginModule required ldap_url=""ldaps://ldapserver:636"" user_dn_template=""uid={0},cn=users,cn=accounts,dc=firm,dc=site"";**  The following is the sample kafka.properties allow the server to use this callback handler when SASL/PLAIN over SSL is enabled: **listener.name.sasl_ssl.plain.sasl.server.callback.handler.class=org.apache.kafka.common.security.ldap.internals.LdapPlainServerCallbackHandler**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","in-park","2019-10-08T09:52:54Z","2019-10-21T13:01:01Z"
"","7365","DRAFT: Controller should fence responses from brokers with older epochs","This is an experimental change. It is possible for the controller to receive responses from brokers in an older epoch. For example, this can happen if the controller has a large backlog of events at the time the broker is restarted. When this happens, we need to be a little careful about acting on these responses. It is probably best to just ignore them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","hachikuji","2019-09-18T18:59:21Z","2019-09-18T18:59:21Z"
"","7233","KAFKA-8421: Pt1, Use different connections for group coordinator [WIP]","This is a pre-requisite for KAFKA-8421 to use different connections for commit / fetch offsets and rebalance protocol.  Some unit tests still need to be fixed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-08-21T17:45:16Z","2020-04-24T23:58:14Z"
"","7442","KAFKA-8974: trim whitespaces in topic","This is a minor change that will improve the UX when the topic list includes whitespace in the `topics` config for a SinkConnector  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-10-03T22:53:58Z","2020-10-16T06:17:31Z"
"","7463","MINOR: unify calls to get committed offsets and metadata","This is a follow up to #7304 and #6694 to unify the calls to `consumer.committed()` further and only call it once during startup instead of twice.","closed","streams,","mjsax","2019-10-08T06:23:06Z","2019-10-15T04:38:44Z"
"","7206","KAFKA-8789: kafka-console-consumer timeout-ms setting behaves incorrectly with older client","This is a draft implementation of [KAFKA-8789](https://issues.apache.org/jira/browse/KAFKA-8789). I dug out the problem and found the following.  Before 2.1.0 (Confluent Platform 5.1.x, 8a78d764), `ConsoleConsumer.ConsumerWrapper#recordIter` was initialized with `consumer.poll(0)`, which in turn calls `KafkaConsumer#poll(Timer, includeMetadataInTimeout = false)` and fetches metadata regardless of timeout.  However, this code was removed while removing the deprecated `KafkaConsumer#poll(long)`. So, `ConsoleConsumer.ConsumerWrapper#receive` now fetches metadata just before starting consuming, with timeout limitation. If an insufficient timeout is given, it calls the following methods and finally ends with `TimeoutException`.  `KafkaConsumer#poll(Duration, boolean)` -> `KafkaConsumer#updateAssignmentMetadataIfNeeded(Timer)` -> `KafkaConsumer#updateFetchPositions` -> `ConsumerCoordinator#refreshCommittedOffsetsIfNeeded` -> `ConsumerCoordinator#fetchCommittedOffsets` -> `ConsumerNetworkClient#poll(RequestFuture, Timer)` -> `ConsumerNetworkClient#poll(Timer, PollCondition)` -> `ConsumerNetworkClient#poll(Timer, PollCondition, boolean)` -> `ConsumerNetworkClient#failExpiredRequests`  To make this tool to work like before 2.1.0, we need to add a method to update `KafkaConsumer`'s metadata before starting polling. I can't be certain what approach would be the best, I added `Consumer#poll(Duration, boolean)` as a temporal workaround. (That is, this PR will be updated following the related KIP discussion.)  cc/ @hachikuji @viktorsomogyi @ijuma @HeartSaVioR  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2019-08-13T14:17:02Z","2020-09-13T13:00:51Z"
"","6766","KAFKA-7869: Refactor RocksDBConfigSetter API to separate DBOptions and CFOptions","This is a draft implementation of [KAFKA-7869](https://issues.apache.org/jira/browse/KAFKA-7869). @guozhangwang Is this what you intended?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","dongjinleekr","2019-05-19T21:29:30Z","2020-06-12T23:40:44Z"
"","6790","KAFKA-8219: add doc changes for static membership release","This is a doc-change only PR to add upgrade notes for static membership in 2.3","closed","","abbccdda","2019-05-22T19:49:58Z","2019-05-27T21:26:53Z"
"","7101","KAFKA-8678: fix leave group protocol bug in throttling and error response","This is a bug fix PR to resolve errors introduced in https://github.com/apache/kafka/pull/6188.  The PR fixes 2 things: 1. throttle time should be set on version >= 1 instead of version >= 2 2. `getErrorResponse` should set throwable exception within LeaveGroupResponseData  In the meanwhile, adding more unit tests to guarantee correctness for leave group protocol.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-18T06:03:08Z","2019-07-22T23:13:03Z"
"","6718","MINOR: Use request version to ensure metadata reflects subscription changes","This is a backport to 2.2 of a fix which was merged in KAFKA-7831. When there is a subscription change with a metadata request in flight, we may incorrectly interpret the response as applying to the changed subscription. The fix here is to track a separate request version in `Metadata` so that we do not confuse the topic set that was requested.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-11T01:37:02Z","2019-05-13T15:36:22Z"
"","7051","KAFKA-8026: Fix for Flaky RegexSourceIntegrationTest","This is a 2.3-idiomatic recreation of Bill Bejeck 's [original patch](https://github.com/nexiahome/kafka/commit/92c591d) for the 1.1 branch, that seems to have been inadvertently omitted. I'm not sure if this PR ought to be against the 2.3 branch, or trunk; it should apply cleanly to either.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  This change prevents tests from sharing a single KafkaStreams instance across tests, and closes each tests's individual KafkaStreams instance before proceeding to the next test.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  I ran the RegexSourceIntegrationTest 20 times in isolation, and was able to reproduce the flakiness before, and unable to reproduce after this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","emschwar","2019-07-08T22:21:42Z","2019-09-12T16:59:34Z"
"","7120","KAFKA-8345: KIP-455: Admin API changes (Part 2)","This implements the needed ListPartitionReassignments and  AlterPartitionReassignments AdminClient APIs, KafkaAdminClient implementation and tests for KIP-455  Previous PR: https://github.com/apache/kafka/pull/7114","closed","","stanislavkozlovski","2019-07-26T17:13:33Z","2019-08-14T16:44:24Z"
"","6746","KAFKA-8376; Least loaded node should consider connections which are being prepared","This fixes a regression caused by KAFKA-8275. The least loaded node selection should take into account nodes which are currently being connect to. This includes both the CONNECTING and CHECKING_API_VERSIONS states since `canSendRequest` would return false in either case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-16T16:00:21Z","2019-05-16T20:37:19Z"
"","7289","KAFKA-8853: Create sustained connections test for Trogdor","This creates a test that generates sustained connections against Kafka.  There are three different components we can stress with this, KafkaConsumer, KafkaProducer, and AdminClient.  This test tries use minimal bandwidth per connection to reduce overhead impacts.  This test works by creating a threadpool that creates connections and then maintains a central pool of connections at a specified keepalive rate.  The keepalive action varies by which component is being stressed:    * KafkaProducer:  Sends one single produce record.  The configuration for     the produce request uses the same key/value generator as the ProduceBench     test.    * KafkaConsumer: Subscribes to a single partition, seeks to the end, and     then polls a minimal number of records.  Each consumer connection is its     own consumer group, and defaults to 1024 bytes as FETCH_MAX_BYTES to keep     traffic to a minimum.    * AdminClient: Makes an API call to get the nodes in the cluster.  NOTE: This test is designed to be run alongside a ProduceBench test for a specific topic, due to the way the Consumer test polls a single partition. There may be no data returned by the consumer test if this is run on its own. The connection should still be kept alive, but with no data returned.","closed","","scott-hendricks","2019-09-03T21:13:20Z","2019-09-09T02:49:26Z"
"","6765","KAFKA-8206: Allow consumer and producer to rebootstrap","This commit allows a consumer and a producer to repopulate their cluster metadata cache in case no nodes of currently cached are available.  This is useful in the following situation. 1. A cluster was partially available (including some bootstrap nodes were offline) when a client (a producer or consumer) was created. 2. The client used the bootstrap list to get the list of available cluster nodes and cached this list in its metadata cache. 3. All the nodes cached by the client (i.e., the nodes that were available during the bootstrap) went offline. 4. Nodes that were offline during the bootstrap became available.  Before this commit, the client was not able to discover nodes that had become available and could not continue working, despite the cluster was alive. This commit makes this possible by rerunning the bootstrapping process by the client.  This commit also tests this scenario by reproducing the situation with a partially available cluster for a consumer and a producer and ensuring they can operate normally (i.e., can consume and produce messages).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ivanyu","2019-05-19T19:16:44Z","2020-09-26T16:47:20Z"
"","7205","[MINOR] Fix comment about ReplicaFetcherThread","This comment is misleading, as this was fixed during the consolidation of fetcher threads.  The relevant code was removed here https://github.com/apache/kafka/commit/f6890e78687afbbe09ff34b0a9383b548be77ee6#diff-a8437f241a5ae585d5805ee769313080L231-L235 so this comment is out of date.  As this is a comment change, I don't think there's a need for testing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dignan","2019-08-13T13:56:35Z","2020-10-18T14:16:53Z"
"","7389","KAFKA-8805: Bump producer epoch on recoverable errors","This change is the client-side part of KIP-360. It identifies cases where it is safe to abort a transaction, bump the producer epoch, and allow the application to continue without closing the producer. In these cases, when KafkaProducer.abortTransaction() is called, the producer actually sends an InitProducerId request, which causes the ongoing transaction to be aborted and the producer epoch to be bumped. The application can then start a new transaction and continue processing.  For recoverable errors in the idempotent producer, the epoch is bumped locally. In-flight requests for partitions with an error are rewritten to reflect the new epoch, and in-flights of all other partitions are allowed to complete using the old epoch.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-09-25T15:55:00Z","2020-02-16T06:47:11Z"
"","6627","Minor: refactor list offsets usage in Fetcher","This change introduces a ListOffsetsClient which decouples the Fetcher from the ListOffsets API. This was done as follow-on work to KAFKA-7747.","open","","mumrah","2019-04-24T14:01:36Z","2019-04-24T15:51:28Z"
"","6505","KAFKA-8030: Fix flaky tests in TopicCommandWithAdminClientTest","This change adds waits for metadata updates after killing the broker in order to make the tests more stable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-03-27T13:21:13Z","2019-04-04T15:11:35Z"
"","7269","MINOR. implement --expose-ports option in ducker-ak","This change adds a command line option to the `ducker-ak up` command to enable exposing ports from docker containers. The exposed ports will be mapped to the ephemeral ports on the host. The option is called `expose-ports` and can take either a single value (like 5005) or a range (like 5005-5009). This port will then exposed from each docker container that ducker-ak sets up.  Here is one command line example:  > ./docker/ducker-ak up --expose-ports 5005  After doing this, docker will report mapped ports. Here is example output of `docker ps` command:  CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS                PORTS                     NAMES d470b708391d        ducker-ak-openjdk-8               ""/bin/sh -c 'sudo se…""   16 minutes ago      Up 16 minutes         0.0.0.0:32845->5005/tcp   ducker14 4a25ae952f28        ducker-ak-openjdk-8               ""/bin/sh -c 'sudo se…""   16 minutes ago      Up 16 minutes         0.0.0.0:32844->5005/tcp   ducker13 d7b8dad9259d        ducker-ak-openjdk-8               ""/bin/sh -c 'sudo se…""   16 minutes ago      Up 16 minutes         0.0.0.0:32843->5005/tcp   ducker12 ...  Tested by launching a bin/kafka-topics.sh command and then verifying that I can attach the debugger successfully to the launched binary.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-08-28T23:21:29Z","2019-09-09T14:57:30Z"
"","7042","MINOR: Split at first occurrence of '=' in kafka.py props parsing (cherry-pick on 2.0)","This also cherry picks the fix that was missed when #6949 cherry-picked the changes introduced by #5226 to branch 2.0. The original message of the fix is as follows:   This is a fix to #5226 to account for config properties that have an equal char in the value.   Otherwise if there is one equal char in the value the following error occurs:  dictionary update sequence element #XX has length 3; 2 is required  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-07-08T12:30:11Z","2019-07-08T21:34:48Z"
"","6772","KAFKA-8395: Add the ability to back up segment files on truncation","This adds the ability to rename/back up segment files on truncation to zero. This is useful in the rare case that offset conflict resolution results in an offset reset, which can result in data loss.  To enable, turn set `segment.backup.on.truncate.to.zero` to true in the configuration.  We have unit tests included here and are starting to stress test a working cluster to see if we can reproduce the issue in a live environment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","axiak","2019-05-20T15:53:22Z","2019-05-20T18:35:42Z"
"","6611","KAFKA-8259: Build an RPM for easier installation","This adds support for building an RPM to Kafka.  RPMs are a standard installation method for a lot of open source software.  This also provides a handy parameter for CI systems to override `rpmRelease` which is the build version.  It will also automatically track the project version.  I have built the RPM with this and installed it for a working system.","open","","dignan","2019-04-19T18:00:31Z","2019-07-04T12:33:28Z"
"","7276","KAFKA-8730: Add API to delete consumer offsets (KIP-496)","This adds an administrative API to delete consumer offsets of a group as well as extends the mechanism to expire offsets of consumer groups.  It makes the group coordinator aware of the set of topics a consumer group (protocol type == 'consumer') is actively subscribed to, allowing offsets of topics which are not actively subscribed to by the group to be either expired or administratively deleted. The expiration rules remain the same.  For the other groups (non-consumer), the API allows to delete offsets when the group is empty and the expiration remains the same.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-08-30T09:35:26Z","2019-09-14T09:21:51Z"
"","6912","KAFKA-8519 Add trogdor action to slow down a network","This adds a new Trogdor fault spec for inducing network latency on a network device for system testing. It operates very similarly to the existing network partition spec by executing the `tc` linux utility. This utility was already available in our Docker image so no change was needed there.  The spec JSON looks like: ```json {   ""class"": ""org.apache.kafka.trogdor.fault.DegradedNetworkFaultSpec"",   ""startMs"": 0,   ""durationMs"": 60000,   ""nodeSpecs"": {     ""node1"": {""latencyMs"": 500, ""networkDevice"": ""eth0""},     ""node2"": {""latencyMs"": 400, ""networkDevice"": ""eth0""},   } } ```  The executed command looks like: ``` sudo tc qdisc add dev eth0 root netem delay 100ms 10ms distribution normal ```  If you find this confounding, you are not alone. Here are some resources I used:  * https://www.badunetworks.com/traffic-shaping-with-tc/ * https://netbeez.net/blog/how-to-use-the-linux-traffic-control/ * `sudo tc qdisc add dev eth0 root netem help`  For now this fault spec targets a specific network device, meaning all inbound and outbound traffic will be subject to the added latency. `tc` also supports more advanced traffic filtering so eventually we could introduce latency for only certain links (like simulating multiple datacenter).","closed","","mumrah","2019-06-10T21:51:35Z","2019-06-21T15:30:06Z"
"","7166","Minor: Add fetch from follower system test","This adds a basic system test that enables rack-aware brokers with the rack-aware replica selector for fetch from followers (KIP-392). The test asserts that the follower was read from at least once and that all the messages that were produced were successfully consumed.","closed","","mumrah","2019-08-06T15:53:16Z","2019-08-13T19:33:06Z"
"","6506","KAFKA-8164: Add unitTestWithFlakyRetry and testWithFlakyRetry tasks","These tasks run the tests and collect failures without failing the build. Failed tests are run again in a second turn and the build is failed only if there are test failures in this second run.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-03-27T14:13:51Z","2020-01-27T14:54:07Z"
"","7359","MINOR: Fix broken version assertions in `ControllerChannelManagerTest`","These assertions were broken for a number of reasons. Mainly the configuration wasn't being set correctly and we were overriding the intended version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-18T04:59:11Z","2019-09-18T13:38:53Z"
"","7249","MINOR: Clean up partition assignment logic","These are just some ""tidying up"" changes I made when I was preparing to start working on KIP-441.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-08-24T00:03:12Z","2019-09-09T16:03:02Z"
"","6794","MINOR: Add 2.0, 2.1 and 2.2 to broker and client compat tests","These are important to ensure we don't break compatibility.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-23T13:24:13Z","2019-05-23T21:51:39Z"
"","6846","KAFKA-5998: Create state dir and checkpoint file if it doesn't exist when checkpointing","There can exist a condition where the state directory for a task gets deleted by the cleanup thread as the directory has not been modified within the clean-up delay.  But the directory is still needed for the given task.    This PR will catch the `FileNotFoundException` and create the required task directory when checkpointing.  Tests have been updated to validate the task directory and checkpoint file are created when a `FileNotFoundException` is thrown.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-05-30T21:35:25Z","2019-06-04T12:49:03Z"
"","6858","KAFKA-8461: enlarge flaky test timeout","There are 3 reported times of this test timeout. We have to increase the timeout to stablize it for now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-01T14:58:17Z","2019-06-12T13:21:55Z"
"","7218","MINOR: Fix issue where the Trogdor Coordinator tracks its own node as an agent","The Trogdor coordinator would erroneously think that every node in the defined topology is an Agent and check its `/agent/status` endpoint. Since the Coordinator is defined in the topology, it would send requests to itself on a typically invalid default port which only timed out","open","","stanislavkozlovski","2019-08-16T14:53:38Z","2019-08-18T00:21:51Z"
"","6695","KAFKA-8332: Restore determinism in protocol selection during JoinGroupRequest handling","The transition from List to Set for the group of embedded protocols that is supported combined with implicit conversions in scala during map on collections discards the order of preference for the supported embedded protocols. This fix restores determinism of selection based on the order. This bug may affect any group of Kafka clients (Connect workers, Consumers) that support the same set of more than one embedded protocols. In this case unanimity does not resolve selection alone and the order of preference needs to be respect and lead to deterministic selection.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-05-08T00:05:43Z","2019-05-11T06:41:20Z"
"","7396","Update record timestamp in MockProducer","The timestamp in the callback is returning -1. It should rather return the timestamp in the ProducerRecord object if available or return NO_TIMESTAMP if not available.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","gowthamgutha","2019-09-26T09:44:43Z","2019-10-06T06:13:22Z"
"","7054","KAFKA-8615: Change to track partition time breaks TimestampExtractor","The timestamp extractor takes a `previousTimestamp` parameter which should be the partition time. This PR adds back in partition time tracking for the extractor, and renames `previousTimestamp` --> `partitionTime`  Should be cherry-picked back to 2.1","closed","streams,","ableegoldman","2019-07-09T00:48:50Z","2019-07-19T00:09:21Z"
"","6667","KAFKA-6521: Use timestamped stores for KTables","The tests are failing because #6661 is not merged yet..   The goal of this PR is to swap out, KeyValueStore and WindowStore with the corresponding new TimestampedXxxStore in the DSL where appropriate. We don't leverage the timestamps yet in the `Processors`, to keep the PR contained. Thus, no semantic change yet.","closed","kip,","mjsax","2019-05-02T21:20:01Z","2020-06-12T23:41:28Z"
"","6753","KAFKA-8379; Fix flaky test KafkaAdminClientTest.testUnreachableBootstrapServer","The test starts an AdminClient with a MockClient. After the admin client network thread had started, it was disconnecting one of the nodes and marking it unreachable from the main thread. This interferes with the admin client network thread, causing a timing issue if the disconnect occurred while the network thread was processing the first metadata request. This PR makes the test safer by marking the node unreachable in MockClient before starting the admin client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-05-17T10:16:52Z","2019-05-17T13:20:05Z"
"","6834","MINOR: Fix transient failure in PartitionTest.testAddAndRemoveMetrics","The test is failing because other test cases are leaving some metrics in the registry. This patch cleans the registry before and after test runs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-28T23:51:05Z","2019-05-29T14:49:20Z"
"","7257","MINOR: Refactor tag key for store level metrics","The tag key for store level metrics specified in StreamsMetricsImpl is unified with the tag keys on thread and task level.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-27T19:29:52Z","2019-10-21T11:41:30Z"
"","7323","KAFKA-8859: Expose built-in streams metrics version in `StreamsMetricsImpl`","The streams config `built.in.metrics.version` is needed to add metrics in a backward-compatible way. However, not in every location where metrics are added a streams config is available to check `built.in.metrics.version`. Thus, the config value needs to be exposed through the `StreamsMetricsImpl` object.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-09-11T20:10:01Z","2020-06-12T23:35:26Z"
"","6878","KAFKA-8470: State change logs should not be in TRACE level","The StateChange logger in Kafka should not be logging its state changes in TRACE level. We consider these changes very useful in debugging. Given that we configure that logger to log in TRACE levels by default, we obviously deem it important. Let's have these logs be in INFO and unlock the use of a proper TRACE level log down the line  https://issues.apache.org/jira/browse/KAFKA-8470","closed","","stanislavkozlovski","2019-06-04T08:58:28Z","2020-03-20T14:59:45Z"
"","7284","KAFKA-8863 - Add InsertHeader and DropHeader transforms for connect","The SMTs InsertHeader and DropHeaders described in [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) have been implemented, extending the work done in [PR4319](https://github.com/apache/kafka/pull/4319).  Unit tests are added for InsertHeader and ConnectHeaders, covering the different types of Header (int8, int16, int32, int64, float32, float64, Time, Date and Timestamp).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","alozano3","2019-09-03T14:24:35Z","2021-04-19T12:00:38Z"
"","6980","MINOR: Disable scoverage in 2.0","The scoverage plugin is causing the build to fail with recent versions of gradle. I see the following error: ``` * What went wrong: A problem occurred evaluating root project 'kafka'. > Failed to apply plugin [id 'org.scoverage']    > Could not create an instance of type org.scoverage.ScoverageExtension.       > You can't map a property that does not exist: propertyName=testClassesDir ``` This patch disables the plugin since we are not typically checking coverage for old branches anyway.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-21T17:27:33Z","2019-06-21T22:57:55Z"
"","7452","KAFKA-8991: Enable scalac optimizer","The scalac optimizer is able to inline methods to avoid lambda allocations, eliminating the runtime cost of higher order functions in many cases. The compilation parameters we are using here were introduced in 2.12.x, so we don't enable them for Scala 2.11. Also, we enable a more aggressive inlining policy for the `core` project since it's not meant to be used as a library.  See https://www.lightbend.com/blog/scala-inliner-optimizer for more information about the optimizer.  I verified that the lambda allocation in the code below (from LogCleaner.scala) went away after this change with Scala 2.12 and 2.13.  ```scala private def consumeAbortedTxnsUpTo(offset: Long): Unit = {   while (abortedTransactions.headOption.exists(_.firstOffset","closed","","ijuma","2019-10-06T23:18:55Z","2020-11-03T14:34:42Z"
"","7196","MINOR: Avoid unnecessary leaderFor calls when ProducerBatch queue empty","The RecordAccumulator ready calls `leaderFor` unnecessarily when the ProducerBatch queue is empty. When producing to many partitions, the queue is often empty and the `leaderFor` call can be expensive in comparison. Remove the unnecessary call.","closed","","lbradstreet","2019-08-11T19:24:01Z","2019-08-14T04:35:11Z"
"","7072","KAFKA-8653; Default rebalance timeout to session timeout for JoinGroup v0","The rebalance timeout was added to the JoinGroup protocol in version 1. Prior to 2.3, we handled version 0 JoinGroup requests by setting the rebalance timeout to be equal to the session timeout. We lost this logic when we converted the API to use the generated schema definition (#6419) which uses the default value of -1. The impact of this is that the group rebalance timeout becomes 0, so rebalances finish immediately after we enter the PrepareRebalance state and kick out all old members. This causes consumer groups to enter an endless rebalance loop. This patch restores the old behavior.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-11T01:32:45Z","2020-02-07T06:28:13Z"
"","7103","MINOR: New PayloadGenerator for Trogdor tests + Updated Documentation","The RandomNullPayloadGenerator creates approximately half null payloads and half uniformly random payloads. The behavior is specified by a seed that uses java.util.Random for reproducible results.  A test was added to show that both null and non-null payloads are created.   In addition, another example that uses a non-default PayloadGenerator configuration was added to TROGDOR.md. Hopefully this will make it a little easier to configure Trogdor tests.","closed","","jolshan","2019-07-19T22:24:14Z","2019-07-31T21:00:50Z"
"","6865","KAFKA-8472: Use composition for better isolation of fetcher logic","The purpose of this refactor is to provide a cleaner isolation between the follower state machine and the implementation of the follower fetch mechanics (i.e. how to actually retrieve log data). As before, the state machine is implemented in the fetcher thread. This patch adds a new `Fetcher` trait which is implemented by the log dir fetcher and the replica fetcher. This reduces complexity and simplifies testing.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-03T08:42:21Z","2020-04-27T22:50:26Z"
"","7014","KAFKA-8355: add static membership to range assignor","The purpose of this PR is to add static membership support for range assignor. More details for the motivation in [here](https://github.com/apache/kafka/pull/6815).   Similar to round robin assignor, if we are capable of persisting member identity across generations, we will reach a much more stable assignment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-28T20:33:35Z","2019-09-12T16:57:23Z"
"","7055","MINOR: Refactor high watermark access and validation","The purpose of this patch is to restrict the paths for updating and accessing the high watermark and the last stable offset. By doing so, we can validate that these offsets always remain within the range of the log. We also ensure that we never expose `LogOffsetMetadata` unless it is fully materialized. Finally, this patch makes a few naming changes. In particular, we remove the `highWatermark_=` and `highWatermarkMetadata_=` which are both misleading and cumbersome to test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-09T07:27:07Z","2019-07-11T20:52:05Z"
"","6815","KAFKA-8356: add static membership info to round robin assignor","The purpose here is to leverage static membership information during round robin consumer assignment, because persistent member id could help make the assignment remain the same during rebalance. The comparison logic is changed to: 1. If member A and member B both have `group.instance.id`, then compare their `group.instance.id` 2. If member A has `group.instance.id`, while member B doesn't, then A < B 3. If both member A and B don't have `group.instance.id`, compare their `member.id`  In round robin assignor, we use ephemeral `member.id` to sort the members in order for assignment. This semantic is not stable and could trigger unnecessary shuffle of tasks. By leveraging `group.instance.id` the static member assignment shall be persist when satisfying following conditions: 1. number of members remain the same across generation 2. static members' identities persist across generation  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-25T01:12:23Z","2019-06-28T04:12:40Z"
"","6935","K7149: Reduce assignment data size","The PR contains the changes related to Assignment Info re-design where the TopicPartitions are replaced with TaskIDs and GZIP compression is being done on assignmentInfo to reduce the assignment size in version 4. This work is done by @brary.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-06-13T18:07:24Z","2020-04-24T23:58:08Z"
"","7012","MINOR: Streams tutorial creates code in wrong directory","The Pipe.java file should exist within the myapps package directory  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","wheresalice","2019-06-28T15:56:21Z","2019-07-03T18:45:02Z"
"","6795","KAFKA-8425: Fix for correctly handling immutable maps (KIP-421 bug)","The originals map passed to the AbstractConfig class can be immutable. The resolveConfigVariable function was modifying the originals map. If this map is immutable it will result in an exception.  This fix resolves this issue  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tadsul","2019-05-23T15:15:26Z","2019-06-03T11:43:12Z"
"","6898","KAFKA-8499: ensure java is in PATH for ducker system tests","The openjdk docker images used include java binaries at /usr/bin. It has since been moved to /usr/local/openjdk-VERSION/bin, which will cause problems when the system tests invoke any java binary if the latest openjdk image has been pulled. The openjdk images have been updated with the same tag, so this can happen suddenly without any other code changes if the new version is pulled.  The primary symptom is that ZooKeeper will fail to start due to java not being found when running the system tests in ducker.  This PR allows PermitUserEnvironment, and sets the ducker user's `.ssh/environment` to include JAVA_HOME/bin in its path. We cannot set it via the ducker user's `.profile` as ssh commands will not use the shell.","closed","","lbradstreet","2019-06-06T20:53:06Z","2019-06-07T21:23:50Z"
"","6784","MINOR: Updated configuration docs with RocksDBConfigSetter#close","The old docs [here](https://kafka.apache.org/documentation/streams/developer-guide/config-streams.html#rocksdb-config-setter) used a now deprecated method to set the block cache size. In switching over to the new one we would now need to construct a Cache object and therefore also need to close it, so this is a good opportunity to demonstrate the RocksDBConfigSetter#close method that will need to be implemented by users","closed","","ableegoldman","2019-05-22T01:40:53Z","2019-05-24T17:36:01Z"
"","6928","KAFKA-8530; Check for topic authorization errors in OffsetFetch response","The OffsetFetch requires Topic Describe permission. If a client does not have this, we return TOPIC_AUTHORIZATION_FAILED at the partition level. Currently the consumer does not handle this error explicitly, but raises it as a generic `KafkaException`. For consistency with other APIs and to fix transient test failures, we should raise `TopicAuthorizationFailedException` instead.  Note that this fixes some transient failures in `PlaintextEndToEndAuthorizationTest`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-12T22:12:10Z","2019-06-14T15:21:55Z"
"","7044","MINOR: use different key when create results for CreatePartitionsResponse in RequestResponseTest","The method createCreatePartitionsResponse put duplicate keys ""my_topic"" in the result map. So I use ""my_other_topic"" as second key to keep same with the method createCreatePartitionsRequest  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","Wennn","2019-07-08T15:10:32Z","2019-07-08T15:10:32Z"
"","6642","KAFKA-8294; Batch StopReplica requests when possible and improve test coverage","The main problem we are trying to solve here is the batching of StopReplica requests and the lack of test coverage for `ControllerChannelManager`. Addressing the first problem was straightforward, but the second problem required quite a bit of work because of the dependence on `KafkaController` for all of the events. It seemed to make sense to separate the events from the processing of events so that we could remove this dependence and improve testability. With the refactoring, I was able to add test cases covering most of the logic in `ControllerChannelManager` including the generation of requests and the expected response handling logic. Note that I have not actually changed any of the event handling logic in `KafkaController`.  While refactoring this logic, I found that the event queue time metric was not being correctly computed. The problem is that many of the controller events were singleton objects which inherited the `enqueueTimeMs` field from the `ControllerEvent` trait. This would never get updated, so queue time would be skewed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-26T17:57:17Z","2019-05-14T01:58:43Z"
"","6618","MINOR: improve Session expiration notice","The log for expired session windows only shows the window end time and expiration time, not the current stream time, which makes it not obvious why the window is expired, unless you are intimately familiar with stream-time calculations. Explicitly logging the current stream time should help.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-04-22T16:23:31Z","2019-05-02T22:44:47Z"
"","6715","KAFKA-8335; Clean empty batches when sequence numbers are reused","The log cleaner attempts to preserve the last entry for each producerId in order to ensure that sequence/epoch state is not lost. The current validation checks only the last sequence number for each producerId in order to decide whether a batch should be retained. There are two problems with this:  1. Sequence numbers are not unique alone. It is the tuple of sequence number and epoch which is uniquely defined. 2. The group coordinator always writes batches beginning with sequence number 0, which means there could be many batches which have the same sequence number.  The complete fix for the second issue is probably to do the proper sequence number bookkeeping in the coordinator. For now, I have left the coordinator implementation unchanged and changed the cleaner logic to use the last offset written by a producer instead of the last sequence number.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-10T22:26:26Z","2019-05-13T22:21:31Z"
"","7190","KAFKA-8786: Deprecated Gradle features making it incompatible with Gradle 6.0","The lines 549-552 of the build.gradle makes it incompatible with Gradle 6.0. It was fixed by changing  ``` additionalSourceDirs = files(javaProjects.sourceSets.main.allSource.srcDirs) sourceDirectories = files(javaProjects.sourceSets.main.allSource.srcDirs) classDirectories = files(javaProjects.sourceSets.main.output) executionData = files(javaProjects.jacocoTestReport.executionData) ```  to  ``` additionalSourceDirs.from = javaProjects.sourceSets.main.allSource.srcDirs sourceDirectories.from = javaProjects.sourceSets.main.allSource.srcDirs classDirectories.from = javaProjects.sourceSets.main.output executionData.from = javaProjects.jacocoTestReport.executionData ``` No expected changes to unit or integration tests.","closed","","AljoschaP","2019-08-10T14:02:16Z","2020-01-09T00:27:02Z"
"","6868","KAFKA-8469: Fix topology compatibility for suppress","The KTable#suppress operator can take a user-supplied name for the operator. If the user does supply a name, the code should still increment the name counter index to ensure any downstream operators that use the generated name don't change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-06-03T15:07:18Z","2019-06-03T16:44:54Z"
"","6863","KAFKA-8469: Fix topology numbering for suppress","The KTable#suppress operator can take a user-supplied name for the operator. If the user does supply a name, the code should still increment the name counter index to ensure any downstream operators that use the generated name don't change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-06-03T02:03:34Z","2019-06-03T15:04:01Z"
"","7397","KAFKA-8934: Create version file during build for Streams","The Kafka Clients library includes a version file that contains its version and Git commit ID. Since Kafka Streams wants to expose version and commit ID in the metrics it needs to read the version file. To enable the users to check during runtime for version mismatches between the Streams library and the Clients library, a version file is also created for Streams during build time and during runtime only the Streams version file is read. If Streams would read the Clients' version file during runtime, it would read a wrong version and commit ID if the libraries were built from repositories in different states.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-09-26T10:19:29Z","2019-09-27T19:12:31Z"
"","7270","MINOR: fix integer overflow in LRUCacheBenchmark","The jmh LRUCacheBenchmark will exhibit an int overflow when run on a fast machine:  ``` java.lang.ArrayIndexOutOfBoundsException: Index -3648 out of bounds for length 10000 	at org.apache.kafka.jmh.cache.LRUCacheBenchmark.testCachePerformance(LRUCacheBenchmark.java:70) 	at org.apache.kafka.jmh.cache.generated.LRUCacheBenchmark_testCachePerformance_jmhTest.testCachePerformance_thrpt_jmhStub(LRUCacheBenchmark_testCachePerformance_jmhTest.java:119) 	at org.apache.kafka.jmh.cache.generated.LRUCacheBenchmark_testCachePerformance_jmhTest.testCachePerformance_Throughput(LRUCacheBenchmark_testCachePerformance_jmhTest.java:83) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:566) 	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453) 	at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437) 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 	at java.base/java.lang.Thread.run(Thread.java:834) ```","closed","","lbradstreet","2019-08-29T01:18:25Z","2019-08-30T01:02:30Z"
"","6694","[KAFKA-7994] Improve Stream time accuracy for restarts and rebalances","The issue for this PR could be found here: https://issues.apache.org/jira/browse/KAFKA-7994?jql=project%20%3D%20KAFKA%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened%2C%20%22Patch%20Available%22)  As noted in the JIRA description, stream time is incorrectly set to -1 after rebalances and restarts. To help resolve this issue, one approach is to commit the individual partition time with last message processed for each RecordQueue. Hence, after a restart, we could set the partition time to the last committed partition time.   We would also have to forward timestamps down stream after the head of the DAG receives records that updates stream time and global time.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ConcurrencyPractitioner","2019-05-07T23:23:12Z","2019-09-19T18:53:29Z"
"","7182","KAFKA-8748: Fix flaky testDescribeLogDirsRequest","The introduction of KIP-480: Sticky Producer Partitioner had the side effect that `generateAndProduceMessages` can often write messages to a lower number of partitions to improve batching.  `testDescribeLogDirsRequest` (and potentially other tests) relies on the messages being written somewhat uniformly to the topic partitions. We fix the issue by including a monotonically increasing key in the produced messages.  I also included a couple of minor clean-ups I noticed while debugging the issue.  The test failed very frequently when executed locally before the change and it passed 100 times consecutively after the change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-09T05:16:19Z","2019-08-09T13:16:37Z"
"","6732","KAFKA-6474: remove KStreamTestDriver","The implementation of KIP-258 broke the state store methods in KStreamTestDriver. These methods were unused in this project, so the breakage was not detected. Since this is an internal testing utility, and it was deprecated and partially removed in favor of TopologyTestDriver, I opted to just complete the removal of the class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-05-14T19:54:44Z","2019-05-20T17:56:50Z"
"","6883","KAFKA-8483/KAFKA-8484; Ensure safe handling of producerId resets","The idempotent producer attempts to detect spurious UNKNOWN_PRODUCER_ID errors and handle them by reassigning sequence numbers to the inflight batches. The inflight batches are tracked in a PriorityQueue. The problem is that the reassignment of sequence numbers depends on the iteration order of PriorityQueue, which does not guarantee any ordering. So this can result in sequence numbers being assigned in the wrong order.  This patch fixes the problem by using a sorted set instead of a priority queue so that the iteration order preserves the sequence order. Note that resetting sequence numbers is an exceptional case.  This patch also fixes KAFKA-8484, which can cause an IllegalStateException when the producerId is reset while there are pending produce requests inflight. The solution is to ensure that sequence numbers are only reset if the producerId of a failed batch corresponds to the current producerId.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-05T01:46:44Z","2019-06-12T19:54:18Z"
"","6662","KAFKA 8311: better handle timeout exception on Stream thread","The goals for this small diff are:  1. Give user guidance if they want to relax commit timeout threshold 2. Indicate the code path where timeout exception was caught  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-05-02T04:58:47Z","2019-06-04T19:14:45Z"
"","6792","KAFKA 8311 : Augment the error message to let user change `default.api.timeout.ms`","The goal of this commit is from : https://issues.apache.org/jira/browse/KAFKA-8311  1.  It is required to suggest a more meaningful message when `default.api.timeout.ms` that Timeout Exception could have occurred. The 'default.api.timeout.ms' option coverage that was referenced by https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75974886  2. It also needs to provide what stream thread was occurred of time out exception. so I augment the timeout Exception message on Stream Thread which includes Restore Consumer and Thread Consumer in run() method on Stream Thread Class.  I`m wondering that the KAFKA 8311 merge request ticket is already written by  abbccdda  on https://github.com/apache/kafka/pull/6662   I don`t know when the commit ticket was merged so I tried to write this merge request. If this ticket is not appropriate on KAFKA policy. please comment on reply.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","clearpal7","2019-05-23T01:44:50Z","2020-06-30T20:40:13Z"
"","6895","KAFKA-8489: Remove group immediately upon Dead state","The goal is to shorten the transient period of dead group in order to save unnecessary round trips from client.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","abbccdda","2019-06-06T06:58:43Z","2019-06-21T01:31:51Z"
"","6689","MINOR: Update command options for kafka-console-consumer.sh in vagrant/README.md","The following command in vagrant/README.md doesn't work, since `--zookeeper` option has been unsuppored from v2.0.0. This PR updates its command options to fix it.  ``` bin/kafka-console-consumer.sh --zookeeper zk1:2181 --topic sandbox --from-beginning ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-07T06:48:04Z","2019-05-19T02:56:57Z"
"","6546","KAFKA-7192: Cherry-pick 5430 to 1.1","The first PR of KAFKA-7192 is cherry-picked to 1.1 but the follow-up (https://github.com/apache/kafka/pull/5430) is not. This is causing flaky EOS system test failures.  Some test results:  In 2.0 branch, running 25 times (the streams_eos_test has 4 tests, so = 100 tests), no failures:  http://confluent-kafka-2-0-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554466177--apache--2.0--db22e3d/report.html  In 1.1 branch before this PR, running 5 times, failed 10 tests:  http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-02--001.1554239700--guozhangwang--KMinor-1.1-eos-test--8395fce/report.html  In this branch (after this PR), running 25 times, no failures:  http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-05--001.1554465488--guozhangwang--KMinor-1.1-eos-test--897aa03/report.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-04-05T15:34:00Z","2019-04-07T05:07:44Z"
"","6575","[HOT FIX] Check for null before deserializing in MeteredSessionStore","The fetchSession() method of SessionStore searches for a (single) specific session and returns null if none are found. This is analogous to fetch(key, time) in WindowStore or get(key) in KeyValueStore. MeteredWindowStore and MeteredKeyValueStore both check for a null result before attempting to deserialize, however MeteredSessionStore just blindly deserializes and as a result NPE is thrown when we search for a record that does not exist.  Also piggyback on this: fetch() in the Metered layer currently delegates to findSessions with time range [0, Long.MAX_VALUE]. It should instead call the wrapped store's fetch directly (which may then delegate to findSessions or not)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-04-12T23:12:28Z","2020-06-26T22:39:16Z"
"","6978","MINOR: Reflection free implementation of `defaultKerberosRealm`","The existing implementation triggers warnings in Java 9+ and relies on internal classes that vary depending on the JDK provider. The proposed implementation fixes these issues and it's more concise.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-21T07:02:53Z","2019-06-22T01:00:57Z"
"","6888","MINOR: Not all fields specified were printed in log event.","The error was missed out preventing users to know the actual cause.  Before it didn't state the error cause: `2019-04-26 21:31:26,355 ERROR org.apache.kafka.connect.runtime.WorkerSinkTask:261 - WorkerSinkTask{id=foobar-4} Commit of offsets threw an unexpected exception for sequence number 29: {foobar-topic-7=OffsetAndMetadata{offset=288227113, leaderEpoch=null, metadata=''}}`  With this change: `2019-04-26 21:31:26,355 ERROR org.apache.kafka.connect.runtime.WorkerSinkTask:261 - WorkerSinkTask{id=foobar-4} Commit of offsets threw an unexpected exception for sequence number 29: {foobar-topic-7=OffsetAndMetadata{offset=288227113, leaderEpoch=null, metadata=''}} with error: `","closed","","RickardCardell","2019-06-05T10:50:56Z","2019-06-07T10:38:59Z"
"","6902","MINOR: Not all fields specified were printed in log event.","The error was missed out preventing users to know the actual cause.  Before it didn't state the error cause: `2019-04-26 21:31:26,355 ERROR org.apache.kafka.connect.runtime.WorkerSinkTask:261 - WorkerSinkTask{id=foobar-4} Commit of offsets threw an unexpected exception for sequence number 29: {foobar-topic-7=OffsetAndMetadata{offset=288227113, leaderEpoch=null, metadata=''}} ` With this change: `2019-04-26 21:31:26,355 ERROR org.apache.kafka.connect.runtime.WorkerSinkTask:261 - WorkerSinkTask{id=foobar-4} Commit of offsets threw an unexpected exception for sequence number 29: {foobar-topic-7=OffsetAndMetadata{offset=288227113, leaderEpoch=null, metadata=''}} with error: `","closed","","RickardCardell","2019-06-07T08:47:00Z","2019-06-07T10:38:56Z"
"","7242","MINOR: Fix the doc of scheduled.rebalance.max.delay.ms config property","The doc of scheduled.rebalance.max.delay.ms config property was incorrectly the same as the one for the connect.protocol config  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-08-22T21:31:34Z","2020-10-16T06:17:30Z"
"","7449","KAFKA-8983; AdminClient deleteRecords should not fail all partitions unnecessarily","The deleteRecords API in the AdminClient groups records to be sent by the partition leaders. If one of these requests fails, we currently fail all futures, including those tied to requests sent to other leaders. It would be better to fail only those partitions included in the failed request.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-10-04T20:52:55Z","2019-10-08T15:19:08Z"
"","6660","MINOR: Remove header and key/value converter config value logging","The debug log lines in the `Plugins` class that log header and key/value converter configurations should be altered as the configurations for these converters may contain secrets that should not be logged in plaintext. Instead, only the keys for these configs are safe to expose.  This should be backported through to 1.1.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-05-01T21:55:14Z","2020-10-16T06:19:07Z"
"","7208","MINOR. Replace Utils::readFileAsString method to read file as stream","The current Utils::readFileAsString method creates a FileChannel and memory maps file and copies its content to a String and returns it. But that means that we need to know the size of the file in advance. This precludes us from reading files whose size is not known in advance, i.e. any file opened with flag S_IFIFO.  This change updates the method to use stream to read the content of the file. It has couple of practical advantages:  1. Allows bash process substitution to pass in strings as file. So we can say `./bin/kafka-reassign-partitions.sh --reassignment-json-file","closed","","soondenana","2019-08-13T19:55:07Z","2019-11-15T22:11:54Z"
"","6588","KAFKA-8237; Untangle TopicDeleteManager and add test cases","The controller maintains a bunch of state across `ControllerContext`, `PartitionStateMachine`, `ReplicaStateMachine`, and `TopicDeletionManager`. None of this state is actually isolated from the rest. For example, topics undergoing deletion are intertwined with the partition and replica states. As a consequence of this, each of these components tends to be dependent on all the rest, which makes testing and reasoning about the system difficult. This is a first step toward untangling all the state. I have simply moved it all into `ControllerContext` and removed many of the circular dependencies. So far, this is mostly a direct translation, but in the future we can add additional validation in `ControllerContext` to make sure that state is maintained consistently.  Additionally, I have created several mock objects to enable easier testing: `MockReplicaStateMachine` and `MockPartitionStateMachine`. These have simplified logic for updating the current state. This is used to create some new test cases for `TopicDeletionManager`. I found that I had to change the valid previous state of `ReplicaDeletionIneligible` to include `OfflineReplica` to get the new tests to pass, but please make sure there is not a bug in the test case.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-17T00:48:24Z","2019-04-25T05:23:16Z"
"","6823","KAFKA-8437; Await node api versions before checking if offset validation is possible","The consumer should await api version information before determining whether the broker supports offset validation. In KAFKA-8422, we skip the validation if we don't have api version information, which means we always skip validation the first time we connect to a node.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-27T18:34:52Z","2019-05-27T21:22:40Z"
"","7387","[2.2 branch] KAFKA-8398 Prevent NPE when trying to forceUnmap","The commit here includes a simple change to fix a NPE that reported in https://issues.apache.org/jira/browse/KAFKA-8398","closed","","jaikiran","2019-09-25T05:48:35Z","2020-07-06T01:00:03Z"
"","6590","MINOR: make LogCleaner.shouldRetainRecord more clear to read","The comments of LogCleaner.shouldRetainRecord are pretty clear to tell us when to retain a record. However, I think the code is not so clear as the comments so that i make some changes: `val redundant = foundOffset >= 0 && record.offset < foundOffset` ---> `val latest= foundOffset < 0 || record.offset >= foundOffset`  if there dose not exist a message with the key, foundOffset will be -1, record.offset is larger than -1: `val latest = record.offset() >= foundOffset`","closed","","FreeeeLy","2019-04-17T03:19:20Z","2019-04-25T17:20:24Z"
"","7408","MINOR: Add missing `+` in LogSegment.toString","The closing `)` was previously being discarded.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-28T20:41:17Z","2019-09-29T06:34:20Z"
"","6949","MINOR: Fix system test logic to handle duplicates between kafka.properties and server_prop_overides","The changes are cherry-picked from commit 914ffa9dbef3c8ad6851b380276a1cb7c5aa4a0d which is present in 2.1+ versions  The reason for this patch is that the 2.0 `log_dir_failure_test.py` system test very frequently fails due to expecting one partition of the `__consumer_offsets` topic. The override config for the partitions count is passed but gets overridden by the value in kafka.properties.","closed","","stanislavkozlovski","2019-06-17T14:42:49Z","2019-06-18T08:30:36Z"
"","7373","KAFKA-7895: Revert suppress changelog bugfix for 2.1","The bugfix from (#6536) (#6616) breaks compatibility with brokers using log format less than 0.11. (Because record headers are not supported in 0.10 brokers)  Even though this fix is useful, we should not break broker compatibility in a bugfix release, so I'm reverting just the changelog format change. Note: this re-introduces the suppress bug related to changelog restoration, so folks using suppress are recommended to upgrade to 2.2.1 at least.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-09-20T01:06:15Z","2019-09-28T02:08:59Z"
"","7116","KAFKA-8715: fix timestamp bug for static member.id generation","The bug is that we accidentally used the loaded timestamp for the group instead of the real current time. Fix is made and unit test to make sure the timestamp is properly encoded within the returned member.id.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-25T18:41:09Z","2019-07-26T22:31:31Z"
"","7173","MINOR: Format AdminUtils::assignReplicasToBrokers java documentation","The assignReplicasToBrokers method has helpful but a large unformatted javadoc comment that results in a big blob in generated html. This change formats the comment so that generated javadoc is nice.  Here are javadoc images before and after.  ![OriginalJavadoc2](https://user-images.githubusercontent.com/50422828/62586498-c9a00200-b872-11e9-8072-23274f2d0791.png)  ![NewJavadoc](https://user-images.githubusercontent.com/50422828/62581839-3f9a6e00-b85f-11e9-997c-5cea8d4a3cee.png)","closed","","soondenana","2019-08-06T22:31:46Z","2019-08-12T19:53:29Z"
"","6971","MINOR: Fix Partition::toString method","The AllReplicas was only printing remote replica ids. This change prints all ids, including local one.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-06-19T19:16:14Z","2019-06-20T16:50:40Z"
"","7444","KAFKA-8976 [WIP]:  AdminClient should not fetch metadata unnecessarily","The AdminClient should not do unnecessary metadata lookups when methods like `createTopics` and `deleteTopics` are called with empty input maps.   No new tests needed?  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","aurlien","2019-10-04T14:00:11Z","2020-08-19T06:21:42Z"
"","6809","KAFKA-8475: Temporarily restore SslFactory.sslContext() helper","The `sslContext()` method was recently removed in #6674 as part of a refactoring. Even though this is not a public API, removing the method would break any non-AK components that use this class to easily set up SSL clients. Since the cost of keeping this is low, this PR restores the method temporarily to give developers time to move away from reusing this utility class that is not part of the AK public API.  This should be backported to the `2.3` branch, which is where #6674 was originally applied.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-05-24T15:01:36Z","2019-06-03T17:36:04Z"
"","6852","MINOR: Increase timeouts to 30 seconds for ResetIntegrationTest","The `ResetIntegrationTest` has experienced several failures and it seems the current timeout of 10 seconds may not be enough time  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-05-31T16:56:01Z","2019-06-11T01:16:47Z"
"","6853","MINOR: Create a new topic for each test for flaky RegexSourceIntegrationTest","The `RegexSourceIntegrationTest` has some flakiness as it deletes and re-creates the same output topic before each test.  This PR reduces the chance for errors by creating a unique output topic for each test.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-05-31T19:09:04Z","2019-07-12T19:18:50Z"
"","7200","KAFKA-8041: Enable producer retries in log dir failure test to address flakiness","testProduceAfterLogDirFailureOnLeader currently disables producer retries in order to catch and validate the exception thrown by a failure, and then tries to produce successfully once the leadership changes. This second produce can intermittently fail, causing test flakiness. This patch splits these validations into two tests in order to allow retries for the produce request after the leadership change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-08-12T22:31:55Z","2019-08-17T21:25:40Z"
"","6581","KAFKA-8232; Test topic delete completion rather than intermediate state","Test verifies that delete path exists in ZooKeeper immediately after deleting topic using admin client. But since the actual delete is async, the delete may potentially complete, removing the topic's delete path before the test verifies it. Wait for the topic's paths to be deleted to make the test robust.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-04-14T09:01:39Z","2019-04-15T11:42:24Z"
"","6838","MINOR: Fix red herring when ConnectDistributedTest.test_bounce fails.","Test `ConnectDistributedTest.test_bounce` requires 5 nodes(3 Workers, 1 Broker, 1 ZK) when it succeeds and it also needs one more node for `ConsoleConsumer` when it fails and tries to collect data from a topic for debugging purposes.  With this change, the test gives a real failure reason: ``` Triggering test 2 of 17... 21:08:45 [INFO:2019-05-29 21:08:45,918]: RunnerClient: Loading test {'directory': '/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/connect', 'file_name': 'connect_distributed_test.py', 'method_name': 'test_bounce', 'cls_name': 'ConnectDistributedTest', 'injected_args': {'clean': True}} 21:08:45 [INFO:2019-05-29 21:08:45,937]: RunnerClient: kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.clean=True: Setting up... 21:08:45 [INFO:2019-05-29 21:08:45,938]: RunnerClient: kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.clean=True: Running... 21:12:46 [ERROR - 2019-05-29 21:12:46,469 - connect_distributed_test - test_bounce - lineno:409]: Duplicate source sequence numbers for task 0 21:12:46 [ERROR - 2019-05-29 21:12:46,523 - connect_distributed_test - test_bounce - lineno:429]: Duplicate sink sequence numbers for task 0 21:12:46 [ERROR - 2019-05-29 21:12:46,569 - connect_distributed_test - test_bounce - lineno:409]: Duplicate source sequence numbers for task 1 21:12:46 [ERROR - 2019-05-29 21:12:46,629 - connect_distributed_test - test_bounce - lineno:429]: Duplicate sink sequence numbers for task 1 21:12:46 [ERROR - 2019-05-29 21:12:46,684 - connect_distributed_test - test_bounce - lineno:409]: Duplicate source sequence numbers for task 2 21:12:46 [ERROR - 2019-05-29 21:12:46,734 - connect_distributed_test - test_bounce - lineno:429]: Duplicate sink sequence numbers for task 2 21:12:54 [INFO:2019-05-29 21:12:54,416]: RunnerClient: kafkatest.tests.connect.connect_distributed_test.ConnectDistributedTest.test_bounce.clean=True: FAIL: Found validation errors: 21:12:54 Found duplicate source sequence numbers for task 0: ... 21:13:10   Found duplicate sink sequence numbers for task 1: ... 21:13:10   Found duplicate source sequence numbers for task 2: ...  21:13:10   Found duplicate sink sequence numbers for task 2: ... ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","avocader","2019-05-29T19:55:24Z","2020-10-16T06:17:27Z"
"","7035","MINOR: Use Scala's `-release` flag if possible","TBD  #6115 Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-07-04T17:04:47Z","2019-08-11T18:40:52Z"
"","7338","MINOR: add unsigned varint support","Support reading and writing unsigned varints.","closed","","cmccabe","2019-09-16T15:46:20Z","2019-09-16T20:13:07Z"
"","7342","KAFKA-8584: Support of ByteBuffer for bytes field implemented.","Support of the `java.nio.ByteBuffer` for bytes field implemented. To generate code using 'ByteBuffer` one should set `useByteBuffer` flag to true.  `TestByteBufferDataTest` added to test the new feature.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","nizhikov","2019-09-16T20:15:26Z","2019-10-24T02:40:52Z"
"","7384","KAFKA-8938: Improve allocations during Struct validation in ConnectSchema","Summary: Struct value validation in Kafka Connect can be optimized to avoid creating an Iterator when the expectedClasses list is of size 1. This is a meaningful enhancement for high throughput connectors.  This contribution is my original work and I license the work to the project under the project's open source license.  Stack Trace from the Couchbase Kafka Connector:  ``` * java.util.Collections.singletonIterator(Object) * java.util.Collections$SingletonList.iterator() * org.apache.kafka.connect.data.ConnectSchema.validateValue(String, Schema, Object) * org.apache.kafka.connect.data.Struct.put(Field, Object) * org.apache.kafka.connect.data.Struct.put(String, Object) * com.couchbase.connect.kafka.handler.source.DefaultSchemaSourceHandler.buildValue(SourceHandlerParams, CouchbaseSourceRecord$Builder) ```  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","performance,","amcrn","2019-09-24T18:16:55Z","2020-06-11T15:38:06Z"
"","6540","Add pluggability of KeyManager to generate the broker Private Keys an…","Summary of Changes:  Currently, in SslFactory.java, when the keystore is created null (caused by passing an empty config value to ssl.keystore.location), the default Sun KeyManager is used ignoring the 'ssl.keymanager.algorithm' provided.  I included changes to fetch KeyManager from the KeyManagerFactory based on the provided keymanager algorithm, populated by 'ssl.keymanager.algorithm' if the keystore is found empty","closed","","saisandeep","2019-04-05T00:07:43Z","2019-05-02T22:11:43Z"
"","6579","KAFKA-8229: Only change nextCommit if and only if now >= to it","Summary When using the commitRequest api within sink context, the next commit timer should not advance. Timer should only advance when it is the cause of the commit. Prior to this change, the time would advance _each_ time a commit happens -- including when a commit happens because it was requested by the `Task`. When a `Task` requests a commit several times, the clock advances far into the future preventing commits from happening.  This commit changes the behavior, if the `Task` requested a commit and the `nextCommit` is in the _near_ future, `nextCommit` will not advance.  KAFKA-8229  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sdreynolds","2019-04-13T23:27:37Z","2019-05-23T06:50:49Z"
"","6623","KAFKA-8204: fix Streams store flush order (#6555)","Streams previously flushed stores in the order of their registration, which is arbitrary. Because stores may forward values upon flush (as in cached state stores), we must flush stores in topological order.  Cherry-picked from #6555 / 58d6043d4  Reviewers: Bill Bejeck , Guozhang Wang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-23T17:33:29Z","2019-04-24T14:40:37Z"
"","6555","KAFKA-8204: fix Streams store flush order","Streams previously flushed stores in the order of their registration, which is arbitrary. Because stores may forward values upon flush (as in cached state stores), we must flush stores in topological order.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-09T19:29:46Z","2019-04-22T22:27:07Z"
"","6972","KAFKA-8569: integrate warning message under static membership","Static members never leave the group, so potentially we could log a flooding number of warning messages in the hb thread. The solution is to only log as warning when we are on dynamic membership.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-19T20:12:11Z","2019-06-20T20:56:01Z"
"","6674","MINOR: Refactor SslFactory","SslFactory: split the part of SslFactory that creates SSLEngine instances into SslEngineBuilder.  When (re)configuring, we simply create a new SslEngineBuilder.  This allows us to make all the builder fields immutable.  It also simplifies the logic for reconfiguring.  Because we   sometimes need to test old SslEngine instances against new ones, being able to use both the old and the new builder at once is useful.  Create an enum named SslClientAuth which encodes the possible values for ssl.client.auth.  This will simplify the handling of this configuration.  SslTransportLayer#maybeProcessHandshakeFailure should treat an SSLHandshakeException with a ""Received fatal alert"" message as a handshake error (and therefore an authentication error.)  SslFactoryTest: add some line breaks for very long lines.  ConfigCommand#main: when terminating the command due to an uncaught exception, log the exception using debug level in slf4j, in addition to printing it to stderr.  This makes it easier to debug failing junit tests, where stderr may not be kept, or may be reordered with respect to  other slf4j messages.  The use of debug level is consistent with how we handle other types of exceptions in ConfigCommand#main.  StateChangeLogMerger#main: spell out the full name of scala.io.Source rather than abbreviating it as io.Source.  This makes it clearer that it is part of the Scala standard library.  It also avoids compiler errors when other libraries whose groupId starts with ""io"" are used in the broker.","closed","","cmccabe","2019-05-03T22:03:20Z","2019-05-20T18:51:34Z"
"","7130","KAFKA-8179: Part 4, add CooperativeStickyAssignor","Splits the existing StickyAssignor logic into an AbstractStickyAssignor class, which is extended by the existing (eager) StickyAssignor and by the new CooperativeStickyAssignor which supports incremental cooperative rebalancing.  There is no actual change to the logic -- most methods from StickyAssignor were moved to AbstractStickyAssignor to be shared with CooperativeStickyAssignor, and the `abstract MemberData memberData(Subscription)` method converts the Subscription to the embedded list of owned partitions for each assignor.  The ""generation"" logic is left in, however this is always `Optional.empty()` for the CooperativeStickyAssignor as `onPartitionsLost` should always be called when a generation is missed.","closed","","ableegoldman","2019-07-29T21:31:59Z","2020-06-26T22:38:31Z"
"","7321","KAFKA-8179: do not suspend standby tasks during rebalance","Some work needs to be done in Streams before we can incorporate cooperative rebalancing. This PR lays the groundwork for it by doing some refactoring, including a behavioral change that affects eager (""normal"") rebalancing as well: will no longer suspend standbys in `onPartitionsRevoked`, instead we just close any that were reassigned in `onPartitionsAssigned`","closed","","ableegoldman","2019-09-11T02:32:19Z","2020-06-26T22:38:28Z"
"","6747","MINOR: Work around OpenJDK 11 javadocs issue.","Some versions of OpenJDK 11 do not properly handle external javadocs links referencing previous Java versions. See: https://bugs.openjdk.java.net/browse/JDK-8212233.  Failure symptom: `> Task :connect:api:javadoc javadoc: error - The code being documented uses modules but the packages defined in https://docs.oracle.com/javase/8/docs/api/ are in the unnamed module. 1 error`  This PR conditionally sets the Java api docs link for the affected Gradle tasks. I verified that the links render correctly in the generated documentation when building with `1.8.0_181` and `11.0.3`. For example, in `build/docs/javadoc/org/apache/kafka/connect/source/SourceTask.html` the hyperlink to `java.nio.channels.Selector` points to a valid page on Oracle's site in both cases.  Note that while the workaround is also applied to the `aggregatedJavadoc` task, this task was already failing for another, unrelated reason. There is some discussion in https://github.com/apache/kafka/pull/6439. I validated the workaround locally by disabling the `:streams:test-utils:javadoc` task.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","colinhicks","2019-05-16T16:17:34Z","2019-05-21T00:33:38Z"
"","7297","[MINOR] allow additional JVM args in KafkaService","some tweaks on the OS and JVM that allow us to run `KafkaService` in high-scale production-like tests  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-09-04T17:36:34Z","2019-11-18T19:00:45Z"
"","6709","KAFKA-8349. Add Windows batch files corresponding to kafka-delete-records.sh and kafka-log-dirs.sh","Some shell scripts don't have corresponding batch files in bin\windows. For improving Windows platform support, This PR adds the following batch files:  - bin\windows\kafka-delete-records.bat - bin\windows\kafka-log-dirs.bat  I confirmed that they worked on Windows 10 Pro as follows:  ``` PS C:\kafka-2.2.0> .\bin\windows\kafka-log-dirs.bat --bootstrap-server localhost:9092 --describe  (snip)  Querying brokers for log directories information Received log directory information from brokers 0 {""version"":1,""brokers"":[{""broker"":0,""logDirs"":[{""logDir"":""C:\\tmp\\kafka-logs"",""error"":null,""partitions"":[{""partition"":""sandbox-0"",""size"":213,""offsetLag"":0,""isFuture"":false}]}]}]} ```  ``` PS C:\kafka-2.2.0> .\bin\windows\kafka-delete-records.bat --bootstrap-server localhost:9092 --offset-json-file offset-json-file.txt  (snip)  Executing records delete operation Records delete operation completed: partition: sandbox-0    low_watermark: 2 ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-10T08:21:12Z","2019-06-11T02:33:19Z"
"","7468","MINOR: remove unused imports in Streams system tests","Some of the python files under tests/kafkatest/test/streams had unused imports, just some minor cleanup to remove them.","closed","","ableegoldman","2019-10-08T20:36:57Z","2019-10-08T23:01:37Z"
"","6926","MINOR: add group coordinator test coverage","Some edge cases are not currently being tested. Add more tests to cover.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-12T16:17:32Z","2019-06-12T22:38:07Z"
"","7447","MINOR: followup to Version Probing improvements","Small follow-up to trunk PR https://github.com/apache/kafka/pull/7423  While debugging the 2.3 VP PR we realized we should remove the leader-tracking from the VP system test altogether. We'd already merged the corresponding trunk PR so I made a quick new PR for trunk (also fixes a missed version bump in one of the log messages)","closed","","ableegoldman","2019-10-04T19:41:53Z","2020-06-26T22:38:21Z"
"","7448","MINOR: followup to Version Probing improvements2.2","Small follow-up to trunk PR #7426   While debugging the 2.3 VP PR we realized we should remove the leader-tracking from the VP system test altogether. We'd already merged the corresponding 2.2 PR so I made a quick new PR for 2.2","closed","","ableegoldman","2019-10-04T19:45:31Z","2019-10-07T22:33:58Z"
"","6500","MINOR: Move KTable source topic for changelog to optimization framework","Since we've added Kafka Streams optimizations in `2.1` we need to move the optimization for source `KTable` nodes (use source topic as changelog) to the optimization framework.  Updated streams tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-25T19:36:17Z","2019-03-29T23:54:00Z"
"","6509","KAFKA-6399: Remove Streams max.poll.interval override","Since we now call `poll` during restore, we can decrease the timeout to a reasonable value, which should help Streams make progress if threads get stuck.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-03-27T16:42:47Z","2019-04-09T20:16:07Z"
"","6985","KAFKA-8585; Controller should change leader and isr optimistically","Since it has the cached leader and ISR state, the controller can optimistically assume it has the latest state when performing an update and save a little latency. If the state was changed by the leader, it can do the lookup and retry.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","hachikuji","2019-06-22T07:18:48Z","2019-06-25T23:18:44Z"
"","7451","MINOR: Augment log4j to add generation number in performAssign","Since generation is private in AbstractCoordinator, I need to modify the generation() to let it return the object directly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-06T22:17:16Z","2020-04-24T23:49:40Z"
"","7459","HOTFIX: Hide built-in metrics version","Since currently not all refactorings on streams metrics proposed in KIP-444 has yet been implemented, this commit  hides the built-in metrics version config from the user.  Thus, the user cannot switch to the refactored streams metrics.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-10-07T16:43:12Z","2019-10-07T17:59:55Z"
"","7428","[DO NOT MERGE YET] MINOR: Hide built-in metrics version","Since currently not all refactorings on streams metrics proposed in KIP-444 has yet been implemented and the release is coming up, this commit hides the built-in metrics version config from the user. Thus, the user cannot switch to the refactored streams metrics. Once all refactorings will have been implemented, this commit will be reverted.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-10-01T07:54:14Z","2019-10-07T17:48:05Z"
"","7191","KAFKA-8782: Close metrics in QuotaManagerTests","Since `Metrics` was constructed with `enableExpiration=false`, this was not a source of flakiness given the current implementation. This could change in the future, so good to follow the class contract.  Included a few clean-ups with regards to redundant casts and type parameters as well as usage of try with resources for inline usage of `Metrics`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-10T22:36:10Z","2019-08-11T22:31:46Z"
"","7281","KAFKA-8861 Fix flaky RegexSourceIntegrationTest.testMultipleConsumers…","similar to https://issues.apache.org/jira/browse/KAFKA-8011 and https://issues.apache.org/jira/browse/KAFKA-8026  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","chia7712","2019-09-03T10:42:17Z","2019-09-04T15:34:22Z"
"","6499","KAFKA-6988: Reduce classpath via classpath jar","Similar to #5960, we hit an issue with starting up Kafka on Windows when installed to a local with a very long file path; however, we came up with a slightly different solution that attempts to not pollute the classpath with unnecessary items.  Instead we build a classpath jar via Gradle and append it to the classpath.  The Gradle function handles the regex originally done by `should_include_file`.","open","","ward-eric","2019-03-25T17:49:00Z","2019-04-18T13:07:23Z"
"","7197","KAFKA-8774: Regex can be found anywhere in config value","Signed-off-by: Arjun Satish   The original method incorrectly used matcher.matches() instead of matcher.find(). The former method expects the entire string to match the regex, whereas the second one can find a pattern anywhere within the input string (which fits this use case more correctly).  Added unit tests.  Should be backported to 2.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","wicknicks","2019-08-12T01:50:34Z","2020-10-16T06:17:29Z"
"","7412","MINOR: Mark RocksDBStoreTest as integration test","shouldNotThrowExceptionOnRestoreWhenThereIsPreExistingRocksDbFiles takes 1m30s, which is too long for a unit test.  `RocksDBTimestampedStoreTest` inherits from `RocksDBStoreTest` and it's implicitly considered an integration test too.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-29T04:42:08Z","2019-10-01T13:11:29Z"
"","6963","KAFKA-8179: Part 4, add CooperativeStickyAssignor","Should be rebased after #6884 is merged","closed","","ableegoldman","2019-06-18T21:14:15Z","2019-07-26T23:28:20Z"
"","6547","KAFKA-8157: fix the incorrect usage of segment.index.bytes (2.2)","Should be cherry-picked to older branches as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-04-05T17:52:09Z","2019-04-07T04:26:39Z"
"","7132","KAFKA-8731: InMemorySessionStore throws NullPointerException on startup","Should be cherry-picked to 2.3","closed","streams,","ableegoldman","2019-07-29T23:29:53Z","2019-07-31T21:31:53Z"
"","7050","KAFKA-8637: WriteBatch objects leak off-heap memory","Should be cherry-picked back to 2.3 (picked from 2.2 to 2.1 in  [7077](https://github.com/apache/kafka/pull/7077) )","closed","streams,","ableegoldman","2019-07-08T22:08:51Z","2019-07-15T23:43:49Z"
"","6769","Add compute() to InMemoryKeyValueStore","Should also be added to the KeyValueStore interface (at least as default method).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","gowthamgutha","2019-05-20T10:42:13Z","2019-05-24T18:34:07Z"
"","6768","Add compute() to InMemoryKeyValueStore","Should also be added to the KeyValueStore interface (at least as default method).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gowthamgutha","2019-05-20T10:19:18Z","2019-05-20T10:44:02Z"
"","6615","KAFKA-7895: fix Suppress changelog restore (#6536)","Several issues have come to light since the 2.2.0 release: upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata restoring a tombstone incorrectly didn't update the buffer size and min-timestamp  Cherry-picked from #6536 / 6538e9e4d6c1f64fe3045a5c3fbfe306277a1bee  Reviewers: Guozhang Wang , Matthias J. Sax ,  Bruno Cadonna ,  Bill Bejeck   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-22T14:37:11Z","2019-04-23T15:07:34Z"
"","6616","KAFKA-7895: fix Suppress changelog restore (#6536)","Several issues have come to light since the 2.2.0 release: upon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata restoring a tombstone incorrectly didn't update the buffer size and min-timestamp  Cherry-picked from #6536 / 6538e9e  Reviewers: Guozhang Wang , Matthias J. Sax ,  Bruno Cadonna ,  Bill Bejeck   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-22T14:47:27Z","2019-04-23T17:46:17Z"
"","6536","KAFKA-7895: fix Suppress changelog restore","Several issues have come to light since the 2.2.0 release:  * upon restore, `suppress` incorrectly set the record metadata using the changelog record, instead of preserving the original metadata * restoring a tombstone incorrectly didn't update the buffer size and min-timestamp  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-04-03T16:38:14Z","2019-04-22T14:12:40Z"
"","6687","MINOR: MetricsIntegrationTest should set StreamsConfig.STATE_DIR_CONFIG","Sets StreamsConfig.STATED_DIR_CONFIG to temp directory in MetricsIntegrationTest, to match StreamsTestUtils.","closed","","lbradstreet","2019-05-07T02:04:26Z","2019-05-07T16:04:08Z"
"","6953","MINOR: fix optional import","seems last commit forgets to rebase ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-17T20:36:49Z","2019-06-17T20:46:49Z"
"","6728","KAFKA-8305: support default partitions & replication factor in AdminClient#createTopic","See: [KIP-464](https://cwiki.apache.org/confluence/display/KAFKA/KIP-464%3A+Defaults+for+AdminClient%23createTopic) for more information.  ### Description  This change makes the two required changes to support creating topics using the cluster defaults for replication and partitions:  1. Adds a `NewTopic(String)` constructor to the `NewTopic` API 2. Changes the `AdminManager` to accept `-1` as valid options for replication factor and partitions. If this is the case, it will resolve it using the default configuration. 3. This also adds a dependency on scalaJava8Compat library to make it simpler to convert Scala `Option` to `Optional`.  ### Testing  - Updated unit tests with the new conditions  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agavra","2019-05-14T16:53:20Z","2019-06-05T21:10:01Z"
"","7175","[KAFKA-8522] Tombstones can survive forever / be deleted early","See the two JIRAs below for details. https://issues.apache.org/jira/browse/KAFKA-8522 https://issues.apache.org/jira/browse/KAFKA-4545  Currently an implementation of approach 1 on the following KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-534%3A+Retain+tombstones+for+approximately+delete.retention.ms+milliseconds","closed","","ConcurrencyPractitioner","2019-08-07T18:09:00Z","2019-10-20T22:33:35Z"
"","6910","KAFKA-8514: move the scala-java8-compat import to the :core project insetad of :clients","See https://issues.apache.org/jira/browse/KAFKA-8514 - this makes it so that only the `:core` module depends on the newly added java8 convertors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agavra","2019-06-10T18:41:36Z","2019-06-12T20:48:38Z"
"","6781","KAFKA-8199: Implement ValueGetter for Suppress","See also https://github.com/apache/kafka/pull/6684  KTable processors must be supplied with a KTableProcessorSupplier, which in turn requires implementing a ValueGetter, for use with joins and groupings.  For suppression, a correct view only includes the previously emitted values (not the currently buffered ones), so this change also involves pushing the Change value type into the suppression buffer's interface, so that it can get the prior value upon first buffering (which is also the previously emitted value).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-05-21T19:46:18Z","2019-05-31T21:04:40Z"
"","7354","KAFKA-8595: Support deserialization of JSON decimals encoded in NUMERIC","see [KIP-481](https://cwiki.apache.org/confluence/display/KAFKA/KIP-481%3A+SerDe+Improvements+for+Connect+Decimal+type+in+JSON) for more details  Review Guide: - Added `DecimalFormat` enum to represent different JSON decimal node types and corresponding config in `JsonConverterConfig` - Split `LogicalTypeConverter` to have two strongly typed methods (`toJson` and `toConnect`) so that we could group SerDe methods together (this caused some code to move around because I could combine `TO_JSON_LOGICAL_CONVERTERS` and `TO_CONNECT_LOGICAL_CONVERTERS`) - Implemented the SerDe in the line containing `LOGICAL_CONVERTERS.put(Decimal.LOGICAL_NAME, new LogicalTypeConverter()` - Added `CaseInsensitiveValidString` to the validators  Unit tests covering all of the formats are added.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","agavra","2019-09-17T23:57:21Z","2020-10-16T05:50:59Z"
"","6617","MINOR: Allow scripts to be symlinked","Scripts now support one level of symlinking, for example from /usr/bin to your Kafka install directory.  Previously the scripts would break if you didn't run them directly from the installation directory.  I installed this on a test system and ran every script from both the installation directory and as a symlink from /usr/bin/","closed","","dignan","2019-04-22T15:35:09Z","2020-05-14T00:56:24Z"
"","6989","MINOR: Make the build compile with Scala 2.13","Scala 2.13 support was added to build via #5454. This PR adjusts the code so that it compiles with 2.11, 2.12 and 2.13.  Changes: * Add `scala-collection-compat` dependency. * Import `scala.collection.Seq` in a number of places for consistent behavior between Scala 2.11, 2.12 and 2.13. * Remove wildcard imports that were causing the Java classes to have priority over the Scala ones, related Scala issue: https://github.com/scala/scala/pull/6589. * Replace parallel collection usage with `Future`. The former is no longer included by default in the standard library. * Replace val _: Unit workaround with one that is more concise and works with Scala 2.13 * Replace `filterKeys` with `filter` when we expect a `Map`. `filterKeys` returns a view that doesn't implement the `Map` trait in Scala 2.13. * Replace `mapValues` with `map` or add a `toMap` as an additional transformation when we expect a `Map`. `mapValues` returns a view that doesn't implement the `Map` trait in Scala 2.13. * Replace `breakOut` with `iterator` and `to`, `breakOut` was removed in Scala 2.13. * Replace to() with toMap, toIndexedSeq and toSet * Replace `mutable.Buffer.--` with `filterNot`. * ControlException is an abstract class in Scala 2.13. * Variable arguments can only receive arrays or immutable.Seq in Scala 2.13. * Use `Factory` instead of `CanBuildFrom` in DecodeJson. `CanBuildFrom` behaves a bit differently in Scala 2.13 and it's been deprecated. `Factory` has the behavior we need and it's available via the compat library. * Fix failing tests due to behavior change in Scala 2.13, ""Map.values.map is not strict in Scala 2.13"" (https://github.com/scala/bug/issues/11589). * Use Java collections instead of Scala ones in StreamResetter (a Java class). * Adjust CheckpointFile.write to take an `Iterable` instead of `Seq` to avoid unnecessary collection copies. * Fix DelayedElectLeader to use a Map instead of Set and avoid `to` call that doesn't work in Scala 2.13. * Use unordered map for mapping in SimpleAclAuthorizer, mapping of ordered maps require an `Ordering` in Scala 2.13 for safety reasons. * Adapt `ConsumerGroupCommand` to compile with Scala 2.13. * CoreUtils.min takes an `Iterable` instead of `TraversableOnce`, the latter does not exist in Scala 2.13. * Replace `Unit` with `()` in a couple places. Scala 2.13 is stricter when it expects a value instead of a type. * Fix bug in CustomQuotaCallbackTest where we did not necessarily set `partitionRatio` correctly, `forall` can terminate early. * Add a couple of spotbugs exclusions that are needed by code generated by Scala 2.13 * Remove unused variables, simplify some code and remove procedure syntax in a few places. * Remove unused `CoreUtils.JSONEscapeString`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-24T10:30:38Z","2019-07-03T19:36:27Z"
"","7214","MINOR: Remove Scala Procedure Syntax","Scala 2.13 deprecates procedure syntax. This means that for every usage, there will be a build warning stating this. There are 1800+ instances of procedure syntax in Kafka, and this PR attempts to remove them.","closed","","jolshan","2019-08-14T18:21:14Z","2019-08-16T20:28:11Z"
"","7126","MINOR: Update dependencies for Kafka 2.4","Scala 2.12.9 brings another 5% ~ 10% improvement in compiler performance, improved compatibility with JDK 11/12/13, and experimental infrastructure for build pipelining.  zstd update includes performance improvements, among which the primary improvement is that decompression is ~7% faster.  Level | v1.4.0 | v1.4.1 | Delta -- | -- | -- | -- 1 | 1390 MB/s | 1453 MB/s | +4.5% 3 | 1208 MB/s | 1301 MB/s | +7.6% 5 | 1129 MB/s | 1233 MB/s | +9.2% 7 | 1224 MB/s | 1347 MB/s | +10.0% 16 | 1278 MB/s | 1430 MB/s | +11.8%  Jetty 9.4.19 includes a number of bug fixes: https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.19.v20190610  Mockito 3.0.0 switched the Java requirement from 7 to 8.  Several updates to owaspDepCheckPlugin (4.0.2 -> 5.2.1).  The rest are patch updates.  # Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-07-28T06:21:30Z","2019-08-08T13:01:27Z"
"","6526","fix compile error for example","scala 2.12.8","closed","","jarrodu","2019-04-01T07:58:22Z","2019-04-01T15:39:22Z"
"","6656","MINOR: Add option to rebuild source for system tests","Save developer time for running mandatory commands.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-04-30T18:54:00Z","2020-05-14T00:54:44Z"
"","6708","MINOR: Remove unused variables from vagrant-up.sh","Running vagrant/vagrant-up.sh shows the following error message on the environment which doesn't have the realpath command, e.g. macOS:  ``` vagrant/vagrant-up.sh: line 21: realpath: command not found usage: dirname path ```  In the first place, these variables are not used and can be removed.  ``` readonly PROG_DIR=$(dirname $(realpath $0)) readonly INVOKE_DIR=$(pwd) readonly ARGS=""$@"" ```  I ran vagrant-up.sh without these variables and confirmed it worked.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sekikn","2019-05-10T07:20:55Z","2019-05-10T07:20:55Z"
"","6941","KAFKA-8456: Stabilize flaky StoreUpgradeIntegrationTest","Running those tests locally, most run about 10 seconds. The default timeout is 15 seconds. I think it makes sense to provide more head room on Jenkins.  This PR increase the timeout to 60 seconds.","closed","tests,","mjsax","2019-06-14T20:38:38Z","2019-08-01T18:31:04Z"
"","7209","KAFKA-8579: Expose RocksDB metrics","RocksDB metrics are added to the Kafka metrics. For each segmented state store only one set of metrics is exposed rather than one set of metrics for each segment.  The metrics are not computed yet.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-08-14T06:36:07Z","2020-06-12T23:36:19Z"
"","7142","KAFKA-8729, pt 1: Add 4 new metrics to keep track of various types of invalid record rejections","Right now we only have very generic `FailedProduceRequestsPerSec` and `FailedFetchRequestsPerSec` metrics that mark whenever a record is failed on the broker side. To improve the debugging UX, I added 4 new metrics in `BrokerTopicStats` to log various scenarios when an `InvalidRecordException` is thrown when `LogValidator` fails to validate a record:  -- `NoKeyCompactedTopicRecordsPerSec`: counter of failures by compacted records with no key -- `InvalidMagicNumberRecordsPerSec`: counter of failures by records with invalid magic number -- `InvalidMessageCrcRecordsPerSec`: counter of failures by records with crc corruption -- `NonIncreasingOffsetRecordsPerSec`: counter of failures by records with invalid offset  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-07-31T21:43:33Z","2019-12-06T13:52:52Z"
"","7212","KAFKA-8802: ConcurrentSkipListMap shows performance regression in cache and in-memory store","Reverts the TreeMap -> ConcurrentSkipListMap change that caused a performance regression in 2.3, and fixes the ConcurrentModificationException by copying (just) the key set to iterate over","closed","streams,","ableegoldman","2019-08-14T17:15:17Z","2019-09-22T10:00:28Z"
"","6629","MINOR: Remove implicit return statement","Return doesn't need to be stated in Scala  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-04-24T19:19:45Z","2019-04-25T05:31:12Z"
"","6713","KAFKA-8352 : Fix Connect System test failure 404 Not Found","Retry when there is a 404 error for connect service. This could happen when the connect is first started and the resources are not initialized.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-05-10T20:35:42Z","2020-10-16T06:19:08Z"
"","7247","Reset Gradle Nexus Preview Release Settings","Reset Gradle Nexus Preview Release Settings","closed","","mohnishbasha","2019-08-23T22:16:09Z","2019-08-23T22:17:54Z"
"","6508","MINOR: replication factor is short","replication factor is short on brokers, but there are many places it is typed in. this fixes that.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","norwood","2019-03-27T16:07:47Z","2022-03-09T23:40:55Z"
"","7353","KAFKA-8471: Replace control requests/responses with automated protocol","Replaced UpdateMetadata{Request, Response}, LeaderAndIsr{Request, Response} and StopReplica{Request, Response} with the automated protocol classes.  Updated the JSON schema for the 3 request types to be more consistent and less strict (if needed to avoid duplication).  The general approach is to avoid generating new collections in the request classes. Normalization happens in the constructor to make this possible. Builders still have to group by topic to maintain the external ungrouped view.  Introduced new tests for LeaderAndIsrRequest and UpdateMetadataRequest to verify that the new logic is correct.  A few other clean-ups/fixes in code that was touched due to these changes: * KAFKA-8956: Refactor DelayedCreatePartitions#updateWaiting to avoid modifying collection in foreach. * Avoid unnecessary allocation for state change trace logging if trace logging is not enabled * Use `toBuffer` instead of `toList`, `toIndexedSeq` or `toSeq` as it generally performs better and it matches the performance characteristics of `java.util.ArrayList`. This is particularly important when passing such instances to Java code. * Minor refactoring for clarity and readability. * Removed usage of deprecated `/:`, unused imports and unnecessary `var`s. * Include exception in `AdminClientIntegrationTest` failure message. * Move StopReplicaRequest verification in `AuthorizerIntegrationTest` to the end to match the comment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-17T14:42:01Z","2019-09-29T02:39:50Z"
"","6870","KAFKA-8474: Use HTML lists for config layout","Replace the `` elements by `` so the full page width can be used for the configuration descriptions instead of only a very narrow column. I moved the other fields (Type, Default Value, etc) below each entry.  Before: ![Screenshot_2019-06-03 Apache Kafka(1)](https://user-images.githubusercontent.com/903615/58820373-5c7daf80-862a-11e9-9c02-0e16261f5e74.png)  After: ![Screenshot_2019-06-03 Apache Kafka](https://user-images.githubusercontent.com/903615/58820379-61426380-862a-11e9-906f-592456526c8c.png)  Note: This does not touch the sidebar on the left, I just did not include it in my test page.  This requires some new CSS in kafka-site: https://github.com/apache/kafka-site/pull/211  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-06-03T17:07:44Z","2020-01-22T00:14:44Z"
"","6550","KAFKA-8277: Fix NPEs in several methods of ConnectHeaders","Replace `headers.isEmpty()` by calls to `isEmpty()` as the latter does a null check on heathers (that is lazily created).  Testing strategy: extending the existing unit tests for the case in which headers is null.  No need to change docs because not throwing NPEs is what the user expects from the docs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","sortega","2019-04-08T15:26:31Z","2020-10-16T05:50:54Z"
"","7161","Minor: Rename methods to add metrics to sensor in `StreamsMetricsImpl`","Renames method names in `StreamsMetricsImpl` to make them consistent.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-05T14:28:52Z","2019-10-21T11:40:50Z"
"","7345","MINOR: remove unused import","Remove unused import that's slipping past checkstyle somehow","closed","streams,","ableegoldman","2019-09-16T21:53:00Z","2019-09-18T19:13:03Z"
"","6919","MINOR: Remove uncommitted code","Remove unused code  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-06-12T05:59:52Z","2019-06-12T13:33:04Z"
"","6761","KAFKA-8389: Remove redundant bookkeeping from MockProcessor","Remove processedKeys / processedValues / processedWithTimestamps as they are covered with processed already.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-19T03:34:37Z","2019-05-30T21:58:57Z"
"","6819","MINOR:Replace duplicated code with common function in utils","Remove duplcated code, use common function in utils instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hustclf","2019-05-26T15:27:04Z","2019-05-31T01:18:39Z"
"","6788","MINOR: Remove ControllerEventManager metrics on close","Remove created metrics when shutting down `ControllerEventManager`. This fixes transient failures in `ControllerEventManagerTest.testEventQueueTime` and is generally good hygiene.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-22T17:23:48Z","2019-05-28T04:15:32Z"
"","7222","KAFKA-8806 Reduce calls to validateOffsetsIfNeeded","Remember the `updateVersion` of the last update to Metadata so we can avoid unnecessarily checking each partition for leader epoch changes on every call to `KafkaConsumer#poll`","closed","","mumrah","2019-08-19T12:44:15Z","2020-08-21T14:25:53Z"
"","6684","KAFKA-8199: Fix ClassCastException when trying to groupBy after supp…","Reference to : https://github.com/apache/kafka/pull/6646 Rebased to trunk  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Fixes #8199 ClassCastException when trying to groupBy after suppress  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.* Test method has been added **groupByAfterSuppresShouldRuns**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ouertani","2019-05-06T13:17:24Z","2019-06-02T18:04:02Z"
"","7154","KAFKA-8600: Use RPC generation for DescribeDelegationTokens protocol","Refactors the DescribeDelegationToken to use the generated RPC classes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-08-02T13:33:05Z","2019-08-16T17:36:09Z"
"","7391","MINOR: refactor CheckpointFile to improve testability","Refactors CheckpointFile such that buffers can be read in lieu of files. This is a relatively simple refactoring as we already create a buffered reader over the checkpoint file.  https://github.com/apache/kafka/pull/6742, which improves the performance of the checkpointing code, requires a similar refactoring of code, although it does go further than this.","closed","","lbradstreet","2019-09-25T18:56:19Z","2020-02-03T14:36:07Z"
"","7277","MINOR: reduce three iterations to one","reduce three iterations to one  Reduce unnecessary repetition  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-08-30T16:53:06Z","2020-01-10T16:17:31Z"
"","7023","MINOR: Change ConnectDistributed system test to use KafkaVersion instead of string","Recently, #7000 added a method to the `KafkaVersion` class used in the Kafka system tests. Until then, ConnectDistributed tests passed a string as the version, and this happened to work since no methods were required. But the `KafkaVersion.support_named_listeners()` method added in #7000 and required in `tests/kafkatest/services/kafka/templates/kafka.properties` appears to have broken the ConnectDistributed system tests. Changing the latter to use `KafkaVersion` instead of string fixes the system tests.  I'll run a system test build before this should be merged to `trunk` and at least `2.3` (ideally back to `1.0`).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-07-02T00:58:43Z","2019-07-02T18:10:42Z"
"","7118","MINOR: Fix Streams metadata upgrade system test","Recent ZK version bump broke the system test.  Kafka Streams depends on ZK in version 0.10.0 and 0.10.1. Previously, when running the system test we got the dependency added to the class path from the broker dependencies. With the ZK version bump, incorrect dependencies were added, resulting in class loading errors.  In this PR, we (1) remove core dependencies completely for the case we run an upgrade test, because Streams does not need those dependencies anyway. (2) For the case of running the upgrade test, we add the correct jars to the class path for 0.10.0 and 0.10.1 versions.","closed","tests,","mjsax","2019-07-26T02:51:41Z","2019-07-29T01:19:08Z"
"","7248","KAFKA-8868: Generate SubscriptionInfo protocol message","Rather than maintain hand coded protocol serialization code, Streams could use the same code-generation framework as Clients/Core.  There isn't a perfect match, since the code generation framework includes an assumption that you're generating ""protocol messages"", rather than just arbitrary blobs, but I think it's close enough to justify using it, and improving it over time.  Using the code generation allows us to drop a lot of detail-oriented, brittle, and hard-to-maintain serialization logic in favor of a schema spec.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-08-23T23:20:54Z","2019-11-01T17:04:06Z"
"","6886","Minor: Replace InternalTopicMetadata with InternalTopicConfig","Quick tech debt cleanup. For some reason StreamsPartitionAssignor uses an InternalTopicMetadata class which wraps an InternalTopicConfig object along with the number of partitions. But InternalTopicConfig already has a numPartitions field, so we should just use it directly instead.","closed","","ableegoldman","2019-06-05T06:52:56Z","2019-06-06T18:37:37Z"
"","7096","[POC] Add UserData to Heartbeat","Prototype of adding an extra payload to the heartbeat protocol, with KIP-441 in mind.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2019-07-16T21:17:06Z","2019-07-23T21:58:04Z"
"","7394","MINOR: FetchSessionHandler unnecessarily generates strings for DEBUG logging at INFO level","Profiling while benchmarking shows FetchSessionHandler unnecessary calls to responseDataToLogString when logging was set to INFO level. This leads to 1.47% of the JVM CPU time going to responseDataToLogString. This PR fixes this issue.  ![image](https://user-images.githubusercontent.com/252189/65657942-86dce980-dfda-11e9-9af3-c590bc532ac8.png)","closed","","lbradstreet","2019-09-26T04:22:33Z","2019-09-29T00:03:07Z"
"","7086","KAFKA-8662; Fix producer metadata error handling and consumer manual assignment","Producer adds a topic to its Metadata instance when send is requested. If metadata request for the topic fails (e.g. due to authorization failure),  we retain the topic in Metadata and continue to attempt refresh until a hard-coded expiry time of 5 minutes. Due to changes introduced in https://github.com/apache/kafka/commit/460e46c3bb76a361d0706b263c03696005e12566, subsequent sends to any topic including valid authorized topics report authorization failures in any topic in the metadata, rather than just the topic to which send is requested. As a result, the producer remains unusable for 5 minutes if a send is requested on an unauthorized topic. This PR fails send only if metadata for the topic being sent to has an error (or there is a fatal exception like authentication failure).  Consumer adds a topic to its Metadata instance on `subscribe()` or `assign()`. Even though `assign()` is not incremental and replaces existing assignment, new assignments were being added to existing topics in SubscriptionState#groupSubscriptions, which is used for fetching topic metadata. This PR does a replace for manual assignment alone.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-07-13T21:19:29Z","2019-07-16T12:08:17Z"
"","6515","KAFKA-8171: Set callback to null in addStopReplicaRequestForBrokers when replica …","Problem:   In ControllerChannelManager.sendRequestsToBrokers, for STOP_REPLICA requests, it will try to group the requests based on deletePartition flag and callback:  > val (replicasToGroup, replicasToNotGroup) = replicaInfoList.partition(r => !r.deletePartition && r.callback == null)  When both conditions meet, controller is expected to only send one request to destionation broker. However, when adding the requests in ReplicaStateMachine, it's putting in non-null callback **_(_,_)=>()_**. Therefore, replicasToGroup is always empty and controller will always first sends an empty request followed by #partitions requests.   Fix: set the callback to null in addStopReplicaRequestForBrokers when replica state transits to offline.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kehuum","2019-03-28T17:18:05Z","2019-05-11T06:48:16Z"
"","7431","MINOR: Do not falsely log that partitions are being reassigned on controller startup","Previously we would log the following on each controller startup: ``` [2019-09-13 22:40:10,272] INFO [Controller id=2] DEPRECATED: Partitions being reassigned through ZooKeeper: Map() ```","closed","","stanislavkozlovski","2019-10-02T09:46:14Z","2019-10-02T15:50:03Z"
"","7000","Fix for a failing upgrade test","Previous PR (https://github.com/apache/kafka/pull/6938) added partial support for named listeners to kafka.py. Though we did run full test suite, somehow we missed one of the upgrade tests. Since upgrade tests start kafka from older versions, they may try to start it from a version that doesn't support named listeners yet.  This PR addresses the issue by adding a method to `version.py` called `supports_named_listeners()` and calling that method from kafka.properties template.  According to KIP-103 (KAFKA-4636), `inter.broker.listener.name` property was introduced in 0.10.2.0, so I used this value in `supports_named_listeners()` method.  Testing: Verified that a previously failing test passes locally. Ran upgrade tests on Jenkins - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2747, all green  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","stan-confluent","2019-06-26T01:05:08Z","2019-06-27T17:29:11Z"
"","7382","KAFKA-8319: Make KafkaStreamsTest a non-integration test class","Previous KafkaStreamsTest takes 2min20s on my local laptop, because lots of its integration test which is producing / consuming records, and checking state directory file system takes lots of time. On the other hand, these tests should be well simplified with mocks.  This test reduces the test from a clumsy integration test class into a unit tests with mocks of its internal modules. And some other test functions should not be in KafkaStreamsTest actually and have been moved to other modular test classes. Now it takes 2s.  Also it helps removing the potential flakiness of the following (some of them are claimed resolved only because we have not seen them recently, but after looking at the test code I can verify they are still flaky):  * KAFKA-5818 (the original JIRA ticket indeed exposed a real issue that has been fixed, but the test itself remains flaky) * KAFKA-6215 * KAFKA-7921 * KAFKA-7990 * KAFKA-8319 * KAFKA-8427   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-09-24T16:52:58Z","2020-04-24T23:52:20Z"
"","6997","KAFKA-8601: Sticky Partitioner","PR to create a sticky partitioner for the default partitioner used for non-keyed records. More information can be found in [KIP-480](https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner).  The general idea is that records will go to a set partition for a topic until the batch is sent in order to improve batching and decrease latency.  Tests included to ensure the sticky partitioner's methods are able to set and receive the sticky partition as expected.","closed","","jolshan","2019-06-25T15:40:53Z","2019-08-01T21:36:13Z"
"","7025","MINOR: Pass KafkaVersion to nodes in KafkaService. Related to PR#7000","PR #7000 added a `KafkaVersion.support_named_listeners()` method and used this in `tests/kafkatest/services/kafka/templates/kafka.properties`. However, if `KafkaService` expects the `version` parameter to be a string (as denoted in the doc), then `KafkaService` needs to construct a `KafkaVersion` and pass it to the nodes rather than passing the string, since the string does not have the `support_named_listeners()` method.  This could be an alternative to #7023.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-07-02T02:41:45Z","2019-07-03T01:25:15Z"
"","7140","KAFKA-8860: Let SslPrincipalMapper split SSL principal mapping rules","PR #6099 tried to undo the splitting of the ssl.principal.mapper.rules list on comma with whitespace by sophisticated rejoining of the split list using a comma as separator. However, since possibly surrounding whitespace is not reconstructed this approach fails in general. Consider the following test case:  @Test public void testCommaWithWhitespace() throws Exception {     String value = ""RULE:^CN=((\\\\, *|\\w)+)(,.*|$)/$1/,DEFAULT"";      @SuppressWarnings(""unchecked"")     List rules = (List) ConfigDef.parseType(""ssl.principal.mapper.rules"", value, Type.LIST);      SslPrincipalMapper mapper = SslPrincipalMapper.fromRules(rules);     assertEquals(""Tkac\\, Adam"", mapper.getName(""CN=Tkac\\, Adam,OU=ITZ,DC=geodis,DC=cz"")); } The space after the escaped comma is essential. Unfortunately, it has disappeared after splitting and rejoining.  Moreover, in joinSplitRules the decision to rejoin list elements is based on local information only which might not be sufficient. It works for ""RULE:^CN=([^,ADEFLTU,]+)(,.*|$)/$1/"" but fails for the equivalent regular expression ""RULE:^CN=([^,DEFAULT,]+)(,.*|$)/$1/"".  The approach of the current PR is to change the type of the ssl.principal.mapper.rules attribute from LIST to STRING and to delegate the splitting of the rules to the SslPrincipalMapper. It knows about the structure of the rules and can perform the splitting context-based.","closed","","teebee","2019-07-30T22:05:22Z","2019-09-02T18:03:41Z"
"","6894","KAFKA-8487: Only request re-join on REBALANCE_IN_PROGRESS in CommitOffsetResponse","Plus some minor cleanups on AbstractCoordinator.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-06-06T00:03:43Z","2020-04-24T23:58:05Z"
"","7088","[KAFKA-8658] A way to configure the jmx rmi port","Please see https://issues.apache.org/jira/browse/KAFKA-8658 for description.","closed","","ConcurrencyPractitioner","2019-07-15T02:13:32Z","2021-06-30T17:44:54Z"
"","6954","MINOR: rename subscription construction function","Per discussion on https://github.com/apache/kafka/pull/6936, some nit fixes to the Subscription initialization path.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-17T20:54:57Z","2019-06-18T00:11:12Z"
"","7165","KAFKA-4740 Fields of partition and offset in case of deserializer fails.","Partition and offset information are included as a part of the SerializationException.  Some consumers may choose to skip that offset, this metadata can be used for that purpose.  Since DeserializationException extends SerializationException, nothing should be broken.","open","","yamanyar","2019-08-06T13:39:51Z","2019-08-06T13:39:51Z"
"","7271","Murmur3 Hash with Guava dependency","Part of supporting KIP-213 ( https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable ). Murmur3 hash is used as a hashing mechanism in KIP-213 for the large range of uniqueness. The Murmur3 class and tests are ported directly from Apache Hive, with no alterations to the code or dependencies.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bellemare","2019-08-29T15:00:36Z","2019-09-19T23:39:10Z"
"","6673","KAFKA-8284: enable static membership on KStream","Part of KIP-345 effort. The strategy is to extract user passed in `group.instance.id` config and pass it in with given thread-id (because consumer is currently per-thread level).   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","abbccdda","2019-05-03T21:34:10Z","2020-06-12T23:41:22Z"
"","6518","MINOR: Optimize ConnectionStressWorker","Optimize ConnectionStressWorker by avoiding creating a new ChannelBuilder each time we want to open a new connection.","closed","","cmccabe","2019-03-28T22:04:22Z","2019-03-29T22:04:08Z"
"","6920","KAFKA-7760","Only segment size is fixed in streams. Segment ms is yet to be fixed","closed","","dulvinw","2019-06-12T06:54:14Z","2019-06-12T09:38:03Z"
"","7366","MINOR: add time breakdown for produce request","Only a POC, do not merge ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-09-18T21:43:10Z","2019-09-18T23:39:20Z"
"","6793","MINOR: Extend RocksDB section of Memory Management Docs","Now that we can configure RocksDB to bound the total memory we should include docs describing how, as well as touching on some possible options that should be considered when taking advantage of this feature.","closed","","ableegoldman","2019-05-23T04:53:03Z","2019-05-30T11:31:48Z"
"","7268","KAFKA-8760; New Java Authorizer API (KIP-504)","New Java Authorizer API and a new out-of-the-box authorizer that implements the new interface.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-08-28T21:52:54Z","2019-09-04T12:27:29Z"
"","7018","MINOR: system tests - avoid 'sasl.enabled.mechanisms' in listener overrides","named listener config `sasl.enabled.mechanisms` should not be prefixed with sasl mechanism  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-06-30T21:41:57Z","2019-07-03T16:07:40Z"
"","7340","MINOR: improve the Kafka RPC code generator","Move the generator checkstyle suppressions to a special section, rather than mixing them in with the other sections.  For generated code, do not complain about variable names or cyclic complexity.  Remove FieldType#isInteger.  This way, we don't have to decide whether a UUID is an integer or not (there are arguments for both choices).  Add FieldType#serializationIsDifferentInFlexibleVersions and FieldType#isVariableLength.  HeaderGenerator: add the ability to generate static imports.  Add IsNullConditional, VersionConditional, and ClauseGenerator as easier ways of generating ""if"" statements.  Add Versions#subtract, and a unit test for it.  Add CodeBufferTest to test the existing CodeBuffer class.","closed","","cmccabe","2019-09-16T17:59:43Z","2019-09-25T15:58:55Z"
"","7243","MINOR: Move the resetting from revoked to the thread loop","Move the error code resetting logic from the onPartitionsRevoked callback into the streamthread directly after we've decided to rejoin the group, since onPartitionsRevoked are not guaranteed to be triggered.  Ran system tests on the originally failed tests 10 times: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-08-23--001.1566527172--guozhangwang--KMinor-upgrade-system-test--de48dd4/report.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-08-23T01:35:13Z","2020-04-24T23:53:15Z"
"","7358","Fix bug2","modify the kafka-server-start.sh, if the server.properties exist ,use the server.properties, else use the $KAFKA_HOME/config/server.properties","open","","liangkiller","2019-09-18T03:26:34Z","2019-09-18T03:26:34Z"
"","6982","MINOR: Fix DistributedHerderTest after adding reason to maybeLeaveGroup","Mocking of WorkerCoordinator was not precise after adding an argument (reason) to AbstractCoordinator#maybeLeaveGroup in KAFKA-8569:  Unit test case for DistributedHerderTest is now precise with respect to the expected argument and succeeds  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-06-21T19:00:25Z","2020-10-16T05:50:56Z"
"","6960","Do not Merge - Mistakenly Opened","Mistakenly opened on wrong branch","closed","","gokhansari","2019-06-18T14:05:58Z","2019-06-18T14:23:08Z"
"","7414","MINOR: Fixed a division by 0 scenario","MINOR: Fixed a division by 0 scenario by ensuring that totalCapacity had a default value of 1 in the event that sumCapacity was 0, thereby negating a division by 0 event which could have caused a denial of service.  This was tested via static analysis while I was reviewing the code. There is no need for additional unit tests since this is indeed a minor change.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","TimYagan","2019-09-29T18:51:42Z","2020-02-12T18:46:48Z"
"","7383","MINOR: Gracefully handle non-assigned case in fetcher metric","Minor tweak to gracefully handle a possible IllegalStateException while checking a metric value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-09-24T17:22:26Z","2019-10-07T14:47:36Z"
"","7106","MINOR: Ensure in-memory metadata is removed before physical deletion of segment","Minor refactoring of the places where we delete log segments, to ensure we always remove the in-memory metadata of the segment before performing physical deletion.","closed","","dhruvilshah3","2019-07-23T19:01:17Z","2019-07-25T15:30:50Z"
"","7440","KAFKA-8969 Log the partition being made online due to unclean leader election.","Minor refactor to PartitionLeaderElectionAlgorithms and Election to move side effects outside of PartitionLeaderElectionAlgorithms and into higher level Election.  Added a unit test to the PartitionStateMachine to verify that unclean leader election correctly assigns a partition and increments the ule metric.","open","","smccauliff","2019-10-03T19:33:14Z","2019-10-24T21:04:49Z"
"","7461","MINOR: Modified Exception handling for KIP-470","Minor Exception handling changes for KPI-470 IllegalStateException when Provided `TestRecord` does not have a timestamp and no timestamp overwrite was provided via `time` parameter. IllegalStateException when Null keys with readKeyValuesToMap method  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","jukkakarvanen","2019-10-07T18:56:11Z","2020-06-12T23:33:53Z"
"","6507","MINOR: Fix some spelling corrections in comments","Minor doesn't even do it justice, really minor change, mostly to fix node->note in a comment.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soenkeliebau","2019-03-27T15:43:45Z","2019-03-28T15:10:59Z"
"","7282","Minor return value","Minor code enhancement: remove unnecessary check of nullity.","closed","streams,","khaireddine120","2019-09-03T13:04:54Z","2019-09-04T14:12:36Z"
"","6485","MINOR: Clean up ThreadCacheTest","Minor clean up  of`ThreadCacheTest`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-03-22T01:59:57Z","2019-03-22T13:28:23Z"
"","6904","KAFKA-8501: Removing key and value from exception message","Messages containing key and value were moved to the TRACE logging level, however the exception is still adding the key and value. This commits remove the key and value from StreamsException.  *Summary of testing strategy (including rationale) The content of the exception message is not tested anywhere, and I think it would be really difficult to provide a reliable test to check for content inside a string.  ### Committer Checklist (excluded from commit message) - [ X ] Verify design and implementation  - [ X ] Verify test coverage and CI build status - [ X ] Verify documentation (including upgrade notes)","closed","streams,","carlosduclos","2019-06-07T13:17:04Z","2019-06-12T11:31:21Z"
"","6630","KAFKA-3729: Revert adding Serde auto-config","Merged PRs were not backward compatible. Need to revert for 2.3 release. Ticket reopened with more details.","closed","","mjsax","2019-04-24T23:34:43Z","2019-05-01T07:33:58Z"
"","7053","Merging Apache Kafka 2.3 to Microsoft Kafka 2.4","merge latest changes from Apache/kafka to microsoft/kafka trunk  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kondetibharat","2019-07-08T23:30:39Z","2019-07-08T23:31:58Z"
"","6665","MINOR: Upgrade dependencies for Kafka 2.3","Many patch and minor updates.  Scalatest and Jetty deprecated classes that we use. I removed usages for the former and filed KAFKA-8316 for the latter (I suppressed the relevant deprecation warnings until the JIRA is fixed). As part of the scalatest fixes, I also removed `TestUtils.fail` since it duplicates `Assertions.fail`.  I also fixed a few compiler warnings that have crept in since my last sweep.  Updates of note: - Jetty: 9.4.14 -> 9.4.18   * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.15.v20190215   * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.16.v20190411   * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.17.v20190418   * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.17.v20190418   * https://github.com/eclipse/jetty.project/releases/tag/jetty-9.4.18.v20190429 - zstd: 1.3.8-1 -> 1.4.0-1   * https://github.com/facebook/zstd/releases/tag/v1.4.0   * zstd's fastest strategy, 6-8% faster in most scenarios - zookeeper: 3.4.13 -> 3.4.14   * https://zookeeper.apache.org/doc/r3.4.14/releasenotes.html  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-02T14:38:45Z","2019-05-03T17:35:08Z"
"","6917","MINOR: seal the HostedPartition enumeration","Makes HostedPartition a sealed trait and make all ofthe match cases explicit  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","jsancio","2019-06-11T20:33:06Z","2019-06-12T17:26:13Z"
"","7405","MINOR: Make `TopicDescription`'s other constructor public","Make the other TopicDescriptions package private constructor public, so that it can be used by clients.  For example, KSQL needs the constructor for its use of proxies around Kafka APIs.  Another common use case would likely be tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2019-09-27T12:29:43Z","2019-10-03T22:27:57Z"
"","7152","KAFKA-8745: DumpLogSegments doesn't show keys, when the message is null","Make sure to show the message key, even when the message value is null.  This changes the output of one of the tools. Is the output of the tool considered a public API? Does this need a discussion or a KIP?  Testing: Ran the tool on a compacted topic. Previously, the tool did not show any message keys for tombstone messages (messages where the value is null). Now, the tool shows message keys.","closed","","wushujames","2019-08-01T23:53:18Z","2020-03-11T05:46:28Z"
"","7133","KAFKA-8465:make sure that the copy of the same topic is evenly distributed across a broker's disk.","Make sure that the copy of the same topic is evenly distributed across a broker's disk. When some partiton's replication is assigned to a broker, which disks should these copies be placed on the broker? The original strategy is to allocate according to the number of partiitons。This strategy will result in uneven disk allocation for the topic dimension. In order to solve this problem, we propose an improved strategy: first ensure that the number of partitions of each disk in the topic dimension is even. If the number of partitions of a topic on two disks is equal, then sort according to the total number of partitions on the disk. Select a disk with the least number of partitions to store the current replication.","open","","lordcheng10","2019-07-30T05:12:00Z","2019-08-08T07:04:39Z"
"","7082","KAFKA-8659: SetSchemaMetadata SMT fails on records with null value and schema","Make SetSchemaMetadata SMT ignore records with null `value` and `valueSchema` or `key` and `keySchema`.  The transform has been unit tested for handling null values gracefully while still providing the necessary validation for non-null values.","closed","connect,","bfncs","2019-07-12T09:03:38Z","2022-03-01T19:09:48Z"
"","7080","KAFKA-8659: SetSchemaMetadata SMT fails on records with null value and schema","Make SetSchemaMetadata SMT ignore records with null `value` and `valueSchema` or `key` and `keySchema`.  The transform has been unit tested for handling null values gracefully while still providing the necessary validation for non-null values.","closed","","bfncs","2019-07-12T08:00:39Z","2019-07-12T09:07:20Z"
"","7223","KAFKA-8816: Make offsets immutable to users of RecordCollector.offsets","Make offsets immutable to users of RecordCollector.offsets. Fix up an existing case where offsets could be modified in this way. Add a simple test to verify offsets cannot be changed externally.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cpettitt-confluent","2019-08-19T19:57:27Z","2019-09-09T23:43:33Z"
"","6735","MINOR: Update lz4-java to 1.6.0 for 12-18% decompression improvement","lz4-java 1.6.0 relies on lz4 1.9.1, which includes significant decompression performance improvements first released as part of 1.9.0:  Version | v1.8.3 | v1.9.0 | Improvement -- | -- | -- | -- enwik8 | 4090 MB/s | 4560 MB/s | +12% calgary.tar | 4320 MB/s | 4860 MB/s | +13% silesia.tar | 4210 MB/s | 4970 MB/s | +18%  See https://github.com/lz4/lz4/releases/tag/v1.9.0 for more details.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-15T03:15:19Z","2019-05-15T06:23:28Z"
"","6969","KAFKA-8526: logdir fallback on getOrCreateLog","LogManager#getOrCreateLog() selects a log dir for the new replica from  _liveLogDirs, if disk failure is discovered at this point, before  LogDirFailureHandler finds out, try using other log dirs before failing  the operation.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soarez","2019-06-19T14:31:02Z","2019-07-23T15:57:38Z"
"","7159","MINOR: Fix potential bug in LogConfig.getConfigValue and improve test coverage","LogConfig.getConfigValue would throw a NoSuchElementException if any log config was defined without a server default mapping.  Added a unit test for `getConfigValue` and a sanity test for `toHtml`/`toRst`/`toEnrichedRst`, which were previously not exercised during the test suite.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-03T18:00:16Z","2019-08-04T07:00:23Z"
"","7010","MINOR: Preserve the assignment order from the LeaderAndIsr request","Leaders should make changes to the assignment and the ISR at the same time as part of processing the LeaderAndIsr requests. The leader should also preserve the order of assignment mainly for consistency with the Controller's code and data representation.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jsancio","2019-06-28T00:09:02Z","2019-07-03T15:38:29Z"
"","6593","KAFKA-8147 Add changelog topic configuration to KTable suppress","KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-446%3A+Add+changelog+topic+configuration+to+KTable+suppress Jira: https://issues.apache.org/jira/browse/KAFKA-8147  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjduijn","2019-04-17T20:23:46Z","2020-06-12T23:44:10Z"
"","7326","KIP-499/https://issues.apache.org/jira/browse/KAFKA-8507 - add --bootstrap-server to various command line tools.","KIP-499/https://issues.apache.org/jira/browse/KAFKA-8507  This PR adds --bootstrap-server as the preference connection flag for 4 command line tools.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","mitchell-h","2019-09-12T17:57:20Z","2020-01-09T06:47:23Z"
"","6799","KAFKA-8419 Add SASL callback handler support to KafkaLog4jAppender","KIP-470  https://cwiki.apache.org/confluence/display/KAFKA/KIP-470%3A+Enable+KafkaLog4JAppender+to+use+SASL+Authentication+Callback+Handlers","closed","","rnpridgeon","2019-05-23T20:42:33Z","2022-02-12T10:53:44Z"
"","7351","MINOR: Send latest LeaderAndIsr version","KIP-455 (18d4e57f6e8c67ffa7937fc855707d3a03cc165a) bumped the LeaderAndIsr version to 3 but did not change the Controller code to actually send the new version. The ControllerChannelManagerTest had a bug which made it assert wrongly, hence why it did not catch it. This patch fixes said test. Because the new fields in LeaderAndIsr are not used yet, the gap was not caught by integration tests either.","closed","","stanislavkozlovski","2019-09-17T10:39:34Z","2019-09-19T22:55:35Z"
"","6780","HOTFIX: Fix recent protocol breakage","KIP-345 and KIP-392 introduced a couple breaking changes for old versions of bumped protocols. This patch fixes them.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-21T06:46:43Z","2019-05-22T15:02:03Z"
"","7273","KAFKA-8833/KIP-514 - Broker Consumer Lag metrics in size and time","KIP  https://cwiki.apache.org/confluence/display/KAFKA/KIP-514%3A+Broker+Consumer+Lag+metrics+in+size+and+time  To measure the byte lag : The relative position of the offset is determined from the fetch info data from logsegment.read() and then newer segments size is added to determine the lag  To measure time lag : batch maxTimestamp is used to determine the fetched offset timestamp and activesegment.maxtimestampsofar is used.  The changes have been made at the log layer and the following assumption has been made (please help review)  -  ll fetches with isolation != FetchLogEnd are consumer fetches   Optionally LagMetaData can be added to the FetchDataInfo structure and use the fetchOnlyFromLeader = isFromFollower to derive this information. Note that broker metrics are not available at partition later but that is something that can be added.  Unit Tests have beed added to test the byte and time lag  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rite2nikhil","2019-08-29T20:02:08Z","2019-10-23T05:22:21Z"
"","7386","KAFKA-8179: Part 7, cooperative rebalancing in Streams","Key improvements with this PR:  - tasks will remain available for IQ during a rebalance (but not during restore) - continue restoring and processing standby tasks during a rebalance - continue processing active tasks during rebalance until the `RecordQueue` is empty* - only revoked tasks must suspended/closed - `StreamsPartitionAssignor` tries to return tasks to their previous consumers within a client  *but do not try to commit, for now (pending [KAFKA-7312](https://github.com/apache/kafka/pull/7312))","closed","kip,","ableegoldman","2019-09-25T00:54:06Z","2020-06-26T22:38:20Z"
"","7238","KAFKA-8755: Fix state restore for standby tasks with optimized topology","Key changes include:  1. Moves general offset limit updates down to StandbyTask. 2. Updates offsets for StandbyTask at most once per commit and only when we need and updated offset limit to make progress. 3. Avoids writing an 0 checkpoint when StandbyTask.update is called but we cannot apply any of the records. 4. Avoids going into a restoring state in the case that the last checkpoint is greater or equal to the offset limit (consumer committed offset). This needs special attention please. Code is in StoreChangelogReader. 5. Does update offset limits initially for StreamTask because it provides a way to prevent playing to many records from the changelog (also the input topic with optimized topology).  NOTE: this PR depends on KAFKA-8816, which is under review separately. Fortunately the changes involved are few. You can focus just on the KAFKA-8755 commit if you prefer.  @guozhangwang @mjsax @cadonna   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cpettitt-confluent","2019-08-22T17:21:34Z","2019-09-13T22:45:30Z"
"","6736","KAFKA-8361. Fix ConsumerPerformanceTest#testNonDetailedHeaderMatchBody to test a real ConsumerPerformance's method","kafka.tools.ConsumerPerformanceTest#testNonDetailedHeaderMatchBody doesn't work as a regression test, since it checks the number of the fields which are output by an inline `println`, not by a real method of ConsumerPerformance. This PR makes ConsumerPerformance's output logic an independent method and testNonDetailedHeaderMatchBody test it. It also includes some formatting fixes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","sekikn","2019-05-15T03:40:13Z","2019-05-15T03:40:13Z"
"","7361","KAFKA-8834: Add reassignment metrics (KIP-352)","KAFKA-8834: Add reassignment metrics (KIP-352)  KIP-352 aims to add several new metrics in order to track reassignments much better. We will be able to measure bytes in/out rate and the count of partitions under active reassignment. We also change the semantic of the UnderReplicatedPartitions metric to cater better for reassignment. Currently it reports under-replicated partitions when during reassignment extra partitions are added as part of the process but this PR changes it so it'll always take the original replica set into account when computing the URP metrics.  The newly added metrics will be:  - kafka.server:type=ReplicaManager,name=ReassigningPartitions - kafka.server:type=BrokerTopicMetrics,name=ReassignmentBytesOutPerSec - kafka.server:type=BrokerTopicMetrics,name=ReassignmentBytesInPerSec  The changed URP metric: - kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-09-18T16:13:56Z","2019-10-18T05:54:29Z"
"","6698","MINOR: correctly parse version OffsetCommitResponse version < 3","KAFKA-7903: automatically generate OffsetCommitRequest (#6583) introduced a change that cause consumer breakage when OffsetCommitResponse versions < 3 are parsed, as they do not include a throttle_time_ms field. This PR fixes the parsing by supplying the correct version to the OffsetCommitResponse constructor in AbstractResponse.parseResponse.  I have tested this change against many of the compatibility system tests, and it has fixed all the failures that I have tested thus far.","closed","","lbradstreet","2019-05-08T02:17:50Z","2019-05-08T05:55:05Z"
"","7475","KAFKA-8725: Improve LogCleanerManager#grabFilthiestLog error handling","KAFKA-7215 improved the log cleaner error handling to mitigate thread death but missed one case. Exceptions in grabFilthiestCompactedLog still cause the thread to die.  This patch improves handling to ensure that errors in that function still mark a partition as uncleanable and do not crash the thread.","closed","","stanislavkozlovski","2019-10-09T09:31:40Z","2019-10-11T22:10:30Z"
"","7127","KAFKA-8725: Improve LogCleanerManager#grabFilthiestLog error handling","KAFKA-7215 improved the log cleaner error handling to mitigate thread death but missed one case. Exceptions in `grabFilthiestCompactedLog` still cause the thread to die.   This patch improves handling to ensure that errors in that function still mark a partition as uncleanable and do not crash the thread.","closed","","stanislavkozlovski","2019-07-29T08:56:50Z","2019-10-09T09:32:23Z"
"","7483","KAFKA-8897: Warn about no guaranteed backwards compatibility in RocksDBConfigSetter","Kafka Streams cannot guarantee backwards compatibility of code in a user defined RocksDBConfigSetter since it does not control backwards compatibility of RocksDB. RocksDB may remove methods without deprecating them as it has already done in the past with CompactionOptionsFIFO#setTtl().  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-10-10T11:13:08Z","2019-10-18T14:10:57Z"
"","6968","KAFKA-8564: Fix NPE on deleted partition dirs","Kafka should not NPE while loading a deleted partition dir with no log segments  Co-authored-by: Edoardo Comar  Co-authored-by: Mickael Maison   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-06-19T13:55:02Z","2019-06-25T16:04:44Z"
"","7203","KAFKA-8792; Default ZK configuration to disable AdminServer","Kafka ships with default ZK configuration. With the upgrade to ZK 3.5, our defaults include running ZK's AdminServer on port 8080. This is an unfortunate default as it tends to cause conflicts.  I suggest we default to disable ZK's AdminServer in the default ZK configs that we ship. Users who want to use AdminServer can enable it and set the port to something that works for them. Realistically, in most production environments, a different ZK server will be used anyway. So this is mostly to save new users who are trying Kafka on their own machine from running into accidental and frustrating port conflicts.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gwenshap","2019-08-13T05:14:54Z","2019-08-14T06:10:43Z"
"","7240","Increased Warnings for Production Deployment","Kafka is insecure by default and must be securely configured prior to any network deployment. The documentation didn't clearly call this out, so some slight updates we're made with linkable id's.","open","","tacticalrce","2019-08-22T18:38:16Z","2019-08-23T11:05:49Z"
"","7246","MINOR: Post commit hooks for connect sink task","Kafka connect `WorkerSinkTask`s have a callback for when offsets have been committed. For connectors that may want to use checkpointing with these offsets it would be helpful to feed this back to the `SinkTask` itself.","open","connect,","charlibot","2019-08-23T15:28:41Z","2020-03-22T00:03:40Z"
"","6932","Kafka-8534","Kafka 8534 - Fix retention.bytes","closed","","Evelyn-Bayes","2019-06-13T11:14:54Z","2019-06-18T10:54:43Z"
"","7016","MINOR: Remove redundant placeholder in log message","Just remove redundant placeholder in log message, and the comment in here  > ... this should basically never happen --  It will raise RecordTooLargeException if the message size is larger than the maximum request size you have configured with the max.request.size configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pan3793","2019-06-29T02:45:58Z","2019-07-03T18:23:24Z"
"","6983","MINOR: Remove redundant placeholder in log message","Just remove redundant placeholder in log message, and the comment in here  > ... this should basically never happen --  It will raise RecordTooLargeException if the message size is larger than the maximum request size you have configured with the max.request.size configuration.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pan3793","2019-06-21T19:16:36Z","2019-06-24T07:01:57Z"
"","6889","remove redundant semicolon","just remove 2 redundant semicolon in KafkaApis.scala","closed","","Wennn","2019-06-05T16:20:21Z","2019-06-25T16:09:55Z"
"","7144","MINOR: substitute INDEX_INTERVAL_BYTES_DOC for INDEX_INTERVAL_BYTES_DOCS","just for consistency :)","closed","","chia7712","2019-08-01T07:59:43Z","2020-03-17T08:59:17Z"
"","7219","MINOR: Fixing log format typo","Just fixed a log format typo.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pan3793","2019-08-17T11:41:45Z","2019-08-17T15:58:44Z"
"","7424","MINOR: Port changes from trunk for test stability to 2.3 branch","Just a port of changes from trunk to `2.3` for test stability.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-09-30T23:40:14Z","2019-10-14T22:57:05Z"
"","7228","Remove CompletedFetch type from Fetcher","Jira issue [KAFKA-8822](https://issues.apache.org/jira/browse/KAFKA-8822).  The `CompletedFetch` type is no longer required due to the work merged in PR #6988 ([KAFKA-7548](https://issues.apache.org/jira/browse/KAFKA-7548)).  This PR consolidates `CompletedFetch` into `PartitionRecords` as discussed in https://github.com/apache/kafka/pull/6988#issuecomment-516767890 with @hachikuji.","closed","","seglo","2019-08-21T01:11:59Z","2019-09-06T06:03:14Z"
"","7357","MINOR: Add last modified time and deletion horizon to clear log message","It's useful to know when the cleaner runs what the last modified time of the segment and the deletion horizon is. The current log message only allows you to infer that one is greater than the other.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-09-18T01:34:57Z","2019-09-19T22:18:54Z"
"","7335","MINOR: Cleanup few warnings","It removes few warnings. Mainly unused imports or vars.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-09-16T08:16:45Z","2019-09-17T07:08:44Z"
"","6811","KAFKA-8429; Handle offset change when OffsetForLeaderEpoch inflight","It is possible for the offset of a partition to be changed while we are in the middle of validation. If the OffsetForLeaderEpoch request is in-flight and the offset changes, we need to redo the validation after it returns.  Additionally, this patch adds test cases for the SubscriptionState validation API. We fix a small bug handling broker downgrades. Basically we should skip validation if the latest metadata does not include leader epoch information.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-24T21:06:03Z","2019-05-30T15:50:46Z"
"","7032","KAFKA-8628: Auto-generated Kafka RPC code should be able to use zero-copy ByteBuffers","It is often more efficient to deal with large fields of type ""bytes"" as ByteBuffer objects, rather than Java arrays.  ByteBuffers can be used in a ""zero-copy"" fashion, where they simply serve as pointers into already allocated memory.  This avoids the need to copy the data, as would be necessary when using a regular Java byte array.  There is a drawback to using zero-copy ByteBuffers, though: when they are used, the ByteBuffer from which we deserialized the request must remain valid until we're done with the request.  So the underlying buffer cannot be reused during this period.  Therefore, we don't want all bytes fields to show up as ByteBuffers.  This change adds a ""style"" field to the RPC specifications which allows specifying that a ""bytes"" field should be deserialized as a zero-copy ByteBuffer rather than as a byte array.","closed","","cmccabe","2019-07-03T23:53:09Z","2019-11-15T19:23:07Z"
"","6939","MINOR: Add system tests README link from main README","It is not immediately obvious to a newcomer where our system tests are and how to run them.","closed","","stanislavkozlovski","2019-06-14T13:14:44Z","2019-06-14T13:21:38Z"
"","6947","KAFKA-8544: Remove legacy kafka.admin.AdminClient","It has been deprecated since 0.11.0, it was never meant as a publicly supported API and people should use `org.apache.kafka.clients.admin.AdminClient` instead. Its presence causes confusion and people still use them accidentally at times.  `BrokerApiVersionsCommand` uses one method that is not available in `org.apache.kafka.clients.admin.AdminClient`, we inline it for now.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-16T08:06:58Z","2019-06-21T06:43:39Z"
"","7362","KAFKA-8901; Extend consumer group command to use the new Admin API to delete consumer offsets (KIP-496)","It add support to delete offsets in the `kafka-consumer-group`.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-09-18T16:54:34Z","2020-08-11T06:49:17Z"
"","7456","KAFKA-8997; Make Errors a first class type in the auto-generated protocol.","Introduces a new `error` type in the generated protocol's specs which maps to the `Errors` enum in Java/Scala. The `error` type does not change the wire format which remains an `int16`.  As a first example, the `ApiVersionsResponse` has been updated to use the new type. Tests in the `generator` are limited but the unit and integration tests around the `ApiVersionsResponse` cover the new `error` type.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-10-07T11:53:22Z","2022-02-04T20:38:37Z"
"","6592","KAFKA-8326: Introduce List Serde","Introduce List serde for primitive types or custom serdes with a serializer and a deserializer according to https://cwiki.apache.org/confluence/display/KAFKA/KIP-466:+Add+support+for+List%3CT%3E+serialization+and+deserialization  Test cases for the new serde will be added once the PR is reviewed by repo maintainers.","closed","kip,","yeralin","2019-04-17T20:04:01Z","2021-05-14T13:36:40Z"
"","6527","MINOR: Unnecessary Access-specifiers","interface does not need an access specifier  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-04-01T08:39:09Z","2019-04-09T09:20:38Z"
"","7143","MINOR: Avoid dividing by zero","IntelliJ showed a warning about it...  This affects `2.3`, `2.2`, and `2.1` and we should backport the fix.","closed","streams,","mjsax","2019-08-01T01:19:00Z","2019-08-03T21:28:01Z"
"","7441","KAFKA-8972 (2.4 blocker): correctly release lost partitions during consumer.unsubscribe()","Inside onLeavePrepare we would look into the assignment and try to revoke the owned tasks and notify users via RebalanceListener#onPartitionsRevoked, and then clear the assignment.  However, the subscription's assignment is already cleared in this.subscriptions.unsubscribe(); which means user's rebalance listener would never be triggered. In other words, from consumer client's pov nothing is owned after unsubscribe, but from the user caller's pov the partitions are not revoked yet. For callers like Kafka Streams which rely on the rebalance listener to maintain their internal state, this leads to inconsistent state management and failure cases.  Before KIP-429 this issue is hidden away since every time the consumer re-joins the group later, it would still revoke everything anyways regardless of the passed-in parameters of the rebalance listener; with KIP-429 this is easier to reproduce now.  Our fixes are following:  • Inside `unsubscribe`, first do `onLeavePrepare / maybeLeaveGroup` and then `subscription.unsubscribe`. This we we are guaranteed that the streams' tasks are all closed as revoked by then. • [Optimization] If the generation is reset due to fatal error from join / hb response etc, then we know that all partitions are lost, and we should not trigger `onPartitionRevoked`, but instead just `onPartitionsLost` inside `onLeavePrepare`. This is because we don't want to commit for lost tracks during rebalance which is doomed to fail as we don't have any generation info. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","abbccdda","2019-10-03T20:57:20Z","2019-10-29T17:44:08Z"
"","7177","KAFKA-8736: Track size in InMemoryKeyValueStore","InMemoryKeyValueStore uses ConcurrentSkipListMap#size which takes linear time as it iterates over the entire map. We should just track size ourselves for `approximateNumEntries`  Should be cherry-picked back to 2.3","closed","streams,","ableegoldman","2019-08-07T23:16:43Z","2020-06-26T22:38:42Z"
"","7169","MINOR: Make AbstractConfig logger protected","Inherited configs (Producer, Consumer, Streams, Kafka, Log) could use the logger of the AbstractConfig as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-08-06T18:15:23Z","2020-04-24T23:49:43Z"
"","7224","KAFKA-8391, KAFKA-8661: Improve flaky Connect rebalance integration tests","Increased the timeout by a substantial amount, though this will only take effect if the test conditions are not met successfully.  Added logic to allow tests to wait for a number of records to have been written to / consumed from a specific topic. This involved extracting the previously-duplicated logic in `ConnectorHandle` and TaskHandle (test framework classes) into a new `RecordLatches` class that can be easily reused by both those handle classes.  These integration tests are passing were occasionally failing for me locally, but now I’ve run them successfully nearly a dozen times in a row. If the Jenkins builds are a bit slower, this might help the tests fail less frequently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-08-19T19:59:06Z","2020-06-12T15:27:34Z"
"","7151","MINOR: Sticky Partitioner Cleanup","Includes some minor formatting changes for Sticky Partitioner code.","closed","","jolshan","2019-08-01T22:06:12Z","2019-08-12T21:17:29Z"
"","6945","MINOR: Fix expected output in Streams quickstart","Include the topic config `segment.bytes`.","closed","","vahidhashemian","2019-06-15T16:49:04Z","2019-06-16T04:02:24Z"
"","6957","KAFKA-8538 (part of KIP-345): add group.instance.id to DescribeGroup","Include group.instance.id in the describe group result for better visibility. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-17T21:32:54Z","2020-07-06T21:04:05Z"
"","6783","MINOR: Refined code for electPreferredLeaders","In ZkCommand.electPreferredLeaders, `partitionFromZk` is always evaluated no matter if it will used or not later. Should move this calculation near where it's used.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huxihx","2019-05-22T01:08:37Z","2019-05-23T13:17:04Z"
"","6806","KAFKA-8422; Client should send OffsetForLeaderEpoch only if broker supports latest version","In the olden days, OffsetForLeaderEpoch was exclusively an inter-broker protocol and required Cluster level permission. With KIP-320, clients can use this API as well and so we lowered the required permission to Topic Describe. The only way the client can be sure that the new permissions are in use is to require version 3 of the protocol which was bumped for 2.3. If the broker does not support this version, we skip the validation and revert to the old behavior.  Additionally, this patch fixes a problem with the newly added `replicaId` field when parsed from older versions which did not have it. If the field was not present, then we used the consumer's sentinel value, but this would limit the range of visible offsets by the high watermark. To get around this problem, this patch adds a separate ""debug"" sentinel similar to APIs like Fetch and ListOffsets.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-24T07:04:43Z","2019-05-25T04:30:02Z"
"","6640","MINOR: Fix directory name inconsistency in the Kafka Streams tutorial","In the Kafka Streams tutorial, ""streams.examples"" is specified for `mvn archetype:generate`'s `artifactId` parameter. So the project directory is created as the same name, but it's changed to ""streams-quickstart"" in the following explanations.","open","","sekikn","2019-04-26T15:09:46Z","2019-05-13T23:01:48Z"
"","6755","KAFKA-8265 : Fix config name to match KIP-458, Return a copy of the ConfigDef in Client Configs.","In the initial implementation for KIP-458 https://github.com/apache/kafka/pull/6624, the config name was incorrect and not consistent with what was specified in the KIP. This PR fixes the inconsistency.  There was also a concern raised about the mutability of `ConfigDef` in https://github.com/apache/kafka/pull/6624#pullrequestreview-238877899. I have made an attempt to fix it by returning a copy every time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-05-17T17:12:41Z","2020-10-16T06:19:09Z"
"","6606","MINOR: Move log statement stating producer closed after possible exception","In the `KafkaProducer#close` method we have a debug log statement `Kafka producer has been closed` then a few lines later an exception `KafkaException(""Failed to close kafka producer"", exception);` can occur.    This could be confusing to users, so this PR simply moves the log statement to after the possible exception to avoid confusing information in the logs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","bbejeck","2019-04-18T22:42:28Z","2019-04-22T22:59:16Z"
"","7341","KAFKA-8842: Reading/Writing confused in Connect QuickStart Guide","In step 7 of the QuickStart guide, ""**Writing** data from the console and writing it back to the console"" should be ""**Reading** data from the console and writing it back to the console"".  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","alaazbair","2019-09-16T18:42:23Z","2020-03-22T05:07:32Z"
"","6893","Hot Fix: Close unused ColumnFamilyHandle","In RocksDBTimestampedStore#openRocksDB we try to open a db with two column families. If this succeeds but the first column family is empty (db.newIterator.seekToFirst.isValid() == false) we never actually close its ColumnFamilyHandle","closed","streams,","ableegoldman","2019-06-05T23:47:47Z","2019-06-06T18:30:38Z"
"","7124","KAFKA-8722: Data crc check repair","In our production environment, when we consume kafka's topic data in an operating program, we found an error:  org.apache.kafka.common.KafkaException: Record for partition rl_dqn_debug_example-49 at offset 2911287689 is invalid, cause: Record is corrupt (stored crc = 3580880396, computed crc = 1701403171) at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:869) at org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(Fetcher.java:788) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:480) at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1188) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1046) at kafka.consumer.NewShinyConsumer.receive(BaseConsumer.scala:88) at kafka.tools.ConsoleConsumer$.process(ConsoleConsumer.scala:120) at kafka.tools.ConsoleConsumer$.run(ConsoleConsumer.scala:75) at kafka.tools.ConsoleConsumer$.main(ConsoleConsumer.scala:50) at kafka.tools.ConsoleConsumer.main(ConsoleConsumer.scala) At this point we used the kafka.tools.DumpLogSegments tool to parse the disk log file and found that there was indeed dirty data. By looking at the code, I found that in some cases kafka would not verify the data and write it to disk, so we fixed it. We found that when record.offset is not equal to the offset we are expecting, kafka will set the variable inPlaceAssignment to false. When inPlaceAssignment is false, data will not be verified.","closed","","lordcheng10","2019-07-27T07:46:30Z","2019-07-31T15:30:55Z"
"","7123","[KAFKA-8722]Data crc check repair","In our production environment, when we consume kafka's topic data in an operating program, we found an error:  org.apache.kafka.common.KafkaException: Record for partition rl_dqn_debug_example-49 at offset 2911287689 is invalid, cause: Record is corrupt (stored crc = 3580880396, computed crc = 1701403171) at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:869) at org.apache.kafka.clients.consumer.internals.Fetcher.parseCompletedFetch(Fetcher.java:788) at org.apache.kafka.clients.consumer.internals.Fetcher.fetchedRecords(Fetcher.java:480) at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1188) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1046) at kafka.consumer.NewShinyConsumer.receive(BaseConsumer.scala:88) at kafka.tools.ConsoleConsumer$.process(ConsoleConsumer.scala:120) at kafka.tools.ConsoleConsumer$.run(ConsoleConsumer.scala:75) at kafka.tools.ConsoleConsumer$.main(ConsoleConsumer.scala:50) at kafka.tools.ConsoleConsumer.main(ConsoleConsumer.scala) At this point we used the kafka.tools.DumpLogSegments tool to parse the disk log file and found that there was indeed dirty data. By looking at the code, I found that in some cases kafka would not verify the data and write it to disk, so we fixed it. We found that when record.offset is not equal to the offset we are expecting, kafka will set the variable inPlaceAssignment to false. When inPlaceAssignment is false, data will not be verified.","closed","","lordcheng10","2019-07-27T07:27:19Z","2019-07-27T07:45:10Z"
"","6635","add a shell file to build project.","In order to facilitate the compilation of project code, a build script file has been added.","closed","","lordcheng10","2019-04-25T08:09:52Z","2019-04-25T08:26:13Z"
"","7275","MONIR: Check for NULL in case of version probing","In case of version probing we would skip the logic for setting cluster / assigned tasks; since these values are initialized as null they are vulnerable to NPE when code changes.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-08-29T22:38:15Z","2020-04-24T23:52:23Z"
"","7015","KAFKA-8436: use automated protocol for AddOffsetsToTxn","In an effort to replace the protocol of AddOffsetsToTxn API to automated. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-29T00:00:00Z","2020-04-11T11:54:54Z"
"","7401","KAFKA-8609: add rebalance-total-time","In addition to the existing metrics added in KAFKA-8609, add the total (cumulative) time spent during rebalances.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-09-26T20:19:23Z","2019-10-01T15:05:16Z"
"","6992","Fix for resolving variables for dynamic config as per KIP-421","In add/alter new configs for DynamicConfigs it does not go through the KafkaConfig eg: bin/kafka-configs --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --alter --add-config log.cleaner.threads=2 However the bootstrap-server localhost is parsed through the kafkaConfig to create the adminClient but not the log.cleaner.thread.   As the configs are not parsed using the KafkaConfig if we pass variables in configs they are bot resolved at run time.  In order to resolve the variables in alterConfig/addConfigs flow we need to parse the new configs  using KafkaConfig before they are parsed.","closed","","tadsul","2019-06-24T15:29:46Z","2019-08-16T07:39:38Z"
"","7235","KAFKA-8824: bypass value serde on null","In a KTable context, we should not pass `null` into a user-supplied serde.  Testing: I verified that the change to the test results in test failures without the patch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-08-21T23:43:03Z","2019-08-22T21:51:02Z"
"","7443","KAFKA-9039: Optimize ReplicaFetcher fetch path","Improves the performance of the replica fetcher for high partition count fetch requests, where a majority of the partitions did not update between fetch requests. All benchmarks were run on an r5x.large.  Vanilla Benchmark                                  (partitionCount)  Mode  Cnt        Score       Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    26491.825 ±   438.463  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   153941.952 ±  4337.073  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   339868.602 ±  4201.462  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  2588878.448 ± 22172.482  ns/op  From 100 to 5000 partitions the latency increase is 2588878.448 / 26491.825 = 97.  Avoid gettimeofdaycalls in steady state fetch states 854588804d7cf96a10d6e4a74db533deb7e26b55  Benchmark                                  (partitionCount)  Mode  Cnt        Score      Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    22685.381 ±  267.727  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   113622.521 ± 1854.254  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   273698.740 ± 9269.554  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  2189223.207 ± 1706.945  ns/op  From 100 to 5000 partitions the latency increase is 2189223.207 / 22685.381 = 97X  Avoid copying partition states to maintain fetch offsets  29fdd6094b828153c762f1d99b645481f7200cee  Benchmark                                  (partitionCount)  Mode  Cnt        Score      Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    17039.989 ±  609.355  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15    99371.086 ± 1833.256  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   216071.333 ± 3714.147  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  2035678.223 ± 5195.232  ns/op  From 100 to 5000 partitions the latency increase is 2035678.223 / 17039.989 = 119X  Keep lag alongside PartitionFetchState to avoid expensive isReplicaInSync check 0e57e3e725ff70ad8845681b02873229c3c64544  Benchmark                                  (partitionCount)  Mode  Cnt        Score      Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    15131.684 ±  382.088  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15    86813.843 ± 3346.385  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   193050.381 ± 3281.833  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  1801488.513 ± 2756.355  ns/op  From 100 to 5000 partitions the latency increase is 1801488.513 / 15131.684 = 119X  Fetch session optimizations (mostly presizing the next hashmap, and avoiding making a copy of sessionPartitions, as a deep copy is not required for the ReplicaFetcher) 2614b2417a6aac87e5decbae0a29d4f37ba9c7e3  Benchmark                                  (partitionCount)  Mode  Cnt        Score      Error  Units ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    11386.203 ±  416.701  ns/op ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15    60820.292 ± 3163.001  ns/op ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15   146242.158 ± 1937.254  ns/op ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  1366768.926 ± 3305.712  ns/op  From 100 to 5000 partitions the latency increase is 1366768.926 / 11386.203 = 120","closed","","lbradstreet","2019-10-04T02:10:01Z","2019-10-16T16:49:54Z"
"","6798","MINOR: Update jackson to 2.9.9","Important fix: https://github.com/FasterXML/jackson-databind/issues/2326  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-23T20:34:10Z","2019-05-24T00:01:16Z"
"","6686","KAFKA-8286; Leader Election Admin RPC (KIP-460)","Implements KIP-460: https://cwiki.apache.org/confluence/display/KAFKA/KIP-460%3A+Admin+Leader+Election+RPC  Pending: 1. Implement additional tests 1. Improve documentation  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jsancio","2019-05-06T22:54:28Z","2019-05-29T18:57:56Z"
"","6624","KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458)","Implementation to enable policy for Connector Client config overrides. This is implemented per the KIP-458.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-04-23T23:19:36Z","2020-10-16T06:19:06Z"
"","6843","KAFKA-8447: New Metric to Measure Number of Tasks on a Connector","Implementation of KIP-475.  Includes a new metric for Connectors: ""connector-task-count"".  A unit test is added.  Manually tested ensured the expected behavior.","closed","connect,","cyrusv","2019-05-30T05:56:51Z","2020-10-16T06:17:27Z"
"","7288","KAFKA-7931 : [Proposal] Fix metadata fetch for ephemeral brokers behind a Virtual IP","If we have ephemeral brokers sitting behind a Virtual IP and when all the brokers go down, the client won't be able to reconnect as mentioned in: https://issues.apache.org/jira/browse/KAFKA-7931. This is because we take the bootstrap nodes and completely forget about it once the first metadata response comes in (and then we create a new metadata cache and a new cluster). Now when all the brokers go down before the metadata is updated, then the client will be stuck unless it is rebooted.   This patch simply stores the bootstrap brokers list. Instead of simply giving up when a 'leastLoadedNode' is not found, we simply use one of the bootstrap nodes to get the metadata. Also we can make sure to use the bootstrap nodes only when the bootstrap node is not part of the set of nodes on the cluster.  Testing -------- * Manual Testing - Setup ephemeral brokers behind a VIP. Recreate all the ephemeral brokers (so that they change their IPs) * NetworkClient Unit Test - Test metadata with bootstrap - being the same as the node on the cluster and also different than the node on the cluster.  Note: This doesn't change any existing system behavior and this code path will be hit only if we are unable to find any `leastLoadedNode`","open","","aravindvs","2019-09-03T20:33:57Z","2020-09-08T13:25:30Z"
"","7091","KAFKA-8670: Fix kafka-topics.sh --describe without --topic when cluster has no topics.","If there are no topics in a cluster, kafka-topics.sh --describe without a --topic option should return empty list, not throw an exception.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  We pass a boolean flag to `ensureTopicExists` method indicating whether to throw an exception if there are no topics in the cluster. In case of `kafka-topics.sh --describe`, the exception **should NOT** be thrown if either of these are true - 1. A `--topic` option was not passed to the CLI. In that case, the output should be empty. 2. A `--if-exists` option was passed to the CLI.  Earlier, the first condition was not part of the check. This bugfix adds the first condition mentioned above to the check.  I added the necessary unit test to check for this case.  Also, I created a Kafka cluster, and without creating any topics on it, ran  ``` ./kafka-topics.sh --zookeeper  --describe ```  This did not throw any exception, like it used to earlier.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wyuka","2019-07-15T21:21:13Z","2019-07-15T22:51:52Z"
"","7094","KAFKA-8670: Fix exception for kafka-topics.sh --describe without --topic mentioned","If there are **no topics** in a cluster, kafka-topics.sh --describe without a --topic option should return empty list, not throw an exception.  We pass a boolean flag to `ensureTopicExists` method indicating whether to throw an exception if there are no topics in the cluster. In case of `kafka-topics.sh --describe`, the exception **should NOT** be thrown if either of these are true - 1. A `--topic` option was not passed to the CLI. In that case, the output should be empty. 2. A `--if-exists` option was passed to the CLI. Earlier, the first condition was not part of the check. This bugfix adds the first condition mentioned above to the check.  **NOTE**: I have added the `desiredTopicName` argument to the ensureTopicExists to check if it is defined. I could have simply passed `!desiredTopicName.isDefined || topicOptWithExists` in case of describe and it would still fix this bug. However, to fix bug KAFKA-8053, we need to improve the error message of the exception. The `desiredTopicName` will be required to build this exception message. I have not combined that messaging fix with this one to keep the commits for two bugs separate.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  I added the necessary unit test to check for this case. Also, I ran these commands. The output before and after my change are mentioned inline  **Describe without topic name - Before** ``` ./kafka-topics.sh --zookeeper 10.0.32.180:2181,10.0.16.211:2181,10.0.0.220:2181 --describe Error while executing topic command : Topics in [] does not exist [2019-07-15 23:23:25,674] ERROR java.lang.IllegalArgumentException: Topics in [] does not exist 	at kafka.admin.TopicCommand$.kafka$admin$TopicCommand$$ensureTopicExists(TopicCommand.scala:416) 	at kafka.admin.TopicCommand$ZookeeperTopicService.describeTopic(TopicCommand.scala:332) 	at kafka.admin.TopicCommand$.main(TopicCommand.scala:66) 	at kafka.admin.TopicCommand.main(TopicCommand.scala)  (kafka.admin.TopicCommand$) ```  **Describe without topic name - After (no output)** ``` ./kafka-topics.sh --zookeeper 10.0.32.180:2181,10.0.16.211:2181,10.0.0.220:2181 --describe ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wyuka","2019-07-15T23:43:21Z","2019-07-18T20:25:26Z"
"","7251","fix bug:If the client loses a response, the queue corresponding to th…","If the client loses a response, the corresponding queue will never succeed because the correlationId in the response is always greater than the last one in the request queue","closed","","songShiPeng","2019-08-25T03:13:59Z","2021-03-17T07:34:58Z"
"","7309","KAFKA-8884: class cast exception improvement","If a processor causes a class cast exception, atm you get a bit of a cryptic error if you're not used to them, and without a context sensitive suggestion as to what could be wrong. Often these can be cause by miss-configured Serdes (defaults).  Old error: ``` Caused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.String ([B and java.lang.String are in module java.base of loader 'bootstrap') ```  An example of the improvement over the case exception: ``` org.apache.kafka.streams.errors.StreamsException: Exception caught in process. taskId=0_0, processor=KSTREAM-SOURCE-0000000000, topic=streams-plaintext-input, partition=0, offset=0, stacktrace=org.apache.kafka.streams.errors.StreamsException:  ClassCastException invoking Processor. Do the Processor's input types match the deserialized types? Check  the Serde setup and change the default Serdes in StreamConfig or provide correct Serdes via method  parameters. Deserialized input types are: key: [B, value: [B 	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:123) 	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:201) 	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:180) 	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:133) 	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:87) 	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:366) 	at org.apache.kafka.streams.TopologyTestDriver.pipeInput(TopologyTestDriver.java:419) 	at org.apache.kafka.streams.processor.internals.ProcessorNodeTest.testTopologyLevelClassCastException(ProcessorNodeTest.java:176) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:566) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305) 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:365) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:330) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:78) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:328) 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:65) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:292) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:305) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:412) 	at org.junit.runner.JUnitCore.run(JUnitCore.java:137) 	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) 	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) 	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) 	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) Caused by: java.lang.ClassCastException: class [B cannot be cast to class java.lang.String ([B and java.lang.String are in module java.base of loader 'bootstrap') 	at org.apache.kafka.streams.kstream.internals.AbstractStream.lambda$withKey$1(AbstractStream.java:103) 	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:40) 	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:119) 	... 32 more ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","astubbs","2019-09-06T13:57:14Z","2019-10-17T07:24:32Z"
"","6619","KAFKA-8275; Take throttling into account when choosing least loaded node","If a node is currently throttled, we should take it out of the running for `leastLoadedNode`. Additionally, current logic seems to favor connecting to new nodes rather than using existing connections which have one or more in flight requests. The javadoc is slightly vague about whether this is expected, but it seems not.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-22T16:45:22Z","2019-05-07T20:13:19Z"
"","7237","KAFKA-8391: Temporarily ignore flaky Connect rebalance integration tests","I've spent quite a bit of time on trying to discover the root cause, with no luck so far. I have been able to reproduce it locally by running the following 100 times: ``` ./gradlew connect:runtime:clean connect:runtime:test --tests org.apache.kafka.connect.integration.RebalanceSourceConnectorsIntegrationTest ``` The `testReconfigConnector` test failed 28% of the time and the others failed 0%. This issue and KAFKA-8661 suggest that `testDeleteConnector` and `testStartTwoConnectors` are also flaky, though I've not seen those tests fail locally.  Because this flakiness is causing issues for the rest of the project, I'm going to temporarily ignore several of the flaky ITs while I continue to investigate: * `RebalanceSourceConnectorsIntegrationTest.testReconfigConnector` * `RebalanceSourceConnectorsIntegrationTest.testDeleteConnector` * `RebalanceSourceConnectorsIntegrationTest.testStartTwoConnectors`   **This should be backported to the `2.3` branch, which is when these integration tests were first added.**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rhauch","2019-08-22T15:10:32Z","2019-08-25T21:30:30Z"
"","7040","MINOR: Improve KafkaController code","I was reading through the KafkaController code (I only managed to go halfway through) and cleaning code along the way, figuring that if I find enough stuff it may be worth opening a minor PR to improve things. Well, here we are:  * Do not initialize the KafkaScheduler for auto preferred leader election when the functionality is configured to be off * Add a constant for the empty controlled ID value of -1 * Some minor typo corrections, comment and code refactors","closed","","stanislavkozlovski","2019-07-07T12:00:26Z","2019-10-29T09:32:42Z"
"","7262","MINOR: Fix a wrong path in tests/docker/Dockerfile","I tried to build a Docker image with tests/docker/Dockerfile, but it failed with the following error. This PR fixes a wrong path in it.  ``` $ docker build tests/docker  (snip)  Step 37/43 : RUN curl -s ""$KAFKA_MIRROR/kafka-streams-2.3.0-test.jar"" -o /opt/kafka-2.2.0/libs/kafka-streams-2.3.0-test.jar  ---> Running in 73353b7f1850 The command '/bin/sh -c curl -s ""$KAFKA_MIRROR/kafka-streams-2.3.0-test.jar"" -o /opt/kafka-2.2.0/libs/kafka-streams-2.3.0-test.jar' returned a non-zero code: 23 ```  As a manual test, I ran the same command with this fix and confirmed that the image was successfully built.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-08-28T01:33:12Z","2019-08-28T10:46:11Z"
"","7432","MINOR: Start correlation id at 0 in SaslClientAuthenticator","I tried to add a test for this, but it's actually pretty hard to verify what we want to verify. I could add a test that checks the correlation field after the connection has been established, but it would not catch this kind of bug where the issue is not the value we store, but the value we create the request header with.  I have another PR that avoids intermediate structs during serialization/deserialization, which has a test that fails without this change. So we'll get coverage that way.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-10-02T13:27:34Z","2019-10-03T04:02:37Z"
"","6567","MINOR: Immutable Logic Update","I think it is good to force the code immutable Logic can reduce mistake  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-04-12T05:56:51Z","2020-01-09T23:51:56Z"
"","6566","MINOR: mutable Collection -> Immutable Collection","I think it is good to force the code immutable collections can reduce mistake  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-04-12T05:40:14Z","2019-04-12T05:44:44Z"
"","7356","KAFKA-8086: Use 1 partition for offset topic when possible","I realized some flaky tests failed at `setup` or calls that tries to create offset topics, and I think using one partition and one replica would be sufficient in these cases.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-09-18T01:23:07Z","2020-04-24T23:58:17Z"
"","7064","MINOR: Only invoke hw update logic for follower fetches","I noticed a lot of messages like the following when debugging a test case: ``` [2019-07-10 02:01:23,946] WARN [ReplicaManager broker=0] While updating the HW for follower -1 for partition connect-storage-topic-connect-cluster-0, the replica could not be found. (kafka.server.ReplicaManager:70) ``` In the KIP-392 PR, we added logic to track the high watermark of followers, but it is invoked even for consumer fetches, which results in this log spam. This patch just adds the missing follower check.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-10T09:14:13Z","2019-07-10T15:57:34Z"
"","7204","KAFKA-8794: Deprecate DescribeLogDirsResponse.[LogDirInfo, ReplicaInfo]","I found this glitch while I was working on some monitoring system.  note:  - [Javadoc of `DescribeLogDirsResult` (2.3.0)](https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/admin/DescribeLogDirsResult.html): does not provide link to `org.apache.kafka.common.requests.DescribeLogDirsResponse.LogDirInfo`. - [Javadoc of `DescribeReplicaLogDirsResult` (2.3.0)](https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/admin/DescribeReplicaLogDirsResult.html): provides link to [`DescribeReplicaLogDirsResult.ReplicaLogDirInfo `](https://kafka.apache.org/23/javadoc/org/apache/kafka/clients/admin/DescribeReplicaLogDirsResult.ReplicaLogDirInfo.html)  cc/ @hachikuji  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-08-13T06:45:25Z","2020-07-08T14:27:12Z"
"","6710","MINOR: Remove unnecessary OptionParser#accepts method call from PreferredReplicaLeaderElectionCommand","I found a strange hyphen between the header and the body in kafka-preferred-replica-election.sh's help message. This PR removes it.  ``` $ bin/kafka-preferred-replica-election.sh --help  (snip)  Option                                  Description ------                                  ----------- - --admin.config     Admin client config properties file to ```  I confirmed that its unit test succeeded and the command worked without that line.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-10T08:34:54Z","2019-05-11T06:36:16Z"
"","7211","KAFKA-8800: Increase poll timeout in poll[Records]UntilTrue","I dug out this problem and found the following.  The failing method, `TestUtils#pollRecordsUntilTrue`, was added with commit 1e92b703. However, it calls `Consumer#poll(Duration)` which throws timeout if the given period is expired while fetching both of the metadata and records. (i.e., dislike to deprecated `Consumer#poll(long)` method.) For this reason, it is easy to fail and making the related tests flaky.  It seems like there are two approaches to fix this problem:  1. Increase the timeout duration (current PR) 2. Use a dedicated method to fetch the metadata.  How do you think? cc/ @mjsax  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-08-14T13:50:02Z","2019-08-17T16:28:39Z"
"","7313","KAFKA-8817: Remove timeout for the whole test","I bumped into this flaky test while working on another PR. It's a bit different from the reported PR, where it actually timed out at parsing localhost:port already. I think what we should really test is that the closing call can complete, so I removed the whole test timeout and add the timeout for the shutdown latch instead.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-09-07T05:24:31Z","2020-04-24T23:58:15Z"
"","7393","KAFKA-8944: Fixed KTable compiler warning.","https://issues.apache.org/jira/browse/KAFKA-8944  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","huxihx","2019-09-26T02:24:38Z","2019-10-09T06:11:31Z"
"","7236","KAFKA-8819 : fix plugin path loader for converter","https://issues.apache.org/jira/browse/KAFKA-8819 describes the issue and scenario in which it happens in detail.  * Create the Connector config after we set the class loader to be connector's classloader. * Set the Converter's classloader as the current class loader while instantiating and configuring the connector. ( Maybe we should potentially do this for the actual converter invocation as well )  The changes were manually tested by including the AvroConverter which used the ServiceLoader mechanism internally in 2 different plugin path. When the connectors were created with AvroConverter only one of them would start successfully before the fix and the other one used to fail with some class mismatch issues in the ServiceLoader. After the fix proposed in the PR, we are able to create the connectors successfully.  This PR doesn't change any deployment requirements for Converter. It allows Converters to be included in either the Plugin path of the connector or in its own path.    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-08-22T04:47:32Z","2020-10-16T05:50:58Z"
"","7112","KAFKA-8713: JsonConverter NULL Values are replaced by default values even in NULLABLE fields","https://issues.apache.org/jira/browse/KAFKA-8713  This should not be merged without an approved KIP.  Update Mar 18 2020 Create [KIP-581: Value of optional null field which has default value](https://cwiki.apache.org/confluence/display/KAFKA/KIP-581%3A+Value+of+optional+null+field+which+has+default+value)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pan3793","2019-07-25T07:09:39Z","2020-04-28T17:25:34Z"
"","6913","KAFKA-8503: Ignore retries config if a custom timeout is provided","https://issues.apache.org/jira/browse/KAFKA-8503  When custom timeout is provided, `retries` config could be ignored for individual APIs in KafkaAdminClient. Tweaked code path for `Call.fail` to pass NPathComplexity check.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-06-11T08:02:47Z","2020-01-30T23:30:51Z"
"","6836","KAFKA-8442:Inconsistent ISR output in topic command","https://issues.apache.org/jira/browse/KAFKA-8442  Modify leader and ISR output for --zookeeper path in TopicCommand, to be consistent with --bootstrap-server path.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-05-29T06:18:42Z","2019-07-30T15:44:12Z"
"","6786","MINOR: Remove superfluous required argument check for bootstrap server, alter option and partitions","https://issues.apache.org/jira/browse/KAFKA-8406  Running ``` ./kafka-topics --bootstrap-server  --alter --config retention.ms=3600000 --topic topic ``` Results in ``` Missing required argument ""[partitions]"" ```  Running ``` ./kafka-topics --bootstrap-server  --alter --config retention.ms=3600000 --topic topic --partitions 25 ``` Results in ``` Option combination ""[bootstrap-server],[config]"" can't be used with option ""[alter]"" ``` For better clarity, we should just throw the last error first.","closed","","stanislavkozlovski","2019-05-22T09:32:31Z","2019-05-29T11:50:51Z"
"","6737","KAFKA-8338: consumer offset expiration should consider subscription.","https://issues.apache.org/jira/browse/KAFKA-8338  Currently only empty groups will be checked to seek any expired offsets. However, if a group is in Stable state but no longer subscribes any partitions, the offsets for these partitions will never be removed.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-05-15T04:07:02Z","2020-06-23T22:22:19Z"
"","6561","KAFKA-9538: Flaky test: testResetOffsetsExportImportPlan","https://issues.apache.org/jira/browse/KAFKA-8211  Reduced offset-committing interval from 5 seconds to 1 second, hoping consumer#committed returns offset more quickly. Besides, enriched the output message for the exceptional case.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-04-11T04:19:02Z","2020-02-13T06:40:43Z"
"","6532","KAFKA-8183: Add retries to WorkerUtils#verifyTopics","https://issues.apache.org/jira/browse/KAFKA-8183","closed","","stanislavkozlovski","2019-04-02T18:57:19Z","2019-04-03T00:09:40Z"
"","6543","Fix with resize","https://issues.apache.org/jira/browse/KAFKA-8154  Resize the appReadBuffer on buffer overflow and continue to the next iteration of the loop to retry the unwrap operation.   The modified jar was used in an environment that reliably reproduced the problem when using an unmodified kafka library. The modified library did not manifest the problem.","open","","Spatterjaaay","2019-04-05T07:05:59Z","2019-04-05T07:05:59Z"
"","6542","attempt to fix KAFKA-8154 buffer overflow exceptions","https://issues.apache.org/jira/browse/KAFKA-8154  Replace the inner while loop with an if statement. The purpose of the loop if to retry the unwrap after resizing of emptying the AppReadBuffer, however, neither of those operations are currently guaranteed.   The modified jar was used in an environment that reliably reproduced the problem when using an unmodified kafka library. The modified library did not manifest the problem.","open","","Spatterjaaay","2019-04-05T07:04:08Z","2019-04-05T07:04:08Z"
"","6541","attempt to fix KAFKA-8154 buffer overflow exceptions","https://issues.apache.org/jira/browse/KAFKA-8154  Break the inner while loop after a successful unwrap operation. This allows the function to return if dst is full.   The modified jar was used in an environment that reliably reproduced the problem when using an unmodified kafka library. The modified library did not manifest the problem.","open","","Spatterjaaay","2019-04-05T06:59:40Z","2019-04-05T06:59:40Z"
"","6557","KAFKA-7965: Fix testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup","https://issues.apache.org/jira/browse/KAFKA-7965  Most of the time, the group coordinator runs on broker 1. Occasionally the group coordinator will be placed on broker 2. If that's the case, the loop starting at line 320 have no chance to check and update `kickedOutConsumerIdx`. A quick fix is to always restart the coordinator to ensure that `kickedOutConsumerIdx` could be checked after the new coordinator with new config gets to work.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-04-10T03:48:02Z","2019-04-17T19:41:47Z"
"","6679","MINOR: Remove workarounds for lz4-java bug affecting byte buffers","https://github.com/lz4/lz4-java/pull/65 was included in lz4-java 1.4.0.  Relying on existing tests for verification.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-05T19:06:19Z","2019-05-06T13:53:54Z"
"","7395","KAFKA-8874: Add consumer metrics to observe user poll behavior (KIP-517)","https://cwiki.apache.org/confluence/display/KAFKA/KIP-517%3A+Add+consumer+metrics+to+observe+user+poll+behavior  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2019-09-26T05:46:44Z","2019-10-17T09:16:59Z"
"","7089","KAFKA-8453: AdminClient describeTopic should handle partition level errors","Here is the draft fix. The approach is simple:  1. Add a new method, `MetadataResponse#partitionErrors`, which returns a map from topic to the set of its partition errors. (It follows how `MetadataResponse#topicMetadata` does.) 2. `Call#handleResponse` in `KafkaAdminClient#describeTopics` now uses both of topic level error map and partition level error map; if there is `ListenerNotFound` error in partition level error map, it calls `KafkaFuture#completeExceptionally`.  However, since it now throws a new Exception, it brokes following tests which are related to the leader election.  - `kafka.server.DynamicBrokerReconfigurationTest.testUncleanLeaderElectionEnable` - `kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersAndNoop` - `kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersForManyPartitions` - `kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersForAllPartitions` - `kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersForOnePartition` - `kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersWhenNoLiveBrokers` - `kafka.api.SaslSslAdminClientIntegrationTest.testElectUncleanLeadersAndNoop` - `kafka.api.SaslSslAdminClientIntegrationTest.testElectUncleanLeadersForManyPartitions` - `kafka.api.SaslSslAdminClientIntegrationTest.testElectUncleanLeadersForAllPartitions` - `kafka.api.SaslSslAdminClientIntegrationTest.testElectUncleanLeadersForOnePartition` - `kafka.api.SaslSslAdminClientIntegrationTest.testElectUncleanLeadersWhenNoLiveBrokers` - `kafka.admin.TopicCommandWithAdminClientTest.testDescribeUnavailablePartitions` - `kafka.admin.TopicCommandWithAdminClientTest.testDescribeUnderMinIsrPartitionsMixed`  Which approach would be appropriate to fix the problem above? Do you have something in mind?  cc/ @hachikuji  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2019-07-15T14:35:07Z","2019-08-14T12:39:59Z"
"","7083","KAFKA-8482: alterReplicaLogDirs should be better documented","Here is the draft fix. In summary:  1. `AdminClient#alterReplicaLogDirs`     - Add documentation on when `InterruptedException` is thrown.     - Add note on `AlterReplicaLogDirsResult` instance. 2. `AlterReplicaLogDirsResult`     - Add a guide to retrieve the results to class documentation.     - Add detailed guide to `AlterReplicaLogDirsResult#values` what returns or what is thrown in which situation.     - Add detailed guide to `AlterReplicaLogDirsResult#all` what returns or what is thrown in which situation.  cc/ @cmccabe  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-07-12T10:29:15Z","2019-10-20T11:43:18Z"
"","6495","Buffer overflow / Underflow exception handling.","Hello,    While testing 3 Node Kafka Cluster in our docker container (using Swarm) environment, we noticed buffer overflow issues when we enable SSL. We run into the exceptions shown below. We actually never ran into a scenario where the buffer exceeded the size of the application buffer size or packet buffer size. But, we see overflow/underflow where they exactly match the size. We noticed these exceptions with both 2.1.0 and 2.1.1.   In our application we have consumers/producers that are Java and Go based.  While I am not aware of what the original reasoning was to actually generate an exception due to buffer over/underflow. As per SSLEngine documentation, one is supposed to resize respective buffers and retry the operation appropriately.   https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/SSLEngine.html  We have both Go (librdkafka) and Java (confluent) clients and these are the tracebacks we see on the brokers, very consistently. This is not even a very high traffic scenario. We are using 2.1.0 Broker. Java version is openjdk version ""1.8.0_191"" OpenJDK Runtime Environment (build 1.8.0_191-b12) OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode  Inviting your comments and insights on the code changes in this PR.   [2019-03-20 10:00:47,664] WARN [SocketServer brokerId=22] Unexpected error from /192.168.2.44; closing connection (org.apache.kafka.common.network.Selector) java.lang.IllegalStateException: Buffer overflow when available data size (16384) >= application buffer size (16384) at org.apache.kafka.common.network.SslTransportLayer.read(SslTransportLayer.java:550) at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94) at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:381) at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:342) at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:609) at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:541) at org.apache.kafka.common.network.Selector.poll(Selector.java:467) at kafka.network.Processor.poll(SocketServer.scala:689) at kafka.network.Processor.run(SocketServer.scala:594) at java.lang.Thread.run(Thread.java:748)  [2019-03-20 10:01:53,423] WARN [ReplicaFetcher replicaId=22, leaderId=23, fetcherId=0] Unexpected error from /192.168.2.23; closing connection (org.apache.kafka.common.network.Selector) java.lang.IllegalStateException: Buffer overflow when available data size (16384) >= application buffer size (16384) at org.apache.kafka.common.network.SslTransportLayer.read(SslTransportLayer.java:550) at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:94) at org.apache.kafka.common.network.KafkaChannel.receive(KafkaChannel.java:381) at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:342) at org.apache.kafka.common.network.Selector.attemptRead(Selector.java:609) at org.apache.kafka.common.network.Selector.pollSelectionKeys(Selector.java:541) at org.apache.kafka.common.network.Selector.poll(Selector.java:467) at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:521) at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:93) at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:97) at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:190) at kafka.server.AbstractFetcherThread.kafka$server$AbstractFetcherThread$$processFetchRequest(AbstractFetcherThread.scala:241) at kafka.server.AbstractFetcherThread$$anonfun$maybeFetch$1.apply(AbstractFetcherThread.scala:130) at kafka.server.AbstractFetcherThread$$anonfun$maybeFetch$1.apply(AbstractFetcherThread.scala:129) at scala.Option.foreach(Option.scala:257) at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:129) at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:111) at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)   Unit Test: Brought up the full cluster and verified that the buffer overflow exception is not seen any more.  Cluster is up and running for a few days.","open","","rnataraja","2019-03-23T21:48:45Z","2019-07-28T00:59:04Z"
"","7411","KAFKA-8952: Update Jackson to 2.10.0","Guava hasn't been upgraded due to potential incompatibility with the reflections library:  https://github.com/ronmamo/reflections/issues/194  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-29T04:33:36Z","2019-10-01T13:31:26Z"
"","7331","MINOR: Default to 5 partitions of the __consumer_offsets topic in Streams integration tests","Given that the tests do not create clusters larger than 3, we do not gain much by creating 50 partitions for that topic. Reducing it should slightly increase test startup and shutdown speed","closed","tests,","stanislavkozlovski","2019-09-13T19:45:49Z","2019-09-17T16:43:40Z"
"","7409","KAFKA-10818: Skip conversion to `Struct` when serializing generated requests/responses","Generated request/response classes have code to serialize/deserialize directly to `ByteBuffer` so the intermediate conversion to `Struct` can be skipped for them. We have recently completed the transition to generated request/response classes, so we can also remove the `Struct` based fallbacks.  Additional noteworthy changes: * `AbstractRequest.parseRequest` has a more efficient computation of request size that relies on the received buffer instead of the parsed `Struct`. * Use `SendBuilder` for `AbstractRequest/Response` `toSend`, made the superclass implementation final and removed the overrides that are no longer necessary. * Removed request/response constructors that assume latest version as they are unsafe outside of tests. * Removed redundant version fields in requests/responses. * Removed unnecessary work in `OffsetFetchResponse`'s constructor when version >= 2. * Made `AbstractResponse.throttleTimeMs()` abstract. * Using `toSend` in `SaslClientAuthenticator` instead of `serialize`. * Various changes in Request/Response classes to make them more consistent and to rely on the Data classes as much as possible when it comes to their state. * Remove the version argument from `AbstractResponse.toString`. * Fix `getErrorResponse` for `ProduceRequest` and `DescribeClientQuotasRequest` to use `ApiError` which processes the error message sent back to the clients. This was uncovered by an accidental fix to a `RequestResponseTest` test (it was calling `AbstractResponse.toString` instead of `AbstractResponse.toString(short)`).  Rely on existing protocol tests to ensure this refactoring does not change  observed behavior (aside from improved performance).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-09-29T03:30:10Z","2020-12-09T03:54:42Z"
"","7034","MINOR: Do not end Javadoc comments with `**/`","From https://blog.joda.org/2012/11/javadoc-coding-standards.html :  > Use the standard style for Javadoc comments > Javadoc only requires a `/**` at the start and a `*/` at the end. > [...] > Do not use `**/` at the end of the Javadoc.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","eneveu","2019-07-04T12:01:26Z","2019-07-09T09:42:12Z"
"","7093","MINOR: kafkatest - adding whitelist for interbroker sasl configs","forgot to include this line in https://github.com/apache/kafka/pull/7018  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-07-15T23:41:49Z","2019-07-22T08:38:29Z"
"","6676","KAFKA-8323: Should close filter in RocksDBStoreTest as well","Forgot to also close the filter in RocksDBStoreTest in time. Thanks @bbejeck for merging (too!) quickly 😄   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-05-04T00:02:11Z","2019-05-06T20:36:18Z"
"","7049","[DO NOT MERGE] KAFKA-8637: WriteBatch objects leak off-heap memory","For testing","open","","ableegoldman","2019-07-08T21:48:19Z","2019-11-21T19:40:33Z"
"","6650","KAFKA-8225 & KIP-345 part-2: fencing static member instances with conflicting group.instance.id","For static members join/rejoin, we encode the current timestamp in the new `member.id`. The format looks like `group.instance.id-timestamp`.  During consumer/broker interaction logic (Join, Sync, Heartbeat, Commit), we shall check the whether `group.instance.id` is known on group. If yes, we shall match the `member.id` stored on static membership map with the request `member.id`. If mismatching, this indicates a conflict consumer has used same group.instance.id, and it will receive a fatal exception to shut down.  Right now the only missing part is the system test. Will work on it offline while getting the major logic changes reviewed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-04-29T23:48:33Z","2019-05-18T14:28:51Z"
"","7084","KAFKA-8660: Make ValueToKey SMT work only on a whitelist of topics","For source connectors that publish on multiple topics it is essential to be able to configure transforms to be active only for certain topics.This PR adds this config option for the ValueToKey SMT.  I'm also interested in opinions if this would make sense to add as a configurable option to all packaged SMTs or even as a capability for SMTs in general.","open","connect,","bfncs","2019-07-12T13:59:59Z","2020-04-07T21:06:40Z"
"","7308","KAFKA-8660: Add connect SMT topic whitelist config","For source connectors that publish on multiple topics it is essential to be able to configure transforms to be active only for certain topics.  This adds the possibility to define a whitelist of topics each single message transform (SMT) is applied to by adding a comma-separated list of these topics as key `topics` to the transforms configuration.  It solves [KAFKA-8660](https://issues.apache.org/jira/browse/KAFKA-8660) not only for a single but for all SMTs and is an alternative to #7084.  Since this introduces a change breaking backwards compatibility (because anyone could have used `topics` as a configuration key already), this PR is merely a request for comments: would this be something the maintainers of this project would be interested in adding? If yes, a strategy for smooth migration would be needed, and I would add the needed integration tests and documentation.  ## Example configuration:  ``` transforms: ""ValueToKey"" transforms.ValueToKey.type: ""org.apache.kafka.connect.transforms.ValueToKey"" transforms.ValueToKey.fields: ""userId,city,state"" transforms.ValueToKey.topics: ""my-addresses"" ```  This makes sure the transform `ValueToKey` is only applied for records published on the `my-addresses` topic.","open","connect,","bfncs","2019-09-06T13:53:07Z","2020-03-22T00:04:19Z"
"","6645","KAFKA-6455: Session Aggregation should use window-end-time as record timestamp","For session-windows, the result record should have the window-end timestamp as record timestamp.","closed","kip,","mjsax","2019-04-28T12:06:38Z","2020-06-12T23:41:42Z"
"","6703","KAFKA-8344. Fix vagrant-up.sh to work with AWS properly","For now, `vagrant/vagrant-up.sh --aws` fails because the `vagrant hostmanager` command in that script lacks the `--aws` option. This PR adds it.  I ran `vagrant/vagrant-up.sh --aws` with and without `--no-parallel` option and confirmed both worked as expected.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-09T08:00:03Z","2019-05-10T05:50:37Z"
"","6909","KAFKA-8513: Add kafka-streams-application-reset.bat for Windows platform","For improving Windows support, this PR adds a batch file correponding to bin/kafka-streams-application-reset.sh.  I Ran the added batch file and confirmed it worked on Windows 10 as follows:  ``` PS C:\kafka-trunk> .\bin\windows\kafka-streams-application-reset.bat --application-id streams-pipe --input-topics streams-plaintext-input --to-earliest  (snip)  Reset-offsets for input topics [streams-plaintext-input] Following input topics offsets will be reset to (for consumer group streams-pipe) Topic: streams-plaintext-input Partition: 0 Offset: 0 Done. Deleting all internal/auto-created topics for application streams-pipe Done. ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","sekikn","2019-06-08T02:15:21Z","2019-07-19T17:53:51Z"
"","7221","Consumer benchmark test for paused partitions","For details about this new Kafka Consumer benchmark test see Jira issue [KAFKA-8814](https://issues.apache.org/jira/browse/KAFKA-8814).  Original PR and Jira:  * PR: [KAFKA-7548: KafkaConsumer should not throw away already fetched data for paused partitions (v2)](https://github.com/apache/kafka/pull/6988) * Jira: [KAFKA-7548](https://issues.apache.org/jira/browse/KAFKA-7548)  To recreate the tests from the Jira issue:  ``` # Run on trunk TC_PATHS=""tests/kafkatest/benchmarks/core/benchmark_test.py::Benchmark.test_consumer_throughput"" bash tests/docker/run_tests.sh # Rebase onto tag 2.3.0 git rebase --onto 2.3.0 trunk # Run on 2.3.0 TC_PATHS=""tests/kafkatest/benchmarks/core/benchmark_test.py::Benchmark.test_consumer_throughput"" bash tests/docker/run_tests.sh ```  @ijuma @hachikuji Please review at your convenience.","open","","seglo","2019-08-19T01:35:52Z","2019-09-02T21:14:18Z"
"","7364","HOTFIX: fix Kafka Streams upgrade note for broker backward compatibility","For `trunk` and `2.3` branch, cf. #7363   Also compare kafka-site PR https://github.com/apache/kafka-site/pull/229","closed","docs,","mjsax","2019-09-18T18:03:24Z","2019-09-24T16:54:05Z"
"","6697","KAFKA-8324: Add close() method to RocksDBConfigSetter","Following KIP-453, this PR adds a default close() method to the RocksDBConfigSetter interface and calls it when closing a store.","closed","kip,","ableegoldman","2019-05-08T00:36:41Z","2020-06-26T22:38:58Z"
"","6943","KAFKA-8542; Cache transaction first offset metadata on follower","Followers should cache the log offset metadata for the start offset of each transaction in order to be able to compute the last stable offset without an offset index lookup. This helps with follower fetching in KIP-392.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-06-15T07:16:27Z","2019-06-29T23:49:43Z"
"","7110","KAFKA-8179: PartitionAssignorAdapter","Follow up to [new PartitionAssignor interface](https://issues.apache.org/jira/browse/KAFKA-8703) merged in [7108](https://github.com/apache/kafka/pull/7108) is merged  Adds a PartitionAssignorAdapter class to [maintain backwards compatibility](https://issues.apache.org/jira/browse/KAFKA-8704)","closed","","ableegoldman","2019-07-25T00:13:33Z","2019-08-02T01:22:44Z"
"","7107","KAFKA-8179: PartitionAssignorAdapter for backwards compatibility","Follow up to [new PartitionAssignor interface](https://issues.apache.org/jira/browse/KAFKA-8703) -- should be rebased after [7100](https://github.com/apache/kafka/pull/7100) is merged  Adds a PartitionAssignorAdapter class to [maintain backwards compatibility](https://issues.apache.org/jira/browse/KAFKA-8704)","closed","","ableegoldman","2019-07-24T02:48:10Z","2019-07-25T20:21:59Z"
"","7294","KAFKA-8866; Return exceptions as Optional in authorizer API","Follow up on review comment in PR #7268   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-04T12:26:54Z","2019-09-05T08:21:01Z"
"","6610","MINOR: Add unit test for SerDe auto-configuration","Follow up of (including) #6461 (first commit only -- original commits squashed)  If we are happy with this, we can merge #6461 and than I can rebase this and drop the first commit.","closed","streams,","mjsax","2019-04-19T14:51:51Z","2019-04-22T17:15:50Z"
"","6609","MINOR: Add unit test for SerDe auto-configuration","Follow up of (including) #6461 (first commit only -- original commits squashed)  If we are happy with this, we can merge #6461 and than I can rebase this and drop the first commit.","closed","streams,","mjsax","2019-04-19T04:45:06Z","2019-04-19T15:22:07Z"
"","6832","KAFKA-8443 Broker support for fetch from followers","Follow on to #6731, this PR adds broker-side support for [KIP-392](https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica) (fetch from followers).   Changes: * All brokers will handle FetchRequest regardless of leadership * Leaders can compute a preferred replica to return to the client * New ReplicaSelector interface for determining the preferred replica * Incremental fetches will include partitions with no records if the preferred replica has been computed * Adds new JMX to expose the current preferred read replica of a partition in the consumer  Two new conditions were added for completing a delayed fetch. They both relate to communicating the high watermark to followers without waiting for a timeout: * For regular fetches, if the high watermark changes within a single fetch request  * For incremental fetch sessions, if the follower's high watermark is lower than the leader  A new JMX attribute `preferred-read-replica` was added to the `kafka.consumer:type=consumer-fetch-manager-metrics,client-id=some-consumer,topic=my-topic,partition=0` object. This was added to support the new system test which verifies that the fetch from follower behavior works end-to-end. This attribute could also be useful in the future when debugging problems with the consumer.","closed","","mumrah","2019-05-28T21:14:27Z","2019-07-04T15:18:52Z"
"","7005","MINOR: Fix typos in upgrade guide","Fixes some typos and adds a missing link to table of contents in memory-management section","closed","streams,","ableegoldman","2019-06-26T19:26:45Z","2019-06-27T16:08:51Z"
"","6914","KAFKA-8523 Enabling InsertField transform to be used with tombstone events","Fixes https://issues.apache.org/jira/browse/KAFKA-8523  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gunnarmorling","2019-06-11T10:29:59Z","2020-10-16T06:17:28Z"
"","7117","KAFKA-8705: Remove parent node after leaving loop to prevent NPE","Fixes case where multiple children merged from a key-changing node causes an NPE.  I've updated the tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-07-25T19:58:16Z","2019-12-16T17:42:15Z"
"","7109","KAFKA-8705: Remove parent node after leaving loop to prevent NPE","Fixes case where multiple children merged from a key-changing node causes an NPE.  I've updated the tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bbejeck","2019-07-24T22:09:53Z","2019-12-12T21:24:31Z"
"","6587","MINOR: Removed unused import","Fixes a checkstyle error than snuck in with a docs HTML cherry-pick    ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-04-16T18:03:53Z","2019-04-17T20:29:42Z"
"","6923","KAFKA-7760","Fixed segment.bytes.  Segment.ms is still pending","closed","","dulvinw","2019-06-12T12:25:31Z","2020-05-01T08:20:24Z"
"","6867","Fixed error in KTable.java","Fixed issue with data type of keyedStream in `.toStream` Code Example.","open","","himani1","2019-06-03T11:46:25Z","2019-06-03T11:46:25Z"
"","6952","MINOR: Improve Javadoc style","Fixed a few Javadoc issues:  1) Do not end comments with `**/`  2) Align leading asterisks  3) Start comments with `/**` instead of `/*`     Javadoc comments must start with `/**`.     When starting with `/*`, Javadoc tools do not recognize those comments as Javadoc comments, it thinks they are simple block comments, and does not associate them with their class / member.  4) Fix dangling Javadoc comments  5) Avoid `@see` in the middle of Javadoc     When the `@see` tag is in the middle of a Javadoc comment, the Javadoc tool thinks the rest of the comment is part of the `@see` description.     There are two ways to fix this:     - If we really want an `@see` tag, we can move it at the bottom of the comment, where tags are supposed to be.     - When we want a link in the middle of the content, we can do: `See Link title.`  6) Fix `@throws` syntax An `@throws` declaration should be followed by the exception name, not by an `@link` or an `@see`.   I followed the Javadoc / Scaladoc guidelines as well as Stephen Colebourne's article about Javadoc coding standards: https://blog.joda.org/2012/11/javadoc-coding-standards.html .   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","eneveu","2019-06-17T20:29:52Z","2019-07-04T11:44:59Z"
"","7295","MINOR: Fix few typos in the javadocs/docs","fix typo(an ssl)","closed","","KangZhiDong","2019-09-04T17:05:45Z","2019-09-05T03:37:23Z"
"","6654","KAFKA-8289: Fix Session Expiration and Suppression","Fix two problems in Streams: * Session windows expired prematurely (off-by-one error), since the window end is inclusive, unlike other windows * Suppress duration for sessions incorrectly waited only the grace period, but session windows aren't closed until `gracePeriod + sessionGap`  Update the tests accordingly  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-30T18:45:02Z","2019-05-03T14:39:33Z"
"","6671","KAFKA-8289: Fix Session Expiration and Suppression (#6654)","Fix two problems in Streams:  * Session windows expired prematurely (off-by-one error), since the window end is inclusive, unlike other windows * Suppress duration for sessions incorrectly waited only the grace period, but session windows aren't closed until gracePeriod + sessionGap  Update the tests accordingly  Reviewers: A. Sophie Blee-Goldman , Boyang Chen , Matthias J. Sax , Bill Bejeck , Guozhang Wang   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-05-03T18:00:41Z","2019-05-04T11:26:58Z"
"","6670","KAFKA-8289: Fix Session Expiration and Suppression (#6654)","Fix two problems in Streams:  * Session windows expired prematurely (off-by-one error), since the window end is inclusive, unlike other windows * Suppress duration for sessions incorrectly waited only the grace period, but session windows aren't closed until gracePeriod + sessionGap  Update the tests accordingly  Reviewers: A. Sophie Blee-Goldman , Boyang Chen , Matthias J. Sax , Bill Bejeck , Guozhang Wang   *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-05-03T17:36:41Z","2019-05-03T20:24:39Z"
"","6776","KAFKA-8265 : Match client config override prefix to match with the KIP","Fix to use the correct prefix for the configurations that are being overridden match with the documentation in KIP-458.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","mageshn","2019-05-20T21:37:59Z","2020-10-16T06:19:09Z"
"","6517","KAFKA-8172: Fix to close file handlers before renaming files/directories","Fix to close file handlers before renaming files / directories and open them back if required  Following are the file renaming scenarios: - Files are renamed to .deleted so they can be deleted - .cleaned files are renamed to .swap as part of Log.replaceSegments flow - .swap files are renamed to original files as part of Log.replaceSegments flow  Following are the folder renaming scenarios: - When a topic is marked for deletion, folder is renamed - As part of replacing current logs with future logs in LogManager  In above scenarios, if file handles are not closed, we get file access violation exception  Idea is to close the logs and file segments before doing a rename and open them back up if required.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","kondetibharat","2019-03-28T19:28:26Z","2021-02-28T23:41:10Z"
"","7369","Fix bug3","fix the script kafka-server-stop.sh cannot work  on centos 6  CDH 5.15","open","","liangkiller","2019-09-19T07:44:34Z","2019-12-31T09:33:11Z"
"","7337","KAFKA-8910: Incorrect javadoc at KafkaProducer.InterceptorCallback#onCompletion","Fix the inconsistency between `KafkaProducer.InterceptorCallback#onCompletion` and `Callback#onCompletion`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-09-16T14:49:20Z","2020-02-20T18:56:12Z"
"","7363","HOTFIX: fix Kafka Streams upgrade note for broker backward compatibility","Fix should be cherry-picked to 2.3 branch.  For 2.2 branch, cf #7364   Also compare kafka-site PR https://github.com/apache/kafka-site/pull/229","closed","docs,","mjsax","2019-09-18T18:02:53Z","2019-09-24T16:56:11Z"
"","6707","KAFKA-8348 document optimized","Fix out of date and wrong example in document.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","hustclf","2019-05-10T06:05:34Z","2019-05-31T01:33:39Z"
"","7188","KAFKA-8671: NullPointerException occurs if topic associated with GlobalKTable changes","Fix NullPointerException that occurs when the global/.checkpoint file contains a line with an obsolete (but valid) topic by pruning non-relevant topics from the checkpoint cache.  Added a unit test to verify that non-relevant topics are pruned from the checkpoint cache by GlobalStateManagerImpl. Also, manually ran a streams application with a modified global/.checkpoint file containing an obsolete topic partition and verified that 1) the NullPointerException no longer occurs and 2) the obsolete topic is removed from the global/.checkpoint file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","amleung21","2019-08-09T19:40:04Z","2019-10-02T17:23:09Z"
"","7046","MINOR: Fixes AK config typos","Fix minor typos in config  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","joel-hamill","2019-07-08T17:02:41Z","2019-07-09T08:55:38Z"
"","6849","KAFKA-8187 Bugfix for branch 2.0","Fix KAFKA-8187: State store record loss across multiple reassignments when using standby tasks.  - Do not let the thread to transit to RUNNING until all tasks (including standby tasks) are ready.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","hustclf","2019-05-31T05:00:17Z","2020-02-13T02:02:37Z"
"","6818","KAFKA-8187: Add wait time for other thread in the same jvm to free the locks","Fix KAFKA-8187: State store record loss across multiple reassignments when using standby tasks.  - Do not let the thread to transit to RUNNING until all tasks (including standby tasks) are ready.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","hustclf","2019-05-26T15:04:11Z","2019-10-18T10:44:34Z"
"","6748","MINOR: Fix race condition in Streams tests","Fix IntelliJ warning about non-atomic operation on volatile variable.","closed","streams,","mjsax","2019-05-16T17:00:24Z","2019-05-17T18:02:20Z"
"","7043","MINOR: Fix deprecated scala syntax","Fix deprecated scala syntax via scalafix and remove tons of warning in case of scala-2.13 build   Used scalafix config ``` rules = [   ProcedureSyntax,   NoValInForComprehension,   LeakingImplicitClassVal,   NoAutoTupling,   RemoveUnused ] ```  ### How changes was been made - Put config to `.scalafix.conf` at root of repo. - Install scalafix  ```bash coursier bootstrap ch.epfl.scala:scalafix-cli_2.12.8:0.9.5 -f --main scalafix.cli.Cli -o ~/bin/scalafix ```  - Run `scalafix --syntactic `   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","narma","2019-07-08T14:24:20Z","2019-11-15T13:43:50Z"
"","7439","MINOR: Fix static membership link in Streams upgrade notes","Fix broken link in Streams docs. Same as https://github.com/apache/kafka/commit/11337afecb9a4fd0b993402c5195ef33dbd7a539#diff-8100f2416b657c1e1e4238dabf8a15e0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-03T19:31:07Z","2019-12-19T23:47:39Z"
"","6807","MINOR: Fix a toString npe in tableProcessorNode","Fix a toString npe in tableProcessorNode. This gets hit when we run KSQL with debug logging enabled.","closed","","rodesai","2019-05-24T07:29:56Z","2019-05-25T04:20:50Z"
"","6664","KAFKA-8315: fix the JoinWindows retention deprecation doc","Fix a javadoc mistake introduced in https://github.com/apache/kafka/pull/5911/files#diff-35e3523474fa277a63e36a3fe9e22af8 .  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-05-02T13:36:00Z","2019-05-21T17:55:25Z"
"","7479","MINOR: fix compatibility-breaking bug in RequestHeader","Fix a compatibility break in trunk introduced by the work on KIP-511 and KIP-482.  Older brokers must be able to read the first part of any version of ApiVersionsRequest that they get sent without encountering an error. Therefore, the first part of any new version of the RequestHeader must match the first part in the old header version used alongside the original ApiVersionsRequest.","closed","","cmccabe","2019-10-09T23:01:27Z","2019-10-10T20:52:08Z"
"","7260","KAFKA-8840: Fix bug in ClientCompatibilityFeaturesTest and Dockerfile","Fix a bug where ClientCompatibilityFeaturesTest fails when running multiple iterations.  Also, fix a typo in tests/docker/Dockerfile.","closed","","cmccabe","2019-08-27T22:51:47Z","2019-08-30T23:08:00Z"
"","6525","KAFKA-8029: In memory session store","First pass at an in-memory session store implementation. Waiting for KIP to be voted on/accepted  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","ableegoldman","2019-03-29T23:25:55Z","2020-06-26T22:39:04Z"
"","6721","KAFKA-8336; Enable dynamic reconfiguration of broker's client-side certs","Enable reconfiguration of SSL keystores and truststores in client-side channel builders used by brokers for controller, transaction coordinator and replica fetchers. This enables brokers using TLS mutual authentication for inter-broker listener to use short-lived certs that may be updated before expiry without restarting brokers.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-05-13T15:01:38Z","2019-10-28T14:40:57Z"
"","7430","KAFKA-5609: Connect log4j should also log to a file by default (KIP-521)","Enable Kafka Connect to redirect log4j messages to a file by default, additionally to the redirection to standard output. The file-based log4j export is set to be daily and shares the same pattern with the stdout appender.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-10-02T03:13:26Z","2020-10-16T06:17:31Z"
"","6745","MINOR: Enable console logs in Connect tests","Enable console logs in Connect unit and integration tests.   Ran the Connect tests and the verbosity of logs seems acceptable  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-05-15T22:25:17Z","2020-10-16T06:19:08Z"
"","6533","MINOR: fix throttling and status in ConnectionStressWorker","Each separate thread should have its own throttle, so that it can sleep for an appropriate amount of time when needed.  ConnectionStressWorker should avoid recalculating the status after shutting down the runnables.  Otherwise, if one runnable is slow to stop, it will skew the average down in a way that doesn't reflect reality.  This change moves the status calculation into a separate periodic runnable that gets shut down cleanly before the other ones.","closed","","cmccabe","2019-04-02T23:55:44Z","2019-05-20T18:53:17Z"
"","7343","KAFKA-6098: Fix race between topic deletion and re-creation.","During topic deletion, there exists a window where a broker updates its metadata cache to remove a deleted topic's partitions, and the controller removing the topic's znode. Consequently, it was possible for a broker to believe a topic no longer existed, however it couldn't be re-created due to outstanding ZK metadata.  The fix is to return a transient error for when this condition is encountered. Given the window is anticipated to be small, a retry will eventually resolve the issue.  Additionally, in rare cases, there existed a window where it was possible to re-create a topic's znode before its deletion process was fully complete. This is fixed by making topic creation account for any outstanding topic deletion znode.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bdbyrne","2019-09-16T20:15:37Z","2019-10-10T15:21:38Z"
"","7438","MINOR: Fix stale comment in partition reassignment javadoc","During the cleanup phase of an overriding reassignment, we failed to mention that we still add replicas in the adding_replicas field for the initial LeaderAndIsr request","closed","","stanislavkozlovski","2019-10-03T07:31:44Z","2019-10-11T17:49:32Z"
"","7315","KAFKA-8340, KAFKA-8819: Use PluginClassLoader while statically initializing plugins","During plugin class loading, instance creation, and configuration change the current thread's context ClassLoader to the PluginClassLoader. This prevents issues where static initialization loads the wrong resources, or fails to find resources that are inside the plugin. This change impacts the following classes: Plugins, DelegatingClassLoader, Worker.  In order to verify the original bug and verify the fix, a group of test plugins for collecting data are included in this commit:  * `AlwaysThrowException` shows that exceptions are captured * `Sampling` is for basic initialization and field aliasing * `SamplingConverter` takes samples during converter method calls * `SamplingHeaderConverter` takes samples during header converter method calls * `SamplingConfigProvider` takes samples during config provider method calls * `SamplingConfigurable` takes samples during configurable method calls * `ServiceLoaderPlugin` samples the active classloader for service loaded classes  These plugins are stored in the resources directory. Each plugin has it's own source tree, and can have arbitrary code. The plugins are compiled into class files in a separate temporary directory. They are then packaged into a jar in a temporary file  which is deleted after the process exits. This jar is then exposed for use in `plugin.path`, and tests can access to the plugin class names through constants.  These changes will be backported to all versions of AK which have plugin isolation (introduced in `0.11.0.0`): * `0.11.0.4` * `1.0.3` * `1.1.2` * `2.0.2` * `2.1.2` * `2.2.2` * `2.3.1` * `2.4`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","gharris1727","2019-09-09T16:20:07Z","2020-10-16T05:50:59Z"
"","6568","KAFKA-7601; Clear leader epoch cache on downgraded format in append","During a partial message format upgrade, it is possible for the message format to flap between new and old versions. If we detect that data appended to the log is on an old format, we can clear the leader epoch cache so that we revert to truncation by high watermark. Once the upgrade completes and all replicas are on the same format, we will append to the epoch cache as usual. Note this is related to KAFKA-7897, which handles message format downgrades through configuration.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-12T06:55:03Z","2019-05-03T00:46:02Z"
"","6720","MINOR: Fix code section formatting in TROGDOR.md","Due to the lack of a blank line, a code section in TROGDOR.md is not properly rendered on the GitHub site. This PR fixes it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-11T07:16:11Z","2019-05-11T07:28:38Z"
"","7017","MINOR: Assert ExecutionException details in testCreatePartitions","Due to the accidental duplication of `case e: ExecutionException`, the verification code was not actually running.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-06-29T16:20:31Z","2019-07-01T15:45:26Z"
"","6521","[MINOR] Guard against crashing on invalid key range queries","Due to KAFKA-8159, Streams will throw an unchecked exception when a caching layer or in-memory underlying store is queried over a range of keys from negative to positive. We should add a check for this and log it then return an empty iterator (as the RocksDB stores happen to do) rather than crash  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-03-29T00:46:39Z","2020-06-26T22:39:24Z"
"","7334","MINOR: Fix bug where we would incorrectly load partition reassignment info from ZK","Due to a typo in the initial PR, we would load the `adding_replicas` structure as the `removing_replicas` when loading reassignment info from ZK. This meant that API reassignments would not work during a failover.","closed","","stanislavkozlovski","2019-09-15T16:55:51Z","2019-10-02T12:50:35Z"
"","7066","KAFKA-8648: Throw exception if bad --property sent to ConsoleConsumer","Doesn’t check ConsoleProducer Doesn’t check properties sent to key/value deserialisers  Includes unit test, also adds a test for the Custom Value Deserializer option. Also includes a more reliable way to test for system exit, if the code under test might throw an illegal argument exception.  For example, if you try to use  `./bin/kafka-console-consumer --topic c3test --bootstrap-server $BROKER:9092 --property interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor`  It will fail saying:  `Unrecognised arguments ({interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor}) passed to kafka.tools.DefaultMessageFormatter.`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","astubbs","2019-07-10T16:44:58Z","2019-07-16T10:50:21Z"
"","6586","KAFKA-7778: document scala suppress API","Document the minor API change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-16T15:40:31Z","2019-04-17T16:28:12Z"
"","7434","KAFKA-8835: KIP-352 docs update","Doc updates for KIP-352  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-10-02T18:12:40Z","2020-04-30T15:36:02Z"
"","6951","MINOR [POC]: Reduce purge-data frequency","DO NOT MERGE.  Flip a coin, and only try to purge-data every 10th of the time.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-06-17T20:13:02Z","2020-04-24T23:53:20Z"
"","6632","KAFKA-8285: enable localized thread IDs in Kafka Streams","Details in the JIRA: https://issues.apache.org/jira/browse/KAFKA-8285 Basically we want to avoid sharing of atomic updates for thread id with multiple stream instances on one JVM.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","abbccdda","2019-04-25T03:59:10Z","2019-05-03T02:48:04Z"
"","7352","KAFKA-8911: Using proper WindowSerdes constructors in their implicit definitions","Detailed info is available in the ticket: https://issues.apache.org/jira/browse/KAFKA-8911  Briefly, `implicit defs` are calling empty constructors, which exists only for reflection object creation. Therefore, while using the implicit definitons, a NPE occurs when Serde is called.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","atais","2019-09-17T11:03:52Z","2019-10-03T18:30:09Z"
"","6791","KAFKA-8404: Add HttpHeader to RestClient HTTP Request and Connector REST API","Description: The ticket: https://issues.apache.org/jira/browse/KAFKA-8404 Add HttpHeader to Connector REST API. Adding Authorization header to the Connector's Rest Client HTTP Request Backporting to Kafka version 2.0  Testing:  Logic Test with Mocking.   ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","haidangdam","2019-05-23T00:22:57Z","2020-10-16T06:19:10Z"
"","7259","MINOR: A bugfix for GlobalKTable with different source topics.","Description of changes: When we use `GlobalKtable`, we found that if the source topic(s) of the `GlobalKtable` changed, then the `checkpoint` still keeps the old topic(s) and offsets and try to continue to consume messages from the topic(s). This issue caused kafka global stream thread was in `ERROR`. The exception: ``` java.lang.NullPointerException: null >---at org.apache.kafka.streams.processor.internals.GlobalStateUpdateTask.update(GlobalStateUpdateTask.java:77) ~[kafka-streams-1.1.0.jar!/:na] >---at org.apache.kafka.streams.processor.internals.GlobalStreamThread$StateConsumer.pollAndUpdate(GlobalStreamThread.java:241) ~[kafka-streams-1.1.0.jar!/:na] >---at org.apache.kafka.streams.processor.internals.GlobalStreamThread.run(GlobalStreamThread.java:289) ~[kafka-streams-1.1.0.jar!/:na] ```   There is no functionality changed  ### Committer Checklist (excluded from commit message) - [x ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","xin-au","2019-08-27T22:10:45Z","2019-09-04T18:31:25Z"
"","6716","KAFKA-8346; Improve replica fetcher behavior for handling partition failure","Description The replica fetcher thread is terminated in case a partition crashes which leads to under replication. This behavior can be improved by dropping the failed partition. The thread can continue monitoring the rest of the partitions. If all partitions of a thread have failed, the thread would be shut down. KIP - https://cwiki.apache.org/confluence/display/KAFKA/KIP-461+-+Improve+Replica+Fetcher+behavior+at+handling+partition+failure  Testing Strategy Added unit test to check the fetcher thread is not terminated in case one of the partitions fails. After a leader epoch, the failed partitions are marked as un-failed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","agg111","2019-05-10T22:52:59Z","2019-06-07T21:51:16Z"
"","6833","KAFKA-8410: [POC] strong typing for processors","Demonstrate what stronger typing on the processor/context would look like.  Not fully implemented, so not all tests will compile.  Note that this change breaks PAPI backward compatibility. A backward-compatible version of this work is possible, but requires further design.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-05-28T22:24:11Z","2019-05-31T22:18:26Z"
"","6856","KAFKA-8410: [POC] strong typing for processors","Demonstrate what stronger typing on the processor/context would look like.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","vvcephei","2019-05-31T22:22:17Z","2019-06-06T21:45:41Z"
"","6738","MINOR: Remove spammy log message during topic deletion","Deletion of a large number of topics can cause a ton of log spam. In a test case on 2.2, deletion of 50 topics with 100 partitions each caused about 158 Mb of data in the controller log. With the improvements to batch StopReplica and the patch here, we reduce that to about 1.5 Mb.  Kudos to @gwenshap for spotting these spammy messages.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-15T07:16:07Z","2019-05-17T23:58:12Z"
"","6848","KAFKA-8452: Compressed BufferValue","De-duplicate the common case in which the prior value is the same as the old value.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-05-31T01:24:01Z","2019-06-14T14:29:46Z"
"","6840","KAFKA-8418: Wait until REST resources are loaded when starting a Connect Worker.","Currently, we are waiting on Worker's port to start listening, but it might take some time until all REST resources are actually loaded - so this PR changes the behavior and waits until REST resources are ready.  The following test was run in order to test the change - `tests/kafkatest/tests/connect/connect_rest_test.py`. The PR fixes following intermittent error in `ConnectRestApiTest.test_rest_api`: ```    Error 404 Not Found  HTTP ERROR 404 Problem accessing /connector-plugins/. Reason:     Not FoundPowered by Jetty:// 9.4.18.v20190429     Traceback (most recent call last):   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.5-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run     data = self.run_test()   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.5-py2.7.egg/ducktape/tests/runner_client.py"", line 189, in run_test     return self.test_context.function(self.test)   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/tests/kafkatest/tests/connect/connect_rest_test.py"", line 89, in test_rest_api     assert set([connector_plugin['class'] for connector_plugin in self.cc.list_connector_plugins()]) == {self.FILE_SOURCE_CONNECTOR, self.FILE_SINK_CONNECTOR}   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/tests/kafkatest/services/connect.py"", line 218, in list_connector_plugins     return self._rest('/connector-plugins/', node=node)   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/tests/kafkatest/services/connect.py"", line 234, in _rest     raise ConnectRestError(resp.status_code, resp.text, resp.url) ConnectRestError ```  With this PR merged, the folllowing error is expected, and tracked in https://issues.apache.org/jira/browse/KAFKA-8449: ``` Data added to input file was not seen in the output file in a reasonable amount of time. Traceback (most recent call last):   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.5-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run     data = self.run_test()   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.5-py2.7.egg/ducktape/tests/runner_client.py"", line 189, in run_test     return self.test_context.function(self.test)   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/tests/kafkatest/tests/connect/connect_rest_test.py"", line 178, in test_rest_api     wait_until(lambda: self.validate_output(self.LONGER_INPUT_LIST), timeout_sec=120, err_msg=""Data added to input file was not seen in the output file in a reasonable amount of time."")   File ""/home/jenkins/workspace/system-test-kafka_5.3.x/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.5-py2.7.egg/ducktape/utils/util.py"", line 41, in wait_until     raise TimeoutError(err_msg() if callable(err_msg) else err_msg) TimeoutError: Data added to input file was not seen in the output file in a reasonable amount of time. ```  This PR has to be backported up to `1.0`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","avocader","2019-05-30T00:54:27Z","2020-10-16T05:50:55Z"
"","7063","KAFKA-8610: Don't use /bin/bash in scripts","Currently, the interpreters (i.e., `/bin/sh`, `/bin/bash`, and `#!/usr/bin/env bash`) and indentations in the scripts are not unified. This PR improves portability by unifying them to use `/bin/sh`.  It was validated with `shellcheck` and manual testing, but the more comprehensive test would be welcomed.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","dongjinleekr","2019-07-10T08:15:43Z","2020-09-13T08:33:32Z"
"","7420","KAFKA-8960 Move Task determineCommitId in gradle.build to Project Level","Currently, the gradle task determineCommitId is present twice in gradle.build, once in subproject clients and once in subproject streams. Task determineCommitId shall be moved to project-level such that both subprojects (and also other subprojects) can call it without being dependent on an implementation detail of another subproject. Ran the build after updating  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rabi-kumar","2019-09-30T16:59:16Z","2019-11-25T01:44:21Z"
"","6578","[MINOR] RocksDBConfigSetter docs: warn users to (re)set Filter","Currently when opening a new instance of RocksDB, we first set the default options before reading in any user supplied ones through RocksDBConfigSetter. One such option is the TableFormatConfig, for which we create a new BlockBasedTableConfig object, set its specific options, and then pass to the options#setTableFormatConfig.  A new TableFormatConfig has its filter parameter initialized to null, which we set to a new BloomFilter() by default and which most users probably want to keep even if they override the other defaults. The problem is if they do override other defaults involving tableConfig, they must do so by making a new BlockBasedTableConfig and then calling options#setTableFormatConfig with it.   If they only change e.g. the block cache size through this parameter, they override the default tableConfig with their custom one which has its filter set to null. So we should warn users whose RocksDBConfigSetter involves setting a new BlockBasedTableConfig that they should probably also set the filter on this object as well.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-04-13T01:32:41Z","2020-06-26T22:39:17Z"
"","6800","KAFKA-8333; Load high watermark checkpoint lazily when initializing replicas","Currently we load the high watermark checkpoint separately for every replica that we load. This patch makes this loading logic lazy and caches the loaded map while a LeaderAndIsr request is being handled.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-23T21:32:40Z","2019-06-11T02:00:00Z"
"","6696","KAFKA-8333; Cache checkpointed high watermarks for reuse on LeaderAndIsr request","Currently we load the high watermark checkpoint file separately for every partition that is loaded on a broker. This patch adds logic to load the checkpoint file only once when a LeaderAndIsr request is received. The results are reused for all partitions included in the request.  Additionally, this patch removes the dependence on `ReplicaManager` inside `Partition`, which should make testing easier. I have also simplified the future replica loading logic since it was unnecessarily loading the checkpoint file.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-05-08T00:32:22Z","2019-05-23T20:56:25Z"
"","6633","[MINOR] Better estimate size of cache","Currently we consider the size of the cache to be the sum of its (doubly-linked) LRUNodes, where we include the next/prev references in our estimate. This completely ignores the other two main data structures of NamedCache, the cache Map and dirtyKeys Set. If we are going to count references we should at least also account for these data structures.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ableegoldman","2019-04-25T04:28:43Z","2020-06-26T22:38:51Z"
"","7256","MINOR: Only send delete request if there are offsets in map","Currently on commit streams will attempt to delete offsets from repartition topics.  However, if a topology does not have any repartition topics, then the `recordsToDelete` map will be empty.  This PR adds a check that the `recordsToDelete` is not empty before executing the `AdminClient#deleteRecords()` method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-08-27T19:09:05Z","2019-08-28T16:23:57Z"
"","7310","KAFKA-8804: Secure internal Connect REST endpoints","Currently includes:     • Internal request validation     • All configurations outlined in KIP-507     • Javadocs     • Addition of the ""sessioned"" protocol to all applicable Connect system tests     • Unit tests for all modified and added classes     • A new integration test to ensure that the internal endpoint requires authentication when the `sessioned` protocol is in use  [Jira](https://issues.apache.org/jira/browse/KAFKA-8804), [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-507%3A+Securing+Internal+Connect+REST+Endpoints)  The changes here add authentication to the internal `POST /connectors//tasks` endpoint as detailed in [KIP-507](https://cwiki.apache.org/confluence/display/KAFKA/KIP-507%3A+Securing+Internal+Connect+REST+Endpoints).  Currently, there are unit tests and some small alterations to system tests to target the new `sessioned` Connect subprotocol. An integration test is added to ensure that the endpoint is actually secured (i.e., requests with missing/invalid signatures are rejected with a `400 BAD RESPONSE` status).  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-09-06T21:40:03Z","2020-10-16T05:50:58Z"
"","6717","MINOR: Throw ProducerFencedException directly but with a new instance","Currently in maybeFailWithError, we always wrap the lastError as a KafkaException. For ProducerFencedException however, we should just throw the exception itself; however we throw a new instance since the previous book-kept one's call trace is not from this call, and hence could be confusing.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-11T01:21:18Z","2020-04-24T23:53:57Z"
"","6891","1.0 MINOR: add SSL support for core/kafka/tools/GetOffsetShell","current GetOffsetShell does not support SSL in command line , so I add a option with `--command.config configFile` , can be call like ``` kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list $BRK_LIST --topic $TOPIC --time -1 --offsets 1 --command.config ~/kafka [GetOffsetShell.scala.zip](https://github.com/apache/kafka/files/3258690/GetOffsetShell.scala.zip) .cfg ```","closed","","sizhang-clgx","2019-06-05T19:15:28Z","2019-06-06T23:04:10Z"
"","6859","KAFKA-8463: Fix redundant reassignment of tasks when leader worker leaves","Current assignments need to be considered along with previous reassignments when leader worker leaves the group. When the leader is present, current assignments are a subset of previous assignments.   Adjusted unit tests that now fail without the fix Fixes system test on consecutive worker restarts  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-06-01T22:24:29Z","2020-10-16T06:17:28Z"
"","7192","KAFKA-8788: Optimize client metadata handling with a large number of partitions","Credit to @lbradstreet for profiling the producer with a large number of partitions.  Cache `topicMetadata`, `brokers` and `controller` in the `MetadataResponse` the first time it's needed avoid unnecessary recomputation. We were previously computing`brokersMap` 4 times per partition in one code path that was invoked from multiple places. This is a regression introduced via a42f16f980 and first released in 2.3.0.  The `Cluster` constructor became significantly more allocation heavy due to 2c44e77e2f20, first released in 2.2.0. Replaced `merge` calls with more verbose, but more efficient code. Added a test to verify that the returned collections are unmodifiable.  Add `topicAuthorizedOperations` and `clusterAuthorizedOperations` to `MetadataResponse` and remove `data()` method.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-11T03:33:03Z","2019-08-15T02:44:42Z"
"","7199","KAFKA-8601: UniformStickyPartitioner","Creates a partitioner that utilizes the sticky partitioning strategy on both records with null keys and non-null keys. Builds off of KIP-480. This partitioner will not be used as the default; usage is configured by the user.   Includes tests of both keyed and non-keyed scenarios as well as tests for when a partition is unavailable.","closed","","jolshan","2019-08-12T20:42:46Z","2019-08-16T23:04:04Z"
"","7400","Draft!: Broken into many PRs created cogroup option","Created the option to make a KCogroupStream object from a grouped stream. The KCogroupStream object then can be further cogrouped or aggregated.  This PR has been broken into parts for review  first part: https://github.com/apache/kafka/pull/7538  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wcarlson5","2019-09-26T18:43:52Z","2020-08-18T17:18:01Z"
"","7227","MINOR: Add support for bootstrap-server in create_topic","create_topic method uses --zookeeper option to create a new topic. This change adds support for it to switch to --bootstrap-server config if it is supported.  As ""--zookeeper"" option is deprecated and ""--bootstrap-server"" is preferred, this change allows us to move away from the deprecated option.  Tested by launching tests and confirming they pass and use the new --bootstrap-server option.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-08-20T23:14:47Z","2020-03-04T21:55:41Z"
"","7011","MINOR: Typo correction in server.properties","Corrected language error which was confusing in server.properties  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajexp","2019-06-28T11:58:20Z","2019-07-03T17:54:47Z"
"","6702","MINOR: Add missing option for running vagrant-up.sh with AWS to vagrant/README.md","Contrary to the previous explanation, a command example in vagrant/README.md lacks the option to specify the aws provider.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-05-09T04:50:16Z","2019-05-10T05:53:18Z"
"","6548","MINOR: Move common consumer integration test logic into separate abstract class","ConsumerBounceTest redundantly executes a couple test cases which were included in the abstract class `BaseConsumerTest`. We should try to keep a cleaner separation of testing logic and utility logic so that this does not happen (the build time is long enough without doing unnecessary work). This PR moves the cluster initialization and consumer utilities out of BaseConsumerTest and into a new class AbstractConsumerTest. We then let ConsumerBounceTest extend AbstractConsumerTest.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-05T20:24:38Z","2019-04-05T22:33:38Z"
"","6677","[MINOR] Consolidate in-memory/rocksdb unit tests for window & session store","Consolidated the unit tests by having {RocksDB/InMemory}{Window/Session}StoreTest extend {Window/Session}BytesStoreTest. Besides some implementation-specific tests (eg involving segment maintenance) all tests were moved to the abstract XXXBytesStoreTest class. The test coverage now is a superset of the original test coverage for each store type.   The only difference made to existing tests (besides moving them) was to switch from list-based equality comparison to set based, in order to reflect that the stores make no guarantees regarding the ordering of records returned from a range fetch.  There are some implementation-specific tests that were left in the corresponding test class. The RocksDBWindowStoreTest, for example, had several tests pertaining to segments and/or the underlying filesystem. Another key difference is that the in-memory versions should delete expired records aggressively, while the RocksDB versions should only remove entirely expired segments","closed","streams,","ableegoldman","2019-05-04T01:32:37Z","2020-06-26T22:39:02Z"
"","7129","MINOR: Refactor abstractConfig#configuredInstance","Consolidate replicated logic for constructing configurable instance with reflection.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-07-29T21:24:37Z","2020-04-24T23:53:17Z"
"","7265","Don't merge this, just testing something in the build","Comparing the results here with that of #7222 which has had lots of seemingly flaky failures","closed","","mumrah","2019-08-28T19:22:08Z","2020-10-26T14:38:54Z"
"","7329","HOTFIX: fix compile error in 2.0 branch","Compare #7327 and #7328","closed","streams,","mjsax","2019-09-12T21:33:28Z","2019-09-16T05:22:06Z"
"","7328","HOTFIX: fix compile error in 2.1 branch","Compare #7327  \cc @guozhangwang @rhauch","closed","streams,","mjsax","2019-09-12T21:23:47Z","2019-09-16T05:22:10Z"
"","6612","MINOR: Remove erroring lock.unlock() call from RoundTripWorker","Commit 62171781396b613b6be8e13a2541ab0895b9bb6b added a ReentrantLock to the Trogdor RoundTripWorker, but includes an extra lock.unlock() which was causing system tests to fail with java.lang.IllegalMonitorStateException. This patch removes the offending call. Tested by running system test.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-04-19T23:53:12Z","2019-04-21T17:53:08Z"
"","6641","KAFKA-8254: Pass Changelog as Topic in Suppress Serdes (#6602)","Cherry-picked from #6602   Reviewers:  Matthias J. Sax , Guozhang Wang   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-26T16:52:31Z","2019-04-29T14:28:22Z"
"","7157","MINOR: Fix binary compatibility break in KafkaClientSupplier.getAdminClient","Changing the return type in an interface is a binary incompatible change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-02T21:59:13Z","2019-08-03T04:37:46Z"
"","6817","test commit","Change-Id: I9c83328e9e412bad5a92bfa4015a0e8dbaf2bdcd  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lqcamen","2019-05-25T10:29:08Z","2019-05-25T10:31:06Z"
"","7184","KAFKA-8403: Suppress needs a Materialized variant","cc/ @vvcephei @mjsax  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","dongjinleekr","2019-08-09T11:39:15Z","2020-06-12T23:39:52Z"
"","7367","KAFKA-8859: Refactor cache-level metrics","Cache-level metrics are refactor according to KIP-444: - tag `client-id` changed to `thread-id` - name `hitRatio` changed to `hit-ratio` - made backward compatible by using streams config `built.in.metrics.version`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-09-19T00:59:00Z","2019-09-23T14:13:03Z"
"","6734","[HOT FIX] Properly init BloomFilter for 2.2","Bug-fix that moves the initialization of the Bloom Filter Rocks object out of the @before test setup method. Don't use KIP-453 so we can apply to previous versions","closed","streams,","ableegoldman","2019-05-14T21:35:28Z","2020-06-26T22:38:57Z"
"","6733","[HOT FIX] Properly init BloomFilter without ConfigSetter#close","Bug-fix that moves the initialization of the Bloom Filter Rocks object out of the @Before test setup method. Don't use KIP-453 so we can apply to previous versions","closed","","ableegoldman","2019-05-14T21:28:28Z","2019-05-14T22:16:12Z"
"","7113","Update KafkaConfig.scala","Better clarify the auto leader rebalance config documentation  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gemma-singleton","2019-07-25T09:26:03Z","2019-07-31T16:08:21Z"
"","6940","KAFKA-8452: Compressed BufferValue review follow-up","Belatedly address a few code review comments from #6848   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-06-14T15:40:22Z","2019-06-19T16:53:09Z"
"","7480","MM2 ducktape tests","Basic system tests for MirrorMaker 2.0.","open","tests,","ryannedolan","2019-10-09T23:05:36Z","2021-06-05T00:32:53Z"
"","6725","KAFKA-6455: Improve DSL operator timestamp semantics","Basic idea:  - KTable-KTable join: set `max(left-ts,right-ts)` for result  - #agg(...) (stream/table windowed/non-windowed): set `max(ts1, ts2, ts3,...)` of all input records that contribute to the aggregation result  - for all stateless transformation: input-ts -> output-ts","closed","kip,","mjsax","2019-05-14T01:17:03Z","2020-06-12T23:41:01Z"
"","7148","KAFKA-8602: Backport bugfix for standby task creation","Backports bugfix in standby task creation from PR #7008. A separate PR is needed because some tests in the original PR use topology optimizations and mocks that were introduced afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-01T11:10:04Z","2019-10-21T11:40:43Z"
"","7147","KAFKA-8602: Backport bugfix for standby task creation","Backports bugfix in standby task creation from PR #7008. A separate PR is needed because some tests in the original PR use topology optimizations and mocks that were introduced afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-01T10:52:44Z","2019-10-21T11:40:42Z"
"","7146","KAFKA-8602: Backport bugfix for standby task creation","Backports bugfix in standby task creation from PR #7008. A separate PR is needed because some tests in the original PR use topology optimizations and mocks that were introduced afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-01T09:20:11Z","2019-10-21T11:40:41Z"
"","7145","KAFKA-8602: Backport bugfix for standby task creation","Backports bugfix in standby task creation from PR #7008. A separate PR is needed because some tests in the original PR use topology optimizations and mocks that were introduced afterwards.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-01T09:13:15Z","2019-08-01T09:17:46Z"
"","7327","HOTFIX: fix compile error in 2.2 branch","Backport of #7223 broke 2.2 branch -- `assertThrows` is not available in `2.2`. I assume 2.1 and 2.0 are affected, too. Verifying now.  Call for review @rhauch @guozhangwang","closed","streams,","mjsax","2019-09-12T21:09:30Z","2019-09-12T22:18:10Z"
"","7071","KAFKA-8570: Grow buffer to hold down converted records if it was insufficiently sized","Backport https://github.com/apache/kafka/pull/6974 to 1.1  When the log contains out of order message formats (for example v2 message followed by v1 message) and consists of compressed batches typically greater than 1kB in size, it is possible for down-conversion to fail. With compressed batches, we estimate the size of down-converted batches using:  ```     private static int estimateCompressedSizeInBytes(int size, CompressionType compressionType) {         return compressionType == CompressionType.NONE ? size : Math.min(Math.max(size / 2, 1024), 1","closed","","dhruvilshah3","2019-07-10T22:27:10Z","2019-07-12T17:28:54Z"
"","7193","MINOR: Eliminate unnecessary map operations in RecordAccumulator.isMuted","Avoids calling both `containsKey` and `get` from isMuted, when only a single get is necessary. Also avoid calling `remove` unless necessary. This could be a reduction of map operations from 3 to 1.   isMuted showed up as a hotspot in profiling when using the producer with high numbers of partitions.","closed","","lbradstreet","2019-08-11T05:37:15Z","2019-08-11T18:44:31Z"
"","6812","MINOR: Auth operations must be null when talking to a pre-KIP-430 broker","Authorized operations must be null when talking to a pre-KIP-430 broker.  If we present this as the empty set instead, it is impossible for clients to know if they have no permissions, or are talking to an old broker.","closed","","cmccabe","2019-05-24T21:50:43Z","2019-05-29T07:08:01Z"
"","6524","KAFKA-8162: IBM JDK Class not found error when handling SASL","Attempt to load the IBM internal class but fallback on loading the Sun class if the IBM one is not found.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","edoardocomar","2019-03-29T16:04:58Z","2020-01-30T17:46:27Z"
"","6801","MINOR: improve error message for Serde type miss match","Atm, if the type does not match, the error is miss leading and not helpful: ``` A serializer (value: org.apache.kafka.streams.state.internals.ValueAndTimestampSerializer) is not compatible to the actual value type (value type: org.apache.kafka.streams.state.ValueAndTimestamp). Change the default Serdes in StreamConfig or provide correct Serdes via method parameters. ```  This PR makes sure that the actual value type information is in the message to make the `ValueAndTimestamp` wrapping transparent to the user and to make the error message useful.  This should be cherry-picked to `2.3` branch.","closed","streams,","mjsax","2019-05-23T21:45:07Z","2019-05-28T18:44:30Z"
"","6831","MINOR: set default `group.instance.id` in JoinGroupResponse to null","As we are planning to add on more supporting features for rebalancing under static membership, we need to make sure the behavior for `group.instance.id` is consistent throughout the whole stack.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-28T17:06:42Z","2019-05-28T20:55:39Z"
"","6637","KAFKA 8291 : System test fix","As titled, this [PR](https://github.com/apache/kafka/commit/409fabc5610443f36574bdea2e2994b6c20e2829) changed the default reset policy to latest accidentally for system tests, which in fact was earliest: ```  parser.addArgument(""--reset-policy"")                 .action(store())                 .required(false)                 .setDefault(""earliest"")                 .type(String.class)                 .dest(""resetPolicy"")                 .help(""Set reset policy (must be either 'earliest', 'latest', or 'none'""); ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-04-25T19:17:04Z","2019-04-25T21:16:35Z"
"","7039","KAFKA-8618: Replace Txn marker with automated protocol","As title. This one is tricky because the original txn marker entry has been widely used, so we choose to minimize the change by making the transformation internal for the txn marker class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-07T03:11:39Z","2020-03-19T14:26:42Z"
"","7062","KAFKA-8640: Replace OffsetFetch request with automated protocol","As title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-10T02:10:42Z","2019-07-30T15:29:46Z"
"","7052","KAFKA-8616: Replace ApiVersionsRequest request/response with automated protocol","As title.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-08T23:30:36Z","2020-02-13T18:21:06Z"
"","7048","KAFKA-8636: add documentation change for max poll interval with static members","As title, static members' behavior under `max.poll.interval.ms` is slightly different from dynamic members.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-08T19:08:54Z","2019-07-10T08:37:42Z"
"","6830","KAFKA-8430: unit test to make sure null `group.id` and valid `group.instance.id` are valid combo","as title suggests, this unit test is just a double check. No need to push in 2.3  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-28T16:57:34Z","2019-05-28T23:34:47Z"
"","6762","KAFKA-8386: Use COORDINATOR_NOT_AVAILABLE to replace UNKNOWN_MEMBER_ID exception when group is dead or not found","As title suggests, the motivation is to avoid blindly resetting the generation info, as within static membership we need `member.id` to fence against duplicate members. This change could reduce the chance of resetting generation info and thus improve the detection of conflict member.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-19T05:57:12Z","2019-06-05T21:20:05Z"
"","6962","MINOR: fix consumer group failure message typo","As title suggests, the error message should be `did not complete` instead of `did complete`. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-18T20:06:01Z","2019-06-18T20:15:05Z"
"","6805","KAFKA-8424: replace ListGroups request/response with automated protocol","As title suggested.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-24T04:44:12Z","2019-07-10T21:48:38Z"
"","6877","KAFKA-8331: stream static membership system test","As title suggested, we boost 3 stream instances stream job with one minute session timeout, and once the group is stable, doing couple of rolling bounces for the entire cluster. Every rejoin based on restart should have no generation bump on the client side.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-04T03:56:24Z","2019-06-08T00:07:41Z"
"","6779","KAFKA-8399: bring back internal.leave.group.on.close config for KStream","As title states. We plan to merge this to both trunk and 2.3 if it could fix the stream system tests globally. Reference implementation: https://github.com/apache/kafka/pull/6673 ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-20T23:50:35Z","2019-05-22T16:27:10Z"
"","7029","KAFKA-8617: Use automated protocol for End Txn","As title  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-03T16:48:41Z","2020-01-10T01:08:16Z"
"","7097","KAFKA-8676: Avoid Unnecessary stops and starts of tasks","As this is testing for initialization behaviors, I do not think it is a  good idea to do unit and integrate testing. System tests are needed.  Log INFO output and KafkaConnect API for checking the status of  connectors and tasks are useful tools when checking which  connectors and tasks start/stop.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","LuyingLiu","2019-07-17T10:01:59Z","2020-10-16T05:50:57Z"
"","6639","MINOR: improve JavaDocs for KStream.through()","As reported on https://issues.apache.org/jira/browse/KAFKA-8288, the JavaDocs are somewhat fuzzy.  Additionally, fixing some JavaDoc bugs.","closed","streams,","mjsax","2019-04-25T23:45:28Z","2019-05-09T13:45:55Z"
"","6872","KAFKA-8473: Adjust Connect system tests for incremental cooperative rebalancing","As predicted by the new incremental cooperative rebalancing protocol, when a worker leaves the group a delay is triggered, controlled by the scheduled.rebalance.max.delay.ms property, during which the tasks detected as lost due to the workers departure are not reassigned. After this delay, workers are joining for another rebalancing round to reschedule the lost tasks.   System tests that perform worker bounces need to account for this delay.  This PR is also enabled connect system tests on distributed workers for both eager and incremental cooperative rebalancing protocols.   Tested by repeated runs of system tests.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-06-03T18:11:10Z","2020-10-16T05:50:56Z"
"","7229","KAFKA-8202: Clean up inFlightBatches in Sender when ProducerBatch split happens","As part of the producer delivery timeout support (KAFKA-5886, KIP-91), the inFlightBatches map is introduced in the producer Sender but currently the batches in that map will only be cleaned up when completing, expiring and failing a batch. However, when batch split happens, the original batch (created prior to the split) will never be cleaned up from inFlightBatches map. This cause excessive memory pressure in the producer because these zombie batches are normally big batches and eventually it will cause OutOfMemory in the producer.  This patch addresses the issue by removing the original batch from the Sender inFlightBatchMap after split happens. The test case in SenderTest also gets modified accordingly to cover this scenario.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hzxa21","2019-08-21T08:34:30Z","2019-08-22T17:26:19Z"
"","6729","KAFKA-8354: Replace Sync group request/response with automated protocol","As part of https://issues.apache.org/jira/browse/KAFKA-7830  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-14T18:56:25Z","2019-05-15T22:09:01Z"
"","6844","MINOR: Remove stale streams producer retry default docs.","As part of https://github.com/apache/kafka/pull/5425 the streams default override for producer retries was removed. The documentation was not updated to reflect that change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","cwildman","2019-05-30T10:50:02Z","2019-07-19T17:04:21Z"
"","7272","MINOR. Fix 2.3.0 streams systest dockerfile typo","As part of commit 4d1ee26a136997d31dbd6ddca07e09b34c41c77d streams version 2.3.0 test jar was added, but there was a simple typo in the path that specified the version.  `ducker-ak up` was failing because of that. Fixed that.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-08-29T18:15:27Z","2019-08-29T21:12:09Z"
"","6560","KAFKA-8215: Pt I. Share block cache between instances","As of v5.12, RocksDB provides an API for sharing the block cache across all instances in a single process. Because it involves passing the same cache to the Rocks Options, this ability is not supported by the current Streams configs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ableegoldman","2019-04-11T01:19:19Z","2019-04-17T22:21:14Z"
"","7150","KAFKA-8729, pt 2: Add error_records and error_message to PartitionResponse","As noted in the [KIP-467](https://cwiki.apache.org/confluence/display/KAFKA/KIP-467%3A+Augment+ProduceResponse+error+messaging+for+specific+culprit+records), the updated `ProduceResponse` is  ``` Produce Response (Version: 8) => [responses] throttle_time_ms   responses => topic [partition_responses]     topic => STRING     partition_responses => partition error_code base_offset log_append_time log_start_offset       partition => INT32       error_code => INT16       base_offset => INT64       log_append_time => INT64       log_start_offset => INT64       error_records => [INT32]         // new field, encodes the relative offset of the records that caused error       error_message => STRING          // new field, encodes the error message that client can use to log itself     throttle_time_ms => INT32 ```  with a new error code: ``` INVALID_RECORD(86, ""Some record has failed the validation on broker and hence be rejected."", InvalidRecordException::new); ```  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-08-01T18:57:07Z","2019-10-01T03:08:38Z"
"","7316","KAFKA-8886; Make Authorizer create/delete methods asynchronous","As discussed on the KIP-504 mailing list, createAcls and deleteAcls methods in the new Authorizer API have been made asynchronous. Will submit a separate PR under https://issues.apache.org/jira/browse/KAFKA-8887 to handle these requests asynchronously in KafkaApis using a purgatory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-09T17:44:07Z","2019-09-11T15:56:57Z"
"","6511","KAFKA-7190: KIP-443; Remove streams overrides on repartition topics","As described in KIP-443 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-443%3A+Return+to+default+segment.ms+and+segment.index.bytes+in+Streams+repartition+topics). We want to remove the aggressive overrides of segment.ms and segment.index.bytes for repartition topics. The remaining segment.bytes should still be effective in bounding its footprint.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-03-27T22:48:49Z","2019-04-03T15:47:21Z"
"","7388","KAFKA-7190: Retain producer state until transactionalIdExpiration time passes","As described in KIP-360, this patch changes producer state retention so that prodcuer state remains cached even after it is removed from the log. Producer state will only be removed now when the trasnactional id expiration time has passed. This is intended to reduce the incidence of UNKNOWN_PRODUCER_ID errors for producers when records are deleted or when a topic has a short retention time. Tested with unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","bob-barrett","2019-09-25T15:42:03Z","2019-10-08T21:06:37Z"
"","6522","KAFKA-8175: The broker block some minutes may occur expired error message for a period of time","As described in  (https://issues.apache.org/jira/browse/KAFKA-8175).   if one of the node is block in the cluster,and when the client can not send updateMetaData to the antother node,the client will print much log like `org.apache.kafka.common.errors.TimeoutException: Expiring 1062 record(s) for kafka_test_111-8: 23967 ms has passed since batch creation plus linger timeFri Mar 29 11:34:14 CST 2019 ` . in someTimes the controller can not  find the broker is down is offline soon,then client's batches can not send to the offline node,and also can not trigger the update metaData.so we need to check the connection's read state,if it not ready in the config time,close the channel,and trigger update the metaData ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","huangyiminghappy","2019-03-29T08:21:00Z","2019-04-23T07:48:00Z"
"","6672","KAFKA-8323: Close RocksDBStore's BloomFilter","Any RocksJava object that inherits from org.rocksdb.AbstractNativeReference must be closed explicitly in order to free up the memory of the backing C++ object. The BloomFilter extends RocksObject (which implements AbstractNativeReference) and should be also be closed in RocksDBStore#close to avoid leaking memory.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ableegoldman","2019-05-03T20:48:02Z","2019-05-04T00:00:46Z"
"","6804","[MINOR] Anonymous classes can be marked as lambda","Anonymous classes can be marked as lambda  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","highluck","2019-05-24T01:53:14Z","2020-01-09T23:38:36Z"
"","6647","MINOR: Fix an example in the Kafka Streams tutorial to be compilable","An example code in the Kafka Streams tutorial lacks some import statements and a closing parenthesis. This PR fixes them and makes that code to be compilable.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","sekikn","2019-04-29T13:34:59Z","2020-11-03T02:49:50Z"
"","6723","KAFKA-8341. Retry Consumer group operation for NOT_COORDINATOR error","An api call for consumer groups is made up of two calls: 1. Find the consumer group coordinator 2. Send the request to the node found in step 1  But the coordinator can get moved between step 1 and 2. In that case we currently fail. This change fixes that by detecting this error and then retrying.  Following APIs are impacted by this behavior: 1. listConsumerGroupOffsets 2. deleteConsumerGroups 3. describeConsumerGroups  Each of these call result in AdminClient making multiple calls to the backend. As AdminClient code invokes each backend api in a separate event loop, the code that detects the error (step 2) need to restart whole operation including step 1. This needed a change to capture the ""Call"" object for step 1 in step 2.  This change thus refactors the code to make it easy to perform a retry of whole operation. It creates a Context object to capture the api arguments that can then be referred by each ""Call"" objects. This is just for convenience and makes method signature simpler as we only need to pass one object instead of multiple api arguments.  The creation of each ""Call"" object is done in a new method, so we can easily resubmit step 1 in step 2.  This change also modifies corresponding unit test to test this scenario.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-05-13T19:19:28Z","2019-05-25T00:25:55Z"
"","7081","KAFKA-8717: Reuse cached offset metadata when reading from log","Although we currently cache offset metadata for the high watermark and last stable offset, we don't use it when reading from the log. Instead we always look it up from the index. This patch pushes fetch isolation into `Log.read` so that we are able to reuse the cached offset metadata.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-07-12T08:20:08Z","2019-07-30T15:50:14Z"
"","6534","[MINOR] Allow specifying an optional adminClientBootstrapServers in Trogdor","Allow specifying an optional adminClientBootstrapServers when launching Trogdor workloads which use the AdminClient to create topics.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","gardnervickers","2019-04-03T03:42:44Z","2019-05-07T22:11:55Z"
"","7421","KAFKA-8962: Use least loaded node for AdminClient#describeTopics","Allow routing of `AdminClient#describeTopics` to any broker in the cluster than just the controller, so that we don't create a hotspot for this API call. `AdminClient#describeTopics` uses the broker's metadata cache which is asynchronously maintained, so routing to brokers other than the controller is not expected to have a significant difference in terms of metadata consistency; all metadata requests are eventually consistent.","closed","","dhruvilshah3","2019-09-30T17:55:44Z","2019-10-31T07:45:29Z"
"","7220","MINOR: Allow KafkaZkClient and ZooKeeperClient to wait for the underlying ZooKeeper client to fully close.","Allow KafkaZkClient and ZooKeeperClient to wait for the underlying ZooKeeper client to fully close, including the shutdown of the ZooKeeper client's event thread and send thread. The ZooKeeperTestHarness is also extended to take advantage of this functionality. It will now assert that the close was successful as part of tearDown().  I had considered supplying the close timeout as an optional parameter on `close()`, but a zero value for the close timeout will block forever (implementation detail of `join()`).","open","","gardnervickers","2019-08-17T21:58:35Z","2019-08-20T19:07:25Z"
"","7167","KAFKA-8729, pt 3: Add broker-side logic to handle the case when there are record_errors and error_message","All the changes are in `ReplicaManager.appendToLocalLog` and `ReplicaManager.appendRecords`. Also, replaced `LogAppendInfo. unknownLogAppendInfoWithLogStartOffset` with `LogAppendInfo. unknownLogAppendInfoWithAdditionalInfo` to include those 2 new fields  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-08-06T17:36:42Z","2019-10-10T21:50:05Z"
"","7472","KAFKA-8743: Flaky Test Repartition{WithMerge}OptimizingIntegrationTest","All four flavors of the repartition/optimization tests have been reported as flaky and failed in one place or another: `RepartitionOptimizingIntegrationTest.shouldSendCorrectRecords_OPTIMIZED` `RepartitionOptimizingIntegrationTest.shouldSendCorrectRecords_NO_OPTIMIZATION` `RepartitionWithMergeOptimizingIntegrationTest.shouldSendCorrectRecords_OPTIMIZED` `RepartitionWithMergeOptimizingIntegrationTest.shouldSendCorrectRecords_NO_OPTIMIZATION`  They're pretty similar so it makes sense to knock them all out at once. This PR does three things: 1. Switch to in-memory stores wherever possible 2. Name all operators and update the Topology accordingly (not really a flaky test fix, but had to update the topology names anyway because of the IM stores so figured might as well) 3. Port to TopologyTestDriver -- this is the ""real"" fix, should make a big difference as these repartition tests required multiple roundtrips with the Kafka cluster (while using only the default timeout)  (If we want to backport this I can open another PR without the naming, and if we want to go all the way back to pre-TopologyTestDriver versions I think it makes sense to just double the timeout)","closed","","ableegoldman","2019-10-09T02:36:18Z","2020-06-26T22:38:20Z"
"","7469","MINOR: Just one put and flush to generation rocksDB File in RocksDBStoreTest","After merged https://github.com/apache/kafka/pull/7412 we realized it does not necessarily need that long time: instead of putting 2 million records, we can just have a single put followed by a flush, to make sure that rocksDB file exists locally (verified that after flush the `sst` file always exist).  Now the RocksDBStoreTest takes about 2.5 seconds, and removing the integration annotation from it.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-10-08T21:17:16Z","2020-04-24T23:52:17Z"
"","7287","MINOR: Add unit test for KAFKA-8676 to guard against unrequired task restarts","After KIP-415 requesting restart only of the affected connector tasks is required to avoid unnecessary task restarts. This PR adds a unit test to guard against changes in this behavior.   The original fix is introduced by the PR prefixed with KAFKA-8676  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-09-03T18:39:01Z","2020-10-16T05:50:58Z"
"","7355","MINOR: configurable package in codegen","Adds the ability to specify what package the codegen will produce.  Extracted from #7248  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-09-18T00:09:18Z","2019-09-20T17:20:17Z"
"","7291","MINOR: Add UUID type to API code generation","Adds the ability to specify fields of type UUID.  Extracted from https://github.com/apache/kafka/pull/7248  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-09-03T21:55:45Z","2019-09-16T18:24:51Z"
"","7446","KAFKA-8981 Add rate limiting to NetworkDegradeSpec","Adds egress rate limiting to our `tc` based network degrading. Also fix things to work in AWS when run with Vagrant.   Now the JSON for a network degrade task looks like:  ```json {   ""class"": ""org.apache.kafka.trogdor.fault.DegradedNetworkFaultSpec"",   ""startMs"": 0,   ""durationMs"": 60000,   ""nodeSpecs"": {     ""node1"": {""latencyMs"": 100, ""rateLimitKbit"": 1000000, ""networkDevice"": ""eth0""},     ""node2"": {""latencyMs"": 80, ""rateLimitKbit"": 2000000, ""networkDevice"": ""eth0""}   } } ```  To include this in a system test, you need the Trogdor service running. Then network degrade specs can be submitted like:  ```python # add 100ms latency for 30 seconds spec = DegradedNetworkFaultSpec(0, 30000, {}) for node in nodes_to_degrade:     spec.add_node_spec(node.name, ""eth0"", 100) task = self.trogdor.create_task(""slow"", spec) ```  tc references  * https://netbeez.net/blog/how-to-use-the-linux-traffic-control/ * https://www.badunetworks.com/traffic-shaping-with-tc/","closed","","mumrah","2019-10-04T18:11:43Z","2019-11-19T01:36:01Z"
"","6771","KAFKA-3333: Adds RoundRobinPartitioner with tests","Adds a new partitioner ""RoundRobinPartitioner"" for users to use. This is to be used when records are expected to be written to partitions in a ""Round Robin"" fashion regardless of a record key.","closed","","mmanna-sapfgl","2019-05-20T14:50:36Z","2020-05-08T23:37:09Z"
"","7090","Add security providers in kafka security config","Adds a new custom security provider class configuration. The configured classes are added to the Security provider via Security.addProvider api KIP - https://cwiki.apache.org/confluence/display/KAFKA/KIP-492%3A+Add+java+security+providers+in+Kafka+Security+config","closed","","saisandeep","2019-07-15T20:24:44Z","2019-08-26T17:01:03Z"
"","6711","KIP-392 Draft","Adds `ReplicaSelector` interface which is used by the broker to calculate the preferred read replica for consumers. The implementation is selected by the `replica.selector.class` broker config. Only the leader can calculate the preferred replica, and it only does so for regular consumer fetch requests. Three implementations are included here, but for the first phase we will probably only include the leader-only selector.  A new `client.rack` consumer configuration property is added which allows the consumer to indicate its rack.  FetchRequest now includes ""rack_id"" field which can optionally be specified by the consumer. FetchResponse includes an optional ""preferred_read_replica"" field for each partition in the response.  ReplicaManager was also modified to allow fetch requests from regular consumers to non-leader replicas if (and only if) the consumer includes ""rack_id"" field in the fetch request.","closed","","mumrah","2019-05-10T13:48:26Z","2019-05-15T17:37:58Z"
"","6770","Kafka 7358: Adds ""always"" Round-Robin partition","Adds ""Round-Robin"" partitioner which always uses round-robin regardless of record key bytes. This is **NOT** the default behaviour and does not require a doc change. No change in `partitioner.class` value. User has to change it based on their scenarios.  The only unit tests necessary for this is the partitioning without/without key bytes. No other change should be necessary.","closed","","mmanna-sapfgl","2019-05-20T13:06:21Z","2019-05-20T13:21:01Z"
"","7379","MINOR: Address review comments for KIP-504 authorizer changes","Addressing these review comments from @ijuma : - Rename `listener` in interfaces to `listenerName` - Make `Endpoint#listenerName` Optional since it is in common package - Remove inheritance in the internal Scala EndPoint class - Move inner classes to companion object  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-24T08:53:20Z","2019-09-26T07:28:06Z"
"","6977","KAFKA-8392: Fix old metrics leakage by brokers that have no leadership over any partition for a topic","Addresses: https://issues.apache.org/jira/browse/KAFKA-8392  - Added `removeOldLeaderMetrics` in `BrokerTopicStats` to remove `MessagesInPerSec`, `BytesInPerSec`, `BytesOutPerSec` for any broker that is no longer a leader of any partition for a particular topic - Modified `ReplicaManager` to remove the metrics of any topic that the current broker has no leadership (meaning the broker either becomes a follower for all of the partitions in that topic or stops being a replica)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuvtran","2019-06-20T21:27:48Z","2019-07-19T22:25:19Z"
"","7347","KAFKA-8609: Add consumer rebalance metrics","Adding the following metrics in   1. AbstractCoordinator (for both consumer and connect)  * rebalance-latency-avg * rebalance-latency-max * rebalance-total * rebalance-rate-per-hour * failed-rebalance-total * failed-rebalance-rate-per-hour * last-rebalance-seconds-ago  2. ConsumerCoordinator  * partition-revoked-latency-avg * partition-revoked-latency-max * partition-assigned-latency-avg * partition-assigned-latency-max * partition-lost-latency-avg * partition-lost-latency-max   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-09-16T23:42:34Z","2020-04-24T23:58:22Z"
"","6879","MINOR Improve logging in the consumer for epoch updates","Adding a bit more context for some logging in the consumer when handling leader epoch updates. This should make debugging issues like [KAFKA-8477](https://issues.apache.org/jira/browse/KAFKA-8477) easier.","closed","","mumrah","2019-06-04T17:26:53Z","2019-06-04T21:49:31Z"
"","7019","KAFKA-8391: Improved the Connect integration tests to make them less flaky","Added the ability for the connector handles and task handles, which are used by the monitorable source and sink connectors used to verify the functionality of the Connect framework, to record the number of times the connector and tasks have each been started, and to allow a test to obtain a `RestartLatch` that can be used to block until the connectors and/or tasks have been restarted a specified number of types.  Typically, a test will get the `ConnectorHandle` for a connector, and call the `ConnectorHandle.expectedRestarts(int)` method with the expected number of times that the connector and/or tasks will be restarted, and will hold onto the resulting `RestartLatch`. The test will then change the connector (or otherwise cause the connector to restart) one or more times as desired, and then call `RestartLatch.await(long, TimeUnit)` to block the test up to a specified duration for the connector and all tasks to be started the specified number of times.  This commit also increases several of the maximum wait times used in other integration tests. It doesn’t hurt to potentially wait longer, since most test runs will not need to wait the maximum amount of time anyway. However, in the rare cases that do need that extra time, waiting a bit more is fine if we can reduce the flakiness and minimize test failures that happened to time out too early.  Unit tests were added for the new `RestartLatch` and `StopAndStartCounter` utility classes. This PR only affects the tests and does not affect any runtime code or API.  **This should be merged on `trunk` and backported to the `2.3.x` branch.**  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-07-01T16:09:31Z","2020-10-16T05:50:56Z"
"","7407","MINOR: Added name of provider to trace.","Added name of provider to trace when create SSLContext. This will be valuable for debugging and verifying the Provider used when create SSLContext.","closed","","jeffhuang26","2019-09-28T18:22:46Z","2019-10-02T14:57:53Z"
"","6847","KAFKA-8448: Too many kafka.log.Log instances (Memory Leak)","Added a method to delete the PeriodicProducerExpirationCheck task that kept adding to the log. Includes a test to show that the task exists and then is removed when the log is closed.","closed","","jolshan","2019-05-31T00:01:01Z","2019-06-18T17:53:00Z"
"","6973","MINOR: add useConfiguredPartitioner and skipFlush options for ProduceBench","Added a boolean to specify testing with no keys and pre-set partitions for records. Also added a boolean to specify whether the batch is flushed by the throttle. Usually this is to prevent added latency measurements, but it prevents testing with linger.ms and other scenarios to fill batches.  Modified a test to include these new parameters--both set to false which was the default behavior. All previous Trogdor ProduceBench tests will run as before if these fields are not specified.","closed","","jolshan","2019-06-19T23:43:59Z","2019-07-04T00:24:38Z"
"","7168","[WIP] KAFKA-8729, pt 4: Add client-side logic to augment error handling and partially fail records specified in error_records","Added `Sender#partialFailBatch` and `ProducerBatch#partiallyDone` to only fail and invoke callbacks of records in `error_records`.  Modified `FutureRecordMetadata` to re-bind a record's `ProduceRequestResult` to that of a new batch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","tuvtran","2019-08-06T17:40:33Z","2020-01-08T23:20:17Z"
"","7344","MINOR: Improve the org.apache.kafka.common.protocol code","Add UUID to the list of types documented in Type#toHtml.  This was overlooked in the change which added UUIDs as a supported KRPC type.  Type, Protocol, ArrayOf: use Type#isArray and Type#arrayElementType rather than typecasting to handle arrays.  This is cleaner.  It will also make it easier for us to add compact arrays (as specified by KIP-482) as a new array type distinct from the old array type.  Add MessageUtil#byteBufferToArray, as well as a test for it.  We will need this for handling tagged fields of type ""bytes"".  Schema#Visitor: we don't need a separate function overload for visiting arrays.  We can just call ""visit(Type field)"".  TestUUID.json: reformat the JSON file to match the others.  ProtocolSerializationTest: improve the error messages on failure.  Check that each type has the name we expect it to have.","closed","","cmccabe","2019-09-16T20:17:52Z","2019-09-25T15:20:52Z"
"","6739","MINOR: Add test for ConsumerNetworkClient.trySend","add test for the change of ConsumerNetworkClient.trySend in #6264,   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-05-15T09:21:29Z","2019-05-18T04:10:13Z"
"","6822","KAFKA-8433 : Use serializers and deserializers with IntegrationTestUtils","Add serializers and deserializers to methods  Only IntegrationTestUtils is modified. No regression detected in stream tests.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","callaertanthony","2019-05-26T21:20:22Z","2019-06-05T20:45:27Z"
"","7450","2.3 release.py sftp retries","Add retry capability to the `cmd` function in release.py. This allows for selectively retrying certain commands which might be flaky, like the SFTP puts.","closed","","mumrah","2019-10-04T22:17:46Z","2020-01-30T19:28:14Z"
"","6934","KIP-477 Add PATCH method for connector config in Connect REST API [WIP]","Add PATCH method for connector config in Connect REST API https://cwiki.apache.org/confluence/display/KAFKA/KIP-477%3A+Add+PATCH+method+for+connector+config+in+Connect+REST+API  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","ivanyu","2019-06-13T14:09:56Z","2020-03-21T23:52:47Z"
"","6724","MINOR: add docs for KIP-354 KAFKA-7321","add docs in design.html, configuration.html. change description for LogCleanerMinCleanRatioDoc, LogCleanerMaxCompactionLagMsDoc, and MIN_CLEANABLE_DIRTY_RATIO_DOC.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","xiowu0","2019-05-13T23:23:52Z","2019-05-14T05:54:07Z"
"","7372","MINOR: add versioning to request and response headers","Add a version number to request and response headers.  The header version is determined by the first two 16 bit fields read (API key and API version).  For now, ControlledShutdown v0 has header version 0, and all other requests have v1.  Once KIP-482 is implemented, there will be a v2 of the header which supports tagged fields.","closed","","cmccabe","2019-09-19T21:07:57Z","2019-10-10T16:01:04Z"
"","6551","KAFKA-7841: Implement KIP-419 adding SourceTask.stopped method","Add a new SourceTask.stopped method called as the last method in the lifecycle of a SourceTask. Called just as the resources are cleaned up in the Kafka Connect runtime.  Testing by adding checks that the new method is called as expected in the existing Kafka Connect runtime tests.  The contribution is my original work and I license the work to the project under the project's open source license.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","connect,","AndrewJSchofield","2019-04-08T19:00:05Z","2020-03-22T03:43:07Z"
"","7160","[KIP-498][KAFKA-4090] Add client-side configuration for maximum response size","Add a new property `max.response.size` as part of the configuration of consumers and producers. This allows to set `NetworkReceive#maxSize` with a user-configurable value and enforce validating the size of messages before memory allocation. The objective is to prevent clients from running out of memory when message size is unexpectedly large.  [KIP-498: Add client-side configuration for maximum response size to protect against OOM](https://cwiki.apache.org/confluence/display/KAFKA/KIP-498%3A+Add+client-side+configuration+for+maximum+response+size+to+protect+against+OOM)  - Tested via integration tests for consumers. - Verified no `OutOfMemoryError` was thrown in network thread when sending a plaintext message to a TLS endpoint and a heap size of 256 MB.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Hangleton","2019-08-05T08:07:23Z","2020-06-20T08:29:30Z"
"","6658","KAFKA-8309: Add Consolidated Connector Endpoint to Connect REST API","Add `?expand` query param for additional info on `/connectors`. See [KIP-465](https://cwiki.apache.org/confluence/display/KAFKA/KIP-465%3A+Add+Consolidated+Connector+Endpoint+to+Connect+REST+API).  *This should only be merged if KIP-465 is approved.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","norwood","2019-05-01T17:35:08Z","2020-10-16T06:19:07Z"
"","7467","KAFKA-8993: EOS perf test framework","Adapted from ProducerPerformance framework to test EOS application performance in a consume-process-produce loop.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","tests,","abbccdda","2019-10-08T16:38:34Z","2019-11-04T17:45:47Z"
"","7415","Fix type of timestampDelta in documentation","According to the `DefaultRecord` implementation, Kafka uses `varlong` instead of `varint` for the timestamp delta.  More info: https://github.com/apache/kafka/blob/9aa660786e46c1efbf5605a6a69136a1dac6edb9/clients/src/main/java/org/apache/kafka/common/record/DefaultRecord.java#L190","closed","","lcobucci","2019-09-29T19:43:47Z","2020-10-18T14:32:00Z"
"","6750","KAFKA-8426; Fix for keeping the ConfigProvider configs consistent with KIP-297","According to KIP-297 a parameter is passed to ConfigProvider with syntax ""config.providers.{name}.param.{param-name}"". Currently AbstractConfig allows parameters of the format ""config.providers.{name}.{param-name}"". With this fix AbstractConfig will be consistent with KIP-297 syntax.","closed","","tadsul","2019-05-17T00:11:44Z","2019-06-03T11:56:32Z"
"","6744","Added missing method parameter to javadoc","Accidentally recognized a missing parameter of one of the abstract methods of `AbstractCoordinator` while reading code","closed","","sandmannn","2019-05-15T22:18:17Z","2019-05-16T15:16:19Z"
"","7390","MINOR: AbstractRequestResponse should be an interface","AbstractRequestResponse should be an interface, since it has no concrete elements or implementation.  Move AbstractRequestResponse#serialize to RequestUtils#serialize and make it package-private, since it doesn't need to be public.","closed","","cmccabe","2019-09-25T16:47:34Z","2019-10-17T16:22:19Z"
"","7471","[KAFKA-8820] [WIP] Ongoing work for changing the ReassignPartitionsCommand","A work-in-progress collection of changes to implement KIP-455 for the ReassignPartitionsCommand.  These changes are intended to remove the dependence of the ReassignPartitionsCommand on the ZK client and utilize the new AdminClient interfaces for doing partition reassignments. Eventually the command will support incremental changes to ongoing reassignments, including cancellation.  This is being done in the background and will be updated irregularly (hence the WIP designation).  The changes are significant, but the commits are ordered to be relatively small and easy to understand and build on each other; it is strongly recommended that the review be done a commit at a time rather than all at once.  The first several commits are ""infrastructure"" work that gets things ready. Then a service abstraction which can be used with either ZK or Admin clients is provided and implemented, so that the command can (eventually) run entirely with just the Admin client. Unit tests are provided for the Service client (XXX: not all tests are implemented yet but those that exist should be fine.)","closed","","steverod","2019-10-09T01:37:53Z","2020-05-12T21:06:10Z"
"","6763","MINOR: fix Streams version-probing system test","A version probing rebalance, triggers a second rebalance. If the second rebalance happens quickly, we see the log about successful group assignment twice.","closed","tests,","mjsax","2019-05-19T17:04:05Z","2019-05-21T09:38:19Z"
"","6854","KAFKA-7853: Refactor coordinator config","A test attempt to refactor current coordinator logic.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-31T19:47:24Z","2019-06-17T17:58:45Z"
"","6583","KAFKA-7903 : use automated protocol for offset commit request","A sub-effort for fully auto-mated Kafka protocols.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-04-15T05:03:17Z","2019-04-25T23:42:55Z"
"","6682","MINOR: Document improvement for JMH-Benchmark module","A slight document improvement for JMH-Benchmark module.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hustclf","2019-05-06T08:38:14Z","2019-05-06T12:42:04Z"
"","6549","KAFKA-5636: Sliding Window Aggregations in the DSL [WIP]","A simple POC implementation of sliding windows in the DSL. The underlying design can definitely be improved upon and optimized further: this is meant to kick off discussion and facilitate review of the proposed API, semantics/behavior, etc  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ableegoldman","2019-04-06T02:54:28Z","2019-04-13T01:07:58Z"
"","6841","KAFKA-8457: Move log from replica into partition","A partition object contain one or many replica objects. These replica objects in turn can have the ""log"" if the replica corresponds to the local node. All the code in Partition or ReplicaManager peek into replica object to fetch the log if they need to operate on that. As replica object can represent a local replica or a remote one, this lead to a bunch of ""if-else"" code in log fetch and offset update code.  NOTE: In addition to a ""log"" that is in use during normal operation, if an alter log directory command is issued, we also create a future log object. This object catches up with local log and then we switch the log directory. So temporarily a Partition can have two local logs. Before this change both logs are inside replica objects.  This change is an attempt to untangle this relationship. In particular it moves ""log"" from a replica object to Partition. So a partition contains a local log to which all writes go. And it maintains a list of replica for offset and ""caught up time"" data that it uses for replication protocol. The replica correspoding to Local node contains a log object, but the object is now read only and no code except Replica and test code use it. Every other part of code in Partion and ReplicaManger use the log object stored in Partition. This uncouples the replica-log relation and all the ""if-else"" code went away. Couple of more structural changes are made in this change: 1. Two subclasses of Replica are introduced: LocalReplica and RemoteReplica. This makes it clear what each replica stores and is capable of. 2. The ""log"" in Partition is also wrapped in a LogInfo wrapper, which encapuslates all the code that either operated on ""log"" or maintained state of it.  Unit tests have been updated to take care of change in heirarchy. Tested by running multiple brokers and produced and consumed data. Also changed log directory back and forth to make sure that alter log directory use case works.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-05-30T01:04:14Z","2019-06-17T16:29:48Z"
"","6839","KAFKA-8001: Move log from replica into partition","A partition object contain one or many replica objects. These replica objects in turn can have the ""log"" if the replica corresponds to the local node. All the code in Partition or ReplicaManager peek into replica object to fetch the log if they need to operate on that. As replica object can represent a local replica or a remote one, this lead to a bunch of ""if-else"" code in log fetch and offset update code.  NOTE: In addition to a ""log"" that is in use during normal operation, if an alter log directory command is issued, we also create a future log object. This object catches up with local log and then we switch the log directory. So temporarily a Partition can have two local logs. Before this change both logs are inside replica objects.  This change is an attempt to untangle this relationship. In particular it moves ""log"" from a replica object to Partition. So a partition contains a local log to which all writes go. And it maintains a list of replica for offset and ""caught up time"" data that it uses for replication protocol. The replica correspoding to Local node contains a log object, but the object is now read only and no code except Replica and test code use it. Every other part of code in Partion and ReplicaManger use the log object stored in Partition. This uncouples the replica-log relation and all the ""if-else"" code went away. Couple of more structural changes are made in this change: 1. Two subclasses of Replica are introduced: LocalReplica and RemoteReplica. This makes it clear what each replica stores and is capable of. 2. The ""log"" in Partition is also wrapped in a LogInfo wrapper, which encapuslates all the code that either operated on ""log"" or maintained state of it.  Unit tests have been updated to take care of change in heirarchy. Tested by running multiple brokers and produced and consumed data. Also changed log directory back and forth to make sure that alter log directory use case works.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-05-30T00:49:59Z","2019-05-30T00:53:45Z"
"","7305","KAFKA-8813: Refresh log config if it's updated before initialization","A partition log in initialized in following steps:  1. Fetch log config from ZK 2. Call LogManager.getOrCreateLog which creates the Log object, then 3. Registers the Log object  Step #3 enables Configuration update thread to deliver configuration updates to the log. But if any update arrives between step #1 and #3 then that update is missed. It breaks following use case:  1. Create a topic with default configuration, and immediately after that 2. Update the configuration of topic  There is a race condition here and in random cases update made in seocond step will get dropped.  This change fixes it by tracking updates arriving between step #1 and #3 Once a Partition is done initialzing log, it checks if it has missed any update. If yes, then the configuration is read from ZK again.  Added unit tests to make sure a dirty configuration is refreshed. Tested on local cluster to make sure that topic configuration and updates are handled correctly.  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","soondenana","2019-09-06T00:06:24Z","2019-10-15T18:22:27Z"
"","7437","KAFKA-8671: NullPointerException occurs if topic associated with GlobalKTable changes","A NullPointerException occurs when the global/.checkpoint file contains a line with an obsolete (but valid) topic. Log an error and throw exception when non-relevant topic-partitions from checkpoint file are encountered.  Added a unit test to verify that non-relevant topics are detected and an exception is thrown. Also, manually ran a streams application with a modified global/.checkpoint file containing an obsolete topic partition and verified that the error is logged and initialization fails.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","amleung21","2019-10-02T21:38:55Z","2019-10-16T02:07:29Z"
"","7418","KAFKA-8807: Flaky GlobalStreamThread test","A minor refactor to explicitly verify that `Processor#close` is only called once.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-09-30T14:21:23Z","2019-10-01T13:34:12Z"
"","7419","MINOR: Adjust logic of conditions to set number of partitions in step zero of assignment.","A minor change in logic to account for repartition topics where we might not have the num partitions yet in the metadata.  Ran all existing tests plus all streams system tests ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-09-30T16:17:29Z","2019-09-30T23:28:42Z"
"","7263","KAFKA-8580: Compute RocksDB metrics","A metric recorder runs in it own thread and regularly records RocksDB metrics from RocksDB's statistics. For segmented state stores the metrics are aggregated over the segments.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-08-28T13:04:20Z","2020-06-12T23:35:55Z"
"","6803","[MINOR] Improve docs for Global Store operations","A lot of confusion seems to have arisen from the StreamBuilder#addGlobalStore(...ProcessorSupplier) method. Users have assumed they can safely use this to transform records before populating their global state store; unfortunately this results in corrupted data as on restore the records are read directly from the source topic changelog, bypassing their custom processor.  We should probably provide a means to do this at some point but for the time being we should clarify the proper use of #addGlobalStore as it currently functions","closed","","ableegoldman","2019-05-24T00:11:16Z","2019-05-30T11:23:34Z"
"","6775","Minor: follow up for KAFKA-8365","A few minor things left over from #6731   * Callers can specify replicaId in OffsetsForLeaderEpoch request * Improved FetchRequest constructor","closed","","mumrah","2019-05-20T16:42:53Z","2019-05-21T22:50:22Z"
"","7060","KAFKA-8643: bring back public MemberDescription constructor","a compatibility fix aiming to avoid breaking user's code by accidentally removing a public constructor. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-07-09T23:48:16Z","2019-07-12T04:54:34Z"
"","6622","MINOR: Add support for Standalone Connect configs in Rest Server extensions","A bug was introduced in 7a42750d which was caught in system tests: The rest extensions fail if a Standalone worker config is passed, which does not have a definition for rebalance timeout.  The code will now check that the config is defined before parsing the value  A new unit test is added to cover Standalone mode for quick testing.","closed","connect,","cyrusv","2019-04-22T21:16:23Z","2020-10-16T06:19:06Z"
"","6991","KAFKA-8591: WorkerConfigTransformer NPE on connector configuration reloading","A bug in `WorkerConfigTransformer` prevents the connector configuration reload when the ConfigData TTL expires.   The issue boils down to the fact that `worker.herder().restartConnector` is receiving a null callback.   ``` [2019-06-17 14:34:12,320] INFO Scheduling a restart of connector workshop-incremental in 60000 ms (org.apache.kafka.connect.runtime.WorkerConfigTransformer:88) [2019-06-17 14:34:12,321] ERROR Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:227) java.lang.NullPointerException         at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1187)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1183)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:273)         at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:219)         at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)         at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)         at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)         at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)         at java.base/java.lang.Thread.run(Thread.java:834) ``` This patch keeps the same behaviour than before in the `WorkerConfigTransformer` in terms of ignoring any error returned from the callback. Do we still want to behave in the same way or we would like to handle any potential error? 🤔 maybe we can rely on the `scheduleReload` retrying again whenever the TTL expires or the connector task fails due to a stale configuration.    ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","nachomdo","2019-06-24T12:10:22Z","2019-07-09T06:07:11Z"
"","6493","KAFKA-7986: Distinguish logging from different ZooKeeperClient instances","A broken can have more than one instance of ZooKeeperClient. For example, SimpleAclAuthorizer creates a separate ZooKeeperClient instance when configured.  This commit makes it possible to optionally specify the name for the ZooKeeperClient instance. The name is specified only for a broker's ZooKeeperClient instances, but not for commands' and tests'.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ivanyu","2019-03-23T19:33:38Z","2019-03-26T05:27:14Z"
"","6820","MINOR: move connectorConfig to AbstractHerder","`StandaloneHerder` and `DistributedHerder` have identical implementations of `connectorConfig` (apart from one line of logging). This commit moves the common implementation of `connectorConfig` to `AbstractHerder`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","ivanyu","2019-05-26T17:16:35Z","2020-11-04T04:23:16Z"
"","6759","MINOR: fix typo in heartbeat request protocol","`s/generationid/generationId`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-05-18T00:56:25Z","2019-05-18T23:20:00Z"
"","7178","MINOR: remove unnecessary #remove overrides","`Iterator#remove` has a default implementation that throws `UnsupportedOperatorException` so there's no need to override it with the same thing.  Should be cherry-picked back to whenever we switched to Java 8","closed","streams,","ableegoldman","2019-08-08T01:05:33Z","2020-06-26T22:38:40Z"
"","7028","MINOR: Embedded connect cluster should mask exit procedures by default","`EmbeddedConnectCluster` has the ability to mask system exits to avoid killing the jvm. It appears that the default was intended to be `true`, but is actually `false`. The `maskExitProcedures` method on `EmbeddedConnectCluster.Builder` documents the parameter as:  ``` * @param mask if false, exit and halt procedures remain unchanged; true is the default. ``` Because this is not enabled by default as intended, we are seeing some build failures which exit abruptly: ``` 17:29:11 Execution failed for task ':connect:runtime:integrationTest'. 17:29:11 > Process 'Gradle Test Executor 25' finished with non-zero exit value 1 ``` The culprit often appears to be `ExampleConnectIntegrationTest`, which indeed does not override the default value of `maskExitProcedures`.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","hachikuji","2019-07-03T00:57:25Z","2020-10-16T06:31:23Z"
"","6523","Comment spelling nit","`CYGINW` probably should be `CYGWIN`  *More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mgrubent","2019-03-29T15:24:35Z","2019-03-30T00:40:33Z"
"","6727","MINOR: Fix flaky ConsumerTopicCreationTest","`ConsumerTopicCreationTest` relied on `KafkaConsumer#poll` to send a `MetadataRequest` within 100ms to verify if a topic is auto created or not. This is brittle and does not guarantee if the request made it to the broker or was processed successfully. This PR fixes the flaky test by adding another topic; we wait until we consume a previously produced record to this topic. This ensures MetadataRequest was processed and we could then check if the topic we're interested in was created or not.","closed","","dhruvilshah3","2019-05-14T01:52:25Z","2019-05-16T03:17:49Z"
"","7024","MINOR: Add more versions to connect compatibility","`ConnectDistributedTest.test_broker_compatibility` hasn't been updated with recent Kafka versions in quite some time.  This can be merged to `trunk` and easily backported to `2.3`, but if backported farther back some versions would have to be removed for each branch. For example, the `2.2` branch should not have a compatibility test for 2.3, and the `2.1` branch should not have a compatibility test for 2.3 or 2.2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","rhauch","2019-07-02T02:27:48Z","2020-10-16T05:50:57Z"
"","7189","KAFKA-7335: Store clusterId locally to ensure broker joins the right cluster","`clusterId` is stored in the `meta.properties`. During startup, the broker checks that it joins the correct cluster and fails fast otherwise.  The `meta.properties' is versioned. I have decided to not bump the version because 1) the clusterId is null anyway if not present in the file; and 2) bumping it means that rolling back to a previous version won't work.  I have refactored the way the metadata is read and written as it was strongly coupled with the brokerId bits. Now, the metadata is read independently during the startup and used to 1) check the clusterId and 2) get or generate the brokerId (as before).   Unit tests have been extended to cover the change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-08-09T23:50:06Z","2019-08-14T16:57:38Z"
"","7073","MINOR: Remove warning in generating RoundRobinAssignor javadoc","``` > Task :clients:javadoc clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java:78: warning - invalid usage of tag > clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java:78: warning - invalid usage of tag > clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java:78: warning - invalid usage of tag > 3 warnings ```  `'>'` character in javadoc should be replaced with `>`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-07-11T09:24:43Z","2019-11-30T14:38:46Z"
"","7180","[WIP] KAFKA-8779: Fix flaky tests introduced by KAFKA-7800","_WIP_","open","","stanislavkozlovski","2019-08-08T13:49:12Z","2019-08-09T16:48:28Z"
"","7075","KAFKA-8657: Client-side Automatic Topic Creation on Producer","[WIP]  This is a PR for KIP 487: https://cwiki.apache.org/confluence/display/KAFKA/KIP-487%3A+Automatic+Topic+Creation+on+Producer  To deprecate the broker configuration to automatically create topics, the producer now has configurations to automatically create topics. These include enabling auto-creation, the number of partitions and replication factor for automatically created topics. Tests have been added to show that the topic can be created automatically when the broker configuration is no longer enabled.","open","","jolshan","2019-07-11T21:42:53Z","2019-08-15T18:01:00Z"
"","7100","KAFKA-8179: KIP-429, new PartitionAssignor interface","[Subtask JIRA](https://issues.apache.org/jira/browse/KAFKA-8703) Main changes of this PR * Deprecate old consumer.internal.PartitionAssignor and add public consumer.PartitionAssignor * Refactor assignment/subscription data related classes for cleaner separation of user and consumer-provided state, plus easier to evolve API  Other previously-discussed cleanup included in this PR: * Remove Assignment.error added in [pt 1](https://github.com/apache/kafka/pull/6528/files) * Remove ConsumerCoordinator#adjustAssignment added in [pt 2](https://github.com/apache/kafka/pull/6778/)","closed","","ableegoldman","2019-07-18T00:30:51Z","2019-07-25T20:18:19Z"
"","7108","KAFKA-8179: add public ConsumerPartitionAssignor interface","[Subtask JIRA](https://issues.apache.org/jira/browse/KAFKA-8703) Main changes of this PR * Deprecate old consumer.internal.PartitionAssignor and add public consumer.ConsumerPartitionAssignor with all OOTB assignors migrated to new interface * Refactor assignor's assignment/subscription related classes for easier to evolve API * Removed version number from classes as it is only needed for serialization/deserialization  Other previously-discussed cleanup included in this PR: * Remove Assignment.error added in [pt 1](https://github.com/apache/kafka/pull/6528/files) * Remove ConsumerCoordinator#adjustAssignment added in [pt 2](https://github.com/apache/kafka/pull/6778/)","closed","","ableegoldman","2019-07-24T21:13:04Z","2019-10-28T18:36:59Z"
"","7425","KAFKA-8649: send latest commonly supported version in assignment","[PR 7423](https://github.com/apache/kafka/pull/7423) but targeted at 2.3","closed","","ableegoldman","2019-10-01T00:54:00Z","2019-10-07T22:34:05Z"
"","7426","KAFKA-8649: send latest commonly supported version in assignment","[PR 7423](https://github.com/apache/kafka/pull/7423) but targeted at 2.2","closed","","ableegoldman","2019-10-01T00:55:20Z","2019-10-07T22:33:50Z"
"","7427","KAFKA-8649: send latest commonly supported version in assignment","[PR 7423](https://github.com/apache/kafka/pull/7423) but targeted at 2.1","closed","","ableegoldman","2019-10-01T00:56:35Z","2019-10-07T22:33:45Z"
"","7087","KAFKA-8454: Add Java AdminClient Interface (KIP-476)","[KIP-476](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476%3A+Add+Java+AdminClient+Interface)  Adds a `Admin` interface.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2019-07-14T15:39:19Z","2019-07-23T08:02:39Z"
"","7162","KIP-476: Add new getAdmin method to KafkaClientSupplier","[KIP-476 ](https://cwiki.apache.org/confluence/display/KAFKA/KIP-476) saw the introduction of a new `Admin` interface to replace the use of the `AdminClient` abstract class.  A key driver for this was to allow the use of dynamic proxies for the admin client supplied to Kafka streams via the `KafkaClientSupplier`.  Unfortunately, it would be a binary incompatible change to change `KafkClientSupplier.getAdminClient` to return the `Admin` interface, rather than `AdminClient`. See https://github.com/apache/kafka/pull/7157 which reverted the change.  To avoid this, I've added a new method `getAdmin` and deprecated the old `getAdminClient`.  The new `getAdmin` method has a default implementation that simply calls the old `getAdminClient` method.  Both `DefaultKafkaClientSupplier` and `MockClientSupplier` implement the new method.  All KS code has been switched to call the new method.  This has the benefit of being binary compatible, but does leave users still needing to implement the, now deprecated, `getAdminClient` method, even though it's not called by KS code.  Thoughts?   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","big-andy-coates","2019-08-05T14:39:16Z","2019-08-23T08:36:05Z"
"","7392","KAFKA-8945/KAFKA-8947: Fix bugs in Connect REST extension API","[KAFKA-8945 Jira ticket](https://issues.apache.org/jira/browse/KAFKA-8945) [KAFKA-8947 JIra ticket](https://issues.apache.org/jira/browse/KAFKA-8947)  The changes here are simple: • KAFKA-8945: a few checks to ensure that constructor parameters in the `AbstractState` and `ConnectorHealth` classes are non-null and non-empty are fixed. The current logic erroneously ensures that they are either null or empty. • KAFKA-8947: a bug in how the Connect framework instantiates `TaskState` objects for use by REST extensions (specifically, incorrect order in arguments provided to a constructor) is fixed. The current logic, while erroneous, should never have arisen in the past by REST extension developers, as it would have been covered by KAFKA-8945.  The `RestExtensionIntegrationTest` is expanded on to test these changes in the wild, verifying that they work and preventing any future regressions.  This fix should be backported through to 2.0, when Connect REST extensions were initially introduced.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-09-25T23:56:50Z","2020-10-16T06:17:31Z"
"","7267","[WIP] KAFKA-8677: Flaky test testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl","[KAFKA-8800](https://github.com/apache/kafka/pull/7211) attempted to fix a flaky test, however the tests remained inconsistent even after the fix. Since this test is not reproducible locally, modifying the Jenkins builds and digging through the logs would help debug the cause of this issue.  Potential causes for this test's failure: 1. Client (consumer) did not get metadata for this partition 2. Client's metadata is out-of-date 3. Broker that hosts the leader does not know it is the leader. 4. It takes along time to consume  Findings: - Adding asserts before consuming records to ensure the client has up-to-date metadata was not the problem. - When running only the individual test, the test passes all checks. - When running only integration tests, the test failed failed inconsistently. - When running all JUnit and integration tests, the test failed inconsistently.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","anatasiavela","2019-08-28T21:40:52Z","2019-08-30T23:42:35Z"
"","6993","KAFKA-8586: Fail source tasks when producers fail to send records","[Jira](https://issues.apache.org/jira/browse/KAFKA-8586)  Previously, if the producer for a source task failed to send a record with a non-retriable error, the record would be silently skipped over. The source task would be allowed to commit offsets for the skipped record, and its status would remain at `RUNNING`.  The changes here cause source tasks to transition to the `FAILED` state if their producers fail to send a record with a non-retriable error, and they also change the logic for offset commits to wait for confirmation that records have made it to Kafka before their offsets can be committed.  Tested by running Connect unit tests locally.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-06-24T16:16:57Z","2021-01-12T18:32:47Z"
"","6959","KAFKA-8550: Fix plugin loading of aliased converters in Connect","[Jira](https://issues.apache.org/jira/browse/KAFKA-8550)  Summary of issue: connector validation fails if an alias is used for the converter since the validation for that is done via `ConfigDef.validateAll(...)`, which in turn invokes `Class.forName(...)` on the alias. Even though the class is successfully loaded by the `DelegatingClassLoader`, some Java implementations will refuse to return a class from `Class.forName(...)` whose name differs than the argument provided.  Summary of fix: alter `ConfigDef.parseType(...)` to _first_ invoke `ClassLoader.loadClass(...)` on the class in order to get a handle on the actual class object to be loaded, _then_ invoke `Class.forName(...)` with the fully-qualified class name of the to-be-loaded class and return the result. The invocation of `Class.forName(...)` is necessary in order to allow static initialization to take place; simply calling `ClassLoader.loadClass(...)` is insufficient.  Summary of testing: tested manually on trunk. Added unit test to `ConfigDefTest` that simulates the plugin-aliasing behavior of the `DelegatingClassLoader` and then invokes `ConfigDef.parseType` on an aliased class; this test fails on the current trunk but succeeds with the changes proposed here.  This should be backported at _least_ to 2.0; it's likely the issue goes back further than that but since it's been around for so long and has yet to be noted by anyone else it doesn't seem worth the effort to backport that much further if there are any significant merge conflicts. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-06-18T01:02:43Z","2020-10-16T06:17:29Z"
"","6789","KAFKA-8407: Fix validation of class and list configs in connector client overrides","[Jira](https://issues.apache.org/jira/browse/KAFKA-8407)  Because of how config values are converted into strings in the `AbstractHerder.validateClientOverrides()` method after being validated by the client override policy, an exception is thrown if the value returned by the policy isn't already parsed as the type expected by the client `ConfigDef`. A more thorough writeup of how this happens is available in the linked Jira ticket.  The fix here involves parsing client override properties before passing them to the override policy.  A unit test is added to ensure that several different types of configs are validated properly by the herder.  This bug fix should be included in the recently-cut 2.3 branch.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-05-22T17:29:48Z","2020-10-16T06:19:09Z"
"","6726","KAFKA-8363: Fix parsing bug for config providers","[Jira](https://issues.apache.org/jira/browse/KAFKA-8363)  The regex used to parse config provider syntax can fail to accurately parse provided configurations when multiple path-less configs are requested (e.g., `${provider:pathOne} ${provider:pathTwo}`). This change fixes that parsing and adds a unit test to prevent regression.  This bug is present since the addition of config providers and so should be backported through to 2.0, when they were first added. ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-05-14T01:23:05Z","2020-10-16T06:19:08Z"
"","6651","KAFKA-8304: Fix registration of Connect REST extensions","[Jira](https://issues.apache.org/jira/browse/KAFKA-8304)  Currently, a REST extension can cause deadlock if it requests the list of connectors from its `ConnectRestExtensionContext.clusterState()` in its `ConnectRestExtension.register(...)` method. This is because the `ConnectClusterState` implementation is backed by a `HerderProvider` that, at that time, has no associated `Herder` instance, and since that `Herder` is given to the `HerderProvider` later by the same thread, deadlock occurs until the call to `HerderProvider.get()` made by the `ConnectClusterStateImpl` times out. At this point, startup fails and the Connect worker dies.  The changes here separate `RestServer.start(...)` into two separate methods. The first, `RestServer.initializeServer()`, starts the Jetty server and binds to a port, which ensure the accuracy of the `RestServer.advertisedUrl()` method that is used later on by both the `ConnectStandalone` and `ConnectDistributed` classes to determine the worker ID. The second, `RestServer.initializeResources(Herder herder)` actually creates the Connect REST resources (`RootResource`, `ConnectorsResource`, etc.) and registers all REST extensions.  Since these changes make `HerderProvider` obsolete and it is not part of any public API, that interface is also removed.  This approach ensures that the Connect REST interface is started only when all of its REST extensions have been successfully registered, which is important for security use cases where request filters are installed and parts of the Connect REST API are subject to authentication/authorization.  Between the call to `RestServer.intializeServer()` and `RestServer.initializeResources(...)`, any requests made to the worker will result in a 404 response. This is slightly less desirable than the current behavior, which is to block on requests until the herder is up and running, but shouldn't be too much of an issue.  A new integration test is added that verifies that a call to `ConnectRestExtensionContext.clusterState().connectors()` succeeds. This test fails on the current `trunk` branch, and succeeds with the changes involved in this PR.  These changes should be backported through to 2.0.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-04-30T00:17:16Z","2020-10-16T06:19:07Z"
"","6644","KAFKA-8299: Add generic-safe getConfiguredInstance() methods to AbstractConfig","[Jira](https://issues.apache.org/jira/browse/KAFKA-8299)  The changes here add four new methods to the `AbstractConfig` class that mirror the existing `getConfiguredInstance(...)` and `getConfiguredInstances(...)` methods; the only difference is that any parameter of type `Class` is changed to `TypeLiteral` and that type checking is done with `TypeUtils.isInstance(...)` instead of `Class.isInstance(...)`  A new unit test (`AbstractConfigTest.testGenericClassInstantiation()`) confirms this behavior, including the lack of unchecked cast warnings and the throwing of a runtime exception when an attempt is made to instantiate a class that extends from/implements the proper raw type but with improper type parameters.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","C0urante","2019-04-26T23:21:00Z","2022-02-27T03:56:40Z"
"","6584","KAFKA-8231: Expansion of ConnectClusterState interface","[Jira](https://issues.apache.org/jira/browse/KAFKA-8231) and [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-454%3A+Expansion+of+the+ConnectClusterState+interface)  The changes here add new methods to the `ConnectClusterState` interface so that Connect REST extensions can be more aware of the current state of the Connect cluster they are added to. The new methods allow extensions to query for connector and task configurations, as well as the ID of the Kafka cluster targeted by the Connect cluster.  All new methods have new unit tests added for their implementations in the `ConnectClusterStateImpl` class.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","C0urante","2019-04-15T18:23:12Z","2022-02-27T03:56:46Z"
"","7045","KAFKA-6263: Expose metrics for group and transaction metadata loading duration","[JIRA](https://issues.apache.org/jira/browse/KAFKA-6263)  - Add metrics to provide visibility for how long group metadata and transaction metadata take to load in order to understand some inactivity seen in the consumer groups - Tests include mocking load times by creating a delay after each are loaded and ensuring the measured JMX metric is as it should be  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","anatasiavela","2019-07-08T16:07:36Z","2019-08-04T04:03:15Z"
"","7077","KAFKA-8637: WriteBatch objects leak off-heap memory","[7050](https://github.com/apache/kafka/pull/7050) for branch 2.2, to be cherry-picked back to 2.1","closed","","ableegoldman","2019-07-11T23:52:08Z","2019-07-12T17:21:09Z"
"","7125","MINOR: Upgrade jackson-databind to 2.9.9.3","2.9.9.1 and 2.9.9.2 include security fixes while 2.9.9.3 fixes a regression introduced in 2.9.9.2.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-07-28T02:15:22Z","2019-08-07T06:01:14Z"
"","7153","KAFKA-8741: optimize the use of mx4j web interface","1. Since mx4j-tools.jar is not packaged into the final released tgz file by default, we can't monitor kafka brokers by web inferface. If we want to do it, we must find a mx4j-tools.jar in the maven repository by our own and put it under the directory $KAFKA_HOME/libs, which is very inconvenient. 2. In Mx4jLoader, we read the property values of 'kafka_mx4jenable' and 'mx4jport' from system properties, but there is no way for us to set the switch and customize the port of mx4j-tools. We could add a configuration item in kafka-run-class.sh. For example, if MX4J_PORT is set, we add -Dkafka_mx4jenable=true -Dmx4jport=$MX4J_PORT into the start command.  testing strategy: 1. build the release tar.gz file 2. check if mx4j-tools.jar is under the directory of $kAFKA_HOME/libs  3. start broker, and check if http://$broker_ip:$MX4J_PORT works.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","maohong","2019-08-02T07:18:12Z","2019-08-19T08:35:46Z"
"","6767","MINOR: trivial cleanups","1. Remove unused imports from `CommandLineUtils`, `KafkaConfig`. 2. Add ignore case in `KafkaController#process`, `GroupCoordinator#doSyncGroup`, and `GroupCoordinator#handleHeartbeat`.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-05-19T21:48:50Z","2019-05-24T16:08:28Z"
"","6653","MINOR: trivial cleanups","1. Remove `ListenerConnectionQuota#listenerPropName`: never used. 2. `MirrorMaker`: remove unused import, `java.util.concurrent.TimeUnit`. 3. `ConsumerBounceTest`: remove unused import, `util.control.Breaks._`. 4. `DynamicConnectionQuotaTest#[testDynamicConnectionQuota, testDynamicListenerConnectionQuota]`: remove unused local variables.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-04-30T14:08:12Z","2019-05-07T15:37:00Z"
"","7312","KAFKA-8421: Still return data during rebalance","1. Not wait until `updateAssignmentMetadataIfNeeded` returns true, but only call it once with 0 timeout. Also do not return empty if in rebalance.  2. Trim the pre-fetched records after long polling since assignment may have been changed.  3. Also need to update SubscriptionState to retain the state in `assignFromSubscribed` if it already exists (similar to `assignFromUser`), so that we do not need the transition of INITIALIZING to FETCHING.  4. Unit test: this actually took me the most time :)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","consumer,","guozhangwang","2019-09-07T05:16:44Z","2020-04-24T23:43:42Z"
"","6498","KAFKA-6820: Refactor Stream Metrics","1. Keep each level's sensor creation and metrics registration / de-registration logic inside the XXMetrics class itself. Making StreamsMetricsImpl a util function centric plus enwrapping the Metrics registry which is going to be shared for all the XXXMetrics (KAFKA-6819)  1.a Each XXMetrics object will keep track of all the sensors it has ever created, and a `clear` function which must be called when the corresponding object (a thread, a task, etc) is closed.  1.b From anywhere inside the code base, as long as the class can access the StreamsMetricsImpl class it can use the util functions to get the sensor it wants to record from the `metrics` registry following the protocol. Only when it cannot access StreamsMetricsImpl the sensor need to be pre-accessed and passed through the constructor.  1.c De-couple the original thread-level sensors as `StreamThreadMetrics `, extracted from the StreamsMetricsImpl class -- hence the latter can become a pure util functions provider and not keep track of any sensors.  2. Make metric / group / tag names constant strings, residing either in each level's metrics class if they belong to specific sensors, and in StreamsMetricsImpl if they are util strings like suffix / prefix.  3. Change public StreamsMetrics interface to be more intuitive for users (KAFKA-6820). And make all internal usages to leverage on the provided util functions of StreamsMetricsImpl as well.  4. Remove default parent-sensors from everywhere.  5. TEST: use `getMetricByName` and `getMetricByNameAndTags` from the utils class across all unit test classes.  6. MINOR: because of 1), `MockStreamsMetrics` is not needed any more and can be replaced with StreamsMetricsImpl now since the latter is now a stateless util layer (tech debt KAFKA-5676).  NOTE one thing's missing here is the compatibility path with old metrics, which will be done in a follow-up PR.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-03-25T07:48:23Z","2020-04-24T23:57:59Z"
"","6530","add break keyword, when globalGroups has already updated.","1. Just add break keyword, when globalGroups has already updated.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","jffree","2019-04-02T10:50:09Z","2019-04-02T10:50:20Z"
"","6778","KAFKA-8179: Part 2, ConsumerCoordinator Algorithm","1. In ConsumerCoordinator, select the protocol as the common protocol from all configured assignor instances' supported protocols with the highest number.  1.b. In onJoinPrepare: only call onPartitionRevoked with EAGER. 1.a. In onJoinComplete: call onPartitionAssigned with EAGER; call onPartitionRevoked following onPartitionAssigned with COOPERATIVE, and then request re-join if the error indicates so. 1.c. In performAssignment: update the user's assignor returned assignments by **excluding** all partitions that are still owned by some other members.  2. I've refactored the Subscription / Assignment such that: assigned partitions, error codes, and group instance id are not-final anymore, instead they can be updated. For the last one, it is directly related to the logic of this PR but I felt it is more convienent to go with other fields.  3. Testing: primarily in ConsumerCoordinatorTest, make it parameterized with protocol, and add necessary scenarios for COOPERATIVE protocol.  I intentionally omitted the documentation change since there are some behavioral updates that needs to be finalized in later PRs, and hence I will also only add the docs in later PRs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-20T23:36:34Z","2020-04-24T23:58:10Z"
"","7349","fix-bug","1. fix the error 'sed: invalid option -- 'E''  , when the  bin/kafka-run-class.sh scripts run on the centos 7;  2. fix the error 'java.nio.file.NoSuchFileException: server.properties' , when run the command 'kafka-server-start.sh   server.properties'  in anyother directory;","open","","liangkiller","2019-09-17T05:42:21Z","2019-09-17T05:42:21Z"
"","7244","Improve connect exception reporting","1. Avoid the ""null"" reason in the dreaded Connect error message ""Error getting config definition from Transformation: null""  2. Always log the exception in the Connect server. In my scenario, a java.lang.InstantiationException would result in no logging in Connect.","closed","connect,","leigh-perry","2019-08-23T07:50:14Z","2021-03-10T05:41:50Z"
"","6649","KAFKA-9526: Augment topology description with serdes [WIP]","1. Augment the Source / Sink / Store with serde class names in topology description.  2. Also found a couple bugs that in our serde inheritance, some of the serdes are not corrected inherited (thanks to the exposed topology in 1).  3. Adjusted unit tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","guozhangwang","2019-04-29T17:25:18Z","2020-04-21T23:32:02Z"
"","7304","KAFKA-8880: Add overloaded function of Consumer.committed","1. Add the overloaded functions. 2. Update the code in Streams to use the batch API for better latency (this applies to both active StreamsTask for initialize the offsets, as well as the StandbyTasks for updating offset limits). 3. Also update all unit test to replace the deprecated APIs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","guozhangwang","2019-09-05T23:48:12Z","2020-06-12T23:32:05Z"
"","6884","KAFKA-8179: Part 3, Add PartitionsLost API for resetGenerations and metadata/subscription change","1. Add onPartitionsLost into the RebalanceListener, which will be triggered when a) reset generations due to errors, and b) topic metadata changed and owned partitions no longer exist.  When resetting generations proactively and subscription changes, we will trigger onPartitionsRevoked instead.  2. Semantical behavior change: with COOPERATIVE protocol, if the revoked / lost partitions are empty, do not trigger the corresponding callback at all. For added partitions though, even if it is empty we would still trigger the callback as a way to notify the rebalance event; with EAGER protocol, revoked / assigned callbacks are always triggered.  The ordering of the callback would be the following:  a. Callback onPartitionsRevoked / onPartitionsLost triggered. b. Update the assignment (both revoked and added). c. Callback onPartitionsAssigned triggered.  In this way we are assured that users can still access the partitions being revoked, whereas they can also access the partitions being added.  3. Semantical behavior change (KAFKA-4600): if the rebalance listener throws an exception, pass it along all the way to the `consumer.poll` caller, but still completes the rest of the actions. Also, the newly assigned partitions list does not gets affected with exception thrown since it is just for notifying the users.  4. Semantical behavior change: the ConsumerCoordinator would not try to modify assignor's returned assignments, instead it will validate that assignments and set the error code accordingly: if there are overlaps between added / revoked partitions, it is a fatal error and would be communicated to all members to stop; if revoked is not empty, it is an error indicate re-join; otherwise, it is normal.   5. Minor: with the error code removed from the Assignment, ConsumerCoordinator will request re-join if the revoked partitions list is not empty.  6. Updated ConsumerCoordinatorTest accordingly. Also found a minor bug in MetadataUpdate that removed topic would still be retained with null value of num.partitions.  7. Updated a few other unit tests that are exposed due to this change.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-06-05T01:48:10Z","2020-04-24T23:58:12Z"
"","6528","KAFKA-8179: Part I, Bump up consumer protocol to v2","1. Add new fields of subscription / assignment and bump up consumer protocol to v2. 2. Update tests to make sure old versioned protocol can be successfully deserialized, and new versioned protocol can be deserialized by old byte code.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-04-01T23:25:54Z","2020-04-24T23:58:03Z"
"","7423","KAFKA-8649: send latest commonly supported version in assignment","...instead of sending the leader's version and having older members try to blindly upgrade.   The only other real change here is that we will also set the `VERSION_PROBING` error code and return early from `onAssignment` when we are _upgrading_ our used subscription version (not just downgrading it) since this implies the whole group has finished the rolling upgrade and all members should rejoin with the new subscription version.  Also piggy-backing on a fix for a potentially dangerous edge case, where every thread of an instance is assigned the same set of active tasks.  Should be cherry-picked back to 2.1 2.4/trunk: this PR 2.3: [PR #7425](https://github.com/apache/kafka/pull/7425) 2.2: [PR #7426](https://github.com/apache/kafka/pull/7426) 2.1: [PR #7427](https://github.com/apache/kafka/pull/7427)","closed","","ableegoldman","2019-09-30T20:15:07Z","2020-06-26T22:38:23Z"
"","6545","MINOR: Fixed a few warning in core and connects","- var -> val - unused imports - javadoc fix  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-04-05T14:36:07Z","2019-04-05T18:00:49Z"
"","7322","MINOR: Use Admin Interface instead of AdminClient in Kafka tools.","- Typo corrections.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-09-11T10:59:01Z","2021-01-08T07:19:40Z"
"","7457","KAFKA-8262, KAFKA-8263: Fix flaky test `MetricsIntegrationTest` (#6922)","- Timeout occurred due to initial slow rebalancing. - Added code to wait until `KafkaStreams` instance is in state RUNNING to check registration of metrics and in state NOT_RUNNING to check deregistration of metrics. - I removed all other wait conditions, because they are not needed if `KafkaStreams` instance is in the right state.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-10-07T15:32:02Z","2019-10-07T19:59:08Z"
"","6922","KAFKA-8262, KAFKA-8263: Fix flaky test `MetricsIntegrationTest`","- Timeout occurred due to initial slow rebalancing. - Added code to wait until `KafkaStreams` instance is in state RUNNING to check registration of    metrics and in state NOT_RUNNING to check deregistration of metrics. - I removed all other wait conditions, because they are not needed if `KafkaStreams` instance is in    the right state.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-06-12T11:48:45Z","2019-10-21T10:53:48Z"
"","6512","Add KStream branch with fluent API","- stream.branch() (with no arguments) returns a KBranchedStream rather  than array of KStreams, allowing for a more fluent definition of  branches off of an original stream.  This is intended only as a proof of concept for discussion for a fluent branching API on KStream, in discussion in KIP 418.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","pgwhalen","2019-03-28T03:03:20Z","2019-03-28T03:03:20Z"
"","7455","MINOR: Bump version to 2.5.0-SNAPSHOT","- Set DEFAULT_FIX_VERSION to 2.5.0 in kafka-merge-pr.py   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-10-07T10:23:41Z","2019-10-07T14:34:59Z"
"","6552","MINOR: Correct KStream documentation","- Replaces 'punctuate each 1000ms' with 'punctuate each second' - Deletes text stating ProcessorContext#process() returns something  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-04-08T22:57:23Z","2019-10-21T10:53:19Z"
"","7429","KAFKA-8964: Rename tag client-id for thread-level metrics and below","- Renamed tag client-id to thread-id for thread-level metrics and below - Corrected metrics tag keys for state store that had suffix ""-id"" instead   of ""state-id""  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-10-01T13:41:32Z","2020-06-12T23:34:51Z"
"","6986","MINOR: trivial cleanups","- Remove unused method: `LogValidatorTest#createDiscontinuousOffsetRecords` - Remove unused default argument: `LogValidatorTest#createTwoBatchedRecords` - Remove unused local value: `PartitionTest#testGetReplica` - Remove unused local value: `ReplicaAlterLogDirsThreadTest#shouldTruncateToInitialFetchOffsetIfReplicaReturnsUndefinedOffset` - Remove unused local value: `ReplicaFetcherThreadTest#shouldTruncateToInitialFetchOffsetIfLeaderReturnsUndefinedOffset` - Remove unthrown exceptions: `ByteBufferLogInputStreamTest` (RuntimeExceptions are thrown here.)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-06-23T18:02:22Z","2019-07-08T23:53:03Z"
"","7202","MINOR: trivial cleanups","- Reformat header: `CustomDeserializerTest`, `ReplicaVerificationToolTest` - Remove unused constructor: `ConsumerGroupDescription` - Remove unused variables in `TimeOrderedKeyValueBufferTest#shouldRestoreV2Format` - Remove deprecated `Number` consturctor calls; use `Number#valueOf` instread.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dongjinleekr","2019-08-13T04:49:07Z","2019-12-18T15:39:29Z"
"","6601","KAFAK-3522: add API to create timestamped stores","- part of KIP-258  - adds public API to `Stores`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2019-04-18T07:49:13Z","2020-06-12T23:41:49Z"
"","7416","KAFKA-8934: Introduce instance-level metrics for streams applications","- Moves `StreamsMetricsImpl` from `StreamThread` to `KafkaStreams` - Adds instance-level metrics as specified in KIP-444, i.e.: -- version -- commit-id -- application-id -- topology-description -- state  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-09-30T09:08:11Z","2020-06-12T23:34:10Z"
"","6757","KAFKA-8381; Disable hostname validation when verifying inter-broker SSL","- Make endpoint validation configurable on SslEngineBuilder when creating an engine - Disable endpoint validation for engines created for inter-broker SSL validation since it is unsafe to use `localhost` - Use empty hostname in validation engine to ensure tests fail if validation is re-enabled by mistake - Add tests to verify inter-broker SSL validation  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-05-17T18:38:46Z","2019-05-20T18:18:37Z"
"","7474","KAFKA-8964: Refactor thread-level metrics depending on built-in metrics version","- Made commit-over-tasks sensor and skipped-records sensor optional since they are removed in   the latest version - Refactored methods for sensor creation - Adapted unit and integration tests   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-10-09T09:30:12Z","2020-06-12T23:31:28Z"
"","7258","KAFKA-8839 : Improve streams debug logging","- log lock acquistion failures on the state store  - Document required uniqueness of state.dir path  - Move bunch of log calls around task state changes to DEBUG  - More readable log messages during partition assignment  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vinothchandar","2019-08-27T22:02:01Z","2019-09-24T18:40:05Z"
"","7185","KAFKA-7149 : Reducing streams assignment data size","- Leader instance uses dictionary encoding on the wire to send topic partitions  - Topic names (most expensive component) are mapped to an integer using the dictionary  - Follower instances receive the dictionary, decode topic names back  - Purely an on-the-wire optimization, no in-memory structures changed  - Test case added for version 5 AssignmentInfo  *Summary of testing strategy (including rationale)*  - [x] Adding new unit tests   - [x] Size reduction tests  - [x] Test upgrade scenarios  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vinothchandar","2019-08-09T18:10:27Z","2019-09-24T18:39:54Z"
"","6824","KAFKA-7523: Add ConnectedStoreProvider to Processor API","- Implementing KIP-401, ConnectedStoreProvider is now the  superinterface of Processor/TransformerSuppliers, allowing the user to  provide state stores that will automatically be added to the topology  and connected to the associated processor or transformer.  This is an  alternate way to connect state stores to a processor, and cannot be  used along with passing the varags stateStoreNames to  KStream::process/transform. It also works with Topology::addProcessor.  - WordCountTransformerDemo is a middle ground between the low level  Processor API and the high level DSL, using KStream::transform and  ConnectedStoreProvider  - The restriction on adding the same state store twice is relaxed so  that multiple ConnectedStoreProviders can add the same store.  https://issues.apache.org/jira/browse/KAFKA-7523  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","pgwhalen","2019-05-27T18:40:35Z","2020-06-12T23:40:37Z"
"","7482","Fixes in org.apache.kafka.connect.health","- Fixed Parameter order - Fixed NPE check conditionals","closed","connect,","941design","2019-10-10T10:13:38Z","2020-03-22T00:49:12Z"
"","7266","MINOR: Fix capitalization inconsistency in ConfigCommand.scala","- Fix a minor capitalization inconsistency in `ConfigCommand.scala`  https://github.com/apache/kafka/blob/fcfee618ee440cb78e422963ba7db1fa03620b33/core/src/main/scala/kafka/admin/ConfigCommand.scala#L157  Example output: `Completed Updating config for entity: topic 'test'.`  Other similar output lines: https://github.com/apache/kafka/blob/fcfee618ee440cb78e422963ba7db1fa03620b33/core/src/main/scala/kafka/admin/ConfigCommand.scala#L335-L339  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","KevinLiLu","2019-08-28T20:21:51Z","2020-11-03T02:35:11Z"
"","7346","KAFKA-8913: Document topic based configs & ISR settings for Streams apps","- Documented how to set topic based configs   - Documented how to achieve MIN_IN_SYNC_REPLICAS_CONFIG = 2 for reliable apps  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","vinothchandar","2019-09-16T22:19:34Z","2019-09-19T18:45:28Z"
"","6821","Add ConnectedStoreProvider to Processor API","- ConnectedStoreProvider allows a Processor/TransformerSupplier to  specify a store it uses, which will be automatically added and  connected to the topology  I _do not_ recommend merging, this was a proof-of-concept implementation for KIP-401, and I don't think it should be the final version.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pgwhalen","2019-05-26T18:56:20Z","2019-05-27T18:41:27Z"
"","7255","KAFKA-8837: KafkaMetricReporterClusterIdTest may not shutdown ZooKeeperTestHarness","- Call `assertNoNonDaemonThreads` in test method instead of tear down method to avoid situation where parent's class tear down is not invoked. - Pass the thread prefix in tests that call `assertNoNonDaemonThreads` so that it works correctly. - Rename `verifyNonDaemonThreadsStatus` to `assertNoNonDaemonThreads` to make it clear that it may throw.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","anatasiavela","2019-08-26T22:24:09Z","2019-08-28T06:07:21Z"
"","6685","KAFKA-8240: Fix NPE in Source.equals()","- backport of PR #6589 to 2.2 branch","closed","streams,","mjsax","2019-05-06T13:36:53Z","2019-05-09T13:58:46Z"
"","6496","Allow Transformer/ProcessorSuppliers to declare store usage (proof of concept only)","- All public and private Transformer/ProcessorSupplier interfaces now  extend StateStoresSupplier, which allows them to specify what state  stores they use.  This eliminates the need for users defining a  topology with KStream::process/transform to give state store names in  while defining the topology.  This is intended only as demonstration of a possible API for KIP 401 (https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=97553756)  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pgwhalen","2019-03-23T23:07:42Z","2019-08-13T15:02:10Z"
"","6663","BUGFIX: Add missing recording of close of stand-by task","- Adds recording of close of a stand-by task to the `task-closed` metric  - Adds unit tests to verify the recording   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-05-02T10:34:30Z","2019-10-21T10:53:26Z"
"","6979","KAFKA-8578: Add basic functionality to expose RocksDB metrics","- Adds `RocksDBMetrics` class that provides methods to get   sensors from the Kafka metrics registry and to setup the   sensors to record RocksDB metrics - Extends `StreamsMetricsImpl` with functionality to add the   required metrics to the sensors.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-06-21T15:50:29Z","2020-06-12T23:40:22Z"
"","7417","MINOR: Shutdown RockDB metrics recording trigger thread","- added shutdown for thread that triggers recording of RocksDBMetrics - added unit tests to verify the start and shutdown of the thread - refactored a bit of code  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-09-30T14:09:40Z","2019-10-02T14:54:27Z"
"","6631","KAFKA-6819: Pt. 1 - Refactor thread-level Streams metrics","- `StreamsMetricsImpl` wraps the Kafka Streams' metrics registry and provides logic to create     and register sensors and their corresponding metrics. An example for such logic can be found in     `threadLevelSensor()`. Furthermore, `StreamsMetricsmpl` keeps track of the sensors on the     different levels of an application, i.e., thread, task, etc., and provides logic to remove sensors per     level, e.g., `removeAllThreadLevelSensors()`. There is one `StreamsMetricsImpl` object per     application instance. - `ThreadMetrics` contains only static methods that specify all built-in thread-level sensors and     metrics and provide logic to register and retrieve those thread-level sensors, e.g., `commitSensor()`.  - From anywhere inside the code base with access to `StreamsMetricsImpl`, thread-level sensors can     be accessed by using ThreadMetrics. - `ThreadsMetrics` does not inherit from `StreamsMetricsImpl` anymore.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-04-25T00:28:18Z","2019-10-21T10:53:30Z"
"","6875","KAFKA-7315 DOCS update TOC internal links serdes all versions","- **apache/kafka** docs update per https://issues.apache.org/jira/browse/KAFKA-7315 - follow up to PR https://github.com/apache/kafka-site/pull/177 to update same links in other version folders - associated PR for kafka-site repo is https://github.com/apache/kafka-site/pull/212  Reviewers: @mjsax , @JimGalasyn , @joel-hamill  Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-06-04T01:57:48Z","2019-06-10T22:43:11Z"
"","6967","KAFKA-8563: removing sizeDelimit call","*Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cryptoe","2019-06-19T10:08:02Z","2019-06-21T16:55:32Z"
"","7104","KAFKA-7849: Fix the warning when using GlobalKTable","*More detailed description of your change, This PR intend to fix the issue in KAFKA-7849 by filtering out the global topics from the main consumer, thus the main consumer won't subscribe to any of the global topics  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","streams,","omarsmak","2019-07-22T08:56:23Z","2019-09-04T07:03:46Z"
"","7056","KAFKA-8633: Fix Auto Generated Kafka Configuration Docs","*More detailed description of your change, This PR fixes 2 things for auto-generated Kafka documents, namely configuration table part. 1. Remove the duplicated /td tag. 2. Change angle brackets in some description into square brackets, so browsers can display properly.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","weichuliu","2019-07-09T11:06:28Z","2021-06-29T13:53:22Z"
"","6678","Fixes #8198 KStreams testing docs use non-existent method pipe","*More detailed description of your change, Minor fix of #8198 https://github.com/apache/kafka-site/pull/210  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [X ] Verify documentation (including upgrade notes)","closed","","ouertani","2019-05-05T08:22:11Z","2019-07-12T15:27:49Z"
"","6837","MINOR: fix integration tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.* Starting a few days ago, Kafka integration tests started to fail, with the error ""Zookeeper node failed to start"". The reason is kafka-run-class.sh can't find java in the PATH. Further investigation shows it is caused by a breaking change in openjdk:8 docker image: https://github.com/docker-library/openjdk/commit/3eb0351b208d739fac35345c85e3c6237c2114ec  The image no longer installs a symlink of java in standard path /usr/bin, but uses ENV directive to specify custom PATH. On the other hand, ducktape use Paramiko to SSH to other docker instances to start ZK, which doesn't source any bashrc, thus not able to find the java.  This PR fixes it by using ForceCommand directive in sshd_config so that the environment variable is set even for Paramiko sessions.  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  Running integration tests.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","likan999","2019-05-29T18:26:13Z","2019-06-13T21:38:40Z"
"","6964","KAFKA-8559: avoid kafka array list grow","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  see https://issues.apache.org/jira/browse/KAFKA-8559  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  oneliner, no test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenhoujx","2019-06-18T23:11:21Z","2019-06-19T04:28:05Z"
"","6965","KAFKA-8488 avoid String.format(), it's gabage.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  related to ticket: https://issues.apache.org/jira/browse/KAFKA-8488  `string.format` is causing too much allocation.   *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  trivial change, no test.   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenhoujx","2019-06-18T23:21:04Z","2019-11-15T02:21:55Z"
"","6646","KAFKA-8199: Fix ClassCastException when trying to groupBy after suppress","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  Fixes #8199 ClassCastException when trying to groupBy after suppress  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  https://github.com/apache/kafka/blob/da53141d931342563f5bbcf6b4846671d59e3033/streams/src/test/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessorTest.java#L451 ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","ouertani","2019-04-28T18:18:01Z","2019-05-06T22:19:18Z"
"","7031","KAFKA-8592; Fix for resolving variables for dynamic config as per KIP-421.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jeffhuang26","2019-07-03T17:47:57Z","2019-08-16T07:43:25Z"
"","7003","2.3","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","ShowLuu","2019-06-26T10:03:13Z","2019-06-26T10:04:19Z"
"","6553","2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","footprint8","2019-04-09T02:05:23Z","2019-04-25T16:15:07Z"
"","6845","tt","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","logoc","2019-05-30T15:31:04Z","2019-06-01T17:53:14Z"
"","7481","Scala bindings for flatTransform and flatTransformValues in KStream","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","kokachev","2019-10-10T04:54:34Z","2019-10-10T05:39:53Z"
"","6740","2.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","tuzunyu","2019-05-15T12:18:21Z","2019-05-15T18:13:07Z"
"","6535","KAFKA-8157","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [X ] Verify design and implementation  - [ X] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ouertani","2019-04-03T12:11:22Z","2019-04-04T11:02:59Z"
"","6999","Minor: code enhancment","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [* ] Verify design and implementation  - [* ] Verify test coverage and CI build status - [* ] Verify documentation (including upgrade notes)","closed","streams,","khaireddine120","2019-06-25T16:29:08Z","2019-06-26T07:13:45Z"
"","7462","MINOR: code and JavaDoc cleanup","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-10-08T03:45:58Z","2019-10-15T20:02:38Z"
"","7445","MINOR; Rework error messages in ConsumerGroupCommand.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dajac","2019-10-04T16:26:29Z","2020-10-06T20:11:35Z"
"","7402","MINOR:fixed typo and removed outdated variable name","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-09-27T06:10:34Z","2019-10-09T03:14:31Z"
"","7399","KAFKA-8807: Fix for potential test flakiness","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","bbejeck","2019-09-26T16:57:03Z","2019-09-27T17:54:54Z"
"","7376","KAFKA-8927: Deprecate PartitionGrouper interface","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2019-09-20T18:12:06Z","2020-06-12T23:35:14Z"
"","7368","Initial version of KIP 280","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ningliums","2019-09-19T05:20:56Z","2019-09-20T18:06:58Z"
"","7299","MINOR: enhance return value","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","khaireddine120","2019-09-05T07:53:58Z","2019-09-24T04:04:43Z"
"","7292","test","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lokeshdulla","2019-09-04T02:31:42Z","2019-09-04T02:32:08Z"
"","7274","MINOR: replace `late` with `out-of-order` in JavaDocs and docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2019-08-29T22:00:18Z","2019-09-20T01:29:19Z"
"","7250","KAFKA-8818 : Updated documentation to assign correct datatype.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asutosh936","2019-08-24T15:05:29Z","2020-03-29T05:11:52Z"
"","7232","MINOR: Use new `Admin` interface instead of `KafkaAdminClient` class","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-08-21T17:19:37Z","2019-09-04T06:21:52Z"
"","7225","KAFKA-8819 : fix plugin path loader for converter","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mageshn","2019-08-19T22:08:26Z","2019-08-22T04:43:40Z"
"","7174","KAFKA-8765: Remove interface annotations in Streams API","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-08-07T17:55:03Z","2019-08-14T05:39:58Z"
"","7135","Test2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-07-30T06:52:40Z","2019-08-25T07:52:07Z"
"","7134","Test1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-07-30T06:48:09Z","2021-01-12T08:11:36Z"
"","7131","KAFKA-8594: Add version 2.3 to Streams system tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","mjsax","2019-07-29T22:31:37Z","2019-08-21T17:27:01Z"
"","7002","MINOR: remove unnecessary if check","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","khaireddine120","2019-06-26T08:16:08Z","2019-09-06T06:49:51Z"
"","6970","update kafka","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","khaireddine120","2019-06-19T14:48:11Z","2019-07-04T07:56:57Z"
"","6950","MINOR: cleanup of some redundant checkstyle","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-17T20:01:28Z","2019-06-29T16:16:58Z"
"","6946","[KAFKA-8543]Optimize fetch thread allocation","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","ZhaoBo02","2019-06-16T06:05:31Z","2019-08-02T09:57:14Z"
"","6929","poc-4: define multiple concrete types of Materialized to bypass the current API","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-13T00:14:57Z","2019-06-13T21:01:22Z"
"","6921","KAFKA-7760","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","dulvinw","2019-06-12T09:39:01Z","2019-06-12T12:24:25Z"
"","6918","POC-3: make window store sub type of key value store","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-11T22:29:32Z","2019-06-13T21:00:02Z"
"","6897","MINOR: Fixed compiler warnings in LogManagerTest","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-06T20:38:45Z","2019-06-07T00:32:14Z"
"","6896","0.10.2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","SrikanthInavolu","2019-06-06T09:20:33Z","2019-06-06T13:26:12Z"
"","6885","KAFKA-8515: (POC) add state store type in KTable for correct materialization","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-05T02:18:22Z","2019-06-21T20:43:12Z"
"","6882","POC: introduce abstract table class to have state store type overloading","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-05T00:13:08Z","2019-06-07T22:16:41Z"
"","6874","MINOR: Add 2.2.1 to system tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vahidhashemian","2019-06-04T01:46:31Z","2019-06-04T01:47:44Z"
"","6873","MINOR: Bump system test version from 2.2.0 to 2.2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2019-06-03T23:33:41Z","2019-08-09T21:33:25Z"
"","6871","HOTFIX: Allow multi-batches for old format and no compression","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-06-03T17:30:49Z","2019-06-03T23:56:34Z"
"","6864","test1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-06-03T08:11:55Z","2019-06-03T10:58:43Z"
"","6862","[KAFKA-8465]Copying strategy for the topic dimension","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-06-02T12:42:28Z","2019-06-03T08:29:05Z"
"","6861","Copying strategy for the topic dimension","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-06-02T12:27:21Z","2019-06-02T12:41:25Z"
"","6860","KAFKA-8435: replace delete groups request/response with automated protocol","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","abbccdda","2019-06-02T05:31:44Z","2019-07-20T00:04:23Z"
"","6835","MINOR: Account for different versions in upgrade","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-05-29T00:42:01Z","2019-05-29T21:08:49Z"
"","6829","MINOR: Take a generic approach for the dev version to just look for SNAPSHOT","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","bbejeck","2019-05-28T15:23:14Z","2019-05-28T17:19:00Z"
"","6827","2.3","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","18842648483","2019-05-28T10:02:58Z","2019-05-29T05:45:25Z"
"","6758","MINOR: add Kafka Streams upgrade notes for 2.3 release","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-05-17T21:33:14Z","2019-05-24T17:58:17Z"
"","6752","MINOR: Log Cleaner need grouping empty segments even if the range of offsets is greater than Int.MaxValue","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","lambdaliu","2019-05-17T07:31:07Z","2019-05-18T18:17:53Z"
"","6749","MINOR: fixed broken link to the IBM article about j-zerocopy","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","commandini","2019-05-16T23:46:17Z","2019-05-18T21:41:40Z"
"","6688","0.10.2.better.expand.version2","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lordcheng10","2019-05-07T04:26:31Z","2019-05-07T06:28:14Z"
"","6683","2.0","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","umeshnhce85","2019-05-06T10:53:32Z","2019-05-11T06:42:10Z"
"","6661","KAFKA-3522: Interactive Queries must return timestamped stores","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2019-05-02T00:05:05Z","2020-06-12T23:41:35Z"
"","6614","[WIP -- do not merge] KAFKA-8272: Changed(De)Serializer should forward call to configure()","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","mjsax","2019-04-22T01:39:51Z","2019-10-08T06:35:57Z"
"","6607","MINOR: Fix IllegalStateException in Producer","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","producer,","mjsax","2019-04-18T23:20:13Z","2019-04-20T18:24:06Z"
"","6605","HOTFIX: remove unused import","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-04-18T22:02:53Z","2019-04-19T02:05:39Z"
"","6600","MINOR: Java8 cleanup","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-04-18T04:51:39Z","2019-04-18T04:53:47Z"
"","6599","MINOR: Java8 cleanup","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-04-18T03:10:08Z","2019-04-20T01:42:31Z"
"","6598","MINOR: Java8 cleanup","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-04-18T03:07:34Z","2019-04-20T01:50:07Z"
"","6597","KAFKA-8155: Add 2.2.0 release to system tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2019-04-18T02:56:55Z","2019-06-04T04:11:24Z"
"","6596","KAFKA-8155: Add 2.1.1 release to system tests","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mjsax","2019-04-18T02:53:19Z","2019-06-13T09:43:14Z"
"","6589","KAFKA-8240: Fix NPE in Source.equals()","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-04-17T01:15:44Z","2019-05-06T13:06:21Z"
"","6565","KAFKA-6455: KStream-KStream join should set max timestamp for result record","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","mjsax","2019-04-11T21:37:06Z","2020-06-12T23:44:17Z"
"","6562","MINOR: fixed missing close of Iterator, used try-with-resource where appropriate","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","pkleindl","2019-04-11T08:03:51Z","2019-04-17T00:02:11Z"
"","6556","KAFKA-8200: added Iterator methods for output to TopologyTestDriver","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","pkleindl","2019-04-09T21:50:11Z","2020-06-12T23:44:25Z"
"","6554","1.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","footprint8","2019-04-09T02:48:37Z","2019-04-25T16:14:29Z"
"","6537","MINOR: Fix ThrottledReplicaListValidator doc error.","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","huxihx","2019-04-04T07:08:12Z","2019-05-14T08:14:58Z"
"","6492","2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Captainhzy","2019-03-23T10:48:10Z","2019-03-23T10:49:40Z"
"","6491","2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Captainhzy","2019-03-23T08:14:30Z","2019-03-23T08:14:53Z"
"","6490","MINOR: add MacOS requirement to Streams docs","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","mjsax","2019-03-23T00:29:13Z","2019-03-25T17:00:31Z"
"","6486","2.1","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","Captainhzy","2019-03-22T12:51:33Z","2019-04-03T15:49:48Z"
"","6484","KAFKA-8142: Fix NPE for nulls in Headers","*More detailed description of your change, if necessary. The PR title and PR message become the squashed commit message, so use a separate comment to ping reviewers.*  *Summary of testing strategy (including rationale) for the feature or bug fix. Unit and/or integration tests are expected for any behaviour change and system tests should be considered for larger changes.*  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","mjsax","2019-03-21T18:05:13Z","2019-03-22T07:15:15Z"
"","6890","KAFKA-8488: guarded debug log to avoid frequent toString allocation","*More detailed description of your change, guarded debug log to avoid frequent toString allocation  *Summary of testing strategy (including rationale) no test b/c it's a trivial change to reduce TLAB allocation  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","wenhoujx","2019-06-05T18:16:41Z","2019-06-14T16:24:28Z"
"","7020","KAFKA-4212 & KAFKA-4273: RocksDB KV cache w/ TTL","*Expose a RocksDB KV cache with a TTL*  *Includes a passing unit test*","closed","","3rdLaw","2019-07-01T20:48:28Z","2019-08-06T22:35:42Z"
"","6911","KAFKA-7760","*Add broker configuration to set minimum value for segment.bytes*  segment.ms is not resolved as of yet","closed","","dulvinw","2019-06-10T20:03:27Z","2019-06-12T06:11:15Z"
"","7360","KAFKA-8920: Add a new property to enable/disable token clean scheduler","**Detail:** Introduce a new property to enable/disable the expired token clean scheduler Providing this new property would allow the developers to manipulate expired tokens externally by themselves, and Kafka process does not need to spend its resource to clean up the expired tokens, which is not the main feature.  **The testing strategy:** ensure the value of scheduler enabling flag is based on - PasswordEncoderSecret Property is set or not (same as variable: tokenAuthEnabled) - DelegationTokenCleanSchedulerEnable Property If one of them is false, this flag must be false  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","bentocast","2019-09-18T13:00:12Z","2019-09-18T13:00:12Z"
"","6915","KAFKA-8334 Executor to retry delayed operations failed to obtain lock","**ASF**: https://issues.apache.org/jira/browse/KAFKA-8334  ### Brief Summary: We have seen `OffsetCommit` timed out when we do manual offset commit with low traffic. When we append the offset commit to the topic `__consumer_offsets`, the `DelayedProduce` would need to obtain the group metadata in order to complete. If the group metadata is obtained by others (such as `HeartBeat`), it would fail and it would only be retried when there is a next `OffsetCommit`  ### Reproduce #### Methodology 1. DelayedProduce on __consumer_offsets could not be completed if the group.lock is acquired by others 2. We spam requests like Heartbeat to keep acquiring group.lock 3. We keep sending OffsetCommit and check the processing time  #### Reproduce Script https://gist.github.com/windkit/3384bb86dc146111d1e0857e66b85861 - jammer.py - join the group ""wilson-tester"" and keep spamming Heartbeat - tester.py - fetch one message and do a long processing (or sleep) and then commit the offset  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","windkit","2019-06-11T13:13:56Z","2020-10-29T18:16:24Z"
"","7213","[DO NOT MERGE] KAFKA-8802: ConcurrentSkipListMap shows performance regression in cache and in-memory store","* replace ConcurrentSkipListMap * synchronize  See #7212   ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-08-14T17:32:19Z","2019-08-16T18:01:25Z"
"","6797","KAFKA-8417: Remove redundant network definition --net=host when starting testing docker containers","* Remove non-user-defined network --net=host which is redundant when starting system test docker containers  * Tested by running a round of system tests locally and on jenkins  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-05-23T18:39:30Z","2020-03-24T21:49:30Z"
"","6796","KAFKA-8415: Interface ConnectorClientConfigOverridePolicy needs to be excluded from class loading isolation","* New Connect plugin interface ConnectorClientConfigOverridePolicy needs to be excluded from the class loading isolation  * Added missing unit tests similar to the ones existing for previous plugins  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kkonstantine","2019-05-23T15:54:09Z","2019-05-23T19:56:55Z"
"","6680","MINOR: Fix bug in Struct.equals and use Objects.equals/Long.hashCode","* Fixed bug in Struct.equals where we returned prematurely and added tests * Update RequestResponseTest to check that `equals` and `hashCode` of the struct is the same after serialization/deserialization only when possible. * Use `Objects.equals` and `Long.hashCode` to simplify code * Removed deprecated usages of `JUnitTestSuite`  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-05-05T22:13:52Z","2019-05-09T13:21:30Z"
"","7030","KAFKA-5998: fix checkpointableOffsets handling","* fix checkpoint file warning by filtering checkpointable offsets per task * clean up state manager hierarchy to prevent similar bugs  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-07-03T17:01:09Z","2019-07-17T23:40:55Z"
"","7027","KAFKA-5998: fix checkpointableOffsets handling","* each task should write only the checkpoint offsets for changelog/partitions that it owns * check upon loading checkpointableOffsets and also immediately upon writing checkpointableOffsets that the task actually owns the partitions in question. This will prevent checkpoint file corruption in the future, and also help people clear out their corrupted checkpoint files so that they can be rebuild correctly.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-07-02T16:12:00Z","2019-07-03T20:57:54Z"
"","6958","MINOR: Update docs for KIP-415","* Details are added under Connect's section to describe the behavior under the new Connect protocol * Incremental cooperative rebalancing is added in the notable changes for 2.3.0  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","connect,","kkonstantine","2019-06-17T22:47:10Z","2020-10-16T05:50:56Z"
"","7057","KAFKA-8696: clean up Sum/Count/Total metrics","* Clean up one redundant and one misplaced metric * Clarify the relationship among these metrics to avoid future confusion  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","vvcephei","2019-07-09T17:04:06Z","2019-07-24T22:35:23Z"
"","6876","MINOR: not use COMMAND in bin/ scripts","$1 to the bin/ scripts is not command at all. Better not set ""COMMAND=$1"", to avoid confusion.","open","","lwjli","2019-06-04T03:14:53Z","2019-06-04T03:14:53Z"
"","6981","system tests - adding listener overrides","### what Now that we have an abstraction for listeners, people should be allowed to pass in certain overrides for their listeners. This allows a user to override both client & interbroker listeners with a dictionary of configs.  ### why There may be certain listener configs a user would want to override. Currently, to do so, they would either have to completely override all listener configs OR try to shoehorn an override based on the name of the security-protocol or an internal constant. This makes things a bit cleaner such that we handle the prefixing and simply add on some additional configs.  ### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-06-21T17:29:39Z","2019-06-27T17:10:44Z"
"","6700","KAFKA-7817 ConsumerGroupCommand Regex Feature","### KAFKA-7817 ConsumerGroupCommand Regex Feature  **Description**:  Add ability to select a subset of consumer groups using regex for operations: --describe, --delete and --reset-offsets **Motivation**: Suppose we have a subset of consumer groups with similar prefixes: group1, group2, ..., groupN and we want to apply operations (--describe/--delete/--reset) over [group1, group2, ..., group7] using CLI.  It would be easier to declare a regex instead of listing all groups by name: ```bash bin/kafka-consumer-groups.sh --bootstrap-server :9092 --describe      --regex group[1-7] bin/kafka-consumer-groups.sh --bootstrap-server :9092 --delete        --regex group[1-7] bin/kafka-consumer-groups.sh --bootstrap-server :9092 --reset-offsets --regex group[1-7] \                              --topic topic --to-offset 50 ```  **JIRA**: [Multiple Consumer Group Management with Regex](https://issues.apache.org/jira/browse/KAFKA-7817) **Discussion 1**: [Re: Multiple Consumer Group Management](https://www.mail-archive.com/dev@kafka.apache.org/msg93781.html) **Discussion 2**: [Re: ConsumerGroupCommand tool improvement?](https://www.mail-archive.com/dev@kafka.apache.org/msg90561.html) **Tests:**: *Unit tests implemented*  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","","rootex-","2019-05-08T07:55:37Z","2020-07-25T09:36:18Z"
"","7074","MINOR: Refactor admin client helpers for checking leader and ISR","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","jsancio","2019-07-11T21:40:07Z","2019-07-16T20:16:11Z"
"","6648","MINOR: Gradle and Gradle plugins versions are extracted into 'gradle/dependencies.gradle' file","### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","","dejan2609","2019-04-29T15:35:34Z","2019-05-04T18:23:19Z"
"","7476","MINOR: Add wait condition for state RUNNING to avoid flakiness","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","tests,","cadonna","2019-10-09T12:20:33Z","2019-10-11T17:27:28Z"
"","7473","MINOR: Add missing space in isolation-level doc","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-10-09T09:06:06Z","2020-01-02T13:35:12Z"
"","7454","MINOR: Update documentation.html to refer to 2.4","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-10-07T08:02:46Z","2019-10-07T08:06:21Z"
"","7404","KAFKA-8887; Use purgatory for ACL updates using async authorizers","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-27T12:23:10Z","2019-10-01T21:25:57Z"
"","7385","KAFKA-8880: Docs on upgrade-guide.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-09-24T22:42:20Z","2020-04-24T23:50:13Z"
"","7380","KAFKA-8907; Return topic configs in CreateTopics response (KIP-525)","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-24T11:04:38Z","2019-09-27T12:28:01Z"
"","7374","KAFKA-8848; Update system tests to use new AclAuthorizer","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-20T10:42:20Z","2019-09-24T09:30:18Z"
"","7370","MINOR: Test for non-blocking send using max.block.ms=0","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-19T08:10:08Z","2019-12-09T18:18:02Z"
"","7319","KAFKA-8892: Display the sorted configs in Kafka Configs Help Command.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-09-10T12:43:45Z","2019-09-23T08:33:50Z"
"","7296","KIP-396: Add AlterConsumerGroup/List Offsets to AdminClient","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-09-04T17:32:29Z","2020-05-15T23:08:19Z"
"","7279","KAFKA-8856: Add Streams config for backward-compatible metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","kip,","cadonna","2019-09-02T13:26:06Z","2020-06-12T23:35:40Z"
"","7278","KAFKA-8857; Don't check synonyms while determining if config is readOnly","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-09-02T12:43:32Z","2019-09-05T08:23:12Z"
"","7253","MINOR: Remove `activeTaskCheckpointableOffsets` from `AbstractTask`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-26T13:42:26Z","2019-10-21T11:41:12Z"
"","7230","MINOR: Small logging fixes in AbstractCoordinator","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-08-21T11:00:15Z","2019-11-18T22:32:09Z"
"","7216","MINOR: Add jmh benchmark for Streams cache","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","vvcephei","2019-08-15T01:34:45Z","2020-06-08T16:06:08Z"
"","7210","MINOR: Correct typo in test name `TimetampedSegmentsTest`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-08-14T06:55:55Z","2019-10-21T11:40:52Z"
"","7194","MINOR: Remove Utils.notNull, use Objects.requireNonNull instead","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-11T15:18:00Z","2019-08-12T03:42:25Z"
"","7149","MINOR: Update docs to reflect the ZK 3.5.5 upgrade","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","ijuma","2019-08-01T15:48:46Z","2019-08-02T13:09:55Z"
"","7141","KAFKA-8698 : Updated documentation to remove typo error.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","asutosh936","2019-07-31T01:13:45Z","2019-08-23T13:13:07Z"
"","7138","MINOR: Upgrade ducktape to 0.7.6","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","brianbushree","2019-07-30T20:59:01Z","2019-08-20T00:12:58Z"
"","7098","KAFKA-8599: Use automatic RPC generation in ExpireDelegationToken","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-07-17T10:50:01Z","2019-08-07T08:04:59Z"
"","7038","KAFKA-8598: Use automatic RPC generation in RenewDelegationToken","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-07-05T15:36:36Z","2019-08-09T07:22:31Z"
"","7037","KAFKA-8598: Use automatic RPC generation in RenewDelegationToken","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-07-05T15:12:55Z","2019-07-05T15:34:24Z"
"","7008","KAFKA-8602: Fix bug in stand-by task creation","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-06-27T19:58:48Z","2019-10-21T11:40:22Z"
"","7007","MINOR: Streams code cleanup.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-06-27T11:56:50Z","2019-08-12T14:53:45Z"
"","7006","MINOR: Fix typo in the exception message.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-06-27T10:52:19Z","2019-07-08T05:10:50Z"
"","7004","MINOR: Fix version in environment setup","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-06-26T13:12:48Z","2019-10-21T11:40:20Z"
"","7001","MINOR: Typo error corrected in docs.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-06-26T05:05:38Z","2019-07-08T05:11:17Z"
"","6930","MINOR: Refactor `MetricsIntegrationTest`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-06-13T06:57:37Z","2019-10-21T11:40:19Z"
"","6924","MINOR: Refactor `MetricsIntegrationTest`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-06-12T13:26:27Z","2019-10-21T10:53:52Z"
"","6887","KAFKA-8461: Wait for follower to join the ISR in testUncleanLeaderElectionDisabled Test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-06-05T07:55:45Z","2019-06-07T06:47:40Z"
"","6851","MINOR: Check for needed commits inside numIterations loop to avoid txn timeouts","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","open","streams,","bbejeck","2019-05-31T15:05:34Z","2019-09-04T16:24:27Z"
"","6828","KAFKA-8390: Use automatic RPC generation in CreateDelegationToken","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-05-28T11:31:59Z","2019-07-06T12:19:25Z"
"","6826","MINOR: Rename unit test class for `MeteredTimestampedWindowStore`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","cadonna","2019-05-27T20:15:05Z","2019-10-21T10:53:41Z"
"","6825","MINOR: Add tests to verify setting of serdes in timestamped key-value store","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-05-27T19:49:10Z","2019-10-21T10:53:44Z"
"","6816","KAFKA-8428: Always require a single batch with compressed messages","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","guozhangwang","2019-05-25T05:14:11Z","2020-04-24T23:58:01Z"
"","6808","HOTFIX: Fix wrong setting of Serde in `MeteredTimestampWindowStore`","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","cadonna","2019-05-24T08:48:11Z","2019-10-21T10:53:34Z"
"","6787","MINOR: Use `jps` cmd to find out the pid of TransactionalMessageCopier","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-05-22T10:53:16Z","2019-05-22T13:45:20Z"
"","6706","MINOR: Fixed the error in checking the AclCommand arguments.","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","kamalcph","2019-05-10T02:30:26Z","2019-05-10T10:02:10Z"
"","6691","KAFKA-8256: Replace Heartbeat request/response with automated protocol","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","mimaison","2019-05-07T13:25:23Z","2019-05-16T20:08:50Z"
"","6634","MINOR: fix 404 security features links","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","pierDipi","2019-04-25T07:48:27Z","2019-04-25T16:05:44Z"
"","6603","MINOR: fix core test build","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-18T16:27:47Z","2019-04-18T16:41:41Z"
"","6602","KAFKA-8254: Pass Changelog as Topic in Suppress Serdes","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","vvcephei","2019-04-18T15:37:50Z","2019-04-26T20:23:34Z"
"","6585","KAFKA-8241; Handle configs without truststore for broker keystore update","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-04-16T09:44:46Z","2019-04-17T20:15:37Z"
"","6573","KAFKA-8210: Fix link for streams table duality","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-04-12T17:03:28Z","2019-04-16T15:18:48Z"
"","6572","KAFKA-8208: Change paper link directly to ASM","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-04-12T15:45:51Z","2019-04-16T14:13:16Z"
"","6571","MINOR: Fix log message error of loadTransactionMetadata","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","lambdaliu","2019-04-12T11:29:44Z","2019-04-25T16:13:34Z"
"","6564","KAFKA-8209: Wrong link for KStreams DSL in core concepts doc","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","docs,","bbejeck","2019-04-11T20:51:34Z","2019-04-16T14:37:27Z"
"","6544","MINOR: Add security considerations for remote JMX in Kafka docs","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","rajinisivaram","2019-04-05T11:31:54Z","2019-04-10T07:09:12Z"
"","6538","MINOR: Use generated InitProducerId RPC","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","hachikuji","2019-04-04T16:31:09Z","2019-04-11T15:27:08Z"
"","6531","MINOR: Tighten up metadata upgrade test","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","streams,","guozhangwang","2019-04-02T17:14:28Z","2019-04-05T19:50:46Z"
"","6514","KAFKA-7981: Add fetcher and log cleaner thread count metrics","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","viktorsomogyi","2019-03-28T15:01:22Z","2019-10-11T17:53:33Z"
"","6483","MINOR: Add null check to array type fields in toString generation code","### Committer Checklist (excluded from commit message) - [ ] Verify design and implementation  - [ ] Verify test coverage and CI build status - [ ] Verify documentation (including upgrade notes)","closed","","omkreddy","2019-03-21T15:37:17Z","2019-03-24T03:08:34Z"
"","7158","DOCS-2227 kafka feedback copyedits","## Description  kafka repo updates per customer feedback, detailed on this Docs ticket:  https://confluentinc.atlassian.net/browse/DOCS-2227  Related PR on `kafka-site` repo: https://github.com/apache/kafka-site/pull/225  @mjsax , @joel-hamill , @JimGalasyn , @guozhangwang   Signed-off-by: Victoria Bialas   Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-08-03T08:22:25Z","2019-08-03T20:55:22Z"
"","6529","Removed Avro topic from TOC on kafka","## Description  - Removed TOC entry in Streams Developer Guide for Avro, since we have no content for this  ## Related  - Jira Ticket: https://issues.apache.org/jira/browse/KAFKA-8181 - PR on `kafka-site`: https://github.com/apache/kafka-site/pull/195   ### Committer Checklist (excluded from commit message)  - [ ] Verify documentation (including upgrade notes)  cc:  @guozhangwang, @joel-hamill    Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-04-01T23:29:05Z","2019-04-23T00:26:40Z"
"","6625","KAFKA-8227 DOCS Fixed missing links duality of streams tables","## Description  - Jira ticket: https://issues.apache.org/jira/browse/KAFKA-8227 - Related PR: https://github.com/apache/kafka-site/pull/204 - DOCS update to fix missing and broken links   ## Review Requests @bbejeck , @mjsax , @JimGalasyn , @joel-hamill   Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-04-24T02:18:35Z","2019-04-24T22:05:43Z"
"","6574","KAFKA-8213 DOCS - Fix typo in Streams Dev Guide","## Description  - **Jira ticket**:  https://issues.apache.org/jira/browse/KAFKA-8213  - Fixed typo in Streams Developer Guide interactive queries section  ## Related  PR on `kafka-site`: https://github.com/apache/kafka-site/pull/201  **cc**: @mjsax , @guozhangwang , @joel-hamill , @JimGalasyn   Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-04-12T19:38:35Z","2019-04-23T00:26:38Z"
"","6576","KAFKA-8212 DOCS (kafka) - Fix Maven artifacts table from cutting off …","## Description  **Jira ticket**: https://issues.apache.org/jira/browse/KAFKA-8212  - Fixed the table from cutting off the text by changing the table class to `datatable` (re-using a style that was already in the CSS and has an `overflow` property set).    - This isn't the ideal fix. Given the current CSS, the `` items _should_ have a horizontal scroll, but they don't. If the entries in the first column of this table get too long, it will cut off the final column text. I tried several strategies, and nothing seemed to work. Can revisit if I have more time, this should work for now.  ## Related PR on `kafka-site`: https://github.com/apache/kafka-site/pull/200  ## Reviewers, cc:  @mjsax , @guozhangwang , @joel-hamill , @JimGalasyn  Signed-off-by: Victoria Bialas","closed","","londoncalling","2019-04-12T23:13:11Z","2019-04-23T00:26:38Z"
"","7170","KIP-221 / Add KStream#repartition operation","# KIP-221: Enhance DSL with Connecting Topic Creation and Repartition Hint Tickets: [KAFKA-6037](https://issues.apache.org/jira/browse/KAFKA-6037) [KAFKA-8611](https://issues.apache.org/jira/browse/KAFKA-8611)  ## Description This is PR for [KIP-221](https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+DSL+with+Connecting+Topic+Creation+and+Repartition+Hint). Goal of this PR is to introduce new `KStream#repartition` operator and underline machinery that can be used for repartition configuration on `KStream` instance.   ## Notable Changes - Introduced `org.apache.kafka.streams.kstream.internals.graph.UnoptimizableRepartitionNode`. This node is **NOT** subject of optimization algorithm, therefore, each `repartition` operation is excluded from optimization algorithm. - Introduced `org.apache.kafka.streams.processor.internals.InternalTopicProperties` class that can be used for capturing repartition topic configurations passed via DSL operations - Added `org.apache.kafka.streams.processor.internals.InternalTopologyBuilder#internalTopicNamesWithProperties` map for storing mapping between internal topics and their corresponding configuration. If configuration is present `RepartitionTopicConfig` is enriched with configurations passed via DSL operations (In this case via `org.apache.kafka.streams.kstream.Repartitioned` class). - Added `KStreamRepartitionIntegrationTest` for testing different scenarios of `KStream#repartition` 1) Should create repartition topic if key changing operation was NOT performed 2) Should Perform Key Select Operation When Repartition Operation Is Used With Key Selector 3) Should Create Repartition Topic With Specified Number Of Partitions 4) Should Inherit Repartition Topic Partition Number From Upstream Topic When Number Of Partitions Is Not Specified. 5) Should Create Only One Repartition Topic When Repartition Is Followed By GroupByKey. 6) Should Generate Repartition Topic When Name Is Not Specified.  ### Committer Checklist (excluded from commit message) - [x] Verify design and implementation  - [x] Verify test coverage and CI build status - [x] Verify documentation (including upgrade notes)","closed","kip,","lkokhreidze","2019-08-06T18:20:30Z","2021-08-05T06:16:52Z"
"","7306","2.0","""wu_ba-0-C-1"" #18 prio=5 os_prio=0 tid=0x00007f4931e62800 nid=0x1c runnable [0x00007f48e07ce000]    java.lang.Thread.State: RUNNABLE 	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method) 	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269) 	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:93) 	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) 	- locked  (a sun.nio.ch.Util$3) 	- locked  (a java.util.Collections$UnmodifiableSet) 	- locked  (a sun.nio.ch.EPollSelectorImpl) 	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) 	at org.apache.kafka.common.network.Selector.select(Selector.java:691) 	at org.apache.kafka.common.network.Selector.poll(Selector.java:411) 	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:510) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:271) 	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:242) 	at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1247) 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1187) 	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1154) 	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:728) 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:745)  大量这样的状态导致cpu飙升，如何解决？","closed","","hw1992","2019-09-06T03:28:27Z","2019-11-21T17:47:42Z"
"","7477","KAFKA-8984: Improve tagged fields documentation","","closed","","cmccabe","2019-10-09T18:16:53Z","2019-11-09T05:08:05Z"
"","7458","MINOR: Change topic zNode's new reassigning fields names to snake case in order to be consistent with other fields","","closed","","stanislavkozlovski","2019-10-07T16:17:53Z","2019-10-11T17:50:11Z"
"","7375","KAFKA-6883: Add toUpperCase support to sasl.kerberos.principal.to.local rule (KIP-309)","","closed","","omkreddy","2019-09-20T15:05:55Z","2019-09-27T18:44:01Z"
"","7350","MINOR: Update authorizer start-up check to handle end point with ephemeral port","","closed","","omkreddy","2019-09-17T10:18:24Z","2019-09-19T15:13:43Z"
"","7325","KAFKA-8885: The Kafka Protocol should Support Optional Tagged Fields","","closed","","cmccabe","2019-09-11T21:40:07Z","2019-10-09T19:11:24Z"
"","7314","KAFKA-8723: Increase timeout on LeaderElectionCommandTest","","closed","","stanislavkozlovski","2019-09-09T15:44:24Z","2019-11-01T11:30:12Z"
"","7307","Pluggable sslenginebuilder","","closed","","maulin-vasavada","2019-09-06T05:16:35Z","2019-09-06T05:16:52Z"
"","7303","Add SslMetrics for cipher type","","closed","","cmccabe","2019-09-05T23:29:44Z","2019-11-15T18:49:43Z"
"","7245","MINOR: Remove duplicate definition of transactional.id.expiration.ms config","","closed","","stanislavkozlovski","2019-08-23T12:16:20Z","2019-09-17T21:28:23Z"
"","7241","DO NOT MERGE: reset VERSION_PROBING error code in onPartitionsAssigned","","closed","","ableegoldman","2019-08-22T20:20:33Z","2019-08-27T21:57:19Z"
"","7239","KAFKA-8826: Improve logging for SSL ciphers and principals","","closed","","cmccabe","2019-08-22T17:58:59Z","2019-09-09T14:29:19Z"
"","7234","KIP-482 WIP","","closed","","cmccabe","2019-08-21T22:57:59Z","2019-09-11T21:40:31Z"
"","7231","MINOR: Log firstUnstableOffset in debug log line in LogCleanerManager#cleanableOffsets","","open","","stanislavkozlovski","2019-08-21T12:23:21Z","2020-04-23T14:34:43Z"
"","7226","MINOR: remove some duplicated code in ApiKeys","","closed","","cmccabe","2019-08-20T22:30:53Z","2020-12-20T16:45:59Z"
"","7201","KAFKA-8791: RocksDBTimestampedStore should open in regular mode by default","","closed","streams,","mjsax","2019-08-13T01:15:58Z","2019-08-14T01:47:12Z"
"","7171","Threaded purgatory","","open","","cmccabe","2019-08-06T18:47:28Z","2019-08-12T01:01:46Z"
"","7128","KAFKA-8345 (KIP-455): Controller and KafkaApi changes (part 3/4)","","closed","","stanislavkozlovski","2019-07-29T18:47:49Z","2019-09-11T16:07:11Z"
"","7067","MINOR: improve error message for incorrect window-serde initialization","","closed","streams,","mjsax","2019-07-10T17:06:36Z","2019-07-19T01:00:05Z"
"","7065","MINOR: Fix log format which does not match parameters","","closed","","liketic","2019-07-10T10:44:57Z","2020-07-08T00:09:50Z"
"","7059","KAFKA-8644. The Kafka protocol generator should allow null defaults for bytes and array fields","","closed","","cmccabe","2019-07-09T23:40:51Z","2019-07-11T16:55:10Z"
"","7058","MINOR: some small style fixes to RoundRobinPartitioner","","closed","","cmccabe","2019-07-09T20:10:12Z","2019-08-06T19:27:57Z"
"","7013","MINOR: add upgrade text","","closed","","cmccabe","2019-06-28T17:18:33Z","2019-06-28T21:50:38Z"
"","6996","Hot Fix: remove unused footnote from memory-management docs","","closed","","ableegoldman","2019-06-25T00:34:47Z","2019-06-25T17:45:32Z"
"","6995","MINOR: Add compatibility tests for 2.3.0","","closed","","cmccabe","2019-06-24T22:23:20Z","2019-06-28T16:25:08Z"
"","6966","KAFKA-8560. The Kafka protocol generator should support common structures","","closed","","cmccabe","2019-06-18T23:30:58Z","2019-07-28T17:02:35Z"
"","6955","Kip 455 II (for testing)","","closed","","cmccabe","2019-06-17T21:09:26Z","2019-11-16T00:14:04Z"
"","6942","MINOR: fix some warnings in the broker","","closed","","cmccabe","2019-06-15T00:31:14Z","2019-06-16T02:24:04Z"
"","6937","MINOR: WorkerUtils#topicDescriptions must unwrap exceptions properly","","closed","","cmccabe","2019-06-13T22:26:09Z","2019-07-03T23:08:40Z"
"","6892","KAFKA-8448: Fix ""too many kafka.log.Log instances""","","closed","","cmccabe","2019-06-05T20:31:02Z","2019-06-18T19:50:31Z"
"","6881","MINOR: Update docs to say 2.3","","closed","","cmccabe","2019-06-04T22:34:39Z","2019-06-04T22:38:19Z"
"","6880","KAFKA-8384: Leader election command tests","","closed","","jsancio","2019-06-04T21:04:01Z","2019-06-07T15:59:17Z"
"","6869","KAFKA-4893: Fix deletion and moving of topics with long names","","closed","","cmccabe","2019-06-03T16:51:38Z","2019-06-04T21:17:05Z"
"","6857","KAFKA-8383: Integration tests for unclean electLeaders","","closed","","jsancio","2019-06-01T00:18:52Z","2019-06-03T21:10:17Z"
"","6777","MINOR: Run the Java and Scala documentation in Jenkins","","closed","","jsancio","2019-05-20T22:25:51Z","2021-07-14T19:48:57Z"
"","6774","MINOR: Bump version to 2.4.0-SNAPSHOT","","closed","","cmccabe","2019-05-20T16:32:35Z","2019-05-20T19:47:38Z"
"","6756","KAFKA-3522: TopologyTestDriver should only return custom stores via untyped getStateStore() method","","closed","streams,","mjsax","2019-05-17T17:39:46Z","2019-05-18T17:39:33Z"
"","6693","Sync upstream changes","","closed","","lbradstreet","2019-05-07T22:49:04Z","2019-05-07T23:03:02Z"
"","6655","Fix up the metrics","","closed","","cmccabe","2019-04-30T18:49:06Z","2019-04-30T18:57:51Z"
"","6652","KAFKA-8306: Initialize log end offset accurately when start offset is non-zero","","closed","","dhruvilshah3","2019-04-30T04:28:30Z","2019-05-04T21:08:17Z"
"","6628","MINOR: Remove an unnecessary character from broker's startup log","","closed","","sekikn","2019-04-24T15:13:00Z","2019-04-25T15:07:49Z"
"","6621","MINOR: reformat settings.gradle to be more readable","","closed","","cmccabe","2019-04-22T18:02:37Z","2019-05-20T18:52:44Z"
"","6620","MINOR: log which signals are handled on startup","","closed","","cmccabe","2019-04-22T17:14:27Z","2019-05-20T18:52:49Z"
"","6558","MINOR: ConnectionStressWorker: add missing executor shutdown","","closed","","cmccabe","2019-04-10T19:34:56Z","2019-05-20T18:53:15Z"
"","6516","MINOR: WorkerUtils#abort: fix bug in abort logic","","closed","","cmccabe","2019-03-28T18:04:55Z","2019-05-20T18:53:54Z"
"","6510","KAFKA-8168: Add a generated ApiMessageType class","","closed","","cmccabe","2019-03-27T21:35:59Z","2019-05-20T18:53:31Z"
"","6504","MINOR: code cleanup TopologyTestDriverTest","","closed","streams,","mjsax","2019-03-26T22:00:30Z","2019-04-17T00:21:07Z"
"","6503","KAFKA-8158: Add EntityType for Kafka RPC fields","","closed","","cmccabe","2019-03-26T21:06:20Z","2019-05-20T18:51:44Z"
"","6502","KAFKA-8134: `linger.ms` must be a long","","closed","","dhruvilshah3","2019-03-26T05:21:04Z","2019-04-30T16:45:10Z"
"","6501","MINOR: Add 2.2.0 upgrade instructions","","closed","","mjsax","2019-03-25T20:23:13Z","2019-03-26T16:43:44Z"