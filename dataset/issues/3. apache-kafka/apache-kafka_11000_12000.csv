"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","1334","KAFKA-3669: Add secondary constructor for KafkaConfig with a default …","…value for doLog","closed","","markgrover","2016-05-06T18:46:01Z","2016-05-06T20:47:27Z"
"","1333","KAFKA-3669: Add secondary constructor for KafkaConfig with a default …","…value for doLog","closed","","markgrover","2016-05-06T18:40:34Z","2016-05-06T19:22:01Z"
"","895","KAFKA-3221: Pass requesting user information to authorizer to enable …","…user authorization before modifying acls.","closed","","SinghAsDev","2016-02-10T02:46:45Z","2016-02-18T18:39:17Z"
"","763","KAFKA-3091: Broker with an invalid id would not start when its id is …","…updated to a new valid one","closed","","granthenke","2016-01-13T05:40:02Z","2016-01-19T04:34:33Z"
"","1395","KAFKA-3699: Update protocol page on website to explain how KIP-35 sho…","…uld be used","closed","","SinghAsDev","2016-05-17T13:13:45Z","2016-06-14T16:51:07Z"
"","663","KAFKA-2070: Replace Offset{Request,Response} with o.a.k.c requests eq…","…uivalent","closed","","granthenke","2015-12-10T20:22:42Z","2016-02-17T06:07:19Z"
"","896","KAFKA-2508: Replace UpdateMetadata{Request,Response} with o.a.k.c.req…","…uests equivalent","closed","","granthenke","2016-02-10T04:24:24Z","2016-02-17T06:05:50Z"
"","851","KAFKA-3194: Validate security.inter.broker.protocol against the adver…","…tised.listeners protocols","closed","","granthenke","2016-02-02T22:46:33Z","2016-02-17T06:06:21Z"
"","679","KAFKA-2547: Make DynamicConfigManager to use the ZkNodeChangeNotifica…","…tionListener introduced as part of KAFKA-2211","closed","","Parth-Brahmbhatt","2015-12-15T21:17:13Z","2016-02-17T04:35:33Z"
"","611","KAFKA-2926: [MirrorMaker] InternalRebalancer calls wrong method of ex…","…ternal rebalancer","closed","","gwenshap","2015-12-02T02:19:48Z","2015-12-10T21:10:05Z"
"","875","KAFKA-3037: Test number of alive brokers known after single node clus…","…ter startup","closed","","granthenke","2016-02-05T04:53:29Z","2016-02-17T06:05:58Z"
"","515","KAFKA-2822: DescribeConsumerGroup now returns empty list for non-exis…","…tent group, it used to throw IllegalArgumentException","closed","","SinghAsDev","2015-11-12T20:10:35Z","2015-11-12T20:22:37Z"
"","1005","KAFKA-3266: List/Alter Acls - protocol and server side implemen…","…tation","closed","","granthenke","2016-03-03T17:10:11Z","2017-12-22T01:35:44Z"
"","647","KAFKA-2509: Replace LeaderAndIsr{Request,Response} with o.a.k.c reque…","…sts equivalent","closed","","granthenke","2015-12-09T03:15:00Z","2016-01-19T04:35:08Z"
"","993","KAFKA-2944: Replaced the NPE with a nicer error and clean exit and added debug me…","…ssage to assist with figuring this out.","closed","","gwenshap","2016-03-02T03:46:46Z","2016-03-03T02:51:38Z"
"","811","KAFKA-3132: URI scheme in ""listeners"" property should not be case-sen…","…sitive","closed","","granthenke","2016-01-26T05:39:15Z","2016-02-17T06:06:28Z"
"","1023","KAFKA-3343: Use NoTimestamp in GroupMetadataManager when message v0 i…","…s used.","closed","","becketqin","2016-03-07T21:08:18Z","2016-03-09T00:34:31Z"
"","927","KAFKA-2068; KAFKA-2069; Replace OffsetCommit and OffsetFetch Request/…","…Response with o.a.k.c.requests equivalent","closed","","granthenke","2016-02-17T15:29:34Z","2016-03-08T02:34:15Z"
"","1114","KAFKA-3301: CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_DOC is incor…","…rect","closed","","granthenke","2016-03-22T17:42:13Z","2016-03-22T22:01:31Z"
"","1134","KAFKA-3449: Rename filterOut() to filterNot() to achieve better termi…","…nology  Hi all,  This is my first contribution and I hope it will be good.  The PR is related to this issue: https://issues.apache.org/jira/browse/KAFKA-3449  Thanks a lot,  Andrea","closed","","oscerd","2016-03-24T16:25:39Z","2016-03-25T22:02:33Z"
"","1133","KAFKA-3349: Rename filterOut() to filterNot() to achieve better termi…","…nology  Hi all,  This is my first contribution and I hope it will be good.  The PR is related to this issue: https://issues.apache.org/jira/browse/KAFKA-3449  Thanks a lot,  Andrea","closed","","oscerd","2016-03-24T16:22:39Z","2016-03-24T16:24:35Z"
"","687","KAFKA-2422: Allow copycat connector plugins to be aliased to simpler …","…names","closed","","gwenshap","2015-12-17T04:32:40Z","2016-01-05T10:58:24Z"
"","838","KAFKA-3164: Document client and mirrormaker upgrade procedure/require…","…ments","closed","","granthenke","2016-01-29T22:09:43Z","2016-02-17T06:06:24Z"
"","923","KAFKA-3243: Fix Kafka basic ops documentation for Mirror maker, black…","…list is not supported for new consumers","closed","","SinghAsDev","2016-02-16T23:36:25Z","2016-02-26T18:38:19Z"
"","910","KAFKA-3088: (0.9 branch) broker crash on receipt of produce request w…","…ith empty client ID - Adds  NULLABLE_STRING Type to the protocol - Changes client_id in the REQUEST_HEADER to NULLABLE_STRING with a default of """" - Fixes server handling of invalid ApiKey request and other invalid requests  Specifically for 0.9 branch: - Changes legacy 'readFrom' methods to default client id to """" on read","closed","","granthenke","2016-02-13T04:04:31Z","2016-02-17T06:05:50Z"
"","1022","KAFKA-3334: Add a note on potential message loss while using auto top…","…ics creation.","closed","","SinghAsDev","2016-03-07T19:47:18Z","2021-12-19T21:42:51Z"
"","1075","KAFKA-3250: release tarball is unnecessarily large due to duplicate l…","…ibraries  This ensures duplicates are not copied in the distribution without rewriting all of the tar'ing logic. A larger improvement could be made to the packaging code, but that should be tracked by another jira.","closed","","granthenke","2016-03-15T15:54:38Z","2016-03-17T21:15:52Z"
"","529","KAFKA-2838: Allow comma in super users, allow comma in CLI authz prop…","…erties.","closed","","Parth-Brahmbhatt","2015-11-13T23:02:34Z","2015-11-17T18:43:35Z"
"","866","KAFKA-3088: broker crash on receipt of produce request with empty cli…","…ent ID - Adds  NULLABLE_STRING Type to the protocol - Changes client_id in the REQUEST_HEADER to NULLABLE_STRING with a default of """" - Fixes server handling of invalid ApiKey request and other invalid requests","closed","","granthenke","2016-02-04T18:23:39Z","2016-02-17T06:05:50Z"
"","1095","KAFKA-3306: Change metadata response to include required additional fi…","…elds - Adds boolean type to the protocol - Allows protocol arrays to be null (optionally) - Adds support to ask for no topics in the metadata request - Adds new fields to the Metadata response protocol - Adds server code to handle new fields   - Support no-topic metadata requests   - Track controller id in the metadata cache   - Check if a topic is considered internal   - Included rack information if present   - Include all replicas and ISRs, even if node is down - Adds test code to test new functionality independent of the client","closed","","granthenke","2016-03-18T16:45:04Z","2016-04-27T00:03:55Z"
"","545","Kafka-2852:Updating the Authorizer CLI to use a consistent way to specify a list of values for a config options.","…ecify a list of values for a config options.","closed","","Parth-Brahmbhatt","2015-11-17T19:01:48Z","2015-11-18T01:47:17Z"
"","1372","KAFKA-3565: Add documentation to warn user about the potential messag…","…e throughput drop due to the addition of timestamp field.","closed","","becketqin","2016-05-11T22:33:56Z","2016-05-15T16:04:44Z"
"","1006","KAFKA-3328: SimpleAclAuthorizer can lose ACLs with frequent add/remov…","…e calls  Changes the SimpleAclAuthorizer to: - Track and utilize the zookeeper version when updating zookeeper to prevent data loss in the case of stale reads and race conditions - Update local cache when modifying ACLs - Add debug logging","closed","","granthenke","2016-03-03T21:11:34Z","2016-03-22T15:08:22Z"
"","900","KAFKA-3226: Replicas collections should use List instead of Set in or…","…der to maintain order","closed","","granthenke","2016-02-10T19:05:17Z","2016-02-17T06:05:53Z"
"","744","KAFKA-3084: Topic existence checks in topic commands (create, alter, …","…delete)","closed","","granthenke","2016-01-08T20:21:39Z","2016-01-19T04:34:34Z"
"","869","KAFKA-3208: Default security.inter.broker.protocol based on configure…","…d advertised.listeners","closed","","granthenke","2016-02-04T22:12:08Z","2016-02-17T06:05:59Z"
"","1299","MINOR: MetadataCache brokerId is not set on first run with generated …","…broker id  This is because the id passed into the MetadataCache is the value from the config before the real broker id is generated.","closed","","granthenke","2016-04-30T05:53:17Z","2016-05-04T15:45:21Z"
"","1296","KAFKA-3644: Use Boolean protocol type for StopReplicaRequest delete_p…","…artitions","closed","","granthenke","2016-04-29T22:30:05Z","2016-04-30T00:27:28Z"
"","1014","KAFKA-3331: Refactor TopicCommand to make describeTopic testable and …","…add unit tests.","closed","","SinghAsDev","2016-03-04T19:35:29Z","2021-12-19T21:42:57Z"
"","955","KAFKA-3272: Add debugging options to kafka-run-class.sh so we can easily run remote debugging","…able remote debugging to Kafka tools scripts","closed","","christian-posta","2016-02-23T19:40:58Z","2016-02-24T00:37:22Z"
"","640","KAFKA-2507: Replace ControlledShutdown{Request,Response} with o.a.k.c…","….requests equivalent","closed","","granthenke","2015-12-08T18:23:01Z","2015-12-10T19:19:57Z"
"","942","added a few more variables and descriptions to what you can customize…","… with the Vagranfile.local file","closed","","christian-posta","2016-02-21T22:28:04Z","2016-02-22T06:53:20Z"
"","1222","KAFKA-3430: Allow users to set key in KTable.toStream and in KStream. …","… With KStream the method selectKey was added to enable getting a key from values before perfoming aggregation-by-key operations on original streams that have null keys.","closed","","bbejeck","2016-04-15T02:41:29Z","2016-04-17T03:19:23Z"
"","975","KAFKA-3291: DumpLogSegment tool should also provide an option to only…","… verify index sanity.","closed","","Parth-Brahmbhatt","2016-02-26T00:15:08Z","2016-02-29T17:15:20Z"
"","1162","KAFKA-3483: Restructure ducktape tests to simplify running subsets of…","… tests","closed","","granthenke","2016-03-29T18:07:32Z","2016-04-04T03:05:16Z"
"","564","KAFKA-2866: Bump up commons-collections version to 3.2.2 to address a…","… security flaw","closed","","granthenke","2015-11-20T00:24:47Z","2016-02-17T06:07:28Z"
"","876","KAFKA-2589: the default value for the ""rebalance.backoff.ms"" property…","… is not specified correctly","closed","","granthenke","2016-02-05T05:09:37Z","2016-02-17T06:05:57Z"
"","803","KAFKA-3134: Fix missing value.deserializer error during KafkaConsumer…","… initialization","closed","","happymap","2016-01-22T19:25:55Z","2016-01-24T09:56:07Z"
"","1039","Minor: Fix system test broken by change of consumer group tool output…","… format","closed","","gwenshap","2016-03-10T05:59:10Z","2016-03-10T19:12:38Z"
"","856","KAFKA-3189: Kafka server returns UnknownServerException for inherited…","… exceptions","closed","","granthenke","2016-02-03T03:14:59Z","2016-02-17T06:06:02Z"
"","766","KAFKA-2999: Errors enum should be a 1 to 1 mapping of error codes and…","… exceptions","closed","","granthenke","2016-01-13T18:18:13Z","2016-01-19T04:34:33Z"
"","1171","KAFKA-2910: Close Zookeeper clients in unit tests","Zookeeper clients that are not closed after the server is shutdown keep trying to reconnect, reloading JAAS configuration. This impacts subsequent tests which rely on JAAS config to be reset.","closed","","rajinisivaram","2016-04-01T08:45:18Z","2016-04-01T17:53:39Z"
"","1224","KAFKA-3525; getSequenceId should return 1  for first data node creation","ZkUtils.getSequenceId() method is used to generate broker id sequence numbers. During startup, each broker updates the data at /brokers/seqid zk path and returns stat.getVersion as next sequence id.   stat.getVersion returns ""1"" for first data update. So ZkUtils.getSequenceId() should return ""1"" on first data update.","closed","","omkreddy","2016-04-15T12:23:46Z","2018-07-03T15:42:19Z"
"","868","HOTFIX: fix partition ordering in assignment","workround partition ordering not preserved by the consumer group management. @guozhangwang","closed","","ymatsuda","2016-02-04T22:07:00Z","2016-02-17T21:22:48Z"
"","1007","HOTFIX: Include RocksDB dependency in release tarballs","Without this change `./gradlew releaseTarGz` (and its variants) will not include the RocksDB jar, which is required for Kafka Streams, in Kafka's `libs/` folder.  The impact is that any Streams job will fail when it runs against a broker that was installed via a release tarball.  @guozhangwang @junrao : please review.","closed","","miguno","2016-03-03T21:37:01Z","2016-03-03T21:43:29Z"
"","922","KAFKA-3176: Add partition/offset options to the new consumer","With this pull request the new console consumer can be provided with optional --partition and --offset arguments so only messages from a particular partition and starting from a particular offset are consumed.  The following rules are also implemented to avoid invalid combinations of arguments: - If --partition or --offset is provided --new-consumer has to be provided too. - If --partition is provided --topic has to be provided too. - If --offset is provided --partition has to be provided too. - --offset and --from-beginning cannot be used at the same time.  This patch is co-authored with @rajinisivaram.","closed","","vahidhashemian","2016-02-16T14:29:14Z","2016-06-21T06:54:02Z"
"","1418","KAFKA-3747; Close `RecordBatch.records` when append to batch fails","With this change, `test_producer_throughput` with message_size=10000, compression_type=snappy and a snappy buffer size of 32k can be executed in a heap of 192m in a local environment (768m is needed without this change).","closed","","ijuma","2016-05-23T16:54:59Z","2016-05-24T08:16:12Z"
"","1293","HOTFIX: wrong keyvalue equals logic when keys not equal but values equal","With the previous logic, if key does NOT equal, but value DOES equal, then equals returns TRUE.","closed","","enothereska","2016-04-29T18:53:01Z","2016-04-30T07:04:22Z"
"","956","Kafka 3270 Added some Happy Path Tests for the Reassign Partitions Command","with help from @enothereska  :)","closed","","benstopford","2016-02-23T21:29:10Z","2016-04-26T18:47:35Z"
"","1210","MINOR: fix incorrect exception message","While playing with client got the next exception:  ``` java java.lang.IllegalArgumentException: Invalid partition given with record: 1 is not in the range [0...1]. ```  It's obviously incorrect, so I've fixed it.","closed","","stepio","2016-04-09T07:10:57Z","2016-04-11T19:03:33Z"
"","994","MINOR: Fixed error in test by moving commit validation","While looking at the failure here: https://builds.apache.org/job/kafka-trunk-jdk7/ws/connect/runtime/build/reports/tests/classes/org.apache.kafka.connect.runtime.WorkerSourceTaskTest.html  I noticed some stray errors of ""Unexpected method call SourceTask.commit()"", so I moved the location where we expect commits to inside the expectedFlush method since this is where the commit happens (on successful flush).","closed","","gwenshap","2016-03-02T06:14:44Z","2016-03-03T01:22:53Z"
"","1314","KAFKA-3651 : Remove the condition variable waiting on memory availability in Bufferpool when a TimeoutException is thrown","Whenever the BufferPool throws a ""Failed to allocate memory within the configured max blocking time"" exception, it should also remove the condition object from the waiters deque","closed","","MayureshGharat","2016-05-04T00:09:33Z","2016-05-05T22:51:26Z"
"","1363","KAFKA-3664: Commit offset of unsubscribed partitions of the new consumer on a subscription change","When users are using group management, if they call consumer.subscribe() or consumer.unsubscribe() to change the subscription, the removed subscriptions will be immediately removed and their offset will not be committed.  This pull request fixes this issue by performing a commitAsync() in subscribe() and unsubscribe() methods to trigger an offset commit.  Special thanks to @hachikuji and @becketqin for helping out and sharing their feedback along the way.","closed","","vahidhashemian","2016-05-10T21:28:09Z","2016-09-09T05:09:01Z"
"","698","KAFKA-1860 File system errors are not detected unless Kafka tries to write","When the disk (raid with caches dir) dies on a Kafka broker, typically the filesystem gets mounted into read-only mode, and hence when Kafka tries to read the disk, they'll get a FileNotFoundException with the read-only errno set (EROFS). However, as long as there is no produce request received, hence no writes attempted on the disks, Kafka will not exit on such FATAL error and keep on throwing exception : java.io.FileNotFoundException  In this case, the JVM should stop if the underlying file system goes in to Read only mode.","closed","","MayureshGharat","2015-12-18T23:22:15Z","2016-02-01T22:42:37Z"
"","953","In daemon mode,redirect stdout to /dev/null","when start in deamon mode,don't output console to file,because the console output file will keep increased. so redirect stdout to /dev/null.  I also wonder the  meaning of ""< /dev/null"" in the old file,if someone can answer.","closed","","tomjiang1987","2016-02-23T09:32:30Z","2020-08-20T15:46:49Z"
"","783","KAFKA-3106: Fix updating an existing connector config from REST API, GET a connector config will fail","When query for connector and task config info by REST API, method of ""checkConfigSynced"" will check if herder's config is in sync with the current config;  But the configState's offset may be bigger than the assignment's offset after updating an existing connector, because the connector can pick up the config change without a rebalance;","closed","","jinxing64","2016-01-17T06:38:46Z","2017-01-12T02:05:02Z"
"","609","KAFKA-2851 Using random dir under /temp for local kdc files to avoid conflicts.","when multiple test jobs are running.  I manually separated changes for KAFKA-2851 from this PR:  https://github.com/apache/kafka/pull/570 which also had KAFKA-2825 changes.","closed","","apovzner","2015-12-02T00:09:13Z","2015-12-02T23:06:50Z"
"","1034","MINOR: Update gradlew.bat as per latest gradle release","When invoking `gradle` on a recent version, it updates `gradlew.bat` to fix a typo. It's an annoyance at development time as it causes a diff on whatever branch one is working on.","closed","","ijuma","2016-03-09T10:27:02Z","2016-03-09T17:25:01Z"
"","1305","KAFKA-3448: Support zone index in IPv6 regex","When an address is written textually, the zone index is appended to the address, separated by a percent sign (%). The actual syntax of zone indices depends on the operating system.","closed","","soumyajit-sahu","2016-05-03T01:28:04Z","2016-05-03T18:24:50Z"
"","682","Kafka-2992: Guard trace statements in the inner loop of the replica fetcher","We're seeing some GC pause issues in production, and during our investigation found that the thunks created during invocation of three trace statements guarded in this PR were responsible for ~98% of all allocations by object count and ~90% by size. While I'm not sure that this was actually the cause of our issue, it seems prudent to avoid useless allocations in a tight loop.  I realize that the trace() call does its own guarding internally, however it's insufficient to prevent allocation of the thunk.  This is my original work, and I license it to the Kafka project under the project's Apache license.","closed","","hkolbeck","2015-12-15T23:52:00Z","2015-12-16T01:10:28Z"
"","1357","MINOR: Use `Record` instead of `ByteBufferMessageSet` in `ProduceRequestTest`","We want to phase out `ByteBufferMessageSet` eventually, so new code should favour `Record` where possible.  Also use a fixed timestamp in `testCorruptLz4ProduceRequest` to ensure that the checksum is always the same.","closed","","ijuma","2016-05-09T23:40:30Z","2016-05-18T18:01:54Z"
"","845","MINOR: Reorder StreamThread shutdown sequence","We need to close producer first before closing tasks to make sure all messages are acked and hence checkpoint offsets are updated before closing tasks and their state. It was re-ordered mistakenly before.","closed","","guozhangwang","2016-02-01T21:46:53Z","2016-02-02T01:33:09Z"
"","1359","MINOR: Double timeout passed to `producer.close` in `sendAndVerifyTimestamp`","We have had transient failures in this method when Jenkins is overloaded.","closed","","ijuma","2016-05-10T15:18:20Z","2016-05-11T00:31:12Z"
"","621","KAFKA-2940: Make available to use any Java options at startup scripts","We cannot specify any Java options (e.g. option for remote debugging) at startup scrips such as kafka-server-start.sh . This ticket makes we can specify to use ""JAVA_OPTS"" environmental variables.","closed","","sasakitoa","2015-12-03T02:43:59Z","2015-12-17T23:58:03Z"
"","574","MINOR: Introduce `installAll` and accept major as well as full Scala version","We can take advantage of the fact that major Scala versions are binary compatible (since 2.10) to make the build a little more user-friendly.","closed","","ijuma","2015-11-23T13:14:55Z","2016-03-01T22:48:28Z"
"","684","MINOR: Change return type of `Schema.read` to be `Struct` instead of `Object`","We always return a `Struct` from `Schema.read` and this means that we can remove a large number of casts.","closed","","ijuma","2015-12-16T15:31:46Z","2016-03-01T22:54:00Z"
"","603","MINOR: Update ""Java Version"" section","Use the latest information from LinkedIn and mention that the latest released version is recommended from a security perspective.","closed","","ijuma","2015-11-30T13:08:21Z","2016-03-01T22:48:23Z"
"","546","KAFKA-2850: Fix SSL invalid endpoint validation test","Use invalid hostname to ensure that test works in all environments","closed","","rajinisivaram","2015-11-17T21:42:39Z","2016-01-06T23:01:40Z"
"","738","KAFKA-3070: Use IBM Kerberos module for SASL tests if running on IBM JDK","Use IBM Kerberos module and properties for SASL tests if using IBM JRE","closed","","rajinisivaram","2016-01-06T20:48:40Z","2017-04-19T11:49:03Z"
"","583","KAFKA-2718: Prevent temp directory being reused in parallel test runs","Use Files.createTempDirectory to avoid reuse, for log directories create a new temp directory as parent","closed","","rajinisivaram","2015-11-24T20:46:50Z","2015-11-25T22:21:09Z"
"","533","KAFKA-2844; Separate keytabs for sasl tests","Use a different keytab for server and client in SASL tests  Also: - Improve approach used to build the JAAS files programmatically - Delete stale `kafka_jaas.conf` file - Move `FourLetterWords` to its own file, add `Zk` prefix and clean-up its usage","closed","","ijuma","2015-11-16T18:55:30Z","2016-04-01T22:26:11Z"
"","750","KAFKA-3038: use async ZK calls to speed up leader reassignment","Updated failure code path to deal specifically with issue identified at affecting latency most.  @fpj could you have a look please?","closed","","enothereska","2016-01-09T20:13:49Z","2017-04-09T12:13:54Z"
"","765","KAFKA-3096; Leader is not set to -1 when it is shutdown if followers are down","Update leader to -1 before throwing `NoReplicaOnlineException` in `OfflinePartitionLeaderSelector` as suggested by Guozhang. This fixes the test, which is great, but it seems a bit out of place. Would it be better to change the code somewhere else?  Fix bug in `waitUntilLeaderIsElectedOrChanged` and simplify result type. The bug was for the following case:  ``` scala leader.isDefined && oldLeaderOpt.isEmpty && newLeaderOpt.isDefined && newLeaderOpt.get != leader.get ```  We would consider it a successful election even though we should not. I also changed the result type as we never return `None` (we throw an exception instead). Fixing this bug is what uncovered the leader issue being solved in this PR.  Also included: - Various mechanical clean-ups found while trying to understand the code (as usual, this is in separate commits, so I can submit them separately if desired). - Make logging more regular in `PartitionLeaderSelector.scala`","closed","","ijuma","2016-01-13T16:21:27Z","2019-05-21T14:15:42Z"
"","1238","KAFKA-3586: Revert to default quota when client quota override is deleted","Update client quota when override is deleted from config","closed","","rajinisivaram","2016-04-19T15:29:16Z","2017-01-16T10:28:02Z"
"","1273","KAFKA-3617: Unit tests for SASL authenticator","Unit tests for SASL authenticator, tests for SASL/PLAIN and multiple mechanisms, authorization test for SASL/PLAIN","closed","","rajinisivaram","2016-04-27T11:12:25Z","2016-04-28T20:39:59Z"
"","1268","KAFKA-3614: Consolidate duplicate code in KGroupedTableImpl","Two methods aggregate() and reduce() share common code that is consolidated in this patch.","closed","","vahidhashemian","2016-04-25T21:59:36Z","2016-05-02T16:09:54Z"
"","1458","MINOR: close parenthesis in PartitionInfo.toString","Trivial patch is trivial.","closed","","reftel","2016-06-01T21:53:57Z","2016-06-01T23:50:23Z"
"","612","modify config specification of topic level","topic level config delete config options  use --delete-config instead of --deleteConfig","closed","","EamonZhang","2015-12-02T03:07:51Z","2016-01-13T23:01:40Z"
"","1430","MINOR: Use `--force` instead of `--yes` in `AclCommand`","To be consistent with `ConfigCommand` and `TopicCommand`.  No release includes this option yet, so we can simply change it.","closed","","ijuma","2016-05-25T20:57:51Z","2016-05-26T16:59:26Z"
"","1264","KAFKA-3616: Make kafka producers/consumers injectable for KafkaStreams","Ticket: https://issues.apache.org/jira/browse/KAFKA-3616","closed","","kawamuray","2016-04-24T15:13:00Z","2016-05-06T15:50:18Z"
"","1146","KAFKA-3471: min.insync.replicas isn't respected when there's a delaying follower who still in ISR","Ticket: https://issues.apache.org/jira/browse/KAFKA-3471  The number of followers which are already caught up until requiredOffset should be used instead of high watermark to consider whether there are enough number of replicas for a produce request. Please see the ticket for the detail.","closed","","kawamuray","2016-03-26T18:03:50Z","2016-03-31T03:29:56Z"
"","980","KAFKA-3201: Added rolling upgrade system tests from 0.8 and 0.9 to 0.10","Three main tests: 1. Setup: Producer (0.8) → Kafka Cluster → Consumer (0.8) First rolling bounce: Set inter.broker.protocol.version = 0.8 and message.format.version = 0.8 Second rolling bonus, use latest (default) inter.broker.protocol.version and message.format.version 2. Setup: Producer (0.9) → Kafka Cluster → Consumer (0.9) First rolling bounce: Set inter.broker.protocol.version = 0.9 and message.format.version = 0.9 Second rolling bonus, use latest (default) inter.broker.protocol.version and message.format.version 3. Setup: Producer (0.9) → Kafka Cluster → Consumer (0.9) First rolling bounce: Set inter.broker.protocol.version = 0.9 and message.format.version = 0.9 Second rolling bonus: use inter.broker.protocol.version = 0.10 and message.format.version = 0.9  Plus couple of variations of these tests using old/new consumer and no compression / snappy compression.","closed","","apovzner","2016-02-26T23:26:02Z","2016-03-08T07:18:59Z"
"","1084","KAFKA-3403: Upgrade ZkClient to 0.8","This ZkClient version adds authentication validation and a conditional delete method needed for other patches","closed","","granthenke","2016-03-16T21:54:32Z","2016-03-16T22:08:02Z"
"","1107","MINOR: fix documentation version","This will need to be double-committed.","closed","","gwenshap","2016-03-21T20:12:04Z","2016-03-21T20:37:50Z"
"","1152","MINOR: Fix typo and tweak wording in `RecordAccumulator` comments","This was recently introduced in: https://github.com/apache/kafka/commit/1fbe445dde71df0023a978c5e54dd229d3d23e1b","closed","","ijuma","2016-03-28T07:47:39Z","2016-03-28T16:01:05Z"
"","1388","MINOR: Inline `Record.write` into `Compressor.putRecord`","This simplifies the call path making the code easier to understand. Previously, `Record.write(ByteBuffer...)` called `Compressor.putRecord` which after a few overloaded calls, would end up in`Record.write(Compressor...)`.","closed","","ijuma","2016-05-15T15:20:08Z","2016-05-16T19:04:25Z"
"","1178","KAFKA-725: Return OffsetOutOfRange error from ReplicaManager when non-follower attempts reading an offset that's above high watermark.","This should make Log.read act the same when startOffset is larger than maxOffset as it would if startOffset was larger than logEndOffset. The current behavior can result in an IllegalArgumentException from LogSegment if a consumer attempts to fetch an offset above the high watermark which is present in the leader's log. It seems more correct if Log.read presents the view of the log to consumers as if it simply ended at maxOffset (high watermark).  I've tried to describe an example scenario of this happening here https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673  I'm not sure I understand why ReplicaManager sets maxOffset to the high watermark, and not high watermark + 1. Isn't the high watermark the last committed message, and readable by consumers?  Tests passed for me locally on second try, seems like it just hit a flaky test.","closed","","srdo","2016-04-02T14:56:09Z","2017-05-26T11:28:22Z"
"","1079","KAFKA-3407 - ErrorLoggingCallback trims helpful diagnostic information.","This should help when diagnosing issues with the console producer. This allows the logger to use `exception` rather than `exception.getMessage()`.","closed","","jcustenborder","2016-03-16T02:38:19Z","2016-03-24T18:04:14Z"
"","1192","MINOR: ensure original use of prop_file in verifiable producer","This PR: https://github.com/apache/kafka/pull/958 fixed the use of prop_file in the situation when we have multiple producers (before, every producer will add to the config). However, it assumes that self.prop_file is initially """". This is correct for all existing tests, but it precludes us from extending verifiable producer and adding more properties to the producer config (same as console consumer).   This is a small PR to change the behavior to the original, but also make verifiable producer use prop_file method to be consistent with console consumer.  Also few more fixes to verifiable producer came up during the review: -- fixed each_produced_at_least() method -- more straightforward use of compression types  @granders please review.","closed","","apovzner","2016-04-06T00:53:17Z","2016-04-08T01:18:18Z"
"","1390","Add a newline to a Javadoc sample in KafkaConsumer","This PR simply adds a newline to a Javadoc sample in `KafkaConsumer` to flush the outputs.","closed","","izeye","2016-05-16T02:26:50Z","2016-06-02T04:51:34Z"
"","1434","KAFKA-2394: move to RollingFileAppender by default for log4j","This PR sets up the default max log size to be about 1GB.  That default should probably be vetted.","closed","","cotedm","2016-05-26T14:02:23Z","2016-07-22T13:43:24Z"
"","1336","KAFKA-3144: Report members with no assigned partitions in ConsumerGroupCommand","This PR makes a couple of enhancements to the `--describe` option of `ConsumerGroupCommand`: 1. Listing members with no assigned partitions. 2. Showing the member id along with the owner of each partition (owner is supposed to be the logical application id and all members in the same group are supposed to set the same owner). 3. Printing a warning indicating whether ZooKeeper based or new consumer API based information is being reported.  It also adds unit tests to verify the added functionality.  Note: The third request on the corresponding JIRA (listing active offsets for empty groups of new consumers) is not implemented as part of this PR, and has been moved to its own JIRA (KAFKA-3853).","closed","","vahidhashemian","2016-05-06T22:26:24Z","2016-10-22T02:35:48Z"
"","1382","MINOR: Allow Serdes subclasses to access WrapperSerde","This PR loosens access restrictions on `WrapperSerde` to `protected` so that users can define a `Serdes` subclass that provides additional custom serde members following the same pattern as the parent class.  This is my own work and is compatible with Kafka's license.","closed","","jklukas","2016-05-13T17:13:30Z","2016-05-14T03:03:53Z"
"","1367","KAFKA-3584; Fix synchronization issue between deleteOldSegments() and delete() methods","This PR is to fix synchronization issue between deleteOldSegments() and delete() method calls. log.deleteOldSegments() call throws NullPointerException after log.delete() method call.   cc @ijuma @junrao","closed","","omkreddy","2016-05-11T09:29:27Z","2018-07-03T15:42:20Z"
"","1477","KAFKA-3443 [Kafka Stream] support for adding sources to KafkaStreams via Pattern","This PR is the follow on to the closed PR #1410.","closed","","bbejeck","2016-06-07T03:25:09Z","2016-06-16T02:21:22Z"
"","1277","KAFKA-3613: Consolidate TumblingWindows and HoppingWindows into TimeWindows","This PR includes the same code as https://github.com/apache/kafka/pull/1261 but is rebased on latest trunk.","closed","","miguno","2016-04-27T21:34:17Z","2016-04-29T14:44:37Z"
"","943","KAFKA-3259 KAFKA-3253; KIP-31/KIP-32 Follow-up","This PR includes a number of clean-ups: - Code style - Documentation wording improvements - Efficiency improvements","closed","","ijuma","2016-02-22T09:13:09Z","2016-03-01T22:52:42Z"
"","1020","KAFKA-2273: Sticky partition assignment strategy (KIP-54)","This PR implements a new partition assignment strategy called ""sticky"", and it's purpose is to balance partitions across consumers in a way that minimizes moving partitions around, or, in other words, preserves existing partition assignments as much as possible.  This patch is co-authored with @rajinisivaram and @edoardocomar.","closed","","vahidhashemian","2016-03-07T18:59:20Z","2017-05-18T03:18:11Z"
"","1138","KAFKA-3461: Fix typos in Kafka web documentations.","This PR fixes 8 typos in HTML files of `docs` module. I wrote explicitly here since Github sometimes does not highlight the corrections on long lines correctly. - docs/api.html: compatability => compatibility - docs/connect.html: simultaneoulsy => simultaneously - docs/implementation.html: LATIEST_TIME => LATEST_TIME, nPartions => nPartitions - docs/migration.html: Decomission => Decommission - docs/ops.html: stoping => stopping, ConumserGroupCommand => ConsumerGroupCommand, youre => you're","closed","","dongjoon-hyun","2016-03-25T00:31:42Z","2016-04-12T20:50:41Z"
"","1179","KAFKA-3496 - add policies for client reconnect attempts","This PR enables to configure a policy for the client reconnection attempts. It provides this following policies :  - ConstantReconnectAttemptPolicy (default policy which uses the reconnect.backoff.ms property) - ExponentialReconnectAttemptPolicy  Example :   ``` java Properties config = new Properties(); config.put(ConsumerConfig.RECONNECT_ATTEMPTS_POLICY_CLASS_CONFIG, ""org.apache.kafka.clients.ExponentialReconnectAttemptPolicy""); config.put(ConsumerConfig.RECONNECT_EXPONENTIAL_MAX_DELAY_MS_CONFIG, 5000); config.put(ConsumerConfig.RECONNECT_EXPONENTIAL_BASE_DELAY_MS_CONFIG, 50); ```  This implementation is inspired from the cassandra client driver.  In addition, this PR fixes some log level as describe in this issue : https://issues.apache.org/jira/browse/KAFKA-2998","closed","","fhussonnois","2016-04-02T16:03:31Z","2017-12-22T01:38:34Z"
"","857","KAFKA-3197 Fix producer sending records out of order","This patch reuse max.in.flight.request.per.connection. When it equals to one, we take it as user wants order protection. The current approach is make sure there is only one batch per partition on the fly.","closed","","becketqin","2016-02-03T09:13:17Z","2016-03-08T22:41:53Z"
"","1215","KAFKA-3163: Add time based index to Kafka.","This patch is for KIP-33. https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index","closed","","becketqin","2016-04-11T22:59:35Z","2016-08-22T21:31:19Z"
"","509","MINOR: Fix building from subproject directory","This patch fixes some releative paths so bulding from a subproject directory like ($rootDir/core) will not fail","closed","","granthenke","2015-11-12T16:16:45Z","2016-01-19T04:36:04Z"
"","1240","MINOR: Fix typos in code comments","This patch fixes all occurances of two consecutive 'the's in the code comments.  Author: Ishita Mandhan (imandha@us.ibm.com)","closed","","imandhan","2016-04-19T23:51:20Z","2016-04-20T00:41:28Z"
"","1429","KAFKA-3158: ConsumerGroupCommand should tell whether group is actually dead","This patch fix differentiates between when a consumer group is rebalancing or dead and reports the appropriate error message.","closed","","imandhan","2016-05-25T19:21:51Z","2016-05-28T23:08:02Z"
"","1149","KAFKA-3436: Speed up controlled shutdown.","This patch does the followings: 1. Batched LeaderAndIsrRequest and UpdateMetadataRequest during controlled shutdown. 2. Added async read and write method to an extending ZkClient. Used the async zk operation for LeaderAndIsr read and update. The async method can be used in other places as well (e.g. preferred leader election, replica reassignment, controller bootstrap, etc), but those are out of the scope of this ticket.  Conducted some rolling boucne test, a controlled shutdown involving 2500 partitions takes around 3 seconds now. Previously it can takes more than 30 seconds.","closed","","becketqin","2016-03-28T01:38:30Z","2017-12-22T01:38:06Z"
"","1245","KAFKA-3592: System test - configurable paths","This patch adds logic for the following: - remove hard-coded paths to various scripts and jars in kafkatest service classes - provide a mechanism for overriding path resolution logic with a ""pluggable"" path resolver class","closed","","granders","2016-04-20T20:39:34Z","2016-05-06T18:11:02Z"
"","690","KAFKA-3006: Make collection default container type for sequences in the consumer API","This makes it much easier when using the library from some JVM languages. In clojure for instance you can go from  ``` clojure (.pause consumer (into-array TopicPartition partitions)) ```  to simply:  ``` clojure (.pause consumer partitions) ```","closed","","pyr","2015-12-17T15:54:26Z","2016-04-26T19:19:36Z"
"","544","MINOR: Introduce `producer.config` property to `ConsoleProducer`","This makes it easier to pass security properties in the same way to `ConsoleConsumer` and `ConsoleProducer`.","closed","","ijuma","2015-11-17T18:55:23Z","2016-03-01T22:48:39Z"
"","551","KAFKA-2858; Introduce `SimplePrincipal` and use it in the authentication layer","This makes it clear that we only support a principal name at the authentication layer (principalType is only used at the authorization layer).","closed","","ijuma","2015-11-18T17:57:57Z","2017-11-10T09:45:01Z"
"","951","KAFKA-3196: Added checksum and size to RecordMetadata and ConsumerRecord","This is the second (remaining) part of KIP-42. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-42%3A+Add+Producer+and+Consumer+Interceptors","closed","","apovzner","2016-02-22T23:54:25Z","2016-03-02T17:41:01Z"
"","854","KAFKA-3162: Added producer and consumer interceptors","This is the most of the KIP-42: Producer and consumer interceptor. (Except exposing CRC and record sizes to the interceptor, which is coming as a separate PR; tracked by KAFKA-3196).  This PR includes: 1. Add ProducerInterceptor interface and call its callbacks from appropriate places in Kafka Producer. 2. Add ConsumerInterceptor interface and call its callbacks from appropriate places in Kafka Consumer. 3. Add unit tests for interceptor changes 4. Add integration test for both mutable consumer and producer interceptors.","closed","","apovzner","2016-02-03T01:04:19Z","2016-02-10T05:10:42Z"
"","937","MINOR: Update streams RocksDb to 4.1.0","This is the latest version in Maven even though HISTORY.md includes releases all the way to 4.5.0.","closed","","ijuma","2016-02-19T10:56:14Z","2016-03-01T22:52:50Z"
"","1187","KAFKA-3338 [Kafka Streams] : Add print and writeAsText to KStream and KTable interfaces","This is the first pass at implementing the print and writeAsText functionality for KStream and KTable.  I have some uncertainty on how I implemented the use of Serdes.  This contribution is my original work and I license this contribution to Apache-Kafka.","closed","","bbejeck","2016-04-05T03:04:23Z","2016-04-09T01:05:21Z"
"","1391","KAFKA-3704; Revert ""Remove hard-coded block size in KafkaProducer""","This is not an exact revert as the code changed a bit since the original commit. We also include a note in `upgrade.html`.  The original commit is 1182d61deb23b5cd86cbe462471f7df583a796e1.","closed","","ijuma","2016-05-16T15:54:32Z","2016-05-16T19:01:08Z"
"","968","KAFKA-3280: KafkaConsumer Javadoc contains misleading description of heartbeat behavior and correct use","This is my original work and I license the work to the project under the project's open source license.","closed","","rwhaling","2016-02-25T02:17:52Z","2016-02-26T02:25:16Z"
"","739","MINOR: Add property to configure showing of standard streams in Gradle","This is handy when debugging certain kinds of Jenkins failures.","closed","","ijuma","2016-01-07T10:37:51Z","2016-03-01T22:54:38Z"
"","1143","MINOR: change initial value of Min stat to Double.MAX_VALUE (not MIN)","this is consistent with the Max stat implementation.","closed","","zackdever","2016-03-25T22:03:12Z","2016-04-29T22:50:20Z"
"","1196","KAFKA-1614: Partition log directory name and segments information exposed via JMX","This is an updated version of https://issues.apache.org/jira/browse/KAFKA-1614 patch which was made for 0.8.1.1 version.   It makes partition log directory and single segments information exposed via JMX. This is useful to: - monitor disks usage in a cluster and on single broker; - calculate disk space taken by different topics; - estimate space to be freed when segments are expired.  Yahoo's Kafka Manager uses it to show information about topics and partitions sizes:  ![topics](https://cloud.githubusercontent.com/assets/1021271/14336342/aa3a197a-fc17-11e5-9ae8-7a4a8bf32562.png) ![partitions](https://cloud.githubusercontent.com/assets/1021271/14336348/b38e2354-fc17-11e5-930d-9b36c7aae1e8.png)  Our internal tool uses it to show how segments sizes were changing over time:  ![segments](https://cloud.githubusercontent.com/assets/1021271/14336349/b566ed8c-fc17-11e5-811f-d9bca5e2e099.png)  @junrao could you please review when you have time?","open","","xdralex","2016-04-06T23:55:10Z","2022-06-14T12:21:06Z"
"","1377","MINOR: Change type of StreamsConfig.BOOTSTRAP_SERVERS_CONFIG to List","This is an improved version of https://github.com/apache/kafka/pull/1374, where we include a unit test.","closed","","miguno","2016-05-12T10:04:40Z","2016-05-12T14:58:15Z"
"","1291","WIP - KAFKA-1880: Add support for checking binary/source compatibility","This is a WIP pull request to show how I am generating the reports attached to the Jira. I am putting it up now so that we understand what has been changed/broken before the 0.10 release.   At some point we may want to leverage something like this to break the build too, but I think generating a report is a good start.","open","","granthenke","2016-04-29T18:05:56Z","2022-06-14T12:52:32Z"
"","1202","KAFKA-3480: Autogenerate metrics documentation","This is a proof of concept for autogenerating documentation for metrics, to see whether this approach is feasible.  I applied it to the metrics in the TestMetrics.java file. I will attach the generated file to the JIRA, for reference.","closed","","wushujames","2016-04-08T01:46:17Z","2017-05-08T04:45:49Z"
"","557","MINOR: Use /usr/bin/env to find bash","This is a minor change to the shell scripts in the bin directory that use bash which switches them to find bash via /usr/bin/env which should provide more cross platform compatibility.  Specifically it will allow these scripts to work out of the box on the various BSDs with bash is installed in the default manner, in /usr/local/bin instead of /bin and should remain compatible with other OSes that typically put most things in /bin","closed","","dwimsey","2015-11-19T02:30:03Z","2016-12-26T22:38:01Z"
"","1015","KAFKA-3303: Pass partial record metadata to ProducerInterceptor.onAcknowledgement on error","This is a KIP-42 followup.   Currently, If sending the record fails before it gets to the server, ProducerInterceptor.onAcknowledgement() is called with metadata == null, and non-null exception. However, it is useful to pass topic and partition, if known, to ProducerInterceptor.onAcknowledgement() as well. This patch ensures that  ProducerInterceptor.onAcknowledgement()  gets record metadata with topic and maybe partition. If partition is not set in 'record' and KafkaProducer.send() fails before partition gets assigned, then ProducerInterceptor.onAcknowledgement() gets RecordMetadata with partition == -1. Only time when  ProducerInterceptor.onAcknowledgement() gets null record metadata is when the client passes null record to KafkaProducer.send().","closed","","apovzner","2016-03-05T01:11:19Z","2016-03-17T00:29:54Z"
"","520","KAFKA-2824: MiniKDC based tests don't run in VirtualBox","This is a hack which works. Is there a better way?  Build (v2) of the replication_test.py running here: http://jenkins.confluent.io/job/kafka_system_tests_branch_builder/185/","closed","","benstopford","2015-11-13T00:00:18Z","2015-11-19T23:59:17Z"
"","607","MINOR: Update LinkedIn JVM tuning settings","This is a follow-up as the previous update was missing some changes.","closed","","ijuma","2015-12-01T10:41:52Z","2016-03-01T22:48:24Z"
"","1366","KAFKA-3421: Connect developer guide update and several fixes","This is a follow up of KAKFA-3421 to update the connect developer guide to include the configuration validation. Also includes a couple of minor fixes.","closed","","Ishiihara","2016-05-11T07:10:36Z","2016-05-13T01:16:00Z"
"","918","Add expected Error Codes to ProduceResponse documentation","This is a documentation-only patch discussed on the mailing list. The intent is to have these changes propagated to the protocol wiki (https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol) .","closed","","dpkp","2016-02-15T19:55:34Z","2017-01-24T12:45:44Z"
"","1094","KAFKA-3378; Client blocks forever if SocketChannel connects instantly","This is a different implementation to the one in #1085 by Larkin Lowrey (@llowrey). The hard part here was actually finding the problem and all credit goes to @llowrey.  This PR also fixes our handling of `finishConnect` (we now check the return value).","closed","","ijuma","2016-03-18T14:41:30Z","2016-03-18T23:14:03Z"
"","820","KAFKA-3152; kafka-acl doesn't allow space in principal name (backport to 0.9.0)","This is a backport of the trunk PR and it excludes the test changes due to conflicts and the fact that the changes were not directly related to the bug in the end (it was something we did not test for, but the non-shell code was already correct).  Details: - Add quotes to `$` in shell scripts   This is necessary for correct processing of quotes in the   user command. - Minor improvements to AclCommand messages","closed","","ijuma","2016-01-27T19:25:30Z","2016-03-01T22:53:32Z"
"","1251","KAFKA-3600: Enhance java clients to use ApiVersion Req/Resp to check if the broker they are talking to supports required api versions","This includes changes from https://github.com/apache/kafka/pull/986. To ease reviewing and as requested by release manager, @gwenshap , these are separate PRs.","closed","","SinghAsDev","2016-04-21T21:13:15Z","2016-12-17T12:27:35Z"
"","824","KAFKA-3161: Fixed ProducerConfig/ConsumerConfig so that defaults are used in java.util.Properties","This impacts the ProducerConfig and ConsumerConfig.  I added a unit test to reflect the new case.  While running unit tests I found that the order for assertEquals(, ) were backwards - I fixed this.  This is my original work to be licensed to the kafka product (Instructions 5d).","closed","","crhyne","2016-01-28T16:11:00Z","2022-02-09T19:33:39Z"
"","1385","KAFKA-3713; Close `compressor` to fix memory leak","This fixes test_producer_throughput with compression_type=snappy.  Also: added heap dump on out of memory error to `producer_performance.py` and corrected the upgrade note related to the change in buffer size for compression streams.","closed","","ijuma","2016-05-14T03:46:40Z","2016-05-14T10:44:36Z"
"","708","MINOR: Use ""new line"" delimeters in a portable way in a unit test","This fixes failure of the FileStreamSinkTaskTest unit test on Windows  Signed-off-by: glikson glikson@il.ibm.com","closed","","glikson","2015-12-22T09:44:52Z","2015-12-23T01:07:47Z"
"","898","KAFKA-3219: Fix long topic name validation","This fixes an issue with long topic names by considering, during topic validation, the '-' and the partition id that is appended to the log folder created for each topic partition.","closed","","vahidhashemian","2016-02-10T16:41:53Z","2016-03-22T20:10:33Z"
"","728","MINOR: Add secondary constructor that takes only `props` in `KafkaConfig`","This fixes a compatibility break for users that use `new KafkaConfig(map)` instead of `KafkaConfig(map)`.","closed","","ijuma","2016-01-04T20:11:24Z","2016-03-01T22:54:10Z"
"","1029","KAFKA-3047: Explicit offset assignment in Log.append can corrupt the log","This fix was suggested by Maciek Makowski, who also reported the problem.","closed","","ijuma","2016-03-08T08:12:03Z","2016-03-12T01:57:29Z"
"","1186","Guards against null-pointer during schema validation","This edit fixes a null-pointer exception which we encountered during testing. Truncated stack trace:  ``` [2016-04-04 18:27:03,667] ERROR CRITICAL: Failed to serialize offset data, making it impossible to commit offsets under namespace test-fileconnector-source. This likely won't recover unless the unserializable partition or offset information is overwritten. (org.apache.kafka.connect.storage.OffsetStorageWriter:152) [2016-04-04 18:27:03,667] ERROR Cause of serialization failure: (org.apache.kafka.connect.storage.OffsetStorageWriter:155) java.lang.NullPointerException     at org.apache.kafka.connect.storage.OffsetUtils.validateFormat(OffsetUtils.java:50)     at org.apache.kafka.connect.storage.OffsetStorageWriter.doFlush(OffsetStorageWriter.java:141)     at org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets(WorkerSourceTask.java:267) ... ```","closed","","SmedbergM","2016-04-05T01:15:47Z","2018-02-25T07:10:15Z"
"","1419","KAFKA-3744: Allocate 2 attribute bits to signal payload format","This documentation update proposes a mechanism to signal the serialization used for the message payload, resolving issue https://issues.apache.org/jira/browse/KAFKA-3744.  No change is made to the message structure; two previously-reserved bits in the attribute byte now have defined values, and for one of four cases the key field is defined to be a JSON object.  No change is required to messaging software.   No change is required to existing producer and consumer clients that use pre-agreed payload serialization.   Misc notes: 1) Only one attribute bit would be needed if serialization were always signalled using the key field.  But it seems preferable to define two common serializations that do not have any dependency on the key field.  Selection of the common formats is arbitrary; text and avro seem reasonable but any two could be used instead. 2) The compression attribute uses three bits but only two are defined.  If the intent is to use all three bits for compression the undefined values should be listed as reserved; if not, the timestamp attribute can slide down to bit 2 and serialization to bits 3~4, leaving bits 5~7 reserved. 3) It's unclear why message field 6 should be called ""key"" - a variable-length field is more likely to be described as ""attributes"" or ""metadata"", and 1-byte field 3 would be called ""flags"" instead of ""attributes"". 4) Field 8 is called ""payload"" under message format and ""value"" under on-disk format.  Changed to payload in both places.","closed","","davek2","2016-05-24T01:12:23Z","2018-02-25T07:21:55Z"
"","878","MINOR: Updated kafkatest version to match version specified in gradle.properties","This discrepancy was causing a few system tests which to some version validation to fail consistently on 0.9.0 branch. Namely:  ``` Module: kafkatest.sanity_checks.test_verifiable_producer Class:  TestVerifiableProducer Method: test_simple_run Arguments: {   ""producer_version"": ""0.8.2.2"" }  Module: kafkatest.sanity_checks.test_verifiable_producer Class:  TestVerifiableProducer Method: test_simple_run Arguments: {   ""producer_version"": ""trunk"" }  Module: kafkatest.sanity_checks.test_kafka_version Class:  KafkaVersionTest Method: test_multi_version ```","closed","","granders","2016-02-05T21:01:54Z","2016-02-09T17:56:52Z"
"","1055","KAFKA-3392: ConsumerRecords iterator throws NoSuchElementException when a TopicPartition is empty","This contribution is my original work, and I license it under the project's open source license.  CC @jkreps","closed","","drausin","2016-03-13T17:11:19Z","2016-03-17T17:59:14Z"
"","1212","KAFKA-3160: Fix LZ4 Framing","This contribution is my original work and I license the work under Apache 2.0.","closed","","dpkp","2016-04-11T05:41:27Z","2016-05-07T18:49:16Z"
"","1270","MINOR: Fix some copy-pasted Javadoc in StreamsConfig.java","This contribution is my original work and I license the work to the Kafka project under the project's open source license.  cc @guozhangwang @miguno @ymatsuda","closed","","jklukas","2016-04-26T13:54:02Z","2016-04-26T16:12:10Z"
"","721","MINOR: fixing typos in docs","This commit contains minor grammatical fixes.  Some of the changes are just removing rogue commas, which can be hard to see in the diff.    This contribution is my original work and I license the work to the project under the project's open source license.","closed","","samjhecht","2015-12-30T21:47:31Z","2015-12-31T01:03:16Z"
"","837","KAFKA-2607: Review `Time` interface and its usage","This change adds some documentation and changes some calls from using absolute time to relative time.  Since I am new to the Kafka codebase I will mark the locations where I made the change from absolute to relative times to make it easier for reviewers to find any bugs that I might have introduced.","open","","afine","2016-01-29T22:04:01Z","2022-06-14T12:20:46Z"
"","511","KAFKA-2817; Check if socketChannel is connected in `SslTransportLayer.close`","This avoids spurious log warning messages. Also tweak log message if wrapResult.getStatus != CLOSED.","closed","","ijuma","2015-11-12T17:34:44Z","2016-03-01T22:48:10Z"
"","1155","KAFKA-3475; Introduce our own `MiniKdc`","This also fixes KAFKA-3453 and KAFKA-2866.","closed","","ijuma","2016-03-28T14:34:04Z","2016-03-31T02:31:03Z"
"","1443","KAFKA-3767: Failed Kafka Connect's unit test with Unknown license.","This address to https://issues.apache.org/jira/browse/KAFKA-3767.","closed","","sasakitoa","2016-05-28T09:08:55Z","2016-05-28T10:33:54Z"
"","1297","KAFKA-3581: add timeouts to joins in background thread services","This actually removes joins altogether, as well as references to self.worker_threads, which is best left as an implementation detail in BackgroundThreadService.  This makes use of @hachikuji 's recent ducktape patch, and updates ducktape dependency to 0.5.0.","closed","","granders","2016-04-29T23:06:55Z","2016-05-05T20:13:29Z"
"","1396","MINOR: Exclude jline, netty and findbugs annotations","These dependencies are unnecessary and they are acquired transitively via zkclient (jline, netty) and reflections (annotations).  Ewen did the hard work in figuring out why we have unexpected additional dependencies since 0.9.x.","closed","","ijuma","2016-05-17T14:59:37Z","2016-05-18T02:20:55Z"
"","940","MINOR: Example style improvements","These are minor, but no reason to make our example code look worse than it has to.","closed","","ijuma","2016-02-20T09:44:29Z","2016-03-01T22:52:45Z"
"","899","MINOR: add retry to state dir locking","There is a possibility that the state directory locking fails when another stream thread is taking long to close all tasks. Simple retries should alleviate the problem.","closed","","ymatsuda","2016-02-10T17:46:16Z","2016-02-24T21:42:07Z"
"","897","MINOR: Remove multi-byte charactor in docs","There are multi-byte characters In quickstart.html and security.html. This PR will fix it.","closed","","sasakitoa","2016-02-10T09:07:35Z","2016-02-10T19:51:24Z"
"","1271","MINOR: Upgrade to Gradle 2.13","There are a few improvements in 2.12 and 2.13. I am particularly interested in the performance improvements: - 2.12: ""This release brings support for compile only dependencies, improved build script compilation speed and even better IDE support."" - 2.13: ""We've achieved performance improvements during Gradle's configuration and execution phase, where we have measured up to 25% improvements to build time in our performance tests. No changes to your build script are necessary to start taking advantage of these improvements.""","closed","","ijuma","2016-04-26T21:26:11Z","2016-04-26T22:15:01Z"
"","1466","KAFKA-3787: Preserve the message timestamp in mirror maker","The timestamp of messages consumed by mirror maker is not preserved after sending to target cluster. The correct behavior is to keep create timestamp the same in both source and target clusters.","closed","","xiaotao183","2016-06-03T09:12:34Z","2016-06-05T07:59:06Z"
"","1398","KAFKA-3717; Support building aggregate javadoc for all project modules","The task is called `aggregatedJavadoc` and the generated html will be under `/build/docs/javadoc/`.  I also disabled javadoc for `tools` and `log4j-appender` as they are not public API.","closed","","ijuma","2016-05-17T21:24:03Z","2016-05-18T23:45:01Z"
"","558","KAFKA-2361: unit test failure in ProducerFailureHandlingTest.testNotE…","the same issue stated at https://issues.apache.org/jira/browse/KAFKA-1999, but on branch 0.8.2, still not fixed @junrao","closed","","jinxing64","2015-11-19T03:48:22Z","2015-11-21T07:17:59Z"
"","1437","KAFKA-3761: Remove BrokerState ""RunningAsController""","The reasons to remove it are: 1. It's currently broken.  The purpose of the [JIRA](https://issues.apache.org/jira/browse/KAFKA-3761) was to report that the RunningAsController state gets overwritten back to ""RunningAsBroker"". 2. It's not a useful state.   a. If clients want to use this metric to know whether a broker is ready to receive requests or not, they do not care whether or not the broker is the controller   b. there is already a separate boolean property, KafkaController.isActive which contains this information.","closed","","theduderog","2016-05-27T05:59:13Z","2016-06-29T20:59:28Z"
"","883","MINOR: fix RocksDBStore range search","The range is inclusive according to KeyValueStore's java doc.","closed","","ymatsuda","2016-02-08T17:46:48Z","2016-02-09T23:51:31Z"
"","986","KAFKA-3307: Add ApiVersions Request/Response and server side handling.","The patch does the following. 1. Adds ApiVersionsRequest/Response. 2. Adds UNSUPPORTED_VERSION error and UnsupportedVersionException. 3. Adds broker side handling of ApiVersionsRequest.","closed","","SinghAsDev","2016-02-29T23:35:04Z","2016-04-27T18:31:17Z"
"","658","KAFKA-2975","The newtorkClient should request a metadata update after it gets an error in the handleResponse().  Currently in data pipeline, 1) Lets say Mirror Maker requestTimeout is set to 2 min and metadataExpiry is set to 5 min 2) We delete a topic, the Mirror Maker get UNKNOWN_TOPIC_PARTITION and tries torefresh its Metadata. 3) It gets LeaderNotAvailableException, may be because the topic is not created yet. 4) Now its metadata does not have any information about that topic. 5) It will wait for 5 min to do the next refresh. 6) In the mean time the batches sitting in the accumulator will expire and the mirror makers die to avoid data loss.  To overcome this we need to refresh the metadata after 3).  Well there is an alternative solution to have the metadataExpiry set to be less then requestTimeout, but this will mean we make more metadataRequest over the wire in normal scenario as well.","closed","","MayureshGharat","2015-12-10T03:08:19Z","2016-01-06T17:48:49Z"
"","1016","KAFKA-3340: add some enhancements to MockConsumer","The MockConsumer class should support adding records concurrently. This allow to implement more complex test scenarios in which records are added concurrently with the records are polled.  In addition, I've implemented the rebalance callback in simple manner.","closed","","fhussonnois","2016-03-05T10:15:34Z","2018-05-26T18:45:17Z"
"","770","KAFKA-3080: Fix ConsoleConsumerTest by checking version when service is started","The MessageFormatter being used was only introduced as of 0.9.0.0. The Kafka version in some tests is changed dynamically, sometimes from trunk back to an earlier version, so this option must be set based on the version used when the service is started, not when it is created.","closed","","ewencp","2016-01-14T08:16:08Z","2016-01-21T19:18:47Z"
"","982","MINOR: Add vagrant up wrapper for simple parallel bringup on aws","The main impediment to bringing up aws machines in parallel using vagrant was the interaction between `vagrant-hostmanager` and `vagrant-aws`. If you disable hostmanager during the `up` phase, and run it after the cluster is up, parallel bringup is possible. The only caveat is that machines must be brought up in small-ish batches to prevent rate limit errors from AWS since `vagrant-aws` doesn't seem to have mechanisms to   This PR: - disables `vagrant-hostmanager` during bringup - adds a wrapper script to make it convenient to bring machines up in batches on aws","closed","","granders","2016-02-27T00:23:48Z","2016-03-20T23:47:16Z"
"","692","MINOR: Fix broken documentation link","The link to 'Producer Configs' section of the documentation is updated with this PR.","closed","","vahidhashemian","2015-12-17T22:32:10Z","2015-12-18T00:52:50Z"
"","817","MINOR: valueFactory's key is consumerId","the key of partitionAssignment's valueFactory is consumerId, not topic. partitionAssignment.getAndMaybePut(threadId.consumer)","closed","","zqhxuyuan","2016-01-27T07:00:24Z","2019-01-05T09:43:11Z"
"","592","KAFKA-2892 Consumer Docs Use Wrong Method","The KafkaConsumer docs use a non-existent method for assigning partitions (consumer.assign).  The JavaDocs show as:  ``` String topic = ""foo""; TopicPartition partition0 = new TopicPartition(topic, 0); TopicPartition partition1 = new TopicPartition(topic, 1); consumer.assign(partition0); consumer.assign(partition1); ```  Should be:  ``` String topic = ""foo""; TopicPartition partition0 = new TopicPartition(topic, 0); TopicPartition partition1 = new TopicPartition(topic, 1); consumer.assign(Arrays.asList(partition0, partition1)); ```","closed","","eljefe6a","2015-11-26T04:39:48Z","2015-11-26T06:31:23Z"
"","1362","KAFKA-3701: Expose KafkaStreams Metrics in public API","The Kafka clients expose their metrics registries through a `metrics` method presenting an unmodifiable collection (see [KafkaProducer's metrics() method](https://github.com/apache/kafka/blob/2790d0cb1fc6ade216f2c1bf2667a0ebb53d85e3/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L609-L615)), but `KafkaStreams` does not expose its registry.  This PR exposes an unmodifiable collection of metrics on `KafkaStreams` in direct analogy to `KafkaProducer`.  This contribution is my original work and I license the work to the project under the project's open source license.  cc @guozhangwang @miguno @mjsax","closed","","jklukas","2016-05-10T20:58:46Z","2016-12-06T13:30:23Z"
"","697","KAFKA-1860","The JVM should stop if the underlying file system goes in to Read only mode","closed","","MayureshGharat","2015-12-18T23:04:07Z","2015-12-18T23:16:51Z"
"","1378","MINOR: Remove  tag from doap file","The information from this file is displayed in:  https://projects.apache.org/project.html?kafka  I removed the tag instead of fixing it because it's one more thing to maintain and neither Hadoop nor ZooKeeper have it (httpd does though).","closed","","ijuma","2016-05-12T15:07:26Z","2016-05-12T21:06:41Z"
"","1301","KAFKA-3645: Fix ConsumerGroupCommand and ConsumerOffsetChecker to correctly read endpoint info from ZK","The host and port entries under /brokers/ids/ gets filled only for PLAINTEXT security protocol. For other protocols the host is null and the actual endpoint is under ""endpoints"". This causes NPE when running the consumer group and offset checker scripts in a kerberized env. By always reading the host and port values from the ""endpoint"", a more meaningful exception would be thrown rather than a NPE.","closed","","arunmahadevan","2016-04-30T18:04:02Z","2016-06-04T08:43:20Z"
"","867","MINOR: Removed unnecessary Vagrantfile hack","The hack here is no longer necessary with up-to-date versions of Vagrant, vagrant-hostmanager, and vagrant-aws. What's more, the change in c8b60b63 caused a chain of infinite recursion on OSX, preventing bringup of VMs on a typical laptop.","closed","","granders","2016-02-04T18:55:19Z","2016-02-05T02:39:09Z"
"","842","KAFKA-3179 Fix seek on compressed messages","The fix itself is simple.   Some explanation on unit tests. Currently we the vast majority of unit test is running with uncompressed messages.  I was initially thinking about run all the tests using compressed messages. But it seems uncompressed messages are necessary in a many test cases because we need the bytes sent and appended to the log to be predictable. In most of other cases, it does not matter whether the message is compressed or not, and compression will slow down the unit test. So I just added one method in the BaseConsumerTest to send compressed messages whenever we need it.","closed","","becketqin","2016-02-01T01:46:13Z","2016-02-05T00:53:01Z"
"","989","KAFKA-3310: Fix for NPEs observed when throttling clients.","The fix basically ensures that the throttleTimeSensor is non-null before handing off to record the metric value. We also record the throttle time to 0 so that we don't recreate the sensor always.","closed","","auradkar","2016-03-01T20:29:23Z","2016-03-04T00:17:26Z"
"","782","KAFKA-3054: Connect Herder fail if sent a wrong connector config or task config, catch exceptions to guard","The exceptions may propagate to DistributedHerder when start or stop connectors or tasks; The best solution is to track the track the status of connectors and tasks;  This patch catch exceptions to guard the DistributedHerder; this a short-term solution;","closed","","jinxing64","2016-01-17T03:22:20Z","2016-12-26T22:37:56Z"
"","667","KAFKA-2964: Split Security Rolling Upgrade Test by Client and Broker Protocols","The core of this PR is to ensure we evaluate enabling security in a running cluster where we have different broker and client protocols.  Also in this PR are some improvements to the validation process in produce_consume_validate.py which make it easier to work out where missing messages have been lost: - Fail fast if producer or consumer stop running.  - If messages go missing, check in the data files to see if the cause was data loss or the consumer missing messages.  - Make it possible for the ConsoleConsumer to log both what it consumed and when it consumed it (and enable this feature in produce_consume_validate tests)","closed","","benstopford","2015-12-11T09:32:20Z","2015-12-18T08:37:01Z"
"","729","KAFKA-2937 : Disable the leaderIsr check if the topic is to be deleted.","The check was implemented in KAFKA-340 : If we are shutting down a broker when the ISR of a partition includes only that broker, we could lose some messages that have been previously committed. For clean shutdown, we need to guarantee that there is at least 1 other broker in ISR after the broker is shut down.  When we are deleting the topic, this check can be avoided.","closed","","MayureshGharat","2016-01-04T23:05:50Z","2016-01-06T22:17:15Z"
"","654","MINOR: Trivial doc/ typo fixes.","The change in `docs/design.html` is hard to catch in the diff -- a `tbe` is changed to `the`. All other changes show up clearly in the diff.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","alexlod","2015-12-09T23:16:30Z","2015-12-09T23:40:13Z"
"","743","MINOR: Fix bug in `waitUntilLeaderIsElectedOrChanged` and simplify result type","The bug was for the following case:  `leader.isDefined && oldLeaderOpt.isEmpty && newLeaderOpt.isDefined && newLeaderOpt.get != leader.get`  We would consider it a successful election/change even though we should not.  I also changed the result type as we never return `None` (we throw an exception instead).","closed","","ijuma","2016-01-08T17:43:38Z","2016-03-01T22:54:35Z"
"","987","MINOR: DefaultMessageFormatter custom deserializer fixes","The ability to specify a deserializer for keys and values was added in a recent commit (845c6eae1f6c6bcf11), but it contained a few issues.","closed","","ijuma","2016-02-29T23:58:11Z","2016-03-01T22:44:44Z"
"","496","KAFKA-2771: Added rolling upgrade system test (ducktape) for Secured Cluster","Tests rolling upgrade from PLAINTEXT to SSL","closed","","benstopford","2015-11-11T14:52:55Z","2015-11-30T22:15:57Z"
"","1241","KAFKA-3591: JmxTool should exit out if a provided query matches no values.","Testing: I did manual tests by building locally and running the class against a remote 0.9.0.0 broker JMX URL. Running with a bad object-name produces the exit condition, and running with a good one continues to behave as before (prints the values). Running with no object name dumps all the metrics (KAFKA-2278).  Docs: This should need no doc changes.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","QwertyManiac","2016-04-20T12:08:04Z","2018-03-05T23:39:23Z"
"","1349","KAFKA-3662: Fix timing issue in SocketServerTest.tooBigRequestIsRejected","Test sends large request using multiple writes of length followed by request body. The first write should succeed, but since the server closes the connection on processing the length that is too big, subsequent writes may fail. Modified test to handle this exception.","closed","","rajinisivaram","2016-05-09T10:11:18Z","2016-05-09T13:48:49Z"
"","1131","KAFKA-3453; Transient test failures due to MiniKDC port allocation strategy","Temporarily copy fixed `MiniKdc` class until `MiniKdc` 2.8.0 is released.","closed","","ijuma","2016-03-24T12:57:48Z","2016-03-24T19:52:29Z"
"","1184","Kafka Connect metrics NPE prevention","Summary: When a worker is started up but hasn't setup the assignmentSnapshot it is possible to throw a NPE. This happens in our environment as we have a custom reporter running every 10 seconds. This change prevents that null pointer from happening","closed","connect,","SupermanScott","2016-04-04T22:31:54Z","2020-03-18T21:22:29Z"
"","933","MINOR: catch a commit failure due to rebalance","StreamThread should keep going after a commit was failed due to a group rebalance.  Currently the thread just dies. @guozhangwang","closed","","ymatsuda","2016-02-18T21:58:47Z","2016-02-23T05:40:00Z"
"","971","KAFKA-3281: Improve message of *-server-stop.sh when process is not running","Stop scritps such as kafka-server-stop.sh log messages of kill command's error when processes aren't running. This PR changes this message to ""No kafka server to stop"".","closed","","sasakitoa","2016-02-25T09:05:42Z","2016-03-04T01:42:13Z"
"","888","KAFKA-3218: Use static classloading for default config classes","Static classloading is better for default classes used in config to ensure that the classes can be loaded in any environment (OSGi, JEE etc. which rely on different classloading strategies).","closed","","rajinisivaram","2016-02-09T08:13:50Z","2016-07-29T09:06:24Z"
"","572","KAFKA-2872 Fixed addSink method connecting sink with parent source(s)…","Starting a KafkaStream was getting an error due to the fact that the TopologyBuilder.addSink method was not connecting the sink with it parent(s) processor/sources.  Just needed to wire up the sink with it parent(s) in TopologyBuilder.addSink .","closed","","bbejeck","2015-11-21T14:43:12Z","2015-11-22T02:47:16Z"
"","657","KAFKA-2927: reduce system test storage footprint","Split kafka logging into two levels - DEBUG and INFO, and do not collect DEBUG by default.","closed","","granders","2015-12-10T01:48:47Z","2016-01-31T23:52:19Z"
"","715","MINOR: Update to Gradle 2.10","Some of the Improvements Include: - The Checkstyle task now produces a human friendly HTML report - Potential performance improvements - Bug Fixes","closed","","granthenke","2015-12-22T22:46:32Z","2016-01-19T04:34:57Z"
"","915","MINOR: Doc of topic-level configuration is misleadable","Some configuration of Broker topic-level should be specified Int or Long. Currently docs will mislead to allow to use unit such as GB, days and so on.","closed","","sasakitoa","2016-02-15T08:30:03Z","2016-02-17T00:56:51Z"
"","792","KAFKA-3112: Change verification of bootstrap servers","so that unresolvable DNS names are ignored and only throw an error if no other bootstrap servers are resolvable.","closed","","bondj","2016-01-20T03:13:04Z","2016-05-07T08:06:42Z"
"","806","KAFKA-3141: Add checks to catch invalid authorizer porperties","Skip misformed properties instead of throwing ArrayIndexOutOfBoundsException","closed","","SinghAsDev","2016-01-23T00:26:08Z","2016-02-11T18:36:53Z"
"","1352","KAFKA-3682 ArrayIndexOutOfBoundsException thrown by","SkimpyOffsetMap.get() when full  Limited number of attempts to number of map slots after the internal positionOf() goes into linear search mode. Added unit test  Co-developed with @mimaison","closed","","edoardocomar","2016-05-09T16:10:20Z","2016-05-27T22:39:35Z"
"","575","MINOR: Bump version to 0.9.1.0-SNAPSHOT","Since we created the 0.9.0 branch a while back.","closed","","ijuma","2015-11-23T14:06:53Z","2016-03-01T22:48:26Z"
"","1457","KAFKA-3774: Make 'time' an optional argument of GetOffsetShell","Since the 'time' argument has a default value of -1, it makes sense to make it an optional argument.","closed","","vahidhashemian","2016-06-01T20:52:45Z","2016-06-02T05:54:14Z"
"","745","MINOR: Security doc fixes","Simple fixes that have tripped users.","closed","","ijuma","2016-01-08T21:32:32Z","2016-03-01T22:54:37Z"
"","819","MINOR: Fix syntax used for comment in JAAS config file","Simple fix, but important because the incorrect syntax causes the server not to start.","closed","","ijuma","2016-01-27T11:29:14Z","2016-03-01T22:53:34Z"
"","1425","KAFKA-3728 EndToEndAuthorizationTest offsets_topic misconfigured","Set OffsetsTopicReplicationFactorProp to 3 like MinInSyncReplicasProp  Else a consumer was able to consume via assign but not via subscribe, so the testProduceAndConsume is now duplicated to check both paths","closed","","edoardocomar","2016-05-24T21:03:01Z","2016-06-03T20:31:57Z"
"","1414","KAFKA-3728 EndToEndAuthorizationTest offsets_topic misconfigured","Set OffsetsTopicReplicationFactorProp to 3 like MinInSyncReplicasProp and OffsetCommitRequiredAcksProp to 1 to avoid timeouts  unit test for consumer that subscribes added","closed","","edoardocomar","2016-05-20T12:35:25Z","2016-05-24T19:07:10Z"
"","832","KAFKA-3170: Set default fetch_min_bytes in new consumer to 1","Set default to 1 instead of 1024, this matches the existing doc and feels like a better default value. Have run the unit tests with the change.","closed","","rajinisivaram","2016-01-29T13:57:54Z","2016-02-03T05:30:12Z"
"","1286","KAFKA-3618: Handle ApiVersionsRequest before SASL authentication","Server-side implementation and tests for handling ApiVersionsRequest before SaslHandshakeRequest.","closed","","rajinisivaram","2016-04-28T21:53:39Z","2016-04-29T18:15:46Z"
"","1276","KAFKA-3618: Handle ApiVersionsRequest before SASL authentication","Server-side implementation and tests for handling ApiVersionsRequest before SaslHandshakeRequest.","closed","","rajinisivaram","2016-04-27T19:45:12Z","2016-04-28T21:28:01Z"
"","1151","[documentation] Fix small typo in design section","Sentence was missing ""as"", minor grammar clean up.","closed","","paulcavallaro","2016-03-28T03:47:45Z","2016-04-04T01:15:37Z"
"","1078","Fixes for Windows #154","See the link: https://github.com/apache/kafka/pull/154","closed","","JeffersJi","2016-03-16T00:35:59Z","2022-02-09T19:31:32Z"
"","764","KAFKA-3025 Added timetamp to Message and use relative offset.","See KIP-31 and KIP-32 for details.  A few notes on the patch: 1. This patch implements KIP-31 and KIP-32. The patch includes features in both KAFKA-3025,  KAFKA-3026 and KAFKA-3036 2. All unit tests passed. 3. The unit tests were run with new and old message format. 4. When message format conversion occurs during consumption, the consumer will not be able to detect the message size too large situation. I did not try to fix this because the situation seems rare and only happen during migration phase.","closed","","becketqin","2016-01-13T14:03:13Z","2016-02-26T00:22:44Z"
"","1448","KAFKA-3770: KStream job should be able to specify linger.ms","See https://issues.apache.org/jira/browse/KAFKA-3770","closed","","gfodor","2016-05-30T08:00:31Z","2016-06-03T01:07:53Z"
"","1479","KAFKA-3711: Ensure a RecordingMap is passed to configured instances","See https://issues.apache.org/jira/browse/KAFKA-3711  I've tested locally that this change does indeed resolve the warning I mention in the ticket:  ``` org.apache.kafka.clients.consumer.ConsumerConfig: The configuration metric.dropwizard.registry = kafka-metrics was supplied but isn't a known config. ```  where `metric.dropwizard.registry` is a configuration value defined in a custom `MetricReporter` (https://github.com/SimpleFinance/kafka-dropwizard-reporter).  With this change, the above warning no longer appears, as @ewencp predicted.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","jklukas","2016-06-07T16:03:33Z","2016-06-08T21:07:14Z"
"","719","KAFKA-3049: VerifiableProperties does not respect 'default' properties of underlying java.util.Properties instance","See https://issues.apache.org/jira/browse/KAFKA-3049 for more information.  An alternative solution can be found at: https://github.com/apache/kafka/compare/trunk...jeffreyolchovy:KAFKA-3049_immutable-property-names.","closed","","jeffreyolchovy","2015-12-29T21:23:22Z","2022-02-09T20:50:52Z"
"","1032","KAFKA-3357; Update to Scala 2.11.8","Scala 2.11.8 has been released with a number of bug fixes: - http://www.scala-lang.org/news/2.11.8/  There are a few important collection fixes: - https://issues.scala-lang.org/browse/SI-9497 - https://github.com/scala/scala/pull/4714 - https://github.com/scala/scala/pull/4693  And also some pattern matcher fixes.","closed","","ijuma","2016-03-09T09:14:16Z","2016-03-10T01:57:47Z"
"","720","fix sasl.kerberos.service.name","sasl.kerberos.service.name surround by double quote didn't work, have to remove.","closed","","29x10","2015-12-30T09:05:59Z","2015-12-30T16:01:05Z"
"","577","KAFKA-2878: Guard against OutOfMemory in Kafka broker","Sanity check array size in requests before allocation","closed","","rajinisivaram","2015-11-23T22:47:14Z","2015-11-26T21:44:56Z"
"","571","KAFKA-2361: unit test failure in ProducerFailureHandlingTest.testNotE…","same issue with KAFKA-1999, so I want to fix it https://issues.apache.org/jira/browse/KAFKA-1999","closed","","jinxing64","2015-11-21T07:21:15Z","2015-11-27T06:57:21Z"
"","559","KAFKA-2643: Run mirror maker ducktape tests with SSL and SASL","Run tests with SSL, SASL_PLAINTEXT and SASL_SSL. Same security protocol is used for source and target Kafka.","closed","","rajinisivaram","2015-11-19T08:51:46Z","2015-11-25T23:06:07Z"
"","1282","KAFKA-2693: Ducktape tests for SASL/PLAIN and multiple mechanisms","Run a sanity test with SASL/PLAIN and a couple of replication tests with SASL/PLAIN and multiple mechanisms.","closed","","rajinisivaram","2016-04-28T09:48:58Z","2016-04-29T16:41:46Z"
"","1242","MINOR: Remove RollingBounceTest since its functionality is covered by the ReplicationTest system test","RollingBounceTest is a system test that cannot be run reliably in unit tests and ReplicationTest is a superset of the functionality: in addition to verifying that bouncing leaders eventually results in a new leader, ReplicationTest also validates that data continues to be produced and consumed.","closed","","ewencp","2016-04-20T17:06:58Z","2016-04-20T17:17:19Z"
"","538","KAFKA-2820: systest log level","Restores control over log level in system test service class KafkaService.","closed","","granders","2015-11-17T00:43:37Z","2015-11-20T19:04:40Z"
"","753","MINOR: Fix a type of variable","requestTimeoutMs should be int.","closed","","8da2k","2016-01-11T09:24:18Z","2018-01-26T18:56:07Z"
"","1435","MINOR: Fix tracing in KafkaApis.handle()","requestObj() returns null for the o.a.k.c.requests objects so use header() for these.  Once all the requests will have been replaced by o.a.k.c.requests objects, we should be able to clean that up, but in the meantime it's useful to trace both.","closed","","mimaison","2016-05-26T17:54:08Z","2018-04-18T13:32:05Z"
"","1444","Replace all pattern match on boolean value by if/else block.","Replaced all pattern match on boolean value by if/else block.  [KAFKA-3768](https://issues.apache.org/jira/browse/KAFKA-3768)","closed","","satendrakumar","2016-05-28T18:07:39Z","2016-05-31T04:53:39Z"
"","1118","KAFKA-3432; Cluster.update() thread-safety","Replace `update` with `withPartitions`, which returns a copy instead of mutating the instance.","closed","","ijuma","2016-03-23T01:53:00Z","2016-03-23T20:54:12Z"
"","1111","KAFKA-3428 Remove metadata sync bottleneck from mirrormaker's producer","Repalce topics with a concurrent hashset so it would not require to be modified inside a synchrnoized method. Make cluster a volatile varialble so fetch which just returns the pointer does not have to be synchrnized.  @ijuma @becketqin  the contribution is my original work and that i license the work to the project under the project's open source license.","closed","","maysamyabandeh","2016-03-22T03:49:23Z","2022-02-09T19:31:19Z"
"","906","MINOR: remove streams config params from producer/consumer configs","Removing streams' specific config params from producer/consumer configs to reduce warning messages.","closed","","ymatsuda","2016-02-12T00:53:14Z","2016-02-18T01:40:08Z"
"","735","KAFKA-3065: Remove unused topic partitions from RecordAccumulator","Removes unused topic partitions from RecordAccumulator#batches to prevent the map growing indefinitely. Replaces CopyOnWriteMap with ConcurrentHashMap to support deletes without double locking. Producer performance tests show no significant difference.","closed","","rajinisivaram","2016-01-06T15:41:25Z","2022-02-09T19:34:17Z"
"","632","KAFKA-2950: Fix performance regression in the producer","Removes all the System.currentTimeMillis calls to help with performance on small messages.","closed","","jkreps","2015-12-05T23:27:16Z","2015-12-05T23:57:54Z"
"","1332","KAFKA-3587: LogCleaner fails due to incorrect offset map computation …","Removed the over pessimistic require and instead attempt to fill the dedup buffer. Use the (only) map until full; this may allow to process all dirty segment (optimism) or may happen in the middle of a dirt segment. In either case, do compaction using the map loaded that way.  This patch was developed with @edoardocomar","closed","","mimaison","2016-05-06T15:25:13Z","2018-04-18T13:32:06Z"
"","503","KAFKA-2805","Removed the check for expiring only those batches whose metadata is unavailable. Now the batches will be expired irrespective of whether the leader is available or not, as soon as it reaches the requestimeout threshold.","closed","","MayureshGharat","2015-11-12T02:04:22Z","2015-11-12T02:42:06Z"
"","525","KAFKA-2791: removed deprecated properties","Removed support for BLOCK_ON_BUFFER_FULL_CONFIG (block.on.buffer.full) Removed support for METADATA_FETCH_TIMEOUT_CONFIG  Removed support for TIMEOUT_CONFIG (aka timeout.ms)  Added support for MAX_BLOCK_MS_CONFIG Added support for REQUEST_TIMEOUT_MS_CONFIG","closed","","benstopford","2015-11-13T18:18:54Z","2015-11-13T18:43:05Z"
"","1442","KAFKA-3765; Kafka Code style corrections","Removed explicit returns, not needed parentheses, corrected variables, removed unused imports Using isEmpty/nonEmpty  instead of size check, using head, flatmap instead of map-flatten","closed","","rekhajoshm","2016-05-28T00:10:57Z","2016-05-29T17:12:39Z"
"","556","KAFKA-2820: Remove log threshold on appender in tools-log4j.properties","Removed a config in tools-log4j.properties which prevented certain service classes from logging at TRACE level.","closed","","granders","2015-11-19T02:02:31Z","2015-11-20T19:04:41Z"
"","846","KAFKA-3175 : Topic not accessible after deletion even when delete.topic.enable is disabled","Remove topics under /admin/delete_topics path in zk if deleteTopic is disabled. The topic should never be enqueued for deletion.","closed","","MayureshGharat","2016-02-01T21:55:54Z","2016-10-10T21:07:14Z"
"","825","KAFKA-3147 : Memory records is not writable in MirrorMaker","Remove the batch from the RecordAccumulator once its closed while aborting batches. Make sure we don't accept new batch appends to RecordAccumulator while the producer is being closed.","closed","","MayureshGharat","2016-01-28T18:49:42Z","2016-02-13T08:10:33Z"
"","936","KAFKA-3237 - Remove test cases testInvalidDefaultRange() and testInva…","Remove test cases testInvalidDefaultRange() and testInvalidDefaultString(). Defaults if not overridden will get checked on parse. Testing the defaults is unnecessary. This allows you to set that a parameter is required while setting a validator for that parameter. Added a test case testNullDefaultWithValidator that allows a null default with a validator for certain strings.","closed","","jcustenborder","2016-02-19T06:07:35Z","2016-03-09T05:57:14Z"
"","759","KAFKA-3063; LogRecoveryTest causes JVM to exit occasionally","Remove deletion of tmp file in `OffsetCheckpoint`'s constructor. This delete causes unintuitive behaviour like `LogRecoveryTest` causing a `System.exit` because the test creates an instance of `OffsetCheckpoint` in order to call `read()` on it (while unexpectedly deleting a file being written by another instance of `OffsetCheckpoint`).  Also: - Improve error-handling in `OffsetCheckpoint` - Also include minor performance improvements in `read()` - Minor clean-ups to `ReplicaManager` and `LogRecoveryTest`","closed","","ijuma","2016-01-12T22:37:48Z","2016-03-01T22:54:43Z"
"","1436","[DUPLICATE] KAFKA-3760: Set broker state as running after publishing to ZooKeeper","Redoing PR #1426 with a JIRA","closed","","theduderog","2016-05-26T23:18:55Z","2016-05-26T23:42:18Z"
"","1298","MINOR: Add version check on enable-systest-events flag","Recent patch adding enable-systest-events flag without any version check breaks all uses of versioned console consumer. E.g. upgrade tests, compatibility tests etc.  Added a check to only apply the flag if running 0.10.0 or greater.","closed","","granders","2016-04-30T00:45:56Z","2016-05-03T04:43:00Z"
"","1331","KAFKA-3666: Update links for new consumer API","Pull request to update the consumer API links in the docs.","closed","","cotedm","2016-05-06T14:33:40Z","2016-05-06T15:34:05Z"
"","979","KAFKA-3297: Fair consumer partition assignment strategy (new consumer)","Pull request for https://issues.apache.org/jira/browse/KAFKA-3297","closed","","noslowerdna","2016-02-26T14:20:03Z","2020-02-17T18:18:38Z"
"","847","MINOR: Improve error message for inconsistent broker ids","Provides a more actionable and descriptive error message.","closed","","granthenke","2016-02-01T22:53:06Z","2016-02-17T06:06:23Z"
"","762","KAFKA-3012: Avoid reserved.broker.max.id collisions on upgrade","Provides a configuration to opt out of broker id generation.","closed","","granthenke","2016-01-13T03:33:34Z","2016-01-19T04:34:30Z"
"","1370","MINOR: Improve handling of channel close exception","Propagate IOException in SslTransportLayer channel.close to be consistent with PlaintextTransportLayer,  close authenticator on channel close even if transport layer close fails.","closed","","rajinisivaram","2016-05-11T15:02:32Z","2016-05-11T20:12:00Z"
"","882","KAFKA-3217: Close producers in unit tests","Producers that are not closed auto-create topics in subsequent tests when Kafka server port is reused. Added missing close().","closed","","rajinisivaram","2016-02-08T08:34:10Z","2016-02-09T17:50:01Z"
"","1386","Update doc","Process grep command has been updated. Previous ""ps  | grep server-1.properties""  command is showing nothing.","closed","","satendrakumar","2016-05-14T13:37:34Z","2016-05-25T00:08:21Z"
"","493","KAFKA-2801: Process any remaining data in SSL network read buffer after handshake","Process any remaining data in the network read buffer in `SslTransportLayer` when `read()` is invoked. On handshake completion, there could be application data ready to be processed that was read into `netReadBuffer` during handshake processing. `read()` is already invoked from `Selector` after handshake completion, but data already read into the `netReadBuffer` was not being processed. This PR adds a check for remaining data and continues with processing data if data is available.","closed","","rajinisivaram","2015-11-10T23:37:46Z","2015-11-11T00:37:34Z"
"","613","KAFKA-2718: Add logging to investigate intermittent unit test failures","Print port and directories used by zookeeper in unit tests to figure out which may be causing conflict.","closed","","rajinisivaram","2015-12-02T08:49:27Z","2015-12-03T18:52:03Z"
"","1165","MINOR: Advance system test ducktape dependency from 0.3.10 to 0.4.0","Previous version of ducktape was found to have a memory leak which caused occasional failures in nightly runs.","closed","","granders","2016-03-29T23:32:36Z","2016-03-30T07:51:20Z"
"","620","MINOR: Manually ported changes in 8c3c9548b636cdf760d2537afe115942d13bc003","Porting manually to 0.9.0 (no cherry-pick due to conflicts)  This will allow concurrent system test runs on the same machine","closed","","granders","2015-12-02T23:45:04Z","2015-12-03T17:56:13Z"
"","518","KAFKA-2825, KAFKA-2851: Extended existing ducktape replication tests to include controller failover","Plus fixed error copying key tab file when multiple builder jobs are running","closed","","apovzner","2015-11-12T23:01:42Z","2015-11-20T22:10:01Z"
"","1147","[KAFKA-3472] Allow MirrorMaker to copy selected partitions and choose target topic name","Please see the jira issue for detail: https://issues.apache.org/jira/browse/KAFKA-3472","closed","","ooasis","2016-03-27T06:08:31Z","2022-02-09T19:30:42Z"
"","730","KAFKA-2653: Alternative Kafka Streams Stateful API Design","ping @ymatsuda for reviews.","closed","","guozhangwang","2016-01-05T01:35:40Z","2016-01-14T16:46:15Z"
"","852","MINOR: Increment ducktape dependency","Pin kafka system tests to a newer version of ducktape.  Ran in branch builder; only one preexisting (transient) failure: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2016-02-01--001.1454333721--confluentinc--increment-ducktape-dependency--a40f474/report.html","closed","","granders","2016-02-03T00:01:30Z","2016-02-04T06:18:20Z"
"","1031","MINOR: Ignoring streams tests until there is fix for KAFKA-3354","Per discussion with @guozhangwang, `@ignore` failing streams system tests until fix for KAFKA-3354 is checked in.","closed","","granders","2016-03-09T02:38:55Z","2016-03-09T05:41:03Z"
"","670","Documentation updates for reserved.broker.max.id","People are facing problems upgrading their clusters with configured broker IDs above 1000 due to `reserved.broker.max.id` which wasn't very well announced.  This PR attempts to improve that somewhat by fixing the broker config docs and adding a note to the upgrade documentation.","closed","","edenhill","2015-12-11T18:47:08Z","2015-12-15T03:13:18Z"
"","501","KAFKA-2813: selector doesn't close socket connection on non-IOExceptions","Patched Selector.poll() to close the connection on any exception.","closed","","junrao","2015-11-12T01:14:15Z","2015-11-12T06:18:38Z"
"","903","KAFKA-3227; Conservative update of Kafka dependencies","Patch version bumps for bouncy castle, minikdc, snappy, slf4j, scalatest and powermock. Notable fixes: - Snappy: fixes a resource leak - Bouncy castle: security fixes  Also update Gradle to 2.11 (where the notable change is improved IDE integration) and the grgit build dependency.","closed","","ijuma","2016-02-11T09:56:47Z","2016-03-01T22:52:53Z"
"","711","KAFKA-3029:Marking class org.apache.kafka.common.TopicPartition as serializable","Patch for issue KAFKA-3029  Given that the fix is trivial no new test case is needed. I have run the test suite using gradle (as mentioned @ https://github.com/apache/kafka/blob/trunk/README.md) and suite runs clean.","closed","","praveend","2015-12-22T16:34:03Z","2016-01-30T04:28:14Z"
"","683","KAFKA-2979: Enable authorizer and ACLs in ducktape tests","Patch by @fpj and @benstopford.","closed","","fpj","2015-12-16T13:59:28Z","2016-01-08T04:04:59Z"
"","904","KAFKA-3229 ensure that root statestore is registered with ProcessorStateManager","Pass through the root StateStore in the init method so the inner StateStore can register that object.","closed","","tomdearman","2016-02-11T16:04:46Z","2016-02-11T18:36:29Z"
"","655","KAFKA-2896: Added system test for partition re-assignment","Partition re-assignment tests with and without broker failure.","closed","","apovzner","2015-12-10T01:09:16Z","2015-12-11T01:07:20Z"
"","785","KAFKA-3119: Adding -daemon option to zookeeper-server-start.sh USAGE, similar to kafka-server-start.sh","Output after fix: # satul# ./kafka-server-start.sh  USAGE: ./kafka-server-start.sh [-daemon] server.properties # satul# ./zookeeper-server-start.sh  USAGE: ./zookeeper-server-start.sh [-daemon] zookeeper.properties","closed","","atulsm","2016-01-18T13:51:04Z","2016-01-19T02:16:41Z"
"","862","KAFKA-3199: LoginManager should allow using an existing Subject","One possible solution which doesn't require a new configuration parameter: But it assumes that if there is already a Subject you want to use its existing credentials, and not login from another keytab specified by kafka_client_jaas.conf.  Because this makes the jaas.conf no longer required, a missing KafkaClient context is no longer an error, but merely a warning.","closed","","kunickiaj","2016-02-04T02:38:34Z","2017-06-09T04:08:16Z"
"","498","KAFKA-2809: Improve documentation linking","Often it is useful to link to a specific header within the documentation. Especially when referencing docs in the mailing lists.  This adds anchors and links for all headers in the docs.","closed","","granthenke","2015-11-11T20:02:29Z","2017-09-07T22:43:47Z"
"","984","HOTFIX: Missing streams jar in release","Observation: when doing ""gradlew releaseTarGz"" the streams jar was not included in the tarball. Adding a line to include it. @ijuma @guozhangwang could you please review. Thanks.","closed","","enothereska","2016-02-29T20:11:33Z","2016-02-29T20:52:00Z"
"","1109","MINOR: update new version in additional places","Note: This goes only to trunk. 0.10.0 branch will need a separate PR with different versions.","closed","","gwenshap","2016-03-21T20:31:41Z","2016-03-21T20:41:34Z"
"","747","Kafka 3078","Note that KAFKA-3077 will be required to run these tests.","closed","","SinghAsDev","2016-01-09T00:03:38Z","2016-01-12T07:16:16Z"
"","664","MINOR: Change SaslSetup workDir to be under the build folder","Moves test output from the project files and allows `gradle clean` to clean up the output.","closed","","granthenke","2015-12-10T21:11:44Z","2016-01-19T04:35:23Z"
"","549","MINOR: Update to Gradle 2.9 and update generated `gradlew` file","More performance improvements:  ""In many cases, Gradle 2.9 is much faster than Gradle 2.8 when performing incremental builds.  Very large builds (many thousands of source files) could see incremental build speeds up to 80% faster than 2.7 and up to 40% faster than 2.8.  Gradle now uses a more efficient mechanism to scan the filesystem, making up-to-date checks significantly faster. This improvement is only available when running Gradle with Java 7 or newer.  Other improvements have been made to speed-up include and exclude pattern evaluation; these improvements apply to all supported Java versions.  Gradle now uses much less memory than previous releases when performing incremental builds. By de-duplicating Strings used as file paths in internal caches, and by reducing the overhead of listing classes under test for Java projects, some builds use 30-70% less memory that Gradle 2.8.""  https://docs.gradle.org/current/release-notes","closed","","ijuma","2015-11-18T09:15:47Z","2016-03-01T22:48:36Z"
"","1428","KAFKA-3396 : Unauthorized topics are returned to the user","Modified KafkaApis to return Errors.UNKNOWN_TOPIC_OR_PARTITION if principal has no Describe access to topic  Unit tests expanded  Some paths cause the client to block due to bug https://issues.apache.org/jira/browse/KAFKA-3727?filter=-2 tests work around this by executing in separate thread","closed","","edoardocomar","2016-05-25T16:53:24Z","2016-09-26T18:15:39Z"
"","1026","KAFKA-3347 - Configure java to prefer ipv4","Modified KAFKA_JVM_PERFORMANCE_OPTS to include -Djava.net.preferIPv4Stack=true. Added an additional space at the end of the string to be consistent with the other variables.","closed","","jcustenborder","2016-03-07T23:16:14Z","2018-05-16T16:44:02Z"
"","733","MINOR: Replace deprecated property in MirrorMaker","MirrorMaker uses deprecated property ""block.on.buffer.full"" as default. Replace this with ""max.block.ms"" which is recommended.","closed","","sasakitoa","2016-01-05T08:15:14Z","2016-01-06T01:31:53Z"
"","821","KAFKA-3157 - Mirror maker doesn't commit offset","Mirror maker doesn't commit offset with new consumer enabled when data volume is low. This is caused by infinite loop in `receive()` which would never jump out of loop if no data coming","closed","","xiaotao183","2016-01-28T07:44:42Z","2016-02-05T17:11:57Z"
"","1068","KAFKA-3397: use -1(latest) as time default value for tools.GetOffsetShell","minor fix for prompt the user, or we will get a error message:  > Missing required argument ""[time]""","closed","","vesense","2016-03-15T02:50:42Z","2016-03-18T20:43:37Z"
"","1415","KAFKA-3737: Change log level for error during produce request","Minor change for https://issues.apache.org/jira/browse/KAFKA-3737","closed","","fhussonnois","2016-05-20T14:28:51Z","2022-02-09T19:29:32Z"
"","540","Merge pull request #1 from apache/trunk","Merging the kafka latest to my fork","closed","","prabcs","2015-11-17T06:02:07Z","2015-11-17T06:06:48Z"
"","1128","KAFKA-3427 - Broker should return correct version of FetchResponse on exception","Merging the fix from: https://issues.apache.org/jira/browse/KAFKA-3427 The original version of the code, returned a response using V0 of the response protocol. This caused clients to break because they expected the throttle_time_ms field to be present.","closed","","auradkar","2016-03-24T00:19:23Z","2018-02-25T07:12:21Z"
"","677","MINOR: Improve README","Mention Java 7 requirement, fix and improve formatting and remove obsolete reference to SVN website.","closed","","ijuma","2015-12-15T13:05:38Z","2016-03-01T22:53:50Z"
"","1110","MINOR: update new version in additional places","matching set of version fixes. @ewencp @junrao","closed","","gwenshap","2016-03-21T20:45:02Z","2016-03-22T02:18:44Z"
"","671","KAFKA-2977: Transient Failure in kafka.log.LogCleanerIntegrationTest.cleanerTest","Make MinCleanableDirtyRatioProp configurable(default 0.0F)in makeCleaner, thus log cleaning is always undergoing; Also removed minDirtyMessages.","closed","","jinxing64","2015-12-13T14:56:52Z","2015-12-21T06:07:44Z"
"","685","KAFKA-3002: Make available to specify hostname with Uppercase at broker list","Make available to specify hostname with Uppercase at broker list","closed","","sasakitoa","2015-12-17T00:57:26Z","2015-12-24T04:56:33Z"
"","636","KAFKA-1911","Made delete topic on brokers async. This patch renames the directory (Topic-Partition) to (Topic-Partition.UUID.delete) on calling markForDeletion and then delete it asynchronously.  I need to update the test cases for this patch.","closed","","MayureshGharat","2015-12-07T22:09:54Z","2016-12-26T22:37:57Z"
"","723","KAFKA-2944: fix NullPointerException in KafkaConfigStorage","Lost of ""config messages"" can affect the logic of KafkaConfigStorage; Call readToEnd after sending each message to KafkaBasedLog to ensure that all config messages are flushed to Kafka; Since ""config messages""  sending to KafkaBasedLog are metadata, it will not affect performance too much;","closed","","jinxing64","2016-01-01T15:29:38Z","2017-01-12T02:05:17Z"
"","772","KAFKA-3087: Fix retention.ms property documentation in config docs","Log retention settings can be set it in broker and some properties can be overriden at topic level.  |Property |Default|Server Default property| Description| |retention.ms|7 days|log.retention.minutes|This configuration controls the maximum time we will retain a log before we will discard old log segments to free up space if we are using the ""delete"" retention policy. This represents an SLA on how soon consumers must read their data.|  But retention.ms is in milli seconds not in minutes. So corresponding _Server Default property_ should be _log.retention.ms_ instead of _log.retention.minutes_.","closed","","rajubairishetti","2016-01-14T08:55:08Z","2018-02-25T21:37:49Z"
"","807","KAFKA-3138: 0.9.0 docs still say that log compaction doesn't work on compressed topics.","Log compaction is supported on compressed topics as of 0.9.0, so update the docs to reflect that.","closed","","wushujames","2016-01-24T06:50:52Z","2016-02-05T17:55:33Z"
"","831","KAFKA-3169: Limit receive buffer size for SASL packets in broker","Limit receive buffer size to avoid OOM in broker with invalid SASL packets","closed","","rajinisivaram","2016-01-29T13:13:59Z","2016-01-30T11:49:41Z"
"","1323","KAFKA-3494: add metric id to client mbeans","KafkaConsumer and KafkaProducer mbeans currently only have a client-id granularity. Client-id represents a logical name for an application.  Given that quotas encourage reuse of client-ids, it's not unexpected to have multiple clients share the same client-id within the same jvm. When a later client gets instantiated with the same client-id as an earlier client in the same jvm, JmxReporter unregisters the original client's mbeans that collide with the new client's mbeans.  This commit makes client mbeans have a metric-id granularity to prevent mbean collision and the original client mbean unregistration.","open","","onurkaraman","2016-05-05T05:32:22Z","2022-06-14T12:52:53Z"
"","1145","MINOR: server stop scripts don't work correctlly in shell","kafka-server-stop.sh and zookeeper-server-stop.sh don't work in shell. (however these work correctlly in bash)","closed","","sasakitoa","2016-03-26T12:19:29Z","2016-04-04T02:35:30Z"
"","617","Minor: ConsoleConsumer - Fix number of processed messages count","kafka-console-consumer.sh is showing an incorrect number of messages processed, counting one more message than the actual number of processed messages.","closed","","luafran","2015-12-02T20:03:50Z","2015-12-03T16:59:10Z"
"","924","KAFKA-3242: minor rename / logging change to Controller","KAFKA-3242: minor rename / logging change to references to 'adding partitions' to indicate 'modifying partitions'","closed","","benstopford","2016-02-17T00:12:24Z","2016-02-23T20:52:29Z"
"","858","KAFKA-3198: Ticket Renewal Thread exits prematurely due to inverted c…","KAFKA-3198: Ticket Renewal Thread exits prematurely due to inverted comparison  The >= should be < since we are actually able to renew if the renewTill time is later than the current ticket expiration.","closed","","kunickiaj","2016-02-03T23:32:40Z","2016-07-12T08:41:50Z"
"","633","KAFKA-1148 check leader epoch for DelayedProduce","KAFKA-1148: check leader epoch for DelayedProduce","closed","","iBuddha","2015-12-06T12:05:43Z","2015-12-07T12:52:48Z"
"","1446","KAFKA-3715: add granular metrics per node","Kafka Streams: add granular metrics per node, also expose ability to … …register non latency metrics in StreamsMetrics  from https://github.com/apache/kafka/pull/1362#issuecomment-218326690------- We can consider adding metrics for process / punctuate / commit rate at the granularity of each processor node in addition to the global rate mentioned above. This is very helpful in debugging.  We can consider adding rate / total cumulated metrics for context.forward indicating how many records were forwarded downstream from this processor node as well. This is helpful in debugging.  We can consider adding metrics for each stream partition's timestamp. This is helpful in debugging. ## Besides the latency metrics, we can also add throughput latency in terms of source records consumed.  More discussions here https://issues.apache.org/jira/browse/KAFKA-3715","closed","","aartigupta","2016-05-30T05:06:28Z","2017-01-11T20:35:17Z"
"","1267","KAFKA-3619 - File handles are leaked on .lock files","Kafka Streams seems to hold file handles on the `.lock` files for the state dirs, resulting in an explosion of filehandles over time. Running `lsof` shows the number of open filehandles on the `.lock` file increasing rapidly over time. In a separate test project, I reproduced the issue and determined that in order for the filehandle to be relinquished the `FileChannel` instance must be properly closed. Applying this patch seems to resolve the issue in my job.","closed","","gfodor","2016-04-25T07:09:06Z","2016-04-25T20:46:21Z"
"","1074","Changed port of bootstrap server to default.","Kafka is typically running on port 9092. The example named a different port what makes it difficult to run a bootstrap example without any further configuration.","closed","","raphw","2016-03-15T15:16:14Z","2016-04-26T18:53:08Z"
"","1394","KAFKA-3718: propagate all KafkaConfig __consumer_offsets configs to OffsetConfig instantiation","Kafka has two configurable compression codecs: the one used by the client (source codec) and the one finally used when storing into the log (target codec). The target codec defaults to KafkaConfig.compressionType and can be dynamically configured through zookeeper.  The GroupCoordinator appends group membership information into the __consumer_offsets topic by: 1. making a message with group membership information 2. making a MessageSet with the single message compressed with the source codec 3. doing a log.append on the MessageSet  Without this patch, KafkaConfig.offsetsTopicCompressionCodec doesn't get propagated to OffsetConfig instantiation, so GroupMetadataManager uses a source codec of NoCompressionCodec when making the MessageSet. Let's say we have enough group information such that the message formed exceeds KafkaConfig.messageMaxBytes before compression but would fall below the threshold after compression using our source codec. Even if we had dynamically configured __consumer_offsets with our favorite compression codec, the log.append will throw RecordTooLargeException during analyzeAndValidateMessageSet since the message was unexpectedly uncompressed instead of having been compressed with the source codec defined by KafkaConfig.offsetsTopicCompressionCodec.","closed","","onurkaraman","2016-05-17T09:12:30Z","2016-05-26T08:18:11Z"
"","560","KAFKA-2423: Introduce Scalastyle","Just the buildscript changes and rules configuration.","closed","","granthenke","2015-11-19T14:49:09Z","2022-02-09T19:35:17Z"
"","967","KAFKA-3279: Remove checks for JAAS system property","JAAS configuration may be set using other methods and hence the check for System property doesn't  always match where the actual configuration used by Kafka is loaded from.","closed","","rajinisivaram","2016-02-24T15:51:25Z","2016-03-09T07:40:44Z"
"","1405","MINOR: Move `Incorporating Security Features in a Running Cluster` to its own section under `Security`","It was previously in the SASL section (probably by mistake).","closed","","ijuma","2016-05-19T15:06:20Z","2016-05-19T16:17:06Z"
"","638","MINOR: Remove unused DoublyLinkedList","It used to be used by MirrorMaker but its usage was removed in KAFKA-1997.","closed","","granthenke","2015-12-08T03:48:40Z","2016-01-19T04:35:18Z"
"","689","KAFKA-2058: ProducerTest.testSendWithDeadBroker transient failure","It turns that waitUntilMetadataIsPropagated is not enough; in ""onBrokerStartup"", methods below will send send both LeaderAndIsrRequest and UpdateMetadataRequest to KafkaApis:     replicaStateMachine.handleStateChanges(allReplicasOnNewBrokers, OnlineReplica)     partitionStateMachine.triggerOnlinePartitionStateChange() The two kinds of request are handled seperately and we are not sure about the order; If UpdateMetadataRequest is handled first, metadataCache of kafkaApis will be updated, thus TestUtils.waitUntilMetadataIsPropagated will be satisfied, and consumer can(will) start fetching data; But if the LeaderAndIsrRequest is not handled at this moment, ""becomeLeaderOrFollower"" cannot be called , thus structures like ""leaderReplicaOpt"" cannot be updated, which leads to failure of consumer's fetching data; To fix above, consumer should start fetching data after partition's leaderReplica is refreshed, not just the leader is elected; So added ""TestUtils.waitUntilLeaderIsKnown(servers, topic, 0)""","closed","","jinxing64","2015-12-17T14:12:08Z","2015-12-21T06:04:10Z"
"","793","MINOR: Remove the very misleading comment lines","It is not true in practice. Maybe the implied feature is not yet implemented or removed. These lines can be super misleading.  Please merge. Thank you.","closed","","gaob13","2016-01-20T09:40:55Z","2016-03-22T18:58:31Z"
"","771","KAFKA-3105: Use `Utils.atomicMoveWithFallback` instead of `File.rename`","It behaves better on Windows and provides more useful error messages.  Also: - Minor inconsistency fix in `kafka.server.OffsetCheckpoint`. - Remove delete from `streams.state.OffsetCheckpoint` constructor (similar to the change in `kafka.server.OffsetCheckpoint` in https://github.com/apache/kafka/commit/836cb1963330a9e342379899e0fe52b72347736e#diff-2503b32f29cbbd61ed8316f127829455L29).","closed","","ijuma","2016-01-14T08:51:38Z","2016-03-01T22:54:48Z"
"","1460","KAFKA-3775: Throttle maximum number of tasks assigned to a single KafkaStreams","Issue: https://issues.apache.org/jira/browse/KAFKA-3775  POC. Discussion in progress.","closed","","kawamuray","2016-06-02T06:53:57Z","2016-06-12T15:20:41Z"
"","1289","KAFKA-3642: Fix NPE from ProcessorStateManager when the changelog topic not exists","Issue: https://issues.apache.org/jira/browse/KAFKA-3642","closed","","kawamuray","2016-04-29T17:53:00Z","2017-06-25T23:50:31Z"
"","1119","KAFKA-3448: Include % character in IPV6 Regex","IPV6 addresses could have the % character. When an address is written textually, the zone index is appended to the address, separated by a percent sign (%).","closed","","soumyajit-sahu","2016-03-23T03:49:25Z","2016-03-23T03:53:18Z"
"","1120","KAFKA-3448: add % character to ipv6 regex","IPV6 address can contain % character","closed","","soumyajit-sahu","2016-03-23T04:30:13Z","2016-05-03T01:28:36Z"
"","788","KAFKA-3111: Fix ConsumerPerformance reporting to use time-based instead of message-based intervals","Interval lengths for ConsumerPerformance could sometime be calculated as zero. In such cases, when the bytes read or messages read are also zero a NaN output is returned for mbRead per second or for nMsg per second, whereas zero would be a more appropriate output.  In cases where interval length is zero but there have been data and messages to read, an output of Infinity is returned, as expected.","closed","","vahidhashemian","2016-01-18T21:59:18Z","2018-06-11T17:33:42Z"
"","1223","KAFKA-3559: lazy initialisation of state stores","Instead of initialising state stores on init(), they are initialised on first access.","closed","","enothereska","2016-04-15T09:04:02Z","2017-04-09T12:13:01Z"
"","1379","MINOR: Fix bugs in KafkaStreams.close()","Initially proposed by @ijuma in https://github.com/apache/kafka/pull/1362#issuecomment-218293662  @mjsax commented:  > StreamThread.close() should be extended to call metrics.close() (the class need a private member to reference the Metrics object, too)  The `Metrics` instance is created in the `KafkaStreams` constructor and shared between all threads, so closing it within the threads doesn't seem like the right approach. This PR calls `Metrics.close()` in `KafkaStreams.close()` instead.  cc @guozhangwang","closed","","jklukas","2016-05-12T16:18:04Z","2016-05-13T09:42:35Z"
"","1424","KAFKA-3511: Initial commit for aggregators [WiP]","Initial structure. Removed initialiser. Two simple aggregators.","closed","","enothereska","2016-05-24T14:28:18Z","2016-05-26T07:53:20Z"
"","885","MINOR: Use explicit type in AclCommand","Inference sometimes fails for this case.","closed","","ijuma","2016-02-08T18:29:10Z","2016-03-01T22:53:19Z"
"","1235","KAFKA-3564: Count metric always increments by 1.0","Increment the counter by the value passed into the function","closed","","kichristensen","2016-04-18T21:14:07Z","2016-12-26T22:37:51Z"
"","521","MINOR: Tuned timeout parameter to reduce chance of transient failure","Increased timeout in downstream consumer doing validation step. This addresses a transient failure case in mirror maker tests with mirror maker failover.","closed","","granders","2015-11-13T01:08:07Z","2015-12-09T19:41:42Z"
"","1205","KAFKA-3533: Incorrect logic","Incorrect logic in Config Validation for KafkaConsumer.   https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L556-L557  From what I can tell the error message is correct and the logic is not.","closed","","rnpridgeon","2016-04-08T18:39:41Z","2016-04-08T19:49:32Z"
"","1470","MINOR: Move `ConsoleProducerTest` to the unit test directory","Included a couple of clean-ups: removed unused variable and the instantiated `KafkaProducer` is now closed.","closed","","ijuma","2016-06-05T07:40:05Z","2016-06-06T02:35:23Z"
"","939","KAFKA-3233: expose consumer per-topic metrics","In version of 0.8.2.1, the old consumer will provide the metrics reporter per-topic consumer metrics under group 'ConsumerTopicMetrics'. For example:  *.ConsumerTopicMetrics.clientId.[topic name].BytesPerSec.count *.ConsumerTopicMetrics.clientId.[topic name].MessagesPerSec.count  These consumer metrics are useful since it helps us monitor consumer rate for each topic. But the new consumer(0.9.0.0) doesn't expose per topic metrics anymore, even though I did find sensor objects in consumer metrics object collecting per-topic metrics.  After investigation, I found that these sensors are not registering any KafkaMetrics.","closed","","happymap","2016-02-19T18:39:01Z","2016-03-11T18:58:12Z"
"","905","KAFKA-3233: expose per topic consumer metrics to metrics reporter","In version of 0.8.2.1, the old consumer will provide the metrics reporter per-topic consumer metrics under group 'ConsumerTopicMetrics'. For example:  *.ConsumerTopicMetrics.clientId.[topic name].BytesPerSec.count *.ConsumerTopicMetrics.clientId.[topic name].MessagesPerSec.count  These consumer metrics are useful since it helps us monitor consumer rate for each topic. But the new consumer(0.9.0.0) doesn't expose per topic metrics anymore, even though I did find sensor objects in consumer metrics object collecting per-topic metrics.  After investigation, I found that these sensors are not registering any KafkaMetrics.","closed","","happymap","2016-02-11T23:17:28Z","2016-02-19T18:55:01Z"
"","504","MINOR: Do not collect zk persistent data by default","In system tests zookeeper service, it is overkill and space-intensive to collect zookeeper data logs by default. This minor patch turns off default collection.","closed","","granders","2015-11-12T02:37:34Z","2015-12-09T19:41:40Z"
"","1447","KAFKA-3769 - KStream job spending 60% of time writing metrics","In profiling a complex Kafka Streaming job: - 50% of CPU time was found to be spent writing the metrics for process time to the process time Sensor. - 7-8% of CPU time was spent doing various unnecessary work in Fetcher.  This PR addresses the first by rolling up the process time per iteration into a single metric, and addresses the second point directly (see patch.) I think this PR is incomplete because the metric itself has changed in meaning. Additionally, the latency metrics recorded during writes to rocksdb also incur significant cost. If there is a better fix in mind, please advise.","closed","","gfodor","2016-05-30T06:51:57Z","2016-06-09T18:29:46Z"
"","929","KAFKA-3236: Honor Producer Configuration ""block.on.buffer.full""","In Kafka-0.9, `max.block.ms` is used to control how long the following methods will block.  `KafkaProducer.send()` when - Buffer is full - Metadata is unavailable  `KafkaProducer.partitionsFor()` when - Metadata is unavailable  However when `block.on.buffer.full` is set to false, `max.block.ms` is in effect whenever a buffer is requested/allocated from the Producer BufferPool. Instead it should throw a BufferExhaustedException without waiting for `max.block.ms`  This is particulary useful if a producer application does not wish to block at all on `KafkaProducer.send()`. We avoid waiting on `KafkaProducer.send()` when metadata is unavailable by invoking `send()` only if the producer instance has fetched the metadata for the topic in a different thread using the same producer instance. However `max.block.ms` is still required to specify a timeout for bootstrapping the metadata fetch.  We should resolve this limitation by decoupling `max.block.ms` and `block.on.buffer.full`. - `max.block.ms` will be used exclusively for fetching metadata when `block.on.buffer.full = false` (in pure non-blocking mode ) - `max.block.ms` will be applicable to both fetching metadata as well as buffer allocation when `block.on.buffer.full = true`","closed","","knusbaum","2016-02-17T16:58:38Z","2016-02-18T16:26:22Z"
"","716","KAFKA-3043: Replace request.required.acks with acks in docs.","In Kafka 0.9, request.required.acks=-1 which configration of producer is replaced by acks=all,  but this old config is remained in docs.","closed","","sasakitoa","2015-12-25T05:32:01Z","2015-12-27T07:15:02Z"
"","1438","MINOR: Kafka And ZooKeeper Stop Scripts doesn't return PID","In Centos 6.5, 'ps -ax | grep java' command doesn't returns the full process name. So, the command doesn't returns the PID even Kafka and ZooKeeper is running.  Updated the script to use Java Process command (jps) which is elegant to retrieve the running java process PID","closed","","kamalcph","2016-05-27T13:20:21Z","2016-09-14T06:01:51Z"
"","580","KAFKA-2882: Add constructor cache for Snappy and LZ4 Output/Input streams in Compressor.java","In `wrapForOutput` and `wrapForInput` methods of `org.apache.kafka.common.record.Compressor`,  `Class.forName(""[compression codec]"")` and `getConstructor` methods are invoked for each `wrapForOutput` / `wrapForInput` call. Reflection calls are expensive and impact performance at high volumes. This patch adds a cache for `Constructor` to reduce the reflection overhead.  In our production deployments, this has reduced producer CPU usage by about 20%","closed","","logarithm","2015-11-24T13:26:26Z","2015-11-27T16:08:41Z"
"","562","Adding .gitignore files to filter Eclipse IDE bin directories","In  the case of ""core"" project, .gitignore includes .cache-main and .cache-tests from gradlew jar command.","closed","","jeqo","2015-11-19T22:14:41Z","2015-11-19T23:24:03Z"
"","778","MINOR: Improve Kafka documentation","Improve the documentation by fixing typos, punctuations, and correcting the content.","closed","","vahidhashemian","2016-01-14T22:27:36Z","2016-01-24T06:27:23Z"
"","1348","KAFKA-1489: Global threshold on data retention size","Implemented a ""log retention policy"" based on keeping a certain percentage of disk space free. In dynamic situations where topics are added in unpredictable ways, the other log retention parameters are not entirely sufficient to prevent out-of-disk conditions from occurring. The new log.retention.disk.usage.percent parameter provides this guarantee. It is applied after all the other retention parameters are applied, at the end of each log cleanup cycle. Oldest segments (across all topics) are pruned until usage falls below this percentage of each disk's capacity. The default value is 100, which effectively disables the feature.  This is my original work and I license the work to the project under the project's open source license.  @junrao, @jkreps, @gwenshap","open","","bendrees","2016-05-09T06:33:20Z","2022-06-14T12:53:14Z"
"","1249","KAFKA-3594; After calling MemoryRecords.close() method, hasRoomFor() method should return false","IllegalStateException can occur when producer is trying to append a record to a Re-enqueued record batch in the accumulator. We should not allow to add a record to Re-enqueued record batch. This is due a bug in MemoryRecords.java/hasRoomFor() method. After calling MemoryRecords.close() method, hasRoomFor() method should return false.","closed","","omkreddy","2016-04-21T10:47:17Z","2018-07-03T15:42:15Z"
"","1389","MINOR: document increased network bandwidth of 0.10 under replication","If you're pushing close to the network capacity, 0.10's additional 8 bytes per message can lead to overload of your network. We (Heroku Kafka) ran into this issue whilst benchmarking 0.10 RC and the @ijuma suggested it belonged in the update note.  Comments/suggestions welcome.","closed","","tcrayford","2016-05-15T20:27:07Z","2016-05-16T12:40:46Z"
"","1469","KAFKA-724; Allow automatic socket.send.buffer from operating system in SocketServer","If socket.receive.buffer.bytes/socket.send.buffer.bytes are set to -1, use the OS defaults.","closed","","rekhajoshm","2016-06-05T01:17:09Z","2016-06-08T23:44:45Z"
"","1089","KAFKA-3415: Add partitions check if 0 partitions are to be added","If no partitions are to be added, the command will exit gracefully with retcode 0 and a printed message.  This patch was developed with @mimaison","closed","","edoardocomar","2016-03-17T16:48:04Z","2021-11-04T10:48:27Z"
"","645","KAFKA-2948: Remove unused topics from producer metadata set","If no messages are sent to a topic during the last refresh interval or if UNKNOWN_TOPIC_OR_PARTITION error is received, remove the topic from the metadata list. Topics are added to the list on the next attempt to send a message to the topic.","closed","","rajinisivaram","2015-12-09T00:35:30Z","2016-06-06T18:57:41Z"
"","736","KAFKA-3069: Fix recursion in ZkSecurityMigrator","I'm also fixing a bug in the testChroot test case.","closed","","fpj","2016-01-06T17:14:12Z","2016-01-12T17:49:21Z"
"","709","KAFKA-3010; Include error in log when ack 0","I verified this by trying to produce to __consumer_offsets and the logged message looks like:  [2015-12-22 10:34:40,897] INFO [KafkaApi-0] Closing connection due to error during produce request with correlation id 1 from client id console-producer with ack=0 Topic and partition to exceptions: [__consumer_offsets,43] -> kafka.common.InvalidTopicException (kafka.server.KafkaApis)","closed","","ijuma","2015-12-22T10:47:42Z","2016-03-01T22:54:11Z"
"","1380","MINOR: Fix checkstyle failure in `StreamsConfigTest`","I removed the hamcrest matcher to unbreak the build, but we probably want to tweak the `import-control.xml` as it currently only allows it for ``, which is weird.","closed","","ijuma","2016-05-12T18:26:38Z","2016-05-12T19:37:18Z"
"","610","KAFKA-2851 Using random file names for local kdc files to avoid conflicts.","I originally tried to solve the problem by using tempfile, and creating and using scp() utility method that created a random local temp file every time it was called. However, it required passing miniKdc object to SecurityConfig setup_node which looked very invasive, since many tests use this method. Here is the PR for that, which I think we will close: https://github.com/apache/kafka/pull/609  This change is the least invasive change to solve conflicts between multiple tests jobs.","closed","","apovzner","2015-12-02T02:03:38Z","2015-12-02T19:18:20Z"
"","829","MINOR: Fix spelling and grammar issues in ReplicaFetcherThread detailed comment","I noticed them while looking at the recent commit:  https://github.com/apache/kafka/commit/87eccb9a3bea56e5d7d5696aaddef1421f038903","closed","","ijuma","2016-01-29T07:58:05Z","2016-03-01T22:53:28Z"
"","836","MINOR: Remove unused imports from tests whose packages were fixed","I missed this when reviewing:  https://github.com/apache/kafka/commit/a58b459bddd5b1dcab224b53cda0196e947a5b09","closed","","ijuma","2016-01-29T19:23:02Z","2016-03-01T22:53:22Z"
"","786","MINOR: MemoryRecords.sizeInBytes throws NPE when non-writable.","I just noticed that `MemoryRecords.sizeInBytes` throws NPE when MemoryRecords is non-writable. `compressor` is explicitly set to null when `writable` is false (L56) at the construction time, for instance when `MemoryRecords.readableRecords` is used.  @guozhangwang Could you take a look when you have time?","closed","","dajac","2016-01-18T18:09:33Z","2020-08-11T06:48:25Z"
"","1455","MINOR: Fix setting of ACLs and ZK shutdown in test harnesses","I found both issues while investigating the issue described in PR #1425.","closed","","ijuma","2016-06-01T16:22:14Z","2016-06-03T00:22:53Z"
"","1181","[TRIVIAL] fix typo in JavaDoc","I could also include this in https://github.com/apache/kafka/pull/1180 (just wanted to keep PR-1180 ""clean"")","closed","","mjsax","2016-04-04T08:56:51Z","2016-04-04T10:45:58Z"
"","570","KAFKA-2825, KAFKA-2851: Controller failover tests added to ducktape replication tests and fix to temp dir","I closed an original pull request that contained previous comments by Geoff (which are already addressed here), because I got into bad rebase situation. So, I created a new branch and cherry-picked my commits + merged with Ben's changes to fix MiniKDC tests to run on Virtual Box. That change was conflicting with my changes, where I was copying MiniKDC files with new scp method, and temp file was created inside that method. To merge Ben's changes, I added two optional parameters to scp(): 'pattern' and 'subst' to optionally substitute string while spp'ing files, which is needed for krb5.conf file.","closed","","apovzner","2015-11-20T22:05:26Z","2016-04-25T23:27:38Z"
"","648","KAFKA-2837: fix transient failure of kafka.api.ProducerBounceTest > testBrokerFailure","I can reproduced this transient failure, it seldom happen; code is like below:  // rolling bounce brokers     for (i <- 0 until numServers) {       for (server <- servers) {         server.shutdown()         server.awaitShutdown()         server.startup()         Thread.sleep(2000)       }  ```   // Make sure the producer do not see any exception   // in returned metadata due to broker failures   assertTrue(scheduler.failed == false)    // Make sure the leader still exists after bouncing brokers   (0 until numPartitions).foreach(partition => TestUtils.waitUntilLeaderIsElectedOrChanged(zkUtils, topic1, partition)) ```  Brokers keep rolling restart, and producer keep sending messages; In every loop, it will wait for election of partition leader; But if the election is slow, more messages will be buffered in RecordAccumulator's BufferPool; The limit for buffer is set to be 30000; TimeoutException(""Failed to allocate memory within the configured max blocking time"") will show up when out of memory; Since for every restart of the broker, it will sleep for 2000 ms,  so this transient failure seldom happen; But if I reduce the sleeping period, the bigger chance failure happens;  for example if the broker with role of controller suffered a restart, it will take time to select controller first, then select leader, which will lead to more messges blocked in KafkaProducer:RecordAccumulator:BufferPool; In this fix, I just enlarge the producer's buffer size to be 1MB; @guozhangwang , Could you give some comments?","closed","","jinxing64","2015-12-09T14:04:05Z","2015-12-14T20:16:24Z"
"","1148","KAFKA-3384: Conform to POSIX kill usage","I believe this addresses KAFKA-3384.  The POSIX kill manpage is at http://pubs.opengroup.org/onlinepubs/9699919799/utilities/kill.html","closed","","matthewlmcclure","2016-03-28T01:08:59Z","2016-04-05T05:08:06Z"
"","602","Trunk kafka 2839","I believe there is a bug in KafkaBasedLogTest implementation; Records are appended to ""KafkaBasedLog"" by : consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0, TP0_KEY, TP0_VALUE)); consumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 1, TP0_KEY, TP0_VALUE_NEW)); consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 0, TP1_KEY, TP1_VALUE)); consumer.addRecord(new ConsumerRecord<>(TOPIC, 1, 1, TP1_KEY, TP1_VALUE_NEW)); and achieved by ""consumedRecords""; Inside of ""ConsumerRecords"", in order to turn ""records"",  which is a HashMap[TopicPartition, List[ConsumerRecord[K, V]]],  into Iterator[ConsumerRecord[K, V]]; We can just guarantee the sequence underneath same key, while we cannot assure the sequence under different keys; Am I wrong? @gwenshap","closed","","jinxing64","2015-11-30T03:53:34Z","2015-12-09T10:30:55Z"
"","1072","MINOR: KAFKA-3373 follow-up, a few val renames remaining","I also slightly tweaked the wording on a couple of warnings.","closed","","ijuma","2016-03-15T12:21:43Z","2016-03-17T21:01:28Z"
"","1172","KAFKA-3489; Update request metrics if a client closes a connection while the broker response is in flight","I also fixed a few issues in `SocketServerTest` and included a few clean-ups.","closed","","ijuma","2016-04-01T12:03:56Z","2016-04-05T22:17:16Z"
"","1135","[KAFKA-3458] Selector should throw InterruptException when interrupted.","https://issues.apache.org/jira/browse/KAFKA-3458","closed","","sruehl","2016-03-24T16:30:29Z","2022-02-09T19:31:01Z"
"","1012","[KAFKA-3333] - Add RoundRobinPartitioner","https://issues.apache.org/jira/browse/KAFKA-3333  The DefaultPartitioner typically distributes using the hash of the keybytes, and falls back to round robin if there is no key. But there is currently no way to do Round Robin partitioning if you have keys on your messages without writing your own partitioning implementation.  I think it'd be helpful to have an implementation of straight Round Robin partitioning included with the library.","closed","","Crim","2016-03-04T15:15:44Z","2020-03-28T23:20:31Z"
"","718","KAFKA-3046: add ByteBuffer Serializer&Deserializer","https://issues.apache.org/jira/browse/KAFKA-3046","closed","","vesense","2015-12-29T06:12:05Z","2016-03-16T03:12:47Z"
"","751","KAFKA-3044: Re-word consumer.poll behaviour","https://issues.apache.org/jira/browse/KAFKA-3044","closed","","praveend","2016-01-10T05:02:53Z","2016-01-11T06:14:44Z"
"","668","KAFKA-2981: Fix javadoc in KafkaConsumer","https://issues.apache.org/jira/browse/KAFKA-2981","closed","","vesense","2015-12-11T10:27:35Z","2015-12-11T19:33:00Z"
"","945","MINOR: tidy up spacing for ConsumerGroupCommand related to KAFKA-1476 …","https://issues.apache.org/jira/browse/KAFKA-1476  Let me know if these kind of contributions should have their own requisite JIRA opened in advance.  Cheers..","closed","","christian-posta","2016-02-22T17:31:24Z","2016-03-08T02:30:22Z"
"","1144","KAFKA-3385: Need to log ""Rejected connection"" as WARNING message","Hi all,  This PR is related to: https://issues.apache.org/jira/browse/KAFKA-3385  Thanks,  Andrea","closed","","oscerd","2016-03-26T11:08:59Z","2016-10-11T09:53:48Z"
"","693","KAFKA-2875:  remove slf4j multi binding warnings when running form source distribution","hi @ijuma I reopened this pr again (sorry for my inexperience using github); I think I did much deduplication for the script; Please have a look when you have time  : - )","closed","","jinxing64","2015-12-18T07:28:13Z","2017-01-12T02:05:49Z"
"","669","KAFKA-2875: Class path contains multiple SLF4J bindings warnings when using scripts under bin","Hi @ijuma  I repopened this PR(the old one is https://github.com/apache/kafka/pull/595); I make slf4jlog4j12 dependency version to be 1.7.13 (latest), by this way, the 1.7.10 and some other older  transitive dependency version will be overriten I think; From my point of view, a proper way to fix the multi binding of slf4j is to specify the classpath inside of parent script of kafka-run-class.sh, like kafka-topics.sh, kafka-console-consumer.sh; As a result, kafka-run-class.sh will be told which command to run and where to find jars contains the class for the command; Am I right?","closed","","jinxing64","2015-12-11T15:03:16Z","2015-12-18T07:01:42Z"
"","1310","KAFKA-3652: Return error response for unsupported version of ApiVersionsRequest","Handle unsupported version of ApiVersionsRequest during SASL auth as well as normal operation by returning an error response.","closed","","rajinisivaram","2016-05-03T22:02:01Z","2016-05-05T01:16:35Z"
"","512","KAFKA-2807: Fix Kafka Connect packaging and move VerifiableSource/Sink into runtime jar.","Gradle does not handle subprojects with the same name (top-level tools vs connect/tools) properly, making the dependency impossible to express correctly since we need to move the ThroughputThrottler class into the top level tools project. Moving the current set of tools into the runtime jar works fine since they are only used for system tests at the moment.","closed","","ewencp","2015-11-12T18:55:57Z","2015-11-12T19:13:47Z"
"","748","MINOR: Fixes version lookup exception.","Given a schema with 2 versions (0 and 1), if you pass in a version of `2` you will get an `OutOfBoundsException` instead of an `IllegalArgumentException`.  This fixes the problem by changing the check from `>` to `>=`, which will now return true in the given scenario.","closed","","MicahZoltu","2016-01-09T10:55:57Z","2016-01-22T22:19:05Z"
"","1028","KAFKA-3123: Follower Broker cannot start if offsets are already out of range","Function abortAndPauseCleaning() doesn't have to throw exception if the state is already LogCleaningAborted or LogCleaningPaused @junrao @granthenke","closed","","soumyajit-sahu","2016-03-08T07:50:50Z","2016-11-29T20:14:54Z"
"","563","KAFKA-2642: Run replication tests with SSL and SASL clients","For SSL and SASL replication tests, set security protocol for clients as well.","closed","","rajinisivaram","2015-11-19T23:39:28Z","2015-12-02T14:08:56Z"
"","1220","KAFKA-3548: Use root locale for case transformation of constant strings","For enums and other constant strings, use locale independent case conversions to enable comparisons to work regardless of the default locale.","closed","","rajinisivaram","2016-04-14T09:11:38Z","2016-04-21T01:55:19Z"
"","756","KAFKA-3085: BrokerChangeListener computes inconsistent live/dead broker list.","Follow up PR as per comments in the ticket.  @junrao It should be correct now as `curBrokers` included only live brokers and live/dead brokers are computed based on it. Could you take a look when you have time?","closed","","dajac","2016-01-12T06:48:22Z","2020-08-11T06:48:10Z"
"","1294","HOTFIX: Fix equality semantics of KeyValue","Fixes wrong KeyValue equals logic when keys not equal but values equal.  Original hotfix PR at https://github.com/apache/kafka/pull/1293 (/cc @enothereska)  Please review: @ewencp @ijuma @guozhangwang","closed","","miguno","2016-04-29T22:00:51Z","2021-02-09T10:54:12Z"
"","608","KAFKA-2915: Fix problem with System Tests that use bootstrap.servers embedded in jinja files","Fixes problems in mirror maker and consumer tests http://jenkins.confluent.io/job/kafka_system_tests_branch_builder/290/ http://jenkins.confluent.io/job/kafka_system_tests_branch_builder/289/","closed","","benstopford","2015-12-01T18:14:44Z","2015-12-01T19:49:00Z"
"","974","MINOR: Connect status tracking API followup","Fixes from @Ishiihara's review.","closed","","hachikuji","2016-02-25T19:20:55Z","2016-02-25T21:59:39Z"
"","656","KAFKA-2928: system test: fix version sanity checks","Fixed version sanity checks by updated kafkatest version to match kafka version","closed","","granders","2015-12-10T01:45:07Z","2016-01-31T23:52:44Z"
"","1104","fix:lru  accessOrder=true","fix:lru  accessOrder=true","closed","","dingjun84","2016-03-21T07:38:50Z","2018-01-26T19:01:56Z"
"","1057","Update design.html","Fix typo.","closed","","ghost","2016-03-14T02:07:43Z","2016-10-27T00:48:07Z"
"","742","MINOR: Fix typos in Kafka website page","Fix two minor typos in Kafka official website page.","closed","","smalldirector","2016-01-08T12:45:14Z","2016-01-09T02:53:44Z"
"","1478","KAFKA-3562: Handle topic deletion during a send","Fix timing window in producer by holding onto cluster object while processing send requests so that changes to cluster during metadata refresh don't cause NPE if a topic is deleted.","closed","","rajinisivaram","2016-06-07T12:53:12Z","2016-07-11T16:32:27Z"
"","805","KAFKA-3140: Fix PatternSyntaxException and hand caused by it in Mirro…","Fix PatternSyntaxException and hand caused by it in MirrorMaker on passing invalid java regex string as whitelist","closed","","SinghAsDev","2016-01-22T22:23:02Z","2016-01-23T00:25:05Z"
"","1124","HOTFIX: fix NPE in changelogger","Fix NPE in StoreChangeLogger caused by a record out of window retention period. @guozhangwang","closed","","ymatsuda","2016-03-23T21:20:21Z","2016-03-23T21:25:35Z"
"","725","KAFKA-3052; Broker properties get logged twice if acl enabled","Fix it by making it possible to pass the `doLog` parameter to `AbstractConfig`. As explained in the code comments, this means that we can continue to benefit from ZK default settings as specified in `KafkaConfig` without having to duplicate code.  Also: - Removed unused private methods from `KafkaConfig` - Removed `case` modifier from `KafkaConfig` so that `hashCode`, `equals`   and `toString` from `AbstractConfig` are used. - Made `props` a `val` and added `apply` method to `KafkaConfig` to   remain binary compatible. - Call authorizer.close even if an exception is thrown during `configure`.","closed","","ijuma","2016-01-04T14:39:41Z","2016-03-01T22:54:05Z"
"","907","KAFKA-3234; Clarify minISR in documentation and auto-generate topic configuration docs","Fix design doc comments to clarify minISR and unclean leader election, switch topic configuration doc to be auto-generated.","closed","","jjkoshy","2016-02-12T02:46:24Z","2022-02-09T19:33:01Z"
"","926","gradlew.bat rat got utf-8 error","fix .\gradlew.bat rat -stacktrace got  - What went wrong:   Execution failed for task ':rat'.   > com.sun.org.apache.xerces.internal.impl.io.MalformedByteSequenceException: 1 字节的 UTF-8 序列的字节 1 无效。  blew detail  - Exception is:   org.gradle.api.tasks.TaskExecutionException: Execution failed for task ':rat'.       at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:69)       at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:46)       at org.gradle.api.internal.tasks.execution.PostExecutionAnalysisTaskExecuter.execute(PostExecutionAnalysisTaskExecuter.java:35)       at org.gradle.api.internal.tasks.execution.SkipUpToDateTaskExecuter.execute(SkipUpToDateTaskExecuter.java:64)       at org.gradle.api.internal.tasks.execution.ValidatingTaskExecuter.execute(ValidatingTaskExecuter.java:58)       at org.gradle.api.internal.tasks.execution.SkipEmptySourceFilesTaskExecuter.execute(SkipEmptySourceFilesTaskExecuter.java:52)       at org.gradle.api.internal.tasks.execution.SkipTaskWithNoActionsExecuter.execute(SkipTaskWithNoActionsExecuter.java:52)       at org.gradle.api.internal.tasks.execution.SkipOnlyIfTaskExecuter.execute(SkipOnlyIfTaskExecuter.java:53)       at org.gradle.api.internal.tasks.execution.ExecuteAtMostOnceTaskExecuter.execute(ExecuteAtMostOnceTaskExecuter.java:43)       at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:203)       at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter$EventFiringTaskWorker.execute(DefaultTaskGraphExecuter.java:185)       at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.processTask(AbstractTaskPlanExecutor.java:66)       at org.gradle.execution.taskgraph.AbstractTaskPlanExecutor$TaskExecutorWorker.run(AbstractTaskPlanExecutor.java:50)       at org.gradle.execution.taskgraph.DefaultTaskPlanExecutor.process(DefaultTaskPlanExecutor.java:25)       at org.gradle.execution.taskgraph.DefaultTaskGraphExecuter.execute(DefaultTaskGraphExecuter.java:110)       at org.gradle.execution.SelectedTaskExecutionAction.execute(SelectedTaskExecutionAction.java:37)       at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)       at org.gradle.execution.DefaultBuildExecuter.access$000(DefaultBuildExecuter.java:23)       at org.gradle.execution.DefaultBuildExecuter$1.proceed(DefaultBuildExecuter.java:43)       at org.gradle.execution.DryRunBuildExecutionAction.execute(DryRunBuildExecutionAction.java:32)       at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:37)       at org.gradle.execution.DefaultBuildExecuter.execute(DefaultBuildExecuter.java:30)       at org.gradle.initialization.DefaultGradleLauncher$4.run(DefaultGradleLauncher.java:154)       at org.gradle.internal.Factories$1.create(Factories.java:22)       at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)       at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:52)       at org.gradle.initialization.DefaultGradleLauncher.doBuildStages(DefaultGradleLauncher.java:151)       at org.gradle.initialization.DefaultGradleLauncher.access$200(DefaultGradleLauncher.java:32)       at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:99)       at org.gradle.initialization.DefaultGradleLauncher$1.create(DefaultGradleLauncher.java:93)       at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:90)       at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:62)       at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:93)       at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:82)       at org.gradle.launcher.exec.InProcessBuildActionExecuter$DefaultBuildController.run(InProcessBuildActionExecuter.java:94)       at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28)       at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35)       at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:43)       at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28)       at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75)       at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:45)       at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:52)       at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:37)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:74)       at org.gradle.launcher.daemon.server.exec.ForwardClientInput$2.call(ForwardClientInput.java:72)       at org.gradle.util.Swapper.swap(Swapper.java:38)       at org.gradle.launcher.daemon.server.exec.ForwardClientInput.execute(ForwardClientInput.java:72)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.health.DaemonHealthTracker.execute(DaemonHealthTracker.java:40)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.LogToClient.doBuild(LogToClient.java:66)       at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.EstablishBuildEnvironment.doBuild(EstablishBuildEnvironment.java:72)       at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.health.HintGCAfterBuild.execute(HintGCAfterBuild.java:41)       at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120)       at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50)       at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:246)       at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54)       at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40)   Caused by: org.gradle.api.UncheckedIOException: com.sun.org.apache.xerces.internal.impl.io.MalformedByteSequenceException: 1 字节的 UTF-8 序列的字节 1 无效。       at org.gradle.internal.UncheckedException.throwAsUncheckedException(UncheckedException.java:43)       at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:78)       at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.doExecute(AnnotationProcessingTaskFactory.java:227)       at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:220)       at org.gradle.api.internal.project.taskfactory.AnnotationProcessingTaskFactory$StandardTaskAction.execute(AnnotationProcessingTaskFactory.java:209)       at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:585)       at org.gradle.api.internal.AbstractTask$TaskActionWrapper.execute(AbstractTask.java:568)       at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeAction(ExecuteActionsTaskExecuter.java:80)       at org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:61)       ... 68 more   Caused by: com.sun.org.apache.xerces.internal.impl.io.MalformedByteSequenceException: 1 字节的 UTF-8 序列的字节 1 无效。       at com.sun.org.apache.xerces.internal.impl.io.UTF8Reader.invalidByte(UTF8Reader.java:691)       at com.sun.org.apache.xerces.internal.impl.io.UTF8Reader.read(UTF8Reader.java:557)       at com.sun.org.apache.xerces.internal.impl.XMLEntityScanner.load(XMLEntityScanner.java:1743)       at com.sun.org.apache.xerces.internal.impl.XMLEntityScanner.peekChar(XMLEntityScanner.java:490)       at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2718)       at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)       at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)       at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)       at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)       at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)       at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)       at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)       at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)       at RatTask.printUnknownFiles(D:\opensource\kafka\gradle\rat.gradle:54)       at RatTask.rat(D:\opensource\kafka\gradle\rat.gradle:88)       at org.gradle.internal.reflect.JavaMethod.invoke(JavaMethod.java:75)       ... 75 more","closed","","jiangzhx","2016-02-17T08:15:32Z","2019-01-05T09:50:11Z"
"","1262","KAFKA-3614: Consolidate duplicate code in KGroupedTableImpl","Feel free to review @guozhangwang @enothereska @mjsax .","closed","","miguno","2016-04-24T02:20:51Z","2016-04-26T18:00:18Z"
"","510","KAFKA-2815: Fix KafkaStreamingPartitionAssignorTest.testSubscription","Fails when order of elements is incorrect","closed","","granthenke","2015-11-12T16:28:10Z","2015-11-17T17:05:31Z"
"","1183","KAFKA-3488: Avoid failing of unsent requests in consumer where possible","Fail unsent requests only when returning from KafkaConsumer.poll().","closed","","rajinisivaram","2016-04-04T09:36:36Z","2016-04-07T22:49:42Z"
"","1350","KAFKA-3679 Allow reuse of implementation of RFC 4616 in PlainSaslServer","extracted authorize(username,password) from evaluateResponse(bytes)","closed","","edoardocomar","2016-05-09T11:08:20Z","2016-05-25T17:13:38Z"
"","1383","KAFKA-3710: MemoryOffsetBackingStore shutdown","ExecutorService needs to be shutdown on close, lest a zombie thread prevent clean shutdown.  @ewencp","closed","","davispw","2016-05-13T20:08:44Z","2016-05-27T04:04:00Z"
"","1278","KAFKA-3597: Query ConsoleConsumer and VerifiableProducer if they shutdown cleanly","Even if a test calls stop() on console_consumer or verifiable_producer, it is still possible that producer/consumer will not shutdown cleanly, and will be killed forcefully after a timeout. It will be useful for some tests to know whether a clean shutdown happened or not. This PR adds methods to console_consumer and verifiable_producer to query whether clean shutdown happened or not.  @hachikuji and/or @granders Please review.","closed","","apovzner","2016-04-27T23:01:21Z","2016-04-29T17:51:55Z"
"","1232","KAFKA-3517: Add documentation for SASL/PLAIN","Documentation corresponding to KIP-43 - SASL/PLAIN and multiple mechanism support.","closed","","rajinisivaram","2016-04-18T07:34:00Z","2016-05-03T16:49:52Z"
"","1035","KAFKA-3359 Parallel log-recovery of un-flushed segments on startup","Did not find any tests for the method. Will be adding them","closed","","ghost","2016-03-09T14:26:44Z","2022-02-09T19:32:08Z"
"","616","KAFKA-2929: Migrate duplicate error mapping functionality","Deprecates ErrorMapping.scala in core in favor or Errors.java in common.  Duplicated exceptions in core are deprecated as well, to ensure the mapping is correct.","closed","","granthenke","2015-12-02T19:37:49Z","2016-01-19T04:34:54Z"
"","944","KAFKA-3258: Delete broker topic metrics of deleted topics","Delete per-topic metrics when there are no replicas of any partitions of the topic on a broker.","closed","","rajinisivaram","2016-02-22T10:06:13Z","2016-05-16T13:37:40Z"
"","694","KAFKA-2455: Test Failure: kafka.consumer.MetricsTest > testMetricsLeak","DelayedFetchMetrics are loaded dynamically in function ""onExpiration""; use assertNull(DelayedFetchMetrics) to initialize DelayedFetchMetrics explicitly;","closed","","jinxing64","2015-12-18T11:37:57Z","2015-12-21T18:46:31Z"
"","954","[MINOR] Fix documentation of parameter ""block.on.buffer.full""","default value is ""false"" and not ""true""  See: https://stackoverflow.com/questions/35578519/kafka-block-on-buffer-full-default-value and https://github.com/apache/kafka/blob/d5b43b19bb06e9cdc606312c8bcf87ed267daf44/clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java#L232","closed","","mjsax","2016-02-23T14:22:29Z","2020-07-08T00:07:46Z"
"","484","KAFKA-2796 add support for reassignment partition to specified logdir","Currently when creating a log, the directory is chosen by calculating the number of partitions in each directory and then choosing the data directory with the fewest partitions.     However, the sizes of different TopicParitions are very different, which lead to usage vary greatly between different logDirs. And usually each logDir corresponds to a disk, so the disk usage between different disks is very imbalance .     The possible solution is to reassign partitions in high-usage logDirs to low-usage logDirs.  I change the format of /admin/reassign_partitions，add replicaDirs field.  At reassigning Partitions, when broker’s LogManager.createLog() is invoked , if replicaDir is specified ,  the specified logDir will be chosen, otherwise the logDir with the fewest partitions will be chosen.  the old /admin/reassign_partitions:  ```   {""version"":1,    ""partitions"":     [      {        ""topic"" : ""Foo"",        ""partition"": 1,        ""replicas"": [1, 2, 3]      }    ]   } ```  the new /admin/reassign_partitions:  ```   {""version"":1,    ""partitions"":     [      {        ""topic"" : ""Foo"",        ""partition"": 1,        ""replicas"": [1, 2, 3],        ""replicaDirs"": {""1"":""/data1/kafka_data"",  ""3"":""/data10/kakfa_data"" }      }    ]   } ```","closed","","yonghuiyang","2015-11-10T08:16:17Z","2016-12-26T22:37:58Z"
"","593","KAFKA-2899: (trivial) Log unexpected exceptions thrown when reading local log","Currently we don't log exceptions raised when reading from the local log which makes tracking down the cause of problems a bit tricky.","closed","","benstopford","2015-11-26T18:55:52Z","2015-11-27T20:15:00Z"
"","925","HOTFIX: release resources on abrupt termination of stream threads","Currently the resources, such as the state dir locks, are not release when a stream thread is abruptly terminated. `KakfaStreams.close()` does not release them for the failed threads.","closed","","ymatsuda","2016-02-17T01:29:52Z","2016-02-17T14:50:27Z"
"","1132","KAFKA-3445","Currently the property TASKS_MAX_CONFIG is not validated against nonsensical values such as 0. This patch leverages the Range.atLeast() method to ensure value is at least 1.","closed","","rnpridgeon","2016-03-24T15:16:37Z","2016-03-24T17:13:06Z"
"","1413","MINOR: set charset of Javadoc to UTF-8","Currently javadoc doesn't specify charset. This pull reqeust will set this to UTF-8.","closed","","sasakitoa","2016-05-20T09:28:27Z","2016-06-03T16:34:29Z"
"","678","MINOR: Add information to upgrade notes","Credit to Gwen for some of the text.","closed","","ijuma","2015-12-15T13:27:13Z","2016-03-01T22:53:48Z"
"","1409","KAFKA-3567 Add --security-protocol option to console consumer and producer","Creating a new pull request, because original pull request #1408 has some issues.","closed","","bharatviswa504","2016-05-20T00:03:47Z","2018-02-25T21:30:05Z"
"","1025","KAFKA-3312: Add utility offset methods to ZkUtils","Create utility getOffset(...) and updateOffset(...) methods to replace readData(...), readDataMaybeNull(...) and updatePersistentPath(...) method calls where they deal with getting or setting offsets.","closed","","vahidhashemian","2016-03-07T21:56:41Z","2018-03-07T05:45:09Z"
"","881","KAFKA-3216: ""Modifying topics"" section incorrectly says you can't change replication factor.","Correct the text that said that you can't change the replication factor of a topic.","closed","","wushujames","2016-02-07T05:46:03Z","2016-02-10T06:50:17Z"
"","840","KAFKA-2522: ConsumerGroupCommand writes error messages to STDERR instead of STDOUT","ConsumerGroupCommand sends errors and valuable output to different streams. It simplifies results parsing.  @ijuma this is reincarnation of a PR #197 - I rebased those changes on top of the current trunk  The contribution is my original work and I license the work to the project under the project's open source license.","closed","","melan","2016-01-30T05:48:23Z","2021-04-22T04:42:23Z"
"","1403","KAFKA-3722 : The PlaintextChannelBuilder should always use the DefaultPrincipalBuilder","Consider this scenario : 1) We have a Kafka Broker running on PlainText and SSL port simultaneously.  2) We try to plugin a custom principal builder using the config ""principal.builder.class"" for the request coming over the SSL port.  3) The ChannelBuilders.createPrincipalBuilder(configs) first checks if a config ""principal.builder.class"" is specified in the passed in configs and tries to use that even when it is building the instance of PrincipalBuilder for the PlainText port, when that custom principal class is only menat for SSL port.  IMO, having a DefaultPrincipalBuilder for PlainText port should be fine.","closed","","MayureshGharat","2016-05-19T01:23:53Z","2018-02-25T21:31:08Z"
"","1421","KAFKA-3680: Enable Kafka clients to run in any classloader env","Configure default classes using class objects instead of class names, enable configurable lists of classes to be specified as class objects, add tests for different classloader configurations.","closed","","rajinisivaram","2016-05-24T10:25:57Z","2016-08-23T11:41:58Z"
"","812","KAFKA-3149: Extend SASL implementation to support more mechanisms","Code changes corresponding to KIP-43 to enable review of the KIP.","closed","","rajinisivaram","2016-01-26T11:54:37Z","2016-04-26T23:57:35Z"
"","1256","KAFKA-3492: Secure quotas for authenticated users","Code associated with KIP-55 to enable secure quotas for authenticated users.","closed","","rajinisivaram","2016-04-22T14:15:14Z","2016-08-17T11:51:00Z"
"","1154","Automatically expire RecordBatches before draining","Check expiration of RecordBatches prior to draining, this behavior could avoid time out message to be sent.","closed","","SoyeeDst","2016-03-28T11:39:22Z","2016-03-30T05:02:20Z"
"","513","KAFKA-2800: Update outdated dependencies","Changes: - org.scala-lang:scala-library [2.10.5 -> 2.10.6]   - Scala 2.10.6 resolves a license incompatibility in scala.util.Sorting   - Otherwise identical to Scala 2.10.5 - org.xerial.snappy:snappy-java [1.1.1.7 -> 1.1.2]   - Fixes SnappyOutputStream.close() is not idempotent - net.jpountz.lz4:lz4 [1.2.0 -> 1.3] - junit:junit [4.11 -> 4.12] - org.easymock:easymock [3.3.1 -> 3.4] - org.powermock:powermock-api-easymock [1.6.2 -> 1.6.3] - org.powermock:powermock-module-junit4 [1.6.2 -> 1.6.3] - org.slf4j:slf4j-api [1.7.6 -> 1.7.12] - org.slf4j:slf4j-log4j12 [1.7.6 -> 1.7.12] - com.fasterxml.jackson.jaxrs:jackson-jaxrs-json-provider [2.5.4 -> 2.6.3] - com.fasterxml.jackson.core:jackson-databind [2.5.4 -> 2.6.3] - org.eclipse.jetty:jetty-server [9.2.12.v20150709 -> 9.2.14.v20151106] - org.eclipse.jetty:jetty-servlet [9.2.12.v20150709 -> 9.2.14.v20151106] - org.bouncycastle:bcpkix-jdk15on [1.52 -> 1.53] - net.sf.jopt-simple:jopt-simple [3.2 -> 4.9] - removed explicit entry for org.objenesis:objenesis:2.2 (resolved transitively)","closed","","granthenke","2015-11-12T19:25:51Z","2016-01-19T04:36:24Z"
"","1052","Changes to KafkaApis and MetadataCache based on KAFKA-2073","Changes to reduce the latency of topic metadata requests based on the PR of KAFKA-2073. This is just to review for now, please do not merge yet.  CC/ @hachikuji @ijuma","closed","","fpj","2016-03-11T19:07:58Z","2017-06-06T13:51:07Z"
"","596","Kafka 2902 streaming config use get base consumer configs","Changes made for using getBaseConsumerConfigs from StreamingConfig.getConsumerConfigs.","closed","","bbejeck","2015-11-27T22:50:52Z","2015-12-03T02:45:59Z"
"","826","KAFKA-3060: Refactor MeteredStore and RockDBStore Impl","Changes include:  1) Move logging logic from MeteredXXXStore to internal stores, and leave WindowedStore API clean by removed all internalPut/Get functions.  2) Wrap common logging behavior of InMemory and LRUCache stores into one class.  3) Fix a bug for StoreChangeLogger where byte arrays are not comparable in HashSet by using a specified RawStoreChangeLogger.  4) Add a caching layer on top of RocksDBStore with object caching, it relies on the object's equals and hashCode function to be consistent with serdes.","closed","","guozhangwang","2016-01-28T19:22:24Z","2016-02-02T00:11:43Z"
"","1303","KAFKA-3640: Reduce the latency of topic metadata requests","Changes based on KAFKA-2073 to reduce the latency of metadata requests.","closed","","fpj","2016-05-01T10:00:15Z","2016-05-03T07:15:31Z"
"","601","Fixed new consumer Javadoc typo","Changed ""an"" to ""and""","closed","","alex-sherwin","2015-11-29T23:22:51Z","2015-11-30T19:10:20Z"
"","1030","KAFKA-3352: Avoid DNS reverse lookups","By using `getHostString` (introduced in Java 7) instead of `getHostName`.","closed","","ijuma","2016-03-08T22:13:00Z","2016-03-09T22:20:30Z"
"","1384","KAFKA-3667:Improve Section 7.2 Encryption and Authentication using SSL to include proper hostname verification configuration","By default Kafka is configured to allow ssl communication without hostname verification. This docs has been amended to include instructions on how to set that up in the event clients would like to take a more conservative approach.","closed","","rnpridgeon","2016-05-13T23:41:49Z","2016-08-03T09:49:58Z"
"","1417","KAFKA-3720 : Deprecated BufferExhaustedException and also removed its use and the related sensor metric","BufferExhaustedException is no longerthrown by the new producer. Removed it from the catch clause and deprecated the exception class and removed the corresponding metrics.","closed","","MayureshGharat","2016-05-20T18:49:30Z","2020-08-02T15:18:53Z"
"","889","HOTFIX: fix NPE after standby task reassignment","Buffered records of change logs must be cleared upon reassignment of standby tasks.","closed","","ymatsuda","2016-02-09T17:36:31Z","2016-02-09T22:59:43Z"
"","784","KAFKA-2993: Calculate compression rate at close() call","Buffer is rewound before the compression rate metric is updated which results in 0 compress rate. The fix is to include a new compressRate field to record the latest compression rate in `close()` and return it to sensor","closed","","xiaotao183","2016-01-17T11:17:51Z","2016-01-18T01:15:36Z"
"","946","KAFKA-3248: AdminClient Blocks Forever in send Method","Block while in bounds of timeout   Author: Warren Green s.green.warren@gmail.com","closed","","WarrenGreen","2016-02-22T19:14:55Z","2017-04-27T02:17:44Z"
"","1302","Fix main classpath libs glob for release (fixup KAFKA-3615 regression)","bin/kafka-run-class.sh does not correctly setup the CLASSPATH in release rc2.","closed","","dpkp","2016-05-01T01:05:39Z","2016-05-01T01:54:23Z"
"","1174","MINOR: Fix BNF output for protocol arrays conataining primitives in docs","Before this patch arrays containing primitive types were not output:  ``` Metadata Request (Version: 0) => [topics] ```  After this patch the type is listed:  ``` Metadata Request (Version: 0) => [topics]       topics => STRING ```","closed","","granthenke","2016-04-01T18:02:37Z","2016-04-01T20:56:38Z"
"","1043","MINOR: Add header and footer to protocol docs","Because protocol.html is going to be in its own page it needs the header and footer included.","closed","","granthenke","2016-03-10T18:03:45Z","2016-03-10T18:45:40Z"
"","495","add example to send customized message by using serializer","based on 0.8.2.2. anyone can help?","closed","","microwishing","2015-11-11T03:05:22Z","2015-11-18T23:26:30Z"
"","830","KAFKA-3166; Disable SSL client authentication for SASL_SSL security protocol (backport)","Backport of https://github.com/apache/kafka/pull/827 to 0.9.0 that only includes the essential code changes (excluded the test changes due to conflicts).","closed","","ijuma","2016-01-29T09:55:06Z","2016-03-01T22:53:27Z"
"","1046","MINOR: Increased default EC2 instance size","AWS instance size increased to m3.xlarge to allow all system tests to pass. @ijuma @ewencp have a look please.","closed","","enothereska","2016-03-10T20:10:59Z","2016-03-10T20:15:23Z"
"","1045","MINOR: Increase AWS instance size","AWS instance size increased to m3.xlarge to allow all system tests to pass.","closed","","enothereska","2016-03-10T19:53:22Z","2016-03-10T20:06:36Z"
"","1407","KAFKA-3733 Avoid long command lines by setting CLASSPATH in environment","Avoid trimming the command line","closed","","amuraru","2016-05-19T21:53:56Z","2021-10-29T11:17:11Z"
"","1130","Update Sender.java","Avoid expired RecordBatch to be sent out to network","closed","","SoyeeDst","2016-03-24T11:19:17Z","2016-03-28T11:27:02Z"
"","714","MINOR: Tiny optimization to avoid mapping twice on the results.","As pointed out by @granthenke in #196, error can be wrapped in JShort directly to avoid second map.","closed","","dajac","2015-12-22T21:05:44Z","2020-08-11T06:47:39Z"
"","985","MINOR: add AUTO_OFFSET_RESET_CONFIG to StreamsConfig,","and remove TOTAL_RECORDS_TO_PROCESS @guozhangwang","closed","","ymatsuda","2016-02-29T22:53:49Z","2016-03-01T00:06:11Z"
"","625","Add Rolling Upgrade Notes to Security Docs","And added info about the krb5.conf file as we don't appear to mention that in the current docs","closed","","benstopford","2015-12-03T16:02:09Z","2016-01-29T04:43:12Z"
"","1166","KAFKA-3205 Support passive close by broker","An attempt to fix KAFKA-3205.  It appears the problem is that the broker has closed the connection passively, and the client should react appropriately.  In NetworkReceive.readFrom() rather than throw an EOFException (Which means the end of stream has been reached unexpectedly during input), instead return the negative bytes read signifying an acceptable end of stream.  In Selector if the channel is being passively closed, don't try to read any more data, don't try to write, and close the key.  I believe this will fix the problem.","closed","","bondj","2016-03-30T04:06:23Z","2016-04-08T22:15:47Z"
"","914","KAFKA-3235: Unclosed stream in AppInfoParser static block","Always close the stream","closed","","kichristensen","2016-02-14T20:57:23Z","2016-02-27T20:42:59Z"
"","1083","KAFKA-3378 Fix for instantly connecting SocketChannels (v2)","Alternative implementation","closed","","llowrey","2016-03-16T20:52:26Z","2016-12-26T22:37:54Z"
"","1153","MINOR: Remove a couple of redundant `CoreUtils.rm` methods","Also: - Rename remaining `CoreUtils.rm` to `delete` for consistency - Use `try with resources` in `Utils` to simplify code - Silence compiler warning due to exception catch clause in `TestUtils`","closed","","ijuma","2016-03-28T08:56:39Z","2016-03-28T21:36:02Z"
"","1265","KAFKA-3128; Add metrics for ZooKeeper events zookeeper metrics","Also: - Remove redundant `time.milliseconds` call in `Sensor.record` - Clean-up a number of tests and remove a manual test that is no longer required","closed","","ijuma","2016-04-24T20:17:22Z","2016-05-03T17:57:41Z"
"","827","KAFKA-3166; Disable SSL client authentication for SASL_SSL security protocol","Also: - Fixed a bug in `createSslConfig` where we were always generating a   keystore even if `useClientCert` was false and `mode` was `Mode.CLIENT`. - Pass `numRecords` to `consumerRecords` and other clean-ups (formatting and scaladoc).","closed","","ijuma","2016-01-28T20:18:00Z","2016-03-01T22:53:30Z"
"","1092","KAFKA-2982; Mark the old Scala producer and related classes as deprecated","Also update server tests to always use new producer.","closed","","ijuma","2016-03-18T00:47:26Z","2016-03-18T01:13:07Z"
"","963","KAFKA-3277; Update trunk version to be 0.10.0.0-SNAPSHOT","Also update `kafka-merge-pr.py` and `tests/kafkatest/__init__.py`.","closed","","ijuma","2016-02-24T06:20:30Z","2016-03-01T22:52:44Z"
"","1177","KAFKA-3495; `NetworkClient.blockingSendAndReceive` should rely on requestTimeout","Also removed the code for handling negative timeouts in `blockingReady` as `Selector.poll` has not supported that for a while.","closed","","ijuma","2016-04-02T09:58:24Z","2016-04-03T23:35:11Z"
"","992","MINOR: Move streams-exmaples source files under src folder","Also remove some unused imports.","closed","","guozhangwang","2016-03-02T00:26:15Z","2016-03-02T02:54:30Z"
"","790","MINOR: Avoid unnecessary `ConcurrentHashMap.get`","Also remove incorrect comment.","closed","","ijuma","2016-01-19T14:58:16Z","2016-03-01T22:53:40Z"
"","542","KAFKA-2847; Remove principal builder class from client configs","Also mark `PrincipalBuilder` as `Unstable` and  tweak docs.","closed","","ijuma","2015-11-17T13:04:04Z","2016-03-01T22:49:01Z"
"","1105","KAFKA-3431: Remove `o.a.k.common.BrokerEndPoint` in favour of `Node`","Also included a minor efficiency improvement in kafka.cluster.EndPoint.","closed","","ijuma","2016-03-21T09:27:48Z","2016-03-23T20:51:07Z"
"","1102","KAFKA-3431; Move `BrokerEndPoint` from `o.a.k.common` to `o.a.k.common.internals`","Also included a minor efficiency improvement in `kafka.cluster.EndPoint`.","closed","","ijuma","2016-03-20T23:29:14Z","2016-03-23T02:21:39Z"
"","1093","KAFKA-3422: Add overloading functions without serdes in Streams DSL","Also include:  1) remove streams specific configs before passing to producer and consumer to avoid warning message; 2) add `ConsumerRecord` timestamp extractor and set as the default extractor.","closed","","guozhangwang","2016-03-18T02:48:28Z","2016-03-18T19:40:15Z"
"","710","MINOR: Update `config/producer.properties` to have new producer properties","Also include some trivial clean-ups in `ProducerConfig`and `BaseProducer`.","closed","","ijuma","2015-12-22T11:41:33Z","2016-03-01T22:53:58Z"
"","1097","MINOR: Add InterfaceStability.Unstable annotations to some Kafka Streams public APIs","Also improves Java docs for the Streams high-level DSL.","closed","","guozhangwang","2016-03-18T20:36:00Z","2016-03-21T19:06:47Z"
"","828","KAFKA-2676: Fix incorrect package name in tests","Also fixed a couple of other tests with the same issue.  This is my original work and I license the work to the project under the project's open source license","closed","","kichristensen","2016-01-28T21:24:52Z","2016-02-27T20:43:12Z"
"","791","KAFKA-3122; Fix memory leak in `Sender.completeBatch` on TOPIC_AUTHORIZATION_FAILED","Also fix missing call to `sensors.record` on this error.","closed","","ijuma","2016-01-20T00:02:00Z","2016-03-01T22:53:39Z"
"","1071","MINOR: Add test that verifies fix for KAFKA-3047","Also clean-up `LogTest` a little.","closed","","ijuma","2016-03-15T11:09:32Z","2016-03-15T19:15:46Z"
"","541","KAFKA-2849; Pass the correct principal to `RequestChannel.Session`","Also change `Authorizer` to use `Principal` instead of `KafkaPrincipal` since this is what `KafkaChannel` returns. The other option would be to change `KafkaChannel` to return `KafkaPrincipal`.","closed","","ijuma","2015-11-17T11:17:06Z","2016-03-01T22:49:18Z"
"","1156","KAFKA-3508: Fix transient SimpleACLAuthorizerTest failures","Allows the the maximum retires when writing to zookeeper to be overridden in tests and sets the value to Int.MaxValue to avoid transient failure.","closed","","granthenke","2016-03-28T15:08:21Z","2016-04-05T22:18:31Z"
"","822","KAFKA-3103; Transient Failure in testIsrAfterBrokerShutDownAndJoinsBack","All three defects have the same root cause.   Root cause is ClientUtils.fetchTopicMetadata returns the BrokerEndPoints in a non-deterministic order, so we need to sort the expected endpoints and the received endpoints so we can correctly compare them.","closed","","rowdyrabbit","2016-01-28T07:50:53Z","2016-01-28T20:26:37Z"
"","1214","MINOR: Remove unused hadoop version","All dependencies on hadoop were removed with MiniKDC. This removes the left over version entry.","closed","","granthenke","2016-04-11T22:10:45Z","2016-04-12T20:54:44Z"
"","1344","Fixup KAFKA-3160: catch decompression errors in constructor","After testing KAFKA-3160 a bit more, I found that the error code was not being set properly in ProduceResponse. This happened because the validation error is raised in the CompressionFactory constructor, which was not wrapped in a try / catch.  @ijuma @junrao   (This contribution is my original work and I license the work under Apache 2.0.)","closed","","dpkp","2016-05-09T01:59:09Z","2016-05-09T17:52:10Z"
"","1002","KAFKA-3325: Out of date instructions in quickstart guide","Adjust the listeners property rather than the port.  Following the original instructions would result in all of the brokers being started with the same listeners setting, and so fail to work.","closed","","CunningBaldrick","2016-03-03T12:26:48Z","2016-03-03T19:48:44Z"
"","973","KAFKA-3286: Add plugin to quickly check for outdated dependencies","Adds a gradle task to generate a report of outdate release dependencies: `gradle dependencyUpdates`  Updates a few minor versions.","closed","","granthenke","2016-02-25T17:26:09Z","2016-03-03T23:10:40Z"
"","1353","KAFKA-3683: Add file descriptor recommendation to ops guide","Adding sizing recommendations for file descriptors to the ops guide.","closed","","cotedm","2016-05-09T17:14:25Z","2016-05-25T00:27:24Z"
"","961","KAFKA-3018: added topic name validator to ProducerRecord","Added validation for topic name when creating a `ProducerRecord`, and added corresponding tests.","closed","","choang","2016-02-24T01:22:50Z","2016-02-24T22:24:28Z"
"","695","KAFKA-3013","Added topic-partition information to the exception message on batch expiry in RecordAccumulator","closed","","MayureshGharat","2015-12-18T18:29:50Z","2016-03-15T15:18:19Z"
"","952","KAFKA-3263 - Support for markdown generation.","Added support to generate markdown from ConfigDef entries. Added test toMarkdown() to ConfigDefTest. Added toMarkdown() to ConfigDef.","closed","","jcustenborder","2016-02-23T06:53:52Z","2016-06-21T20:25:13Z"
"","1044","KAFKA-3378 Fix for instantly connecting SocketChannels.","Added OP_WRITE interestOp when channel connects instantly (socketChannel.connect(address) returns true... even in non-blocking mode). This allows the SocketChannel to be ready on the next call to select(). The poll method was modified to detect this case (OP_CONNECT && OP_WRITE while not key.isConnectable()) to complete the connection setup as if the channel had not connected instantly.","closed","","llowrey","2016-03-10T19:31:39Z","2017-12-22T01:35:55Z"
"","734","KAFKA-2934: Offset storage file configuration in Connect standalone mode is not included in StandaloneConfig","Added offsetBackingStore config to StandaloneConfig and DistributedConfig; Added config for offset.storage.topic and config.storage.topic into DistributedConfig;","closed","","jinxing64","2016-01-05T14:24:09Z","2016-03-03T16:55:38Z"
"","1361","KAFKA-3479: consumer metrics doc","added new consumer metrics section refactored common metrics into new section updated TOC","closed","","coughman","2016-05-10T20:55:22Z","2016-08-07T21:28:50Z"
"","958","KAFKA-3214: Added system tests for compressed topics","Added CompressionTest that tests 4 producers, each using a different compression type and one not using compression.  Enabled VerifiableProducer to run producers with different compression types (passed in the constructor). This includes enabling each producer to output unique values, so that the verification process in ProduceConsumeValidateTest is correct (counts acks from all producers).   Also a fix for console consumer to raise an exception if it sees the incorrect consumer output (before we swallowed an exception, so was hard to debug the issue).","closed","","apovzner","2016-02-23T22:27:16Z","2016-02-26T21:41:06Z"
"","950","KAFKA-3260 - Added SourceTask.commitRecord","Added commitRecord(SourceRecord record) to SourceTask. This method is called during the callback from producer.send() when the message has been sent successfully. Added commitTaskRecord(SourceRecord record) to WorkerSourceTask to handle calling commitRecord on the SourceTask. Updated tests for calls to commitRecord.","closed","","jcustenborder","2016-02-22T23:24:07Z","2016-03-15T21:32:53Z"
"","850","KAFKA-3191: Improve offset committing JavaDoc in KafkaConsumer","Added an example clarifying the correct way to use explicit offsets with commitSync().","closed","","kunickiaj","2016-02-02T19:31:06Z","2016-02-09T00:57:29Z"
"","1406","KAFKA-3732: Add an auto accept option to kafka-acls.sh","Added a new argument to AclCommand: --yes. When set, automatically answer yes to prompts","closed","","mimaison","2016-05-19T17:43:41Z","2018-04-18T13:32:05Z"
"","639","KAFKA-2061: Offer a --version flag to print the kafka version","Add version option to command line tools to print Kafka version","closed","","sasakitoa","2015-12-08T12:10:14Z","2018-05-25T07:47:55Z"
"","776","KAFKA-3095: No documentation on format of sasl.kerberos.principal.to.local.rules","Add some basic documentation about the format, a link to get more detailed information and an example usage.  I didn't want to make a huge section on the format since it documented elsewhere but I can expand is folks want.  https://issues.apache.org/jira/browse/KAFKA-3095","closed","","tgravescs","2016-01-14T20:57:24Z","2016-01-19T14:12:07Z"
"","1239","KAFKA-3579 - Update reference to the outdated consumer property","Add references to the new consumer property 'max.partition.fetch.bytes' along with the old consumer property 'fetch.message.max.bytes' in the corresponding warning messages of TopicCommand. Also, create and leverage a static variable for the default value of the new consumer property. Also, use 'DEFAULT_...' for default propoerty constant names in the code instead of '..._DEFAULT'.","closed","","vahidhashemian","2016-04-19T18:45:36Z","2016-05-08T22:07:22Z"
"","1290","KAFKA-3634: Upgrade tests for SASL authentication","Add a test for changing SASL mechanism using rolling upgrade and a test for rolling upgrade from 0.9.0.x to 0.10.0 with SASL/GSSAPI.","closed","","rajinisivaram","2016-04-29T17:57:53Z","2016-05-09T22:48:10Z"
"","1431","KAFKA-3754 Add GC log retention policy to limit size of log","Add a default log retention policy to keep GC logs from growing too large","closed","","rnpridgeon","2016-05-25T21:49:13Z","2017-05-03T09:52:53Z"
"","1326","KAFKA-3661: fix NPE in o.a.k.c.c.RoundRobinAssignor when topic metadata not found","AbstractPartitionAssignor.assign has an ambiguous line in its documentation:  > @param partitionsPerTopic The number of partitions for each subscribed topic (may be empty for some topics)  Does empty mean the topic has an entry with value zero, or that the entry is excluded from the map altogether? The current implementation in AbstractPartitionAssignor excludes the entry from partitionsPerTopic if the topic isn't in the metadata.  RoundRobinAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value. RangeAssignor interprets emptiness as excluding the entry from the map. RangeAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value.  This implementation chooses to solve the NPE by deciding to exclude topics from partitionsPerTopic when the topic is not in the metadata.","closed","","onurkaraman","2016-05-05T18:15:38Z","2016-05-05T22:33:01Z"
"","1309","Minor fixes to AWS init script for testing","A path was wrong in the script and in the documentation.","closed","","theduderog","2016-05-03T18:41:40Z","2016-05-05T02:25:31Z"
"","1269","KAFKA-3622: Use descriptive error message if port number is missing from url","A new exception is thrown in _parseAndValidateAddresses_ method when the port number is missing from the url.   The change was not trivial as the previous implementation of _getHost_ and _getPort_ both returned _null_ if ex. the port number was missing from the url. To fix this behaviour I added some extra test cases and examples to the unit tests with the expected behaviour and modified the _HOST_PORT_PATTERN_ regex and the above mentioned methods.","closed","","peterableda","2016-04-26T11:14:11Z","2022-02-09T19:29:51Z"
"","932","KAFKA-2832: Add a consumer config option to exclude internal topics","A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers. The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).  This patch is co-authored with @rajinisivaram.","closed","","vahidhashemian","2016-02-18T16:26:57Z","2016-09-26T19:56:10Z"
"","1082","KAFKA-2832: Add a consumer config option to exclude internal topics","A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers. The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).  This patch is co-authored with @rajinisivaram @edoardocomar @mimaison","closed","","edoardocomar","2016-03-16T15:03:34Z","2016-09-26T19:56:56Z"
"","552","KAFKA-2421: Upgrade LZ4 to version 1.3","A few notes on the added test: - I verified this test fails when changing between snappy 1.1.1.2 and 1.1.1.7 (per KAFKA-2189) - The hard coded numbers are passing before and after lzo change","closed","","granthenke","2015-11-18T17:59:25Z","2016-01-19T04:36:07Z"
"","908","MINOR: Reconcile differences in .bat & .sh start scripts","A few minor fixes to reconcile differences between the windows and unix versions of the kafka/zookeeper start scripts that were causing cross-platform inconsistencies during deployment. - Resolve differences in CLASSPATH setup between .bat and .sh start scripts - .bat start scripts honor externally provided KAFKA_HEAP_OPTS and KAFKA_LOG4J_OPTS consistent with .sh - .bat start scripts configure log4j similar to .sh","closed","","fluetm","2016-02-12T17:32:33Z","2016-02-16T20:24:15Z"
"","652","KAFKA-2974; `==` is used incorrectly in a few places in Java code","A few issues found via static analysis.","closed","","ijuma","2015-12-09T18:34:45Z","2016-03-01T22:53:53Z"
"","995","MINOR: use Vector instead of List","`Vector#:+` is more efficient rather than `List#++` in this case.","closed","","xuwei-k","2016-03-02T06:41:56Z","2019-06-13T23:31:36Z"
"","506","MINOR: Remove `InvalidReceiveException` catch in `SocketServer`","`Selector.poll` no longer throws it.","closed","","ijuma","2015-11-12T10:19:57Z","2016-03-01T22:48:04Z"
"","1449","MINOR: Avoid trace logging computation in `checkEnoughReplicasReachOffset`","`numAcks` is only used in the `trace` logging statement so it should be a `def` instead of a `val`. Also took the chance to improve the code and documentation a little.","closed","","ijuma","2016-05-30T09:40:29Z","2016-05-31T16:04:08Z"
"","1304","KAFKA-3648; maxTimeToBlock in BufferPool.allocate should be enforced","`maxTimeToBlock` needs to be updated in each loop iteration. Also record waitTime before throwing `TimeoutException`","closed","","zhuchen1018","2016-05-03T00:10:31Z","2016-05-03T23:18:46Z"
"","1088","KAFKA-3414; Return of MetadataCache.getAliveBrokers should not be mutated by cache updates","`Map.values` returns `DefaultValuesIterable` where the default implementation of `toSeq` is (sadly) `toStream`. `Stream` is a lazy collection and it can reflect changes to the underlying map before it's `forced`.  I verified that the test failed before my change.","closed","","ijuma","2016-03-17T13:22:49Z","2016-03-17T20:56:52Z"
"","1209","KAFKA-3338 [Kafka Streams] : Add print and writeAsText to KStream/KTable","``` Addresses comments from previous PR [#1187] Changed print and writeAsText method return signature to void Flush System.out on close Changed IllegalStateException to TopologyBuilderException Updated MockProcessorContext.topic method to return a String Renamed KStreamPrinter to KeyValuePrinter Updated the printing of null keys to 'null' to match ConsoleConsumer Updated JavaDoc stating need to override toString ```","closed","","bbejeck","2016-04-09T01:13:19Z","2016-04-11T00:44:22Z"
"","1244","MINOR: Docs for ACLs over SSL auth and KAFKA_OPTS","[Motivation](http://mail-archives.apache.org/mod_mbox/kafka-users/201604.mbox/%3CCAFXAVc4%2B6Z863K1%2B-2h1aFaAjnMb3jjCu_A9RvWdVnHZg1s1SQ%40mail.gmail.com%3E).  Adds two distinct notes to the Security documentation, that seek to explain how Kerberos JAAS configurations can be used with Kafka's inbuilt command line tools (Console Producer/Consumers as examples) via `$KAFKA_OPTS`, and a better example of how the User principals for ACLs should appear in the `kafka-acls.sh` commands when Kafka is set to use client authentication via client SSL certificates instead of Kerberos+SASL.  This contribution is my original work and I license the work to the project under the project's open source license.","closed","","QwertyManiac","2016-04-20T19:03:44Z","2022-02-09T19:30:08Z"
"","1445","KAFKA-3768: Replace all pattern match on boolean value by if/else block.","[KAFKA-3768](https://issues.apache.org/jira/browse/KAFKA-3768)","closed","","satendrakumar","2016-05-29T12:27:57Z","2016-06-05T05:56:16Z"
"","1432","[KAFKA-3756] [Javadoc] Improving Javadoc","[KAFKA-3756] [Javadoc] Improving Javadoc","closed","","rekhajoshm","2016-05-25T23:20:11Z","2016-05-29T17:13:56Z"
"","1193","KAFKA-3512: Added foreach operator","@miguno @guozhangwang please have a look if you can.","closed","","enothereska","2016-04-06T13:40:20Z","2016-04-08T16:17:32Z"
"","1426","KAFKA-3760: Set broker state as running after publishing to ZooKeeper","@junrao   Currently, the broker state is set to running before it registers itself in ZooKeeper.  This is too early in the broker lifecycle.  If clients use the broker state as an indicator that the broker is ready to accept requests, they will get errors.  This change is to delay setting the broker state to running until it's registered in ZK.","closed","","theduderog","2016-05-25T01:43:45Z","2016-05-26T23:39:31Z"
"","1236","KAFKA-3582: Remove references to Copcyat from Kafka Connect property files","@junrao","closed","","Ishiihara","2016-04-19T00:44:25Z","2016-04-19T01:49:54Z"
"","1168","KAFKA-1981 Make log compaction point configurable","@jkreps   Implements control over the portion of the head of the log that will not be compacted (i.e. preserved in detail).   The log cleaner can be configured retain a minimum amount of the uncompacted ""head"" of the log.  This is enabled by setting one or more of the compaction lags:  ``` log.cleaner.min.compaction.lag.ms log.cleaner.min.compaction.lag.bytes log.cleaner.min.compaction.lag.messages ```  with similar per topic configurations:  ``` min.compaction.lag.ms min.compaction.lag.bytes min.compaction.lag.messages ```  These can be used to set constraints on the minimum message age, aggregate size, and/or count respectively that may be compacted. If none are set, all log segments are eligible for compaction except for the last segment, i.e. the one currently being written to. The active segment will not be compacted even if all of the compaction lag constraints are satisfied.  In particular this allows for the example use case described in the JIRA: ""any consumer that is no more than 1 hour behind will get every message.""  This contribution is my (Eric Wasserman's) original work and I license the work to the Kafka project under its open source license.","closed","","ewasserman","2016-03-31T00:02:01Z","2016-08-16T17:25:33Z"
"","1474","KAFKA-3748: Add consumer-property to console tools consumer","@ijuma @harshach @edoardocomar Can you please review the changes.  @edoardocomar I have addressed your comment of extra space.","closed","","bharatviswa504","2016-06-06T18:11:05Z","2016-06-07T06:59:20Z"
"","804","KAFKA-3068: Keep track of bootstrap nodes instead of all nodes ever seen","@ijuma @ewencp want to have a look please? Thanks.","closed","","enothereska","2016-01-22T20:37:57Z","2016-01-28T15:19:09Z"
"","1408","Kafka 3567:Add --security-protocol option to console consumer and producer","@harshach Please review the code changes","closed","","bharatviswa504","2016-05-19T23:37:18Z","2016-05-20T02:04:15Z"
"","1248","KAFKA-3459: Returning zero task configurations from a connector does not properly clean up existing tasks","@hachikuji @ewencp Can you take a look when you have time?","closed","","Ishiihara","2016-04-21T00:54:12Z","2016-04-29T21:50:00Z"
"","880","KAFKA-3190 Producer should not fire callback in Send() method","@guozhangwang Would you mind take a look? This was originally introduced in KAFKA-1260. I did not find specific reason in the original rb that we have to handle ApiException separately in callback.","closed","","becketqin","2016-02-05T23:50:01Z","2022-02-09T19:33:25Z"
"","1063","[MINOR]: Remove unused method, redundant in interface definition and add final for object used in sychronization","@guozhangwang Very minor cleanup.","closed","","Ishiihara","2016-03-14T21:52:58Z","2016-03-14T22:10:18Z"
"","978","KAFKA-3278 concatenate thread name to clientId when producer and consumers config is created","@guozhangwang made the changes as requested, I reverted my original commit and that seems to have closed the other pull request - sorry if that mucks up the process a bit","closed","","tomdearman","2016-02-26T10:37:16Z","2016-02-26T23:01:05Z"
"","1453","KAFKA-3561: Auto create through topic for KStream aggregation and join [WIP]","@guozhangwang can you please take a look at this? It is not close to being done but i'd like some feedback to ensure i'm heading down the right path and understand what this JIRA is asking. The main things to look at are the `KStreamImpl.map(...)` method I added and the change made to `TopologyBuilder.copartitionGroups`  There is also a test, `KStreamRepartitionMappedKeyTest`, that passes with these changes.","closed","","dguy","2016-06-01T13:25:26Z","2016-06-04T15:22:21Z"
"","1081","KAFKA-3411: Streams: stop using ""job"" terminology, rename job.id to application.id","@guozhangwang @ymatsuda : please review.","closed","","miguno","2016-03-16T11:16:12Z","2016-03-17T17:42:18Z"
"","1164","MINOR: a simple benchmark for Streams","@guozhangwang @miguno","closed","","ymatsuda","2016-03-29T22:16:26Z","2016-03-30T22:02:21Z"
"","1065","MINOR: kstream/ktable counting method with default long serdes","@guozhangwang @miguno","closed","","ymatsuda","2016-03-14T22:25:20Z","2016-03-15T19:08:50Z"
"","1472","KAFKA-3561: Auto create through topic for KStream aggregation and join [WIP]","@guozhangwang @enothereska @mjsax @miguno  If you get a chance can you please take a look at this. I've done the repartitioning in the join, but it results in 2 internal topics for each join. This seems like overkill as sometimes we wouldn't need to repartition at all, others just 1 topic, and then sometimes both, but I'm not sure how we can know that.   I'd also need to implement something similar for leftJoin, but again, i'd like to see if i'm heading down the right path or if anyone has any other bright ideas.  For reference - https://github.com/apache/kafka/pull/1453 - the previous PR  Thanks for taking the time and looking forward to getting some welcome advice :-)","closed","","dguy","2016-06-05T14:59:10Z","2016-06-16T18:58:57Z"
"","1261","KAFKA-3613: Consolidate TumblingWindows and HoppingWindows into TimeWindows","@guozhangwang @enothereska @mjsax : Feel free to take a look.  Beyond the consolidation of windows, I also added further unit tests, merged `equalsTo` and `equals`, and renamed `every()` to `shiftedBy()`.","closed","","miguno","2016-04-23T20:51:57Z","2016-04-27T23:19:21Z"
"","1163","HOTFIX: RocksDBStore must clear dirty flags after flush","@guozhangwang  Without clearing the dirty flags, RocksDBStore will perform flush for every new record. This bug made the store performance painfully slower.","closed","","ymatsuda","2016-03-29T19:13:14Z","2016-03-29T20:31:26Z"
"","864","HOTFIX: temp fix for ktable look up","@guozhangwang  Temporarily disabled state store access checking.","closed","","ymatsuda","2016-02-04T18:07:40Z","2016-02-17T21:27:42Z"
"","1137","HOTFIX: set timestamp in SinkNode","@guozhangwang  Setting the timestamp in produced records in SinkNode. This forces the producer record's timestamp same as the context's timestamp.","closed","","ymatsuda","2016-03-24T22:13:24Z","2016-04-04T21:57:44Z"
"","816","MINOR: remove FilteredIterator","@guozhangwang  removing an unused class, FilteredIterator, and its test.","closed","","ymatsuda","2016-01-27T00:32:37Z","2016-02-17T21:23:14Z"
"","959","HOTFIX: fix consumer config for streams","@guozhangwang  My bad. I removed ZOOKEEPER_CONNECT_CONFIG from consumer's config by mistake. It is needed by our own partition assigner running in consumers.","closed","","ymatsuda","2016-02-23T22:50:51Z","2016-02-23T22:55:46Z"
"","535","MINOR: do not create a StandbyTask if there is no state store in the task","@guozhangwang  An optimization which may reduce unnecessary poll for standby tasks.","closed","","ymatsuda","2015-11-16T21:40:28Z","2015-12-07T17:33:22Z"
"","726","KAFKA-3016: phase-1. A local store for join window","@guozhangwang  An implementation of local store for join window. This implementation uses ""rolling"" of RocksDB instances for timestamp based truncation.","closed","","ymatsuda","2016-01-04T17:50:00Z","2016-01-15T15:53:17Z"
"","543","MINOR: remove the group id from a restore consumer","@guozhangwang  A restore consumer does not belong to a consumer group.","closed","","ymatsuda","2015-11-17T17:47:39Z","2015-12-07T17:33:07Z"
"","653","HOTFIX: fix table-table outer join and left join. more tests","@guozhangwang  - fixed bugs in table-table outer/left joins - added more tests","closed","","ymatsuda","2015-12-09T21:34:26Z","2016-01-15T15:54:49Z"
"","635","HOTFIX: fix ProcessorStateManager to use correct ktable partitions","@guozhangwang  - fix ProcessorStateManager to use correct ktable partitions - more ktable tests","closed","","ymatsuda","2015-12-07T17:31:07Z","2015-12-10T17:58:33Z"
"","604","KAFKA-2856: add ktable","@guozhangwang  - added KTable API and impl - added standby support for KTable","closed","","ymatsuda","2015-11-30T18:05:56Z","2015-12-10T17:59:15Z"
"","526","KAFKA-2811: add standby tasks","@guozhangwang  - added a new config param ""num.standby.replicas"" (the default value is 0). - added a new abstract class AbstractTask - added StandbyTask as a subclass of AbstractTask - modified StreamTask to a subclass of AbstractTask - StreamThread   - standby tasks are created by calling StreamThread.addStandbyTask() from onPartitionsAssigned()   - standby tasks are destroyed by calling StreamThread.removeStandbyTasks() from onPartitionRevoked()   - In addStandbyTasks(), change log partitions are assigned to restoreConsumer.   - In removeStandByTasks(), change log partitions are removed from restoreConsumer.   - StreamThread polls change log records using restoreConsumer in the runLoop with timeout=0.   - If records are returned, StreamThread calls StandbyTask.update and pass records to each standby tasks.","closed","","ymatsuda","2015-11-13T19:23:49Z","2015-12-07T17:33:44Z"
"","966","MINOR: enhance streams system test","@guozhangwang  - add table aggregate to the system test - actually create change log partition replica","closed","","ymatsuda","2016-02-24T15:03:36Z","2016-02-25T02:11:59Z"
"","661","MINOR: test ktable state store creation","@guozhangwang  - a test for ktable state store creation","closed","","ymatsuda","2015-12-10T19:23:47Z","2016-01-15T15:55:06Z"
"","497","KAFKA-2763: better stream task assignment","@guozhangwang   When the rebalance happens each consumer reports the following information to the coordinator. - Client UUID (a unique id assigned to an instance of KafkaStreaming)  - Task ids of previously running tasks - Task ids of valid local states on the client's state directory  TaskAssignor does the following - Assign a task to a client which was running it previously. If there is no such client, assign a task to a client which has its valid local state. - Try to balance the load among stream threads.   - A client may have more than one stream threads. The assignor tries to assign tasks to a client proportionally to the number of threads.","closed","","ymatsuda","2015-11-11T18:02:19Z","2015-12-07T17:34:00Z"
"","779","KAFKA-3108: custom StreamParitioner for Windowed key","@guozhangwang   When `WindowedSerializer` is specified in `to(...)` or `through(...)` for a key, we use `WindowedStreamPartitioner`.","closed","","ymatsuda","2016-01-14T23:23:07Z","2016-01-15T01:20:30Z"
"","1161","HOTFIX: stop using batch write to rocksdb","@guozhangwang   Removing batch write to RocksDB because I observed segmentation faults in RocksDB's batch write.","closed","","ymatsuda","2016-03-28T22:06:43Z","2016-03-28T23:51:35Z"
"","672","KAFKA-2984: ktable sends old values when required","@guozhangwang   At DAG level, `KTable` sends (key, (new value, old value)) to down stream.  This is done by wrapping the new value and the old value in an instance of `Change` class and sending it as a ""value"" part of the stream. The old value is omitted (set to null) by default for optimization. When any downstream processor needs to use the old value, the framework should enable it (see `KTableImpl.enableSendingOldValues()` and implementations of `KTableProcessorSupplier.enableSensingOldValues()`).  NOTE: This is meant to be used by aggregation. But, if there is a use case like a SQL database trigger, we can add a new KTable method to expose this.","closed","","ymatsuda","2015-12-14T18:26:00Z","2016-01-15T15:55:23Z"
"","536","MINOR: add KStream merge operator","@guozhangwang   Added KStreamBuilder.merge(KStream...).","closed","","ymatsuda","2015-11-16T21:59:31Z","2017-01-07T03:54:51Z"
"","886","HOTFIX: open window segments on init","@guozhangwang   A window store should open all existing segments. This is important for segment cleanup, and it also ensures that the first fetch() call returns the hits, the values in the search range. (previously, it missed the hits in fetch() immediately after initialization).","closed","","ymatsuda","2016-02-08T21:40:46Z","2016-02-09T17:29:55Z"
"","1272","KAFKA-3629 KStreamImpl.to(...) throws NPE when the value SerDe is null","@guozhangwang","closed","","dguy","2016-04-27T07:18:14Z","2016-04-27T17:50:48Z"
"","1176","MINOR: small code optimizations in streams","@guozhangwang","closed","","ymatsuda","2016-04-01T21:51:55Z","2016-04-02T00:15:00Z"
"","1125","MINOR: remove streams-smoke-test.sh","@guozhangwang","closed","","ymatsuda","2016-03-23T21:48:37Z","2016-03-23T21:57:27Z"
"","1062","KAFKA-3395: prefix job id to internal topic names","@guozhangwang","closed","","ymatsuda","2016-03-14T20:16:23Z","2016-03-14T21:50:50Z"
"","948","KAFKA-3245: config for changelog replication factor","@guozhangwang","closed","","ymatsuda","2016-02-22T21:28:36Z","2016-02-23T22:54:40Z"
"","947","HOTFIX: check offset limits in streamtask when recovering KTable store","@guozhangwang","closed","","ymatsuda","2016-02-22T21:25:34Z","2016-02-23T05:37:49Z"
"","912","KAFKA-3133: Add putIfAbsent function to KeyValueStore","@guozhangwang","closed","","kichristensen","2016-02-14T18:41:41Z","2016-02-29T20:46:43Z"
"","887","HOTFIX: RecordCollector should send a record to the specified partition","@guozhangwang","closed","","ymatsuda","2016-02-08T22:52:27Z","2016-02-09T17:29:58Z"
"","873","MINOR: more info in error msg","@guozhangwang","closed","","ymatsuda","2016-02-04T22:59:42Z","2016-02-09T17:29:47Z"
"","844","MINOR: some javadocs for kstream public api","@guozhangwang","closed","","ymatsuda","2016-02-01T20:50:25Z","2016-02-17T21:28:39Z"
"","843","MINOR: removed obsolete class","@guozhangwang","closed","","ymatsuda","2016-02-01T18:48:05Z","2016-02-17T21:23:42Z"
"","834","MINOR: remove the init method from Serdes","@guozhangwang","closed","","ymatsuda","2016-01-29T17:17:33Z","2016-02-17T21:22:56Z"
"","814","MINOR: join test for windowed keys","@guozhangwang","closed","","ymatsuda","2016-01-26T22:01:59Z","2016-02-17T21:24:00Z"
"","808","MINOR: add equals and hashCode to Windowed","@guozhangwang","closed","","ymatsuda","2016-01-25T21:38:05Z","2016-02-17T21:23:55Z"
"","794","KAFKA-3153: KStream,Type and Serialization","@guozhangwang","closed","","ymatsuda","2016-01-20T20:28:08Z","2016-02-17T16:26:25Z"
"","737","KAFKA-3016: phase-2. stream join implementations","@guozhangwang","closed","","ymatsuda","2016-01-06T17:42:34Z","2016-01-15T15:53:14Z"
"","680","MINOR: StreamThread performance optimization","@guozhangwang","closed","","ymatsuda","2015-12-15T22:38:05Z","2016-01-15T15:52:30Z"
"","644","KAFKA-2962: stream-table table-table joins","@guozhangwang","closed","","ymatsuda","2015-12-08T23:42:20Z","2015-12-10T17:59:23Z"
"","591","MINOR: comments on KStream methods, and fix generics","@guozhangwang","closed","","ymatsuda","2015-11-25T23:42:51Z","2015-12-07T17:34:50Z"
"","589","MINOR: initialize Serdes with ProcessorContext","@guozhangwang","closed","","ymatsuda","2015-11-25T22:30:00Z","2015-12-07T17:32:13Z"
"","587","MINOR: change KStream processor names","@guozhangwang","closed","","ymatsuda","2015-11-25T21:19:04Z","2015-12-07T17:32:50Z"
"","586","HOTFIX: fix StreamTask.close()","@guozhangwang","closed","","ymatsuda","2015-11-25T19:56:29Z","2015-12-07T17:31:47Z"
"","629","TEST: remove checkMaybeGetRemainingTime in KafkaProducer","@granders For testing in EC2, 3 brokers, remote producer, message-size = 100bytes","closed","","guozhangwang","2015-12-04T22:51:55Z","2015-12-07T18:16:50Z"
"","1263","KAFKA-3615: Exclude test jars in kafka-run-class.sh","@granders @hachikuji Can you take a look when you have time? Appreciate your time to review.","closed","","Ishiihara","2016-04-24T05:47:39Z","2016-04-29T17:33:11Z"
"","1252","KAFKA-3606: Traverse CLASSPATH during herder start","@ewencp Can you take a quick look?","closed","","Ishiihara","2016-04-21T23:47:41Z","2016-04-22T00:59:51Z"
"","823","KAFKA-3068: Remove retry with nodesEverSeen","@ewencp @ijuma if this looks good please merge when you can. Thanks.","closed","","enothereska","2016-01-28T16:00:10Z","2016-02-03T10:08:50Z"
"","1227","KAFKA-3421: Update docs with new connector features","@ewencp @gwenshap Docs. I also tried to clean up some typos. However, it seems that the we don't have two words without space in between in the source yet they showed up as no space in between in the generated doc.","closed","","Ishiihara","2016-04-15T17:06:09Z","2016-04-19T18:07:11Z"
"","1096","HOTFIX: Renamed tests to match expected suffix","@ewencp @gwenshap @granders could you have a look please? Thanks.","closed","","enothereska","2016-03-18T17:55:12Z","2016-03-18T19:02:24Z"
"","1259","KAFKA-3611: Remove warnings when using reflections","@ewencp @granders Can you take a look? Thanks!","closed","","Ishiihara","2016-04-23T04:54:06Z","2016-04-28T18:59:39Z"
"","801","MINOR: vagrant aws overrideable ec2 instance name prefix","@ewencp  This small change allows users to use Vagrantfile.local to specify a custom prefix for names of ec2 instances brought up with vagrant.  This makes management of multiple aws test clusters a little easier since individual clusters can be assigned different name prefixes.  if `ec2_instance_name_prefix` is not specified in `Vagrantfile.local`, behavior will be exactly the same as before this change.  Testing: - aws: I verified worker nodes, broker nodes, zk nodes with and without the prefix override. Behavior is as expected - locally: I verified that bringing up worker nodes, broker nodes, zk nodes on a local machine is not impacted by this change.","closed","","granders","2016-01-22T00:16:04Z","2016-01-31T23:52:42Z"
"","1307","MINOR: add logfilename to error message when file missing","@ewencp","closed","","norwood","2016-05-03T17:05:10Z","2016-05-03T17:23:05Z"
"","1243","Minor comment fix","@ewencp","closed","","Ishiihara","2016-04-20T19:01:13Z","2016-04-21T00:51:19Z"
"","1195","KAFKA-3520: Add system tests for REST APIs of list connector plugins and config validation","@ewen @granders Ready for review.","closed","","Ishiihara","2016-04-06T22:01:26Z","2016-05-13T01:19:32Z"
"","508","[WIP] Support multiple DNS entries for a given host in `ClientUtils`","@edenhill asked for this and it turns out to be really easy. Probably something to discuss after 0.9.0.0.","closed","","ijuma","2015-11-12T11:43:11Z","2016-02-22T17:05:51Z"
"","969","KAFKA-3257: disable bootstrap-test-env.sh --colour option","@becketqin, when you get a chance, could you take a look at the patch?","closed","","zhuchen1018","2016-02-25T04:40:01Z","2016-03-02T07:54:08Z"
"","1050","KAFKA-3383: remove in flight request only after response parsing succeeds","@becketqin, could you take a look at the patch?","closed","","zhuchen1018","2016-03-11T04:01:21Z","2016-05-03T21:11:29Z"
"","1051","KAFKA-3371: ClientCompatibilityTest system test failing","@becketqin have a look if this looks reasonable to you. Thanks.","closed","","enothereska","2016-03-11T14:40:05Z","2016-03-16T15:48:17Z"
"","1009","KAFKA-3330; Truncate log cleaner offset checkpoint if the log is truncated","@becketqin Can you take a look?","closed","","lindong28","2016-03-03T23:03:42Z","2016-03-17T22:21:50Z"
"","1070","KAFKA-3202: System test that changes message version on the fly","@becketqin @apovzner please have a look. @becketqin the test fails when the producer and consumer are 0.9.x and the message format changes on the fly.","closed","","enothereska","2016-03-15T09:52:13Z","2016-03-17T22:38:01Z"
"","1059","KAFKA-3188: Compatibility test for old and new clients with 0.10 broker","@apovzner @becketqin please have a look if you can. Thanks.","closed","","enothereska","2016-03-14T17:29:18Z","2016-03-17T20:17:26Z"
"","1376","KAFKA-3282: Change tools to use --new-consumer by default and introduce --old-consumer","1. Made new consumer the default in console consumer 2. Added --old-consumer option to choose old consumer","closed","","arunmahadevan","2016-05-12T06:11:36Z","2016-09-25T08:15:58Z"
"","1399","KAFKA-3554 Improve ProducerPerformance test","1. Added multiple thread support. 2. Added value-bound to make compressed data more realistic. 3. Print out the producer metrics.","open","","becketqin","2016-05-17T23:28:46Z","2018-03-22T20:56:13Z"
"","909","KAFKA-3225: Method commit() of class SourceTask never invoked","1. Added a test case to prove commit() on SourceTask was not being called. 2. Added commitSourceTask() which logs potential exceptions. 3. Added after call to finishSuccessfulFlush().","closed","","jcustenborder","2016-02-12T22:07:32Z","2016-02-17T16:13:29Z"
"","1461","KAFKA-3783: Catch proper exception on path delete","- ZkClient is used for conditional path deletion and wraps `KeeperException.BadVersionException` into `ZkBadVersionException` - add unit test to `SimpleAclAuthorizerTest` to reproduce the issue and catch potential future regression","closed","","slaunay","2016-06-02T21:06:51Z","2016-06-07T02:35:29Z"
"","893","HOTFIX: poll even when all partitions are paused. handle concurrent cleanup","- We need to poll periodically even when all partitions are paused in order to respond to a possible rebalance promptly. - There is a race condition when two (or more) threads try to clean up the same state directory. One of the thread fails with FileNotFoundException. Thus the new code simply catches it and ignore.","closed","","ymatsuda","2016-02-09T23:12:45Z","2016-02-10T22:02:52Z"
"","1451","KAFKA-3771; Improving Kafka core code","- Used flatMap instead of map and flatten - Use isEmpty, NonEmpty, isDefined as appropriate - Used head, keys and keySet where appropriate - Used contains, diff and find where appropriate - Removed redundant val modifier for case class constructor - toString has no parameters, no side effect hence without () consistent usage - Removed unnecessary return , parentheses and semi colons.","closed","","rekhajoshm","2016-05-30T22:59:43Z","2016-06-06T16:45:17Z"
"","1225","KAFKA-3558; Add compression_type parameter to benchmarks in benchmark_test.py","- Use a fixed `Random` seed in `EndToEndLatency.scala` for determinism - Add `compression_type` to and remove `consumer_fetch_max_wait` from `end_to_end_latency.py`. The latter was never used. - Tweak logging of `end_to_end_latency.py` to be similar to `consumer_performance.py`. - Add `compression_type` to `benchmark_test.py` methods and add `snappy` to `matrix` annotation - Use randomly generated bytes from a restricted range for `ProducerPerformance` payload. This is a simple fix for now. It can be improved in the PR for KAFKA-3554.","closed","","ijuma","2016-04-15T13:17:24Z","2016-04-18T21:24:19Z"
"","1221","KAFKA-3558; Add compression_type parameter to benchmarks in benchmark_test.py","- Use a fixed `Random` seed in `EndToEndLatency.scala` for determinism - Add `compression_type` to and remove `consumer_fetch_max_wait` from `end_to_end_latency.py`. The latter was never used. - Tweak logging of `end_to_end_latency.py` to be similar to `consumer_performance.py`. - Add `compression_type` to `benchmark_test.py` methods and add `snappy` to `matrix` annotation - Use randomly generated bytes from a restricted range for `ProducerPerformance` payload. This is a simple fix for now. It can be improved in the PR for KAFKA-3554.","closed","","ijuma","2016-04-14T14:30:41Z","2016-04-15T13:21:57Z"
"","1219","KAFKA-3557; Update rocksdb to 4.4.1 and patch updates to snappy and slf4j","- The hope is that RocksDb 4.4.1 is more stable than 4.1.0 (occasional segfaults) and 4.2.0 (very frequent segfaults), release notes for 4.4.1: https://www.facebook.com/groups/rocksdb.dev/permalink/925995520832296/ - slf4j 1.7.21 includes thread-safety fixes: http://www.slf4j.org/news.html - snappy 1.1.2.4 includes performance improvements requested by Spark, which apply to our usage: https://github.com/xerial/snappy-java/blob/master/Milestone.md  I ran the stream tests several times and they passed every time while 4.2.0 segfaulted every time.","closed","","ijuma","2016-04-14T06:26:25Z","2016-04-15T08:23:44Z"
"","1218","MINOR: Patch version updates for snappy and slf4j","- slf4j 1.7.21 includes thread-safety fixes: http://www.slf4j.org/news.html - snappy 1.1.2.4 includes performance improvements requested by Spark: https://github.com/xerial/snappy-java/blob/master/Milestone.md","closed","","ijuma","2016-04-12T21:48:24Z","2016-04-14T06:26:56Z"
"","877","HOTFIX: fix streams issues","- RocksDBStore.putInternal should bypass logging. - StoreChangeLogger should not call context.recordCollector() when nothing to log   - This is for standby tasks. In standby task, recordCollector() throws an exception. There should be nothing to log anyway. - fixed ConcurrentModificationException in StreamThread  @guozhangwang","closed","","ymatsuda","2016-02-05T19:54:51Z","2016-02-09T17:29:51Z"
"","1185","KAFKA-3501: Console consumer process hangs on exit","- replace `System.exit(1)` with a regular `return` in order to release the latch blocking the shutdown hook thread from shutting down the JVM - provide `PrintStream` to the `process` method in order to ease unit testing","closed","","slaunay","2016-04-04T22:34:46Z","2016-06-07T04:35:11Z"
"","1468","KAFKA-3790: Allow for removal of non specific ACLs","- remove ACLs with `aclMatch()` rather than `Object#equals(Object)` - remove unused session argument from `aclMatch()` to reuse it in `removeAcls()` - update test case for ACL removal management - change test method `changeAclAndVerify(...)` to use an expected `Set` of ACLs rather than relying on `Object#equals(Object)`","closed","","slaunay","2016-06-03T17:48:38Z","2022-02-09T20:24:27Z"
"","890","MINOR: Stabilize transient replication test failures in 0.9.0","- ported timeout values in `produce_consume_validate.py` from trunk to 0.9.0 - ported `producer_throughput_value` in `replication_test.py` from trunk to 0.9.0 - fixed `min.insync.replicas` config, which due to an error, was not getting applied to its intended topics","closed","","granders","2016-02-09T18:04:40Z","2016-02-09T20:00:08Z"
"","1281","KAFKA-3633: Kafka Consumer API breaking backward compatibility","- Overload methods seekToBeginning(..), seekToEnd(..), pause(..), resume(..) with varargs parameter for backwards compatibility","closed","","hmcl","2016-04-28T06:53:24Z","2018-02-25T07:03:55Z"
"","1038","KAFKA-3361: Initial protocol documentation page and generation (0.9)","- Moves all generated docs under /docs/generated - Generates docs for Protocol, Errors, and ApiKeys - Adds new protocol.html page","closed","","granthenke","2016-03-10T05:49:01Z","2016-03-10T06:14:23Z"
"","970","KAFKA-3361: Initial protocol documentation page and generation","- Moves all generated docs under /docs/generated - Generates docs for Protocol, Errors, and ApiKeys - Adds new protocol.html page","closed","","granthenke","2016-02-25T07:56:18Z","2016-03-10T05:50:59Z"
"","712","KAFKA-3022: Deduplicate common project configurations","- Move testJar to subprojects config - Move CheckStyle to subprojects config - Move testLogging to subprojects config - Add testSourceJar in subprojects config - Minor cleanup","closed","","granthenke","2015-12-22T16:46:01Z","2016-01-19T04:34:56Z"
"","1188","KAFKA-3510; OffsetIndex thread safety","- Make all fields accessed outside of a lock `volatile` - Only allow mutation within the class - Remove unnecessary `AtomicInteger` since mutation always happens inside a lock","closed","","ijuma","2016-04-05T09:25:48Z","2016-04-05T20:52:14Z"
"","1017","KAFKA-3341: Improve error handling on invalid requests","- Include request id when parsing of request header fails - Don't mute selector on a connection that was closed due to an error (otherwise a second exception is thrown) - Throw appropriate exception from `ApiKeys.fromId` if invalid id is passed - Fail fast in `AbstractRequest.getRequest` if we fail to handle an instance of `ApiKeys` (if this happens, it's a programmer error and the code in `getRequest` needs to be updated)  I ran into the top two issues while trying to figure out why a connection from a producer to a broker was failing (and it made things harder than necessary). While fixing them, I noticed the third and fourth issues.","closed","","ijuma","2016-03-07T00:28:00Z","2016-03-07T19:34:35Z"
"","1462","KAFKA-3784 TimeWindows#windowsFor misidentifies some windows if TimeWindows#advanceBy is used","- Fixed the logic calculating the windows that are affected by a new …event in the case of hopping windows and a small overlap. - Added a unit test that tests for the issue","closed","","trybak","2016-06-03T00:14:10Z","2016-06-03T20:22:25Z"
"","550","MINOR: Documentation improvements","- Fix typo in api.html - Mark security features as beta quality (similar to new consumer). Is there better wording? - Improve wording and clarify things in a number of places - Improve layout of `pre` blocks (tested locally, which doesn't seem to use the same stylesheets as the deployed version) - Use producer.config in console-producer.sh command - Improve SASL documentation structure","closed","","ijuma","2015-11-18T12:37:49Z","2016-03-01T22:48:35Z"
"","1042","KAFKA-3375; Suppress deprecated warnings where reasonable and tweak compiler settings","- Fix and suppress number of unchecked warnings (except for Kafka Streams) - Add `@SafeVarargs` annotation to fix warnings - Suppress unfixable deprecation warnings - Replace deprecated by non-deprecated usage where possible - Avoid reflective calls via structural types in Scala - Tweak compiler settings for scalac and javac  Once we drop Java 7 and Scala 2.10, we can tweak the compiler settings further so that they warn us about more things.","closed","","ijuma","2016-03-10T12:05:52Z","2016-03-15T02:15:08Z"
"","891","HOTFIX: open window segments in order, add segment id check in getSegment","- During window store initialization, we have to open segments in the segment id order and update `currentSegmentId`, otherwise cleanup won't work. - `getSegment()` should not create a segment and clean up old segments if the segment id is greater than `currentSegmentId`. Segment maintenance should be driven not by query but only by data insertion.","closed","","ymatsuda","2016-02-09T18:14:11Z","2016-02-09T22:59:55Z"
"","1412","KAFKA-3736: Add HTTP Metrics reporter","- created a new metrics-reporters module - added new HTTP Metrics reporter - moved CSV reporter into the new module","open","","amuraru","2016-05-20T08:30:26Z","2018-03-02T19:29:31Z"
"","972","KAFKA-3273; MessageFormatter and MessageReader interfaces should be resilient to changes","- Change `MessageFormat.writeTo` to take a `ConsumerRecord` - Change `MessageReader.readMessage()` to use `ProducerRecord`","closed","","ijuma","2016-02-25T13:54:46Z","2016-03-01T22:44:42Z"
"","703","KAFKA-3020: Ensure CheckStyle runs on all Java code","- Adds CheckStyle to core and examples modules - Fixes any existing CheckStyle issues","closed","","granthenke","2015-12-21T19:47:02Z","2016-01-19T04:35:05Z"
"","818","KAFKA-3152; kafka-acl doesn't allow space in principal name","- Add quotes to `$@` in shell scripts   This is necessary for correct processing of quotes in the   user command. - Minor improvements to AclCommand messages - Use a principal with a space in `SslEndToEndAuthorizationTest`   This passed without any other changes, but good avoid regressions. - Clean-up `TestSslUtils`:   Remove unused methods, fix unnecessary verbosity and don't set security.protocol (it should be done at a higher-level).","closed","","ijuma","2016-01-27T11:17:48Z","2016-03-01T22:53:35Z"
"","1287","KAFKA-3440: update JavaDoc","- add class doc for KTable, KStream, JoinWindows - add missing @return tags","closed","","mjsax","2016-04-28T22:30:10Z","2016-04-29T19:50:41Z"
"","532","KAFKA-2605: Replace 'catch: Throwable' clauses with 'NonFatal'","'catch: Throwable' will catch VirtualMachineError, ThreadDeath, InterruptedException, LinkageError, ControlThrowable, NotImplementedError; we don't want to catch those kind of error","closed","","jinxing64","2015-11-15T09:50:36Z","2015-11-15T14:44:55Z"
"","531","KAFKA-2605: Replace 'catch: Throwable' clauses with 'NonFatal'","'catch: Throwable' will catch VirtualMachineError, ThreadDeath, InterruptedException, LinkageError, ControlThrowable, NotImplementedError; we don't want to catch those kind of error","closed","","jinxing64","2015-11-15T07:17:40Z","2015-11-15T09:47:56Z"
"","700","KAFKA-3009 : Disallow star imports","## Summary of code changes  1) Added a new Checkstyle rule to flag any code using star imports 2) Fixed ALL existing code violations using star imports ## Testing  Local build was successful ALL JUnits ran successfully on local.  @ewencp - Request you to please review changes. Thank you !  I state that the contribution is my original work and I license the work to the project under the project's open source license.","closed","","manasvigupta","2015-12-20T10:06:42Z","2015-12-21T21:31:49Z"
"","1476","KAFKA-3781; Errors.exceptionName() can throw NPE","","closed","","ijuma","2016-06-07T02:34:25Z","2016-06-07T03:29:20Z"
"","1475","MINOR: Add comment for round robin partitioner with different subscriptions","","closed","","Ishiihara","2016-06-06T23:52:30Z","2016-06-08T02:39:23Z"
"","1473","KAFKA-3792: Fix log spam in clients for unknown topics","","open","","rajinisivaram","2016-06-06T14:42:38Z","2018-03-02T19:29:33Z"
"","1471","MINOR: Fix producer leak in `PlaintextProducerSendTest`","","closed","","ijuma","2016-06-05T09:24:39Z","2016-06-06T02:33:29Z"
"","1467","KAFKA-3789: Upgrade Snappy to fix snappy decompression errors","","closed","","granthenke","2016-06-03T17:20:35Z","2016-06-04T22:25:27Z"
"","1465","KAFKA-3786: Let ConfigDef filter property key value pairs","","closed","","guozhangwang","2016-06-03T04:22:59Z","2017-07-15T22:08:45Z"
"","1464","KAFKA-3785; Fetcher spending unnecessary time during metrics recording","","closed","","gfodor","2016-06-03T01:48:30Z","2016-06-03T08:41:59Z"
"","1463","KAFKA-3770: KStream job should be able to specify linger.ms","","closed","","gfodor","2016-06-03T01:14:55Z","2016-08-07T21:21:10Z"
"","1459","KAFKA-3748: Add consumer-property to console tools consumer","","closed","","bharatviswa504","2016-06-01T22:40:50Z","2016-06-06T18:09:12Z"
"","1456","MINOR: Add user overridden test logging events","","closed","","guozhangwang","2016-06-01T18:33:26Z","2016-06-03T16:56:21Z"
"","1454","MINOR: added spacing in streams doc in section 9.2","","closed","","coughman","2016-06-01T15:01:53Z","2016-06-01T15:15:06Z"
"","1452","KAFKA-3773: Remove from inflightResponses on client disconnect to prevent memory leak","","closed","","hachikuji","2016-05-31T16:57:57Z","2016-06-01T00:03:23Z"
"","1450","HOTFIX: updated JavaDoc example for 0.9 tech-prev to 0.10","","closed","","mjsax","2016-05-30T16:02:38Z","2016-06-02T22:41:05Z"
"","1441","MINOR: Remove synchronized as the tasks are executed sequentially","","closed","","Ishiihara","2016-05-27T21:35:41Z","2016-05-31T00:11:48Z"
"","1440","MINOR: Fix a couple of typos","","closed","","vahidhashemian","2016-05-27T16:17:53Z","2016-05-27T20:45:42Z"
"","1439","KAFKA-3678: Removed sleep","","closed","","enothereska","2016-05-27T15:57:10Z","2016-05-28T01:40:22Z"
"","1433","delete unused code","","closed","","leisore","2016-05-26T10:12:04Z","2016-05-27T06:21:59Z"
"","1427","KAFKA-2720: expire group metadata when all offsets have expired","","closed","","hachikuji","2016-05-25T03:54:12Z","2016-06-16T16:24:21Z"
"","1423","MINOR: Fix documentation table of contents and `BLOCK_ON_BUFFER_FULL_DOC`","","closed","","ijuma","2016-05-24T11:33:44Z","2016-05-24T20:40:07Z"
"","1422","MINOR: Removed 1/2 of the hardcoded sleep times","","closed","","enothereska","2016-05-24T10:41:20Z","2016-05-25T14:14:51Z"
"","1420","KAFKA-3749, fix ""BOOSTRAP_SERVERS_DOC"" typo","","closed","","manuzhang","2016-05-24T01:55:20Z","2016-05-27T09:26:09Z"
"","1416","Specifiy keyalg RSA for SSL key generation.","","closed","","harshach","2016-05-20T16:46:56Z","2017-05-18T08:31:29Z"
"","1411","KAFKA-3735: Dispose all RocksObejcts upon completeness","","closed","","guozhangwang","2016-05-20T03:18:11Z","2017-07-15T22:07:36Z"
"","1410","KAFKA-3443 [Kafka Streams] added support for subscribing to topics via regex","","closed","","bbejeck","2016-05-20T00:51:55Z","2016-06-07T02:54:38Z"
"","1404","KAFKA-2935: Remove vestigial WorkerConfig.CLUSTER_CONFIG","","closed","","shikhar","2016-05-19T02:11:47Z","2016-05-19T16:25:41Z"
"","1402","MINOR: Catch Throwable in commitSourceTask()","","closed","","Ishiihara","2016-05-19T00:38:28Z","2016-06-14T20:21:58Z"
"","1401","KAFKA-3723 Cannot change size of schema cache for JSON converter","","closed","","christian-posta","2016-05-18T01:39:05Z","2016-05-26T21:14:34Z"
"","1400","KAFKA-3721: Put UpdateMetadataRequest V2 in 0.10.0-IV1","","closed","","becketqin","2016-05-18T01:26:02Z","2016-05-18T04:17:18Z"
"","1397","MINOR: Bump system test ducktape dependency to 0.5.1","","closed","","granders","2016-05-17T16:18:26Z","2016-05-18T21:48:22Z"
"","1393","KAFKA-3716: Validate all timestamps are not negative","","closed","","guozhangwang","2016-05-16T22:28:24Z","2017-07-15T22:07:37Z"
"","1392","MINOR: Add INFO logging if ZK config is not specified","","closed","","guozhangwang","2016-05-16T19:23:13Z","2016-05-16T21:44:53Z"
"","1387","Modifier 'public' is redundant for interface methods","","closed","","philipealves","2016-05-14T18:46:40Z","2016-05-28T14:24:08Z"
"","1381","KAFKA-3709: Create project security page","","closed","","fpj","2016-05-13T08:43:22Z","2016-05-13T09:25:42Z"
"","1375","KAFKA-3698: Update the message format section.","","closed","","becketqin","2016-05-12T04:02:59Z","2016-05-16T16:16:58Z"
"","1374","MINOR: Change type of StreamsConfig.BOOTSTRAP_SERVERS_CONFIG to List","","closed","","guozhangwang","2016-05-12T02:48:15Z","2016-05-12T14:58:35Z"
"","1373","MINOR: Fix order of compression algorithms in upgrade note","","closed","","ijuma","2016-05-12T00:19:26Z","2016-05-12T00:39:32Z"
"","1371","KAFKA-3704: Remove hard-coded block size in KafkaProducer","","closed","","guozhangwang","2016-05-11T22:24:26Z","2017-07-15T22:07:38Z"
"","1369","MINOR: Documentation for Rack Awareness","","closed","","benstopford","2016-05-11T14:25:42Z","2016-05-11T14:32:03Z"
"","1368","MINOR: Ensure that selection key is cancelled on close","","closed","","rajinisivaram","2016-05-11T12:01:07Z","2016-05-11T14:43:01Z"
"","1365","KAFKA-3694: Ensure broker Zk deregistration prior to restart in ReplicationTest","","closed","","hachikuji","2016-05-11T03:44:36Z","2016-05-11T22:49:29Z"
"","1364","KAFKA-3692: Add quotes to variables in kafka-run-class.sh","","closed","","Ishiihara","2016-05-10T22:46:41Z","2016-05-13T11:10:56Z"
"","1360","KAFKA-3690: Avoid to pass null to UnmodifiableMap","","closed","","Ishiihara","2016-05-10T17:15:33Z","2016-05-11T20:06:59Z"
"","1358","KAFKA-3583: Add documentation for Connect status control APIs","","closed","","hachikuji","2016-05-10T03:46:43Z","2016-05-12T00:19:53Z"
"","1356","KAFKA-3684: SinkConnectorConfig does not return topics in config validation.","","closed","","Ishiihara","2016-05-09T23:07:10Z","2016-05-10T00:37:49Z"
"","1355","KAFKA-3421: Follow up to fix name of SourceTask method and add documentation of connector status REST API.","","closed","","ewencp","2016-05-09T19:18:24Z","2016-05-09T22:42:40Z"
"","1354","MINOR: add toString implementations to Subscription and Assignment","","closed","","hachikuji","2016-05-09T18:49:47Z","2016-05-09T22:35:00Z"
"","1351","KAFKA-3681: connect doc formatting","","closed","","coughman","2016-05-09T13:56:12Z","2016-05-09T16:01:48Z"
"","1347","MINOR: Move connect.start() to try catch block","","closed","","Ishiihara","2016-05-09T05:01:37Z","2016-05-09T06:52:55Z"
"","1346","MINOR: Add virtual env to Kafka system test README.md","","closed","","Ishiihara","2016-05-09T04:55:54Z","2016-05-25T00:32:45Z"
"","1345","KAFKA-3676: system tests for connector pause/resume","","closed","","hachikuji","2016-05-09T03:34:15Z","2016-05-09T23:56:59Z"
"","1343","KAFKA-3675; Add lz4 to parametrized `test_upgrade` system test","","closed","","ijuma","2016-05-08T07:27:34Z","2016-05-09T07:22:52Z"
"","1342","MINOR: Cleanup Admin Package","","closed","","Ishiihara","2016-05-07T23:52:57Z","2019-01-05T09:54:42Z"
"","1341","KAFKA-3674: Ensure connector target state changes propagated to worker","","closed","","hachikuji","2016-05-07T16:57:02Z","2016-05-09T07:13:18Z"
"","1340","KAFKA-3673: Connect tests don't handle concurrent config changes","","closed","","Ishiihara","2016-05-07T03:57:37Z","2016-05-09T06:51:26Z"
"","1339","HOTFIX: Ensure connector target state changes propagated to worker","","closed","","hachikuji","2016-05-07T01:05:46Z","2016-05-07T16:51:56Z"
"","1338","KAFKA-3670; ControlledShutdownLeaderSelector should pick the preferred replica as the new leader, if possible","","closed","","ijuma","2016-05-07T01:00:51Z","2016-05-08T17:46:16Z"
"","1337","KAFKA-3658: Validate retention period be longer than window size","","closed","","guozhangwang","2016-05-06T22:29:21Z","2016-05-09T16:31:47Z"
"","1335","KAFKA 3671: Move topics to SinkConnectorConfig","","closed","","Ishiihara","2016-05-06T20:47:57Z","2016-05-09T05:27:08Z"
"","1330","KAFKA-3665; Default ssl.endpoint.identification.algorithm should be https","","closed","","ijuma","2016-05-06T10:35:19Z","2018-10-18T15:02:05Z"
"","1329","KAFKA-3587 Improve logic to build offsetMap in log.Cleaner.","","closed","","alekar","2016-05-06T08:07:16Z","2016-05-06T20:35:32Z"
"","1328","KAFKA-3587: Fix fake large log segment in log cleaner","","closed","","Ishiihara","2016-05-05T22:36:33Z","2016-05-06T18:46:32Z"
"","1327","HOTFIX: follow-up on KAFKA-725 to remove the check and return empty response instead of throw exceptions","","closed","","guozhangwang","2016-05-05T19:04:24Z","2016-05-05T23:55:54Z"
"","1325","KAFKA-3660: Log exception message in ControllerBrokerRequestBatch","","closed","","fpj","2016-05-05T12:49:37Z","2016-06-02T04:04:20Z"
"","1324","HOTFIX: Reverted timeouts to larger values","","closed","","enothereska","2016-05-05T08:35:05Z","2016-05-05T20:52:20Z"
"","1322","KAFKA-3659: Handle coordinator disconnects more gracefully in client","","closed","","hachikuji","2016-05-05T04:52:14Z","2016-05-05T19:29:10Z"
"","1321","MINOR: Add Kafka Streams API / upgrade notes","","closed","","guozhangwang","2016-05-05T03:21:11Z","2016-05-10T07:01:29Z"
"","1320","Minor: added description for lag in consumer group command","","closed","","coughman","2016-05-04T23:08:56Z","2016-05-06T20:31:56Z"
"","1319","KAFKA 3656: Remove logging outstanding messages when producer flush fails","","closed","","Ishiihara","2016-05-04T17:14:16Z","2016-05-04T23:08:42Z"
"","1318","KAFKA-2236; Offset request reply racing with segment rolling","","closed","","ijuma","2016-05-04T09:31:38Z","2017-09-05T09:42:37Z"
"","1317","MINOR: Modify checkstyle to allow import classes only used in javadoc","","closed","","guozhangwang","2016-05-04T03:49:59Z","2016-05-04T21:01:08Z"
"","1316","MINOR: Handle null values in validators.","","closed","","ewencp","2016-05-04T00:33:38Z","2016-05-04T01:27:28Z"
"","1315","KAFKA-3655; awaitFlushCompletion() in RecordAccumulator should always decrement flushesInProgress count","","closed","","zhuchen1018","2016-05-04T00:32:44Z","2016-05-06T20:58:23Z"
"","1313","KAFKA-3654: Config validation should validate both common and connector specific configurations","","closed","","Ishiihara","2016-05-03T23:46:14Z","2016-05-04T01:05:53Z"
"","1312","KAFKA-3632: remove fetcher metrics on shutdown and leader migration","","closed","","hachikuji","2016-05-03T22:29:46Z","2016-05-04T19:11:23Z"
"","1311","KAFKA-3639: Configure default serdes upon construction","","closed","","guozhangwang","2016-05-03T22:04:23Z","2016-05-04T23:24:27Z"
"","1308","KAFKA-3650: Fix vagrant download URL","","closed","","theduderog","2016-05-03T17:15:20Z","2016-05-03T17:20:55Z"
"","1306","MINOR: Clean up of ConsumerCoordinator and PartitionAssignor.","","closed","","Ishiihara","2016-05-03T06:33:38Z","2016-05-03T22:54:55Z"
"","1300","KAFKA-3527: Consumer commitAsync should not expose internal exceptions","","closed","","Ishiihara","2016-04-30T06:55:37Z","2016-05-05T02:21:32Z"
"","1295","KAFKA-3627: consumer fails to execute delayed tasks in poll when records are available","","closed","","hachikuji","2016-04-29T22:17:55Z","2016-05-06T05:24:38Z"
"","1292","KAFKA-3641: Fix RecordMetadata constructor backward compatibility","","closed","","granthenke","2016-04-29T18:34:27Z","2016-04-29T21:50:00Z"
"","1288","KAFKA-3578: Allow cross origin HTTP requests on all HTTP methods","","closed","","Ishiihara","2016-04-29T01:25:15Z","2016-04-29T17:53:27Z"
"","1285","MINOR: Added more integration tests","","closed","","enothereska","2016-04-28T18:00:30Z","2016-05-03T20:26:57Z"
"","1284","KAFKA-3636: change default max session timeout to 5 minutes","","closed","","hachikuji","2016-04-28T17:23:08Z","2016-04-28T21:24:25Z"
"","1283","KAFKA-2651; Remove deprecated config alteration from TopicCommand","","closed","","omkreddy","2016-04-28T15:46:33Z","2018-07-03T15:42:18Z"
"","1280","MINOR: Avoid compiler warnings when registering metrics in KafkaServer","","closed","","Ishiihara","2016-04-28T06:18:14Z","2016-04-28T22:40:59Z"
"","1279","KAFKA-3631: Fix Struct.toString for nullable arrayOf","","closed","","granthenke","2016-04-27T23:50:51Z","2016-04-28T00:31:50Z"
"","1275","KAFKA-3621: Add tests for ApiVersionRequest/Response","","closed","","SinghAsDev","2016-04-27T19:14:52Z","2016-04-28T03:59:18Z"
"","1274","Git ignore idea's specific /out/ and .DS_Store.","","closed","","SinghAsDev","2016-04-27T17:09:20Z","2016-04-27T17:53:19Z"
"","1266","KAKFA-3599: Move WindowStoreUtils to package ""internals""","","closed","","mjsax","2016-04-24T22:08:23Z","2016-04-26T17:55:53Z"
"","1260","KAFKA-3612: Added structure for integration tests","","closed","","enothereska","2016-04-23T17:44:54Z","2016-04-27T23:57:25Z"
"","1258","KAFKA-3607: Close KStreamTestDriver upon completing","","closed","","guozhangwang","2016-04-22T21:40:37Z","2016-04-26T18:40:23Z"
"","1257","KAFKA-3608: Fix ZooKeeper structures and output format in documentation","","closed","","vahidhashemian","2016-04-22T18:34:05Z","2016-05-10T00:36:56Z"
"","1255","MINOR: add unit test for KGroupedTable.count","","closed","","dguy","2016-04-22T11:43:28Z","2016-04-25T18:18:59Z"
"","1254","KAFKA-3602: rename RecordAccumulator dequeFor() and fix usage","","closed","","hachikuji","2016-04-22T00:34:17Z","2016-04-26T13:38:00Z"
"","1253","KAFKA-3605: Return error if connector config includes mismatching connector name.","","closed","","ewencp","2016-04-21T23:50:17Z","2016-04-22T14:09:44Z"
"","1250","KAFKA-3598: Improve JavaDoc of public API","","closed","","mjsax","2016-04-21T12:33:58Z","2016-04-29T15:50:12Z"
"","1247","KAFKA-3117: handle metadata updates during consumer rebalance","","closed","","hachikuji","2016-04-20T22:26:59Z","2016-04-21T00:08:31Z"
"","1246","KAFKA-3589: set inner serializer for ChangedSerde upon initialization","","closed","","guozhangwang","2016-04-20T22:01:20Z","2016-04-21T21:54:09Z"
"","1237","KAFKA-3580; improve error logging in ReplicaFetchThread","","closed","","omkreddy","2016-04-19T05:27:40Z","2018-07-03T15:42:17Z"
"","1234","KAFKA-3529: fix transient failure in testCommitAsync","","closed","","hachikuji","2016-04-18T18:53:55Z","2016-04-18T19:50:12Z"
"","1233","KAFKA-2955; Add a simple "">"" prompt to console producer","","closed","","omkreddy","2016-04-18T11:03:29Z","2018-07-03T15:42:17Z"
"","1231","KAFKA-3337: Extract selector as a separate groupBy operator for KTable aggregations","","closed","","mjsax","2016-04-17T14:25:09Z","2016-04-21T20:45:53Z"
"","1230","KAFKA-3406; Update CommonClientConfigs.RETRY_BACKOFF_MS_DOC doc string","","closed","","omkreddy","2016-04-16T10:29:34Z","2018-07-03T15:42:16Z"
"","1229","KAFKA-3499: prevent array typed keys in KeyValueStore","","closed","","guozhangwang","2016-04-15T23:38:02Z","2016-04-26T14:32:55Z"
"","1228","MINOR: Typo fixes in ReplicaFetchMaxBytesDoc","","closed","","Erethon","2016-04-15T20:33:08Z","2016-04-26T18:26:19Z"
"","1226","KAFKA-3563: Maintain MessageAndMetadata constructor compatibility","","closed","","granthenke","2016-04-15T15:15:45Z","2016-04-18T21:11:07Z"
"","1217","KAFKA-3549: Close consumers instantiated in consumer tests","","closed","","granthenke","2016-04-12T19:17:59Z","2016-04-15T05:02:59Z"
"","1216","MINOR: Add missing `@Override` to `KStreamImpl.through`","","closed","","ijuma","2016-04-12T16:53:25Z","2016-04-12T18:35:21Z"
"","1213","KAFKA-3439: Added exceptions thrown","","closed","","enothereska","2016-04-11T14:12:01Z","2016-04-12T20:01:26Z"
"","1211","MINOR: Update protocol doc link in Introduction.","","closed","","SinghAsDev","2016-04-09T17:41:13Z","2016-05-09T23:47:46Z"
"","1208","HOTFIX: special handling first ever triggered punctuate","","closed","","guozhangwang","2016-04-09T00:44:58Z","2016-04-11T06:10:18Z"
"","1207","MINOR: Make VerifiableProducer in system tests lookup security configuration dynamically instead of at construction.","","closed","","ewencp","2016-04-08T23:35:13Z","2016-04-11T17:37:34Z"
"","1206","KAFKA-3470: treat commits as member heartbeats","","closed","","hachikuji","2016-04-08T20:07:49Z","2016-04-14T01:09:35Z"
"","1204","KAFKA-3519: Refactor Transformer's transform / punctuate to return nullable values","","closed","","guozhangwang","2016-04-08T16:34:09Z","2016-04-11T19:34:13Z"
"","1203","KAFKA-3504: Log compaction for changelog partition","","closed","","enothereska","2016-04-08T14:01:53Z","2016-04-13T00:38:45Z"
"","1201","KAFKA-3528: handle wakeups while rebalancing more gracefully","","closed","","hachikuji","2016-04-08T00:43:22Z","2016-04-09T00:14:37Z"
"","1200","KAFKA-3526: Return string instead of object in ConfigKeyInfo and ConfigValueInfo","","closed","","Ishiihara","2016-04-07T22:42:44Z","2016-04-15T22:52:14Z"
"","1199","MINOR: improve logging of consumer system tests","","closed","","hachikuji","2016-04-07T17:02:00Z","2016-04-08T17:05:21Z"
"","1198","MINOR: Fix wrong comments","","closed","","gyk","2016-04-07T08:14:39Z","2016-05-26T17:23:50Z"
"","1197","KAFKA-3521: validate null keys in Streams DSL implementations","","closed","","guozhangwang","2016-04-07T02:21:38Z","2016-04-09T00:25:47Z"
"","1194","KAFKA-3497: Streams ProcessorContext should support forward() based on child name","","closed","","enothereska","2016-04-06T15:23:27Z","2016-04-07T17:20:46Z"
"","1191","KAFKA-3515: migrate json serde from connect to common","","closed","","guozhangwang","2016-04-06T00:13:00Z","2016-05-11T23:58:13Z"
"","1190","KAFKA-3505: Fix punctuate generated record metadata","","closed","","guozhangwang","2016-04-05T22:21:51Z","2016-04-08T16:00:29Z"
"","1189","KAFKA-3506: Kafka Connect restart APIs","","closed","","hachikuji","2016-04-05T21:58:27Z","2016-04-18T17:51:41Z"
"","1182","MINOR: Fix zk path in KafkaHealthCheck comment","","closed","","srdo","2016-04-04T08:56:52Z","2016-04-26T18:46:54Z"
"","1180","[KAFKA-3477] [Kafka Streams] extended KStream/KTable API to specify custom partitioner for sinks","","closed","","mjsax","2016-04-03T23:39:28Z","2016-04-05T22:56:35Z"
"","1175","MINOR: add null check for aggregate and reduce operators","","closed","","guozhangwang","2016-04-01T18:51:57Z","2016-04-01T20:15:18Z"
"","1173","KAFKA-3490; Multiple version support for ducktape performance tests","","closed","","ijuma","2016-04-01T17:00:32Z","2017-03-17T13:56:23Z"
"","1170","[Minor]: Clean up of SourceTaskOffsetCommiter","","closed","","Ishiihara","2016-03-31T23:24:06Z","2016-04-04T02:05:27Z"
"","1169","KAFKA-3486: fix autocommit when partitions assigned manually","","closed","","hachikuji","2016-03-31T04:39:56Z","2016-04-03T08:33:27Z"
"","1167","MINOR: Add check for empty topics iterator in ReplicaVerificationTool.","","closed","","SinghAsDev","2016-03-30T23:41:53Z","2016-04-01T21:13:21Z"
"","1160","KAFKA-3382: Add system test for ReplicationVerificationTool","","closed","","SinghAsDev","2016-03-28T21:13:48Z","2016-04-28T22:49:35Z"
"","1159","KAFKA-3425: add missing upgrade notes","","closed","","hachikuji","2016-03-28T20:23:02Z","2016-03-29T21:19:40Z"
"","1158","KAFKA-3419: clarify difference between topic subscription and partition assignment","","closed","","hachikuji","2016-03-28T19:42:10Z","2016-04-03T20:44:33Z"
"","1157","KAFKA-3177: log warning when topic/partition doesn't exist","","closed","","hachikuji","2016-03-28T17:20:36Z","2017-01-17T19:38:43Z"
"","1150","KAFKA-3474: add metrics to track replica fetcher timeouts","","closed","","junrao","2016-03-28T03:17:14Z","2022-02-09T19:30:23Z"
"","1142","0.10.0","","closed","","nikhilbhide","2016-03-25T06:44:06Z","2016-04-27T19:25:25Z"
"","1141","KAFKA-3464: Add system tests for Connect with Kafka security enabled","","closed","","ewencp","2016-03-25T01:18:40Z","2016-04-05T01:50:02Z"
"","1140","KAFKA-3463: change default receive buffer size for consumer to 64K","","closed","","hachikuji","2016-03-25T00:58:06Z","2016-03-25T19:52:22Z"
"","1139","KAFKA-3462: Allow SinkTasks to disable consumer offset commit","","closed","","Ishiihara","2016-03-25T00:37:29Z","2018-02-25T21:32:37Z"
"","1136","KAFKA-3460: Remove old 0.7 KafkaMigrationTool","","closed","","granthenke","2016-03-24T19:59:52Z","2016-03-25T17:07:55Z"
"","1129","KAFKA-3418: add javadoc section describing consumer failure detection","","closed","","hachikuji","2016-03-24T02:29:12Z","2016-04-29T17:26:50Z"
"","1127","KAFKA-3454: add Kafka Streams web docs","","closed","","guozhangwang","2016-03-23T23:08:45Z","2016-03-25T23:05:23Z"
"","1126","MINOR: Revert 0.10.0 branch to SNAPSHOT per change in release process","","closed","","gwenshap","2016-03-23T22:34:29Z","2016-03-24T05:56:32Z"
"","1123","KAFKA-3434: add old constructor to ConsumerRecord","","closed","","hachikuji","2016-03-23T19:51:44Z","2016-03-24T17:13:06Z"
"","1122","KAFKA-3441: 0.10.0 documentation still says ""0.9.0""","","closed","","granthenke","2016-03-23T18:17:31Z","2016-03-23T19:54:50Z"
"","1121","KAFKA-3451: Add basic HTML coverage report generation to gradle","","closed","","granthenke","2016-03-23T17:07:10Z","2016-04-05T22:00:44Z"
"","1117","KAFKA-3447; partitionState in UpdateMetadataRequest not logged properly state-change log","","closed","","ijuma","2016-03-22T23:58:46Z","2016-03-23T01:20:34Z"
"","1116","KAFKA-3435: Follow up to fix checkstyle","","closed","","ewencp","2016-03-22T21:07:13Z","2016-03-22T21:09:35Z"
"","1115","KAFKA-3409: handle CommitFailedException in MirrorMaker","","closed","","hachikuji","2016-03-22T20:35:52Z","2016-03-23T16:49:00Z"
"","1113","KAFKA-3435: Remove `Unstable` annotation from new Java Consumer","","closed","","granthenke","2016-03-22T16:37:37Z","2016-03-22T18:53:42Z"
"","1112","KAFKA-3442: Fix FileMessageSet iterator.","","closed","","becketqin","2016-03-22T04:46:17Z","2016-03-23T14:16:29Z"
"","1108","KAFKA-3412: multiple asynchronous commits causes send failures","","closed","","hachikuji","2016-03-21T20:27:14Z","2016-03-22T03:48:11Z"
"","1106","KAFKA-3319: improve session timeout broker/client config documentation","","closed","","hachikuji","2016-03-21T18:32:42Z","2016-03-22T20:09:40Z"
"","1103","KAFKA-3378; Follow-up to ensure we `finishConnect` for immediately connected keys","","closed","","ijuma","2016-03-20T23:43:07Z","2016-03-21T02:59:00Z"
"","1101","KAFKA-3427: broker can return incorrect version of fetch response when the broker hits an unknown exception","","closed","","junrao","2016-03-19T01:25:14Z","2016-03-20T01:05:31Z"
"","1100","KAFKA-3426; Improve protocol type errors when invalid sizes are received","","closed","","ijuma","2016-03-19T00:20:38Z","2016-03-22T19:39:33Z"
"","1099","KAFKA-3424: Add CORS support to Connect REST API","","closed","","ewencp","2016-03-18T23:02:21Z","2016-03-20T01:40:17Z"
"","1098","KAFKA-3006: standardize KafkaConsumer API to use Collection","","closed","","hachikuji","2016-03-18T22:13:36Z","2016-03-18T23:08:08Z"
"","1091","MINOR: Fix FetchRequest.getErrorResponse for version 1","","closed","","granthenke","2016-03-17T21:23:29Z","2016-03-21T03:20:20Z"
"","1090","KAFKA-3316: Add REST API for listing connector plugins","","closed","","Ishiihara","2016-03-17T20:36:02Z","2016-03-25T23:48:20Z"
"","1087","KAFKA-2370: kafka connect pause/resume API","","closed","","hachikuji","2016-03-17T04:19:57Z","2016-04-20T21:10:44Z"
"","1086","KAFKA-3400: Fix describe topic in case there are zero partitions.","","closed","","SinghAsDev","2016-03-16T22:58:20Z","2017-12-22T01:37:27Z"
"","1085","KAFKA-3378 Fix for instantly connecting SocketChannels (v3)","","closed","","llowrey","2016-03-16T22:54:28Z","2017-12-22T01:36:00Z"
"","1080","MINOR: KAFKA-3260 follow up, fix commitRecord calls in tests","","closed","","ewencp","2016-03-16T06:22:48Z","2016-03-16T15:27:12Z"
"","1077","MINOR: Remove unused import in `WordCountJob` to fix checkstyle failure","","closed","","ijuma","2016-03-15T19:54:55Z","2016-03-15T20:38:08Z"
"","1076","KAFKA-3387: Update GetOffsetShell tool to not rely on old producer","","closed","","SinghAsDev","2016-03-15T17:35:33Z","2018-05-26T15:37:17Z"
"","1073","KAFKA-3402; Restore behaviour of MetadataCache.getTopicMetadata when unsupported security protocol is received","","closed","","ijuma","2016-03-15T14:13:16Z","2016-03-16T23:16:21Z"
"","1069","KAFKA-3398: avoid just sending to one partition when record key is empty string","","closed","","vesense","2016-03-15T03:09:25Z","2016-03-25T13:22:27Z"
"","1067","KAFKA-2982; Mark the old Scala producer and related classes as deprecated","","closed","","ijuma","2016-03-15T01:54:21Z","2016-03-15T20:12:46Z"
"","1066","KAFKA-3336: Unify Serializer and Deserializer into Serialization","","closed","","guozhangwang","2016-03-15T01:48:27Z","2016-03-17T22:42:24Z"
"","1064","KAFKA-3394: allow null offset metadata in commit API","","closed","","hachikuji","2016-03-14T22:12:36Z","2016-03-18T20:37:58Z"
"","1061","TRIVIAL: remove TODO in ConsumerNetworkClient after KAFKA-2120","","closed","","christian-posta","2016-03-14T19:04:44Z","2016-12-26T22:37:53Z"
"","1060","KAFKA-3393 : Updated the docs to reflect the deprecation of block.on.buffer.full and usage of max.block.ms","","closed","","MayureshGharat","2016-03-14T18:48:50Z","2016-05-18T01:08:19Z"
"","1058","MINOR: Review change to block.on.buffer.full config","","closed","","granthenke","2016-03-14T14:06:54Z","2016-03-14T20:07:20Z"
"","1056","KAFKA-3388: Fix expiration of batches sitting in the accumulator","","closed","","becketqin","2016-03-13T23:58:49Z","2016-03-26T16:23:29Z"
"","1054","KAFKA-2551: Update Unclean leader election docs","","closed","","omkreddy","2016-03-12T16:40:51Z","2018-07-03T15:42:14Z"
"","1053","KAFKA-3381: Add system test for SimpleConsumerShell","","closed","","SinghAsDev","2016-03-11T20:27:33Z","2016-03-31T02:34:13Z"
"","1049","KAFKA-3373 add 'log' prefix to configurations in KIP-31/32","","closed","","becketqin","2016-03-10T21:58:39Z","2016-03-15T02:13:50Z"
"","1048","KAFKA-3380: Add system test for GetOffsetShell tool","","closed","","SinghAsDev","2016-03-10T21:53:30Z","2016-03-11T20:18:11Z"
"","1047","MINOR: Add unit test for internal topics","","closed","","guozhangwang","2016-03-10T20:18:02Z","2016-03-10T22:55:18Z"
"","1041","KAFKA-3173: Error while moving some partitions to OnlinePartition state","","closed","","fpj","2016-03-10T10:39:11Z","2016-05-05T08:36:40Z"
"","1040","MINOR: update compression design doc to include lz4 protocol","","closed","","omkreddy","2016-03-10T09:04:27Z","2016-03-11T18:56:47Z"
"","1037","MINOR: KAFKA-3361 follow up","","closed","","granthenke","2016-03-10T05:48:49Z","2016-03-10T05:55:38Z"
"","1036","KAFKA-3318: clean up consumer logging and error messages","","closed","","hachikuji","2016-03-09T17:36:58Z","2016-03-10T19:29:32Z"
"","1033","KAFKA-3358; Only request metadata updates once we have topics or a pattern subscription","","closed","","ijuma","2016-03-09T09:33:24Z","2016-03-10T19:06:03Z"
"","1027","MINOR: add back old constructor of ConsumerRecord","","closed","","hachikuji","2016-03-08T01:22:18Z","2016-03-22T16:45:37Z"
"","1024","KAFKA-3344: Remove previous system test's leftover test-log4j.properties","","closed","","SinghAsDev","2016-03-07T21:09:34Z","2016-03-10T08:43:14Z"
"","1021","KAFKA-3339: Fix java docs for seekToEnd and seekToBeginning.","","closed","","SinghAsDev","2016-03-07T19:22:37Z","2016-03-07T19:29:07Z"
"","1019","MINOR: streams javadoc corrections","","closed","","omkreddy","2016-03-07T10:58:53Z","2018-07-03T15:42:14Z"
"","1018","KAFKA-2960: Clear purgatory for partitions before becoming follower","","closed","","becketqin","2016-03-07T04:08:44Z","2016-03-11T19:22:47Z"
"","1013","HOTFIX: Generate javadocs for all Streams packages with the exception of internals","","closed","","miguno","2016-03-04T18:04:57Z","2016-03-04T18:29:18Z"
"","1011","KAFKA-3247: Add option to see unbalanced partitions via kafka-topic tool","","closed","","SinghAsDev","2016-03-04T02:12:04Z","2021-12-19T21:43:05Z"
"","1010","KAFKA-3252: Compression type from broker config should be used during log compaction.","","closed","","SinghAsDev","2016-03-04T00:28:01Z","2021-12-19T21:43:24Z"
"","1008","KAFKA-3290: fix race condition with worker task shutdown and mock validation","","closed","","hachikuji","2016-03-03T22:46:22Z","2016-03-03T22:59:06Z"
"","1004","HOTFIX: Avoid NPE in StreamsPartitionAssignor","","closed","","guozhangwang","2016-03-03T16:45:02Z","2016-03-03T16:58:16Z"
"","1003","MINOR: Fix typos in docs","","closed","","sasakitoa","2016-03-03T14:18:47Z","2016-03-03T19:46:40Z"
"","1001","KAFKA-3324: NullPointerException in StreamPartitionAssignor","","closed","","miguno","2016-03-03T10:24:05Z","2016-03-03T16:21:32Z"
"","1000","HOTFIX: Fix checkstyle failure in KStreams by providing fully qualified class names.","","closed","","ewencp","2016-03-03T06:57:50Z","2016-03-03T07:42:39Z"
"","999","MINOR: Improve JavaDoc for some public classes.","","closed","","guozhangwang","2016-03-03T00:06:42Z","2016-03-03T00:11:41Z"
"","998","KAFKA-3290: fix transient test failures in WorkerSourceTaskTest","","closed","","hachikuji","2016-03-02T22:18:09Z","2016-03-03T01:22:37Z"
"","997","KAFKA-3314: Add CDDL license to LICENSE and NOTICE file","","closed","","junrao","2016-03-02T18:20:26Z","2016-03-03T19:44:26Z"
"","996","MINOR: fix typo","","closed","","xuwei-k","2016-03-02T10:05:35Z","2016-03-03T03:12:54Z"
"","991","HOTFIX: Use the correct serde classes","","closed","","guozhangwang","2016-03-01T22:44:50Z","2016-03-02T18:11:10Z"
"","990","KAFKA-3311: Prepare internal source topics before calling partition grouper","","closed","","guozhangwang","2016-03-01T22:40:05Z","2016-03-02T21:44:14Z"
"","988","KAFKA-2073: migrate to client-side topic metadata request/response","","closed","","hachikuji","2016-03-01T19:33:36Z","2016-03-11T19:15:07Z"
"","983","KAFKA-3300: Avoid over allocating disk space and memory for index files.","","closed","","becketqin","2016-02-29T01:41:37Z","2022-02-09T19:32:29Z"
"","981","KAFKA-3299: Ensure that reading config log on rebalance doesn't hang the herder","","closed","","gwenshap","2016-02-27T00:08:55Z","2016-03-04T22:21:49Z"
"","977","KAFKA-3292; ClientQuotaManager.getOrCreateQuotaSensors() may return a null ClientSensors.throttleTimeSensor","","closed","","ijuma","2016-02-26T00:55:42Z","2016-03-01T22:44:51Z"
"","976","MINOR: Validate inner message compression attribute","","closed","","ijuma","2016-02-26T00:38:33Z","2016-03-01T22:44:54Z"
"","965","KAFKA-3278 add thread number to clientId passed into StreamThread","","closed","","tomdearman","2016-02-24T10:25:46Z","2016-02-26T09:58:32Z"
"","964","KAFKA-3315: Add REST and Connector API to expose connector configuration","","closed","","Ishiihara","2016-02-24T09:33:28Z","2016-03-17T20:26:39Z"
"","962","KAFKA-2698: Add paused() method to o.a.k.c.c.Consumer","","closed","","hachikuji","2016-02-24T01:57:50Z","2016-02-25T03:42:47Z"
"","960","HOTFIX: Add missing file for KeyValue unit test","","closed","","guozhangwang","2016-02-23T23:27:29Z","2016-02-23T23:33:54Z"
"","957","Minor: add useful debug log messages to KConnect source task execution","","closed","","gwenshap","2016-02-23T22:07:31Z","2016-02-24T19:09:30Z"
"","949","KAFKA-3256: Add print.timestamp option to console consumer.","","closed","","becketqin","2016-02-22T21:57:47Z","2016-02-23T01:46:40Z"
"","941","KAFKA-3255 Added unit tests for NetworkClient.connectionDelay(Node node, long now)","","closed","","frankscholten","2016-02-20T18:19:29Z","2016-02-22T18:14:35Z"
"","938","MINOR: Remove usage of deprecated junit.framework.Assert","","closed","","granthenke","2016-02-19T18:37:52Z","2016-03-08T02:24:40Z"
"","935","MINOR - remove unused imports in package kafka.utils","","closed","","zhuchen1018","2016-02-19T03:29:00Z","2016-02-23T02:14:46Z"
"","934","KAFKA-3236: Honor Producer Configuration ""block.on.buffer.full""","","closed","","knusbaum","2016-02-18T22:02:10Z","2016-04-28T07:52:42Z"
"","931","KAFKA-3007: implement max.poll.records (KIP-41)","","closed","","hachikuji","2016-02-17T22:49:02Z","2017-06-23T17:43:06Z"
"","930","KAFKA-2802: kafka streams system tests","","closed","","ymatsuda","2016-02-17T21:15:51Z","2016-02-23T21:11:08Z"
"","928","HOTFIX: make sure to go through all shutdown steps","","closed","","ymatsuda","2016-02-17T16:03:46Z","2016-02-22T21:17:17Z"
"","921","KAFKA-3239: Synchronize removeTopic using controllerLock","","closed","","rajinisivaram","2016-02-16T08:27:41Z","2016-02-16T09:46:13Z"
"","920","KAFKA-3093: Add Connect status tracking API","","closed","","hachikuji","2016-02-16T04:41:03Z","2016-02-25T07:49:36Z"
"","919","KAFKA-3093 [WIP]: Add Connect status tracking API","","closed","","hachikuji","2016-02-16T04:32:25Z","2016-02-16T04:39:03Z"
"","917","Properly quote $JAVA in order to avoid failure due to space char in PATH","","closed","","pbaille","2016-02-15T19:39:18Z","2020-10-20T14:25:30Z"
"","916","Properly quote $JAVA in bin/kafka-run-class.sh in order to avoid failure due to space char in PATH","","closed","","pbaille","2016-02-15T19:33:25Z","2016-02-15T19:37:27Z"
"","913","KAFKA-627: Make UnknownTopicOrPartitionException a WARN in broker","","closed","","kichristensen","2016-02-14T20:48:32Z","2019-07-12T22:15:18Z"
"","911","KAFKA-2757; Consolidate BrokerEndPoint and EndPoint","","closed","","zhuchen1018","2016-02-13T06:23:25Z","2016-02-18T06:07:33Z"
"","902","MINOR: connect hangs on startup failure","","closed","","hachikuji","2016-02-11T00:32:02Z","2016-02-11T01:08:55Z"
"","901","MINOR: catch an exception in rebalance and stop the stream thread","","closed","","ymatsuda","2016-02-10T22:27:20Z","2016-02-12T09:12:00Z"
"","894","MINOR: add setUncaughtExceptionHandler to KafkaStreams","","closed","","ymatsuda","2016-02-10T00:22:12Z","2016-02-11T06:02:36Z"
"","892","KAFKA-3187: Make kafka-acls.sh help messages more generic.","","closed","","SinghAsDev","2016-02-09T21:25:00Z","2016-03-04T01:35:05Z"
"","884","KAFKA-3159: stale high watermark segment offset causes early fetch return","","closed","","hachikuji","2016-02-08T17:53:17Z","2016-02-09T22:58:14Z"
"","879","MINOR: Add note about which files need to be edited when updating the version number","","closed","","ewencp","2016-02-05T21:43:39Z","2016-02-24T18:13:16Z"
"","874","KAFKA-3211: handle WorkerTask stop before start correctly","","closed","","hachikuji","2016-02-05T01:17:51Z","2016-02-05T02:01:19Z"
"","872","MINOR: KTable.count() to only take a selector for key","","closed","","guozhangwang","2016-02-04T22:38:44Z","2016-02-24T00:54:49Z"
"","871","MINOR: log connect reconfiguration error only if there was an error","","closed","","gwenshap","2016-02-04T22:32:03Z","2016-02-04T22:34:54Z"
"","870","KAFKA-3192: Add unwindowed aggregations for KStream","","closed","","guozhangwang","2016-02-04T22:27:18Z","2016-02-29T22:03:56Z"
"","865","KAFKA-3207: Fix StateChangeLogger to use the right topic name","","closed","","guozhangwang","2016-02-04T18:13:54Z","2016-02-04T22:51:54Z"
"","863","MINOR: Pin to system tests to ducktape 0.3.10","","closed","","granders","2016-02-04T06:19:28Z","2016-02-04T18:12:54Z"
"","861","KAFKA-3186: KIP-50: Move Authorizer to a separate package","","closed","","SinghAsDev","2016-02-04T01:10:00Z","2019-12-19T15:11:08Z"
"","860","MINOR: Fix restoring for source KTable","","closed","","guozhangwang","2016-02-04T00:41:42Z","2016-02-04T04:43:06Z"
"","859","HOTFIX: fix broken WorkerSourceTask test","","closed","","hachikuji","2016-02-03T23:48:59Z","2016-02-04T19:19:11Z"
"","855","KAFKA-3195; Transient test failure in OffsetCheckpointTest.testReadWrite","","closed","","ijuma","2016-02-03T01:29:27Z","2016-03-01T22:53:21Z"
"","853","MINOR: some more Javadocs","","closed","","guozhangwang","2016-02-03T00:57:53Z","2016-02-03T19:35:24Z"
"","849","MINOR: fix the logic of RocksDBWindowStore using RocksDBStore Segments","","closed","","guozhangwang","2016-02-02T18:08:18Z","2016-02-02T18:14:55Z"
"","848","0.9.0","","closed","","quanrs","2016-02-02T07:57:30Z","2016-02-02T23:12:46Z"
"","841","KAFKA-3174: Change Crc32 to use java.util.zip.CRC32","","closed","","becketqin","2016-01-30T06:46:55Z","2017-12-22T01:35:10Z"
"","839","KAFKA-3121: Refactor KStream Aggregate to be Lambda-able.","","closed","","guozhangwang","2016-01-29T23:53:55Z","2016-02-02T20:02:17Z"
"","835","KAFKA-3165: Fix ignored parameters in the gradle ""All"" tasks","","closed","","granthenke","2016-01-29T17:25:35Z","2016-02-17T06:06:26Z"
"","833","KAFKA-3075; Fix ClassCastException in `ZookeeperConsumerConnector.commitOffsets`","","closed","","ijuma","2016-01-29T16:35:29Z","2016-03-01T22:53:25Z"
"","815","KAFKA-3092: Replace SinkTask onPartitionsAssigned/onPartitionsRevoked with open/close","","closed","","hachikuji","2016-01-27T00:18:25Z","2016-02-03T19:29:35Z"
"","813","KAFKA-3116: Specify minimum Gradle version required in Readme","","closed","","vahidhashemian","2016-01-26T19:56:12Z","2016-01-26T23:34:11Z"
"","810","KAFKA-2704:Make SimpleConsumer threadsafe","","closed","","ozawa-hi","2016-01-26T03:45:20Z","2018-02-25T21:33:49Z"
"","809","KAFKA-3125: Add Kafka Streams Exceptions","","closed","","guozhangwang","2016-01-26T00:23:39Z","2016-01-26T17:20:13Z"
"","802","Fixed undefined method `update_guest'","","closed","","szwed","2016-01-22T12:04:54Z","2016-01-24T09:17:17Z"
"","800","KAFKA-3136: Rename KafkaStreaming to KafkaStreams","","closed","","guozhangwang","2016-01-22T00:14:36Z","2016-01-22T21:00:26Z"
"","799","MINOR: Remove remnants of hadoop clients from kafka-run-class.sh","","closed","","granthenke","2016-01-21T20:27:32Z","2016-02-17T06:06:29Z"
"","798","MINOR: Upgrade note on compacted topics behaviour on receiving message without key","","closed","","ijuma","2016-01-21T14:03:35Z","2016-03-01T22:53:37Z"
"","797","KAFKA-3066: Demo Examples for Kafka Streams","","closed","","guozhangwang","2016-01-21T06:43:48Z","2016-01-22T23:32:47Z"
"","796","0.8.2","","closed","","chetansomani","2016-01-21T02:02:04Z","2016-02-02T23:12:05Z"
"","795","KAFKA-3121: Remove aggregatorSupplier and add Reduce functions","","closed","","guozhangwang","2016-01-20T20:53:27Z","2016-01-21T00:11:01Z"
"","789","MINOR: Fix javadoc for `PartitionInfo.leader()`","","closed","","ijuma","2016-01-19T13:28:39Z","2016-03-01T22:53:42Z"
"","787","MINOR: complete built-in stream aggregate functions","","closed","","guozhangwang","2016-01-18T21:43:12Z","2016-01-18T21:44:17Z"
"","781","KAFKA-3104: add windowed aggregation to KStream","","closed","","guozhangwang","2016-01-15T22:08:14Z","2016-01-18T20:15:39Z"
"","780","KAFKA-2695: limited support for nullable byte arrays","","closed","","hachikuji","2016-01-15T01:01:22Z","2016-01-18T17:54:36Z"
"","777","KAFKA-3050: Acceptor allows hostnames surrounded by whitespaces","","closed","","Zixxy","2016-01-14T21:19:40Z","2018-02-25T21:35:29Z"
"","775","MINOR: add internal source topic for tracking","","closed","","guozhangwang","2016-01-14T20:28:37Z","2016-01-15T01:10:15Z"
"","774","KAFKA-3098: ""partition.assignment.strategy"" appears twice in documentation","","closed","","dajac","2016-01-14T18:07:09Z","2020-08-11T06:48:18Z"
"","773","KAFKA-3100; Broker.createBroker should work if json is version > 2 and still compatible","","closed","","ijuma","2016-01-14T16:09:07Z","2016-03-01T22:53:43Z"
"","769","KAFKA-2998: log warnings when client is disconnected from bootstrap brokers","","closed","","hachikuji","2016-01-13T22:16:15Z","2016-04-05T04:29:27Z"
"","768","MINOR: Typo in documentation of topic config removal","","closed","","s7anley","2016-01-13T21:22:18Z","2016-01-13T22:58:57Z"
"","767","KAFKA-2886: handle sink task rebalance failures by stopping worker task","","closed","","hachikuji","2016-01-13T18:20:39Z","2016-01-15T17:29:16Z"
"","761","KAFKA-3081: Non-windowed Table Aggregation","","closed","","guozhangwang","2016-01-13T02:38:38Z","2019-09-28T15:36:03Z"
"","760","POC Producer Interceptor","","closed","","apovzner","2016-01-13T00:49:05Z","2016-01-13T01:55:20Z"
"","758","KAFKA-3086: Remove unused method.","","closed","","j-nowak","2016-01-12T12:59:42Z","2016-01-29T16:43:54Z"
"","757","KAFKA-3082: Make LogManager.InitialTaskDelayMs configurable","","closed","","j-nowak","2016-01-12T12:40:24Z","2022-02-09T19:34:00Z"
"","755","KAFKA-3089; VerifiableProducer should do a clean shutdown in stop_node()","","closed","","lindong28","2016-01-11T22:52:09Z","2016-06-28T01:56:17Z"
"","754","KAFKA-3019: Add an exceptionName method to Errors","","closed","","granthenke","2016-01-11T15:03:29Z","2016-01-19T04:34:35Z"
"","752","KAFKA-3085: BrokerChangeListener computes inconsistent live/dead broker list","","closed","","dajac","2016-01-11T08:56:30Z","2016-01-12T06:15:24Z"
"","749","KAFKA-3076: BrokerChangeListener should log the brokers in order","","closed","","konradkalita","2016-01-09T11:08:00Z","2016-01-26T15:49:01Z"
"","746","MINOR: speed up connect startup when full connector class name is provided","","closed","","hachikuji","2016-01-08T22:02:40Z","2016-01-08T23:05:05Z"
"","741","KAFKA-3021: Centralize dependency version management","","closed","","granthenke","2016-01-07T23:20:23Z","2016-01-19T04:34:45Z"
"","740","KAFKA-3077: Enable KafkaLog4jAppender to work with SASL enabled brokers","","closed","","SinghAsDev","2016-01-07T22:35:09Z","2016-01-12T07:14:48Z"
"","732","KAFKA-3058: remove the usage of deprecated config properties","","closed","","konradkalita","2016-01-05T07:50:49Z","2016-01-12T16:54:56Z"
"","731","KAFKA-3045; ZkNodeChangeNotificationListener shouldn't log InterruptedException as error","","closed","","lindong28","2016-01-05T02:25:51Z","2016-01-05T21:27:22Z"
"","727","MINOR: Update version to 0.9.0.1-SNAPSHOT","","closed","","ewencp","2016-01-04T18:45:32Z","2016-01-06T07:26:40Z"
"","724","KAFKA-3051 KAFKA-3048; Security config docs improvements","","closed","","ijuma","2016-01-04T11:21:48Z","2016-03-01T22:54:06Z"
"","722","KAFKA-3055: Fix the JsonConverter mangling the Schema in connect","","closed","","ksenji","2016-01-01T02:05:03Z","2016-01-04T16:48:07Z"
"","717","MINOR: Improve document of MirrorMaker","","closed","","sasakitoa","2015-12-25T07:10:28Z","2016-01-06T01:31:30Z"
"","713","KAFKA-3030: Remove unused scala dependencies","","closed","","granthenke","2015-12-22T17:21:13Z","2016-01-19T04:35:03Z"
"","707","MINOR: Fix typo in documentation","","closed","","vahidhashemian","2015-12-21T23:47:18Z","2015-12-22T18:44:51Z"
"","706","Update 2015-12-21","","closed","","vahidhashemian","2015-12-21T22:29:54Z","2015-12-21T22:30:11Z"
"","705","KAFKA-3024: Remove old patch review tools","","closed","","granthenke","2015-12-21T20:20:59Z","2016-01-19T04:35:01Z"
"","704","KAFKA-2000: Delete topic should also delete consumer offsets.","","closed","","Parth-Brahmbhatt","2015-12-21T20:18:30Z","2018-02-25T06:56:27Z"
"","702","KAFKA-2989: system tests should verify partitions consumed after rebalancing","","closed","","hachikuji","2015-12-21T18:40:33Z","2015-12-23T00:16:11Z"
"","701","MINOR: Fix a typos in comments","","closed","","8da2k","2015-12-21T01:25:11Z","2015-12-23T02:59:13Z"
"","699","Fixed a typos in comments","","closed","","8da2k","2015-12-20T08:58:58Z","2015-12-21T01:32:23Z"
"","696","KAFKA-3014: fix integer overflow problem in leastLoadedNode","","closed","","hachikuji","2015-12-18T22:31:56Z","2015-12-21T18:54:23Z"
"","691","KAFKA-2653: Kafka Streams Stateful API Design","","closed","","guozhangwang","2015-12-17T19:43:26Z","2016-01-29T04:46:48Z"
"","688","KAFKA-3003 Update the replica.highWatermark correctly","","closed","","becketqin","2015-12-17T06:53:21Z","2016-02-05T22:47:17Z"
"","686","KAFKA-2988: Change default configuration of the log cleaner","","closed","","granthenke","2015-12-17T03:46:20Z","2017-03-02T03:42:37Z"
"","681","Minor: updating comment that fell out of sync with code","","closed","","gwenshap","2015-12-15T23:13:58Z","2015-12-16T00:19:13Z"
"","676","MINOR: Fix broken docs link","","closed","","granthenke","2015-12-14T23:43:34Z","2016-02-08T03:41:26Z"
"","675","KAFKA-2990: add explicit ConcurrentMap type annotation to fix NoSuchMethodError in Pool","","closed","","hachikuji","2015-12-14T23:40:53Z","2015-12-15T18:11:49Z"
"","674","KAFKA-2837 Follow-up: Default max block to 60 seconds","","closed","","guozhangwang","2015-12-14T23:10:46Z","2015-12-15T01:52:51Z"
"","673","MINOR: Fix typos in code comments","","closed","","vahidhashemian","2015-12-14T19:51:07Z","2015-12-15T21:47:23Z"
"","666","KAFKA-2978: consumer stops fetching when consumed and fetch positions get out of sync","","closed","","hachikuji","2015-12-11T04:32:51Z","2015-12-14T22:55:06Z"
"","665","KAFKA-2653 Phase I: Stateful Operation API Design","","closed","","guozhangwang","2015-12-10T23:18:03Z","2015-12-17T19:42:50Z"
"","662","MINOR: Fix typos in code comments","","closed","","vahidhashemian","2015-12-10T20:05:45Z","2015-12-15T17:11:40Z"
"","660","KAFKA-2980 Fix deadlock when ZookeeperConsumerConnector create messag…","","closed","","becketqin","2015-12-10T19:15:55Z","2016-12-21T21:55:08Z"
"","659","KAFKA-2578; Client Metadata internal state should be synchronized","","closed","","ijuma","2015-12-10T16:36:34Z","2016-03-01T22:53:52Z"
"","651","Minor: Fix @link in MetricName comment","","closed","","lindong28","2015-12-09T18:11:02Z","2015-12-09T18:59:50Z"
"","650","KAFKA-2973; Fix leak of child sensors on remove","","closed","","ijuma","2015-12-09T17:17:40Z","2016-03-01T22:53:54Z"
"","649","KAFKA-2972; Add missing `partitionsRemaingList.add` in `ControlledShutdownResponse` constructor","","closed","","ijuma","2015-12-09T17:05:18Z","2016-03-01T22:54:33Z"
"","646","[KAFKA-2965]Two variables should be exchanged.","","closed","","boweite","2015-12-09T02:33:10Z","2015-12-09T18:02:32Z"
"","643","KAFKA-2733: Standardize metric name for Kafka Streams","","closed","","guozhangwang","2015-12-08T23:21:39Z","2015-12-10T06:16:46Z"
"","642","KAFKA-2667: fix assertion depending on hash map order in KafkaBasedLogTest.testSendAndReadToEnd","","closed","","hachikuji","2015-12-08T22:28:20Z","2015-12-09T01:49:55Z"
"","641","KAFKA-2957: Fix typos in Kafka documentation","","closed","","vahidhashemian","2015-12-08T19:45:55Z","2015-12-14T19:45:35Z"
"","637","KAFKA-2958: Remove duplicate API key mapping functionality","","closed","","granthenke","2015-12-08T02:12:03Z","2016-01-19T04:35:28Z"
"","634","Minor: ""-daemon"" option is not mentioned in usage of zookeeper-server-start.sh","","closed","","sasakitoa","2015-12-07T10:23:55Z","2016-01-21T00:51:50Z"
"","631","KAFKA-2949: Make EndToEndAuthorizationTest replicated.","","closed","","fpj","2015-12-05T15:32:12Z","2016-01-03T23:26:22Z"
"","630","MINOR: backport fix to partition assignor order","","closed","","hachikuji","2015-12-04T23:10:54Z","2015-12-05T23:02:35Z"
"","628","KAFKA-2893: Add a simple non-negative partition seek check","","closed","","jinxing64","2015-12-04T14:10:02Z","2015-12-10T18:49:45Z"
"","627","0.9.0","","closed","","ChamberQin","2015-12-04T06:47:27Z","2016-01-19T15:25:37Z"
"","626","KAFKA-2945: CreateTopic - protocol and server side implementation","","closed","","granthenke","2015-12-04T02:26:02Z","2016-06-10T16:21:08Z"
"","624","KAFKA-2870: add optional operationRetryTimeout parameter to apply method in ZKUtils.","","closed","","j-nowak","2015-12-03T13:02:23Z","2019-05-11T08:08:01Z"
"","623","KAFKA-2942: inadvertent auto-commit when pre-fetching can cause message loss","","closed","","hachikuji","2015-12-03T06:18:51Z","2015-12-03T19:01:55Z"
"","622","KAFKA-2924: support offsets topic in DumpLogSegments","","closed","","hachikuji","2015-12-03T04:27:03Z","2015-12-09T05:48:48Z"
"","619","KAFKA-2931: add system test for consumer rolling upgrades","","closed","","hachikuji","2015-12-02T22:48:57Z","2015-12-04T23:11:55Z"
"","618","KAFKA-2825: Add controller failover to existing replication tests","","closed","","apovzner","2015-12-02T22:45:06Z","2015-12-03T18:44:22Z"
"","615","KAFKA-2930: Update references to ZooKeeper in the docs.","","closed","","fpj","2015-12-02T17:45:53Z","2016-04-01T22:59:05Z"
"","614","Kafka-2310: Add config to prevent broker becoming controller","","closed","","abiletskyi","2015-12-02T17:43:14Z","2019-05-11T08:03:48Z"
"","606","MINOR - fix typo in index corruption warning message","","closed","","lindong28","2015-12-01T09:38:36Z","2015-12-01T19:51:10Z"
"","605","KAFKA-2913: missing partition check when removing groups from cache","","closed","","hachikuji","2015-12-01T05:24:02Z","2015-12-01T05:34:14Z"
"","600","Minor - remove unused TimeUnit from MetricConfig","","closed","","lindong28","2015-11-29T21:41:35Z","2016-01-12T02:16:34Z"
"","599","KAFKA-2906: Fix Connect javadocs, restrict only to api subproject, and clean up javadoc warnings.","","closed","","ewencp","2015-11-29T19:40:42Z","2015-11-29T21:26:58Z"
"","598","KAFKA-2905: System test for rolling upgrade to enable ZooKeeper ACLs with SASL","","closed","","fpj","2015-11-28T12:49:57Z","2015-12-04T01:48:10Z"
"","597","MINOR: fix verifiable consumer assertion","","closed","","hachikuji","2015-11-28T01:15:37Z","2015-11-30T19:02:13Z"
"","595","KAFKA-2875:  remove slf4j multi binding warnings when running form source distribution","","closed","","jinxing64","2015-11-27T08:19:14Z","2015-12-11T14:42:09Z"
"","594","Avoiding warning about generics in sample code","","closed","","nurkiewicz","2015-11-27T07:50:26Z","2015-11-27T20:25:03Z"
"","590","MINOR: Support GitHub OAuth tokens in kafka-merge-pr.py","","closed","","guozhangwang","2015-11-25T22:56:05Z","2015-11-25T23:06:18Z"
"","588","Grammar on README.md","","closed","","macalinao","2015-11-25T21:23:40Z","2016-01-06T10:31:42Z"
"","585","MINOR: Improve broker id documentation","","closed","","granthenke","2015-11-25T14:52:33Z","2016-01-19T04:36:24Z"
"","584","KAFKA-2881: Improve Consumer Configs and APIs","","closed","","guozhangwang","2015-11-24T23:54:57Z","2015-11-25T00:39:23Z"
"","582","KAFKA-2877: handle request timeout in sync group","","closed","","hachikuji","2015-11-24T19:52:58Z","2015-11-25T00:17:50Z"
"","581","KAFKA-2880: consumer should handle disconnect/timeout for metadata requests","","closed","","hachikuji","2015-11-24T17:42:06Z","2015-12-02T19:28:24Z"
"","579","KAFKA-2804: manage changelog topics through ZK in PartitionAssignor","","closed","","guozhangwang","2015-11-24T00:11:59Z","2015-12-07T23:12:39Z"
"","578","KAFKA-2879: Make MiniKDC test service slightly more generic","","closed","","gwenshap","2015-11-23T23:25:47Z","2015-11-24T01:08:12Z"
"","576","Minor: Fix KafkaConsumer Constructor Summary javadoc","","closed","","jholoman","2015-11-23T16:28:51Z","2015-11-24T02:22:23Z"
"","573","KAFKA-2874: shutdown ZK process reliably","","closed","","miguno","2015-11-23T09:11:11Z","2016-01-06T23:22:07Z"
"","569","MINOR: Log at INFO level in Benchmark tests","","closed","","granders","2015-11-20T19:06:05Z","2015-12-09T19:41:09Z"
"","568","KAFKA-2863; Add  a `close()` method to `Authorizer`","","closed","","ijuma","2015-11-20T15:37:53Z","2016-03-01T22:48:30Z"
"","567","KAFKA-2869; Host used by Authorizer should be IP address not hostname/IP","","closed","","ijuma","2015-11-20T15:07:25Z","2016-03-01T22:48:33Z"
"","566","KAFKA-2867: Fix missing WorkerSourceTask synchronization and handling of InterruptException.","","closed","","ewencp","2015-11-20T05:20:00Z","2015-11-20T18:04:59Z"
"","565","MINOR: Fix typo in sample Vagrantfile.local for AWS system tests","","closed","","ewencp","2015-11-20T03:12:22Z","2015-11-20T04:13:18Z"
"","561","KAFKA-2862: Fix MirrorMaker's message.handler.args description","","closed","","SinghAsDev","2015-11-19T18:27:38Z","2015-11-20T23:59:40Z"
"","555","KAFKA-2846: Add Ducktape test for kafka-consumer-groups","","closed","","SinghAsDev","2015-11-19T01:06:26Z","2016-01-24T01:00:01Z"
"","554","KAFKA-2859: Fix deadlock in WorkerSourceTask.","","closed","","ewencp","2015-11-18T21:25:54Z","2015-11-18T22:20:00Z"
"","553","KAFKA-2860: better handling of auto commit errors","","closed","","hachikuji","2015-11-18T21:04:29Z","2015-11-19T01:20:03Z"
"","548","MINOR: fix shutdownHook in ConsoleConsumer","","closed","","guozhangwang","2015-11-18T01:31:14Z","2015-11-18T02:00:27Z"
"","547","KAFKA-2854: Making KerberosShortNamer implement an interface and making it pluggable.","","closed","","Parth-Brahmbhatt","2015-11-18T01:28:45Z","2019-01-24T22:57:11Z"
"","539","KAFKA-2848: Use client SSL/SASL config utilities in Kafka Connect to avoid duplication of configs.","","closed","","ewencp","2015-11-17T02:56:09Z","2015-11-17T06:00:50Z"
"","537","KAFKA-2845: new client old broker compatibility","","closed","","granders","2015-11-16T22:41:29Z","2015-11-20T19:04:29Z"
"","534","Kafka 2746","","closed","","SinghAsDev","2015-11-16T18:57:07Z","2015-11-17T20:00:35Z"
"","530","KAFKA-2841: safe group metadata cache loading/unloading","","closed","","hachikuji","2015-11-15T02:42:07Z","2015-11-18T02:35:10Z"
"","528","KAFKA-2831; Do not use ZKUtils in `ConsumerGroupCommand` if `new-consumer` is used","","closed","","ijuma","2015-11-13T21:46:36Z","2016-03-01T22:48:17Z"
"","527","KAFKA-2833: print only group offset / metadata according to the formatter","","closed","","guozhangwang","2015-11-13T20:15:00Z","2015-11-13T22:10:53Z"
"","524","Missing License.","","closed","","harshach","2015-11-13T17:56:35Z","2015-11-13T18:27:42Z"
"","523","KAFKA-2830; Change default fix version to 0.9.1.0 in kafka-merge-pr.py","","closed","","ijuma","2015-11-13T11:49:36Z","2016-03-01T22:48:12Z"
"","522","KAFKA-2826: Make Kafka Connect ducktape services easier to extend.","","closed","","ewencp","2015-11-13T01:29:13Z","2015-11-13T02:54:42Z"
"","519","KAFKA-2821: fix deadlock in group metadata write callback","","closed","","hachikuji","2015-11-12T23:10:41Z","2015-11-13T19:12:11Z"
"","517","KAFKA-2814: Make Kafka Connect system test REST requests use hostname that is compatible with running under AWS.","","closed","","ewencp","2015-11-12T21:06:52Z","2015-11-12T21:19:35Z"
"","516","KAFKA-2819: catch NoSuchElementException in ConsoleConsumer","","closed","","guozhangwang","2015-11-12T20:17:58Z","2015-11-13T05:23:44Z"
"","514","MINOR: Increase timeouts for Kafka Connect system test service to make tests more reliable on wimpy nodes.","","closed","","ewencp","2015-11-12T19:29:41Z","2015-11-12T19:40:33Z"
"","507","MINOR: Fix logging message in `NetworkClient.poll` not to mention `producer`","","closed","","ijuma","2015-11-12T10:39:34Z","2016-03-01T22:48:07Z"
"","505","MINOR: Use Kafka artifact compiled with Scala 2.11 in quickstart.html","","closed","","ijuma","2015-11-12T10:12:29Z","2016-03-01T22:48:02Z"
"","502","TRIVIAL: provide clearer error in describe group when group is inactive","","closed","","hachikuji","2015-11-12T01:19:49Z","2015-11-12T17:08:08Z"
"","500","KAFKA-2812: improve consumer integration tests","","closed","","hachikuji","2015-11-11T23:38:36Z","2015-11-20T23:46:58Z"
"","499","KAFKA-2807: Move ThroughputThrottler back to tools jar to fix upgrade tests.","","closed","","ewencp","2015-11-11T21:08:11Z","2015-11-11T23:55:36Z"
"","494","KAFKA-2803: Add hard bounce system test for Kafka Connect.","","closed","","ewencp","2015-11-11T00:37:03Z","2015-11-25T01:39:34Z"
"","492","KAFKA-2752: Follow up to fix checkstlye","","closed","","granthenke","2015-11-10T23:20:43Z","2015-11-17T17:06:49Z"
"","491","KAFKA-2790: doc improvements","","closed","","gwenshap","2015-11-10T22:05:28Z","2015-11-11T18:54:31Z"
"","490","KAFKA-2799: skip wakeup in the follow-up poll() call.","","closed","","guozhangwang","2015-11-10T21:26:41Z","2015-11-10T22:50:05Z"
"","489","KAFKA-2788: Allow specifying principals with comman in ACL CLI.","","closed","","Parth-Brahmbhatt","2015-11-10T21:13:52Z","2015-11-10T22:28:13Z"
"","488","KAFKA-2795: fix potential NPE in GroupMetadataManager.addGroup","","closed","","hachikuji","2015-11-10T20:32:46Z","2015-11-10T21:01:15Z"
"","487","MINOR: update system test readme","","closed","","granders","2015-11-10T18:59:35Z","2015-12-09T19:41:44Z"
"","486","KAFKA-2798: Use prefixedd configurations for Kafka Connect producer and consumer settings so they do not conflict with the distributed herder's settings.","","closed","","ewencp","2015-11-10T18:55:31Z","2015-11-10T19:07:48Z"
"","485","KAFKA-2797: Only run rat when in the .git repository since it require s the .gitignore to generate the list of files to ignore","","closed","","ewencp","2015-11-10T17:49:13Z","2015-11-10T18:17:37Z"
"","483","KAFKA-2794: Added group support to authorizer.","","closed","","Parth-Brahmbhatt","2015-11-10T06:20:38Z","2017-12-22T20:23:56Z"
"","482","KAFKA-2793: Use ByteArrayDeserializer instead of StringDeserializer for keys in ConsoleConsumer with new consumer.","","closed","","ewencp","2015-11-10T05:44:42Z","2015-11-10T22:45:28Z"
"","481","KAFKA-2772: Stabilize failures on replication with hard bounce","","closed","","granders","2015-11-10T03:24:53Z","2015-12-09T19:40:46Z"
"","480","KAFKA-2792: Don't wait for a response to the leave group message when closing the new consumer.","","closed","","ewencp","2015-11-10T02:56:32Z","2015-11-10T18:27:12Z"