"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","689","SOLR-13489: Stop the leader from trying to rejoin the election on ses…","…sion expiration and harden our zk reconnect code path.","closed","","markrmiller","2019-05-28T16:24:32Z","2019-06-03T16:12:58Z"
"","690","SOLR-13517: [ UX improvement ] Dashboard will now store query and filter parameters on page change a…","…s well  1) refreshing the search page wont loose query and filter parameters anymore 2) this will make the filter page url sharable and land directly with filter and query intact 3) changing between multiple tabs(on left side) wont loose the these params if you come back to the search page again 4) This is mostly helpful as dashboard users switch between multiple views while making queries, and this is irritating when the earlier parameters a lost 5) Though refreshing whole page on search page and navigating between pages wont loose the filters but navigating to other page and refreshing on that page would loose it. 6) Above can be handled if I pass the query params on every page. As of now I am storing it in rootScope and query params of search page only (to make it sharable and handle refresh)","closed","","saurav534","2019-05-29T06:36:42Z","2021-12-08T13:21:33Z"
"","1026","Buffer one record from each shard every 5000 reads to ensure connecti…","…ons don't remain idle and cause timeouts.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","joel-bernstein","2019-11-21T15:17:09Z","2019-11-21T15:17:09Z"
"","1029","SOLR-13101: Address flakiness of tests using async pulls and handle i…","…nterrupt properly  Summary - Addressing test flakiness from two issues:   - Tests that rely on async pulls have test scaffolding that allows a custom pulling mechanisms to be injected into mini cluster nodes and countdown CDLs on pull completions. We shouldn't instantiate a new callback object on each call to getCorePullTaskCallback.   - Interrupt exception was being swallowed and causing leaking threads   Changes - Refactored and moved the async test logic into SolrCloudSharedStoreTestCase. Only initiate the callback instance once and not per call - Don't catch InterruptedException in ZkStateReader. I opened a JIRA/PR in master for this https://github.com/apache/lucene-solr/pull/1023 but it hasn't been merged yet so including it here. We'll get it on next upgrade ideally.","closed","","andyvuong","2019-11-22T18:11:02Z","2019-12-17T00:16:47Z"
"","908","Change the file format of README files from README.txt to README.md a…","…nd standardized the files structure.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","pinkeshsharma","2019-09-29T12:08:03Z","2020-12-23T20:00:20Z"
"","862","LUCENE-8971: Enable constructing JapaneseTokenizer with custom dictio…","…nary  # Description  Extends the API of JapaneseTokenizer so it can accept a dictionary other than the built-in one. The built-in dictionary remains the default, so existing usage is unchanged; this just opens up the possibility of supplying a different dictionary/language model to use when tokenizing Japanese.","closed","","msokolov","2019-09-06T22:16:59Z","2019-09-16T17:16:10Z"
"","1028","SOLR-13101:These are pretty much non functional changes, related to S…","…HARED replica concurrent updates https://github.com/apache/lucene-solr/pull/983/commits/581f468f9914ce2488201efbed42fd43dc4b481  - Pull read lock is only required before the local indexing starts. If indexing has to pull it can acquire and release write lock without anything to do with read lock first. Removing the unneeded lock upgrade/downgrade logic to simplify things. - Added one pager explanation in the start of SharedCoreConcurrencyController around overall concurrency design. - Added mores comments around BlobCoreMetadata#generation number and its usage.","closed","","mbwaheed","2019-11-21T22:46:26Z","2019-11-22T01:35:36Z"
"","885","LUCENE-8981: update Kuromoji javadocs, adding experimental tags to Di…","…ctionaryBuilder and JapaneseTokenizer ctor","closed","","msokolov","2019-09-16T17:28:54Z","2019-09-17T11:33:10Z"
"","722","LUCENE-8863: enable loading external Kuromoji dictionary","…abling 13-bit ids  I'm sharing this without tests, for comment, while I try to get the tests running. I'm having trouble with  ```     cd ~/workspace/lucene-solr/lucene/analysis/kuromoji; ant test-tools ```  first I had to make this patch,   ```     -     +         ```  now I'm getting this error:  ``` /home/ANT.AMAZON.COM/sokolovm/workspace/lucene-solr/lucene/common-build.xml:964: Problem: failed to create task or type antlib:com.carrotsearch.junit4:pickseed Cause: The name is undefined. ```  which all makes me wonder whether test-tools has become obsolete?","closed","","msokolov","2019-06-15T19:55:46Z","2019-06-20T23:03:01Z"
"","1507","SOLR-14471: properly apply base replica ordering to last-place shards…","….preference matches","closed","","magibney","2020-05-11T19:05:00Z","2020-05-15T04:31:36Z"
"","811","LUCENE-8920: Fix bug preventing FST duplicate tails from being shared…","… when encoded as array-with-gaps While trying to reduce the size of FSTs with array-with-gap encoding, I found that I had neglected to update the comparison function in NodeHash that is used to determine when two arcs are equal, enabling shared tails to be collapsed together. That behavior wasn't tested anywhere, and relied on some internal details of the Arc encoding to short circuit the equality test when two array Arcs are different-sized.  This patch adds a function to check if arcs are packed array, and thus amenable to such an optimization, and a unit test that demonstrates the size reduction.   This fix won't address the worst-case example Adrien posted, but it addresses a common case, I think. It would be interesting to see how the ES benchmarks are impacted by this fix.","closed","","msokolov","2019-07-28T19:26:22Z","2019-07-30T12:56:17Z"
"","1087","SOLR-14091: Removing deprecated configuration of Jetty's soLingerTime…","… option   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mkr","2019-12-16T09:14:07Z","2019-12-19T00:31:14Z"
"","1555","SOLR-14548 Address warning: static member should be qualified by type…","… name","closed","","madrob","2020-06-08T16:34:14Z","2020-06-09T20:33:38Z"
"","1221","SOLR-14193 Update tutorial.adoc(line no:664) so that command executes…","… in windows enviroment   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","balaji-s","2020-01-28T16:54:08Z","2020-01-28T17:05:37Z"
"","1204","SOLR-14205 Do not fail when given timeout to connectionImpl.isValid()…","… = 0   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description When timeout = 0  handle it as if there is no timeout  # Solution Call the solr-client without specifying a timeout  # Tests add conn.isValid(0) == true to the JdbcTest case   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","nvercamm","2020-01-23T07:32:53Z","2020-01-23T15:26:55Z"
"","770","LUCENE-8746: Component2D topology library that works on encoded space","With the upcoming of a new Shape type working in cartesian space (#726), I think we need to put some structure in the objects that contain spatial logic. In particular I have tried to remove all the mixed notation between latitude/longitude and x/y as well as defined factory methods to create those shapes from LatLonShape.   This library chooses to use X/Y notation as it is mainly cartesian, it works on the encoding space and solves problems like the neighbourhood issue (https://discuss.elastic.co/t/neighboring-touching-geo-shapes-not-found/175543) when not encoded query shapes are used against encoded indexed shapes. It potentially can simplify all the query logic as it is only needed a query by Component2D for this case.  Currently it contains factory methods to create Component2D shapes from LatLonShapes, it should be trivial to add a factory class for XYShapes.  @jpountz @nknize @rmuir @dsmiley  let me know what do you think?","closed","","iverase","2019-07-08T12:15:09Z","2020-02-07T20:19:38Z"
"","1386","SOLR-14275 Policy calculations are very slow for large clusters and large operations","WIP PR, NOT TO BE MERGED","open","","noblepaul","2020-03-28T07:22:24Z","2020-03-31T04:22:24Z"
"","1666","SOLR-14155: Load all other SolrCore plugins from packages","WIP PR, do not commit","closed","packages,","noblepaul","2020-07-11T07:52:32Z","2021-01-13T12:45:38Z"
"","1109","More pervasive use of PackageLoader / PluginInfo","WIP don't merge. This is what I did during my ""hack day"".  I'd be happy to discuss. CC @noblepaul @chatman","closed","","dsmiley","2019-12-20T22:34:28Z","2020-10-23T12:31:45Z"
"","1198","LUCENE-9153: Allow WhitespaceAnalyzer to set a custom maxTokenLen","WhitespaceTokenizer defaults to a maximum token length of 255, and WhitespaceAnalyzer does not allow this to be changed.  This commit adds an optional maxTokenLen parameter to WhitespaceAnalyzer as well, and documents the existing token length restriction.","closed","","romseygeek","2020-01-22T10:07:00Z","2020-01-27T09:22:30Z"
"","1448","LUCENE-9342: Collector's totalHitsThreshold should not be lower than numHits","While looking at SOLR-13289 I noticed this situation. If I create a collector with `numHits` greater than `totalHitsThreshold`, and the number of hits in the query is somewhere between those two numbers, collector’s `totalHitRelation` will be `TotalHits.Relation.GREATER_THAN_OR_EQUAL_TO`, however I expect the count to be accurate in this situation, right? Should this be `TotalHits.Relation.EQUAL_TO`? Maybe make `totalHitsThreshold = Max(totalHitsThreshold, humHits)` on creation? Added a test that fails.","closed","","tflobbe","2020-04-22T17:43:50Z","2020-04-23T19:04:06Z"
"","1437","zkRun+https","When starting an embedded zk server, if we can detect that we are using https, set the urlScheme cluster property accordingly.","closed","","madrob","2020-04-17T19:52:48Z","2020-04-20T18:51:13Z"
"","708","LUCENE-8775: Fix bug when selecting bridge and improve tests","When selecting a bridge, we need to check for shared vertex in its own loop. In addition the test now check that the area of the polygon is equal to the area of the tessellation.","closed","","iverase","2019-06-10T06:33:57Z","2019-06-11T05:16:25Z"
"","1545","LUCENE-9365 FuzzyQuery false negative","when prefix length == search term length   Co-Authored-By: markharwood","closed","","madrob","2020-05-29T15:26:30Z","2020-06-03T20:19:35Z"
"","1180","Update solr-tutorial.adoc","When executing the tutorial in windows 10 platform line 664 throws an error ""& was unexpected at this time."", so adding an escape characters for ""&"" and ""|""   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","balaji-s","2020-01-17T11:59:40Z","2020-01-17T12:03:12Z"
"","899","LUCENE-8989: Allow IndexSearcher To Handle Rejected Execution","When executing queries using Executors, we should gracefully handle the case when Executor rejects a task and run the task on the caller thread.","closed","","atris","2019-09-25T16:49:15Z","2019-10-03T16:26:03Z"
"","1146","SOLR-6613: TextField.analyzeMultiTerm does not throw an exception…","when Analyzer returns no term.","closed","","bruno-roustant","2020-01-06T13:56:51Z","2020-08-31T06:57:30Z"
"","1020","LUCENE-9055: Fixes the detection of lines crossing triangles through edge points","When a line crosses a triangle through an edge point, the crossing is not detected.","closed","","iverase","2019-11-20T10:27:05Z","2019-12-17T08:44:08Z"
"","1278","LUCENE-9243: Add fudge factor when creating a bounding box of a xycircle","We need increase slightly the bounding box  of a `XYCircle` to prevent points at the provided distance to be outside of the bounding box due to numerical error. ~~This patch uses a small percentage of the provided radius to increment the bounding box.~~ We use Math.nextUp() to increase the box.","closed","","iverase","2020-02-24T06:44:27Z","2020-03-02T06:08:43Z"
"","857","LUCENE-8968: Improve performance of WITHIN and DISJOINT queries for Shape queries","We are currently walking the tree twice for INTERSECTS and WITHIN queries in ShapeQuery when we can do it in just one pass. Still we need most of the times to visit all documents to remove false positives due to multi-shapes except in the case where all documents up to maxDoc are on the tree. This pull request refactors that class and tries to improve the strategy for such cases.","closed","","iverase","2019-09-05T13:11:29Z","2019-09-10T12:59:00Z"
"","927","LUCENE-8997: : Add type of triangle info to ShapeField encoding","We are currently encoding three type of triangle in ShapeField:  - POINT: all three coordinates are equal - LINE: two coordinates are equal - TRIANGLE: all coordinates are different  Because we still have two unused bits, it might be worthy to encode this information in those two bits as follows:  - 0 0 : Unknown so this is an index created before adding this information. We can compute in this case the information while decoding for backwards compatibility. - 1 0: The encoded triangle is a POINT - 0 1: The encoded triangle is a LINE - 1 1: The encoded triangle is a TRIANGLE  We can later leverage this information so we don't need to decode all dimensions in case of POINT and LINE and we are currently computing in some of the methods ithe type of triangle we are dealing with, This will go as well.","open","","iverase","2019-10-05T06:54:02Z","2020-05-11T11:17:26Z"
"","1434","LUCENE-9324: Add an ID to SegmentCommitInfo","We already have IDs in SegmentInfo, as well as on SegmentInfos which are useful to uniquely identify segments and entire commits. Having IDs on SegmentCommitInfo is be useful too in order to compare commits for equality and make snapshots incremental on generational files. This change adds a unique ID to SegmentCommitInfo starting from Lucene 8.6. Older segments won't have an ID until the segment receives an update or a delete even if they have been opened and / or committed by Lucene 8.6 or above.","closed","","s1monw","2020-04-16T13:11:34Z","2020-04-18T12:24:58Z"
"","1648","SOLR-14616: Remove CDCR from Solr 9.x","We already deprecated CDCR in 8.6. We need to remove it from master/9.0.","closed","","chatman","2020-07-03T20:23:20Z","2020-08-25T07:57:28Z"
"","1535","SOLR-14474: Fix remaining auxilliary class warnings in Solr","Version 2, incorporating D. Smiley's comments, thanks!","closed","","ErickErickson","2020-05-24T23:59:11Z","2020-05-31T19:20:52Z"
"","1410","LUCENE-9302: Grouping to use long to avoid overflows","Using long instead of int to avoid overflows of hits and counts when multiple shards are involved.","open","","chatman","2020-04-06T17:15:51Z","2020-04-06T17:15:51Z"
"","819","SOLR-13676: Reduce log verbosity in TestDistributedGrouping","using ignoreException  # Description  SOLR-13404 added a test that expects Solr to fail if grouping is called with group.offset < 0. When the test runs it succeeds but the whole stack trace is printed out in the logs.  # Solution  This small patch avoid the stack trace by using ignoreException. I also replaced an `assertTrue` with a more specific check.   # Tests  This patch is improving a test.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [...] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","diegoceccarelli","2019-08-02T17:44:00Z","2019-08-05T03:57:45Z"
"","1217","SOLR-14223 PublicKeyHandler consumes a lot of entropy during tests","Use a non-blocking implementation of SecureRandom for generating RSA Keys.","closed","","madrob","2020-01-28T00:06:31Z","2020-02-24T20:07:15Z"
"","830","LUCENE-8933: Use expectThrows instead of expected.","Use `LuceneTestCase#expectThrows()` instead of `expected`.","closed","","mocobeta","2019-08-14T03:25:27Z","2019-11-10T10:17:16Z"
"","1631","SOLR-14599: Package manager support for cluster level plugins","Usage:     bin/solr package deploy  -y -cluster     bin/solr package deploy : -y -cluster --update     bin/solr package undeploy  -y -cluster","closed","","chatman","2020-06-30T07:30:26Z","2020-11-10T22:43:31Z"
"","1549","LUCENE-9382: update gradle to 6.4.1.","Upgrades gradle to 6.4.1. The log-check scripts doesn't work for me and fails with an internal gradle ugliness (AST incompatibility of some sort). @ErickErickson would you be able to take a look and maybe ask gradle guys on slack?","closed","build,","dweiss","2020-06-01T12:53:52Z","2020-06-02T11:49:58Z"
"","1231","SOLR-13132: single sweep iteration over base, foreground, and backgro…","Updating to master","open","","treygrainger","2020-02-02T06:13:05Z","2020-02-02T06:50:11Z"
"","1090","Update README.txt for analysis-extras","Update the analysis-extras README to include reference to including solr-analysis-extras jar.  I struggled to work out what was missing when I was using ICU for the first time, and this caught me out. There may be a better way of phrasing it.","closed","","tuftii","2019-12-17T15:29:14Z","2020-02-15T21:57:57Z"
"","775","LUCENE-8910: upgrade to icu 62.1 must be completed","Update of ICU files  I updated the version of ICU to use in GenerateUTR30DataFiles.java and checked-in the downloaded and generated files from the `ant gennorm2` target  # Checklist - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite.         * Precommit failed on -ecj-javadoc-lint-src with Solr issues not related to this change         * Lucene test suites all passed - [] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","mariemat","2019-07-11T14:23:14Z","2019-09-05T06:51:25Z"
"","1353","LUCENE-9279: Update dictionary version for Ukrainian analyzer","Update morfologik dictionary version to 4.9.0 for Ukrainian analyzer. There's about 80k of new lemmas","closed","","arysin","2020-03-14T22:14:15Z","2020-03-15T01:02:43Z"
"","1131","SOLR-14134: Add lazy and time-based evictiction of shared core concurrency metada…","Two Main Changes: - This change switches the cache from a ConcurrentHashMap to a TimeAwareLruCache with its underlying implementation simply being a Caffeine Cache. The default configurations, time delay before least-recently accessed (read or write) are evicted and maximum cache size are currently arbitrarily chosen and will later be configurable. - In ZkController#register, we evict any entry that already exists in the SharedCoreConcurrencyController cache if it already exists. This supports the collection reuse case described in the JIRA. This is done here because all ADDREPLICA paths flow through here logically and we want to evict entries before the replica gets marked ACTIVE (done here)","closed","","andyvuong","2019-12-30T21:29:34Z","2020-01-10T18:52:25Z"
"","711","Solr 12013","Trying to get all modern and use pull requests. This  fixes SOLR-12013. Throwing a NoNodeException propagated into AddReplicaCmd, CloudConfigSetService and OverseerConfigSetMessageHandler, any additional eyes welcome.  All tests pass as well as precommit if I didn't mess up this newfangled Git stuff.","closed","","ErickErickson","2019-06-11T19:21:30Z","2019-06-14T12:07:01Z"
"","713","SOLR-12013: collections API CUSTERSTATUS command fails when configset…","Trying to get all modern and use Git, so bear with me.  This fixes the problem, but by throwing a NoNodeException it affected AddReplicaCmd, CloudConfigSetService and OverseerConfigSetMessageHandler, any extra eyes on those changes appreciated.  precommit and all tests pass if I haven't messed up the  whole PR thing.","closed","","ErickErickson","2019-06-11T19:46:59Z","2019-06-13T19:52:08Z"
"","1246","LUCENE-9216: Make sure we index LEAST_DOUBLE_VALUE","Trivial test fix","closed","","iverase","2020-02-10T08:37:40Z","2020-02-10T16:39:01Z"
"","1312","Fix resource leak in TestPolicyCloud","Trivial fix of a test resource leak","closed","","tflobbe","2020-03-03T18:03:19Z","2020-03-03T19:42:24Z"
"","978","SOLR-13207: Handle query errors in calculateMinShouldMatch","Traps error that arises when the < operator is used at the end of a query field. Also handles NumberFormatException when the operand isn't a number.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  When < operator is used with no operand after it (i.e. at the end of the query field) or with a non-numeric operand, an HTTP 500 error arises caused by an ArrayIndexOutOfBoundsException or NumberFormatException respectively.  # Solution  Throws a SolrException with BAD_REQUEST as the error code when either situation arises, since the query is malformed.  # Tests  Added 2 cases in SolrPluginUtilsTest.testMinShouldMatchCalculator, for queries ""1<"" and ""1","closed","","Pr0methean","2019-10-26T22:11:11Z","2019-11-01T17:40:57Z"
"","1459","LUCENE-9347: Add support for forbiddenapis 3.0","TODO: Use task configuration avoidance also in Lucene/Solr build for speedup","closed","enhancement,","uschindler","2020-04-27T00:16:33Z","2020-04-27T09:55:03Z"
"","1110","SOLR-14138: enable request log via environ var","Today you have to edit XML to enable the jetty request log. This makes it work via environment variable.  It also gets this thing off the currently deprecated RequestLog class. SOLR_LOGS_DIR is now respected by default (today it uses hardcoded `logs/`)","closed","","rmuir","2019-12-21T23:02:41Z","2020-10-20T01:51:28Z"
"","1454","Consolidate all IW locking inside IndexWriter","Today we still have one class that runs some tricky logic that should be in the IndexWriter in the first place since it requires locking on the IndexWriter itself. This change inverts the API and now FrozendBufferedUpdates does not get the IndexWriter passed in, instead the IndexWriter owns most of the logic and executes on a FrozenBufferedUpdates object. This prevent locking on IndexWriter out side of the writer itself and paves the way to simplify some concurrency down the road","closed","","s1monw","2020-04-24T13:33:33Z","2020-04-24T17:07:23Z"
"","1325","Consolidated process event logic after CRUD action","Today we have duplicated logic on how to convert a seqNo into a real seqNo and process events based on this. This change consolidated the logic into a single method.","closed","","s1monw","2020-03-06T10:07:03Z","2020-03-09T17:47:44Z"
"","758","LUCENE-8833: Add a #load() method to IndexInput to allow preloading file content into physical memory","Today we have an all or nothing option on MMapDirectory to preload the contents of the file into physical memory. Yet, with recent efforts moving datastructures to disk if they can be accessed efficiently can benefit from a per IndexInput API to make better decisions for defaults if the index is mmapped.","open","","s1monw","2019-07-02T10:34:08Z","2019-08-15T15:03:42Z"
"","1346","LUCENE-9276: Use same code-path for updateDocuments and updateDocument","Today we have a large amount of duplicated code that is rather of complex nature. This change consolidates the code-paths to always use the updateDocuments path.","closed","","s1monw","2020-03-12T18:29:42Z","2020-03-13T19:33:16Z"
"","725","LUCENE-8865: Use incoming thread for execution if IndexSearcher has an executor","Today we don't utilize the incoming thread for a search when IndexSearcher has an executor. This thread is only idling but can be used to execute a search once all other collectors are dispatched.","closed","","s1monw","2019-06-17T16:07:16Z","2019-06-18T12:56:55Z"
"","688","LUCENE-8813: Ensure we never apply deletes from a closed DWPTDeleteQueue","Today we don't have a strong protection that we add and apply deletes / updates on or from an already flushed delete queue. DWPTDeleteQueue instances are replaced once we do a full flush in order to reopen an NRT reader or commit the IndexWriter.  In LUCENE-8813 we tripped an assert that used to protect us from such an situation but it didn't take all cornercases from concurrent flushing into account. This change adds a stronger protection and ensures that we neither apply a closed delete queue nor add any updates or deletes to it.  This change also allows to speculativly freeze the global buffer that might return null now if the queue has already been closed. This is now possible since we ensure that we never see modifications to the queue after it's been closed and that happens right after the last DWPT for the ongoing full flush is done flushing.","closed","","s1monw","2019-05-28T14:58:53Z","2019-06-01T06:52:23Z"
"","963","LUCENE-9022: Never cache GlobalOrdinalsWithScoreQuery","Today we disable caching for GlobalOrdinalsQuery (https://issues.apache.org/jira/browse/LUCENE-8062) but not GlobalOrdinalsWithScoreQuery. However the GlobalOrdinalsWithScoreQuery is sometimes executed in a filter context (despite the name, e.g. when min/max are provided) so we should protect this query for the same reasons invoked in LUCENE-8062.","closed","","jimczi","2019-10-22T11:11:09Z","2019-10-22T13:14:40Z"
"","1460","Fix visibility on member variables in IndexWriter and friends","Today it looks like wild wild west inside IndexWriter and some of it's associated classes. This change makes sure all non-final members have private visibility, methods that are not used outside of IW today are made private unless they have been public. This change also removes some unused or unnecessary members where possible and deleted some dead code from previous refactoring.","closed","","s1monw","2020-04-27T08:34:51Z","2020-04-27T15:49:24Z"
"","706","LUCENE-8843: Only ignore IOException on dirs when invoking force","Today in the method IOUtils#fsync we ignore IOExceptions when fsyncing a directory. However, the catch block here is too broad, for example it would be ignoring IOExceptions when we try to open a non-existent file. This commit addresses that by scoping the ignored exceptions only to the invocation of FileChannel#force. This prevents us from suppressing an exception in case we run into an unexpected issue when opening the file.  However, fsyncing directories on Windows is not possible. We always suppressed this by allowing that an AccessDeniedException is thrown when attemping to open the directory for reading. Yet, per the above, this suppression also allowed other IOExceptions to be suppressed, and that should be considered a bug (e.g., not only the directory not existing, but any filesystem error and other reasons that we might get an access denied there, like genuine permissions issues). Rather than relying on exceptions for flow control and continuing to suppress there, we simply return early if attempting to fsync a directory on Windows (we should not put this burden on the caller).","closed","","jasontedor","2019-06-07T23:32:43Z","2019-06-11T10:50:05Z"
"","1361","LUCENE-8118: Throw exception if DWPT grows beyond it's maximum ram limit","Today if you add documents that cause a single DWPT to grow beyond its maximum ram buffer size we just keep on indexing. If a user  misuses our IndexWriter#addDocuments API and provides a very very large iterable we will run into `ArrayIndexOutOfBoundsException` down the road and abort the IndexWriter. This change is not bulletproof but best effort to ensure that we fail with a better exception message and don't abort the IndexWriter.","open","","s1monw","2020-03-18T12:39:58Z","2020-03-31T11:01:22Z"
"","732","Expose IndexSearchers executor in order to enable searcher cloning","Today if an executor was added to the IndexSearcher it's impossible to clone the searcher with it's cache, similarity and caching policy since the executor is not exposed. This adds a simple getter to make cloning easier.","closed","","s1monw","2019-06-20T14:36:31Z","2019-06-21T08:28:48Z"
"","1394","LUCENE-9300: Fix field infos update on doc values update","Today a doc values update creates a new field infos file that contains the original field infos updated for the new generation as well as the new fields created by the doc values update.  However existing fields are cloned through the global fields (shared in the index writer) instead of the local ones (present in the segment). In practice this is not an issue since field numbers are shared between segments created by the same index writer. But this assumption doesn't hold for segments created by different writers and added through IndexWriter#addIndexes(Directory). In this case, the field number of the same field can differ between segments so any doc values update can corrupt the index by assigning the wrong field number to an existing field in the next generation.  When this happens, queries and merges can access wrong fields without throwing any error, leading to a silent corruption in the index.  This change ensures that we preserve local field numbers when creating a new field infos generation.","closed","","jimczi","2020-04-01T11:05:01Z","2020-04-03T11:58:10Z"
"","755","SOLR-13592: Introduce EmbeddedSolrTestBase for better integration tests","To allow better integration of EmbeddedSolrServer, hidden implementation should be moved out of SolrJettyTestBase","closed","","thomaswoeckinger","2019-07-01T12:09:53Z","2019-09-03T20:23:55Z"
"","1289","LUCENE-9250: Add support for Circle2d#intersectsLine around the dateline.","This was actually an oversight in LUCENE-8707. The original PR did support crossing over the dateline added in b59b24d but it was lost during a refactor of the classes. This PR adds the capability again. It adds more documentation to the class as well.","closed","","iverase","2020-02-26T08:24:26Z","2020-02-28T09:46:36Z"
"","1236","Add back assertions removed by LUCENE-9187.","This time they would only apply to TestFastLZ4/TestHighLZ4 and avoid slowing down all tests.","closed","","jpountz","2020-02-04T08:40:43Z","2020-02-14T09:37:10Z"
"","1244","SOLR-14247 Remove unneeded sleeps","This test is slow because it sleeps a lot. Removing the sleeps, it still passes consistently on my machine, but I would like other folks to confirm this on their different hardware as well.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] ~I have added tests for my changes.~ - [ ] ~I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).~","closed","","madrob","2020-02-07T17:36:51Z","2020-02-11T03:14:02Z"
"","1412","Add MinimalSolrTest for scale testing","This test class does a few very basic things, but it does them at several scales. Improvements to this test will impact nearly everything else in the test suite. Everything that happens here already has test coverage via other tests, but this class is still useful for several scenarios:    * Isolating the startup and teardown costs for a cluster  * Capturing the startup time in a unit test rather than `@Before` methods  * Profiling basic setup operations like cluster start or collection creation  * Easily adjusting the scale of the tests - we rarely touch 100+ nodes in our test suite otherwise  In an effort to minimize overhead and other operations, we disable most logging. When troubleshooting specific areas you will likely find it useful to tweak these settings.  The commented run times for each method are rough measures from a single desktop computer. They will vary between environments, but the intent is to have some baseline for spotting regressions.","open","","madrob","2020-04-06T17:47:49Z","2020-04-30T02:32:40Z"
"","1345","LUCENE-9275: make TestLatLonMultiPolygonShapeQueries more resilient for CONTAINS queries","This test can fail when a circle goes over the pole as distance point to line can have quite a bit of error.","closed","","iverase","2020-03-12T12:32:08Z","2020-03-23T06:28:38Z"
"","1476","LUCENE-9278: Fix javadocs task to work on windows and with whitespace in project folder","This should fix the missing escaping of options in the javadocs options task.","closed","","uschindler","2020-05-01T15:41:45Z","2020-05-01T15:49:53Z"
"","1533","SOLR-14474","This should finish everything in Solr, I'll commit Tuesday barring objections.","closed","","ErickErickson","2020-05-23T18:42:55Z","2020-05-24T13:58:06Z"
"","1251","LUCENE-9134: Port ant-regenerate tasks to Gradle build (util and packed)","This should be the final patch. It turned out to be a 5 minute task to put the hard-coded path in the Ant build so I did that too.  I'll commit in a few and close this JIRA.","closed","","ErickErickson","2020-02-11T15:18:07Z","2020-02-11T23:56:20Z"
"","870","SOLR-13753: ant precommit GitHub action for all PRs","This runs with Java11","closed","","anshumg","2019-09-10T19:32:08Z","2020-01-06T22:08:38Z"
"","1495","Revert ""Bugfix for FuzzyQuery false negative (#1493)""","This reverts commit 28e47549c8ba1a7c17ffe7d9e791e88983ef46c2. The use of RegExpQuery as a fallback has to consider that the search string may contain characters which are illegal regex syntax and need escaping.  Will rethink the approach.","closed","bug,","markharwood","2020-05-07T15:11:26Z","2020-05-07T15:28:20Z"
"","1179","LUCENE-9147: Move the stored fields index off-heap.","This replaces the index of stored fields and term vectors with two `DirectMonotonic` arrays. `DirectMonotonicWriter` requires to know the number of values to write up-front, so incoming doc IDs and file pointers are buffered on disk using temporary files that never get fsynced, but have index headers and footers to make sure any corruption in these files wouldn't propagate to the index.  `DirectMonotonicReader` gets a specialized `binarySearch` implementation that leverages the metadata in order to avoid going to the IndexInput as often as possible. Actually in the common case, it would only go to a single sub `DirectReader` which, combined with the size of blocks of 1k values, helps bound the number of page faults to 2.","closed","","jpountz","2020-01-17T09:28:25Z","2020-02-05T17:35:13Z"
"","728","LUCENE-8867: Store point with cardinality for low cardinality leaves","This pull request adds a new strategy for storing leaves with low cardinality and a new method to the IntersectsVisitor that can accept an array of docs. This can speed up searches when there are many equal points.","closed","","iverase","2019-06-18T08:20:30Z","2019-06-27T08:46:52Z"
"","1516","Lucene 9370: Remove leniency on illegal backslashes in RegExp query","This prevents illegal backslash syntax from user searches being accepted now and throws an error instead. This echoes the [Java Pattern policy](https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#bs) which is designed to allow future expansion to the regex syntax without conflict. It also corrects any misconceptions users might have about predefined character classes we do not support.  This PR also fixes a bug introduced in Lucene-9336 where searches for `\\` would crash.","closed","enhancement,","markharwood","2020-05-14T14:26:44Z","2020-05-15T10:45:19Z"
"","1224","LUCENE-9194: Simplify XYShapeQuery API","This PR tries to simplify the XYShape query API. The trick for this simplification is to introduce a new abstract called XYGeometry.  In addition this patch makes package private most of the internal classes under geo package.","closed","","iverase","2020-01-30T08:37:22Z","2020-02-07T18:36:38Z"
"","1170","LUCENE-9141: Simplify LatLonShapeXQuery API.","This PR tries to simplify the LatLonShape query API. The trick for this simplification is to introduce a new interface called `LatLonGeometry` which is implemented by objects defining queryable geometries, that is `Polygon`, `Line` and the newly introduced `Point`.   The main feature of those geometries is that they can return a `Component2D` object that can be used for query the index.","closed","","iverase","2020-01-15T14:17:27Z","2020-01-30T07:05:51Z"
"","1234","LUCENE-9211 Add compression for Binary doc value fields","This PR stores groups of 32 doc values in LZ4 compressed blocks.   ### Write performance Test results for loading 680mb of log data (1.8m docs) are as follows:  Branch | Load time (seconds, single thread) | Resulting index size (mb) ----|----|---- master| 16| 680 this PR| 11| 78  ### Read performance Time taken to read 5,000 random doc IDs from above indices  Branch | Read time (milliseconds, single thread)  ----|---- master| 284 this PR| 63  On this particular test, the read + write speeds and resulting index size were all improved over the current master implementation. Obviously performance will vary with other tests with the main factors for changes being: * size of fields,  * compress-ability of field contents, * read access patterns (hitting same compressed blocks vs different ones) * variation from doc-to-doc in field value sizes,","closed","","markharwood","2020-02-03T17:31:55Z","2020-02-18T14:02:43Z"
"","1617","LUCENE-8962: Merge small segments on commit","This PR revisits the merge-on-commit patch submitted by @msfroh a little while ago. The only change from that earlier PR is a fix for failures uncovered by TestIndexWriter.testRandomOperations, some whitespace cleanups, and a rebase on the current master branch. The problem was that updateSegmentInfosOnMergeFinish would incorrectly decRef a merged segments' files if that segment was modified by deletions (or updates) while it was being merged.  With this fix, I ran the failing test case several thousands of times with no failures, whereas before it would routinely fail after a few hundred test runs.  Co-authored-by: Michael Froh  Co-authored-by: Michael Sokolov   This is the third try after the patch itself and #1552","closed","","s1monw","2020-06-26T14:04:51Z","2020-06-27T22:07:37Z"
"","1318","LUCENE-9263: Fix wrong transformation of distance in meters to radians in Geo3DPoint","This PR removes the calculation from the planet model and move it to Geo3DUtil. Planet model is generic and it does not need and spheroid representing the earth.  @nknize can you have a look? We should fix it before the 8.5 release?","closed","","iverase","2020-03-05T07:45:33Z","2020-03-09T16:07:56Z"
"","1174","LUCENE-8621: Refactor LatLonShape, XYShape, and all query and utility classes to core","This PR refactors all LatLonShape, XYShape, query, and utility classes from sandbox to core. `experimental` tags are also removed.","closed","","nknize","2020-01-15T19:23:04Z","2020-01-22T19:23:09Z"
"","926","LUCENE-8928: Compute exact bounds every N splits","This PR recomputes the current bounds of the points every N splits for numDim >2. Therefore improving the way we partition the space. Performance test shows a nice speed up in query throughput with a small penalty in indexing time.","closed","","iverase","2019-10-05T06:28:58Z","2020-02-07T20:23:03Z"
"","898","SOLR-13661: A package management system for Solr","This PR raised just for inviting comments. A formal PR will be submitted soon","closed","","noblepaul","2019-09-25T11:50:58Z","2019-11-06T00:59:19Z"
"","1320","LUCENE-9257: Always keep FST off-heap. Remove FSTLoadMode and Reader attributes.","This PR modifies many classes because it removes Reader attributes now unused because FST is always loaded off-heap.","closed","","bruno-roustant","2020-03-05T11:34:25Z","2020-08-31T06:56:51Z"
"","707","Use append() when StringBuilder is being used, instead of regular concat.","This PR is related to the following ticket:  https://issues.apache.org/jira/browse/LUCENE-8847  As stated in the ticket, this was done in my spare time, please do not review this PR during your working hours. More details in the Jira ticket.  This specific commit affects all points in the codebase where the argument of a StringBuilder.append() call is itself a regular String concatenation. This defeats the purpose of using StringBuilder and also introduces an extra alloction. These changes should avoid that.  It's a big commit, yet it's 1 automated change, performed by Intellij's static code analysis.  ant tests have run, succeeded on local machine.","closed","","KoenDG","2019-06-09T21:42:36Z","2019-06-10T20:19:40Z"
"","1255","SOLR-14216 Exclude Healthcheck from auth (do not merge)","This PR is just to demonstrate how we whitelist paths that should not be authenticated","closed","","janhoy","2020-02-13T09:39:11Z","2021-02-28T19:58:09Z"
"","944","Update forbiddenapis to v2.7 and Groovy to v2.4.17","This PR is just for maintenance: - Update forbiddenapis to v2.7 (support for Java 12 and Java 13; allows to use `FileWriter` and `FileReader` since Java 11) - Update Groovy to 2.4.17","closed","","uschindler","2019-10-12T19:26:11Z","2019-10-12T19:42:29Z"
"","1227","SOLR-14219: Revert changes in OverseerSolrRespose and move serialization","This PR is an attempt to fix the serialization incompatibility introduced by SOLR-14095. @andywebb1975 has proposed a different approach in https://github.com/apache/lucene-solr/pull/1210, but I believe this may better resolve the problem. See discussion in https://github.com/apache/lucene-solr/pull/1210","closed","","tflobbe","2020-01-30T21:35:56Z","2020-02-04T19:56:06Z"
"","1008","LUCENE-9046: Fix wrong example in Javadoc of TermInSetQuery","This PR is about the javadoc of TermInSetQuery. There is a wrong example. It is minor but should be fixed.  Please refer to the following JIRA link: [LUCENE-9046](https://issues.apache.org/jira/browse/LUCENE-9046)","closed","","danmuzi","2019-11-13T17:05:59Z","2019-11-15T17:43:25Z"
"","1062","LUCENE-9082: Upgrade Apache Ivy version to 2.5.0","This PR is about the Apache Ivy version.  Not long ago, Apache Ivy 2.5.0 was released. (Oct 24, 2019)  Apache Ivy 2.5.0 can be worked with Java 7 or later. (https://ant.apache.org/ivy/history/2.5.0/compatibility.html) So there is no problem with our current Lucene version(8.x and 9.0).  JIRA link: [LUCENE-9082](https://issues.apache.org/jira/browse/LUCENE-9082)","open","","danmuzi","2019-12-06T16:46:02Z","2019-12-06T16:46:07Z"
"","1477","LUCENE-9321: Port markdown task to Gradle","This PR handles the markdown specific parts of the Lucene/Solr documentation:  - copy static assets (unfortunetely those are not yet unified between borth projects) - convert .md files to HTML using flexmark - create the index files by first iterating through the projects and collecting all links, creating a markdown representation of all links locally and then write as index.html to docus folder","closed","enhancement,","uschindler","2020-05-01T16:27:58Z","2020-05-17T12:46:32Z"
"","800","Lucene-8920: refactor FST.Arc and utilities","This PR contains three commits; I recommend looking at them separately since at least the first one is pretty big and mostly noise. Overall there should be no functional change, just reorg and cleanup.  The first commit makes Arc (and some other FST) members private and adds accessors (setters and getters).  The second commit refactors code that uses the setters, and removes the setters so that Arcs can only be modified from within the FST class.   The main change here was around the way that consumers would navigate from Arc to Arc by setting the internal state of an Arc to the desired index or position in the FST, and then call  `FST.readNextArc` to populate the remainder of the Arc members. With this change, Arc navigation is now done by calling either `FST.readArcByIndex` or `FST.readArcAtPosition`, which do the same thing that was being done before, just moving the code into `FST`.  One other change needed was in the memory codec. It was modifying Arc.output in order to accumulate its result. Instead it now initially copies the Arc.output and subsequently modifies its own private copy.  Finally, the third commit here refactors Arc's binarySearch implementation, which is repeated in several places into a method in `fst.Util`. I also added a few tests (in `TestUtil`) to explicitly test methods in `Util`, uncovering an off-by-one that had crept in to `Util.readCeilArc` during the refactor. That didn't seem to be caught by testing in the blocktreeords codec, its only user, but perhaps I just didn't run enough iterations.  I plan to follow up with subsequent commits for functional improvements; when I do I'll open a separate PR.","closed","","msokolov","2019-07-20T17:43:45Z","2019-07-26T19:56:43Z"
"","1173","LUCENE-8369: Remove obsolete spatial module","This PR closes off a long running issue for removing the obsolete `spatial` module which contains unused classes `MortonEncoder` and `GeoRelationUtils`.","closed","","nknize","2020-01-15T18:55:43Z","2020-01-16T17:40:31Z"
"","1249","LUCENE-9217: Add validation to XYGeometries","This PR adds validation for XYGeometries, in particular checking for non-valid values like NaN, INF and -INF.","closed","","iverase","2020-02-11T07:08:54Z","2020-02-14T12:25:17Z"
"","905","LUCENE-8990: Add estimateDocCount(visitor) method to PointValues","This PR adds the method estimateDocCount(visitor) to `PointValues` class. This can then be used as a way to estimate the cost() of ScorerSupplier instead of using estimatePointCount() that is actually used.  The reason to that is that other suppliers define it cost in respect of documents match and this can create problems when a numerical field contains more than one value.","closed","","iverase","2019-09-27T14:20:35Z","2019-10-04T08:20:13Z"
"","1188","SOLR-14044: Support collection and shard deletion in shared storage","This PR addS support for shard and collection deletion in shared storage (SOLR-14044) and also includes a major refactor of the existing BlobDeleteManager and deletion code.  The BlobDeleteManager uses refactors the existing async processing machinery and BlobDeleteManager manages two deletion pools now - the existing one for handling normal indexing flow deletion (as we push) and a pool used specifically by the Overseer for handling collection and shard deletion.   Currently working on a another test that I should add soon but the rest is review-able.","closed","","andyvuong","2020-01-20T21:46:48Z","2020-02-06T02:20:57Z"
"","1456","SOLR-13289: Support for BlockMax WAND","This PR adds support for BlockMax WAND via a `minExactHits` parameter. Hits will be counted accurately at least until this value, and above that, the count will be an approximation. In distributed search requests, the count will be per shard, so potentially the count will be accurately counted until `numShards * minExactHits`. The response will include the value `numFoundExact` which can be `true` (The value in `numFound` is exact) or `false` (the value in `numFound` is an approximation). I plan to merge this to master and 8x as it is in this PR","closed","","tflobbe","2020-04-24T19:08:28Z","2020-05-08T22:06:38Z"
"","1253","LUCENE-9150: Restore support for dynamic PlanetModel in spatial3d","This PR adds dynamic geographic datum support to Geo3D to make lucene a viable option for indexing/searching in different spatial reference systems (e.g., more accurately computing query shape relations to BKD's internal nodes using datum consistent with the spatial projection). It also refactors some of the variable names in `PlanetModel` to be consistent with the mathematical naming conventions and adds a new static Planet Model for the [Clarke 1866 Ellipsoid](https://georepository.com/ellipsoid_7008/Clarke-1866.html) used in the [North American Datum 1927](https://www.ngs.noaa.gov/datums/horizontal/north-american-datum-1927.shtml) still used in a lot of historic geospatial data.","closed","","nknize","2020-02-12T18:04:54Z","2020-03-02T22:32:39Z"
"","760","LUCENE-8311: Phrase impacts","This PR adds and leverages impacts for phrase queries. It contains two commits that are unrelated but getting them at the same time helps preserve performance: currently impacts don't specialize at all. After doing some digging, I noticed that one reason why the change made things slower is that postings specialize both the docs(+freqs) and docs+freqs+positions cases, while impacts don't specialize anything at all. So I removed the specialization for docs+freqs+positions with postings and added specialization for docs+freqs+positions with impacts instead. The assumption is that phrases are more often used for ranking than for filtering.","closed","","jpountz","2019-07-03T15:11:50Z","2019-07-09T14:01:38Z"
"","1040","LUCENE-9067: Polygon2D#contains is now thread safe","This PR adds an early termination exception for the case a point lies on the boundary of an edge tree.","closed","","iverase","2019-11-26T11:24:58Z","2019-11-27T11:17:11Z"
"","1187","LUCENE-9152: Improve line intersection detection for polygons","This patch changes the logic so we consider the boundary if no points of the triangle are inside the polygon. If all points are inside, then the boundary is not consider.","closed","","iverase","2020-01-20T09:05:54Z","2020-01-29T18:27:30Z"
"","1474","LUCENE-9354: Sync French stop words with latest version from Snowball.","This new version removed some French homonyms from the list   # Description  Sync French stop words with latest version from Snowball.  This new version removed some French homonyms from the list  # Tests  None, I am a French native speaker and reviewed the stop words myself.  # Checklist   - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","filcius","2020-05-01T10:44:10Z","2020-05-02T01:11:36Z"
"","1447","LUCENE-9340: Deprecate SimpleBindings#add(SortField)","This method is trappy; it doesn't work for all SortField types, but doesn't tell you that until runtime.  This commit deprecates it, and removes all other callsites in the codebase.  Applies on top of LUCENE-9338","closed","","romseygeek","2020-04-22T15:58:55Z","2020-04-24T11:08:17Z"
"","721","LUCENE-8781: add FST array-with-gap addressing to Util.readCeilArc","This method got overlooked when I recently added the new FST encoding. Thanks to @dsmiley  for uncovering this.","closed","","msokolov","2019-06-15T11:58:48Z","2019-06-19T20:06:36Z"
"","1136","LUCENE-9113: Speed up merging doc values' terms dictionaries.","This makes the merged view call `TermsEnum#next` on its subs rather than `#lookupOrd`.","closed","","jpountz","2020-01-02T10:47:37Z","2020-01-06T08:01:46Z"
"","741","LUCENE-8879: Improve BKDRadixSelector tests","This issue just add some test specifically for the sorting capability of class BKDRadixSelector and improves the existing ones for the selection capabilities.","closed","","iverase","2019-06-25T05:51:45Z","2019-06-26T07:52:45Z"
"","1319","LUCENE-9164: process all events before closing gracefully","This is yet another / simpler approach to https://github.com/apache/lucene-solr/pull/1274 to ensure that all event are processed if we are closing the IW gracefully. This also improves the case where we closing due to a tragic event where we don't try to be heroic and just drop all pending events on the floor.","closed","","s1monw","2020-03-05T09:44:56Z","2020-03-10T19:40:21Z"
"","839","LUCENE-8954: refactor Nori analyzer","This is the PR for refactoring of the Nori(Korean) Analyzer. There are no logical changes in the Nori itself.  Please check the following link: https://issues.apache.org/jira/browse/LUCENE-8954","closed","","danmuzi","2019-08-20T17:11:24Z","2019-08-26T18:09:17Z"
"","1616","SOLR-14590: Add RankQParserPlugin","This is still WIP, just the query parsing so far. Currently working on the indexing side of things.","closed","","tflobbe","2020-06-25T22:54:25Z","2020-06-29T17:22:51Z"
"","1114","LUCENE-9109: Use stack walker to implement TestSecurityManager's detection of test JVM exit","This is just a small improvement in Lucene/Solr master (Java 11) to detect exit of JVM in our test framework. There are other places in Lucene that use ineffective ways to inspect the stack trace.  This one optimizes the implementation of TestSecurityManager#checkExit(status) to disallow all JVM exits outside of the official test runner by using StackWalker. In addition this needs no additional permissions, because we do not instruct StackWalker to fetch all crazy stuff like Class instances of stack elements.  The way how this works is: Walk through stack trace:  - skip all internal frames (those which come before the actual exit call) - skip all frmes with the actual exit call - limit to one more frame (the method calling System.exit()) - check if that remaining frame is on our whitelist  This can only be commited to master (9.0), as it requires Java 9.","closed","","uschindler","2019-12-23T19:04:16Z","2019-12-25T11:45:27Z"
"","1576","Alternative approach to LUCENE-8962","this is just a prototype - only intended for discussion","closed","","s1monw","2020-06-13T11:17:26Z","2020-06-22T13:52:16Z"
"","929","SOLR-13821: Package Store for storing package artefacts","This is for peer review (not yet ready for commit)  feedback welcome","closed","","noblepaul","2019-10-07T03:52:17Z","2019-11-08T01:16:10Z"
"","1322","Remove some unused lines from addBackcompatIndexes.py related to svn","This is dead code in a python script. We don't use svn anymore. I did not add corresponding git commands since the releaseWizard explicitly does an add after running the script, and noone has complained for so long time :)   Tagging @sarowe since it seems you have touched this script in the past","closed","","janhoy","2020-03-06T09:50:55Z","2020-03-10T23:43:46Z"
"","962","LUCENE-9021 QueryParser: re-use the LookaheadSuccess exception","This is basically the same as https://issues.apache.org/jira/browse/SOLR-11242 , but for Lucene QueryParser  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","pbruski","2019-10-22T09:24:19Z","2020-12-12T14:05:47Z"
"","1223","SOLR-14213: Configuring Solr Cloud to use Shared Storage","This is an initial PR that allows Solr Cloud to be configured to use shared storage or not. Prior to this, shared storage is ""on"" by default with components be lazily initialized as needed e.g. when a collection of type shared is created. This of course is difficult to do correctly.   The solution proposed by this PR is check for the presence of a  section in solr.xml that indicates the user's intention to use shared storage and initiate the components on startup. This also sets the stage for future work we need to do to move some hard coded values into configurable parameters that would be specified in sharedStorage. See the JIRA for further discussion.   Changes: - solr.xml supports a bare-bone  section that should be added to enable shared storage on the cluster - If core discovery discovers a shared collection but the cluster is not shared storage enabled, this is considered a fatal error (user error) and solr will not start in stable state - Collections of type shared can only be created if the cluster is shared storage enabled (all shared functionality is already gated behind the presence of shared replicas within shared collections) - Added a test in SimpleSharedStorageCollectionTest to ensure proper feature gating - Refactored our test setup for anything using SolrCloudSharedStoreTestCase to automatically start a mini solr cluster with a solr.xml (added solr-sharedstorage.xml) containing the sharedStorage section. Also made a change moving the local FS client setup into this parent class  so that any extending test classes don't need to worry about setting the shared directory up unless they want to mock the client themselves","closed","","andyvuong","2020-01-30T00:42:05Z","2020-02-28T02:11:36Z"
"","1274","LUCENE-9164: Prevent IW from closing gracefully if threads are still modifying","This is an alternative approach to https://github.com/apache/lucene-solr/pull/1215 It's still a rough cut and more up for discussion. @dnhatn @jpountz do you wanna take a look","closed","","s1monw","2020-02-21T11:53:08Z","2020-03-08T20:01:08Z"
"","1310","SOLR-13350: Multithreaded search using collector managers","This is almost complete. Here, all queries are multi-threaded. TODO: Implement a query time parameter (default: off) to enable multi-threaded searching.  Tests pass.","open","","chatman","2020-03-03T13:45:30Z","2020-08-31T16:11:20Z"
"","1112","SOLR-14131: add maxQueryLength option","This is a work-in-progress as I'm trying to get tests working - see https://github.com/apache/lucene-solr/pull/1107 for my attempts at this. We can manually demonstrate that the DirectSolrSpellChecker update works, but I'm struggling to understand DirectSolrSpellCheckerTest - I want to write a couple of tests that demonstrate that a given term is corrected when maxQueryLength is not set, and not corrected when maxQueryLength is less than the term length.  # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2019-12-23T13:38:39Z","2019-12-23T14:53:01Z"
"","1488","LUCENE-9321: Refactor renderJavadoc to allow relative links with multiple Gradle tasks","This is a WIP task to convert the renderJavadoc task in the Gradle build to run 2 times: - Once for the Maven artifacts with absolute links (standard ""javadoc"" gradle task replacement. Output is project local). This task will be used for precommit (limited checks, as cross-module links cannot be checked. This task is for everyday use - another task that renders all javadocs to the global documentation folder (one for Lucene, one for Solr). All links inside will be relative.  This PR currently contains: - Refactor task to own class, so we can create multiple named tasks - Add task property to generate relative links (a new closure was added to do this: It produces a relative link from the Gradle project path of current project to the linked project","closed","","uschindler","2020-05-06T08:27:53Z","2020-05-14T13:19:29Z"
"","1316","LUCENE-8929 parallel early termination in TopFieldCollector using minmin score","This is a followup to https://github.com/apache/lucene-solr/pull/1235. I found some bugs in luceneutil measuring impact on concurrent search that were making progress extremely difficult! Finally I am able to measure consistently, my sanity is mostly restored, and I think this is very nearly ready to go.  I'll post a better explanation of what this is, along with some performance measurements, in the JIRA issue.","closed","","msokolov","2020-03-04T20:55:59Z","2020-12-30T21:04:22Z"
"","904","LUCENE-8992: Share minimum score across segment in concurrent search","This is a follow up of LUCENE-8978 that introduces shared minimum score across segment in concurrent search for top field collectors that sort by relevance first.","closed","","jimczi","2019-09-27T14:12:04Z","2019-10-23T15:04:50Z"
"","1098","SOLR-13190 - added maxQueryLength parameter","This is a draft for discussion only, ref https://issues.apache.org/jira/browse/SOLR-13190   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2019-12-18T13:18:03Z","2019-12-23T15:52:54Z"
"","964","LUCENE-9023: GlobalOrdinalsWithScore should not compute occurrences when the provided min is 1","This is a continuation of https://issues.apache.org/jira/browse/LUCENE-9022  Today the GlobalOrdinalsWithScore collector and query checks the number of matching docs per parent if the provided min is greater than 0. However we should also not compute the occurrences of children when min is equals to 1 since this is the minimum requirement for a document to match.","closed","","jimczi","2019-10-22T13:50:00Z","2020-11-10T12:12:11Z"
"","970","Enable back TestTlogReplica in 8x","This is a cherry-pick of https://github.com/apache/lucene-solr/commit/98cdac82a18d4d6594804c30e1687bfedbd68838 (note this PR is against branch_8x), however `testAddDocs` fails, and seems to be related to recent metrics changes. @sigram, mind taking a look at the last commit in this PR? The reason the test fails is that the stats for `updateHandler` come back empty, and I was able to track that to the fact that `getSolrMetricsContext()` returns `null` (from the superclass)","closed","","tflobbe","2019-10-23T20:18:56Z","2019-10-24T19:01:11Z"
"","771","LUCENE-8620: Update Tessellator logic to label if triangle edges belongs to the original polygon","This information is then encoded and triangles are decoded using LatLonShape.decodeTriangle.  Relates to #608","closed","","iverase","2019-07-09T11:57:49Z","2019-09-09T09:21:53Z"
"","1620","SOLR-14590 : Add support for Lucene's FeatureField in Solr","This implements the ""Approach 2""  suggested by Varun here: https://github.com/apache/lucene-solr/pull/1616#discussion_r446404384","closed","","tflobbe","2020-06-26T21:54:05Z","2020-06-30T18:15:45Z"
"","1429","LUCENE-9286: In FST, avoid using raw types.","This fixes some compiler warnings popped up recently.","closed","","jtibshirani","2020-04-13T23:58:51Z","2020-04-14T23:44:59Z"
"","981","LUCENE-9030: Fix different Solr- and WordnetSynonymParser behaviour","This fixes an issue where sets of equivalent synonyms in the Wordnet format are parsed and added to the SynonymMap in a way that leads to the original input token not being typed as ""word"" but as SYNONYM instead. Also the original token doesn't appear first in the token stream output, which is the case for equivalent solr formatted synonym files.  Currently the WordnetSynonymParser adds all combinations of input/output pairs of a synset entry into the synonym map, while the SolrSynonymParser excludes those where input and output term are the same. This change adds the same behaviour to WordnetSynonymParser and adds tests that show the two formats are outputting the same token order and types now.","closed","","cbuescher","2019-10-28T20:06:20Z","2019-11-13T15:40:47Z"
"","765","LUCENE-8150: Remove references to `segments.gen`.","This file isn't used anymore since 4.0, so I tried to contain references to `segments.gen` to the minimum that is required to get the right exception when opening a too old index.","closed","","jpountz","2019-07-05T14:06:03Z","2019-09-04T07:51:28Z"
"","1503","LUCENE-9368: SIMD-based decoding of BKD docIds","This commit reuse the decoding technique used for posting to encode BKD docIds. One if the biggest differences is that in this case we want to expand our integers directly into an integer arrays, or feed the decoding values directly to the `IntersectVisitor`.  In order to do that we reorder the incoming docIds arrays so we can decode them later on in order.","closed","","iverase","2020-05-11T05:52:14Z","2020-05-27T09:59:42Z"
"","1303","LUCENE-9114: Improve ValueSourceScorer's Default Cost Implementation","This commit makes ValueSourceScorer's costing algorithm also take the delegated FunctionValues's cost into consideration when calculating its cost.","closed","","atris","2020-03-01T08:10:59Z","2020-03-05T03:46:51Z"
"","1615","SOLR-14588: Follow Up Fixes and Documentation","This commit is a follow up to the original commit and adds more documentation and adds timing information for circuit breaker check only if circuit breakers are enabled","closed","","atris","2020-06-25T20:18:03Z","2020-06-26T16:50:22Z"
"","742","LUCENE-8882: Extend QueryVisitor To Maintain Metadata State","This commit introduces the notion of state to QueryVisitor API, thus allowing queries to pass metadata across the entire query tree. This can allow things like upper queries making intelligent decisions for cases like IndexOrDocValuesQuery, or cases where a sorted index can benefit from a sorted index query.","open","","atris","2019-06-26T08:18:16Z","2019-07-01T08:58:29Z"
"","923","LUCENE-8988: Introduce Global Feature Based Early Termination For Sorted Fields","This commit introduces maximum-of-minimum based early termination for TopFieldCollector. LUCENE-8949 is also a part of this commit since it is a prerequisite for this change.","open","","atris","2019-10-04T10:09:06Z","2019-10-23T22:00:26Z"
"","757","LUCENE-8857: Introduce Custom Tiebreakers in TopDocs#merge","This commit introduces custom tiebreakers which allows users to specify custom tiebreakers when ordering hits to return. A default tiebreaker is introduced for tie breaking on shard index first and then docID.","closed","","atris","2019-07-01T17:30:14Z","2019-07-02T18:35:19Z"
"","734","LUCENE-8857: Introduce Custom Tiebreakers in TopDocs#merge","This commit introduces custom tiebreakers which allows users to specify custom tiebreakers when ordering hits to return. A default tiebreaker is introduced for tie breaking on shard index first and then docID.","closed","","atris","2019-06-20T20:32:54Z","2019-07-01T14:18:49Z"
"","1049","LUCENE-9074: Slice Allocation Circuit Breakers in IndexSearcher","This commit introduces accounting for the queue length of the ExecutorService being used to perform concurrent search when allocating slices for an IndexSearcher. This commit also introduces an abstraction to define custom parameters for sealing bulkheads under heavy node stress to allow better predictable behaviour of latencies under varying stress","closed","","atris","2019-12-02T10:37:21Z","2020-01-27T10:04:00Z"
"","823","LUCENE-8939: Introduce Shared Count Early Termination In Parallel Search","This commit introduces a shared counter based CollectorManager which allows accurate early termination across all its collectors, once enough hits have been globally collected.","closed","","atris","2019-08-06T06:19:20Z","2019-09-05T08:38:48Z"
"","803","LUCENE-8929: Early Terminating CollectorManager with Global Hitcount","This commit introduces a new collector and collectormanager with capability to accurately early terminate across all collectors when the total hits threshold is globally reached.","open","","atris","2019-07-23T08:08:12Z","2019-07-29T03:21:58Z"
"","729","LUCENE-8862: Introduce Collector Level Memory Accounting","This commit introduces a mechanism to track the dynamic memory utilization of Collectors and allow setting collector level limits for memory usage. A new bitset collecting Collector is introduced which supports the memory tracker functionality.","closed","","atris","2019-06-19T07:52:04Z","2019-07-01T11:47:07Z"
"","1294","LUCENE-9074: Slice Allocation Control Plane For Concurrent Searches","This commit introduces a mechanism to control allocation of threads to slices planned for a query. The default implementation uses the size of backlog queue of the executor to determine if a slice should be allocated a new thread  Supersedes #1214","closed","","atris","2020-02-27T04:56:32Z","2021-06-11T12:05:42Z"
"","1214","LUCENE-9074: Slice Allocation Control Plane For Concurrent Searches","This commit introduces a mechanism to control allocation of threads to slices planned for a query. The default implementation uses the size of backlog queue of the executor to determine if a slice should be allocated a new thread","closed","","atris","2020-01-27T10:03:37Z","2020-02-27T04:59:44Z"
"","897","LUCENE-8978: Maximal Of Minimum Scores Based Concurrent Early Termination","This commit introduces a mechanism for early termination where, for indices sorted by relevance, hits are collected in per thread queue. Full PQs publish their bottom values and the maximum of them is then used by the corresponding collectors to filter further hits","closed","","atris","2019-09-23T08:31:36Z","2019-09-25T08:06:40Z"
"","877","LUCENE-8978: Maximal Bottom Score Based Early Termination","This commit introduces a mechanism for early termination where, for indices sorted by relevance, hits are collected in per thread queue. Full PQs publish their bottom values and the maximum of them is then used by the corresponding collectors to filter further hits","closed","","atris","2019-09-12T13:11:14Z","2019-09-23T08:32:49Z"
"","1537","LUCENE-9381: Add SortOrder interface","This commit extracts a new SortOrder interface from SortField, containing only those methods that are directly necessary to implement a sort.","closed","","romseygeek","2020-05-26T10:05:26Z","2021-09-15T11:19:23Z"
"","1513","LUCENE-9369: Deprecate ValueSourceGroupSelector","This commit deprecates grouping's ValueSourceGroupSelector, and moves a copy of its code into Solr to preserve its group-by-fieldsource functionality.  AllGroupsCollectorTest and AllGroupHeadsCollectorTest are removed entirely, as their test coverage is duplicated by BaseGroupSelectorTestCase; similarly, TestGrouping and GroupingSearchTest are duplicated by both BaseGroupSelectorTestCase and BlockGroupingTest.  DistinctValuesCollectorTest is reworked to depend only on TermGroupSelector.","open","","romseygeek","2020-05-12T09:36:53Z","2020-05-12T09:36:53Z"
"","1606","SOLR-14588: Implement Circuit Breakers","This commit consists of two parts: initial circuit breakers infrastructure and real JVM memory based circuit breaker which monitors incoming search requests and rejects them with SERVICE_TOO_BUSY error if the defined threshold is breached, thus giving headroom to existing indexing and search requests to complete.","closed","","atris","2020-06-23T15:46:01Z","2020-06-25T15:36:23Z"
"","1626","SOLR-14588: Implement Circuit Breakers","This commit consists of two parts: add circuit breakers infrastructure and a ""real"" JVM heap memory based circuit breaker which monitors incoming search requests and rejects them with SERVICE_TOO_BUSY error if the defined threshold is breached, thus giving headroom to existing indexing and search requests to complete.","closed","","atris","2020-06-28T18:02:48Z","2020-07-08T18:46:24Z"
"","1481","LUCENE-9358: remove unnecessary tree rotation for the one dimensional case","This commit changes the way the multi-dimensional tree builder generates the intermediate tree representation to be equal to the one dimensional case. Therefore, the index packing logic can be changed to work on the representation and avoid unnecessary tree and leaves rotation.  A new interface is introduced to avoid copying intermediate List arrays.  split values and split dimensions are handled in different arrays which means we are increasing the number of max points the tree builders can handle.","closed","","iverase","2020-05-04T07:40:37Z","2020-05-11T16:39:16Z"
"","1464","LUCENE-9087: Build always trees with full leaves and lower the default value for maxPointsPerLeafNode","This commit changes the logic used to build BKD trees to always construct trees with full leaves (except the last one). This change gives more control in the expected behaviour of the tree. In addition the special logic the we have for 1D trees constructs the same type of trees, therefore removing some discrepancy.  In addition, this commit lowers the default for maxPointsPerLeafNode from 1024 to 512 and simplifies the logic for rotating tree leaves.","closed","","iverase","2020-04-28T16:47:40Z","2020-05-02T09:54:02Z"
"","748","LUCENE-8889: Add Tests For Accessors Of Ranges in PointRangeQuery","This commit beefs up equality tests for PointRangeQuery by deep checking range points as well. This also serves to have an internal usage of the API so it does not seem orphaned within the code base.","closed","","atris","2019-06-27T11:03:27Z","2019-06-27T11:55:15Z"
"","1232","LUCENE-9171: Add BoostAttribute handling to QueryBuilder","This commit allows QueryBuilder to add boosts to term queries and synonym queries  when a boost is specified on a particular term via a BoostAttribute.","closed","","romseygeek","2020-02-03T12:33:36Z","2020-02-27T09:51:42Z"
"","1505","LUCENE-9328: Add PooledDocValuesReader","This commit adds a PooledDocValuesReader that shares doc values iterators among several consumers, and adds it to grouping second-pass collectors.","open","","romseygeek","2020-05-11T14:38:16Z","2020-05-12T22:56:35Z"
"","1440","LUCENE-9330: Make SortFields responsible for index sorting and serialization","This commit adds a new class `IndexSorter` which handles how a sort should be applied to documents in an index: * how to serialize/deserialize sort info in the segment header * how to sort documents within a segment * how to sort documents from merging segments  SortField has a `getIndexSorter()` method, which will return `null` if the sort cannot be used to sort an index (eg if it uses scores or other query-dependent values).  This also requires a new Codec as there is a change to the SegmentInfoFormat","closed","","romseygeek","2020-04-20T11:07:28Z","2020-05-22T12:33:07Z"
"","1037","LUCENE-9062: QueryVisitor.consumeTermsMatching","This commit adds a `consumeTermsMatching()` method to QueryVisitor, allowing queries that match against a class of terms to report this back to the visitor.  It also changes highlighting code to use this new method, replacing the current implementation via `instanceof` checks.","closed","","romseygeek","2019-11-25T09:57:01Z","2019-11-27T16:29:05Z"
"","832","LUCENE-8952: Use a sort key instead of true distance in NearestNeighbor.","This commit addresses a TODO in `NearestNeighbors` around switching to `SloppyMath.haversinSortKey`. When comparing candidate hits, we now only compute a distance sort key. The sort key is converted to a true distance only when returning the final set of `FieldDocs` and when calculating the bbox for the current search area.","closed","","jtibshirani","2019-08-14T19:27:46Z","2019-08-23T16:40:33Z"
"","1428","LUCENE-7788: fail precommit on unparameterised log.trace messages","This cleans up two directories. I have yet to go through all the changes with fresh eyes, but gradle check succeeds. NOTE: gradle does not yet include the checker program, nor will it until it runs cleanly.  I intend to push multiple batches of changes to the apache repo, there will be far too many to do all at once. Once that's done, I'll make the changes to Gradle precommit to fail on suspicious log messages.  So unless there are objections, I'll: - run ant precommit and test on these changes. - push these changes to Apache tomorrow morning. - start on another batch after I push. - rinse, repeat","closed","","ErickErickson","2020-04-13T18:22:15Z","2020-04-16T14:25:52Z"
"","1397","LUCENE-9304: Refactor DWPTPool to pool DWPT directly","This change removes the `ThreadState` indirection from DWPTPool and pools DWPT directly. The tracking information and locking semantics are mostly moved to DWPT directly and the pool semantics have changed slightly such that DWPT need to be _checked-out_ in the pool once they need to be flushed or aborted. This automatically grows and shrinks the number of DWPT in the system when number of threads grow or shrink.  Access of pooled DWPTs is more straight forward and doesn't require ordinal. Instead consumers can just iterate over the elements in the pool.  This allowed for removal of indirections in DWPTFlushControl like `BlockedFlush`, the removal of DWPTPool setter and getter in `IndexWriterConfig` and the addition of stronger assertions in DWPT and DW.","closed","","s1monw","2020-04-02T20:25:43Z","2020-04-11T10:23:47Z"
"","783","LUCENE-8914: Move compare logic to IntersectVisitor in FloatPointNearestNeighbor","This change proposes to move the logic for discarding inner modes to the IntersectVisitor. This helps taking advantage of the change introduced in LUCENE-7862.","closed","","iverase","2019-07-15T06:18:01Z","2019-07-17T12:14:22Z"
"","1431","Move DWPT private deletes out of FrozenBufferedUpdates","This change moves the deletes tracked by FrozenBufferedUpdates that are private to the DWPT and never used in a global context out of FrozenBufferedUpdates.","closed","","s1monw","2020-04-14T12:04:55Z","2020-04-14T19:37:20Z"
"","1558","LUCENE-9398: BKD tree is now read always off-heap","This change makes the BKD index to be read always off-heap.","closed","","iverase","2020-06-09T11:34:30Z","2020-06-10T06:21:48Z"
"","1451","LUCENE-9345: Separate MergeSchedulder from IndexWriter","This change extracts the methods that are used by MergeScheduler into a MergeSource interface. This allows IndexWriter to better ensure locking, hide internal methods and removes the tight coupling between the two complex classes. This will also improve future testing.","closed","","s1monw","2020-04-23T10:25:02Z","2020-04-24T13:05:31Z"
"","1538","LUCENE-9368: Use readLELongs to read docIds on BKD leaf nodes","This change changes the way we read docIds from the index using readLELongs so we can read most of the docs in one batch into a temporary array. This allows the compiler to run more efficient loops as well as we are copying information between two arrays.  In addition we add two need encoding paths for int8 and int16.","closed","","iverase","2020-05-27T09:58:45Z","2022-02-04T14:17:57Z"
"","1585","LUCENE-8962: Allow waiting for all merges in a merge spec","This change adds infrastructure to allow straight forward waiting on one or more merges or an entire merge specification. This is a basis for LUCENE-8962.","closed","","s1monw","2020-06-16T20:23:25Z","2020-06-17T20:48:19Z"
"","977","SOLR-10751: Don't replicate index when the master version is 0","This also fixes SOLR-11094","open","","tflobbe","2019-10-25T22:49:12Z","2019-11-14T23:43:29Z"
"","1241","Gradle util","This adds the generation targets for util/packed and util/automaton.  Interestingly, for whatever reason my local Python doesn't do anything weird like it did when regenerating the html entities, the generated code is identical.  One thing I'd like to draw attention to is that I had to change createLevAutomata.py to path to the new place moman is downloaded to.  I'll merge upstream sometime over the weekend probably barring objections.  I think this finishes off the regeneration work, so I'll close LUCENE-9134 after merging.","closed","","ErickErickson","2020-02-05T14:06:21Z","2020-02-10T13:21:53Z"
"","1248","LUCENE-9134: Port ant-regenerate tasks to Gradle build","This adds the generation targets for util/packed and util/automaton.  For whatever reason my local Python doesn't do anything weird like it did when regenerating the html entities, the generated code is identical.  One thing I'd like to draw attention to is that I had to change createLevAutomata.py to path to the new place moman is downloaded to.  I'll merge upstream in the next day or two barring objections.  I think this finishes off the regeneration work, so I'll close LUCENE-9134 after merging.","closed","","ErickErickson","2020-02-10T13:24:44Z","2020-02-11T15:14:56Z"
"","1284","LUCENE-9247: Add tests for `checkIntegrity`.","This adds a test to `BaseIndexFileFormatTestCase` that the combination of opening a reader and calling `checkIntegrity` on it reads all bytes of all files (including index headers and footers). This would help detect most cases when `checkIntegrity` is not implemented correctly.","closed","","jpountz","2020-02-24T15:40:39Z","2020-02-28T13:20:01Z"
"","705","LUCENE-8842: add a pull request template","This adds a template for pull requests to the top level of the project.  I'm not sure if the commented out section at the top that explains how to create an issue would appear to users, I was never able to find out from other examples I looked at. If it does not, we could modify the template to make some variation of that text appear to users and they could delete it (although it's likely some would not...probably not the end of the world).","closed","","ctargett","2019-06-07T18:56:08Z","2020-10-20T01:51:47Z"
"","1245","LUCENE-9146: Create gradle precommit action","This adds a gradle precommit action w/ Java11 for all branches.","closed","","anshumg","2020-02-07T22:07:27Z","2020-02-09T17:32:12Z"
"","1139","SOLR-14164: Remove Solr's FunctionRangeQuery & ValueSourceRangeFilter.","These are redundant with Lucene's FunctionRangeQuery.  https://issues.apache.org/jira/browse/SOLR-14164","closed","","dsmiley","2020-01-03T20:55:08Z","2021-05-02T00:21:13Z"
"","1340","Jira/lucene-9004: Improve the recall and running performance of Lucene HNSW --- Refactor the implementation of HNSW according to Faiss","There were some misunderstandings in the implementation of HNSW algorithm, making the recall of Lucene HNSW about [5% lower](https://github.com/jtibshirani/ann-benchmarks/pull/1) than Faiss. In addition, Lucene HNSW runs pretty slow than Faiss, especially in building index. This PR is meant to implement the HNSW algorithm in a way similar to Faiss and make it indexing faster. Here're some reasons that why I consider refactoring the implementation, 1) The major target is to **improve the recall percent and develop a correct implementation**; 2) The performance of Lucene HNSW is another concern. In current implementation, it takes over 20 hours to index sift-1M-128D dataset (single segment or forceMerge(1)), making it difficult to be used in real-life search scenarios. I think it's not a good idea to use List.copyOf in Layer.getNodes() since it was invoked too frequently! After removing it, **the indexing time of sift-1M-128D dataset decreases from 20+ hours to about 16 minutes**; 3) Actually , the relationship between neighborhood is not symmetric. Taking the following picture for example, assume that max connection = 2, for Node3 its nearest neighbors are Node1 and Node2, but the nearest neighbors of Node1 are Node2 and Node4. So I think the operations of connecting nodes and shrinking neighbors in current implementation are incorrect! ![image](https://user-images.githubusercontent.com/8521429/76395621-c04fb880-63b2-11ea-9a35-bcce5ac95e84.png) 4) When insert a node at layer l, HNSW tries to greedy search the nearest one node, denoted by Nnearest, from max level layer to (l + 1) layer. Starting from layer l to layer 0, HNSW uses Nnearest rather than all the neighbors of previous layer to search efContruction nearest neighbors. In Faiss, Nnearest keeps unchanged from layer l to layer 0, while re-selecting the nearest node in [nmslib](https://github.com/nmslib/hnswlib/blob/master/hnswlib/hnswalg.h). I followed the implementation of [Faiss HNSW](https://github.com/facebookresearch/faiss/blob/master/impl/HNSW.cpp) since it's much easier and more efficient. 5) The neighborhood shrinkage strategy is incorrect. It should follow triangle inequality. 6) The default connection size (maxConn) in HNSWGraphWriter is 6, which is too small. To balance the recall and the performance, set the parameter to at least 16 would be better!  Following are my test results using Sift1M dataset, where the second and third columns are the recall and qps, respectively: LuceneGraph(M=6, ef_const=50, ef=80)          0.914      106.970 LuceneGraph(M=6, ef_const=50, ef=100)        0.932      122.905 LuceneGraph(M=6, ef_const=50, ef=200)        0.967      106.091                                              LuceneGraph(M=6, ef_const=50, ef=400)        0.984       86.637 LuceneGraph(M=6, ef_const=50, ef=600)        0.988       71.704 LuceneGraph(M=6, ef_const=50, ef=800)        0.990       61.559  Built index in 972.1924469470978 seconds!  Benchmark tool for Lucene IVFFlat/HNSW could be found [here](https://github.com/irvingzhang/ann-benchmarks).  I think both the recall and running performance are much closer to Faiss and much better than current implementation! And the larger M, the higher recall (in average)!  ![image](https://user-images.githubusercontent.com/8521429/76408933-ae791000-63c8-11ea-975f-6bacf313236e.png)  It is likely not the best implementation. Further suggestions and discussions are welcomed.","open","","irvingzhang","2020-03-11T09:16:17Z","2020-03-12T13:51:55Z"
"","1130","SOLR-14109 Always log to stdout from zkcli.sh","There should be no reason to log anywhere else. This fixes a problem with bogous logging when running the tool from Solr's Docker image.","closed","","janhoy","2019-12-30T15:09:44Z","2019-12-30T15:28:29Z"
"","1129","SOLR-14109 Always log to stdout from zkcli.sh","There should be no reason to log anywhere else. This fixes a problem with bogous logging when running the tool from Solr's Docker image.","closed","","janhoy","2019-12-30T15:07:17Z","2019-12-30T15:28:30Z"
"","1096","SOLR-14109 Always log to stdout from zkcli.sh","There should be no reason to log anywhere else. This fixes a problem with bogous logging when running the tool from Solr's Docker image.","closed","","janhoy","2019-12-18T09:24:07Z","2019-12-30T15:05:48Z"
"","673","Replace instances of Math.random with Random.nextDouble","There is a performance overhead associated with Math.random these is unassociated with Random.nextDouble, which performs the same functionality as Math.random. Switching to Random.nextdouble removes this redundancy overhead.","open","missing Jira,","bd2019us","2019-05-11T17:53:44Z","2020-03-06T11:17:37Z"
"","1285","Revert ""LUCENE-8954: refactor Nori analyzer""","There is a lint error in patch. Sorry for confusing.","closed","","danmuzi","2020-02-24T17:44:00Z","2020-02-24T17:44:32Z"
"","1229","LUCENE-8656: Deprecations in FuzzyQuery","There aren't any functional changes here. This just takes Alan's original patch and updates it on master, and puts the deprecation annotations in queryparser code so now we have a clean compile of lucene/queryparser, no deprecation or lint warnings under the gradle build even after running the javacc task.  precommit and tests pass both for Ant and Gradle.  I'll commit this early next week barring objections or earlier if it gets +1...","closed","","ErickErickson","2020-02-01T14:29:47Z","2020-02-03T13:52:57Z"
"","1643","SOLR-14619 avoid SolrIndexSearcher.getDocSet(q)","There are places where we get a DocSet from a query when we don't need to. Also, improved DocSet.getTopFilter to implement DocSetProducer so as to avoid possible re-construction of the DocSet. Deprecated SolrPluginUtils.numDocs. Nobody calls it. Maybe @yonik or @mkhludnev might be interested in code reviewing?","open","","dsmiley","2020-07-02T20:00:05Z","2020-09-01T06:09:39Z"
"","1120","LUCENE-9110: Switch tests to use StackWalker","There are a lot of tests (especially around IndexWriter) that look into stack traces to inject failures or check that some methods were called in their call stack.  This issue will refactor all those tests by adding a few methods to LuceneTestCase that make it easy to verify if some method call/class is in stack trace. On master (Java 11) we can use StackWalker to do this checks, which has a speedup of sometimes >>2 times (depending on how deep you dive into call stack).  There are a few tests (only 3) that do more complex stack trace analysis. Those should be refactored at some point. For now I added a deprecated method to get the whole StackTrace in Java 11, which is still 2 times faster than using an Exception.  For branch 8.x i will apply the same patch, just the LuceneTestCase methods use the old ""Java 8"" way to inspect stack trace using the thread's stack trace (which is very expensive).  So this issue is mainly about refactoring the tests to use a common method pattern to check the existence of stack frames.  One important thing is: Using StackWalker makes sure that the stack is ""correct"". Stacks from Thread or Exception may miss some frames, as it does not deoptimize the code. So depending on JVMs and optimizations (e.g. Graal), call stacks may change if we still use old code for analysis. This is no longer an issue for Java 8, but may be in future.","closed","","uschindler","2019-12-24T16:10:22Z","2019-12-27T10:54:06Z"
"","1106","LUCENE-9106: UniformSplit postings format allows extension of block/line serializers.","There are 4 public method signature changes in UniformSplitTermsReader, UniformSplitTermsWriter and their shared-terms extensions. However this seems to me it is still ok to push that to 8x.","closed","","bruno-roustant","2019-12-20T14:17:06Z","2020-08-31T06:57:56Z"
"","1313","LUCENE-8962: Split test case","The testMergeOnCommit test case was trying to verify too many things at once: basic semantics of merge on commit and proper behavior when a bunch of indexing threads are writing and committing all at once.  Splitting the test into two should make the tests more robust - one will verify basic behavior, with strict assertions on invariants, while the other just verifies that everything gets indexed and we don't throw an exception when multiple threads are indexing and merging on commit.  Also, the part of the test that is now testMultithreadedMergeOnCommit can take several seconds to run, so moving it to the @Nightly set.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fixing an intermittent test failure on testMergeOnCommit.  # Solution  Split the logic from testMergeOnCommit into two test cases. The basic test has consistently passed, and actually verifies the merge on commit invariants. The more complicated, more potentially-brittle multithreaded test doesn't necessarily satisfy clear invariants (as we may be merging on commit from multiple threads, which could result in multiple segments in the end), but it should never throw an exception or lose any updates.  # Tests  Split existing test case into two test cases. Ran tests multiple times.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","msfroh","2020-03-03T22:16:42Z","2020-03-06T04:41:40Z"
"","1375","Fix TestXYPoint#testEqualsAndHashCode","The test should not assume that different objects have always different hashcodes.","closed","","iverase","2020-03-25T07:22:00Z","2020-03-27T17:52:06Z"
"","1418","LUCENE-9309: Wait for #addIndexes merges when aborting merges","The SegmentMerger usage in IW#addIndexes(CodecReader...) might make changes to the Directory while the IW tries to clean-up files on rollback. This causes issues like FileNotFoundExceptions when IDF tries to remove temp files. This changes adds a waiting mechanism to the abortMerges method that, in addition to the running merges, also waits for merges in addIndices(CodecReader...)","closed","","s1monw","2020-04-08T20:15:03Z","2020-04-10T10:55:06Z"
"","1521","Lucene-9371: Allow external access to RegExp's parsed structure","The RegExp parser holds a useful representation of the parsed logic in a regular expression but does not have a public API that would allow the objects to be transformed to new forms e.g. to render to an English explanation of the logic. This PR exposes that capability by making the internal fields public and final. I toyed with the idea of just adding getter methods but the internal state of this class should ideally be made immutable anyway so instead I opted for making the parsed fields final.","closed","enhancement,","markharwood","2020-05-15T14:40:48Z","2020-05-19T16:38:01Z"
"","1358","SOLR-14256: replaced EMPTY with empty() to fix deadlock","The previous change has caused sporadic test failures relating to class loading race conditions.  Basically an abstract base class cannot declare a static field initialized to that of a subclass.  Apparently on interfaces, this is okay -- DocSet was an interface before.  I had changed from an interface to abstract class before as a simplification and because I wanted to lock down the implementations to the only two that existed, and additionally many methods took a pair of DocSets and made assumptions bout the only implementations that exist.  I still like that, even if I need to not have a public static final EMPTY.","closed","","dsmiley","2020-03-17T20:53:51Z","2020-03-18T12:33:04Z"
"","1095","SOLR-14095: Let the overseer use javabin to store responses in ZooKeeper","The Overseer currently uses java serialization to store command responses in ZooKeeper. This commit changes the code to use Javabin instead, while allowing Java serialization with a System property in case it's needed for compatibility. This PR is for merge into master and branch_8. I'll have a new PR after this is merged to remove the back compat code from master.","closed","","tflobbe","2019-12-17T22:54:17Z","2019-12-20T22:55:06Z"
"","1186","LUCENE-9134: Port ant-regenerate tasks to Gradle build","The only file that counts here really is lucene/queryparser/build.gradle. All the rest are a result of running the regenerate task.  There are a couple of dodgy bits, see //nocommit in the build.gradle file.  NOTES:     this does not address any of the other regenerate tasks yet.     This also incorproates LUCENE-9151, I'll close that JIRA.  Tests  precommit and the full test suite (both gradle and ant) pass, so I think it's close for this part of the regenerate task","closed","","ErickErickson","2020-01-19T00:34:12Z","2020-01-22T17:04:56Z"
"","1336","LUCENE-9270: Update Javadoc about normalizeEntry in the Kuromoji DictionaryBuilder","The normalizeEntry option is missing from the Javadoc of Kuromoji DictionaryBuilder. Without this explanation, users don't know what it means until they see the code. Also, if user follows the usage of Javadoc, it will not be built.  Please check the following JIRA issue: [LUCENE-9270](https://issues.apache.org/jira/browse/LUCENE-9270)","closed","","danmuzi","2020-03-10T18:19:59Z","2020-03-11T17:50:37Z"
"","1047","MINOR: Fix Incorrect Constant Name in Codec Docs","The name was wrong here. Also, added a link to make this doc more fun to navigate in HTML form and make sure it doesn't go bad again.","closed","","original-brownbear","2019-11-30T20:46:18Z","2020-01-06T08:03:41Z"
"","954","LUCENE-9007: MockSynonymFilter should add TypeAttribute","The MockSynonymFilter should add the type TypeAttribute to the synonyms it generates in order to make it a better stand-in for the real filter in tests.","closed","","cbuescher","2019-10-15T17:31:19Z","2021-03-18T11:35:05Z"
"","1389","LUCENE-9298: Improve RAM accounting in BufferedUpdates when deleted doc IDs and terms are cleared","the method clearDeletedDocIds in BufferedUpdates.java has a bug, it can't reset bytesUsed correctly. ```java void clearDeletedDocIds() {   deleteDocIDs.clear();   bytesUsed.addAndGet(-deleteDocIDs.size() * BufferedUpdates.BYTES_PER_DEL_DOCID); } ``` this PR will fix it.","closed","","bringyou","2020-03-30T03:50:12Z","2020-04-10T10:31:04Z"
"","1045","LUCENE-9072: Find matching terms from Matches API","The Matches API tells us which queries have matched, and what positions they have  matched at, but not necessarily which actual terms matched. This commit adds a `getMatchingTerms` method to `Matches`, allowing users to build results pages that  indicate which specific query terms are present or absent in each top-k hit.","open","","romseygeek","2019-11-29T12:45:22Z","2020-01-07T10:03:52Z"
"","1228","LUCENE-9197: fix wrong implementation on Point2D#withinTriangle","The logic is messed up, this PR fix it. It actually simplifies as well the logic for relateTriangle (I have no idea what I was thinking of in the first implementation :)","closed","","iverase","2020-01-31T13:35:17Z","2020-02-04T06:10:09Z"
"","792","Update Wordnet file format description link","The link to the description of the Wordnet prolog database files seems outdated. This change replaces it with a working link.","closed","","cbuescher","2019-07-17T14:23:51Z","2019-10-10T09:39:55Z"
"","1367","ivy settings: local maven repo pattern needs classifier","The ivy settings config has a local maven repo resolver (added by @dweiss 8 years ago).  It's commented out so it's not essential that we get it right.  It has a bug in the pattern that forgot the classifier. https://stackoverflow.com/questions/8617963/ivysettings-xml-add-local-maven-path Locally I configure the build to use the local maven repo and this has been a problem because spatial4j's main JAR file is resolved when trying to get the test JAR!  (I don't think I need a JIRA for this but I can create one if someone really wants that ceremony)","closed","","dsmiley","2020-03-20T14:19:43Z","2020-04-05T03:48:54Z"
"","856","LUCENE-8965 SometimesConcurrentMergeScheduler","The initial version here is copied with simple modifications from one I wrote at Salesforce.  I do not propose we actually commit this version, it's more of a straw-man and show & tell.  Instead of the awkward subclassing, it'd be nicer to enhance CMS directly.","closed","","dsmiley","2019-09-04T19:12:00Z","2020-06-22T00:00:02Z"
"","1461","LUCENE-9348: Add a base grouping test for use with different GroupSelector implementations","The grouping module tests currently all try and test both grouping by term and  grouping by ValueSource. They are quite difficult to follow, however, and it is not  at all easy to add tests for a new grouping type. This commit adds a new BaseGroupSelectorTestCase class which can be extended to test particular GroupSelector implementations, and adds tests for TermGroupSelector and ValueSourceGroupSelector.","closed","","romseygeek","2020-04-27T16:25:30Z","2020-05-04T12:55:04Z"
"","1484","LUCENE-7889: Allow grouping on Double/LongValuesSource","The grouping module currently allows grouping on a SortedDocValues field, or on a ValueSource.  The latter groups only on exact values, and so will not perform well on numeric-valued fields.  This commit adds the ability to group by defined ranges from a Long or DoubleValuesSource.","closed","","romseygeek","2020-05-04T13:02:11Z","2020-05-11T16:34:02Z"
"","756","LUCENE-8896: Override default implementation of IntersectVisitor#visit(DocIDSetBuilder, byte[]) for several queries","The following queries have been implemented:  - LatLonShapeQuery  - RangeFieldQuery  - LatLonPointInPolygonQuery  - LatLonPointDistanceQuery  - PointRangeQuery  - PointInSetQuery","closed","","iverase","2019-07-01T17:11:26Z","2019-07-02T07:13:43Z"
"","1519","SOLR-14482: Fix compile-time warnings in solr/core/search/facet","The facet code has a lot of classes declared in a file with a different name. This tries to fix all the warnings in solr/core/search/facet. gradlew check passes. Here for comments.  Plus one log message I noticed that I'd checked in when I was debugging.  Make any comments by EOD Friday, I'll be pushing this over the weekend otherwise after verifying that it all works under ant precommit/test too and checking it over one more time.","closed","","ErickErickson","2020-05-15T12:25:41Z","2020-05-20T23:47:30Z"
"","1518","SOLR-14482: Fix compile-time warnings in solr/core/search/facet","The facet code has a _lot_ of classes declared in a file with a different name. This tries to fix all the warnings in solr/core/search/facet. gradlew check passes. Here for comments.  Plus one log message I noticed that I'd checked in when I was debugging.  Make any comments by EOD Friday, I'll be pushing this over the weekend otherwise after verifying that it all works under ant precommit/test too and checking it over one more time.","closed","","ErickErickson","2020-05-14T22:29:07Z","2020-05-15T12:30:26Z"
"","696","LUCENE-8831: Fix bug on hashCode for LatLonShapeBoundingBoxQuery","The current problem is the implementation in Rectangle2D but looking into other shape implementations like Line2D or Polygon2D, they do not implement has code, equals or toString methods. To keep symmetry on the implementations this PR removes those implementation in Rectangle2D and uses the provided Rectangle object in LatLonShapeBoundingBoxQuery to generate the hashCode.","closed","","iverase","2019-06-05T10:24:25Z","2019-06-05T14:05:51Z"
"","1082","SOLR-13984: add (experimental, disabled by default) security manager support","The current policy file used by tests is moved to solr/server Additional permissions are granted for the filesystem locations set by bin/solr, and networking everywhere is enabled.  This takes advantage of the fact that permission entries are ignored if properties are not defined: https://docs.oracle.com/javase/7/docs/technotes/guides/security/PolicyFiles.html#PropertyExp","closed","","rmuir","2019-12-14T07:02:54Z","2020-10-20T01:51:30Z"
"","1038","LUCENE-9063: Speed up computation of impacts.","The current design of CompetitiveImpactAccumulator treats norms in -128..127 as a special case that should be optimized. This commit goes a bit further by treating it as the normal case, and only ever adding impacts to the TreeSet if the norm is outside of the byte range. It avoids a number of operations on TreeSets like adding impacts or removing redundant impacts.","closed","","jpountz","2019-11-25T10:22:52Z","2019-11-26T10:50:02Z"
"","1007","LUCENE-9045: Do not use TreeMap/TreeSet in BlockTree and PerFieldPostingsFormat","The BlockTreeTermsReader.iterator() was detected by a profiler to spend abnormal time in TreeMap.keySet().iterator() iteration. The fix is to use a simple sorted list once the collection of fields is built. Same detection and fix for PerFieldPostingsFormat.","closed","","bruno-roustant","2019-11-13T09:33:04Z","2019-12-10T21:34:08Z"
"","1430","SOLR-13101: SHARED replica's distributed indexing","The basic purpose of this change was to refactor most of the SHARED replica logic out of DistributedZkUpdateProcessor. Along with that refactoring I came across couple of functional issues in the logic that have been fixed too.  Functional fixes: -If a replica looses its leadership in the middle of indexing batch, it should still push its changes to the shared store. -SHARED replica does not need to process soft commits and does not need to broadcast hard commits to all the shards of a collection. -Previously we would pull from the shared store even when the doc being added/deleted is not meant for the current core/shard. -Previously DistributedZkUpdateProcessor#processDelete was running its pull logic even before the request had been setup (DistributedZkUpdateProcessor#setupRequest). -DistributedZkUpdateProcessorTest have been deleted in favor of new SharedStoreDistributedIndexingTest and SharedCoreIndexingBatchProcessorTest.  Refactoring: -Most of DistributedZkUpdateProcessor's SHARED replica logic is moved into a new SharedCoreIndexingBatchProcessor. The purpose of this class is to pull from the shared store at the start of an indexing batch (if the core is stale) and push to the shared store at the end of a successfully committed indexing batch. -CoreUpdateTracker has been deleted and its only persistShardIndexToSharedStore method has been renamed to pushCoreToSharedStore and moved to CorePusher. -BlobStoreUtilsTest#syncLocalCoreWithSharedStore is renamed to pullCoreFromSharedStore and moved into a new CorePuller class and the tests to CorePullerTests. -I did rename phrase ""blob store"" to ""shared store"" at some places in the changed classes. But it was not meant to be an exhaustive attempt.","closed","","mbwaheed","2020-04-14T04:51:07Z","2020-05-18T19:19:02Z"
"","1043","LUCENE-9071: Speed up BM25 scores.","The approach of this change is to try to detect when computing the tf component of the BM25 score would return the same result in float and double space. In order to make this condition likely, it trades little precision from the norm by rounding to the nearest float that has 10 trailing zeros.","closed","","jpountz","2019-11-28T09:33:11Z","2019-12-09T17:59:35Z"
"","874","LUCENE-8976: Use exact distance between point and bounding rectangle in FloatPointNearestNeighbor","The algorithm uses now the exact minimum distance between a point and a bounding rectangle to navigate the tree.","closed","","iverase","2019-09-11T08:35:47Z","2019-09-12T05:51:52Z"
"","726","LUCENE-8632: New XYShape Field and Queries for indexing and searching general cartesian geometries","The `LatLonShape` field and `LatLonShape` query classes added the ability to index and search geospatial geometries in the WGS-84 latitude, longitude coordinate reference system. The foundation for this capability is provided by the `Tessellator` that converts an array of vertices describing a `Point` `Line` or `Polygon` into a stream of 3 vertex triangles that are encoded as a seven dimension point and indexed using the BKD `POINT` structure. A nice property of the Tessellator is that `lat, lon` restrictions are artificial and really only bound by the API.   This commit builds on top of / abstracts the `Tessellator`  `LatLonShape` and `LatLonShapeQuery` classes to provide the ability to index & search general cartesian (non WGS84 lat,lon restricted) geometry. It does so by introducing two new base classes: `ShapeField` and `ShapeQuery` that provide the indexing and search foundation for `LatLonShape` and the `LatLonShape` derived query classes (`LatLonShapeBoundingBoxQuery`, `LatLonShapeLineQuery`, `LatLonShapePolygonQuery`) and introducing a new `XYShape` factory class along with `XYShape` derived query classes (`XYShapeBoundingBoxQuery`, `XYShapeLineQuery`, `XYShapePolygonQuery`). The heart of the cartesian indexing is achieved through `XYShapeEncodingUtils` that converts the double precision vertices into an `integer` encoded seven dimension point (similar to LatLonShape).  The test framework is also further abstracted and extended to provide a full test suite for the new `XYShape` capability that works the same way as the `LatLonShape` test suite (but applied to non GIS geometries).","closed","","nknize","2019-06-17T18:27:31Z","2019-07-08T20:29:31Z"
"","858","Harden Up TestDiversifiedTopDocsCollector","TestDiversifiedTopDocsCollector.testInvalidArguments should check for exceptions and corresponding messages, post LUCENE-8905","closed","","atris","2019-09-05T17:25:11Z","2019-09-05T17:32:18Z"
"","1557","LUCENE-9396: Improve truncation detection for points.","TestAllFilesDetectTruncation still passes with this change.","closed","","jpountz","2020-06-09T08:33:12Z","2020-06-16T10:05:37Z"
"","681","Fix SOLR-13347","Test are included in https://github.com/apache/lucene-solr/pull/665","closed","","thomaswoeckinger","2019-05-21T09:18:27Z","2019-06-12T22:56:42Z"
"","880","Tweak header format.","test","closed","","markrmiller","2019-09-15T00:01:41Z","2020-03-06T11:14:29Z"
"","1465","LUCENE-9349: TermInSetQuery should use consumeMatchingTerms in visit()","TermInSetQuery currently iterates through all its prefix-encoded terms  in order to build an array to pass back to its visitor when visit() is called.   This seems like a waste, particularly when the visitor is not actually  consuming the terms (for example, when doing a clause-count check  before executing a search).  This commit changes TermInSetQuery to use  consumeTermsMatching(), and also changes the signature of this method so  that it takes a BytesRunAutomaton supplier to allow for lazy instantiation.  In addition, IndexSearcher's clause count check wasn't counting leaves that called consumeTermsMatching().","closed","","romseygeek","2020-04-28T20:04:04Z","2020-04-29T09:19:10Z"
"","1396","LUCENE-9301: add a manifest entries to JARs.","Suggested patch. I did try to make it similar to what ant produces at the moment but I didn't verify each and every subproject so it'd be good if somebody had a look.","closed","","dweiss","2020-04-02T10:11:49Z","2020-04-04T18:57:08Z"
"","1653","Stored fields","Store schema separately for stored fields  Currently, stored fields for a document are stored in the following format: MetadataForField1, DataForField1 | MetadataForField2, DataForField2 ....  This patch changes the format to: MetadataField1, MetadataField2...|DataField1, DataField2, ...  As metadata are combined together, we hope that this will improve compression.  Co-authored-by: Colin Goodheart-Smithe","open","","mayya-sharipova","2020-07-06T13:25:23Z","2020-07-06T17:15:37Z"
"","1438","SOLR-14414 Admin UI refresh WIP","Still many things to fix, but a big improvement in safety. Still stuck in the call stack searching for memory leaks and debugging control flow issues.   * SOLR-14414: removes EOL code and hopefully makes usability improvements for Solr community as a result.   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The current admin UI, while very useful, is dangerous. Upgrading the Admin UI to use non-EOL code. This app uses the latest version of Angular.   # Solution  Uses the Angular CLI and the dashboard work from a guy with a great design eye. Also in TypeScript.  # Tests  There's a lot of tests here, integration tests, and manual integration tests. There is still some inconsistency here, but working to improve it over the next couple months.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-04-18T18:19:40Z","2020-04-22T16:50:56Z"
"","1157","Add RAT check using Gradle","Still a WIP, but most of the way there.  Need to move the task invocation out to the modules where it belongs. I didn't completely understand the logic for what was included and excluded and how we made those decisions initially.  Am not sure if we should be printing all the violations by default or hiding that behind a `--info` or `--verbose` flag.","closed","","madrob","2020-01-09T22:13:16Z","2020-01-15T16:09:40Z"
"","1363","LUCENE-9284: Refactor out logic that serialises the BKD tree","Split BKDwriter into the logic that build the Kd-tree and the logic that serialises the tree into the index.","closed","","iverase","2020-03-19T11:19:33Z","2020-07-27T16:35:50Z"
"","747","LUCENE-8888: Improve distribution of points with data dimension in BKD tree leaves","Sorting/selection algorithms on the BKD tree are updated to tie-break on the data dimensions when they exist.","closed","","iverase","2019-06-27T07:11:05Z","2019-07-04T09:19:09Z"
"","1547","SOLR-14525 For components loaded from packages SolrCoreAware, ResourceLoaderAware are not honored","SOLR-14525 For components loaded from packages SolrCoreAware, ResourceLoaderAware are not honored","closed","packages,","noblepaul","2020-05-31T09:54:38Z","2020-06-25T05:55:44Z"
"","1153","SOLR-14158: Package trusted keys to come from Package Store, not ZK","SOLR-14158: Package trusted keys to come from Package Store, not ZK  Eliminates need for ZK for trusted keys for package manager.","closed","","chatman","2020-01-07T17:35:33Z","2020-03-03T19:24:24Z"
"","1094","SOLR-14106: Cleanup Jetty SslContextFactory usage","SOLR-14106: Cleanup Jetty SslContextFactory usage  Jetty 9.4.16.v20190411 and up introduced separate client and server SslContextFactory implementations. This split requires the proper use of of SslContextFactory in clients and server configs.  This fixes the following * SSL with SOLR_SSL_NEED_CLIENT_AUTH not working since v8.2.0 * Http2SolrClient SSL not working in branch_8x","closed","","risdenk","2019-12-17T22:40:49Z","2019-12-20T15:40:55Z"
"","1055","SOLR-13932 Review directory locking and Blob interactions","SOLR-13932: Review directory locking and Blob interactions  Take into account all local files in a local index to avoid conflict with files pulled from Blob. Do not make local copy of directory for pushing to Blob. Minor changes.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2019-12-03T15:50:58Z","2020-09-16T22:37:50Z"
"","960","SOLR-13841: Add jackson databind annotations to SolrJ classpath","SOLR-13841:Add jackson databind annotations to SolrJ classpath","closed","","noblepaul","2019-10-19T22:50:17Z","2020-03-03T19:24:37Z"
"","802","SOLR-13626: document the SystemInfoHandler","SOLR-13626: add documentation for SystemInfoHandler  # Description  Add missing documentation for /admin/info/system which implicit-requesthandlers currently links to a 404  # Solution  Provide some basic documentation.  # Tests  Successfully ran ant precommit.  Ran ant build-site from the solr/solr-ref-guide directory and reviewed the result.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the Ref Guide (for Solr changes only).","open","","tonycoz","2019-07-22T10:11:12Z","2019-07-22T10:13:25Z"
"","774","SOLR-13565, SOLR-13553","SOLR-13553: Node level custom RequestHandlers SOLR-13565:Node level runtime libs loaded from remote urls","closed","","noblepaul","2019-07-11T13:35:04Z","2020-07-15T06:45:25Z"
"","766","SOLR-13507: Remove support for addr parameter from the /solr/admin/zookeeper endpoint. (#759)","SOLR-13507: Remove support for addr parameter from the /solr/admin/zookeeper endpoint. (#759)  back-port from master https://github.com/anshumg/lucene-solr/commit/b7090d9c25ba430442628b0dc77c7c700cb35b33","closed","","anshumg","2019-07-05T17:14:20Z","2019-07-05T17:15:14Z"
"","759","SOLR-13507: Remove support for addr parameter from the /solr/admin/zookeeper endpoint.","SOLR-13507: Remove support for addr parameter from the /solr/admin/zookeeper endpoint.","closed","","anshumg","2019-07-03T05:17:55Z","2019-07-03T17:50:02Z"
"","893","Port to 7x","SOLR-13257: Support deterministic replica routing  Deterministic replica routing can help improve caching and allow a more consistent paging when sorting by score  This closes #677  # Conflicts: #	solr/CHANGES.txt #	solr/core/src/java/org/apache/solr/handler/component/HttpShardHandlerFactory.java #	solr/core/src/test/org/apache/solr/handler/component/TestHttpShardHandlerFactory.java #	solr/solrj/src/java/org/apache/solr/common/params/ShardParams.java   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","nddipiazza","2019-09-20T16:27:56Z","2019-09-23T01:03:09Z"
"","849","SOLR-13257: Cleanup code and make the AffinityReplicaTransformer constructors private (#848)","SOLR-13257: Cleanup code and make the constructors private as the constructor is supposed to be called via the static getInstance method.","closed","","anshumg","2019-08-27T23:52:05Z","2019-08-28T15:47:26Z"
"","1359","SOLR-13101: Make dir hash computation optional and resilient","Slight optimizations in ServerSideMetadata given that push operations do not require computing a directory hash of a core's index directory. This change makes the computation optional and resilient by also moving the optional computation to the latest commit capture's retry loop.   If we hit an exception (file not found, etc) either capturing files from the latest commit or during the hash computation (if captureDirHash=true), we should try and recompute both snapshots anyway.","closed","","andyvuong","2020-03-18T00:17:55Z","2020-03-24T14:25:33Z"
"","1351","LUCENE-9280: Collectors to skip noncompetitive documents","Similar how scorers can update their iterators to skip non-competitive documents, collectors and comparators should also provide and update iterators that allow them to skip non-competive documents  This could be useful if we want to sort by some field.","closed","","mayya-sharipova","2020-03-13T22:19:01Z","2020-06-23T20:04:59Z"
"","1317","SOLR-13101: Create metadataSuffix znode only at common shard creating api calls","Shared collections make use of something called the metadataSuffix znode per shard in ZooKeeper to understand what core.metadata file to pull from blob store and hydrate a replica with the most up-to-date and correct index data. Shared collections would create the node on-demand with a default value (e.g. indexing) but this is not correct behavior when considering restore operations in the future. We'll want to create the node with a non-default value down the line.   This change initiates the node when we create the shard in common collection API commands and includes CREATE, CREATESHARD, SPLITSHARD. Other operations such as RESTORE are not currently supported by solr shared storage so are not in scope of this change.","closed","","andyvuong","2020-03-05T00:00:06Z","2020-03-16T17:11:53Z"
"","1573","Cleanup TermsHashPerField","Several classes within the IndexWriter indexing chain haven't been touched for several years. Most of these classes expose their internals through public members and are difficult to construct in tests since they depend on many other classes. This change tries to clean up TermsHashPerField and adds a dedicated standalone test for it to make it more accessible for other developers since it's simpler to understand. There are also attempts to make documentation better as a result of this refactoring.","closed","","s1monw","2020-06-12T15:18:21Z","2020-06-16T12:45:51Z"
"","953","LUCENE-9006: WDGF catenateAll should come before parts","See nocommits and parent issue.  The BaseTokenStreamTestCase edit was a trivial thing to ensure the primary assertTokenStreamContents gets called first.  This helps during dev when there's a failure, it's more relevant to see a failure there.","closed","","dsmiley","2019-10-15T16:26:13Z","2019-10-29T19:04:28Z"
"","1639","SOLR-14022: Deprecate CDCR","See JIRA for more details.","closed","","chatman","2020-07-02T00:37:29Z","2021-04-05T21:22:53Z"
"","1486","SOLR-14423: Use SolrClientCache instance managed by CoreContainer","See Jira for more details.","closed","","sigram","2020-05-05T17:17:48Z","2020-08-13T15:36:34Z"
"","1357","SOLR-13807: Introduce a cache for term facet counts","See issue [SOLR-13807](https://issues.apache.org/jira/browse/SOLR-13807). The latter of the two commits pushed here is the one associated with this PR/issue; the former commit is a ""shim"" introducing changes from upstream dependency [SOLR-13132](https://issues.apache.org/jira/browse/SOLR-13132) (PR #751).","open","","magibney","2020-03-16T17:22:26Z","2020-10-15T02:45:25Z"
"","1403","SOLR-9679: Exception when removing zk node /security.json","See https://issues.apache.org/jira/browse/SOLR-9679","closed","","janhoy","2020-04-04T00:10:01Z","2020-06-07T23:12:00Z"
"","1572","SOLR-14561 CoreAdminAPI's parameters instanceDir and dataDir are now validated","See https://issues.apache.org/jira/browse/SOLR-14561  The `instanceDir` and `dataDir` params must now be relative to either `SOLR_HOME`, `SOLR_DATA_HOME` or `coreRootDir`.  Added new solr.xml config 'allowPaths', controlled by system property 'solr.allowPaths' that allows you to add other allowed paths when needed.","closed","","janhoy","2020-06-11T23:35:12Z","2020-06-18T14:13:59Z"
"","1499","SOLR-14463: Solr Admin ZkStatus page now works with ZK 3.6","See https://issues.apache.org/jira/browse/SOLR-14463","closed","","janhoy","2020-05-08T12:13:09Z","2020-05-12T08:57:37Z"
"","1409","SOLR-14391: getDocSet(Query[]) can use search(query,collector)","See https://issues.apache.org/jira/browse/SOLR-14391  Refactoring to simplify SolrIndexSearcher. ScoreFilter is obsolete now.  All tests pass; of note TestCollapseQParserPlugin.  CC @yonik @joel-bernstein","closed","","dsmiley","2020-04-06T16:15:06Z","2020-04-19T04:16:26Z"
"","1392","SOLR-14371 Zk StatusHandler should know about dynamic zk config","See https://issues.apache.org/jira/browse/SOLR-14371","closed","","janhoy","2020-03-31T14:25:48Z","2020-04-17T14:30:33Z"
"","1400","SOLR-14359: Admin UI collection/core drop-downs had wrong placeholder","See https://issues.apache.org/jira/browse/SOLR-14359","closed","","janhoy","2020-04-03T14:14:22Z","2020-04-03T21:16:18Z"
"","1364","SOLR-14335: Lock Solr's memory to prevent swapping","See https://issues.apache.org/jira/browse/SOLR-14335 Goal of this PR is to start Solr with a custom bootstrap class, which delegates to Jetty's main. In the custom bootstrap class, we will then implement memroy locking through JNA","closed","stale-closed,","janhoy","2020-03-19T15:50:42Z","2021-12-08T10:21:03Z"
"","1300","SOLR-14287: Admin UI Properties screen does not show colons","See https://issues.apache.org/jira/browse/SOLR-14287","closed","","janhoy","2020-02-28T08:59:08Z","2020-02-28T09:32:13Z"
"","1250","SOLR-14250 Fix error logging on Expect: 100-continue","See https://issues.apache.org/jira/browse/SOLR-14250  With this PR, we'll still always try to consume the stream,  but if the input stream is not available due to Expect: header, and an error has already been sent by Solr, the resulting IOException  from Jetty will not be logged at INFO level but instead be a simple line on DEBUG level.  The PR does not try to add a test to validate behaviour since this is a logging change only, so there is no risk that the stream is not consumed anymore.","closed","","janhoy","2020-02-11T08:44:20Z","2020-02-21T09:30:16Z"
"","1211","SOLR-14221: Upgrade restlet to version 2.4.0","See https://issues.apache.org/jira/browse/SOLR-14221","closed","","janhoy","2020-01-26T23:50:54Z","2020-02-02T10:35:20Z"
"","1387","SOLR-14210: Include replica health in healtcheck handler","See https://issues.apache.org/jira/browse/SOLR-14210","closed","","janhoy","2020-03-29T01:15:53Z","2020-04-07T10:26:11Z"
"","1192","SOLR-14198: Nullpointer exception in AuditEvent with AuthorizationContext","See https://issues.apache.org/jira/browse/SOLR-14198","closed","","janhoy","2020-01-21T14:18:09Z","2020-01-22T20:39:48Z"
"","1190","SOLR-14196: AdminUI login not working for JWTAuth when blockUnknown false","See https://issues.apache.org/jira/browse/SOLR-14196","closed","","janhoy","2020-01-21T12:44:10Z","2020-01-23T13:51:42Z"
"","1189","SOLR-14196: AdminUI login not working for JWTAuth when blockUnknown=false","See https://issues.apache.org/jira/browse/SOLR-14196","closed","duplicate,","janhoy","2020-01-21T12:42:05Z","2020-01-21T12:43:03Z"
"","1142","SOLR-14166: fq cache=false should use TwoPhaseIterator","See https://issues.apache.org/jira/browse/SOLR-14166","closed","","dsmiley","2020-01-05T19:47:23Z","2021-04-19T17:16:54Z"
"","1119","SOLR-14146: Zookeeper version not resolved in RefGuide","See https://issues.apache.org/jira/browse/SOLR-14146","closed","","janhoy","2019-12-24T12:42:39Z","2019-12-24T22:06:37Z"
"","1104","SOLR-14129: Reuse Jackson ObjectMapper in AuditLoggerPlugin","See https://issues.apache.org/jira/browse/SOLR-14129","closed","","janhoy","2019-12-20T09:01:48Z","2019-12-30T16:40:37Z"
"","1273","SOLR-14114: Add WARN to Solr log that embedded ZK is not supported in prod","See https://issues.apache.org/jira/browse/SOLR-14114","closed","","janhoy","2020-02-21T08:45:01Z","2020-02-24T08:59:39Z"
"","1099","SOLR-14111: Revert SOLR13541 which breaks SSL client auth","See https://issues.apache.org/jira/browse/SOLR-14111 for details","closed","wontfix,","janhoy","2019-12-18T13:22:17Z","2019-12-20T08:55:04Z"
"","1059","SOLR-13954: Embedded ZK in Solr now does not try to load JettyAdminServer","See https://issues.apache.org/jira/browse/SOLR-13954  Fix is to disable zk adminserver, we don't use it yet.  Once we start using it (to replace 4LW) we can add needed jars.","closed","","janhoy","2019-12-06T00:09:37Z","2019-12-06T10:05:44Z"
"","1393","SOLR-11245 Cloud native Dockerfile","See https://issues.apache.org/jira/browse/SOLR-11245  This is a fork of Solr's official Dockerfile, simplified for quickly building local image. Very much work in progress, it should probably be simplified even more, get rid of most of the scripts, be able to build from gradle's target folder instead of requiring a tgz etc.   Just dumping a branch out there that I worked on several months ago, so it has deviated from docker-solr a bit. But it works :)  @makuk66 @dsmiley","closed","","janhoy","2020-03-31T14:35:29Z","2020-08-26T09:48:50Z"
"","1256","SOLR-10306: Document in Reference Guide how to disable or reduce swapping","See https://issues.apache.org/jira/browse/SOLR-10306","closed","","janhoy","2020-02-13T11:35:59Z","2020-02-24T08:50:08Z"
"","1614","LUCENE-9417: Tessellator might fail when several holes share are connected to the same vertex","see https://issues.apache.org/jira/browse/LUCENE-9417  @nknize Can you have a look?","closed","","iverase","2020-06-25T09:24:54Z","2020-06-30T09:07:08Z"
"","1543","LUCENE-9378: Disable compression on binary values whose length is less than 32.","See https://issues.apache.org/jira/browse/LUCENE-9378  This commit disables compression on short binary values, and also switches from ""fast"" compression to ""high"" compression for long values. The reasoning is that ""high"" compression tends to insert fewer, longer back references, which makes decompression slightly faster.","closed","","jpountz","2020-05-29T09:59:10Z","2021-06-16T08:27:46Z"
"","1483","LUCENE-9359: Always call checkFooter in SegmentInfos#readCommit.","See https://issues.apache.org/jira/browse/LUCENE-9359.  This builds on top of LUCENE-7822. To make the review easier I'd advise to add `?w=1` to the URL to ignore whitespace changes.","closed","","jpountz","2020-05-04T10:12:41Z","2020-05-29T12:59:40Z"
"","1473","LUCENE-9353: Move terms metadata to its own file.","See https://issues.apache.org/jira/browse/LUCENE-9353.","closed","","jpountz","2020-05-01T07:44:52Z","2020-06-16T13:05:34Z"
"","1415","LUCENE-9307: Remove the ability to set the buffer size dynamically on BufferedIndexInput","See https://issues.apache.org/jira/browse/LUCENE-9307","closed","","jpountz","2020-04-07T16:22:38Z","2020-04-15T13:10:16Z"
"","1378","LUCENE-9294: BKDWriter refactor: Make BKDIndexWriter an interface","see https://issues.apache.org/jira/browse/LUCENE-9294  This changes introduces a new BKDIndexWriter interface which replace the InOutput input on BKD writer. With this change, we remove the fork `SimpleTextBKDWriter` and replace it with a `SimpleTextBKDIndexWriter`.","closed","","iverase","2020-03-25T08:49:33Z","2020-07-27T16:36:50Z"
"","1377","LUCENE-9293: BKDWriter refactor: Introduce the concept of a leaf block","see https://issues.apache.org/jira/browse/LUCENE-9293  This change introduces a new interface called `BKDLeafBlock`  that replaces the functional interface that is currently used to extract packedValues.","closed","","iverase","2020-03-25T08:41:30Z","2020-07-27T16:36:30Z"
"","1376","LUCENE-9292: BKDWriter refactor: Group point configuration in its own class","see https://issues.apache.org/jira/browse/LUCENE-9292  This changes introduces a new class called `BKDConfig` which contains the point configuration,","closed","","iverase","2020-03-25T08:31:33Z","2020-07-27T16:37:14Z"
"","1520","LUCENE-9288: poll_mirrors.py release script can handle HTTPS mirrors","see https://issues.apache.org/jira/browse/LUCENE-9288","closed","","iverase","2020-05-15T14:14:50Z","2020-05-15T15:03:14Z"
"","1374","LUCENE-9287:Add DocValuesFieldExistsQuery as a never cache query in UsageTrackingQueryCachingPolicy","see https://issues.apache.org/jira/browse/LUCENE-9287","closed","","iverase","2020-03-24T12:59:09Z","2020-03-24T14:29:15Z"
"","1360","LUCENE-9281: Retire SPIClassIterator from master because Java 9+ uses different mechanism to load services when module system is used","See https://issues.apache.org/jira/browse/LUCENE-9281 for more details:  We currently have our own implementation of the service loader standard (SPI) fo several reasons:  (1) In some older JDKs the order of classpath was not respected and this lead to wrong order of codecs implementing the same SPI name. This caused tests to sometimes use wrong class (we had this in Lucene 4 where we had a test-only read/write Lucene3 codec that was listed before the read-only one). That's no longer an issue, the order of loading does not matter. In addition, Java now does everything correct.  (2) In Analysis, we require SPI classes to have a constructor taking args (a Map of params in our case). We also extract the NAME from a static field. Standard service loader does not support this, it tries to instantiate the class with default ctor.  With Java 9+, the ServiceLoader now has a stream() method that allows to filter and preprocess classes: https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html#stream() This allows us to use the new interface and just get the loaded class (which may come from module-info.class or a conventional SPI file): https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.Provider.html#type()  This change allows us to convert Lucene to modules listing all SPIs in the module-info.java.","closed","","uschindler","2020-03-18T12:29:38Z","2020-03-25T17:03:43Z"
"","1341","LUCENE-9273: Speed up geometry queries by specialising Component2D spatial operations","see https://issues.apache.org/jira/browse/LUCENE-9273","closed","","iverase","2020-03-11T10:10:17Z","2020-04-20T17:31:10Z"
"","1339","LUCENE-9272: Move checksum verification of the `.tip` file to `checkIntegrity()`.","See https://issues.apache.org/jira/browse/LUCENE-9272.","closed","","jpountz","2020-03-11T09:13:11Z","2020-03-11T17:15:37Z"
"","1338","LUCENE-9271: Move BufferedIndexInput to the ByteBuffer API","See https://issues.apache.org/jira/browse/LUCENE-9271.","closed","","jpountz","2020-03-11T08:52:22Z","2020-04-07T11:30:20Z"
"","1311","LUCENE-9260: Verify checksums of CFS files.","See https://issues.apache.org/jira/browse/LUCENE-9260.","closed","","jpountz","2020-03-03T15:19:36Z","2020-04-15T13:11:04Z"
"","1290","LUCENE-9251: Filter equal edges with different value on isEdgeFromPolygon","See https://issues.apache.org/jira/browse/LUCENE-9251  Unfortunately, I haven't been able to produce a meaningful unit test as I can only reproduce the issue with the complex polygon provided by the user. Still I think the change make sense.","closed","","iverase","2020-02-26T11:08:32Z","2020-03-03T06:09:06Z"
"","1280","LUCENE-9239: Change withinTriangle logic for Circles","see https://issues.apache.org/jira/browse/LUCENE-9239 for an explanation.  The idea of the new logic is to exploit the fact that for Circles we know an inside point of the shape. Therefore for the shape to be a candidate it is enough to know that this point is inside one of the triangles.  We only check line intersections for edges that belong to the original polygon.","closed","","iverase","2020-02-24T09:17:31Z","2020-03-03T05:52:15Z"
"","1212","LUCENE-9176: Handle the case when there is only one leaf node in TestEstimatePointCount","see https://issues.apache.org/jira/browse/LUCENE-9176","closed","","iverase","2020-01-27T07:32:32Z","2020-01-27T08:53:44Z"
"","1197","LUCENE-9161: DirectMonotonicWriter checks for overflows.","See https://issues.apache.org/jira/browse/LUCENE-9161","closed","","jpountz","2020-01-22T07:55:48Z","2020-01-28T18:06:54Z"
"","1193","LUCENE-9154:  Remove encodeCeil() to encode bounding box queries","See https://issues.apache.org/jira/browse/LUCENE-9154  IMHO this makes these queries an apple to apple comparison.","closed","","iverase","2020-01-21T17:49:21Z","2021-12-14T15:58:19Z"
"","1475","LUCENE-9148: Move the BKD index to its own file.","See https://issues.apache.org/jira/browse/LUCENE-9148.  This is only a work-in-progress for now, I'll finish when #1440 is merged to not duplicate work and introduce conflicts.","closed","","jpountz","2020-05-01T13:54:35Z","2020-06-09T07:59:19Z"
"","1178","LUCENE-9144: Fix error message on OneDimensionBKDWriter","See https://issues.apache.org/jira/browse/LUCENE-9144","closed","","iverase","2020-01-16T13:57:35Z","2020-01-16T15:27:22Z"
"","1150","LUCENE-9118: BlockTreeTermsReader uses `Arrays#compareUnsigned` to compare suffixes.","See https://issues.apache.org/jira/browse/LUCENE-9118.  The diff is easier to read when ignoring whitespaces: https://github.com/apache/lucene-solr/pull/1150/files?w=1.","closed","","jpountz","2020-01-07T09:03:21Z","2020-01-09T14:09:27Z"
"","1145","LUCENE-9115: NRTCachingDirectory shouldn't cache files of unknown size.","See https://issues.apache.org/jira/browse/LUCENE-9115.","closed","","jpountz","2020-01-06T13:52:33Z","2020-01-09T14:15:35Z"
"","1324","LUCENE-9033 Update ReleaseWizard for new website instructions","See https://issues.apache.org/jira/browse/LUCENE-9033","closed","","janhoy","2020-03-06T09:53:45Z","2020-05-13T21:56:30Z"
"","1482","LUCENE-7822: CodecUtil#checkFooter should throw a CorruptIndexException as the main exception.","See https://issues.apache.org/jira/browse/LUCENE-7822.","closed","","jpountz","2020-05-04T07:42:37Z","2020-05-07T11:04:27Z"
"","939","LUCENE-9003: Compute numDocs() lazily.","See [LUCENE-9003](https://issues.apache.org/jira/browse/LUCENE-9003) for background.","closed","","jpountz","2019-10-10T13:05:14Z","2019-10-14T16:47:59Z"
"","1425","LUCENE-7788","Sample of what cleaning up the logging calls looks like to see if we want to pursue this.","closed","","ErickErickson","2020-04-12T00:43:54Z","2020-04-13T18:15:28Z"
"","699","LUCENE-8827: Speed up poll-mirrors.py and add -once option","Runs polling in 5 parallell processes Adds -once option to exit after one poll","closed","","janhoy","2019-06-05T19:13:30Z","2019-06-05T19:14:36Z"
"","1627","SOLR-14585: Check the current user in SysV init script","Run ""su"" only if the current user is not ""solr"" - this allows executing of /etc/init.d/solr using user ""solr"".","open","","rkosenko","2020-06-28T21:05:37Z","2020-06-28T21:05:37Z"
"","1044","Revert ""LUCENE-8213: Asynchronous Caching in LRUQueryCache""","Reverts apache/lucene-solr#916","closed","","atris","2019-11-28T14:48:34Z","2020-10-20T01:51:37Z"
"","914","Revert ""LUCENE-8213: Introduce Asynchronous Caching in LRUQueryCache""","Reverts apache/lucene-solr#815","closed","","atris","2019-10-02T08:26:10Z","2020-10-20T03:41:26Z"
"","1063","Revert ""SOLR-13990: Switch out woodstox-core-asl with aalto-xml and upgrade woodstox stax-2 API""","Reverts apache/lucene-solr#1050","closed","","madrob","2019-12-06T21:26:45Z","2020-08-13T18:57:53Z"
"","1259","LUCENE-9226: Return CELL_CROSSES_QUERY when point inside the triangle","return the right relation when point inside a triangle.","closed","","iverase","2020-02-14T15:01:19Z","2020-02-14T16:06:46Z"
"","1634","SOLR-14592: Upgrade Zookeeper to 3.6.1","Resolving some of the Zookeeper dependencies was interesting. Zookeeper depends on org.xerial.snappy, commons-lang 2.6 and netty 4.1.48  I had to add org.xerial.snappy as a runtime dependency to get past NoClassDefFound errors that only appear when _running_ either embedded Zookeeper or instantiating a Zookeeper server when running tests. Which we do a lot.  For Ant I needed to do similar, and in addition commons-lang 2.6. Why Gradle apparently uses that automagically is a mystery, but when trying Ant, I'd get a NoClassDefFound error for StringUtils unless I included commons-lang 2.6 explicitly.  Netty was at 4.1.47 originally. I ran into thrashing for checksums until I upgraded netty to at least 4.1.48. Gradle would go ahead and pull this down automatically and update all the relevant checksums, but Ant didn't know about it and would delete all the 4.1.48 checksums and create 4.1.47 ones, which Gradle would delete and add back the ones for 4.1.48 next time updateLicenses was run. So I upgraded it to 4.1.50 on the theory that we might as well get the most recent and now ant and gradle play nice together  Finally, I claim that getMyServerId is unnecessary. It's in there from a long time ago and apparently tries to guess the Zookeeper id in the absence of a myid file, and nobody chimed in when I suggested removing it.","closed","","ErickErickson","2020-06-30T18:02:09Z","2020-07-01T16:33:45Z"
"","1444","LUCENE-9338: Clean up type safety in SimpleBindings","Replaces SimpleBindings' `Map` with a map of  `Supplier` to improve type safety.  Also moves cycle detection directly into ExpressionValuesSource.","closed","","romseygeek","2020-04-22T11:47:00Z","2020-04-24T09:23:51Z"
"","1654","SOLR-14422 progressive render load main admin page","Replacement branch for https://github.com/apache/lucene-solr/pull/1633.","closed","","epugh","2020-07-06T18:28:50Z","2020-08-13T14:51:55Z"
"","1384","Remove CurrentCoreDescriptorProvider","Replace `CurrentCoreDescriptorProvider` with a functional interface so that it is easier to construct since all implementations in our code base were anonymous classes anyway. Added Javadocs explaining the usage instead of relying on class name to convey information.","closed","","madrob","2020-03-27T14:48:09Z","2020-03-27T21:31:42Z"
"","1511","SOLR-13289: minExactHits -> minExactCount","Rename the parameter used to define the number of hits to count","closed","","tflobbe","2020-05-12T03:54:55Z","2020-05-21T23:42:41Z"
"","1424","SOLR-13101: Enable shared store via system property only","Removing a previous [need](https://github.com/apache/lucene-solr/pull/1223) to configure shared storage feature via solr.xml and only requiring it to be enabled via passed system properties.  Pass in -DsharedStoreEnabled=true to the solr binary or set env variable SHARED_STORE_ENABLED. Note sys props passed to the binary will take precedence.","closed","","andyvuong","2020-04-11T00:09:08Z","2020-05-18T19:03:54Z"
"","1079","SOLR-14067: StatelessScriptUpdateProcessor removal due to security concerns","Removed SSUP, added upgrade notes.","open","","chatman","2019-12-12T10:20:28Z","2019-12-12T10:24:50Z"
"","703","LUCENE-8838: Remove support for Steiner points","Remove unused Steiner points support and fail if a hole is reduced to a point when all points are coplanar.","closed","","iverase","2019-06-07T05:21:43Z","2019-06-24T07:45:51Z"
"","1528","SOLR-12823: remove /clusterstate.json","Remove all code dealing with Zookeeper's /clusterstate.json, remove Collection API's MIGRATESTATEVERSION, remove legacyCloud option.  Notes [Edit: these two tests were deleted. Need to create a Jira post merge to create a SolrCloud chroot test]: - TestZkChroot requires more work - BasicZkTest requires more work (or better: be deleted)  Also fixes SOLR-11877: DocCollection.getStateFormat is buggy  # Description  Remove all code dealing with /clusterstate.json in Zookeeper, remove MIGRATESTATEVERSION Collection API action and the notion of “legacyCloud”.  # Solution  Solr 9 will refuse to start if an existing /clusterstate.json in Zookeeper exists and is not empty.  # Tests  No new tests, but adapting old tests that were misbehaving.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","cleanup,","murblanc","2020-05-18T22:47:33Z","2020-09-16T22:38:12Z"
"","844","LUCENE-8860: Make more decision on inner nodes in ShapeBoundingBoxQuery","Relaxes the bounding box intersection check in bounding box queries on the inner node level. For the intersection it is not required that the query bounding box contains the entire cell bounding box. It is sufficient for the bounding box to include at least one edge of min bounding box and max bounding. The edge has to be on the same side of both boxes.","closed","","imotov","2019-08-23T20:15:57Z","2019-10-04T05:13:35Z"
"","1541","RegExp - add case insensitive matching option","Relates to Jira issue [9386](https://issues.apache.org/jira/projects/LUCENE/issues/LUCENE-9386)  Added a new CASE_INSENSITIVE option to the existing flags. The RegExp class is a little strange because instances represent either the parser or the parsed objects it nests in a tree. The `flags` field is only relevant to the root parser and was left blank in all parsed nodes. This PR's changes require that the flags int is propagated to all nodes so that they can see if it includes the case insensitive option (all other bits in the flag represent parsing options so there was no need to propagate before).","closed","enhancement,","markharwood","2020-05-28T13:40:59Z","2020-07-08T15:08:13Z"
"","842","LUCENE-8955: Move compare logic to IntersectVisitor in NearestNeighbor","Relates to #783","closed","","iverase","2019-08-22T07:39:03Z","2019-08-26T07:35:58Z"
"","751","SOLR-13132: single sweep iteration over base, foreground, and background sets for ""relatedness"" calculation","Relatedness essentially calculates facets separately across three different docSets; this patch increments facet counts over foreground, background, and base sets in a single sweep, to leverage the efficiency of existing faceting over the base set","closed","","magibney","2019-06-28T15:59:31Z","2020-10-21T20:41:26Z"
"","717","SOLR-13542: Code cleanup - Avoid using stream filter count where possible","Related to Jira ticket: https://issues.apache.org/jira/browse/SOLR-13542  Code changes made and CHANGES.txt editted.  The majority of the code seems to already be using anyMatch and noneMatch logic, these few remaining instances were using count() and comparing the result to 0. This changes everything to anyMatch and noneMatch, which is also potentially more performant.  Tests have run locally, all succeeded.","closed","","KoenDG","2019-06-13T00:16:49Z","2019-08-27T18:37:40Z"
"","824","LUCENE-8755: QuadPrefixTree robustness","Reimplement QuadPrefixTree and PackedQuadPrefixTree's getCell method.","closed","","chenkovsky","2019-08-07T08:52:21Z","2019-08-20T21:09:21Z"
"","1501","SOLR-13289: Add Refguide changes","Reference guide changes for SOLR-13289","closed","","tflobbe","2020-05-08T22:04:53Z","2020-05-21T23:55:53Z"
"","1233","LUCENE-9202: refactor leaf collectors in TopFieldCollector","Refactoring the two LeafCollector subclasses here into a shared `TopFieldLeafCollector` class to make the code easier on the eyes and amenable to further complications","closed","","msokolov","2020-02-03T16:20:05Z","2020-02-27T13:18:49Z"
"","1004","SOLR-13822 Isolated Classloading from packages","ref guide and some `curl` friendly changes","closed","","noblepaul","2019-11-11T12:23:20Z","2019-11-12T06:52:58Z"
"","980","LUCENE-8920: Reduce the memory used by direct addressing of arcs","Reduce the memory by using ""arc presence"" bits, one bit per arc. So the node only stores the present arcs (no ""missing arc"" anymore). The offset to a given arc is computed by getting the number of ""presence bits"" set up to the corresponding bit for the arc (and multiplied by the number of bytes per arc).","closed","","bruno-roustant","2019-10-27T18:10:50Z","2019-12-02T10:18:48Z"
"","1258","LUCENE-9225: Rectangle should extend LatLonGeometry","Rectangle now extends LatLonGeometry so it can be used as part of a geometry collection. We need to be careful for Contains and we need to split the rectangle in two if it crossest the dateline.   Test is added to check we get the same results from tLatLotBoundingBoxQuery and the corresponding geometry query.","closed","","iverase","2020-02-14T12:02:56Z","2020-03-03T06:45:20Z"
"","1239","LUCENE-9207: Don't build span queries in QueryBuilder","QueryBuilder currently has special logic for graph phrase queries with no slop,  constructing a spanquery that attempts to follow all paths using a combination of  OR and NEAR queries.  Given the known bugs in this type of query (LUCENE-7398)  and that we would like to move span queries out of core in any case, we should  remove this logic and just build a disjunction of phrase queries, one phrase per path.","closed","","romseygeek","2020-02-05T10:26:00Z","2020-02-26T14:32:35Z"
"","988","Update README.md","Pull Request Guidelines should be present in Readme file   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","hemantkadyan","2019-10-31T14:46:44Z","2020-02-15T23:21:49Z"
"","1154","SOLR-13985: Bind to localhost interface by default","Prior to this commit, Solr's Jetty listened for connections on all network interfaces. This commit changes it to only listen on localhost, to prevent incautious administrators from accidentally exposing their Solr deployment to the world.  Administrators who wish to override this behavior can set the SOLR_JETTY_HOST property in their Solr include file (solr.in.sh/solr.in.cmd) to ""0.0.0.0"" or some other value.  A version of this commit was previously reverted due to inconsistency between SOLR_HOST and SOLR_JETTY_HOST.  This commit fixes this issue.   # Description Reinstates the bind-to-localhost functionality, which was previously reverted.  Contains a fix to keep the SOLR_HOST and SOLR_JETTY_HOST properties better aligned by default.  # Tests  The way that our automated tests run, they don't exercise any of this functionality.  So the testing for this was mostly manual.  A lot of my testing was of the form: Start Solr(s) a particular way with particular settings, create a simple collection and a complex collection, index to them both, query them both, try a few other collection-admin actions, and shut everything down.  I did this with a single node deploy, a multi-node deploy, the ""cloud"" example, a multi-node deploy with SOLR_HOST set (this took some /etc/hosts fiddling), and a multi-node deploy with some localhost-bound nodes and some public ones.     # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-07T21:57:52Z","2020-01-13T14:42:36Z"
"","1492","SOLR-11934: Visit Solr logging, it's too noisy.","Preliminary changes for discussion.","closed","cleanup,","ErickErickson","2020-05-07T11:58:33Z","2020-06-04T10:39:39Z"
"","718","SOLR-13541: Upgrade Jetty to 9.4.19.v20190610","Precommit and test succeed on master, but NOT  on 8.x. I'll upload the test results momentarily.","closed","","ErickErickson","2019-06-13T04:07:48Z","2020-01-31T22:10:56Z"
"","1121","SOLR-11207: Add OWASP dependency checker to gradle build","PR against gradle-master branch to add owasp dependency checker plugin. See https://issues.apache.org/jira/browse/SOLR-11207  To invoke, run      ./gradlew dependencyCheckAnalyze     open ./build/reports/dependency-check-report.html  There is also a `dependencyCheckAggregate` target that goes deeper and finds more.","closed","","janhoy","2019-12-25T00:39:06Z","2020-01-26T09:01:52Z"
"","997","SOLR-13822: Isolated Classloading from packages","Porting from master","closed","","noblepaul","2019-11-06T06:30:38Z","2019-11-08T01:15:24Z"
"","1111","SOLR-14136: ip whitelist/blacklist via env vars","Plumbs two simple vars to jetty config so that it is very easy to restrict access by IP.","closed","","rmuir","2019-12-22T15:33:32Z","2020-10-20T01:51:26Z"
"","1100","SOLR-13579: Create resource management API","Please see JIRA for details.","open","","sigram","2019-12-18T16:40:24Z","2020-08-11T17:16:03Z"
"","1608","LUCENE-9379: Encrypting Directory - functional but to optimize","Performance issue because of too many new Ciphers when slicing IndexInput. javax.crypto.Cipher is heavy weight to create and is stateful. I tried a CipherPool, but actually there are many cases where we need to get lots of slices of the IndexInput so we have to create lots of new stateful Cipher. The pool turns out to be a no-go, there are too many Cipher in it.  [Update] Now there is a lighter alternative to Cipher: LightAesCtrEncrypter. It has light constructor, initialization and cloning. And all tests pass.","open","","bruno-roustant","2020-06-24T14:33:21Z","2021-05-27T14:23:05Z"
"","969","SOLR-13855: DistributedZkUpdateProcessor needs to propagate finish()","Passes tests.  There's a nocommit comment on wether a distributed scenario should not propagate the finish() call.  It was not in the past (checked via null ""nodes"" field), but I think it should always propagate because even if ""nodes"" is null processAdd might pass on through a particular document if it's local to the shard receiving the request.  @yonik WDYT?","closed","","dsmiley","2019-10-23T16:15:36Z","2019-10-23T21:50:04Z"
"","1291","LUCENE-9016: RefGuide meta doc for how to publish website","Part of https://issues.apache.org/jira/browse/LUCENE-9016","closed","","janhoy","2020-02-26T15:36:14Z","2020-03-05T17:17:40Z"
"","1569","LUCENE-9356: Add a test that verifies that Lucene catches bit flips.","Opening a reader and then calling checkIntegrity must throw a `CorruptIndexException` or an `IndexFormatToo(Old|New)Exception`.","closed","","jpountz","2020-06-11T11:54:38Z","2020-06-11T16:09:14Z"
"","1070","LUCENE-9089: FST Builder fluent-style constructor.","Only FST Builder is refactored. The other classes are touched to adapt to the new fluent style.","closed","","bruno-roustant","2019-12-10T21:30:57Z","2019-12-13T17:37:06Z"
"","1629","SOLR-14561 Followup - validate params for more core operations","Now validates path params for backup, restore, snapshot, mergeindexes Add template to solr.in scripts Also tested on Windows paths Added RefGuide documentation to some params","closed","","janhoy","2020-06-29T11:04:48Z","2020-06-29T11:18:30Z"
"","1637","SOLR-12847 Cut over implementation of maxShardsPerNode to a collection policy","Note: this is for master (9.0) only.","closed","","sigram","2020-07-01T17:09:29Z","2020-08-13T15:34:16Z"
"","791","LUCENE-8922: Better impacts for DisjunctionMaxQuery.","Note that we already have tests that cover impacts for DisjunctionMaxQuery.","closed","","jpountz","2019-07-17T11:32:31Z","2019-07-24T13:13:03Z"
"","854","Shared PQ Based Early Termination for Concurrent Search","NOCOMMIT.  This is a WIP PR which implements a shared PQ based early termination. Each collector collects independently into a thread local PQ and updates a global count of hits (which will be refactored post merging of LUCENE-8939).  Once enough hits are accumulated, a global PQ is populated by all collectors and then the global PQ serves as the benchmark to filter further hits.  Several optimizations have been performed, such as non blocking building of global PQ, no reduce operation performed during CollectorManager.reduce but rather, returning results directly from the global PQ.  I need some eyes on the overall logic, and especially at two points: 1) Across segment values comparison and 2) Local to global DocID mapping during interactions of thread local PQs and global PQ. I am pretty sure there is a bug in either of the two, so would really appreciate a deep look.","open","","atris","2019-09-04T09:33:22Z","2019-09-12T12:04:49Z"
"","1405","Solr GraphTermsQParser simplifications","No WeightOrDocIdSet; no override of bulkScorer  @joel-bernstein can you please look.  I happened to be viewing this code and didn't understand why the complexity was there (and no comments justifying it either).","closed","","dsmiley","2020-04-04T14:53:15Z","2020-04-28T21:33:44Z"
"","1270","LUCENE-9237: Faster UniformSplit IntersectBlockReader.","New version of TermsEnum intersect for UniformSplit. It is 75% more efficient than the previous version for FuzzyQuery.  Compared to BlockTree IntersectTermsEnum: - It is still slower for FuzzyQuery (-37%) but it is faster than the previous version (which was -65%). - It is slightly slower for WildcardQuery (-5%). - It is slightly faster for PrefixQuery (+5%). Sometimes benchmarks show more improvement.","closed","","bruno-roustant","2020-02-20T22:19:40Z","2020-08-31T06:43:05Z"
"","780","SOLR-11866: Support efficient subset matching in query elevation rules","New SubsetMatchElevationProvider and TrieSubsetMatcher that can elevate documents based on query subset matching very efficiently scalable to a large number of elevation rules.","closed","","bruno-roustant","2019-07-12T13:37:10Z","2019-12-02T10:22:05Z"
"","1003","SOLR-13813: add test for shared storage live split","New live split test for shared storage. Unfortunately, it currently fails.","closed","","yonik","2019-11-10T21:40:59Z","2019-11-22T16:36:01Z"
"","865","LUCENE-8973: XYRectangle2D should work on float space","New implementation of XYRectangle 2D that works on the float space. Because the encoding is not lineal, working on the encoding space changes the spatial relationship between the bounding box and the other objects.","closed","","iverase","2019-09-10T10:05:18Z","2019-12-11T08:38:33Z"
"","1497","SOLR-8394: /admin/luke didn't computeindexHeapUsageBytes","Needed to call FilterLeafReader.unwrap.  https://issues.apache.org/jira/browse/SOLR-8394  CC @sigram","closed","","dsmiley","2020-05-07T19:11:51Z","2020-05-18T18:54:27Z"
"","1380","SOLR-14364: LTR SolrFeature fq improvements","Mostly general code improvements, though it should support postFilters now Add QueryUtils.combineQueryAndFilter","closed","","dsmiley","2020-03-26T04:33:31Z","2020-03-26T13:19:47Z"
"","788","LUCENE-8920: disable FST direct-addressing pending size reduction","Minimal change to disable FST direct addressing","closed","","msokolov","2019-07-15T18:46:16Z","2019-07-20T17:23:20Z"
"","1566","SOLR-14554: Let Solr use WAND algorithm when scores are requested","MaxScoreCollector now sets the minCompetitiveScore to the number of the current highest score, which allow MultiCollector to skip docs if the main query collector can.","closed","","tflobbe","2020-06-10T21:08:29Z","2020-06-22T23:50:59Z"
"","1562","LUCENE-9400: Tessellator might fail when several holes share the same vertex","Make sure we do not build illegal polygons when merging holes that share a single vertex.  see https://issues.apache.org/jira/browse/LUCENE-9400","closed","","iverase","2020-06-10T08:14:44Z","2020-06-16T07:02:49Z"
"","1529","Lucene-9371: Allow external access to RegExp's parsed structure (#1521)","Made RegExp internal fields public final to allow external classes to render eg English explanations of pattern logic Backport of https://github.com/apache/lucene-solr/pull/1521","closed","enhancement,","markharwood","2020-05-19T17:06:11Z","2020-05-19T17:39:55Z"
"","1583","LUCENE-9407: change the visibility of the LatLonXQuery classes to public","LUCENE-9407: change the visibility of the LatLonXQuery classes to public in order to allow using them outside of the org.apache.lucene.document package   # Description  A few years ago the geospatial queries  classes have been refactored to be package-private. This restriction doesn't allow these classes to be used outside of the org.apache.lucene.document package.  # Solution  Changed the visibility of the LatLonXQuery classes to public so that they can be used outside of the org.apache.lucene.document package (e.g. elasticsearch percolator)  # Tests  No Tests required because no functionality has been changed.   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for How to Contribute and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers access to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the master branch. - [x] I have run ant precommit and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the Ref Guide (for Solr changes only).","closed","","mariusneo","2020-06-16T09:22:12Z","2020-07-11T20:59:45Z"
"","1467","LUCENE-9350: Don't hold references to large automata on FuzzyQuery","LUCENE-9068 moved fuzzy automata construction into FuzzyQuery itself.  However, this has the nasty side-effect of blowing up query caches that expect queries to be  fairly small.  This commit restores the previous behaviour of caching the large automata on an AttributeSource shared between segments, while making the construction a bit clearer by factoring it out into a package-private `FuzzyAutomatonBuilder`.","closed","","romseygeek","2020-04-29T13:24:56Z","2020-05-07T10:36:51Z"
"","998","LUCENE-9037: ArrayIndexOutOfBoundsException due to repeated IOExcepti…","LUCENE-9037: ArrayIndexOutOfBoundsException due to repeated IOException during indexing  # Description  This patch forces a reset of the indexing in memory data structures when an IOException is thrown during tokenization.  # Solution  # Tests  Attached to the Jira (see first comment) a test that fails without the patch, but that fails in a different way with the patch... I wasn't able to imagine a realistic test that would pass with the patch (the test does pass in Solr 7.1 with a patch doing something equivalent, but behavior in Solr 8 changed).  # Checklist  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2019-11-06T23:05:14Z","2020-09-16T22:37:56Z"
"","1552","LUCENE-8962: merge small segments on commit","LUCENE-8962: This PR revisits the merge-on-commit patch submitted by @msfroh a little while ago. The only change from that earlier PR is a fix for failures uncovered by TestIndexWriter.testRandomOperations, some whitespace cleanups, and a rebase on the current master branch. The problem was that updateSegmentInfosOnMergeFinish would incorrectly decRef a merged segments' files if that segment was modified by deletions (or updates) while it was being merged.  With this fix, I ran the failing test case several thousands of times with no failures, whereas before it would routinely fail after a few hundred test runs.","closed","","msokolov","2020-06-03T19:50:19Z","2020-06-22T12:31:30Z"
"","1602","SOLR-14582: Expose IWC.setMaxCommitMergeWaitMillis in Solr's index config","LUCENE-8962 added the ability to merge segments synchronously on commit. This isn't done by default and the default MergePolicy won't do it, but custom merge policies can take advantage of this. Solr allows plugging in custom merge policies, so if someone wants to make use of this feature they could, however, they need to set IndexWriterConfig.maxCommitMergeWaitSeconds to something greater than 0. Since this is an expert feature, I plan to document it only in javadoc and not the ref guide.","closed","","tflobbe","2020-06-22T22:33:42Z","2020-08-07T22:10:31Z"
"","1276","LUCENE-8954: refactor Nori analyzer","LUCENE-8954 is an issue created in August last year. (https://issues.apache.org/jira/browse/LUCENE-8954) The patch is already pushed in master branch. (https://github.com/apache/lucene-solr/pull/839) But I forgot to put it in branch_8x. So this PR is for it.","closed","","danmuzi","2020-02-21T20:08:19Z","2020-02-24T15:08:48Z"
"","1287","LUCENE-8954: refactor Nori analyzer","LUCENE-8954 is an issue created in August last year. (https://issues.apache.org/jira/browse/LUCENE-8954) The patch is already pushed in master branch. (#839) But I forgot to put it in branch_8x. So this PR is for it.","closed","","danmuzi","2020-02-24T18:54:33Z","2020-02-25T13:46:27Z"
"","917","SOLR-13101: fix test compilation","Looks like the merge when the original PR was put up broke test compilation.  Here's the simple fix.","closed","","yonik","2019-10-02T16:42:25Z","2019-10-02T17:13:23Z"
"","872","LUCENE-8620: LatLonShape contains","LatLonShape's implementation for spatial relationship CONTAINS.  superseed #608","closed","","iverase","2019-09-11T07:03:11Z","2019-12-12T16:23:28Z"
"","1296","LUCENE-9253: Support custom dictionaries in KoreanTokenizer","KoreanTokenizer does not support custom dictionaries(system, unknown) now, even though Nori provides DictionaryBuilder that creates custom dictionary. In the current state, it is very difficult for Nori users to use a custom dictionary. Therefore, we need to open a new constructor that uses it.  JIRA : https://issues.apache.org/jira/browse/LUCENE-9253","closed","","danmuzi","2020-02-27T17:59:02Z","2020-03-02T17:11:45Z"
"","1315","Add try-with-resources to CloudManager in tests","Just closes some CloudManager instances in tests","closed","","tflobbe","2020-03-04T20:08:32Z","2020-03-04T21:32:48Z"
"","1544","ref_guide - metrics reporting - small typo","just a small typo","closed","","soleuu","2020-05-29T11:13:41Z","2020-06-02T08:23:43Z"
"","829","SOLR-13452: Update the lucene-solr build from Ivy+Ant+Maven (shadow build) to Gradle.","Just a sample PR to help look at changes.","closed","","markrmiller","2019-08-14T03:14:07Z","2020-11-09T15:41:51Z"
"","1240","SOLR-14242: HdfsDirectory#createTempOutput.","JIRA: https://issues.apache.org/jira/browse/SOLR-14242","closed","","jpountz","2020-02-05T10:27:36Z","2020-02-05T15:39:43Z"
"","1489","Lucene-9336: RegEx querying - add support for Java’s predefined character classes like \d for digits","Jira Issue [9336](https://issues.apache.org/jira/browse/LUCENE-9336) proposes adding support for common regex character classes like `\w`. This PR adds the code to RegExp.java and associated tests.  One other consideration is that the shorthand expressions list could perhaps be made configurable e.g. `\h` might be shorthand used to represent hashtags of the form `#\w*` if that was something users routinely searched for and wanted to add to the regex vocabulary.","closed","enhancement,","markharwood","2020-05-06T10:38:38Z","2020-05-14T09:14:02Z"
"","715","LUCENE-7714: Add a range query that takes advantage of index sorting.","I’m opening this draft PR to get feedback on an approach to [LUCENE-7714](https://issues.apache.org/jira/browse/LUCENE-7714).  The PR adds the new query type `IndexSortDocValuesRangeQuery`, a range query that takes advantage of the fact that the index is sorted on the same field as the query. It performs binary search on the field's doc values to find the doc IDs at the lower and upper ends of the range.  The query can only be used if all of these conditions hold: - The index is sorted, and its primary sort is on the same field as the query. - The query field has `SortedNumericDocValues`. - The segments must have at most one field value per document (otherwise we cannot easily   determine the matching document IDs through a binary search).  I was hoping for feedback on the overall approach, and also had a few open questions: - I wasn’t sure on the best way to structure the query. As it stands, it requires that segments are sorted correctly and contain at most one value per document. Perhaps we could introduce a wrapper query that can make the decision on a segment-by-segment basis: it would only use `IndexSortDocValuesRangeQuery` if the right conditions are met, and otherwise fall back to a standard range query. This wrapper query would have similarities to `IndexOrDocValuesQuery`. - Because doc values only support forward iteration, we need to recreate the comparators every time we backtrack in the binary search. I assumed this recreation would be expensive and experimented with some strategies to avoid it, such as starting with a shared binary search that checks both `lowerValue` and `upperValue`, then moving on to the two individual binary searches. However, these efforts didn’t show any performance improvements in my benchmarks. I plan to do more research around sparse + blocked doc values to understand the circumstances in which reloading can be more expensive.  **Benchmarking results** I ingested part of the [http logs](https://github.com/elastic/rally-tracks/tree/master/http_logs) dataset (123M total documents) into an index sorted by `@timestamp`. Every document has a single `@timestamp`, although they are not unique across documents.  The following date ranges were tested: - range with single point [897303051, 897303051], 124 docs - small range (897633930, 897655999], ~2M docs - medium range (897623930, 897655999],  ~5M docs - large range (897259801, 897503930], ~21M docs  ``` | 50th percentile service time |            range-single |   7.50658 |     ms | | 90th percentile service time |            range-single |   7.81589 |     ms | | 50th percentile service time |  range-optimized-single |   8.02532 |     ms | | 90th percentile service time |  range-optimized-single |   8.38834 |     ms |  | 50th percentile service time |               range-small |   12.7203 |     ms | | 90th percentile service time |               range-small |   13.3798 |     ms | | 50th percentile service time |     range-optimized-small |   7.43383 |     ms | | 90th percentile service time |     range-optimized-small |   7.67655 |     ms |  | 50th percentile service time |               range-medium |   20.8554 |     ms | | 90th percentile service time |               range-medium |   22.1637 |     ms | | 50th percentile service time |     range-optimized-medium |   8.38864 |     ms | | 90th percentile service time |     range-optimized-medium |   8.66427 |     ms |  | 50th percentile service time |           range-large |   53.8697 |     ms | | 90th percentile service time |           range-large |   60.2573 |     ms | | 50th percentile service time | range-optimized-large |   7.81584 |     ms | | 90th percentile service time | range-optimized-large |   8.02755 |     ms | ```","closed","","jtibshirani","2019-06-12T00:55:41Z","2020-05-13T03:45:21Z"
"","1445","LUCENE-9339: Only call MergeScheduler when we actually found new merges","IW#maybeMerge calls the MergeScheduler even if it didn't find any merges we should instead only do this if there is in-fact anything there to merge and safe the call into a sync'd method.","closed","","s1monw","2020-04-22T12:51:06Z","2020-04-22T19:26:51Z"
"","1601","LUCENE-8962: Ensure we don't include fully deleted segments in a commit","IW might drop segments that are merged into a fully deleted segment on the floor and deletes the newly created files right away. We should not include these segments in a commit since we can't guarantee valid ref-counts on these files.","closed","","s1monw","2020-06-22T17:46:59Z","2020-06-24T16:51:23Z"
"","697","LUCENE-8833: Allow MMapDirectory subclasses to pre-load per IndexInput","It's useful to select if we preload a mmap on a per index input / file basis. Subclasses should be able to extend MMapDirectory to make it configurable.","closed","","s1monw","2019-06-05T14:24:02Z","2019-07-02T14:11:43Z"
"","1588","SOLR-14577: Return BAD REQUEST when field is missing in terms QP","It will currently throw a NPE, resulting in a server error.","closed","","tflobbe","2020-06-17T17:24:01Z","2020-06-17T23:02:52Z"
"","1500","Backport LUCENE-9350 to branch_8_5","It may be worthwhile to review this by individual commits instead of all at once. I do not intend to squash these when committing.  I'm most concerned about whether the changes in commit 3363df7 are sufficient - they are enough for API and compilation, but it's not clear if I needed to update some implementations of QueryVisitor to tie all of this together.","closed","","madrob","2020-05-08T16:13:08Z","2020-05-12T17:21:28Z"
"","808","LUCENE-8934: promote nori tools to main jar","It is the Nori version of LUCENE-8871. - Move Nori tools to main source tree  Please check the following link: https://issues.apache.org/jira/browse/LUCENE-8934","closed","","danmuzi","2019-07-24T17:41:23Z","2019-07-30T16:14:05Z"
"","776","LUCENE-8912: remove nori/tools dependency on ICU","It is the Nori version of LUCENE-8866. Nori doesn't need the ICU library because it uses Normalizer2 only for NFKC normalization like Kuromoji.  Please check the following link: https://issues.apache.org/jira/browse/LUCENE-8912","closed","","danmuzi","2019-07-11T17:21:56Z","2019-07-23T13:02:29Z"
"","767","LUCENE-8904: enhance Nori DictionaryBuilder tool","It is the Nori version of LUCENE-8863. This patch has two changes. 1) Improve exception handling 2) Enable external dictionary for testing  Please check the following link: https://issues.apache.org/jira/browse/LUCENE-8904","closed","","danmuzi","2019-07-07T15:05:03Z","2019-07-11T15:00:05Z"
"","785","LUCENE-8918: Disallow null terms in PhraseQuery","It is currently possible to create a PhraseQuery consisting of a single `null` term, which will then cause NullPointerExceptions deep inside postings decoding at search time.  We should instead catch this up-front, and prevent construction of the Query.","closed","","romseygeek","2019-07-15T10:26:11Z","2019-07-18T12:40:47Z"
"","1283","LUCENE-9246: Remove `dOff` argument from `LZ4#decompress`.","It is always set to 0 at call sites.","closed","","jpountz","2020-02-24T14:10:56Z","2020-02-28T10:27:00Z"
"","1439","tiny refguide tweaks","It doesn't work otherwise.  Hello, @ctargett I've tried to follow instruction, but turns out it incomplete and loose significant package. Added one caveat and tweak formatting.","closed","","mkhludnev","2020-04-19T10:07:22Z","2020-04-22T22:50:55Z"
"","807","Remove solr.jetty.https.port when SSL is not used","It appears the start script is sending `-Dsolr.jetty.https.port=8180` even if SSL is not enabled.  Changing the condition to `""$SOLR_SSL_ENABLED"" == ""true"" ` will remove the unnecessary confusing parameter when SSL is not enabled.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the Ref Guide (for Solr changes only).","closed","","upendrasoft","2019-07-24T15:23:39Z","2020-04-29T17:02:23Z"
"","1411","SOLR-14381: Integer overflow in total count in grouping results","Integer overflow in total count in grouping results. Please note that this is a PR on branch_8x, since it is more complex than the master change. 8x needs backward and forward compatability, and hence Utils.intIfNotOverflown(long) is used here. In the master variant of this patch, that method wouldn't be there and long would be expected everywhere (including tests).","open","","chatman","2020-04-06T17:27:00Z","2020-04-28T19:07:14Z"
"","1262","LUCENE-9220: regenerate all stemmers/stopwords/test data from snowball 2.0","Instead of patching them after-the-fact (both manually and automatically over the years) we patch the generator.  This is easier to maintain than patches/changes against generated code. See LUCENE-9220 for more information.  There is a remaining nocommit, test data. Also need to hook in and test the new languages that are added here.","closed","","rmuir","2020-02-16T00:19:11Z","2020-10-20T01:51:23Z"
"","786","LUCENE-8916: GraphTokenStreamFiniteStrings preserves all incoming attributes","Instead of cloning terms and position increments, we keep track of all incoming attributes and replay them from `getFiniteStrings()`.  Also incorporates the changes from LUCENE-8644, while keeping the old `newSynonymQuery()` method signature for backwards-compatibility.","closed","","romseygeek","2019-07-15T12:48:14Z","2019-07-19T10:47:57Z"
"","1118","Change 72h voting rules for release wizard","Instead of a hard 3 working day requiremet, we use the ASF standard of 72 clock hours. But use the holiday logic to warn the RM about upcoming holidays and that she may want to extend the deadline.","closed","","janhoy","2019-12-24T12:00:38Z","2019-12-24T22:13:05Z"
"","1580","LUCENE-9405: Ensure IndexWriter only closes merge readers once.","IndexWriter incorrectly calls closeMergeReaders twice when the merged segment is 100% deleted ie. would produce a fully deleted segment.","closed","","s1monw","2020-06-15T19:33:04Z","2020-06-15T20:02:23Z"
"","1590","LUCENE-9408: Ensure OneMerge#mergeFinished is only called once","in the case of an exception it's possible that some OneMerge instances will be closed multiple times. This commit ensures that mergeFinished is really just called once instead of multiple times.","closed","","s1monw","2020-06-17T21:52:15Z","2020-06-24T19:28:54Z"
"","1293","SOLR-14044: Delete collection bug fix by changing sharedShardName to use the same blob delimiter","In shared storage, deletion from the blob store works by listing files beginning with a certain prefix and reading from a blob store requires knowing some or all of the file name.  The sharedShardName, or the identifier prefixing all blob storage files per shard works by concatenating the collection name (uniqueness assumption in solr cloud overall) and the shard name with _  joining the two. _ is allowed in collection names which allows for a bug in listing files from other collections unintentionally.  This change changes the delimiter to use ""/"" which is more consistent with how index files stored in blob are structured anyway /. Piggy-backed a minor unrelated test fix in DistributedZkUpdateProcessorTest.java that I missed earlier.  Change Summary - Use ""/"" in sharedShardName - Update tests - Fix DistributedZkUpdateProcessorTest which tries to delete all collections after every test. In the same cleanup method the entire tmp directory we use as a local blob store gets deleted at the same time so the collection deletion errors out and leaks threads. This is a test setup issue vs a functional one. We spin up a new cluster in the different test cases anyway so we don't need to delete all collections.","closed","","andyvuong","2020-02-27T00:04:53Z","2020-03-05T22:01:45Z"
"","731","LUCENE-8865: Move to executor in IndexSearcher","In order to simplify testing this change moves to use the Executor interface instead of ExecutorService. This change also simplifies customizing execute methods for use-cases that need to add additional logic for forking to new threads. This change also adds a test for the optimization added in LUCENE-8865.  This change is fully backwards compatible since ExecutorService implements Executor.","closed","","s1monw","2019-06-19T12:35:23Z","2019-06-20T12:26:45Z"
"","1664","SOLR-11208: Usage SynchronousQueue in Executors prevent large scale operations","In `OverseerCollectionMessageHandler` replace the `SynchronousQueue` by a `LinkedBlockingQueue` for handing more jobs to the executor service than there are threads.  Given this is an old Jira, the issue is real, the OP suggested that easy fix almost 3 years ago and nobody implemented it, there might be something I missed here, so will not push that code before somebody reviews it.","closed","","murblanc","2020-07-10T16:22:07Z","2020-09-16T22:38:16Z"
"","1479","LUCENE-9278: Improved options file creation","Improved version of the options file creator in the renderJavadocs task: All parameters are escaped automatically, arguments don't need to be strings (they are converted during building options file). This makes it easy to use (adding new options) and e.g., filenames are escaped correctly","closed","","uschindler","2020-05-01T22:53:11Z","2020-05-02T07:53:58Z"
"","1097","LUCENE-9099: Correctly handle repeats in ORDERED and UNORDERED intervals","If you have repeating intervals in an ordered or unordered interval source, you currently get somewhat confusing behaviour:  * `ORDERED(a, a, b)` will return an extra interval over just `a b` if it first matches `a a b`, meaning that you can get incorrect results if used in a `CONTAINING` filter - `CONTAINING(ORDERED(x, y), ORDERED(a, a, b))` will match on the document `a x a b y` * `UNORDERED(a, a)` will match on documents that just containg a single `a`.  This commit adds a `RepeatingIntervalsSource` that correctly handles repeats within ordered and unordered sources.  It also changes the way that gaps are calculated within ordered and unordered sources, by using a new `width()` method on `IntervalIterator`.  The default implementation just returns `end() - start() + 1`, but `RepeatingIntervalsSource` instead returns the sum of the widths of its child iterators.  This preserves `maxgaps` filtering on ordered and unordered sources that contain repeats.  In order to correctly handle matches in this scenario, `IntervalsSource#matches` now always returns an explicit `IntervalsMatchesIterator` rather than a plain `MatchesIterator`, which adds `gaps()` and `width()` methods so that submatches can be combined in the same way that subiterators are.  Extra checks have been added to `checkIntervals()` to ensure that the same intervals are returned by both iterator and matches, and a fix to `DisjunctionIntervalIterator#matches()` is also included - `DisjunctionIntervalIterator` minimizes its intervals, while `MatchesUtils.disjunction` does not, so there was a discrepancy between the two methods.","closed","","romseygeek","2019-12-18T10:30:15Z","2020-02-06T14:44:57Z"
"","1441","LUCENE-9332 validate source patterns using gradle","If we have a gradle task with declared InputFiles and OutputFile then Gradle can figure out when the check is still up to date and when it needs to be rerun. Most of this is just a straight copy from the groovy script. This can take up to 20s off of precommit.  TODO: * Figure out how to enable the ratDocument part of this, it was failing on imports for me. * Split into modules so that we don't run the whole task each time * Possibly split by file types?  The rat part is the main thing that I need help with, I haven't been able to figure out the right incantation of dependency declarations and build script declarations and whatever else we need to make it in scope. Any pointers appreciated.","closed","","madrob","2020-04-20T14:51:04Z","2020-08-31T08:52:21Z"
"","1658","LUCENE-9423: Handle exception in NIOFSDirectory#openInput","If we fail to get the size of a file in the constructor of NIOFSIndexInput, then we will leak a FileChannel opened in NIOFSDirectory#openInput.","closed","bug,","dnhatn","2020-07-07T20:36:31Z","2020-07-09T14:43:43Z"
"","993","LUCENE-9303: maybe can simpler","If there has need to add the `if`, because `fieldType.stored()` is executed before.","closed","","kkewwei","2019-11-02T08:46:18Z","2020-12-21T09:19:00Z"
"","1215","LUCENE-9164: Ignore ACE on tragic event if IW is closed","If an IndexWriter was closed, then AlreadyClosedException should not be considered a tragic event.","closed","","dnhatn","2020-01-27T16:46:48Z","2020-03-06T19:03:30Z"
"","1263","LUCENE-9228: Sort dv updates by terms by applying","If all updates update a single field to the same value, then we can apply these updates in the term order instead of the request order as both will yield the same result. This optimization allows us to iterate the term dictionary faster and de-duplicate updates.","closed","","dnhatn","2020-02-17T03:12:31Z","2020-02-20T18:25:09Z"
"","1579","SOLR-14541: Ensure classes that implement equals implement hashCode or suppress warnings","I've created new hashCode methods for all of the classes that implement equals but not hashCode and removed associated SuppressWarnings. I've marked certain of them with TODOs where I'm really uncertain what the right thing to do is to draw attention, but I'd appreciate people looking at the others.  gw check succeeds, but that's all I'm guaranteeing at present. I need to let this bake a while and come back and re-visit them in detail if people who know the particular code better don't give a thumbs-up. hashCode implementations can be tricky, and these are definitely straw-man.  I'll add some more comments on the JIRA","closed","","ErickErickson","2020-06-15T14:26:12Z","2020-06-25T16:44:23Z"
"","1116","SOLR-14135: Utils.toJavabin returns a byte[] instead of InputStream","I'm not too convinced about this PR honestly, I started thinking in doing this mostly because in the 8x branch we can't use InputStream's `readAllBytes();` method, but this may actually hurt future consumers of this method, if they don't need to read all bytes at once. I'll leave this PR for now, worst case I'll keep the tests.","open","","tflobbe","2019-12-23T21:48:45Z","2020-01-02T18:15:08Z"
"","753","LUCENE-8895: switch all FST usage to enable array-with-gaps encoding","I'm not sure we want to commit this as-is, mainly because of the change to {{blocktreeords}} codec. With this change, if we encoded some node's outgoing Arcs as an array-with-gaps, we treat it as a list, scanning forward with a linear scan, rather than a binary search. Implementing binary search in this case might be possible, but it's quite complex. Another possibility is to leave the option of disabling the array-with-gaps opto and leaving it off for this one case. I'd like to get a sense how widely used this codec is and whether it's worth having that preserved in the API.","closed","","msokolov","2019-06-30T20:18:50Z","2019-07-02T22:41:05Z"
"","1074","BlockTreeTermsWriter should compute prefix lengths using Arrays#mismatch.","I'm not expecting any speedups, this is mostly for consistency.","closed","","jpountz","2019-12-11T18:27:35Z","2020-01-06T08:02:55Z"
"","1526","SOLR-14495: Fix or suppress warnings in solr/search/function","I'll push this Wednesday or so barring objections","closed","","ErickErickson","2020-05-18T17:43:32Z","2020-05-22T01:53:19Z"
"","1085","SOLR-14087 disable package store API if -Denable.packages not set to true","I wish this to be included in 8.4. There are so many security holes in Solr and we want to play extra safe","closed","","noblepaul","2019-12-15T11:18:03Z","2020-03-02T19:49:39Z"
"","1230","LUCENE-9134: Port ant-regenerate tasks to Gradle build","I think this is the last javacc bit, the Solr queryparser","closed","","ErickErickson","2020-02-02T01:45:04Z","2020-02-04T14:17:05Z"
"","1226","LUCENE-9134 Port ant-regenerate tasks to Gradle build","I think this is the final version, I'll merge upstream over the weekend absent objections. The cleanup does three things:  1> makes it all compile 2> allows precommit to pass 3> fixes all the compiler warnings except deprecations. Which means that the ""./gradlew assemble"" produces clean output since the default lint settings don't report them except to say ""there are some"". This should reduce the frequency of people hand-editing these files.  A couple of notes:  1> the deprecations warning are mostly from FuzzyQuery, from changes a long time ago and I don't have the energy to tackle that, it should be a separate JIRA anyway.  2> cast and unchecked SuppressWarnings are no longer necessary, you'll note they are missing. Instead of suppressing warnings, the cleanup code fixes the source. We're using Javacc 5.0 and compiling against Java 11, so until we upgrade JavaCC they should stay I think.  3> all precommit and tests pass under Ant as well as Gradle.","closed","","ErickErickson","2020-01-30T16:22:35Z","2020-01-31T22:05:35Z"
"","1550","LUCENE-9383: benchmark module: Gradle conversion (complete)","I switched from ""java-library"" type of Gradle plugin/module to more plainly ""java"" because this module isn't a library (isn't something depended on by anything), it's closer to an app.  I tried type ""application"" but I didn't have the same control that the ""JavaExec"" task gives you.  One consequence of not using ""java-library"" is that the names of the categories of dependencies are different, and so this appears odd/unusual relative to the other modules.  I did not convert ""collation"" and ""shingle"" Ant targets, but I put there the two-line CLI equivalents for both in the form of a comment.  I ram them and they worked... albeit a confusion in one of the perl scripts that thought ""darwin"" OS was ==~ Windows simply because it contained ""win"" :-).   Notice the style of ""getEnWiki"" and ""getGeoNames"" and ""getTop100kWikiWordFiles"":  One task that does all it needs to do by adding a final step in doLast.  Now notice a different style: ""reuters"" (depending on extractReuters depending on getReuters).  This is more verbose, but admittedly for this case it has to do more.  I'm not well versed enough in Gradle to know which style is preferable.  I lean towards short & concise.  The current state is a nocommit IMO; need to harmonize the approaches.  I did not convert https://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz (aka news20) or https://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz or https://kdd.ics.uci.edu/databases/20newsgroups/mini_newsgroups.tar.gz (aka mini-news) because I could not find .alg files that used them.","closed","","dsmiley","2020-06-01T14:29:32Z","2020-06-05T21:59:31Z"
"","1002","[docs]: fixed two small errors in JavaDoc","I only fixed two small typo errors in the JavaDocs","closed","","0xflotus","2019-11-10T20:13:10Z","2020-01-13T14:30:37Z"
"","1199","LUCENE-9134: Port ant-regenerate tasks to Gradle build","I need to put this down for a while, so please feel free to modify/commit/whatever.  Current state:  I think this covers everything except the analysis part of the regenerate task that Dawid is working on.  I left in the ant javacc task because I ran out of patience. The plugin I thought was going to work would keep executing even for the ""assemble"" task. so I went back to using the ant javacc task.  You can skim pretty much everything except the gradle.build and createLevAutomatoa.py files. The rest are are the results of getting the lint warnings out of the generated code so people don't go back in and hand-edit them. There's a bit of superfluous whitespace that's been changed, sorry about that. StandardSyntaxParser.java has some methods reordered, why I don't know. But they're identical so I'm not going to worry about them.  If I missed any warnings and the like we can address in another go-round.","closed","","ErickErickson","2020-01-22T17:03:53Z","2020-01-31T22:09:57Z"
"","975","Add temporal unit to warmupTime in ref guide","I haven't created a Solr Ticket for this minor change, if really necessary, I can and would update PR and commit message.  # Description  Wanted to know the unit of warmUpTime, but it was not included.  # Solution  Added it, so other people don't have to dig.  # Tests  None, but you can quickly verify by checking source. https://github.com/apache/lucene-solr/search?l=Java&q=warmupTime  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","tobijdc","2019-10-24T22:47:02Z","2019-10-25T20:02:26Z"
"","1021","LUCENE-9056: Fewer conditionals in #advance.","I also cherry-picked LUCENE-8901 (lazy loading of freq blocks) for BlockDocsImpactsEnum.","closed","","jpountz","2019-11-20T14:45:13Z","2019-11-21T17:20:16Z"
"","934","Fix typo","I _think_ there is a missing word here. ""one **which** has too many states""","closed","","harry-wood","2019-10-09T09:29:11Z","2019-11-27T08:53:50Z"
"","919","LUCENE-8994: Code Cleanup - Pass values to list constructor instead of empty constructor followed by addAll().","https://issues.apache.org/jira/projects/LUCENE/issues/LUCENE-8994  If you have actual serious issues to attend, no need to bother with this PR, it is code cleanup, not features or fixes.  A small and unimportant PR. Some code cleanup. And perhaps in some cases a small performance gain.  I would understand if issue was taken concerning readability in some cases.  I could change those cases to look something like this, if it made if more readable:  ``` new ArrayList<>(     nameOfCollection.getMethod(someExtraVars) ); ```  Frankly, it already was equally unreadable in such cases as:  ``` resources.addAll(Accountables.namedAccountables(""field"", fields)); ```  Depends on what the reviewer wants, I suppose.","closed","","KoenDG","2019-10-02T23:57:01Z","2019-10-14T20:05:04Z"
"","1584","Add VectorField: WIP","https://issues.apache.org/jira/projects/LUCENE-ABCD  # Description  Add VectorField   # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","vthacker","2020-06-16T20:15:22Z","2020-06-16T20:15:39Z"
"","1523","SOLR-14494: Refactor BlockJoin to not use Filter","https://issues.apache.org/jira/browse/SOLR-14494  In this PR I may have over-reached the refactorings beyond the scope a bit; it could be scaled back.  I didn't get why the cache interaction was as complex as it was so I simplified it but I may be wrong on this. CC @mkhludnev","closed","","dsmiley","2020-05-17T05:58:33Z","2020-05-30T01:44:56Z"
"","1508","SOLR-14472: AutoScale: ""cores"" also track lazy, transient","https://issues.apache.org/jira/browse/SOLR-14472","closed","","dsmiley","2020-05-11T21:08:10Z","2020-05-18T21:43:30Z"
"","1490","SOLR-14461: Replace commons-fileupload with Jetty","https://issues.apache.org/jira/browse/SOLR-14461","closed","","dsmiley","2020-05-07T05:34:56Z","2020-05-22T04:35:00Z"
"","1466","SOLR-14445: Add Entity Caching documentation","https://issues.apache.org/jira/browse/SOLR-14445","closed","","tkaessmann","2020-04-29T10:49:12Z","2020-10-21T11:37:08Z"
"","1463","SOLR-14440 Cert Auth plugin","https://issues.apache.org/jira/browse/SOLR-14440","closed","","madrob","2020-04-27T22:24:30Z","2020-05-01T17:47:13Z"
"","1453","SOLR-14433: Improve SolrShardReporter default metrics list","https://issues.apache.org/jira/browse/SOLR-14433#  Now includes TLOG and UPDATE./update. These were small bugs to begin with but from user perspective this is an incremental improvement.  CC @sigram","closed","","dsmiley","2020-04-23T21:37:59Z","2020-04-28T03:48:43Z"
"","1455","SOLR-14428 minimize memory footprint of fuzzy query","https://issues.apache.org/jira/browse/SOLR-14428  Make the automata of a fuzzy query mutable so that we don't always have to store them. However, there will be cases when we need to recompute them anyway.  Will add unit tests if this approach makes sense.","closed","","madrob","2020-04-24T17:11:48Z","2020-05-05T23:04:18Z"
"","1487","SOLR-14426 Move top level classes to nested classes","https://issues.apache.org/jira/browse/SOLR-14426  This is as far as I could get before needing to give up. It was partly automated by IntelliJ but still painful manual in some respects. I think I got everything in the analytics module, and made a dent in core.  Probably want to view this comparison ignoring whitespace changes.","closed","","madrob","2020-05-05T21:13:17Z","2020-05-07T15:04:07Z"
"","1446","SOLR-14425: Aliases: ZK sync() needs to be synchronous","https://issues.apache.org/jira/browse/SOLR-14425  CC @dragonsinth @tflobbe Does this make sense?  Separately, I'll file an issue to add CuratorFramework to SolrZkClient","closed","","dsmiley","2020-04-22T15:17:14Z","2020-05-05T19:17:56Z"
"","1442","SOLR-14420 Declare ServletRequests as HttpRequests","https://issues.apache.org/jira/browse/SOLR-14420","closed","","madrob","2020-04-20T22:58:11Z","2020-04-22T17:06:22Z"
"","1401","SOLR-14382: delegating search component","https://issues.apache.org/jira/browse/SOLR-14382","closed","","cpoerschke","2020-04-03T18:19:12Z","2020-04-17T16:47:52Z"
"","1399","SOLR-14376: optimize SolrIndexSearcher.getDocSet matches everything","https://issues.apache.org/jira/browse/SOLR-14376  * getProcessedFilter now returns null filter if it's all docs more reliably * getProcessedFilter now documented clearly as an internal method * getDocSet detects all-docs and exits early with getLiveDocs  The PR depends on #1381 (SOLR-14364 LTR to not call getProcessedFilter) because the current code there incorrectly assumed that a null ProcessedFilter.filter meant match nothing when it's the opposite.  Since a null filter is set more reliably now, it resulted in some tests over there failing.  Really people should avoid this low level method if they can.  The PR has improved javadocs and some changes in getProcessedFilter I added to improve readability (to me, any way).  I also touched getDocSetBits & makeDocSetBits trivially to check where an instanceof check happens because I thought it was clearer.","closed","","dsmiley","2020-04-03T04:03:19Z","2020-04-07T20:25:23Z"
"","1381","SOLR-14364: LTR SolrFeature fq improvements","https://issues.apache.org/jira/browse/SOLR-14364  Admittedly there is no test change.  Again my primary motivation is really just for this class to not call `SolrIndexSearcher.getProcessedFilter` and otherwise behave as it did before.  CC @cpoerschke","closed","","dsmiley","2020-03-26T04:38:46Z","2020-04-05T03:49:16Z"
"","1368","SOLR-14351: Fix/improve MDCLoggingContext usage","https://issues.apache.org/jira/browse/SOLR-14351","closed","","dsmiley","2020-03-20T20:48:18Z","2020-04-04T18:49:06Z"
"","1366","SOLR-14342: Optimize core loading order in SolrCloud.","https://issues.apache.org/jira/browse/SOLR-14342 (see the issue).  I chose to rewrite much of the test to give me greater confidence that it works as designed.  If I, for example, comment out much of CoreSorter.init then the test fails.","closed","","dsmiley","2020-03-19T17:37:41Z","2020-04-05T03:49:07Z"
"","1348","SOLR-14326 Number of tlog replicas off by one when restoring collections","https://issues.apache.org/jira/browse/SOLR-14326  When making a request to restore a collection, the quantity of tlog replicas will always be off by one when restoring a collection that doesn't contain nrt replicas or when specifying the quantity of replicas in the request itself.  This is due to a flawed comparison where an int meant to be an iterator for tlog replicas is checked if it is greater than zero, however, since that variable was initialized as 0 just prior it will never be greater than zero. The fix is to compare the desired number of tlog replicas (like nrt) rather than the iterator.","closed","","CBWallaby","2020-03-13T15:00:14Z","2021-04-12T16:42:59Z"
"","1347","SOLR-14322 Improve AbstractFullDistribZkTestBase.waitForThingsToLevelOut","https://issues.apache.org/jira/browse/SOLR-14322","closed","","madrob","2020-03-12T19:30:15Z","2020-03-26T22:39:36Z"
"","1286","SOLR-14279: remove CSVStrategy's deprecated setters","https://issues.apache.org/jira/browse/SOLR-14279","closed","","cpoerschke","2020-02-24T18:39:31Z","2021-12-31T09:54:04Z"
"","1257","SOLR-14258: DocList should not extend DocSet","https://issues.apache.org/jira/browse/SOLR-14258","closed","","dsmiley","2020-02-14T03:48:45Z","2020-04-19T04:17:16Z"
"","1191","SOLR-14197 Reduce API of SolrResourceLoader","https://issues.apache.org/jira/browse/SOLR-14197","closed","","dsmiley","2020-01-21T13:54:36Z","2020-03-11T21:41:49Z"
"","1202","SOLR-14149: CHANGES.txt Remove off-topic stuff","https://issues.apache.org/jira/browse/SOLR-14149","closed","","dsmiley","2020-01-22T21:42:35Z","2020-04-19T04:17:38Z"
"","1072","Don't start hdfs on windows. Remove tests.disableHdfs.","https://issues.apache.org/jira/browse/SOLR-14053","closed","","dweiss","2019-12-11T13:31:52Z","2019-12-11T14:01:39Z"
"","1166","SOLR-14040: shareSchema support for SolrCloud","https://issues.apache.org/jira/browse/SOLR-14040  The essence of this change is that ConfigSetService does schema caching itself (optionally) instead of schema caching being done in a subclass.  The problem with a subclass was that subclassing was already used to differentiate SolrCloud from standalone, but we want both modes to have this ability.    A consideration I had was how a user could force a new schema to be loaded if for some reason the old one was obsoleted.  Perhaps it wasn't changed but it referenced some file.  Ultimately, users need to forcibly update the schema even if the schema file itself is identical. I'm leaving this as a manual operation, though it'd be nice if the user could do this through Solr's API with some new command that does not yet exist.  The new test in ConfigSetsAPITest.testSharedSchema is admittedly pretty minimal but no more minimal than the existing test in TestCoreContainer.testSharedSchema.  I wasn't sure where to put the new test; TestCoreContainer isn't a SolrCloud test so I couldn't put it there.  Some improvements: * Use Caffeine impl and weak values (to the schema).  Previously the cache never evicted!  Some refactorings: * Renamed ConfigSetService.getConfig to loadConfigSet to more clearly show it loads a ConfigSet (and not SolrConfig). * In CloudConfigSetService.createCoreResourceLoader I moved the lookup of the configSet name from here to all the way out into CloudDescriptor (attached to CoreDescriptor) in its constructor.  The intention is to ensure that coreDescriptor.getConfigSet is correct in SolrCloud mode so that, for example, when we go to potentially cache the schema, we easily know to what configSet it is associated with.  Maybe this is the most contentious change, but it was very confusing during development of this overall issue to see this coreDescriptor.getConfigSet method which I was tempted to call but I had to keep reminding myself not to call it because it was empty in SolrCloud mode.  Unfortunately, this makes creation of a core descriptor not a trivial creation from properties but it also takes in a ZkController which is used to do the lookup.  This must be done *somewhere* of course so it's not about perf but a conceptual conundrum in how we think of these core descriptor thingies.  I'm inclined to think that much of CoreDescriptor is flawed as written, particularly in a SolrCloud world where ZK is truth, not properties files from disk that may only reflect the state of things at the time the core was created.  * In CloudConfigSetService.createCoreResourceLoader there was a comment and some code: ""// for back compat with cores that can create collections without the collections API"".  I removed this as it appears to be impossible now; all tests passed with it gone.  (yay!) ** Probably related: There was a core.properties in the test path under a conf directory, which is really weird.  It appears no test truly uses it, yet nevertheless during debugging of the above I saw a test indirectly fail due to the presence of this file. * Changed static IndexSchemaFactory.getResourceNameToBeUsed to be an instance method on the IndexSchemaFactory instance so that the factory implementation could return a suitable answer.  The static method made assumptions based on the two known types which was not as clean.  Additionally it also used to actually look at the files on disk to get an accurate answer but I removed that because we can handle a reasonable guess response.  See, at this juncture I would rather avoid probing ZooKeeper over which files exist (managed-schema vs schema.xml) over this small matter.  It's really a wart IMO; the name ought to be deterministic.  Consequently someone wanting schema sharing and who is also using a managed schema must ensure their schema is named managed-schema (or whatever they configured it to be in solrconfig.xml).","closed","","dsmiley","2020-01-14T05:12:55Z","2020-01-21T19:00:04Z"
"","990","[SOLR-13885] Typo corrections.","https://issues.apache.org/jira/browse/SOLR-13885  I noticed this while reading the documenation.  Try pronouncing `it's` as `it is` and you'll head the lines no longer making sense.  All fixes should be correct, though perhaps not complete across the documentation. I only checked the `adoc` files.","closed","","KoenDG","2019-11-01T16:34:35Z","2019-12-02T21:35:24Z"
"","777","SOLR-11724: Fix for 'Cdcr Bootstrapping does not cause ''index copying'' to follower nodes on Target' BUG","https://issues.apache.org/jira/browse/SOLR-11724  # Description  When indexing documents in to Source, bootstrapping only copies the index to leader node of shards of the collection and does not start replication to its replicas  # Solution  This change now ensures that the follower recovery request is now sent to replica. Previously this was only sent to the leader which caused issues if the replica was not hosted on the same node.  # Tests  Test case "" testBootstrapWithMultitpleReplicas()"" has been added.  See results of tests here: https://issues.apache.org/jira/browse/SOLR-11724?focusedCommentId=16844254&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16844254  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [X] I have added documentation for the Ref Guide (for Solr changes only).   As a note, I did not develop the .patch linked, however it solved the replication issues I was having and I noticed that no one had submitted the patch or created a pull request for it.","closed","","Tdspringsteen","2019-07-11T19:44:01Z","2019-08-09T12:42:57Z"
"","1531","SOLR-11334: Split some field lists better","https://issues.apache.org/jira/browse/SOLR-11334","closed","","dsmiley","2020-05-23T04:26:02Z","2020-06-03T03:00:48Z"
"","881","LUCENE-8979: Code Cleanup: Use entryset for map iteration wherever possible. - part 2","https://issues.apache.org/jira/browse/LUCENE-8979  It appears not all instances were picked up by the previous sweep, which is strange as it's all automated. I'm hoping this is all of them.  Some simple cleanup. No need to spend company/personal time reviewing this, would be good if someone had some spare time. If you don't, feel free to close this tab.  It's a very simple PR: just changing keySet() to entrySet() when possible. In general just the better choice. Only really a performance gain when going over a TreeMap(as I recall at least), but in general a good idea to consistent with it.  A small move in TermsComponent.java, there's no point in doing the method call to populate the field and then the very next thing that happens is an unrelated check that potentially forces a `continue`, rendering the call pointless for that iteration.  All tests have run locally and succeeded.  Sorry about doing this twice, I don't get how these weren't picked up the first time.","closed","","KoenDG","2019-09-15T00:51:42Z","2019-10-14T20:06:06Z"
"","868","LUCENE-8975: Code Cleanup: Use entryset for map iteration wherever possible.","https://issues.apache.org/jira/browse/LUCENE-8975  Some simple cleanup. No need to spend company/personal time reviewing this, would be good if someone had some spare time. If you don't, feel free to close this tab.  It's a very simple PR: just changing `keySet()` to `entrySet()` **when possible**. In general just the better choice. Only really a performance gain when going over a TreeMap(as I recall at least), but in general a good idea to consistent with it.  2 exceptions where it gets changed to `values()`.  All changes done automatically by Intellij.  And 1 String construction in a loop that got turned into a Stringbuilder because that's just wasteful.  That's over in solr/core/src/java/org/apache/solr/search/facet/FacetRequest.java  All tests have run locally and succeeded.","closed","","KoenDG","2019-09-10T18:59:05Z","2019-09-13T14:44:15Z"
"","1222","LUCENE-9414 EagerCheapMergePolicy","https://issues.apache.org/jira/browse/LUCENE-8962 (contributed by Salesforce)  See early comment: // Extends TieredMergePolicy for convenience but we could adjust to delegate or modify TieredMergePolicy","open","","dsmiley","2020-01-29T16:03:33Z","2020-06-22T21:30:03Z"
"","1169","LUCENE-9004: A minor feature and patch -- support deleting vector values and fix segments merging","Hi, @msokolov and @mocobeta, thanks for your exciting work on [LUCENE-9004](https://issues.apache.org/jira/browse/LUCENE-9004).  This commit includes the following modifications: 1. For fixing the merge process while some segments may contain deleted documents which should be filtered;  2. For fixing ram usage in KnnGraphValuesWriter, this may be a typo;  3. Fix a potential NPE problem in KnnGraphQuery and delete some unused imports;  The modified codes have been tested via test cases.","closed","","irvingzhang","2020-01-14T17:03:23Z","2020-02-09T13:55:25Z"
"","1625","Highlighting the actual state observed in LUCENE-9328","Hi, @ctargett. I'm not sure if it's valid to stroke through, or it's better to just drop this word?","closed","","mkhludnev","2020-06-28T10:38:50Z","2020-08-31T14:51:53Z"
"","915","SOLR-13661: Package management APIs, Package loading, Package store","Here, I've reverted all previously merged half-baked code in a single commit. After that, there's the intended change in a single commit.  Design document is here: https://docs.google.com/document/d/15b3m3i3NFDKbhkhX_BN0MgvPGZaBj34TKNF2-UNC3U8/edit?ts=5d86a8ad#  Discussions in SOLR-13661 and in #solr-plugins channel in Slack.","closed","","chatman","2019-10-02T09:33:01Z","2020-11-10T18:23:08Z"
"","1509","SOLR-10810: Examine precommit lint WARNINGs in non-test code","Here's what I've got for removing some of the warnings in solr/core, here for discussion.  There are a lot of changes from static class declarations to nested declarations, and a few wildcard generics that snuck in.  Please discuss freely.","closed","cleanup,","ErickErickson","2020-05-11T22:03:04Z","2020-07-18T13:46:57Z"
"","1532","Solr 14474","Here's the remaining ones in Solr, I'll merge these in on Tuesday barring objections.","closed","","ErickErickson","2020-05-23T18:41:04Z","2020-05-23T18:41:16Z"
"","1219","LUCENE-9134: Javacc skeleton for Gradle regenerate","Here's the build changes to get javacc to run, modeled on the jflex changes , many thanks for the model. Only two files changed here ;)  If the structure is OK, I'll fill in the ""doLast"" blocks with the cleanup code and maybe be able extract some common parts. NOTE: you can't even compile the result of running this because I wanted the changes to the build structure to be clear first so didn't include the cleanup tasks yet.  So if this structure is OK, should I merge it into master before or after the rest of the cleanup? My assumption is after. I want to try to get all the warnings etc. out of the generated code in the next phase to reduce the temptation for people to make hand-edits.  I didn't intentionally change the line endings in defaults-java, there's no other change there...","closed","","ErickErickson","2020-01-28T12:21:28Z","2020-01-30T01:03:19Z"
"","1218","LUCENE-9134: Javacc skeleton","Here's the build changes to get javacc to run, modeled on the jflex changes , many thanks for the model. Only two files changed here ;)  If the structure is OK, I'll fill in the ""doLast"" blocks with the cleanup code and maybe be able extract some common parts. NOTE: you can't even compile the result of running this because I wanted the changes to the build structure to be clear first so didn't include the cleanup tasks yet.  So if this structure is OK, should I merge it into master before or after the rest of the cleanup? My assumption is after. I want to try to get all the warnings etc. out of the generated code in the next phase to reduce the temptation for people to make hand-edits.","closed","","ErickErickson","2020-01-28T03:37:44Z","2020-01-30T00:55:50Z"
"","983","SOLR-13101: many updates, including concurrent update request handling","Here's an update to the shared storage branch that includes, among many other changes, support for concurrent update requests.","closed","","yonik","2019-10-30T14:59:42Z","2019-10-30T20:45:06Z"
"","1650","SOLR-14021: Deprecate HDFS support in 8.6","Here's a PR that adds a deprecation warning to HdfsDirectory and HdfsBackupRepository, ref guide warning on ""Running Solr on HDFS"" page.","closed","","chatman","2020-07-04T07:03:22Z","2021-04-05T21:22:51Z"
"","1567","LUCENE-9402: Let MultiCollector handle minCompetitiveScore","Here is an idea to make MultiCollector be able to handle `minCompetitiveScore`. Looking at this comment in the code: ```             // Ignore calls to setMinCompetitiveScore so that if we wrap two             // collectors and one of them wants to skip low-scoring hits, then             // the other collector still sees all hits. We could try to reconcile             // min scores and take the maximum min score across collectors, but             // this is very unlikely to be helpful in practice. ``` This implementation tries to make it so that all collectors can see all the hits and only allow skipping if all collectors set a min competitive score. The value passed to the inner scorer is the minimum among all collectors (instead of the maximum as the comment suggests).","closed","","tflobbe","2020-06-10T21:30:11Z","2020-06-18T17:19:53Z"
"","973","LUCENE-9027: Use SIMD instructions to decode postings.","Here is a draft PR in case somebody would like to play with this. This is not ready to merge as the changes to `DataInput` might be controversial and we'd also need to  - remove write support to Lucene 50  - move Lucene50 to backaward-codecs  - add a Completion84PostingsFormat  - remove write support from Completion50PostingsFormat  - move Completion50PostingsFormat to backward-codecs","closed","","jpountz","2019-10-24T16:58:10Z","2019-11-18T18:08:37Z"
"","1016","SOLR-13662: Test fix & Reference guide for package manager","Here are the changes in this PR: 1) Fix SOLR-13662 test, and re-enabling the test. 2) Moving the current reference guide page (solr-packages.adoc) to ""Package Manager Internals"" and minor fixes there. 3) Adding a new package manager guide ""Package Manager"" for user facing actions.  Note: Notes for the package authors will be added to a developer guide later.","closed","","chatman","2019-11-17T20:13:48Z","2020-03-02T19:49:51Z"
"","1644","SOLR-14623: Minor Improvements to SolrCloud tests","Here are some of the changes herein: 1. MiniSolrCloudCluster: Changing the polling to waitForState (based on watch/latch) 2. Improve waiting for Jetty stop, wait until node is not in liveNodes (JettySolrRunner) 3. TestCloudConsistency: Explicitly pick the jetty hosting shard1's leader 4. TestWaitForStateWithJettyShutdowns: Wait until replica becomes active 5. TestCollectionsAPIViaSolrCloudCluster: Don't wait for the collection to be deleted fully at the end 6. TestRecovery: Unset the UpdateLog hooks","closed","","chatman","2020-07-03T17:49:29Z","2020-07-13T23:07:17Z"
"","1618","LUCENE-9418: Fix ordered intervals over interleaved terms","Given the input text 'A B A C', an ordered interval 'A B C' will currently return an incorrect internal [2, 3] in addition to the correct [0, 3] interval.  This is due to a bug in the ORDERED algorithm, where we assume that after the first interval is returned, the sub-intervals are  always in-order.  This assumption only holds during minimization, as minimizing an interval may move the earlier terms beyond the trailing terms.  For example, after the initial [0, 3] interval is found above, the algorithm will attempt to minimize it by advancing A to [2,2].  Because this is still before C at [3,3], but after B at  [1,1], we then try advancing B, leaving it at [Inf,Inf].  Minimization has failed, so we return out original interval of [0,3].  However, when we come to retrieve the next interval, our subintervals look like this: A[2,2], B[Inf,Inf], C[3,3] - the assumption that they are in order is broken.  The algorithm sees that A is before B, assumes that therefore all subsequent subintervals are in order, and returns the new interval.  This commit fixes things by changing the assumption of ordering to only hold during  minimization.  When first finding a candidate interval, the algorithm now checks that all sub-intervals appear in order.","closed","","romseygeek","2020-06-26T15:45:45Z","2020-06-30T09:06:32Z"
"","749","LUCENE-8893: Intervals.prefix() and Intervals.wildcard() should take BytesRef","Generally speaking, when parsers are creating wildcard or prefix  IntervalsSources, they will first want to pass the user input through Analyzer.normalize(). This returns a BytesRef rather than a String,  so the factory methods for these sources should take BytesRef as well.","closed","","romseygeek","2019-06-28T12:21:30Z","2019-07-01T08:30:38Z"
"","1042","LUCENE-9068: Build FuzzyQuery automata up-front","FuzzyTermsEnum can now either take an array of compiled automata, and  an AttributeSource, to be used across multiple segments (eg during  FuzzyQuery rewrite); or it can take a term, edit distance, prefix and transition boolean and build the automata itself if only being used once (eg for fuzzy nearest neighbour calculations).  Rather than interact via attribute sources and specialized attributes, users of FuzzyTermsEnum can get the boost and set minimum competitive boosts directly on the enum.","closed","","romseygeek","2019-11-26T13:33:23Z","2020-01-15T14:58:17Z"
"","1553","LUCENE-9393: FunctionScoreQuery turns TOP_DOCS to COMPLETE in inner weights","FunctionScoreQuery can't really use WAND algorithm even if TOP_DOCS score mode is requested. This commit makes the inner weight created use COMPLETE in this case.","closed","","tflobbe","2020-06-04T23:07:54Z","2020-06-05T18:04:20Z"
"","971","SOLR-13866: Override getSolrMetricsContext in DirectUpdateHandler2","From the Jira description: > DUH2 stats not populated in PluginInfoHandler: I believe this is related to the changes in SOLR-13677. DirectUpdateHandler2 seems to be missing to implement SolrMetricsContext getSolrMetricsContext(), which causes SolrInfoBean.getMetricsSnapshot() to be null","closed","","tflobbe","2019-10-23T22:29:25Z","2019-10-23T23:13:29Z"
"","1651","SOLR-14628: hl.fragsizeIsMinimum now defaults to true","Fragments size like <= Solr 8.4 and addresses a significant performance regression","closed","","dsmiley","2020-07-05T06:07:16Z","2020-07-06T13:31:44Z"
"","1388","LUCENE-9278: Use -linkoffline instead of relative paths to make links to other projects","For gradle build, use absolute urls instead of relative paths to make inter-project links so that javadoc destination directory can be moved under `_project_/build/`.  - docs of "":lucene:core"" prj will go to `lucene/core/build/docs/javadoc/` - docs of "":lucene:analysis:common"" will go to `lucene/analysis/common/build/docs/javadoc/` - docs of "":solr:core"" prj will go to `solr/core/build/docs/javadoc/` - ... and so on  Also, this directly calls [javadoc tool](https://docs.oracle.com/en/java/javase/11/tools/javadoc.html) rather than Ant javadoc task - Ant javadoc task doesn't recognize `element-list`, the successor to `package-list` up until Java 8, so `` no longer correctly work with JDK11 (https://issues.apache.org/jira/browse/SOLR-14352).  All generated docs passed the ""checkJavaDocs.py"" check. In other words, there's no missing package summary.  See also https://issues.apache.org/jira/browse/LUCENE-9278","closed","","mocobeta","2020-03-29T12:27:56Z","2020-04-09T07:18:55Z"
"","828","LUCENE-8753: UniformSplitPostingsFormat","Follow up of PR https://github.com/apache/lucene-solr/pull/633  Moves DeltaBaseTermStateSerializer inside uniformsplit package. Adds lucene.experimental annotation to all classes. Adds package javadoc.","closed","","bruno-roustant","2019-08-13T13:41:00Z","2019-12-02T10:26:22Z"
"","668","SOLR-13453: Adjust auth metrics asserts in tests after SOLR-13449","Fixes the test failures","closed","","janhoy","2019-05-09T14:10:38Z","2019-05-09T18:13:47Z"
"","1546","SOLR: Use absolute paths for server paths.","Fixes relative path ambiguities when the current working directory is atypical.  I discovered this problem when running Solr standalone from IntelliJ using a CWD that was maybe slightly unexpected. Some relative paths like the core instance dir were getting resolved relative to the CWD instead of the solr home or core root dir.  So I think it's preferable to make the major directories of Solr be absolute to avoid this problem.  Not worth a CHANGES.txt entry, I think.  BTW I *thought* maybe this might be related to https://issues.apache.org/jira/browse/SOLR-7323 because I was trying to reproduce it but no it's not.","closed","","dsmiley","2020-05-31T05:20:35Z","2020-06-15T18:53:56Z"
"","1517","SOLR-13289: Use the final collector's scoreMode","Fixes a bug @dsmiley pointed out to in [SOLR-13289](https://issues.apache.org/jira/browse/SOLR-13289?focusedCommentId=17103601&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17103601)","closed","","tflobbe","2020-05-14T22:14:29Z","2020-05-21T22:48:42Z"
"","1181","LUCENE-9145 First pass addressing static analysis","Fixed a bunch of the smaller warnings found by error-prone compiler plugin, while ignoring a lot of the bigger ones.  This is just the warnings found by #1176 without the build changes","closed","","madrob","2020-01-17T17:31:09Z","2020-01-17T19:30:43Z"
"","1176","LUCENE-9143 Add error-prone checks to build, but disabled","Fixed a bunch of the smaller ones, ignored a lot of the bigger ones. Incremental progress is the name of the game here!  This is still a work in progress and should probably not be merged in current state.  # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","madrob","2020-01-16T03:01:40Z","2020-09-22T15:32:09Z"
"","724","SOLR-13523: Atomic Update results in NullPointerException","fix xml loader and assert root is stored when updating non root docs","closed","","moshebla","2019-06-16T03:51:41Z","2019-06-20T16:22:02Z"
"","1493","Bug fix for false negatives produced by FuzzyQuery","Fix for Jira issue [9365](https://issues.apache.org/jira/browse/LUCENE-9365) where search for `abc` doesn't match doc `abcd` if prefixlength = 3 and edit distance =1.  The solution is to rewrite the FuzzyQuery as a RegExpQuery given this condition. Added related test to TestFuzzyQuery.","closed","bug,","markharwood","2020-05-07T13:43:11Z","2020-05-07T14:10:01Z"
"","1496","Bugfix for FuzzyQuery false negative when prefix length == search term length","Fix for Jira issue 9365 where search for `abc` doesn't match doc `abcd` if prefixlength = 3 and edit distance =1. The fix is to rewrite the FuzzyQuery as a specialised form of Automaton query when prefix length == search string length.","closed","bug,","markharwood","2020-05-07T16:34:24Z","2020-06-04T09:58:06Z"
"","674","SOLR-13499: Fix ""Apache License, Version 2.0"" spelling in pom.xml.template","Fix for [SOLR-13499](https://issues.apache.org/jira/browse/SOLR-13499). There are many Java libraries licensed under ""Apache License, Version 2.0"" that do not use its official spelling. This causes issues like https://issues.apache.org/jira/browse/MPIR-382: with every library defining its own spelling, it's difficult in large projects to have a clear view of all licenses in use. This PR changes the license spelling to the official one, as advised by Maven developers.","closed","","don-vip","2019-05-13T16:01:49Z","2020-08-07T16:03:42Z"
"","1184","LUCENE-9142 Refactor IntSet operations for determinize","Fix a bug where a frozen set could be symmetrically unequal to the sorted set that created it because we compared the backing array instead of only the active elements.","closed","","madrob","2020-01-18T00:21:59Z","2020-02-06T20:42:13Z"
"","1406","SOLR-14388: Remove references to rule-based replica placement in the ref guide","First cut of document changes.","closed","","ErickErickson","2020-04-05T14:30:08Z","2020-05-18T12:03:19Z"
"","700","LUCENE-8835: Respect file extension when listing files form FileSwitchDirectory","FileSwitchDirectory splits file actions between 2 directories based on file extensions. The extensions are respected on write operations like delete or create but ignored when we list the content of the directories. Until now we only deduplicated the contents on Directory#listAll which can cause inconsistencies and hard to debug errors due to double deletions in IndexWriter is a file is pending delete in one of the directories but still shows up in the directory listing form the other directory. This case can happen if both directories point to the same underlying FS directory which is a common usecase to split between mmap and noifs.  This change filters out files from directories depending on their file extension to make sure files that are deleted in one directory are not returned form another if they point to the same fs directory.","closed","","s1monw","2019-06-06T10:48:34Z","2019-06-11T15:28:06Z"
"","716","LUCENE-8853: Try parsing original file extension from tmp file","FileSwitchDirectory failes if the tmp file are not in the same directory as the file it's renamed to. This is correct behavior but breaks with tmp files used with index sorting. This change tries best effort to find the right extension directory if the file ends with `.tmp`","closed","","s1monw","2019-06-12T06:11:56Z","2019-06-18T06:48:05Z"
"","1593","LUCENE-9409: Truncation can also cause IndexOutOfBoundsException.","Expect `IndexOutOfBoundsException` when opening indices with truncated files.","open","","jpountz","2020-06-18T07:29:57Z","2020-11-16T13:54:57Z"
"","754","LUCENE-8875: Introduce Optimized Collector For Large Number Of Hits","Existing top score docs collectors prepopulate the priority queue with sentinel objects, which can be expensive for large number of hits requested. This commit introduces a new Collector which optimizes the said case","closed","","atris","2019-07-01T11:57:42Z","2019-11-12T03:31:49Z"
"","1369","SOLR-14213: Allow enabling shared store to be scriptable","Enhancing the configuration section to allow shared storage feature to be enabled via system properties vs just checking for presence of  section in solr.xml. ```    ${sharedStoreEnabled:false}  ``` Shared storage is now only enabled if cloud mode is on,  section is present, and the field sharedStoreEnabled is present and set to true. If  is present but the field isn't, startup fails. The field supports system properties overriding whatever default is specified. This can be passed in via the solr binary such as  `bin/solr start -e cloud -noprompt -DsharedStoreEnabled=true`  or set by specifying the environment variable SHARE_STORE_ENABLED.","closed","","andyvuong","2020-03-21T01:52:42Z","2020-03-26T21:08:19Z"
"","947","SOLR-13815: enhance live split test to fail more often","enhancement to the test case to allow more indexing threads to increase the chance of test failure.  On my laptop, this caused the test to fail almost 50% of the time with 4 threads.  Also changed the failure code to speed up checking what docs were missing (avoid a query-per-doc)","closed","","yonik","2019-10-13T20:49:51Z","2019-10-13T21:07:24Z"
"","1594","Replace DWPT.DocState with simple method parameters","DWPT.DocState had some history value but today in a little bit more cleaned up DWPT and IndexingChain there is little to no value in having this class. It also requires explicit cleanup which is not not necessary anymore.","closed","","s1monw","2020-06-18T13:49:31Z","2020-06-18T18:09:24Z"
"","1350","Cleanup DWPT for readability","DWPT had some complicated logic to account for failures etc. This change cleans up this logic and simplifies the document processing loop.","closed","","s1monw","2020-03-13T20:04:32Z","2020-03-17T14:18:43Z"
"","787","LUCENE-8810: Honor MaxClausesCount in BooleanQuery","During Flattening, BooleanQuery will always try to flatten nested clauses during rewrite. However, this can cause the maximum number of clauses to be violated by the new query. This commit disables flattening in the specific case.  Supersedes #784","closed","","atris","2019-07-15T13:22:46Z","2019-07-15T13:38:10Z"
"","704","SOLR-13546: Fix typo 'hightlight' in webapp query interface","Due to a typo in the webapp query interface, the used query string is incorrect.","closed","","dagwieers","2019-06-07T11:49:07Z","2019-06-13T09:19:36Z"
"","1277","SOLR-14278: data loss when leader dies during live split","Draft patch of the simplest test to show an issue (not for committing yet).  See SOLR-14278 for details.","open","","yonik","2020-02-22T23:08:23Z","2020-02-26T17:56:22Z"
"","1107","SOLR-14131: add maxQueryLength option to DirectSolrSpellChecker","DRAFT - not ready for review yet. This PR extends https://github.com/apache/lucene-solr/pull/1103. I'm confused by how the tests work for this class.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2019-12-20T17:58:42Z","2019-12-23T15:52:40Z"
"","1534","LUCENE-9380: Fix auxiliary class warnings in Lucene","DocFreqValueSource looks like it has a lot more changes than it does. All that's really changed is the aux classes were moved to the bottom of the class and made them static.  Will commit on Tuesday barring objections.","closed","","ErickErickson","2020-05-23T22:10:04Z","2020-06-02T13:26:06Z"
"","942","SOLR-13834: ZkController#getSolrCloudManager() now uses the same ZkStateReader","Details in the JIRA. All tests pass. (FYI, without the changes to AddShardCmd and SplitShardCmd, the CollectionsTooManyReplicasTest was failing.)","closed","","chatman","2019-10-11T16:48:42Z","2020-11-10T18:24:14Z"
"","1506","SOLR-14470: Add streaming expressions to /export handler","Details in Jira.  Please note that this branch depends on SOLR-14423 (PR 1486) so it appears that there are many unrelated changes.","closed","","sigram","2020-05-11T17:29:55Z","2020-08-13T15:35:40Z"
"","1568","SOLR-14537 Improve performance of ExportWriter","Details in Jira.  Initial changes here implement the ""double buffering"" approach to increase the throughput - an additional thread is created to fill in a buffer while the main thread writes out the documents from the other buffer.  Lucene TermEnum-s and DocValues are not multi-thread safe, that's why this change required the documents to be fully materialized in the buffer before handing it over to the other thread for writing. I think this is an acceptable tradeoff between reasonable amount of memory in the buffer and speed.","closed","perf,","sigram","2020-06-11T09:50:52Z","2020-08-13T16:10:58Z"
"","1143","SOLR-14328:HdfsDirectory support createTempOutput","Description: See https://issues.apache.org/jira/browse/SOLR-14328 HdfsDirectory support createTempOutput","closed","","kaynewu","2020-01-06T13:06:58Z","2022-02-09T18:04:39Z"
"","1638","SOLR-14066: Deprecate DIH","Deprecating DIH. See SOLR-14066 for details. This is based on @janhoy's previous PR, with minor changes here and there.","closed","","chatman","2020-07-01T23:58:14Z","2021-04-05T21:22:56Z"
"","1089","LUCENE-9097: Decrease the number of shape used in multi-polygon tests","Decrease the number so test run faster and minimise the possibility of OOM errors.","closed","","iverase","2019-12-17T09:27:06Z","2020-01-27T08:57:14Z"
"","794","LUCENE-8769: Introduce Range Query Type With Multiple Ranges","Currently, multiple ranges need to be specified in different PointRangeQueries, thus leading to performance implications when the BKD tree is deep, since each range query will need a traversal.  This commit introduces a new range query type which has multiple ranges logically connected. All ranges are logically connected by OR operators.","closed","","atris","2019-07-18T05:42:49Z","2019-08-19T09:22:53Z"
"","913","LUCENE-8995: TopSuggestDocsCollector#collect should be able to signal rejection","Currently the suggestion collectors collect method has no way of signaling back to the caller whether the matched completion it received was indeed collected. While this is currenty always the case for the default TopSuggestDocsCollector, there are implementations that overwrite this with custom collection logic that also deduplicates based on e.g. docID and/or context. Currently if those completions are rejected, the calling TopNSearcher has no way of noting these rejections and might therefore terminate early, missing on completions that should be returned.","open","","cbuescher","2019-10-01T14:54:58Z","2019-10-16T15:17:31Z"
"","1073","LUCENE-9088: JapaneseNumberFilter uses inaccurate PartOfSpeechAttribute","Currently the JapaneseNumberFilter reads past a single or multiple numeric tokens and emits the new composed token with the attributes of the following token. This will often lead to e.g. wrong part-of-speech attributes on the numeric token, which in turn can lead to wrong filtering by subsequent filters.  This change keeps track of the state of the last numeric token while iterating over a number group and restores the last seen state before emiting the composed numeric token, so we use the attributes of the last one.","open","","cbuescher","2019-12-11T14:44:22Z","2019-12-18T17:15:21Z"
"","733","SOLR-13568: Expand component should not cache group queries in the filter cache","Currently the expand component is creating queries (bit sets) from the current page document ids. These queries are sadly put in the filter cache. This behavior floods the filter cache and it becomes inefficient.  Therefore, the group query should be wrapped in a query with its cache flag disabled.","open","","ludovic-boutros","2019-06-20T20:23:27Z","2019-10-21T13:31:08Z"
"","1243","LUCENE-9212: Intervals.multiterm() should take CompiledAutomaton","Currently it takes `Automaton` and then compiles it internally, but we need to do things like check for binary-vs-unicode status; it should just take `CompiledAutomaton` instead, and put responsibility for determinization, binaryness, etc, on the caller.","closed","","romseygeek","2020-02-07T13:40:07Z","2020-02-24T11:09:44Z"
"","861","SOLR-10665 POC for a PF4J based plugin system","Creating PR for this old issue","closed","","janhoy","2019-09-06T18:11:37Z","2019-11-27T09:36:43Z"
"","675","SOLR-13350: Multi-threaded search through an index","Creating a github pull request for easier review.","closed","","chatman","2019-05-14T09:22:02Z","2020-08-08T06:23:30Z"
"","1426","Do a bit count on 8 bytes from a long directly instead of reading 8 b…","Couldn't resist. Shows a marginal improvement in tight loops. @bruno-roustant ?","closed","","dweiss","2020-04-12T14:11:22Z","2020-04-13T11:37:33Z"
"","1600","SOLR Move ""userfiles"" stuff from SolrPaths to CoreContainer.","Convert String and File paths to Path API.  CoreContainer seems to me the proper place for the node to expose key directories at a node level.  It's already doing this.  It looked a bit clumsy to me to have the String constant and extra methods elsewhere for userfiles.  And I'd rather not see SolrDispatchFilter touched whatsoever for this feature, even if it was one line.  Once I moved that, and declared as type Path, I couldn't un-see all the code that did heavy back & forth conversion between String paths and File paths.  Not only is String terrible because it's untyped, but Path is the modern replacement, not File.  I wish we could do forbiddenAPI on File but there's lots of old code.  This userfiles code was only added last year though.  I experimented with using Files.walkTree in findReadableFiles() but backed off because I already extended my scope very far and I'd rather eventually see all of the Files streamed instead of needing to materialize _all_ of them into a list. Also, findReadableFiles is pretty clear and wasn't any clearer when I played with a Files.walkTree considering all the rules, particularly ignoring files that aren't readable.  I wonder how intentional that is... maybe it actually _should_ be an error instead of silently ignoring those files.  @gerlowskija","closed","","dsmiley","2020-06-22T14:43:36Z","2021-02-07T15:30:00Z"
"","1582","Remove some needless toAbsolutePath calls","Continuation of #1546 Here, we avoid calling toAbsolutePath when not needed because we know it's already absolute.  Look carefully; there's a few other changes.  I liken this to pointless null checks in code when you know the thing isn't null.","closed","","dsmiley","2020-06-15T22:07:21Z","2020-06-21T20:29:08Z"
"","1660","LUCENE-9386 Bug fix for 8x backport","Constructor passed the wrong flag","closed","","markharwood","2020-07-09T14:04:06Z","2020-07-09T14:04:57Z"
"","709","LUCENE-8850: Calculate the area of a polygon and throw error when values are invalid","Compute area of a polygon and throw an error when values are invalid.","open","","iverase","2019-06-11T07:31:51Z","2019-07-01T14:30:46Z"
"","1126","LUCENE-4702: Terms dictionary compression.","Compress blocks of suffixes in order to make the terms dictionary more space-efficient. Two compression algorithms are used depending on which one is more space-efficient:  - LowercaseAsciiCompression, which applies when all bytes are in the    `[0x1F,0x3F)` or `[0x5F,0x7F)` ranges, which notably include all digits,    lowercase ASCII characters, '.', '-' and '_', and encodes 4 chars on 3 bytes.    It is very often applicable on analyzed content and decompresses very quickly    thanks to auto-vectorization support in the JVM.  - LZ4, when the compression ratio is less than 0.75.  I was a bit unhappy with the complexity of the high-compression LZ4 option, so I simplified it in order to only keep the logic that detects duplicate strings. The logic about what to do in case overlapping matches are found, which was responsible for most of the complexity while only yielding tiny benefits, has been removed.","closed","","jpountz","2019-12-26T13:17:36Z","2020-01-24T13:48:26Z"
"","967","SOLR-13857 - fix QueryParser.jj and FastCharStream such that generated","code will compile, and required minor edits that need to be reproduced after regeneration to pass precommit etc are clearly marked.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gus-asf","2019-10-23T01:30:39Z","2020-05-08T13:46:41Z"
"","1443","LUCENE-9337: Ensure CMS updates it's thread accounting datastructures consistently","CMS today releases it's lock after finishing a merge before it re-acquires it to update the thread accounting datastructures. This causes threading issues where concurrently finishing threads fail to pick up pending merges causing potential thread starvation on forceMerge calls.","closed","","s1monw","2020-04-21T21:31:43Z","2020-04-22T12:30:19Z"
"","1326","Remove unused scripts in dev-tools folder","Cleanup of unused scripts. Please validate my assumption that this is not in use :)","closed","","janhoy","2020-03-06T10:10:22Z","2020-03-10T18:49:28Z"
"","1054","SOLR-13813: change test sleeps to latches","Cleans up the remaining sleeps in the shared storage split test and replaces them with countdown latches.","closed","","yonik","2019-12-03T14:24:33Z","2019-12-03T16:43:20Z"
"","890","SOLR-13773: Prometheus Exporter GC and Heap options (#887)","Cherrypick from master (c7f84873280d7d60ebdfff72e0c72fb60cf24e69) * SOLR-13773: Prometheus Exporter GC and Heap options * Adding info to the ref-guide.","closed","","anshumg","2019-09-18T20:37:38Z","2019-09-18T20:40:48Z"
"","901","LUCENE-8984: MoreLikeThis MLT is biased for uncommon fields (#871)","Cherrypick from master","closed","","anshumg","2019-09-25T22:25:06Z","2019-09-25T22:26:01Z"
"","1630","SOLR-14462: cache more than one autoscaling session","Cherry picked from 25428013fb0ed8f8fdbebdef3f1d65dea77129c2","closed","","murblanc","2020-06-29T13:34:48Z","2020-06-29T18:20:38Z"
"","1075","SOLR-14047: Make sure tests don't pickup other Hadoop installs","Checks that `HADOOP_HOME` environment variable isn't set and Java property `hadoop.home.dir` is not set prior to starting Hadoop tests. This will bail out with a nicer error message. It seems better than trying to much with environment variables or system properties.","closed","","risdenk","2019-12-12T03:42:29Z","2019-12-12T15:26:16Z"
"","1216","LUCENE-4702: Reduce terms dictionary compression overhead.","Changes include:  - Removed LZ4 compression of suffix lengths which didn't save much space    anyway.  - For stats, LZ4 was only really used for run-length compression of terms whose    docFreq is 1. This has been replaced by explicit run-length compression.  - Since we only use LZ4 for suffix bytes if the compression ration is < 75%, we    now only try LZ4 out if the average suffix length is greater than 6, in order    to reduce index-time overhead.","closed","","jpountz","2020-01-27T17:29:13Z","2020-01-28T17:38:34Z"
"","1612","SOLR-11390 Trie* field javadocs to @see *Point","cc: @cpoerschke I can't tag you as a reviewer for some reason, but this is based on your patch and I added two more changes - fixed spelling and removed back link from DatePoint to TrieDate.","closed","","madrob","2020-06-24T21:32:30Z","2020-07-09T17:20:57Z"
"","1252","LUCENE-9218: XYGeometries should expose values as floats","Boxing the values to doubles happen when creating component2D objects.","closed","","iverase","2020-02-12T08:33:10Z","2020-02-14T10:40:58Z"
"","784","LUCENE-8810: Honor MaxClausesCount in BooleanQuery During Flattening","BooleanQuery will always try to flatten nested clauses during rewrite. However, this can cause the maximum number of clauses to be violated by the new query. This commit disables flattening in the specific case.","closed","","atris","2019-07-15T10:19:59Z","2019-07-15T13:23:43Z"
"","1086","SOLR-14072: Deprecate Blob API and runtimeLib","Bare minimum.","closed","","dsmiley","2019-12-15T20:56:33Z","2020-03-26T13:20:38Z"
"","1659","LUCENE-9386 add case insensitive RegExp matching option","Backport of 887fe4c83d4114c6238265ca7f05aa491525af9d","closed","","markharwood","2020-07-08T19:03:28Z","2020-07-09T13:23:04Z"
"","1494","Backport of FuzzyQuery bugfix for issue 9365","Backport of 28e47549c8ba1a7c17ffe7d9e791e88983ef46c2 Fixes false negative in FuzzyQuery when prefix length == search string length.","closed","bug,","markharwood","2020-05-07T14:36:39Z","2020-05-07T15:12:42Z"
"","743","[Backport ]LUCENE-8868: New storing strategy for BKD tree leaves with low cardinality","Backport of #730","closed","","iverase","2019-06-26T08:48:17Z","2019-06-26T09:19:37Z"
"","1610","LUCENE-9384: Backport for field sort optimization","Backport for: LUCENE-9280: Collectors to skip noncompetitive documents (#1351)  Similar how scorers can update their iterators to skip non-competitive documents, collectors and comparators should also provide and update iterators that allow them to skip non-competive documents.  To enable sort optimization for numeric sort fields, the following needs to be done: 1) the field should be indexed with both doc_values and points, that must have the same field name and same data 2) SortField#setCanSkipNonCompetitiveDocs must be set 3) totalHitsThreshold should not be set to max value.","closed","","mayya-sharipova","2020-06-24T15:37:54Z","2020-07-31T18:13:04Z"
"","682","SOLR-13484 autoscaling/diagnostics APIshould be able to give diagnostics output from config pasted as a payload","autoscaling/diagnostics APIshould be able to give diagnostics output from config pasted as a payload","closed","","noblepaul","2019-05-22T06:23:18Z","2019-05-23T06:59:58Z"
"","1390","LUCENE-9266 remove gradle wrapper jar from source","ASF Release Policy states that we cannot have binary JAR files checked in to our source releases, a few other projects have solved this by modifying their generated gradlew scripts to download a copy of the wrapper jar.  This implementation is heavily based on the one found in KAFKA-1714","closed","","madrob","2020-03-30T18:56:31Z","2020-04-02T16:41:45Z"
"","687","LUCENE-8815: Adds a DoubleValues implementation for feature fields","As described in https://issues.apache.org/jira/browse/LUCENE-8815","closed","","colings86","2019-05-28T12:37:49Z","2019-06-10T08:31:54Z"
"","680","LUCENE-8803: Provide a FieldComparator to allow sorting by a feature from a FeatureField","As described in https://issues.apache.org/jira/browse/LUCENE-8803","closed","","colings86","2019-05-17T11:16:45Z","2019-05-24T07:43:32Z"
"","667","LUCENE-8796: Use exponential search in IntArrayDocIdSetIterator#advance","As described in https://issues.apache.org/jira/browse/LUCENE-8796 .","closed","","javanna","2019-05-08T12:17:31Z","2019-06-18T08:29:52Z"
"","1416","LUCENE-9286: FST.Arc.BitTable is read directly from the FST bytes.","Arc is lightweight again and Arc.copyFrom() is light. Faster FSTEnum traversal by keeping the count of bits set in a new FST.Arc field.","closed","","bruno-roustant","2020-04-07T21:40:16Z","2020-08-31T06:44:23Z"
"","1652","SOLR-14639 update the name of the deletegation method to be correct","Applying fix from Andras Salamon.  Thanks!","closed","","epugh","2020-07-06T10:16:38Z","2020-08-13T14:51:52Z"
"","779","LUCENE-8762: Introduce Specialized Impacts For Doc + Freq","ant test for both Lucene and Solr pass. ant precommit passes as well.  I did not add new tests since multiple existing tests exercise the new functionality","closed","","atris","2019-07-12T06:33:39Z","2019-07-17T17:45:00Z"
"","869","ant precommit GitHub action for custom branch","ant precommit GitHub action for custom branch (anshum/test-github-precommit-action)","closed","","anshumg","2019-09-10T19:06:38Z","2019-09-10T19:39:03Z"
"","878","LUCENE-8746: Refactor EdgeTree","Another try in refactoring edge tree. This PR splits Edge Tree class into two and adds a new interface:  * Component2D: Interface defining an object that knows its bounding box and can perform some spatial operations. * ComponentTree: An interval tree containing the different components (e.g polygon or line)  * EdgeTree: An interval tree containing the edges of a components (polygon or line edges)  Unfortunately the PR touches quite a lots of files but most of them are test files. Running benchmark for points results look good, points shows same performance and there is an increase of performance for shapes (we are not computing the bounding box of the triangle many times).    |Approach|Shape|M hits/sec Dev|M hits/sec Base|Diff| QPS  Dev| QPS Base|Diff|Hit count Dev|Hit count Base|Diff| |--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |  |points|polyRussia|13.97|13.98|-0%|3.98|3.99|-0%|3508846|3508846| 0%| |points|poly 10|73.35|71.84| 2%|46.39|45.43| 2%|355809475|355809475| 0%| |points|polyMedium|8.73|8.67| 1%|106.99|106.24| 1%|2693559|2693559| 0%| |shapes|polyRussia|6.88|5.73|20%|1.96|1.63|20%|3508846|3508846| 0%| |shapes|poly 10|27.98|27.03| 4%|17.69|17.10| 4%|355809475|355809475| 0%| |shapes|polyMedium|2.79|2.75| 2%|34.19|33.66| 2%|2693559|2693559| 0%|","closed","","iverase","2019-09-13T14:57:50Z","2019-10-14T17:36:06Z"
"","1205","SOLR-14206: Annotate HttpSolrCall as thread-safe","Annotate HttpSolrCall and V2HttpCall as thread-safe","closed","","anshumg","2020-01-23T16:41:57Z","2020-02-03T18:00:44Z"
"","1160","LUCENE-9125: Improve Automaton.step() with binary search","and introduce Automaton.next().","closed","","bruno-roustant","2020-01-10T14:25:28Z","2020-01-13T10:05:28Z"
"","1407","Use QueryUtils.combineQueryAndFilter more","and check query==MatchAllDocsQuery (minor & cheap optimization)  I more thoroughly examined opportunities to use this new function and found them all.  One or two in some spatial code I left for various reasons (e.g. deprecated code).  Note that BooleanQuery.rewrite has an optimization that looks for a MatchAllDocsQuery paired with filters, thus this check here would ultimately not change the fully rewritten query that gets executed.  But, it's such a cheap/easy check and BooleanQuery is a rather heavy wrapper (it's rewrite is extensive).     CC @cpoerschke","closed","","dsmiley","2020-04-05T15:32:55Z","2020-04-14T20:46:31Z"
"","1105","LUCENE-9105: UniformSplit postings format detects corrupted index","and better handles IO exceptions.  To correctly handle IOException, we do not use Supplier anymore, it is replaced by IndexDictionary.BrowserSupplier which throws (propagates) IOException. This leads to some disrupting refactoring: method signature change, and one class IndexDictionaryBrowserSupplier moved as an inner class FSTDictionary.BrowserSupplier.  I think it does not prevent us to push this change to 8x as I don't think UniformSplit is already extended by the community (was released recently in 8.3).","closed","","bruno-roustant","2019-12-20T10:03:29Z","2020-08-31T06:57:45Z"
"","676","LUCENE-8753: SharedTerms UniformSplit","An extension of UniformSplit to share the terms between fields. The fields share the same FST dictionary in the block file. Each term occur only once in the block file, and the different term states for multiple fields are stored on each term line in the block file. So there is a single FST for multiple fields, which is generally much more compact when there a lot of fields.","closed","","bruno-roustant","2019-05-14T13:12:27Z","2019-12-02T10:25:09Z"
"","772","Payload-stored position length intervals","An experimental token filter and intervals source that use payloads to encode token lengths","open","","romseygeek","2019-07-10T14:00:50Z","2019-07-15T18:59:32Z"
"","1530","SOLR-14690: QueryResultKey.[nc_]minExactCount","Allow with-minExactCount searches to use query cache entries from without-minExactCount searches.  https://issues.apache.org/jira/browse/SOLR-14690","closed","","cpoerschke","2020-05-22T20:47:37Z","2021-03-12T18:15:24Z"
"","912","Make testLUCENE8736 Check For Async Caching","Allow testLUCENE8736 to wait for async caching query loads","closed","","atris","2019-10-01T12:03:20Z","2019-10-01T13:13:06Z"
"","1158","LUCENE-9116: Remove long[] from `PostingsWriterBase#encodeTerm`.","All the metadata can be directly encoded in the `DataOutput`.","closed","","jpountz","2020-01-10T08:24:13Z","2020-01-17T12:39:50Z"
"","1149","LUCENE-9116: Remove long[] from `PostingsWriterBase#encodeTerm`.","All the metadata can be directly encoded in the `DataOutput`.","closed","","jpountz","2020-01-06T18:14:05Z","2020-01-09T14:16:37Z"
"","1080","SOLR-14077: Hadoop shouldn't need to look for metrics config in user home","All tests look good on Mac.","closed","","risdenk","2019-12-13T21:40:27Z","2019-12-14T03:10:13Z"
"","1427","LUCENE-9304: Fix IW#getMaxCompletedSequenceNumber()","After recent refactorings on LUCENE-9304 `IW#getMaxCompletedSequenceNumber()` might return values that belong to non-completed operations if a full flush is running, a new delete queue is already in place but not all DWPTs that participate in the full flush have finished it's in-flight operation. This caused rare failures in `TestControlledRealTimeReopenThread#testControlledRealTimeReopenThread` were documents are not actually visible given the max completed seqNo. This change streamlines the delete queue advance, adds a dedicated testcase and ensures that a delete queues sequence Id space is never exhausted.","closed","","s1monw","2020-04-12T19:31:43Z","2020-04-14T17:39:27Z"
"","855","SOLR-13739: Improve performance on huge schema updates","Affects schemas containing ResourceLoaderAware components, increases creation of a new collection with a huge schema by a factor 60 to 100.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-09-04T10:40:53Z","2019-09-14T08:02:54Z"
"","989","SOLR-12457: improve compatibility/support for sort by field-function","Affects marshal/unmarshal of sort values for field-function sorts, native numeric wrapper for multivalued Trie field docValues, and missingValue (sortMissingFirst/sortMissingLast) wrt field-function sort.","closed","","magibney","2019-10-31T17:26:07Z","2022-04-29T17:58:45Z"
"","1657","SOLR-14462: adjust test so less sessions are used even if test runs s…","adjust test so less sessions are used even if test runs slowly. fix synchronization issue.  cherry picked from 06b1f3e86694b35365fd569a0581b1f6fc2cadb3","closed","","murblanc","2020-07-07T15:29:44Z","2020-07-07T15:30:26Z"
"","1656","SOLR-14462: test fix","adjust test so less sessions are used even if test runs slowly. fix synchronization issue.","closed","","murblanc","2020-07-07T15:10:54Z","2020-07-07T15:13:31Z"
"","1279","LUCENE-9244: In 2D, a point can be shared by four leaves","Adjust test not to fail when point is shared by multiple leaves.","closed","","iverase","2020-02-24T08:01:10Z","2020-04-07T08:49:22Z"
"","762","LUCENE-8903: Add LatLonShape point query","Adds a query to LatLonShape that filters by a provided point.","closed","","iverase","2019-07-04T10:25:04Z","2020-02-07T20:20:21Z"
"","851","LUCENE-8960: Add LatLonDocValuesPointInPolygonQuery","Adds a new query that iterates over LatLonPoint docValues and test if the point is within a provided polygon. In addition, `LatLonPointInPolygonQuery` is rewritten so it can be used efficiently with the new added query using `IndexOrDocValuesQuery`.","closed","","iverase","2019-09-02T14:49:18Z","2019-09-03T08:14:00Z"
"","1619","SOLR-10814 Add short-name feature to RuleBasedAuthz plugin","Additional-Author: Hrishikesh Gadre","closed","","madrob","2020-06-26T19:40:14Z","2020-07-08T15:26:20Z"
"","1498","SOLR-14351: commitScheduler was missing MDC logging","Adding this to https://issues.apache.org/jira/browse/SOLR-14351 even though it's not a perfect fit.  As an aside, I've seen MDC missing logs from searcher warming QuerySenderListener.  There's another JIRA issue for that one indirectly fixed via stacking SolrRequestInfo.","closed","","dsmiley","2020-05-08T05:14:56Z","2020-05-14T00:58:29Z"
"","994","SOLR-13662: Package Manager (CLI)","Adding CLI support for package manager. PackageManagerCLITest is added for test of common operations.  Introduces new dependency on semver library.  Introduces new ZK file, /repositories.json.","closed","","chatman","2019-11-04T15:11:37Z","2020-11-10T22:31:26Z"
"","1078","SOLR-14071: Untrusted configsets shouldn't be allowed to use","Added a test for this.","closed","","chatman","2019-12-12T08:12:24Z","2020-11-10T22:41:26Z"
"","1053","SOLR-13998: Add thread safety annotations to classes","Add thread safety annotations to classes.   The current commit only introduces the annotations, but the actual annotations will take some time but are wip. We can then use these to add information to existing classes, as well as new classes.","closed","","anshumg","2019-12-03T10:24:17Z","2019-12-03T19:49:56Z"
"","714","SOLR-13537: README Build Status","add table for the build statuses on each platform for more accessible project pulse.","closed","","MarcusSorealheis","2019-06-11T23:37:57Z","2019-06-29T12:01:55Z"
"","1331","LUCENE-9268: Add some random tests to IndexWriter","Add some tests that perform a set of operations randomly and concurrently on IndexWriter.","closed","","dnhatn","2020-03-07T18:33:25Z","2020-03-09T03:07:38Z"
"","1623","LUCENE-8962: Merge segments on getReader","Add IndexWriter merge-on-refresh feature to selectively merge small segments on getReader, subject to a configurable timeout, to improve search performance by reducing the number of small segments for searching.","closed","","s1monw","2020-06-27T22:11:26Z","2020-10-20T01:51:05Z"
"","1272","LUCENE-9238: Add new XYPointField, queries and sorting capabilities","Adaptation of LatLonPoint to generic cartesian space.","closed","","iverase","2020-02-21T07:10:03Z","2020-02-21T10:34:43Z"
"","1536","SOLR-14511: Documented node.sysprop shard preference","Actual functionality implemented here: https://issues.apache.org/jira/browse/SOLR-13445  This PR just adds docs to it in two places: CLUSTERPROP documentation (for cluster defaults) and shard preferences (for per-request preferences).","closed","","radu-gheorghe","2020-05-25T12:43:24Z","2020-05-28T16:41:45Z"
"","1050","SOLR-13990: Switch out woodstox-core-asl with aalto-xml and upgrade woodstox stax-2 API","aalto-xml is an ultra-high performance next generation Stax XML processor implementation, implementing both basic Stax API (javax.xml.stream) and Stax2 API extension (org.codehaus.woodstox.stax2). In addition, it also implements SAX2 API.","closed","","anshumg","2019-12-02T10:55:59Z","2019-12-05T13:07:54Z"
"","889","LUCENE-8983: Add PhraseWildcardQuery to control multi-terms expansions in a phrase","A generalized version of PhraseQuery, built with one or more MultiTermQuery that provides term expansions for multi-terms (one of the expanded terms must match). Its main advantage is to control the total number of expansions across all MultiTermQuery and across all segments.  This query is similar to MultiPhraseQuery, but it handles, controls and optimizes the multi-term expansions.   This query is equivalent to building an ordered SpanNearQuery with a list of SpanTermQuery and SpanMultiTermQueryWrapper. But it optimizes the multi-term expansions and the segment accesses. It first resolves the single-terms to early stop if some does not match. Then it expands each multi-term sequentially, stopping immediately if one does not match. It detects the segments that do not match to skip them for the next expansions. This often avoid expanding the other multi-terms on some or even all segments. And finally it controls the total number of expansions.","closed","","bruno-roustant","2019-09-18T16:05:16Z","2019-12-02T10:20:39Z"
"","746","LUCENE-8885: Optimise BKD reader by exploiting cardinality information stored on leaves","A `DocIdSetIterator` is added to the BKD reader and use when we have cardinality information.","closed","","iverase","2019-06-27T06:11:41Z","2019-07-01T06:37:04Z"
"","684","LUCENE-8809: Ensure release segment states","A [failed test](https://github.com/elastic/elasticsearch/issues/30290) from Elasticsearch shows that refresh and rollback concurrently can leave segment states unreleased leads to leaking refCount of some SegmentReaders.  Relates https://github.com/elastic/elasticsearch/issues/30290","closed","","dnhatn","2019-05-23T12:28:08Z","2019-05-23T16:16:41Z"
"","1213","LUCENE-8143: Remove SpanBoostQuery","`SpanBoostQuery` is essentially a null op, as boosts on nested span queries are ignored by the scorer, unless it is used to wrap a query at the top level.  In this  case it can simply be replaced by a normal `BoostQuery`.  It is confusing to have a query that adds essentially nothing, and only works under certain circumstances.  This commit removes `SpanBoostQuery` entirely.","closed","","romseygeek","2020-01-27T10:02:28Z","2021-09-15T11:21:25Z"
"","677","SOLR-13257: support for stable replica routing preferences","`shards.preferences` parameter allows users to assert replica preferences based on inherent replica attributes. This PR integrates support for user-specified ordering behavior applied to replicas that are equivalent with respect to preferred inherent attributes.   The extant implementation shuffles replicas randomly per-request. Propose to leave shuffling as the default, but add support for stable replica routing preferences (list rotation by dividend|hash % number equivalent replicas)","closed","","magibney","2019-05-14T17:25:04Z","2019-08-21T18:37:46Z"
"","1295","Lucene-9004: bug fix for searching the nearest one neighbor in higher layers","`if (dist < f.distance() || results.size() < ef) {    Neighbor n = new ImmutableNeighbor(e.docId(), dist);    candidates.add(n);    results.insertWithOverflow(n);    f = results.top(); }`   If (dist < f.distance()) but results.size() >= ef, the ""Neighbor n"" would be added to ""results"" (""results"" is a sub-type of PriorityQueue). The actual size of ""results"" would be between ""ef"" and results' max queue size, while its expected size is ""ef"". Moreover, this bug could have negative effect on performance.  Consider the following situation in [HNSWGraphReader](https://github.com/apache/lucene-solr/blob/jira/lucene-9004-aknn-2/lucene/core/src/java/org/apache/lucene/util/hnsw/HNSWGraphReader.java):  `FurthestNeighbors neighbors = new FurthestNeighbors(ef, ep);   for (int l = hnsw.topLevel(); l > 0; l--) {     visitedCount += hnsw.searchLayer(query, neighbors, 1, l, vectorValues);   }   visitedCount += hnsw.searchLayer(query, neighbors, ef, 0, vectorValues);`  where the max size of ""neighbors"" (""neighbors"" is also a sub-type of PriorityQueue) is ef (assume ef > 1. Note that the meaning of ""ef"" is different in the above two code segments). When search over a non-zero layer, we are going to find the nearest one neighbor by `hnsw.searchLayer(query, neighbors, 1, l, vectorValues);`, where l is the layer and layer > 0. The actual size of ""neighbors"" may be larger than 1.  Assume that ""results.size() <= ef"" (always meet in HNSW), I think ""results.pop();"" when ""results.size() == ef"" can solve this problem.","closed","","irvingzhang","2020-02-27T12:09:11Z","2020-03-01T13:17:46Z"
"","961","[LUCENE-9019] Patch for making some variable names more consistent with the other parts of the code.","[LUCENE-9019] Patch for making some variable names more consistent with the other parts of the code.  Hello, we're developing an automated system that detects inconsistent variable names in a large software project. Our system checks if each variable name is consistent with other variables in the project in its usage pattern, and proposes correct candidates if inconsistency is detected. This is a part of academic research that we hope to publish soon, but as a part of the evaluation, we applied our systems to your projects and got a few interesting results. We carefully reviewed our system output and manually created a patch to correct a few variable names. We would be delighted if this patch is found to be useful. If you have a question or suggestion regarding this patch, we'd happily answer. Thank you.  P.S. our patch is purely for readability purposes and does not change any functionality. A couple of issues that we've noticed were left untouched. For example, mixed use of variable names ""len"" and ""length"" were fairly widespread, but we have only corrected a few notable instances to minimize our impact.","open","","euske","2019-10-21T08:09:11Z","2019-10-21T08:09:11Z"
"","1182","LUCENE-9149: Increase data dimension limit in BKD","[LUCENE-8496](https://issues.apache.org/jira/browse/LUCENE-8496) added selective indexing; the ability to designate the first K <= N dimensions for driving the construction of the BKD internal nodes. Follow on work stored the ""data dimensions"" for only the leaf nodes and only the ""index dimensions"" are stored for the internal nodes.  This PR increases the `MAX_DIMENSIONS` limit (to something not too crazy like 16; effectively doubling the index dimension limit) while maintaining the `MAX_INDEX_DIMENSIONS` at 8, since `maxPointsInLeafNode` is still important for managing the BKD heap memory footprint (thus we don't want this to get too large).  This improvement will enable us to encode higher dimension data within a lower dimension index (e.g., 3D tessellated triangles as a 10 dimension point using only the first 6 dimensions for index construction)","closed","","nknize","2020-01-17T21:22:07Z","2020-02-07T22:09:23Z"
"","1624","use MethodHandles in AnnotatedAPI","@uschindler Please review","closed","packages,","noblepaul","2020-06-28T01:07:40Z","2020-06-30T14:39:43Z"
"","950","LUCENE-8920: Disable direct addressing of arcs.","@msokolov Would you mind having a look? Reverts weren't clean so I ended up manually undoing the change by removing the special flag that you introduced for ""array with gaps"". I'd appreciate if you could take a look to make sure I didn't get anything wrong.","closed","","jpountz","2019-10-14T12:20:05Z","2019-10-14T16:30:44Z"
"","821","SOLR-13674: Add relica type property to NodeAddedTrigger","* SOLR-13674:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The auto-scaling nodes are always added as NRT as there is no way to specify desired replica type in the trigger. For the TLOG/PULL cluster that does not work.  # Solution  Added replica type property to the NodeAddedTrigger to be used with preferredOperation 'ADDREPLICA'. Set the resolved replica type as suggester hint so the AddReplicaSuggester can make use of it, similarly to the PolicyHelper.  I also updated branch_7x and branch_8x in my repository but not sure as for the process to merge those.  # Tests  Added test that when policy allows for PULL nodes, node_add_trigger configured with replica type PULL adds the expected replica type.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ishaigor","2019-08-03T03:37:18Z","2019-08-08T09:49:02Z"
"","840","SOLR-13524 : Or Stream Evaluator Is Incorrect","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->  # Description  The implementation of RecursiveBooleanEvaluator(parent of OrEvaluator) is  ```java     for(int idx = 1; idx < values.length; ++idx){       if(!checker.test(values[idx - 1], values[idx])){         return false;       }     }     return true; ```  Obviously, it's incorrect for Or Expression  # Solution  Override the doWork function of OrEvaluator.   # Tests  StreamExpressionTest.testOr testOr2  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","chenkovsky","2019-08-21T10:44:10Z","2019-09-22T05:32:39Z"
"","833","SOLR-12069: Default operator parameter q.op ignored.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->  # Description  BooleanClause.Occur is hardcoded, it's incorrect.  # Solution  Just change one line of code. take BooleanClause.Occur based on whether operator == AND_OPERATOR is true.  # Tests  TestSolrQueryParser. TestAnd  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","chenkovsky","2019-08-15T09:10:41Z","2019-09-22T05:35:38Z"
"","940","LUCENE-9002: Query caching leads to absurdly slow queries","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description We have dozens of ES clusters(based on Lucene) for metric scenarios. Most of the queries are like this: host_ip:10.10.10.10 AND timestamp:[2019-10-01 00:00:00 TO 2019-10-05 23:59:59]. And we frequently encounter some absurdly slow queries.  # Solution For a long time range query(e.g. 5 days), each range query will consume tens of megabytes of memory and spend hundreds of milliseconds to cache, but the benefits are not obvious. And those large cache entries will cause frequent cache eviction. So it's better to skip the caching action directly when large range query appears with a selective lead iterator.  Related to [LUCENE-9002](https://issues.apache.org/jira/projects/LUCENE/issues/LUCENE-9002)","closed","","jgq2008303393","2019-10-11T03:24:51Z","2019-11-11T21:07:34Z"
"","773","SOLR-13619: Kerberos 403 when node doesn't host collection","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description Kerberos: 403 when node doesn't host collection  # Solution Forwarded requests with Kerberos should carry forward the original user principal.","closed","","chatman","2019-07-11T00:47:29Z","2020-11-10T22:30:30Z"
"","884","LUCENE-8980: optimise SegmentTermsEnum.seekExact performance","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description In Elasticsearch, which is based on Lucene, each document has an _id field that uniquely identifies it, which is indexed so that documents can be looked up from Lucene. When users write Elasticsearch with self-generated _id values, even if the conflict rate is very low, Elasticsearch has to check _id uniqueness through Lucene API for each document, which result in poor write performance.   # Solution 1. Choose a better _id generator before writing ES Different _id formats have a great impact on write performance. We have verified this in production cluster. Users can refer to the following blog and choose a better _id generator. http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html 2. Optimise with min/maxTerm metrics in Lucene As Lucene stores min/maxTerm metrics for each segment and field, we can use those metrics to optimise performance of Lucene look up API. When calling SegmentTermsEnum.seekExact() to lookup an term in one segment, we can check whether the term fall in the range of minTerm and maxTerm, so that wo skip some useless segments as soon as possible.   # Tests I have made some write benchmark using _id in UUID V1 format, and the benchmark result is as follows:  | Branch      | Write speed after 4h  | CPU cost | Overall improvement | Write speed after 8h  | CPU cost | Overall improvement |  | ---------- | :-----------:  | :-----------: | :-----------:  | :-----------: | :-----------:  | :-----------: | | Original Lucene | 29.9w/s | 68.4% | N/A | 26.7w/s | 66.6% | N/A | | Optimised Lucene | 34.5w/s(+15.4%) | 63.8(-6.7%) | +22.1% | 31.5w/s(18.0%) | 61.5(-7.7%) | +25.7% |  As shown above, after 8 hours of continuous writing, write speed improves by 18.0%, CPU cost decreases by 7.7%, and overall performance improves by 25.7%. The Elasticsearch GET API and ids query would get similar performance improvements.  It should be noted that the benchmark test needs to be run several hours continuously, because the performance improvements is not obvious when the data is completely cached or the number of segments is too small.","closed","","jgq2008303393","2019-09-16T10:39:30Z","2019-12-04T08:08:29Z"
"","949","SOLR-13827: fail on unknown operation in request params API","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description Fail on any unknown operation and return a proper error message  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","munendrasn","2019-10-14T05:03:30Z","2019-10-18T16:02:18Z"
"","1056","SOLR-13999: Added missing parameter under 'topic parameters'","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description Added missing parameter in solr-ref-guide for stream-source-reference under 'topic parameters'.  # Solution Fix documentation. # Tests  Not applicable  # Checklist  Please review the following and check all that apply:  - [*] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [*] I have created a Jira issue and added the issue ID to my pull request title. - [*] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [*] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [*] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [*] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","cheynove","2019-12-03T16:52:54Z","2019-12-10T21:12:49Z"
"","781","updated the pull request template to make checkboxes work","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Updated the Pull Request template to render checkboxes work.  # Solution  Add space in between brackets.  # Tests  Previewed the changes rendered.  # Checklist  Please review the following and check all that apply:  - [] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [] I have created a Jira issue and added the issue ID to my pull request title. - [] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [] I have developed this patch against the `master` branch. - [] I have run `ant precommit` and the appropriate test suite. - [] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","MarcusSorealheis","2019-07-12T20:39:12Z","2019-07-17T16:24:01Z"
"","790","SOLR-13640 : Fix documentation links to build project on IDE in README.md","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Updated links of documentation for running the project in the IDEs. The previous links are outdated and result in 404.   # Solution  The new links are from the confluence and have been tested to be working.    # Tests  Tested the links to be working and having the required information.    # Checklist  Please review the following and check all that apply:  - [] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [] I have created a Jira issue and added the issue ID to my pull request title. - [] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [] I have developed this patch against the `master` branch. - [] I have run `ant precommit` and the appropriate test suite. - [] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","princemanohar","2019-07-16T18:13:11Z","2019-07-23T16:09:30Z"
"","864","SOLR-13101 : Shared storage support in SolrCloud","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This PR is being opened to expose the code for integrating SolrCloud with a shared blobstore. **This is a work in progress** - we are hoping to open this up to community discussion and gather feedback.  I'm making this PR on behalf of Andy Vuong (@andyvuong), Ilan Ginzburg, Prabhdeep Singh Gill (@PrabhdeepsGill), Megha Siddavanahalli, Bilal Waheed, Chock Viswanathan, and others.  # Solution  ## Solr + Blobstore ### Overview This repo introduces a new framework which allows SolrCloud to integrate with an external (typically cloud-based) blobstore. Instead of maintaining a copy of the index on each Solr host, replicating updates to peers, and using a transaction log to maintain consistent ordered updates, Solr hosts will push and pull cores to/from this external store.  **TL;DR**: For now, SolrCloud can be configured to use blobstore at a collection level. Collections backed by blobstore use a new SHARED replica type. When a Solr node makes an update request to a shared shard, it indexes locally and then pushes the change through to a shared blobstore. Zookeeper manages index versioning and provides a source of truth in the case of concurrent writes. Solr nodes in a cluster will no longer use peer-to-peer replication, and instead will pull updates directly from the shared blobstore.  Please note that this project is a work in progress, and is by no means production-ready. This code is being published early get feedback, which we will incorporate in future work.  In order to modularize these changes and maintain existing functionality, most of the blobstore-related code is isolated to the solr/core/src/java/org/apache/solr/store/blob directory. However, there some key integration touchpoints in HttpSolrCall#init, DistributedZkUpdateProcessor, and CoreContainer#load. These classes all have special handling for blobstore-based shards.  ### Pulling from Blobstore Core pulls are, for the most part, asynchronous. When a replica is queried, it enqueues a pull from blobstore but doesn’t wait for the pull to complete before it executes the query, unless the replica is missing a copy of that core altogether. If your operation requires that local cores are in-sync with blobstore, use the method BlobStoreUtils#syncLocalCoreWithSharedStore.  A more in-depth walkthrough of the pull code: - BlobCoreSyncer: manages threads that sync between local and blob store, so that if a pull is in progress, we do not create duplicate work. - Calls into CorePullTracker: creates PullCoreInfo object containing data about the core to be pulled and adds to a deduplicated list. - This queue of pull objects is polled by the CorePullerFeeder, which uses threads from its dedicated thread pool to execute CorePullTasks. - CorePullTask: checks if a pull is already underway for this core; if not, executes a pull from blob store. Resolves differences between blob’s version of the core and local version, and stores the updated core  ### Pushing to Blobstore This happens synchronously. On every local commit, we push to blobstore and only ack that the update was successful when it is committed both locally and in the shared store.  A more in-depth walkthrough of the push code: - DistributedZkUpdateProcessor: once a commit is complete for a SHARED replica (onFinish), we writeToShareStore. - This calls into CoreUpdateTracker, which creates a PushPullData object containing data about the collection, core, and most recently pulled version of the core on this replica. - CorePusher: resolves the differences between blob’s version of the core and local version, and pushes the updated version to blob store  ### Resolving Local and Blobstore The SharedStoreResolutionUtil handles resolving diffs between the Solr node’s local copy of a core and the copy in blobstore. It does so by pulling the metadata for the core from blobstore (BlobCoreMetadata), comparing against the local metadata (ServerSideMetadata), and creating a list of segments to push or pull.  ### Version Management Only the leader node can push updates to blobstore. Because a new leader can be elected at any time, there is still a possibility for race conditions on writes to blobstore. In order to maintain a consistent global view of the latest version of a core, we keep version data in Zookeeper.  Zookeeper stores this version data as a random string called metadataSuffix. When a SolrCloud node makes an update request, it first pushes the files to blobstore and then makes a conditional update to the metadataSuffix variable. If Zookeeper rejects the conditional update, the update request fails, and the failure is propagated back to the client.  This communication with Zookeeper is coordinated in the SharedShardMetadataController. The SharedShardMetadataController belongs to the Overseer (i.e. the leader replica).  ### Try it yourself If you want to try this out locally, you can start up SolrCloud with the given blobstore code. The code will default to using the local blobstore client, with /tmp/BlobStoreLocal as the blobstore directory (see LocalStorageClient). You can create a shared collection through the Solr admin UI by setting “shared store based” to true.  Note: if you want to try testing with the S3StorageClient, you need to store a valid S3 bucket name and credentials as environment variables (see S3StorageClient#AmazonS3Configs).  # Tests  Find tests in the test directory (`solr/core/src/test/org/apache/solr/store/blob/`).  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","megancarey","2019-09-09T20:41:01Z","2019-09-30T17:29:24Z"
"","959","SOLR-13677 fix & cleanup","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This PR is based on Noble's patch plus cleanups & refactoring identified in the Jira review.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests `MetricsHandlerTest.testMetricsUnload()` is probably insufficient to test all corner cases. We still need to add tests for hierarchical metric tags and back-compat of the API.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","sigram","2019-10-17T07:11:10Z","2020-08-13T15:59:10Z"
"","906","LUCENE-8996: maxScore is sometimes missing from distributed responses","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This is an issue in Lucene that affects Solr Distributed Grouped responses: `TopGroupsShardResponseProcessor` uses `TopGroups.merge` to merge together the grouped results of different shard. If a shard didn't have results for a particular group, it will return an empty group with `maxScore == Float.NaN`, `merge` just performs `Math.max` that is going to return `Float.NaN` if one of the two float is `Float.NaN`.   The issue was open in 2017 by Julien Massenet, I experienced it while working on https://github.com/apache/lucene-solr/pull/300 so I updated his solution to the upstream version.   # Solution  The PR adds a function: ``` private static float updateMaxScore(float currentMaxScore, float newMaxScore) ``` that will implement a max that works with `Float.NaN` (same   # Tests  A unit test covering all the different combinations of groups that could be processed by `merge` is provided.   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","diegoceccarelli","2019-09-28T17:22:35Z","2021-12-08T16:38:43Z"
"","850","SOLR-13727: Bug fix for V2Request handling in HttpSolrClient","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This change addresses a bug that was identified in the HttpSolrClients. Because the createMethod method just uses string.replace, it's possible that the wrong portion of the host address string is replaced. The intention is to change the start of the path from ""/solr"" to ""/api"", but if the hostname begins with ""solr"", it replaces the beginning of the hostname.   For example, if the address is `http://solr-host.com:8983/solr/`, instead of changing to `http://solr-host.com:8983/api/` it will become `http:/api-host.com:8983/solr/`.  # Solution  My solution is to create a java.net URL object from the address string, replace the beginning of the path as expected, create a new URL object, and return the string value. This also ensures that the URL is properly formatted.  # Tests  There are no additional tests, but I have verified when running solr locally that this change fixes the bug.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","megancarey","2019-08-30T23:41:46Z","2019-09-05T02:22:45Z"
"","1046","SOLR-13981: Remove unused DistributedQueue interface","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The solr/solrj/src/java/org/apache/solr/common/cloud/DistributedQueue.java interface is not used anywhere, we can remove this interface.  # Solution  Remove this interface.  # Tests  Unit tests  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","asalamon74","2019-11-29T14:46:25Z","2019-12-06T20:49:51Z"
"","933","SOLR-13209 NullPointerException from call in org.apache.solr.search.SolrIndexSearcher.getDocSet","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The root of this issue is the absence of error handling when a null query is passed to SolrIndexSearcher or Grouping$CommandQuery.  # Solution In the first case, this is solved by returning DocSet.EMPTY before any calls are made on the null reference (absQ).  In the second case, this is solved by a local variable assigned to a ternary statement that returns an empty String literal when the query is null. So that no method calls are made on the null reference.  While the second issue is not in the JIRA, it was discovered after applying the fix for the first case.  The new behavior is that an empty DocList is returned to the user instead of a 500 error and NPE.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","CaderHancock","2019-10-09T05:25:27Z","2019-10-10T01:06:36Z"
"","805","SOLR-13649 change the default behavior of the basic authentication plugin.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The default value for `blockUnknown` is `false`. That default behavior is a bit counterintuitive because if someone wishes to enable basic authentication, you would expect that they would want all unknown users to need to authenticate by default. I can imagine cases where you would not, but those cases would be less frequent.   # Solution  set `blockUnknown` to `true`  # Tests   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the Ref Guide (for Solr changes only).","closed","","MarcusSorealheis","2019-07-23T22:56:06Z","2019-08-29T05:27:50Z"
"","895","LUCENE-8985: SynonymGraphFilter cannot handle input stream with tokens filtered.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  SynonymGraphFilter cannot handle input stream with tokens filtered.   # Solution Original implementation BufferedInputToken doesn't track startNode, endNode. I replace BufferedInputToken & BufferedOutputToken with BufferedToken, and track startNode & endNode in every Buffered Token. use new variables to track previous input startNode, previous output StartNode, and delta of StartNode (If we add synonyms, the next token's startNode will change.).    # Tests  TestSynonymGraphFilter.testWithStopwordGaps  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","chenkovsky","2019-09-21T15:16:34Z","2020-04-04T03:49:50Z"
"","853","SOLR-13737 Moving SolrCloud on the README with some cues.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Some haters don't scroll on documentation.  # Solution  Moving SolrCloud before standalone.  # Tests  No tests for the `README`.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","MarcusSorealheis","2019-09-04T00:57:41Z","2019-10-03T03:14:42Z"
"","1041","SOLR-13969: Clean up and document AuditEvent API","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  See https://issues.apache.org/jira/browse/SOLR-13969  # Tests  Added tests for the new getters  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-11-26T12:36:10Z","2019-11-28T08:57:36Z"
"","1048","SOLR-13806: change type of QueryResponse._explainMap","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  QueryResponse._explainMap is of the type Map. When debug.explain.structured=true is used, it throws  ``` java.lang.ClassCastException: class org.apache.solr.common.util.SimpleOrderedMap cannot be cast to class java.lang.String ```  # Solution  Changed the type of this field to Map.  # Tests  Verified if this field is correctly typed for a sample response in the given scenario.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gunasekhardora","2019-12-01T07:59:34Z","2020-10-22T03:23:19Z"
"","1013","SOLR-13934: Documentation for windows users","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Provide example for windows users in documentation  # Solution  Provide example for windows users in documentation  # Tests  Verified formatting by using ascii doc plugin in vscode  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ x] I have created a Jira issue and added the issue ID to my pull request title. - [x ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","RompotiMiranda","2019-11-15T14:29:47Z","2020-01-13T11:34:48Z"
"","1035","SOLR-13966: Decode lat lon bevor used by RealTimeGetComponent","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-11-25T01:19:18Z","2019-12-10T08:10:46Z"
"","1030","SOLR-13961: Fix Atomic Update unset nested documents","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-11-22T18:13:13Z","2019-11-25T21:39:04Z"
"","911","SOLR-13802: Write analyzer property luceneMatchVersion to managed schema","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-09-30T15:34:42Z","2019-10-01T14:42:19Z"
"","902","SOLR-13795: Reload solr core after schema is persisted.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-09-26T13:56:30Z","2019-10-04T17:05:11Z"
"","937","SOLR-13209 fixed by adding a null check that throws a SolrException","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","CaderHancock","2019-10-10T02:26:56Z","2021-08-31T15:25:11Z"
"","883","SOLR-13762: Add binary support to XMLCodec","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","thomaswoeckinger","2019-09-16T10:25:59Z","2019-11-06T11:34:32Z"
"","955","Fix ref guide for autoscaling metric trigger SOLR-13847","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","Ankou76ers","2019-10-15T18:12:31Z","2019-11-07T21:17:07Z"
"","1032","LUCENE-9058: highlighting over underneath field despite IQ field","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mkhludnev","2019-11-24T15:53:59Z","2019-11-26T20:42:44Z"
"","1011","LUCENE-9031: Just highlight term intervals and its' combinations.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mkhludnev","2019-11-14T13:58:08Z","2019-11-24T15:55:28Z"
"","1001","[WIP] Ref scmanager","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","apoorvprecisely","2019-11-10T18:43:33Z","2019-11-16T06:40:59Z"
"","972","SOLR-13452: Update the lucene-solr build from Ivy+Ant+Maven (shadow build) to Gradle.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","markrmiller","2019-10-24T01:28:41Z","2020-11-09T16:06:08Z"
"","907","Branch 7 5","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","pinkeshsharma","2019-09-29T09:55:57Z","2019-09-29T09:57:50Z"
"","948","SOLR-13841: Add jackson databind annotations to SolrJ classpath","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite.","closed","","noblepaul","2019-10-14T02:05:29Z","2019-10-19T23:00:39Z"
"","793","testing the PR template","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the Ref Guide (for Solr changes only).","closed","","MarcusSorealheis","2019-07-17T16:29:33Z","2019-07-17T16:30:36Z"
"","875","merge master","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","yipen9","2019-09-12T01:52:52Z","2019-09-13T12:41:23Z"
"","873","Rename README.txt to README.md","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","matejsoroka","2019-09-11T07:58:49Z","2020-02-15T23:24:45Z"
"","835","Add LICENSE","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ *] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","xlzheng021","2019-08-17T01:42:27Z","2019-08-17T03:26:02Z"
"","979","SOLR-13783: Add space after comma in NamedList.toString()","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  NamedList.toString() differs in format from AbstractMap.toString() when the NamedList has 2 or more entries, in that there's no space after each comma between entries.  # Solution  Added a space after the comma.  # Tests  Added NamedListTest.testToString(), which covers toString() for the cases with 0, 1 and 2 entries. (The method currently seems to have no test coverage.)  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. **(Existing issue)** - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only). **(No changes)**","closed","","Pr0methean","2019-10-26T22:45:20Z","2019-10-29T00:09:09Z"
"","1018","SOLR-13941: Configure JettySolrRunner same as in web.xml","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Make sure tests have same servlet root for DispatchFilter as when running Solr from cmdline. This avoids some subtle confusion in tests.  # Solution  Wire SolrDispatchFilter on `/*` instead of `*`, and make a new `SerlvetUtils` class with a method that concatenates servletPath and pathInfo for more readable and less error prone code.  # Tests  No new tests necessary  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-11-18T23:07:01Z","2019-11-20T09:21:28Z"
"","1005","LUCENE-9042: Refactor TopGroups.merge tests","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  LUCENE 9042: Refactor TopGroups.merge tests  # Solution  This task proposes a refactoring of the test coverage for the TopGroups.merge method implemented in LUCENE-9010. For now it will cover only 3 main cases.  1. Merging to empty TopGroups 2. Merging a TopGroups with scores and a TopGroups without scores (currently broken because of LUCENE-8996 bug)  3. Merging two TopGroups with scores.  I'm planning to increase the coverage testing also invalid inputs but I would do that in a separate PR to keep the code readable.   # Tests  I'm contributing unit tests.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","diegoceccarelli","2019-11-12T16:37:33Z","2020-02-15T22:02:14Z"
"","1023","SOLR-13950: Fix getLeaderRetry swallowing interrupt in ZkStateReader","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  InterruptedException is being swallowed in ZkStateReader’s getLeaderRetry method  # Solution  Catch InterruptedException separately, interrupt current thread, and throw  # Tests  No tests added but ran and got a passing test suite locally  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andyvuong","2019-11-21T00:12:58Z","2019-12-17T00:16:44Z"
"","1036","SOLR-13967: Update query.css to make query form sticky on the top while scrolling page","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  In Admin UI, query screen, the query form and result URL bar are not sticky to the top when scroll the screen.  # Solution  add position sticky and top position by pixel value  # Tests  manually tested by inspecting element using chrome  # Checklist  Please review the following and check all that apply:  - [🗸 ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [🗸 ] I have created a Jira issue and added the issue ID to my pull request title. - [🗸 ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [🗸 ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [🗸 ] I have developed this patch against the `master` branch. - [x ] I have run `ant precommit` and the appropriate test suite. - [🗸 ] I have added tests for my changes. - [x ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","shuson","2019-11-25T06:52:06Z","2020-02-19T11:56:12Z"
"","826","SOLR-13399: add splitByPrefix configuration to IndexSizeTrigger","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  I've added the `splitByPrefix` parameter to the IndexSizeTrigger, so that the trigger is able to execute shard splits by prefix. I also added test cases to validate the split configs baked into this trigger.   Plus, I made a small change to the splitByPrefix code for splitting by ID, and cleaned up the tests a bit.  # Solution  The solution I took to address this problem models the convention of JIRA-12942, which adds the split configs directly into the trigger config.   The change I made to the splitByPrefix code doesn't modify functionality at all, but the implementation uses BytesRef.bytesEquals instead of manually comparing byte arrays.  # Tests  I added additional checks to the test IndexSizeTriggerTest.testTrigger, which validates that the default split configs are added to the trigger config. I also added a check to IndexSizeTriggerTest.testSplitConfig that validates the splitByPrefix override. Finally, I added a test case that attempts trigger configuration with invalid configs, and fails.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [] I have created a Jira issue and added the issue ID to my pull request title.   - I'm piggybacking on existing JIRA SOLR-13399 - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","megancarey","2019-08-09T22:18:10Z","2019-08-12T20:27:49Z"
"","1034","SOLR-13863: payload query function now handles string encoded payload field (delimited_payloads_string)","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  I started to use payloads because I had the classical per-store pricing problem. But standard payload function handles only numbers and, for example, stores can also be in different countries, would be useful also have the currency and other attributes related to the store.  This pull request modify payload function query, this version is now able to read and sort payload field delimited_payloads_string.  Example document:  ```   ""id"":""my sample doc"",   ""currencyPayload"":[     ""store1|EUR"",     ""store2|USD"",     ""store3|GBP""   ] ```   Querying Solr with  ``` fl=payload(currencyPayload,store3) ```  would generate a response like the following:  ```   ""response"":{       ""docs"":[{           ""id"":""my sample doc"",            ""payload(currencyPayload,store3)"":""GBP""       }]    } ```  And executing `payload(payloadCurrency,store2)` returns `EUR`, and so on.  You can use `payload` even as sorting function.  ``` sort=payload(payloadField,value) asc ```  # Solution  I was looking at the original payload function is defined into the ValueSourceParser, this function uses a FloatPayloadValueSource to return the value found.  As said I fixed the current version of payload function, adding a part that handles strings. It is able to extract the string value from the payload.  Given the former example where I have a multivalue field payloadCurrency  ``` payloadCurrency: [ ""store1|USD"", ""store2|EUR"", ""store3|GBP"" ] ```  executing payload(payloadCurrency,store2) returns ""EUR"", and so on for the remaining key/value in the field.  To implement the payload function, I've modified the ValueSourceParser where is defined the function payload and now returns a StringPayloadValueSource with the value inside (does the same thing of former FloatPayloadValueSource).  # Tests  I've created a sample solr collection where test the custom function https://github.com/freedev/solr-payload-string-function-query   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","freedev","2019-11-24T23:15:13Z","2022-01-11T16:22:42Z"
"","996","SOLR-13863: Added spayload query function to read and sort string pay…","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  I started to use payloads because I had the classical per-store pricing problem. But standard payload function handles only numbers and, for example, stores can also be in different countries, would be useful also have the currency and other attributes related to the store.  This pull request add a new function query named spayload that helps to read and sort payload field delimited_payloads_string.  Example document:  ```   ""id"":""my sample doc"",   ""currencyPayload"":[     ""store1|EUR"",     ""store2|USD"",     ""store3|GBP""   ] ```   Querying Solr with  ``` fl=spayload(currencyPayload,store3) ```  would generate a response like the following:  ```   ""response"":{       ""docs"":[{           ""id"":""my sample doc"",            ""spayload(currencyPayload,store3)"":""GBP""       }]    } ```  And executing `spayload(payloadCurrency,store2)` returns `EUR`, and so on.  You can use `spayload` even as sorting function.  ``` sort=spayload(payloadField,value) asc ```  # Solution  I was looking at the original payload function is defined into the ValueSourceParser, this function uses a FloatPayloadValueSource to return the value found.  As said I wrote a new version of payload function that handles strings, I named it spayload, and basically is able to extract the string value from the payload.  Given the former example where I have a multivalue field payloadCurrency  ``` payloadCurrency: [ ""store1|USD"", ""store2|EUR"", ""store3|GBP"" ] ```  executing spayload(payloadCurrency,store2) returns ""EUR"", and so on for the remaining key/value in the field.  To implement the spayload function, I've added a new ValueSourceParser instance to the list of defined functions and which returns a StringPayloadValueSource with the value inside (does the same thing of former FloatPayloadValueSource).  # Tests  I've created a sample solr collection where test the custom function https://github.com/freedev/solr-payload-string-function-query   # Checklist  Please review the following and check all that apply:  - [ x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","freedev","2019-11-05T17:38:37Z","2019-11-24T23:23:10Z"
"","968","Added spayload query function to read and sort string payload field (delimited_payloads_string)","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  I started to use payloads because I had the classical per-store pricing problem. But standard payload function handles only numbers and, for example, stores can also be in different countries, would be useful also have the currency and other attributes related to the store.  This pull request add a new function query named spayload that helps to read and sort payload field delimited_payloads_string.  Example document:  ```   ""id"":""my sample doc"",   ""currencyPayload"":[     ""store1|EUR"",     ""store2|USD"",     ""store3|GBP""   ] ```   Querying Solr with  ``` fl=spayload(currencyPayload,store3) ```  would generate a response like the following:  ```   ""response"":{       ""docs"":[{           ""id"":""my sample doc"",            ""spayload(currencyPayload,store3)"":""GBP""       }]    } ```  And executing `spayload(payloadCurrency,store2)` returns `EUR`, and so on.  You can use `spayload` even as sorting function.  ``` sort=spayload(payloadField,value) asc ```  # Solution  I was looking at the original payload function is defined into the ValueSourceParser, this function uses a FloatPayloadValueSource to return the value found.  As said I wrote a new version of payload function that handles strings, I named it spayload, and basically is able to extract the string value from the payload.  Given the former example where I have a multivalue field payloadCurrency  ``` payloadCurrency: [ ""store1|USD"", ""store2|EUR"", ""store3|GBP"" ] ```  executing spayload(payloadCurrency,store2) returns ""EUR"", and so on for the remaining key/value in the field.  To implement the spayload function, I've added a new ValueSourceParser instance to the list of defined functions and which returns a StringPayloadValueSource with the value inside (does the same thing of former FloatPayloadValueSource).  # Tests  I've created a sample solr collection where test the custom function https://github.com/freedev/solr-payload-string-function-query   # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","freedev","2019-10-23T15:40:42Z","2019-11-05T17:40:19Z"
"","1014","SOLR-13905 Make findRequestType in AuditEvent more robust","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Got NPE for some Audit events such as early errors, due to the way we constructed the resource variable from `httpRequest.getPathInfo()`, which could return null in prod but not in test. We fixed that in SOLR-13941. This PR is to make `findRequestType()` more performant and detect more admin paths.  # Solution  The Path-to-RequestType guessing logic now uses compiled static RegEx Patterns for speed, and also is null-safe in that it explicitly checks.  # Tests  Adds a test for ERROR case that is correctly classified as ADMIN.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-11-15T14:41:18Z","2019-11-22T22:34:25Z"
"","909","Fixing link to Lucene Java Bugs page","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fixing the link to Lucene Java Bugs page  # Solution  Link updated  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","aadel","2019-09-29T12:58:38Z","2019-10-02T08:29:08Z"
"","936","SOLR-13205 fixed as per diffblue suggestion","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fixing a string index out of bounds exception when field is null.  # Solution  charAt(0) == '_' -> startsWith(""_"")  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","CaderHancock","2019-10-10T01:20:01Z","2020-10-27T12:31:12Z"
"","1000","[MINOR] Fix typo","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fix typo   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","asdf2014","2019-11-10T18:38:14Z","2019-11-27T09:44:05Z"
"","871","LUCENE-8984: MoreLikeThis MLT is biased for uncommon fields","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fix MLT not to be biased for fields with few docs  # Solution  Use the per field doc count  # Tests   Tests with uncommon and common fields  # Checklist  Please review the following and check all that apply:  - [x ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x ] I have created a Jira issue and added the issue ID to my pull request title. - [ x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ x] I have developed this patch against the `master` branch. - [x ] I have run `ant precommit` and the appropriate test suite. - [x ] I have added tests for my changes. - [ Not required] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andyhind","2019-09-10T20:25:31Z","2019-09-25T04:58:43Z"
"","879","SOLR-13760 - restore viability of date math in TRA start property","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Fix Date math parsing for TRA start  # Solution  Invoke the DateMathParser.parseMath method rather than using the parseRouteKey method.  # Tests  Added a test that fails without the fix to ensure support for date math is not lost again.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gus-asf","2019-09-14T20:15:12Z","2019-10-10T22:21:58Z"
"","1022","SOLR-13953: Update eviction behavior of cache in Solr Prometheus exporter to allow for larger clusters","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Email thread starts here: http://mail-archives.apache.org/mod_mbox/lucene-dev/201911.mbox/%3cCAOz296DSV-tt7rWBirBZ+P4=vT5g29FZrR_2zHrHF084Xq+gyw@mail.gmail.com%3e . Jira issue here: https://issues.apache.org/jira/browse/SOLR-13953  For large (> 100 node) clusters, the exporter fails with ""connection pool shut down"" for certain nodes. This is caused by only the last 100 HttpSolrClients added to the cache still being open -- earlier additions are evicted and closed, even though the HttpSolrClients are still returned for use in `metricsForAllHosts` and `pingAllCollections` .  # Solution  Change the cache configuration to evict clients from the cache after twice the scrape interval, instead of using a fixed size cache.   Would love opinions on: - any other approaches - whether we should use a different caching strategy - whether we should use some other timeout besides twice the scrape interval.  # Tests  Can add a regression test if it seems valuable, but with past configuration, would need to be with > 100 nodes.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ajablonski","2019-11-20T20:03:35Z","2020-02-15T01:21:57Z"
"","768","SOLR-13472: Defer authorization to be done on forwarded nodes","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Defer authorization to be done on forwarded nodes, skip on forwarding nodes. Note: This is for branch_8x for now, all tests pass.","closed","","chatman","2019-07-08T00:19:19Z","2020-11-10T22:30:12Z"
"","812","[SOLR-13663] Span Positional Range Support in XML Query Parser + test","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Currently the XML Query Parser support a vast array of span queries, including the SpanFirstQuery, but it doesn't support the generic SpanPositionRangeQuery. < SpanPositionRange start=""2"" end=""3""> prejudice       # Solution  Scope of this issue is to introduce the related java builder and allow the possibility to build such queries.  # Tests  org.apache.lucene.queryparser.xml.TestCoreParser#testSpanPositionRangeQueryXML  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the Ref Guide (for Solr changes only).","closed","","alessandrobenedetti","2019-07-30T13:37:38Z","2020-04-28T18:27:41Z"
"","750","LUCENE-8891 Snowball stemmer for Estonian and EstonianAnalyzer class","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Currently lucene doesn't have a stemmer/analyzer for Estonian.   # Solution  Compiled a stemmer made in Snowball into Java and then made the analyzer based off of the stemmer with a custom stopwords list.  # Tests  Wrote test for both the analyzer and stopwords file  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","gpaimla","2019-06-28T13:32:33Z","2019-06-30T12:51:43Z"
"","888","SOLR-13774 add lucene/solr openjdk compatibility matrix to ref guide.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Create a reusable build system to run Lucene/Solr ant test source code suite against different versions of OpenJDK binaries. Generate a table with the results of BUILD SUCCESSFUL or BUILD FAILED and incorporate the output into the Ref Guide here: solr/solr-ref-guide/src/solr-system-requirements.adoc  # Solution  The build system is written in Python and Bash. Several scripts configure the environment, download all the Solr sources for each version, and OpenJDK binaries from jdk.java.net. Java versions are included in the submitted documentation. Python multiprocessing is used to parallelize concurrent Solr+OpenJDK ant test runs. Params for JVM concurrency are passed through scripts to ant tasks as well giving fine-grained concurrency control. Tests that resulted in BUILD SUCCESSFUL are passed as ""Y"" and those resulting in BUILD FAILED are failed as ""N"".  # Tests  Test suites run as described in solution. For validating syntax of submitted documentation in this PR Asciidoctor was run to convert to HTML that was inspected visually.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","lw-nick","2019-09-17T18:39:04Z","2019-10-16T22:04:13Z"
"","723","SOLR-13545: AutoClose stream in ContentStreamUpdateRequest","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  As discussed on SOLR-13545, ContentStreamUpdateRequest doesn't close the InputStream it creates, after a change made in SOLR-12142  # Solution  Use try-with-resources to autoclose the stream  # Tests  I've updated existing tests that make update requests backed by file streams, so that they attempt to delete the file they have streamed to Solr. If the stream isn't closed, the test will fail on Windows, since the file will be locked. On Linux the file will appear to be deleted even if the stream isn't immediately closed.  I think the alternative would be to change the request classes to expose the InputStream so that it can be mocked or spied on, to explicitly check that close is called, but that also feels fragile.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","colvinco","2019-06-15T23:31:31Z","2019-10-19T10:11:28Z"
"","720","SOLR-13549: Fix opentracing mock dependency for Solr core tests","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Adds a dependency on opentracing-mock for the Solr core tests  # Solution  This is just a build fix.  # Tests  Solr builds using maven now.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [n/a] I have added tests for my changes. - [n/a] I have added documentation for the Ref Guide (for Solr changes only).","closed","","dcollins53","2019-06-14T17:09:12Z","2019-07-10T18:09:16Z"
"","845","SOLR-13699 - maxChars no longer working on CopyField with Javabin","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  A regression was introduced as part of the optimizations made in [SOLR-12992](SOLR-12992), which caused the maxChars property to no longer work as expected when indexing via Javabin. Text fields below a certain size limit now are read as ByteArrayUtf8CharSequence instead of Strings, which causes an instanceOf check to fail when attempting to apply the maxChars truncation.  # Solution  Changed the instanceOf check to use the more generic CharSequence, which is implemented by both String and the new ByteArrayUtf8CharSequence. This will allow the truncation to be applied regardless of type.  This solution was discussed with Noble Paul and others in [SOLR-13699](https://issues.apache.org/jira/browse/SOLR-13699).  # Tests  Added a test to DocumentBuilder to test the maxChars truncation given both a String value and a ByteArrayUtf8CharSequence value. Check that both values are truncated appropriately according to maxChar rules.  A future enhancement would be to get all tests running in multiple formats (XML/JSON/Javabin), as this regression would have been caught by the existing CopyField tests if this was the case.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ctroullis","2019-08-25T22:15:12Z","2019-08-25T22:57:18Z"
"","778","SOLR-10288 remove non minified js","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  removing full files from index.html.  # Solution  add the full suffix to the non-minified files so that we can programmatically exclude them in the build file.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  * [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. * [x] I have created a Jira issue and added the issue ID to my pull request title. * [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. * [x] I have developed this patch against the `master` branch. * [ ] I have run `ant precommit` and the appropriate test suite. * [ ] I have added tests for my changes. * [ ] I have added documentation for the Ref Guide (for Solr changes only).","closed","","MarcusSorealheis","2019-07-11T23:43:25Z","2019-07-17T16:18:25Z"
"","1560","LUCENE-9391: Upgrade HPPC to 0.8.2","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Upgrade HPPC to 0.8.2 `ant test` passed `ant precommit` passed  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","zhaih","2020-06-09T21:55:36Z","2020-06-12T05:36:44Z"
"","1382","SOLR-12720 Remove autoReplicaFailoverWaitAfterExpiration","* SOLR-12720: this resolves the issue of removing the rule-based replica placement stategy.   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description There is something problematic that was deprecated a while ago that is no gone.  # Solution  I removed the the deprecated object and its associated artifacts and added how users could still leverage the API for trigger configuration if they so choose.   # Tests  No change to any test.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-03-26T07:49:27Z","2020-04-03T23:46:44Z"
"","1578","SOLR-14570: Edismax round plugin","* SOLR-####: Edismax Match Mode Enable rounding off  LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  To make the rounding of the Match mode in Edismax, i.e if the mm.roundOff is true, it will round off the Edismax Match mode queries  # Solution  In this, we have enabled a flag, when this flag is true It will round off the minimum classes required instead of always selecting the floor value.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  I have run the test cases for the SolrPluginUtils.calculateMinShouldMatch   tests are included in the testMinShouldMatchCalculatorWithRoundoff method # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","MighTguY","2020-06-15T11:12:03Z","2020-06-15T11:17:13Z"
"","1414","SOLR-14385 added shard name thru MDC and collection name to split histogram logs","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. --> https://issues.apache.org/jira/browse/SOLR-14385  # Description  Please provide a short description of the changes you're making with this pull request. Added shard name and collection name to split histogram logs.   # Solution  Please provide a short description of the approach taken to implement your solution. Had access to collection and now accessing shard name directly from MDC.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem. Modified SplitHandlerTest file to include creation of a test cluster with test collection to verify logs. Verified all related tests pass.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ 01894589] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","saatchibhalla","2020-04-06T21:49:49Z","2020-04-09T19:28:51Z"
"","1352","LUCENE-9278: Make javadoc folder structure follow Gradle project path","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->  # Description  See https://issues.apache.org/jira/browse/LUCENE-9278","closed","","mocobeta","2020-03-14T06:38:17Z","2020-03-20T09:52:31Z"
"","1155","LUCENE-8962: Add ability to selectively merge on commit","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->  # Description  If we have many index writer threads, they will flush their DWPT buffers on commit, resulting in many small segments, which can be merged before the commit returns.  # Solution  This adds a new ""findCommitMerges"" method to MergePolicy, which can specify merges to be executed as part of preparing a commit. By default, we return null for backward compatibility (i.e. don't merge on commit).  In IndexWriter, we call findCommitMerges from prepareCommitInternal, after we have cloned the SegmentInfos. If we found commit merges, we wrap each OneMerge so that on completion, they update the cloned SegmentInfos and reference counts of segment files. Outside the flush lock, we wait (up to max time specified in IndexWriterConfig) for the commit merges to complete before calling startCommit.   Also, added an IndexWriterEvents callback (configurable through IndexWriterConfig) so that consuming code can be notified of merge on commit events (to emit metrics on time spent waiting for the merges to complete, for example).  # Tests  1. Added findCommitMerges implementation to MockRandomMergePolicy. 2. Added explicit test case for merge-on-commit behavior in TestIndexWriterMergePolicy.  # Checklist  Please review the following and check all that apply:  - [✓] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [✓] I have created a Jira issue and added the issue ID to my pull request title. - [✓] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [✓] I have developed this patch against the `master` branch. - [✓] I have run `ant precommit` and the appropriate test suite. - [✓] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","msfroh","2020-01-07T23:55:58Z","2020-03-06T13:46:53Z"
"","1261","SOLR-14260: Make SchemaRegistryProvider pluggable in HttpClientUtil","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description HttpClientUtil.java defines and uses an abstract SchemaRegistryProvider for mapping a protocol to an Apache ConnectionSocketFactory. There is only one implementation of this abstract class (outside of test cases). Currently, it is not override-able at runtime.  # Solution Adds the ability to override the schema registry provider at runtime, using the class name value provided by ""solr.schema.registry.provider"", similar to how this class allows for choosing the HttpClientBuilderFactory at runtime.  # Tests We have been using this patch in our internal fork of SOLR. We have verified that SOLR communication is encrypted, and since this patch helps us enable that encryption by setting a custom SSLContext for HTTP clients, we know this patch is working as expected.   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","athrog","2020-02-14T23:39:51Z","2020-03-27T02:16:10Z"
"","1132","SOLR-13839:maxScore shouldn't be NaN","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  When group.query doesn't match anything maxScore returns NaN in the response.  # Solution  maxScore would return 0 instead of NaN in doclist.maxScore.  # Tests  Added an unit test for the same condition.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","gunasekhardora","2019-12-31T07:36:21Z","2020-01-13T03:12:44Z"
"","1115","SOLR-13101: Fix the gson version reference","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Updating a reference to a JAR to keep consistent with SFDC downstream.  # Solution  Modified JAR path.  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","megancarey","2019-12-23T21:29:41Z","2019-12-26T17:53:35Z"
"","1514","SOLR-13749: Change cross-collection join query syntax to {!join method=crossCollection ...}","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Updates the cross-collection join query in #976 based on the feedback in SOLR-13749.  In that ticket there was a preference to consolidate the cross-collection join functionality into the existing join query parser, rather than creating a new separate query parser.  # Solution  This PR integrates the cross-collection join query parser into the existing join query parser plugin.  The syntax for a cross-collection join changes from `{!xcjf ...}` to `{!join method=crossCollection ...}`.  The arguments that could previously be set on the XCJF query parser plugin can now be set on the join query parser plugin.  # Tests  The XCJFQueryTest class has been updated to use the new query syntax (and renamed to CrossCollectionJoinQueryTest).  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","danmfox","2020-05-13T22:34:16Z","2020-06-27T05:28:30Z"
"","1144","SOLR-13756 updated restlet mvn repository url.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Updated old repository URL for the restlet framework to the current official stated at: https://restlet.talend.com/downloads/current/  # Solution  The old repository URL does a redirect for the new one, but Ivy fails to follow it on some platforms. The redirect also points to the updated URL.  # Tests  It could be compiled even after deleting the local ivy cache using `rm -rf ~/.ivy2/cache` .  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [n/a] I have added tests for my changes. - [n/a] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","zsgyulavari","2020-01-06T13:43:12Z","2020-02-18T16:13:05Z"
"","1458","Fix typo in `cloud-scripts` path","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  typo in `cloud-scripts` path  # Solution  Fix  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-04-26T18:32:08Z","2020-04-26T22:42:13Z"
"","1117","SOLR-13101: ant precommit fixes","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This PR is just to fix precommit errors.  # Solution  Added licenses, updated code to remove forbidden method calls, and fixed other miscellaneous issues.  # Tests  N/A - I ran existing tests for the files changed to ensure no change in functionality.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","megancarey","2019-12-24T00:38:23Z","2019-12-26T20:02:57Z"
"","1175","Backport SOLR-13749 Cross-collection join filter to 8.x","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This is a backport of #976 to the 8.x branch.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] ~~I have developed this patch against the `master` branch.~~ - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","danmfox","2020-01-15T22:37:55Z","2020-01-24T17:26:39Z"
"","1421","SOLR-14396: TaggerRequestHandler should not error on empty collection","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The TaggerRequestHandler currently returns a 400 (Bad Request) if used on a collection with no terms in the index. This is inconsistent with how Solr works in general, and while it's certainly possible for the 400 error to be handled client-side for empty collections, the incoming requests aren't really ""bad"" requests in my opinion, the index just doesn't have any data yet. This PR removes the error and just has the TaggerRequestHandler return no matched tags.  # Solution  The explicit exception was removed since it was agreed that it doesn't make sense.  # Tests  I added a unit test (TaggerTest.testEmptyCollection)to verify a response is now returned indicating zero tags.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only). **Change too insignificant to warrant addition to documentation**","closed","","treygrainger","2020-04-09T05:18:54Z","2020-04-13T01:54:50Z"
"","1502","SOLR-13203: Added 400 status code to exception in DynamicField","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The RuntimeException that the DynamicField parser throws is never caught. In at least one case this results in a 500 error being thrown.  # Solution  Replaced the RuntimeException with a SOLRException with an error code set to 400  # Tests  Added TestExtendedDismaxParser.testInvalidDynamicField which covers DynamicField for an invalid input that can cause this bug  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. (Existing issue) - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","bug,","mrsoong","2020-05-10T23:30:59Z","2020-06-08T16:47:27Z"
"","1472","SOLR-13184: Added some input validation in ValueSourceParser","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The joindf parser inside ValueSourceParser did not check for null qfield values before calling JoinDocFreqValueSource(), however JoinDocFreqValueSource() expects qfield to be non-null  # Solution  Perform a simple null check in the ValueSourceParser for qfield  # Tests  Added TestFunctionQuery.testQFieldValidation() which test a query with a joindf function with an invalid input  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. **(Existing issue)** - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mrsoong","2020-05-01T02:23:59Z","2021-02-01T21:27:41Z"
"","1589","SOLR-13195: added check for missing shards param in SearchHandler","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The distributedProcess methods of the search pipeline never checks to see if a request has a shards parameter, eventhough a shards param is required if SOLR is not running with SolrCloud  # Solution  Added code in SearchHandler.getAndPrepShardHandler() to check for a shards param, and throw an exception if it's not defined.  # Tests  Added SearchHandlerTest.testDistribWithoutZk() which covers legacy distributed queries without a shards param  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. **(Existing issue)** - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mrsoong","2020-06-17T21:22:49Z","2021-02-01T21:27:45Z"
"","1385","SOLR-14370: Refactor bin/solr to allow external override of Jetty modules","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  The bin/solr script currently does not allow for externally overriding the modules passed to Jetty on startup.  # Solution  This PR adds the ability to override the Jetty modules on startup by setting `JETTY_MODULES` as an environment variable; when passed, bin/solr will pass through (and not clobber) the string verbatim into `SOLR_JETTY_CONFIG`. For example, you can now run: ``` JETTY_MODULES=--module=foo bin/solr start ```  This PR also allows for external overriding of `SOLR_URL_SCHEME` and `SOLR_SSL_OPTS`. # Tests  We have been using this change in our internal fork of Solr. When creating this PR, I started Solr with `JETTY_MODULES` set, and unset to verify behavior as expected.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","athrog","2020-03-28T00:14:53Z","2020-03-30T22:42:39Z"
"","1165","LUCENE-9133: Fix for potential NPE in TermFilteredPresearcher#buildQuery","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  TermFilteredPresearcher.buildQuery() assumes that LeafReader.terms() always returns a non-null Terms but that is not always the case.  # Solution  Check if the LeafReader  returns a null Terms for a given field.  If  so then skip that field.  # Tests  Added a new test to TestTermPresearcher that illustrates the problem. Run it without the change to buildQuery() and  you get a NPE.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","mjustice3","2020-01-13T22:37:34Z","2020-03-27T22:15:44Z"
"","1167","SOLR-13845: DELETEREPLICA API by count and type","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  SOLR-9319 added support for deleting replicas by count. It would be great to have the feature with added functionality the type of replica we want to delete like we add replicas by count and type.  # Solution  Support parameter 'type' for DELETEREPLICA API, which verifies the replica type and qualifies it to be deleted or not.  The solution verified if enough replicas are available to be deleted for a certain type otherwise it fails.  The solution verified not all TLOG and NRT type replicas are deleted for a shard of a collection so that a valid leader type replica is intact.  # Tests  Tests to verify all the valid factors mentioned in the solutions for single and all shards for a collection.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","sarkaramrit2","2020-01-14T09:51:21Z","2020-02-19T00:05:00Z"
"","1329","SOLR-14275: Policy calculations are very slow for large clusters and large operations","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  See JIRA for the explanation of the problem.  # Solution  Try and reduce the combinatoric explosion in the candidate placements. Use caching more effectively.  # Tests  Manual performance tests using the scenario.txt attached to JIRA.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","sigram","2020-03-06T13:22:49Z","2021-02-25T09:58:34Z"
"","1135","SOLR-14154 Return correct isolation level when retrieving it from the SQL Connection","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  return TRANSACTION_NONE when retrieving TransactionIsolation to indicate transactions are not supported   # Solution  see description  # Tests  None # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ 0] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [0] I have added tests for my changes. - [0] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","nvercamm","2020-01-01T18:56:09Z","2020-01-03T19:46:44Z"
"","1134","SOLR-14154 Return correct isolation level when retrieving it from the SQL Connection","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  return TRANSACTION_NONE when retrieving TransactionIsolation to indicate transactions are not supported   # Solution  see description  # Tests  None # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ 0] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [0] I have added tests for my changes. - [0] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","nvercamm","2020-01-01T18:53:47Z","2020-01-02T21:35:26Z"
"","1597","SOLR-14584: replace obsolete jks references with p12 references","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Replaces references to .jks files, which are obsolete, with references to .p12 files.  # Solution  Edited **bin/solr.in.cmd** and **bin/solr.in.sh**.  # Tests  Per own run through https://lucene.apache.org/solr/guide/8_5/enabling-ssl.html.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","arencambre","2020-06-20T18:38:32Z","2020-06-23T15:29:58Z"
"","1371","SOLR-14333: print readable version of CollapsedPostFilter query","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Prints a readable version of parsed_filter_queries in CollapsedPostFilter query  # Solution  The debug response will now be in the lines of ``` ""parsed_filter_queries"": [ ""CollapsingPostFilter({!collapse field=id, nullPolicy=expand, groupHeadSelectorText=_version_ asc, groupHeadSelectorType=SORT, hint=top_fc, size=5000})"" ] ```  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gunasekhardora","2020-03-21T09:56:30Z","2020-09-29T13:49:50Z"
"","1433","SOLR-14408 Refactor MoreLikeThisHandler implementation","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","NazerkeBS","2020-04-15T10:43:41Z","2020-04-21T13:55:42Z"
"","1527","SOLR-14384 Stack SolrRequestInfo","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","NazerkeBS","2020-05-18T20:35:24Z","2020-06-16T05:47:08Z"
"","1282","LUCENE 9236","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","juanka588","2020-02-24T10:24:01Z","2020-02-24T12:53:13Z"
"","1640","test","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-07-02T02:52:32Z","2020-07-02T02:53:04Z"
"","1636","Remove Compound file formation","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","DEVILCODER-SB99","2020-07-01T10:38:59Z","2020-07-02T04:21:02Z"
"","1605","add no comoression and zstd compression","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","DEVILCODER-SB99","2020-06-23T08:14:02Z","2020-07-01T21:17:56Z"
"","1604","Add No compression and Zstd Compression","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","DEVILCODER-SB99","2020-06-23T04:21:06Z","2020-07-01T21:19:34Z"
"","1603","branch_proj","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","DEVILCODER-SB99","2020-06-23T04:05:40Z","2020-06-24T10:36:17Z"
"","1587","Branch 8 5","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","someoneknown","2020-06-17T11:45:43Z","2020-06-17T12:03:34Z"
"","1551","LUCENESOLR-1912: CompositeIdRouter tries implicit logic first","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","joel-bernstein","2020-06-02T13:44:30Z","2020-06-02T14:14:06Z"
"","1417","SOLR-12847: Auto-create a policy rule that corresponds to maxShardsPerNode","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","sigram","2020-04-08T16:03:29Z","2020-08-13T15:37:08Z"
"","1365","SOLR-14347: Autoscaling placement wrong when concurrent replica placements are calculated","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","sigram","2020-03-19T16:24:44Z","2020-08-13T16:08:54Z"
"","1332","SOLR-14254: Docs for text tagger: FST50 trade-off","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","dsmiley","2020-03-09T17:36:40Z","2020-03-14T02:02:03Z"
"","1540","SOLR-14517 MM local params value is ignored in edismax queries with operators","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Making `mm` specified in local params processing consistent with behavior when it was specified as a query param.  # Solution If  edismax query has operators, we look up user specified value. Now we check both local params and params.  # Tests  Added corresponding test case with mm in local parameter to every existing tests case in `TestExtendedDismaxParser#testMinShouldMatchOptional`  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","yuriy-b-koval","2020-05-27T23:10:04Z","2020-06-01T12:13:12Z"
"","1423","LUCENE-9314: Match a document with MemoryIndex on LuceneMonitor","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Lucene monitor function, match single document, wraps a single document into a array of documents. Hence, it always calls the function, match many documents,  which builds a MultiDocumentBatch rather than a SingletonDocumentBatch. The former uses ByteBuffersDirectory while later uses MemoryIndex.  As per documentation, MemoryIndex is a high-performance single-document main memory Apache Lucene fulltext search index. Hence, Lucene monitor should use it when matching a single document.  see [LUCENE-9314](https://issues.apache.org/jira/browse/LUCENE-9314)  # Solution  This solution chooses the DocumentBatch depending on how many documents are required.  # Tests  Add 3 tests for zero, one and more documents respectively.  Zero document throws an IllegalArgumentException.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","plperron","2020-04-10T14:45:14Z","2020-04-27T09:59:17Z"
"","1200","SOLR-12045: emphasize deployment caveat.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Let's let user know about Analytic Component deployment   # Solution  Add two sentences into Ref Guide.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mkhludnev","2020-01-22T21:05:07Z","2020-01-25T10:32:57Z"
"","1069","Include the closing  tags in documentation for lucene.search.join","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Include closing  tags in package-info.java for Join  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","stevetemple","2019-12-08T22:44:11Z","2019-12-09T10:25:59Z"
"","1206","SOLR-14215: Improve SolrFeature performance when there are no filter queries","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  In cases where a SolrFeature only contains a main query `q` and no filter queries `fq`, it is possible to traverse documents using the doc iterator from `q` instead of constructing one from a processed filter that combines all of the queries.  In our test datasets this made retrieving the feature *10 times faster*.  Specifically, the call to `getProcessedFilter` inside `getDocIdSetIteratorFromQueries` is expensive: https://github.com/apache/lucene-solr/blob/16e537db7efe37eb7519b07dc96c5026c0a1d193/solr/contrib/ltr/src/java/org/apache/solr/ltr/feature/SolrFeature.java#L242-L243 That method aims at returning a doc iterator that traverses over the documents that match all of the criteria of `q` and `fq`. When there are no filter queries, this just returns a doc iterator for `q`.   # Solution  When `fq` is null or empty, use the iterator from `q` directly as it is a lot faster.  # Tests  Existing tests for `SolrFeature` already cover this use case, in particular `TestUserTermScoreWithQ`.  # Checklist  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","victorchenal","2020-01-24T13:29:47Z","2020-01-24T13:29:47Z"
"","1471","SOLR-14014 PR Against Master","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  I added a system property to allow users to disable the Solr Admin UI on start.  # Solution  I added this property, which accepts a boolean value, to SOLR_START_OPTS and START_OPTS.  # Tests  Not easily testable in the current project.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-04-30T21:58:03Z","2020-05-04T21:51:32Z"
"","1457","SOLR-13184: Added a null check in FunctionQParser.parseArg()","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  FunctionQParser.parseArg() did not throw an error when it failed to properly deference a parameter. This would cause a NPE later in JoinDocFreqValueSource.hashCode() where qfield is expected to be non-null.  # Solution  Added a check in FunctionQParser.parseArg() for null parameters  # Tests  Added an extra test in QueryParsingTest.testGetQParser() to check for NPEs resulting from invalid inputs  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. **(Existing issue)** - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mrsoong","2020-04-25T15:22:13Z","2020-04-29T20:49:17Z"
"","1591","SOLR-14578: Update solrcloud-autoscaling-triggers.adoc and test","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  fix action name in doc and test.   # Solution  Change the execute action to be `execute_plan`  # Tests  Fixes existing test  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-06-18T01:20:18Z","2020-06-25T21:50:51Z"
"","1334","SOLR-8306: Enhance ExpandComponent to allow expand.hits=0","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Enables ExpandComponent to allow expand.hits=0 for those who don't want an expanded document returned and only want the numFound returned from the expand section.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  We have been using this patch in our internal fork of SOLR for years so this has been thoroughly tested to ensure this functionality works as desired. Unit tests have been added for this functionality and all tests pass.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ameliahenderson","2020-03-10T13:35:35Z","2020-03-18T03:53:05Z"
"","1592","SOLR-14579 First pass at dismantling Utils","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Eliminating warnings due to static Functions in the Utils class within Solr.   # Solution  Removed a few Function variables from the Utils class which were adding little value and causing warnings throughout the code. Removed a handful of unused variables, etc. to reduce warnings as well.  # Tests  I ran `ant test` from the solr directory, as all of the changes in this PR are limited to Solr.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","megancarey","2020-06-18T05:24:28Z","2020-09-05T16:27:23Z"
"","1622","SOLR-14603: update Restlet version","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  CVEs in current version  # Solution  Upgrade the version to `2.4.3`  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","MarcusSorealheis","2020-06-27T21:39:27Z","2020-07-05T08:21:23Z"
"","1450","SOLR-14429: Convert XXX.txt files to proper XXX.md","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Converted all README.txt to README.md and partially fixed its formatting (as proper markdown). I also fixed pointers to the files.  See https://issues.apache.org/jira/browse/SOLR-14429  # Tests  - Distribution package built by `ant create-package` includes the all .md files - ~TODO: run smoke test~ This branch passed `nightly-smoke` ``` [smoker] SUCCESS! [0:43:25.703442] ```","closed","","mocobeta","2020-04-23T09:37:37Z","2020-04-27T00:00:50Z"
"","1060","SOLR-14024: Invalid html generated by changes2html.pl","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  changes2html.pl generates invalid html  # Solution  Fix it to generate a valid html.  # Tests  Created the html and tested it using https://validator.w3.org/  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","asalamon74","2019-12-06T12:49:03Z","2019-12-06T12:49:03Z"
"","1449","LUCENE-9344: Convert XXX.txt files to proper XXX.md","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Changed README.txt, MIGRATE.txt, etc. to `.md` and partially fix its markdown formatting.  LICENCE.txt and NOTICE.txt were not modified.  See https://issues.apache.org/jira/browse/LUCENE-9344  # Tests  - Distribution package built by `ant package-tgz` includes the all .md files - ~TODO: run smoke test~ This passed `nightly-smoke` ``` [smoker] SUCCESS! [0:44:59.564581] ```","closed","","mocobeta","2020-04-23T09:28:03Z","2020-04-24T05:32:18Z"
"","1141","SOLR-14147 change the Security manager to default to true.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Change the Security Manager to default to true.  # Solution  Changed the boolean values.  # Tests  No tests are affected.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] @rmuir has created a Jira issue and I added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-01-04T09:15:22Z","2020-02-06T00:17:56Z"
"","1067","LUCENE-9085: fix assertion in CharacterUtils","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Assertions in CharacterUtils are not consistent with javadocs   # Solution  Fix assertion  # Tests  Added a testcase that will in current implementation. - `toLowerCase` and `toUpperCase` with non-zero offset.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","dtsuzuku-ibm","2019-12-07T04:13:22Z","2019-12-09T07:32:08Z"
"","1485","LUCENE-9362: Fix rewriting check in ExpressionValueSource","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  [LUCENE-9362](https://issues.apache.org/jira/browse/LUCENE-9362): ExpressionValuesSource does not actually rewrite itself due to small mistake in the check of inner rewrites.  # Solution  `changed |= (rewritten[i] == variables[i]);` should be changed to `changed |= (rewritten[i] != variables[i]);`  # Tests  I added a test that checks that the ExpressionValueSource was rewritten when the inner ValueSource was rewritten, and vice versa  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `gradlew precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","emetsds","2020-05-05T16:49:05Z","2020-05-07T15:18:25Z"
"","1370","SOLR-13492: Ensure explicit GCs are concurrent by default","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  -XX:+DisableExplicitGC disables any explicit calls to System.gc()  # Solution  In many cases, if an explicit GC is invoked, a potential premature garbage collection might degrade application's performance.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gunasekhardora","2020-03-21T04:28:03Z","2020-06-07T10:14:31Z"
"","1419","SOLR-14397: Vector Search in Solr","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  *WORK IN PROGRESS, DO NOT MERGE.* Adds in `vector_cosine` function query and `vector_dotproduct` function queries, which can operate on ValueSources containing dense vector content. Supports multiple vectors per field (separated by `|`. For now, best to use a String field (with `docValues=true`) to create the vectors on documents, but will ultimately be implementing a DenseVector field to handle this more efficiently. See: https://issues.apache.org/jira/browse/SOLR-14397 for background info.   # Solution Since multivalued docvalues don't maintain insertion order, multiple vectors are instead encoded into the same docvalue per document separated by a `|` character. Currently the vectors are represented as raw strings (no Base64 encoding of Binary encoding - will do that later). Initial implementation let's you send in one or more vectors in the field, and at query time, to choose to return the score as the first parameter (the query vector) with either the `first`, `last`, `max`, `min`, or `average` similarity with all of the vectors in the `vectors` field.   # Tests  No unit tests yet. You can use/test the functionality as follows:  *Build and Start* ``` bin/solr stop || ant server && bin/solr start -c -a ""-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=6900"" ```  *Create Collection* ``` curl -H ""Content-Type: application/json"" \ ""http://localhost:8983/solr/admin/collections?action=CREATE&name=vectors&collection.configName=_default&numShards=1"" ```  *Index Documents* ``` curl -X POST -H ""Content-Type: application/json"" \  http://localhost:8983/solr/vectors/update?commit=true \  --data-binary ' [  {""id"": ""1"", ""name_s"":""donut"", ""vectors_s"":[""5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0|4.0,0.0,1.2,3.0,0.3,3.0,3.0,0.75|6.0,0.0,2.0,4.0,0.0,5.0,6.0,0.8""]},  {""id"": ""2"", ""name_s"":""apple juice"",              ""vectors_s"":[""1.0,5.0,0.0,0.0,0.0,4.0,4.0,3.0|0.0,5.0,0.0,0.0,0.0,3.0,5.0,4.0""]},  {""id"": ""3"", ""name_s"":""cappuccino"",              ""vectors_s"":[""0.0,5.0,3.0,0.0,4.0,1.0,2.0,3.0|""]},  {""id"": ""4"", ""name_s"":""cheese pizza"",              ""vectors_s"":[""5.0,0.0,4.0,4.0,0.0,1.0,5.0,2.0""]},  {""id"": ""5"", ""name_s"":""green tea"",              ""vectors_s"":[""0.0,5.0,0.0,0.0,2.0,1.0,1.0,5.0""]},  {""id"": ""6"", ""name_s"":""latte"", ""vectors_s"":[""0.0,5.0,4.0,0.0,4.0,1.0,3.0,3.0""]},  {""id"": ""7"", ""name_s"":""soda"", ""vectors_s"":[""0.0,5.0,0.0,0.0,3.0,5.0,5.0,0.0""]},  {""id"": ""8"", ""name_s"":""cheese bread sticks"",              ""vectors_s"":[""5.0,0.0,4.0,5.0,0.0,1.0,4.0,2.0""]},  {""id"": ""9"", ""name_s"":""water"", ""vectors_s"":[""0.0,5.0,0.0,0.0,0.0,0.0,0.0,5.0""]},  {""id"": ""10"", ""name_s"":""cinnamon bread sticks"", ""vectors_s"":[""5.0,0.0,1.0,5.0,0.0,3.0,4.0,2.0""]} ] ' ```  *Send Query* ``` curl -H ""Content-Type: application/json"" \ ""http://localhost:8983/solr/vectors/select?q=*:*&fl=id,name:name_s,cosine:\$func,vectors:vectors_s&func=vector_cosine(\$donut_vector,vectors_s,average)&sort=\$func%20desc&rows=11&donut_vector=5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0"" ```  *Response:* ``` {   ""responseHeader"":{     ""zkConnected"":true,     ""status"":0,     ""QTime"":1,     ""params"":{       ""q"":""*:*"",       ""func"":""vector_cosine($donut_vector,vectors_s,average)"",       ""donut_vector"":""5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0"",       ""fl"":""id,name:name_s,cosine:$func,vectors:vectors_s"",       ""json"":"""",       ""sort"":""$func desc"",       ""rows"":""11""}},   ""response"":{""numFound"":10,""start"":0,""docs"":[       {         ""id"":""1"",         ""cosine"":0.9884526,         ""name"":""donut"",         ""vectors"":""5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0|4.0,0.0,1.2,3.0,0.3,3.0,3.0,0.75|6.0,0.0,2.0,4.0,0.0,5.0,6.0,0.8""},       {         ""id"":""10"",         ""cosine"":0.98544514,         ""name"":""cinnamon bread sticks"",         ""vectors"":""5.0,0.0,1.0,5.0,0.0,3.0,4.0,2.0""},       {         ""id"":""4"",         ""cosine"":0.88938314,         ""name"":""cheese pizza"",         ""vectors"":""5.0,0.0,4.0,4.0,0.0,1.0,5.0,2.0""},       {         ""id"":""8"",         ""cosine"":0.88938314,         ""name"":""cheese bread sticks"",         ""vectors"":""5.0,0.0,4.0,5.0,0.0,1.0,4.0,2.0""},       {         ""id"":""2"",         ""cosine"":0.524165,         ""name"":""apple juice"",         ""vectors"":""1.0,5.0,0.0,0.0,0.0,4.0,4.0,3.0|0.0,5.0,0.0,0.0,0.0,3.0,5.0,4.0""},       {         ""id"":""7"",         ""cosine"":0.50913316,         ""name"":""soda"",         ""vectors"":""0.0,5.0,0.0,0.0,3.0,5.0,5.0,0.0""},       {         ""id"":""6"",         ""cosine"":0.30926093,         ""name"":""latte"",         ""vectors"":""0.0,5.0,4.0,0.0,4.0,1.0,3.0,3.0""},       {         ""id"":""3"",         ""cosine"":0.25923792,         ""name"":""cappuccino"",         ""vectors"":""0.0,5.0,3.0,0.0,4.0,1.0,2.0,3.0|""},       {         ""id"":""5"",         ""cosine"":0.1939959,         ""name"":""green tea"",         ""vectors"":""0.0,5.0,0.0,0.0,2.0,1.0,1.0,5.0""},       {         ""id"":""9"",         ""cosine"":0.073323555,         ""name"":""water"",         ""vectors"":""0.0,5.0,0.0,0.0,0.0,0.0,0.0,5.0""}]   }} ``` # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","treygrainger","2020-04-09T02:29:34Z","2020-09-18T14:10:16Z"
"","1613","LUCENE-8574 Cache ExpressionFunctionValues","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description   * Add a new DoubleValuesSource which will enforce only one value per name in dependencies  * Add few fields to `ExpressionFunctionValues` to ensure for a given doc, it is only computed once  * Added a unit test that need this mechanism to pass, otherwise would cost huge memory and time  `ant test` and `ant precommit` passed  # Solution  In `CachingExpressionValuesSource`, a modified version of `getValues` is used to get `DoubleValues`, it will be called with additional `valueCache` passed in and update `valueCache` along the creation process to ensure only one value per name is created.  # Tests  A new test method `testFibonacciExpr` is added to `TestExpressionValueSource` to show the difference, normally it takes 1.x sec on my laptop with `n=30`, and much longer when `n=40` because of the exponential growth.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","zhaih","2020-06-24T22:35:13Z","2020-07-09T19:35:22Z"
"","1093","SOLR-14107: Pass log level args to example run tool","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->    # Description  This is a trivial patch to allow the examples to run with (-v) DEBUG (likewise WARN for -q)  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andyvuong","2019-12-17T22:35:29Z","2019-12-20T16:31:17Z"
"","1402","SOLR-12720 Deprecated Auto Add Replica in XML","* https://issues.apache.org/jira/browse/SOLR-12720: remove deprecated autoReplicaFailoverWaitAfterExpiration property.  LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  remove deprecated autoReplicaFailoverWaitAfterExpiration property.  Please provide a short description of the changes you're making with this pull request.  # Solution  Please provide a short description of the approach taken to implement your solution.  # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-04-03T23:46:02Z","2020-04-06T01:55:27Z"
"","1391","SOLR-14014 Add a disable Admin UI Flag","# Description  I added a system property to allow users to disable the Solr Admin UI on start.   # Solution  I added this property, which accepts a boolean value, to `SOLR_START_OPTS` and `START_OPTS`.  # Tests  Not easily testable in the current project.   # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","MarcusSorealheis","2020-03-30T23:45:47Z","2020-04-30T21:58:28Z"
"","740","SOLR-12550 - distribUpdateSoTimeout for configuring socket timeouts in solrcloud doesn't take effect for updates.","* SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  Found this issue on Solr 7.2.1. distribUpdateSoTimeout (configuration property in solr.xml) for configuring socket timeouts in SolrCloud doesn't take effect. This is a fix for the same. We need this property to be configurable to allow for longer socket read timeouts in case of long running commits.  # Solution  The soTimeout property of ConcurrentUpdateSolrClient class remains null and hence in the method sendUpdateStream(), the default of 600000 ms is set as the timeout in HttpPost class instance variable ""method"". When the execute() call on underlying HttpClient is finally made, the Httpclient does contain the configured timeout (as in solr.xml or -DdistribUpdateSoTimeout) but gets overridden by the hard default of 600000 in the ""method"" parameter of the execute() call.  The issue is that the builder object which is used to instantiate a ConcurrentUpdateSolrClient in StreamingSolrClients itself doesn't contain the timeout values. This patch fixes that.   # Tests  Please describe the tests you've developed or run to confirm this patch implements the feature or solves the problem.  # Checklist  Please review the following and check all that apply:  - [check] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [check] I have created a Jira issue and added the issue ID to my pull request title. - [check] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [] I have developed this patch against the `master` branch. - [check] I have run `ant precommit` and the appropriate test suite. - [] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","rahulgoswami","2019-06-24T06:49:07Z","2021-12-08T13:23:48Z"
"","1515","Lucene-9336: Changes.txt addition for RegExp enhancements","9.0 enhancement added to changes","closed","","markharwood","2020-05-14T09:35:34Z","2020-05-14T10:52:01Z"
"","738","SOLR-13532: Fix for non-recovering cores due to low timeouts","15000 timeout is much better than 1000, since 1 second is too low for most clusters","closed","","surilshah","2019-06-21T16:48:49Z","2019-09-16T00:23:19Z"
"","737","SOLR-13532: Fix for non-recovering cores due to low timeouts","15000 timeout is much better than 1000, since 1 second is too low for most clusters","closed","","surilshah","2019-06-21T16:30:23Z","2019-09-16T00:23:07Z"
"","736","SOLR-13532: Fix for non-recovering cores due to low timeouts","15000 timeout is much better than 1000, since 1 second is too low for most clusters","closed","","surilshah","2019-06-21T16:25:19Z","2019-09-16T00:22:43Z"
"","1307","LUCENE-8962: Fix intermittent test failures","1. TestIndexWriterMergePolicy.testMergeOnCommit will fail if the last    commit (the one that should trigger the full merge) doesn't have any    pending changes (which could occur if the last indexing thread    commits at the end). We can fix that by adding one more document    before that commit. 2. The previous implementation was throwing IOException if the commit    thread gets interrupted while waiting for merges to complete. This    violates IndexWriter's documented behavior of throwing    ThreadInterruptedException.   * SOLR-####:   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.  Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->   # Description  This fixes intermittent test failures related to the previous commit on LUCENE-8962.  # Solution  There were two separate bugs in the previous commit:  1. TestIndexWriterMergePolicy.testMergeOnCommit could sometimes fail the last assertion, because the final commit in the test method triggered no merges. This could happen if multiple indexing threads committed after adding their last documents. To guarantee that the final commit in the test method triggers a merge, we can add one more document (so there is a change to commit). 2. TestIndexWriter. testThreadInterruptDeadlock verifies IndexWriter's documented behavior of throwing ThreadInterruptedException when interrupted. The previous commit for LUCENE-8962 violated this behavior. This commit fixes that.  # Tests  After applying these fixes, I have run both TestIndexWriter and TestIndexWriterMergePolicy multiple times with previously-failing seeds and random seeds, and have not seen the test failures occur again.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","msfroh","2020-03-02T22:11:26Z","2020-03-02T23:29:13Z"
"","1645","SOLR-14624: Improvements to waitFor* handling","1. CollectionsHandler#waitForActiveCollection() to only check replica counts 2. Remove polling from OCMH#waitForCoreNodeName(), OCMH#waitForNewShard(), OCMH#waitToSeeReplicasInState()","closed","","chatman","2020-07-03T18:01:30Z","2020-07-13T23:07:28Z"
"","1662","Harden TestBuildingUpMemoryPressure","1. Add specific checks for exception message expectation. 2. Ensure that a non triggering value is returned when the fake circuit breaker should not be tripping.  Also adds temporary logging for debugging, to be removed.","closed","","atris","2020-07-10T04:55:22Z","2020-07-13T09:33:00Z"
"","1288","SOLR-14281: Make sharedLib configurable through SysProp","...and allow multiple comma separated paths  See https://issues.apache.org/jira/browse/SOLR-14281","closed","","janhoy","2020-02-25T23:15:19Z","2020-02-27T13:16:38Z"
"","1027","SOLR-13955: Fixed issues with SHARED core descriptor ZK discovery","-Handled a case where replicas for a node can be null. -Prefer core loading over discovery errors. -Make sure discovered core descriptor is persisted. -Added missing properties. Unit test to ensure that we do not miss anything. -Added test to index on discovered core. -Added a fail fast for indexing pipeline when processing a non-shared replica for shared collection -Removed the obsolete comments from CorePullTask#createCore. -Renamed SimpleSharedStoreMissingCorePullTest.java to SharedStoreMissingCoreTest.java","closed","","mbwaheed","2019-11-21T22:20:25Z","2019-11-22T01:35:41Z"
"","1065","SOLR-13101: Don't invoke push to shared store for isolated commits.","-Don't invoke push to shared store for isolated commits. -Disable the noisy ""going back to task queue"" log line.","closed","","mbwaheed","2019-12-07T00:00:50Z","2019-12-12T18:44:47Z"
"","1081","SOLR-13101: Concurrency tests for SHARED collection.","-Concurrency tests for SHARED collection. -On core container shutdown, shutdown the background blob delete manager too. -Use correct casing for SharedCoreStage enum. -Added INDEXING_BATCH_FINISHED to indicate the end of a batch (BLOB_PUSH_FINISHED does not help if push itself runs into error)","closed","","mbwaheed","2019-12-14T01:42:34Z","2019-12-21T14:38:27Z"
"","922","SOLR-13793: limit number of remote queries to number of shard replicas","- Limit number of remote queries to number of shard replicas when considering inactive replicas - Unchecked remote queries from HttpSolrCall lead to mutual recursive calls to remote node","closed","","kesharee","2019-10-04T07:28:13Z","2020-06-07T12:29:01Z"
"","1314","LUCENE-9136: Coarse quantization that reuses existing formats.","**Note:** this PR is just meant to sketch out an idea and is not meant for detailed review.  This PR shows a kNN approach based on coarse quantization (IVFFlat). It adds a new format `VectorsFormat`, which simply delegates to `DocValuesFormat` and `PostingsFormat` under the hood: * The original vectors are stored as `BinaryDocValues`. * The vectors are also clustered, and the cluster information is stored in postings format. In particular, each cluster centroid is encoded to a `BytesRef` to represent a term. Each document belonging to the centroid is added to the postings list for that term.  Given a query vector, we first iterate through all the centroid terms to find a small number of closest centroids. We then take the disjunction of all those postings enums to obtain a DocIdSetIterator of candidate nearest neighbors. Finally we score each candidate by loading its vector from BinaryDocValues and computing the distance to the query vector.  There are currently some pretty big hacks: * We re-use the existing doc values and postings formats for simplicity. This is fairly fragile since we write to the same files as normal doc values and postings -- I think there would be a conflict if there were both a vector field and a doc values field with the same name.  * To write the postings list, we compute the map from centroid to documents in memory. We then expose it through a hacky `Fields` implementation called `ClusterBackedFields` and pass it to the postings writer. It would be better to avoid this hack and not to compute cluster information using a map.","open","","jtibshirani","2020-03-04T00:23:36Z","2020-04-03T20:34:06Z"
"","1125","LUCENE-9096: Implementation of CompressingTermVectorsWriter.flushOffsets can be simpler","**Description** In CompressingTermVectorsWriter.flushOffsets, the calculation of sumPos and sumOffsets is a little redundant  **Solution** Simplify the process.  **Tests** I have no idea how to test it, I‘m really appreciate anyone giving me ideas. I have compared before&after, the result is same.  **Checklist** Please review the following and check all that apply: - [x] reviewed the guidelines for How to Contribute and my code conforms to the standards described there to the best of my ability. - [x]  I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers access to contribute to my PR branch. (optional but recommended) - [x]  I have developed this patch against the master branch. - [x]  I have run ant precommit and the appropriate test suite. - [ ]  I have added tests for my changes. - [ ]  I have added documentation for the Ref Guide (for Solr changes only).","closed","","kkewwei","2019-12-26T04:57:48Z","2020-01-06T09:15:15Z"
"","1196","SOLR-14132: Upgrade Angular JS 1.3.8 to 1.7.9","* Upgrade Angular JS 1.3.8 to 1.7.9 * Upgrade Angular Chosen v1.3.0 and Chosen to v1.8.7 * Remove older jquery 1.7.2 version * Remove non minified Angular JS files  Signed-off-by: Kevin Risden","closed","","risdenk","2020-01-21T21:56:03Z","2020-01-23T14:21:24Z"
"","1077","SOLR-14069: Ref guide: overhaul: resources, libs, plugins, config-sets","* split ""resource-and-plugin-loading.adoc"" into ""resource-loading.adoc"" and ""libs.adoc"" then overhauled both. * enhanced ""config-sets.adoc"", moving some content in from elsewhere; bit of an overhaul. * solr-plugins.adoc is now top-level; overhauled content  Did not touch plugin management stuff or old blob stuff.","closed","","dsmiley","2019-12-12T06:09:42Z","2020-03-26T16:25:53Z"
"","1306","SOLR-14299: IndexFetcher doesnt' reset count to 0 after the last packet is received","* SOLR-14299: IndexFetcher should reset the `errorCount` to 0 after successfully receive the last packet while fetching the file.  # Description While fetching the files from master `IndexFetcher` retries 5 times before giving up. It resets the  errorCount after successfully receiving the packet except for the last packet. Seems like an oversight.   # Solution  Reset the errorCount to 0 before verifying if it is last packet for the while.  # Tests This is a trivial change, no test cases added for now  # Checklist Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","praste","2020-03-02T18:46:03Z","2020-03-26T22:41:59Z"
"","695","backporting SOLR-13504 & SOLR-13329","* SOLR-13504: In autoscaling policies, use an explicit 'nodeset' attribute for filtering   nodes instead of using them directly at the toplevel (Generally available from SOLR 8.2)(noble) * SOLR-13329: In autoscaling policies, use an explicit 'put : on-each'   to specify the the rules is applied on each node (Generally available from SOLR 8.2)(noble)","closed","","noblepaul","2019-06-03T12:36:08Z","2020-01-07T19:31:26Z"
"","1524","SOLR-15185: Rewrite Hash query","* Simpler * Don't use Filter (to be removed) * Do use TwoPhaseIterator, not PostFilter * Don't pre-compute matching docs (wasteful) * Support more fields, and more field types * Faster hash on Strings (avoid Char conversion) * Stronger hash when using multiple fields  ---- @joel-bernstein, I set out to remove the use of Filter in `{!hash}` (because https://issues.apache.org/jira/browse/SOLR-12336 ) and then I saw a bunch of things that I felt could be improved.  Apparently I had nothing better to do this weekend 😄   j/k     WDYT?","closed","","dsmiley","2020-05-18T06:23:15Z","2021-03-05T04:43:23Z"
"","1647","SOLR-14626: General improvements to coreContainer shutdown","* Removing unnecessary ""preClose()"" * Other changes","closed","","chatman","2020-07-03T18:33:25Z","2020-07-13T23:07:39Z"
"","1646","SOLR-14625: Leader election improvements","* Removing a 2500ms wait.","closed","","chatman","2020-07-03T18:19:12Z","2020-07-13T23:07:34Z"
"","1066","SOLR-13987: fix admin UI to not rely on javascript eval()","* Removes `'unsafe-eval'` from CSP `script-src` * Enables Angular CSP mode * Removes `eval()` JSON parsing in `cloud.js` * Removes `jstree` `themes` error","closed","","risdenk","2019-12-07T02:29:48Z","2019-12-07T21:52:55Z"
"","739","LUCENE-8871: promote kuromoji tools to main jar","* move Kuromoji tools to main build path ** code cleanup When doing this, it became necessary to bring the code up to a different standard to satisfy forbidden-apis, so I took an opportunity to modernize it and follow various suggestions from IntelliJ. *** replace use of java.io.File with java.nio *** change the internal file-related APIs to use Path instead of String *** Use try-with-resources for all the streams and readers *** Sprinkle streams and lambdas around Just to make the code briefer, and IMO easier to read *** removed print statements *** reduced visibility of methods as much as possible ** remove build-tools stuff from build.xml ** functional changes *** exception when an invalid format is specified instead of defaulting to unidic *** exception when a bad dictionary entry is encountered instead of printing a message and soldiering on ** testing *** precompile, test *** also built dictionary verified files are unchanged except for FST version number","closed","","msokolov","2019-06-23T10:38:58Z","2019-06-27T02:34:02Z"
"","1209","SOLR-14209: Upgrade JQuery to 3.4.1","* JQuery 2.1.3 to 3.4.1 * jstree 1.0-rc1 to v3.3.8  Signed-off-by: Kevin Risden","closed","","risdenk","2020-01-25T03:38:48Z","2020-02-08T17:59:52Z"
"","764","SOLR-13538: Fix classcastEx in TrieDateField for atomic updates","* get the string from charsequence and avoid casting","closed","","munendrasn","2019-07-05T04:27:25Z","2019-09-28T09:25:19Z"
"","1083","SOLR-13662: Fixes to package manager","* Better logging and error reporting     * Fixing deploy command to handle previously undeployed packages     * Test now uses @LogLevel annotation     * Deploy command had a hard coded collection name by mistake, fix it","closed","","chatman","2019-12-14T08:46:19Z","2020-11-10T22:32:07Z"
"","1076","SOLR-14048: Improve Hadoop test sanity checks","* Added javadoc to `org.apache.hadoop` package * Added a check to ensure that modified Hadoop classes are picked up before testing","closed","","risdenk","2019-12-12T04:11:05Z","2019-12-13T03:26:55Z"
"","1264","LUCENE-9211 Add compression for Binary doc value fields (#1234)","(cherry picked from commit ce2959fe4cb1d1e77df04464c46004bf7846f6b5)","closed","","markharwood","2020-02-18T14:19:22Z","2020-02-18T14:40:45Z"
"","1260","SOLR-13669: DIH: Add System property toggle for use of dataConfig param","(cherry picked from commit 325824cd391c8e71f36f17d687f52344e50e9715)  Addresses [CVE-2019-0193](https://nvd.nist.gov/vuln/detail/CVE-2019-0193) / [SOLR-13669 ](https://issues.apache.org/jira/browse/SOLR-13669)  Backport from Solr 8.1.2  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2020-02-14T18:59:15Z","2020-02-18T17:24:38Z"
"","1522","LUCENE-9374: Add checkBrokenLinks gradle task","## Description  ~This adds `documentationLint` task to gradle build.~ This adds `checkBrokenLinks` task to gradle build. The task is a placeholder for `:lucene:checkBrokenLinks` and `:solr:checkBrokenLinks` that detect broken or invalid links in docs under `_project_/build/documentation`.  For now, both check tasks fail as there are broken links in the javadocs.  See also https://issues.apache.org/jira/browse/LUCENE-9374  ## Gradle command  ``` ./gradlew checkBrokenLinks ```  or   ``` ./gradlew :lucene:checkBrokenLinks ./gradlew :solr:checkBrokenLinks ```  ## Note  - I added `documentationLint` task that wraps `checkBrokenLinks` to align semantics with `documentation` task, but it can be removed if not needed. - `documentation-lint.gradle` could be placed under `gradle/documentation` ?","closed","","mocobeta","2020-05-16T09:24:33Z","2020-05-20T14:23:25Z"
"","1304","LUCENE-9242: generate javadocs by calling Ant javadoc task","## Description  Draft PR that adds a gradle task to generate javadocs by invoking Ant javadoc task. All generated javadocs passed ""checkMissingDocs"" check.  Related Jira: https://issues.apache.org/jira/browse/LUCENE-9242","closed","","mocobeta","2020-03-01T19:47:13Z","2020-03-12T11:16:11Z"
"","974","[SOLR-13865] Migrate replica routing code to SolrJ","# Tests  Tests have been migrated from solr core.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-10-24T21:19:58Z","2019-10-29T16:14:54Z"
"","1413","The check against null and usage of the field updateThread is typically protected by synchronization, but not in 1 location.","# Issue Report on JIRA  I also reported this issues on JIRA at:  https://issues.apache.org/jira/browse/LUCENE-9306  # Bug Description  Checks against `null` and usages of the field `updateThread`  are typically made atomic by synchronization on `this` (synchronized methods), e.g., at lines: [339-341](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L339-L341), [357-358](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L357-L358), and [380-381](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L380-L381).   However, the `null` check at [line 387](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L387) and the usage at [line 388](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L388) are not protected by `synchronized`.  ```java public String toString() {   String res = ""ReplicationClient"";   if (updateThread != null) { <<<<<<<<<<<<<<<<<<<<<     res += "" ("" + updateThread.getName() + "")""; <<<<<<<<<<<<<<<<< ```  This check against `null` and usage of `updateThread` are in `toString()`.  However, the problem is not that the `toString()` will give a garbled string (i.e., a relatively minor issues).  The problem is that, in between the `null` check and the usage, `updateThread` can be set to `null` (e.g., by [line 369](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L369)) and therefore the code can crash.  I.e., without `synchronized`, the `null` check does not protect the `updateThread.getName()` usage.  I don't know how `toString()` is called concurrently.  However, it seems like a dangerous assumption to make that the callers of `toString()` know it should not be called concurrently with [line 369](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L369), especially as the other methods do protect with synchronization the `null` check and usage.  # This Patch's Code  The fix is very simple: just make the method containing [lines 387-388](https://github.com/apache/lucene-solr/blob/531015245042507d71845b6d584e7e7389303093/lucene/replicator/src/java/org/apache/lucene/replicator/ReplicationClient.java#L387-L388) `synchronized`, just like the other methods containing `null` checks and usages of `updateThread`.  ```java public synchronized String toString() { <<<<<<<<<<<<<< added ""synchronized"" here   String res = ""ReplicationClient"";   if (updateThread != null) {     res += "" ("" + updateThread.getName() + "")""; ```","open","","adriannistor","2020-04-06T19:04:06Z","2020-04-06T20:37:02Z"
"","1064","LUCENE-9084: circular synchronization wait (potential deadlock) in AnalyzingInfixSuggester","# Issue Report On JIRA  I also reported this at:  https://issues.apache.org/jira/browse/LUCENE-9084#  # Bug description  Detailed code (snippets and links) are in the sections after this overview (section **Detailed Code** and **This Patch's Code**).  Method `ensureOpen()` is `synchronized` (acquires `this`) and its body contains a `synchronized (searcherMgrLock)` block (i.e., then acquires `searcherMgrLock`).  Method `ensureOpen()` is called two times from public methods `add()` and `update()`.  A thread calling public methods `add()` or `update()` will acquire locks in order:  ```java this -> searcherMgrLock ```  Public method `build()` has a `synchronized (searcherMgrLock)` block in which it calls method `add()`.  Method `add()`, as described above, calls method `ensureOpen()`.  Therefore, a thread calling public method `build()` will acquire locks in order:  ```java searcherMgrLock -> this -> searcherMgrLock ```  2 threads can acquire locks in different order which may cause a circular wait (deadlock).  I do not know which threads call these methods, but there is a lot of synchronization in these methods and in this file, so I think these methods must be called concurrently.  One thread can acquire:  `this -> searcherMgrLock` (the first order above)  and the other thread can acquire:  `searcherMgrLock -> this` (the second order above).  Note how the above 2 orders lead to a circular wait.  # Detailed Code  Method `ensureOpen()` is `synchronized` and its body contains a `synchronized (searcherMgrLock)`:  ```java   private synchronized void ensureOpen() throws IOException { <<<<<<<<<<< see the synchronized keyword     if (writer == null) {       if (DirectoryReader.indexExists(dir)) {         // Already built; open it:         writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.APPEND));       } else {         writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.CREATE));       }       synchronized (searcherMgrLock) { <<<<<<<<<<<<<<<<<<<<<<< ```  https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L371-L379  Method `ensureOpen()` is called two times from public methods `add()` and `update()`:  ```java   public void add(BytesRef text, Set contexts, long weight, BytesRef payload) throws IOException {     ensureOpen(); <<<<<<<<<<<<<<<<<<<<<< ```   https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L394-L395  ```java   public void update(BytesRef text, Set contexts, long weight, BytesRef payload) throws IOException {     ensureOpen(); <<<<<<<<<<<<<<<<<<<< ```  https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L406-L407   Public method `build()` has a `synchronized (searcherMgrLock)` block in which it calls method `add()`:   ```java   @Override   public void build(InputIterator iter) throws IOException {          synchronized (searcherMgrLock) { <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<       if (searcherMgr != null) {         searcherMgr.close();         searcherMgr = null;       }        if (writer != null) {         writer.close();         writer = null;       }        boolean success = false;       try {         // First pass: build a temporary normal Lucene index,         // just indexing the suggestions as they iterate:         writer = new IndexWriter(dir,             getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.CREATE));         //long t0 = System.nanoTime();          // TODO: use threads?         BytesRef text;         while ((text = iter.next()) != null) {           BytesRef payload;           if (iter.hasPayloads()) {             payload = iter.payload();           } else {             payload = null;           }            add(text, iter.contexts(), iter.weight(), payload);  <<<<<<<<<<<<<<<<<<<<< ```  https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L278-L310  Method `add()` is the same one I linked above.  # This Patch's Code  Note that method `ensureOpen()` (inlined above) is the **only** place (method or synchronization block) that is `synchronized` on `this`.  **All** the other synchronizations in this file are on `searcherMgrLock`.  This CR removes the `synchronized` on `this` (again, being the only `synchronized` on `this`, we can do this change safely).  And moves `synchronized (searcherMgrLock)` a few lines above,  to protect the entire code (that otherwise was protected by `synchronized` on `this`).  The above breaks the lock cycle I described earlier.  The fix looks big because it changes indentation.    But only one line is moved by a few lines up.  I.e., from this:  ```java   private synchronized void ensureOpen() throws IOException {     if (writer == null) {       if (DirectoryReader.indexExists(dir)) {         // Already built; open it:         writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.APPEND));       } else {         writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.CREATE));       }       synchronized (searcherMgrLock) { <<<<<<<<<<<<<<<<<<<<<< move from here         SearcherManager oldSearcherMgr = searcherMgr;         searcherMgr = new SearcherManager(writer, null);         if (oldSearcherMgr != null) {           oldSearcherMgr.close();         }       }     }   } ```  To this:  ```java   private void ensureOpen() throws IOException { <<<<<<<<<<<<<<<<<< remove synchronized --- can lead to circular wait --- and legal to remove     synchronized (searcherMgrLock) { <<<<<<<<<<<<<<<<<<<<< move to here       if (writer == null) {         if (DirectoryReader.indexExists(dir)) {           // Already built; open it:           writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.APPEND));         } else {           writer = new IndexWriter(dir, getIndexWriterConfig(getGramAnalyzer(), IndexWriterConfig.OpenMode.CREATE));         }          SearcherManager oldSearcherMgr = searcherMgr;         searcherMgr = new SearcherManager(writer, null);         if (oldSearcherMgr != null) {           oldSearcherMgr.close();         }       }     }   } ```  Here are all the places where `synchronized (searcherMgrLock)` appears in this file (and again, no other `synchronized` on other objects is done):  - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L281-L332 - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L379-L385 - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L654-L657 - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L870-L873 - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L898-L901 - https://github.com/apache/lucene-solr/blob/master/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L926-L929  I.e., doing the synchronization like above is safe and consistent with the rest of the file.","closed","","paulward24","2019-12-06T21:41:33Z","2021-12-08T16:32:40Z"
"","952","SOLR-12786: update Ref Guide build tooling versions","# Description Upgrading tool versions was held back by needing to fix several headings since Asciidoctor changed the way they construct automatic section ids in the 1.5.7 release. Other tooling had also fallen behind.  # Solution  - Updated asciidoctor-ant version to 1.6.2 for the PDF build. - Updated `solr-ref-guide/README.adoc` to reflect latest versions of the tools. -- Since Asciidoctor 2.0.10 is installed by default with jekyll-asciidoc, removed the requirement to install Asciidoctor separately. -- jekyll-asciidoc version updated to 3.0.0. -- dependencies to install slim, tilt, and concurrent-ruby (replaced thread_safe) added. - Removed pygments.rb as a dependency and replaced it with the rouge highlighter which comes with Jekyll out of the box. - Fixed several pages which failed validation after updating the asciidoctor version. - Updated the Jenkins build script with new supported versions. To prevent nasty surprises in the future, I kept the gem installs to force installation of specific versions, but updated those versions to the latest.  Since there are significant changes in Jekyll 4.x, I did not at this time upgrade that part of the HTML build tooling so v3.5.0 is still required (possible we could go to 3.8.x but I didn't look at that either).  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ctargett","2019-10-14T20:39:00Z","2019-11-07T21:23:59Z"
"","1379","SOLR-14363: Separate /get requests into their own type designation","# Description The SolrLogParseTool parses most log records containing the string ""QTime="" as being a query. While this is useful, the designation would be a little more helpful if operations with a different performance profile were broken out into their own ""type_s"" value. That gives users the ability to view them together or separately, as they prefer.  # Solution  Change the parsing code in SolrLogParseTool to check specifically for ""/get"" requests when parsing query requests.  Other than settings ""type_s"" and ""id_s"" all other fields are handled in a way identical to other queries  # Tests A unit test for the new ""rtg"" type_s value.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-03-25T18:26:31Z","2020-04-01T12:41:59Z"
"","1058","SOLR-13972: Warn about insecure settings on startup","# Description Solr should warn users on startup if insecure settings are being used.  # Solution Currently I've added the detection and message for this in bin/solr.  I'm not sure this is the best place for this logic - I'd like to keep logic out of bash/powershell where possible, and a smarter check would look at security.json in ZooKeeper, which is easier to do within the JVM.  But this PR is still worth reviewing if anyone has feedback on the language or appearance of the warning.  # Tests  I've done manual testing, but that's it.  If we had a `bin/solr` test framework as SOLR-11749 proposes, we could conceivably write tests for this, but that's a major task and isn't going to happen anytime soon.","closed","","gerlowskija","2019-12-05T14:30:09Z","2019-12-11T12:16:15Z"
"","1470","SOLR-14354: HttpShardHandler send requests in async","# Description Sending request in an async way inside HttpShardHandler","closed","","CaoManhDat","2020-04-30T08:54:09Z","2020-07-07T02:17:41Z"
"","1148","LUCENE-9080: Upgrade ICU4j to 62.2 and regenerate","# Description See comments on the JIRA","closed","","ErickErickson","2020-01-06T17:04:32Z","2020-01-12T22:14:25Z"
"","1343","LUCENE-8103: Use two-phase iteration in Query- and DoubleValuesSource","# Description QueryValueSource (in ""queries"" module) is a ValueSource representation of a Query; the score is the value. It ought to try to use a TwoPhaseIterator from the query if it can be offered.  # Solution Always check whether a TwoPhaseIterator can be offered, and use it. Since its `matches()` function may only be called once, while there is no restrictions on QueryValueSource's or DoubleValuesSource's method calling, I unfortunately had to add some state.  # Tests  For each value source, I added a test case that makes use of a query that serves a TwoPhaseIterator. I don't think that's the best solution, but was not sure about how to solve this differently.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","micpalmia","2020-03-11T18:33:38Z","2020-03-15T15:51:07Z"
"","1398","SOLR-13101: Every BLOB_PULL_STARTED should have a matching BLOB_PULL_FINISHED irrespective of failures","# Description If exception is thrown during core pull from blob store, there is no record of the pull finishing. This caused a failure in SharedCoreConcurrencyTest.testIndexingQueriesDeletes() when it identified a BLOB_PULL_STARTED without a matching BLOB_PULL_FINISHED, meaning the pulls are interleaved. We want to record pull as finished upon completion even if it was unsuccessful.   # Solution Move recording the state the pull as finished finally block so it will record pull as finished irrespective of failures.  # Tests Reproduced failure by throwing an exception in pull code and verified that code change fixes the interleaved pull failure. Ran all tests.","closed","","ebehrendt","2020-04-02T23:33:36Z","2020-04-06T17:00:36Z"
"","1404","SOLR-14387: SolrClient.getById() does not escape parameter separators within ids","# Description Having a solr document with a comma in its id (e.g. ""A,B""), SolrClient.getById() is not able to retrieve this document, because it queries `/get?ids=A,B` instead of `/get?ids=A\,B`.  # Solution Fix SolrClient to escape the parts of the `ids` parameter properly  # Tests Adds testcases for documents with separators in their id field to `GetByIdTest`.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] ~I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).~","closed","","schuch","2020-04-04T12:21:10Z","2020-04-16T17:00:52Z"
"","1017","LUCENE-9051: random access for IndexedDISI","# Description Enables random access over DocValues via its existing advanceExact method. With this change, callers may provide docids in any order rather than having to enforce that they never decrease over subsequent calls to the iterator API.","open","","msokolov","2019-11-18T11:48:16Z","2019-11-18T11:48:16Z"
"","1015","SOLR-13936 : expose endpoint to support changing schema without collection","# Description Current apis do not allow modifying a configset if it is not associated with a collection/core For cases where a configset is created, modified and then used this is necessary  *NOTE : Since SolrConfig object doesn't have a listener on zk, if you change it via configset endpoints, a core reload will be required to apply effects* # Solution - Make current apis directly dependent on `SolrResourceLoader` and `ManagedSchema/SolrConfig` instead of `SolrCore` - Expose endpoints on `/api/cluster` level  # Tests Tests for 1. Config - get config - add config on overlay - add config on params - add request handler 2. Schema - get schema - add field to schema  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","apoorvprecisely","2019-11-16T06:44:09Z","2019-12-06T14:25:40Z"
"","1235","LUCENE-8929: parallel early termination sharing counts and max scores across leaves","# Description Briefly, introduces a new object for sharing scores and counts in `TopFieldCollector` in order to get earlier, but still accurate, termination. See the Jira for more dicsussion","closed","","msokolov","2020-02-03T21:28:17Z","2020-12-30T21:04:21Z"
"","1362","SOLR-13768 Remove requiresub parameter in JWTAuthPlugin","# Description As per ticket [SOLR-13768](https://jira.apache.org/jira/browse/SOLR-13768) we need to remove support for deprecated 'requireSub' parameter in JWTAuthPlugin  # Solution Removed the parameter PARAM_REQUIRE_SUBJECT used in JWTAuthPlugin.  # Tests Ran the full test suite using ant test.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","rabi-kumar","2020-03-18T16:24:07Z","2021-05-06T08:46:54Z"
"","841","SOLR-13707: API to expose the currently used package name, details for each plugin","# Description API to expose the currently used package name, details for each plugin","closed","","noblepaul","2019-08-22T06:09:28Z","2019-11-08T01:16:24Z"
"","1128","SOLR-14153 Word choice should be ""starting"", not ""staring""","# Description  Wrong word slipped in, but is properly spelled so autocorrect won't see it.  # Solution Fixed docs.  # Tests n/a  # Checklist  Please review the following and check all that apply:  - [X ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [X ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-12-30T01:55:51Z","2019-12-30T11:50:55Z"
"","1210","SOLR-14219 force serialVersionUID of OverseerSolrResponse","# Description  When the useUnsafeOverseerResponse=true option introduced in SOLR-14095 is used, the serialized OverseerSolrResponse has a different serialVersionUID to earlier versions, making it backwards-incompatible.  # Solution  This PR forces the serialVersionUID of OverseerSolrResponse to be the same value as before.  # Tests  Tested in a prototype environment with a mixed 8.4.1 and master-branch Solr nodes.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2020-01-26T16:42:31Z","2020-02-04T19:43:49Z"
"","1574","SOLR-14566: Add request-ID to all distrib-search requests","# Description  When reading through logs, associating the log message from the coordinator node with the messages for per-shard requests downstream can be difficult.  Nothing unique (or semi-uniquely) identifies shard requests as being part of an upstream request.   # Solution  This PR moves some logic out of DebugComponent that generates a guarantee-ably unique request ID.  On coordinator nodes this value is logged out as a special field in the request log message.  On downstream shard requests it appears as ""rid"" in the parameter list.  # Tests  Manual testing to ensure the log field is added in appropriate situations.  Some changes to DebugComponent to account for its new role as a consumer of ""rid"" only.  Looked to update SearchHandlerTest but couldn't think of a non-hacky way to test this.  Anyone has any ideas please lmk.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-06-12T16:41:13Z","2020-07-08T12:38:31Z"
"","930","SOLR-13823: ClassCastException while grouping with group.query","# Description  When grouping with group.query and returning the score there is a class cast exception in Grouping.CommandQuery  # Solution  This PR shows the issue with a failing test. The solution is to use the needed collector directly as it is available in CommandQuery and do not cast at all. Also you need to populate the scores. I will add another commit with these changes.  # Tests  I added a test to TestGroupingSearch.testGroupAPI that groups by query and returns the score.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","uwej711","2019-10-07T06:57:14Z","2019-10-12T18:07:39Z"
"","943","SOLR-13823: ClassCastException while grouping with group.query (master)","# Description  When grouping with group.query and returning the score there is a class cast exception in Grouping.CommandQuery  # Solution  The solution is to use the needed collector directly as it is available in CommandQuery and do not cast at all. Also you need to populate the scores.   # Tests  I added a test to TestGroupingSearch.testGroupAPI that groups by query and returns the score. There is also a distributed group test.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","uwej711","2019-10-12T18:05:43Z","2019-10-29T08:58:31Z"
"","1091","LUCENE-9098 Report bad term for fuzzy query, SOLR-13190 Surface Fuzzy term errors in Solr","# Description  When a fuzzy query encounters a term that is too complex, the exception should report the term instead of a cryptic message about too many states. This supersedes #554  # Solution  Catch the exception and regrow it with term information present.  # Tests  Added tests which generate long terms and feed them to fuzzy query.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","madrob","2019-12-17T16:30:12Z","2020-01-02T18:43:59Z"
"","1152","SOLR-14172: Collection metadata remains in zookeeper if too many shards requested","# Description  When a Collection CREATE command fails because of too many requested shards error the collection metadata remains in ZooKeeper  # Solution  Delete the metadata from zookeeper if this error happens.  # Tests  Created a new unit tests which fails against master but works after this modification.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","asalamon74","2020-01-07T12:31:00Z","2020-01-23T09:50:10Z"
"","1137","SOLR-14162 TestInjection can leak Timer objects","# Description  We track the outstanding Timer but try to remove TimerTask which will never be found in the collection.  # Solution  Add and remove the same Timer instances.  # Tests  The javac warning goes away.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","madrob","2020-01-03T19:11:50Z","2020-01-24T20:04:26Z"
"","1163","SOLR-14186: Enforce CRLF in Windows files with .gitattributes","# Description  We have nothing that ensures that our Windows files (solr.cmd, etc.) keep the CRLF files they need to run.  # Solution  Introduces a .gitattributes file right under the solr/ directory to ensure that our various OS-specific files are handled correctly.  # Tests Manual testing on Windows and Linux checkouts to make sure that CRLF is retained in our Windows-specific files.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-13T15:55:05Z","2020-01-22T13:19:20Z"
"","838","SOLR-13705 Double-checked locking bug is fixed.","# Description  Using double-checked locking for the lazy initialization of any other type of primitive or mutable object risks a second thread using an uninitialized or partially initialized member while the first thread is still creating it, and crashing the program.  # Solution  Double-checked locking bug is fixed to prevent it.  # Tests  No need to write extra tests. Existing tests should not fail after this implementation.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","kamaci","2019-08-20T12:31:55Z","2020-06-27T11:55:04Z"
"","984","SOLR-12217: Support shards.preference for individual shard requests","# Description  Use shard preference rules when choosing replicas to send individual shard requests. This includes requests from SolrJ as well as requests sent via streaming expressions.  # Solution How routing preferences are determined:  For Streaming Expressions, it will use the preferences from the following sources, in order of preference (defaults last): 1. The params given in the stream source 1. The URL params sent in the streaming request 1. The `DEFAULT_SHARD_PREFERENCES` set in cluster properties  For SolrJ, it will use the preferences from the params of the request it is sending.  # Tests  Current tests pass.  **TODO** Add tests for routing functionality  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-10-30T16:11:40Z","2019-12-09T22:12:04Z"
"","1383","SOLR-14367: Updated Tika version to 1.24","# Description  Upgrade Apache Tika to new released 1.24 to handle [CVE-2020-1950](https://nvd.nist.gov/vuln/detail/CVE-2020-1950).  See https://issues.apache.org/jira/browse/SOLR-14367   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes.","open","","mibo","2020-03-27T08:42:42Z","2020-03-27T08:42:49Z"
"","1006","SOLR-13925: including resource directory in solrj pom template","# Description  Updating the pom template for the solrj module to include resources such as clusterstate.json.   # Solution  See description.  # Tests  No unit tests, wasn't evident how I could include a test of the maven artifacts.   # Checklist  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","jdeolive","2019-11-12T21:39:30Z","2021-10-28T13:34:12Z"
"","935","LUCENE-4056: Japanese Tokenizer (Kuromoji) cannot build UniDic dictionary","# Description  UniDic has a bit different column and unk.def from ipadic. Currently if we use UniDic, then we get some error for building dictionary. We can build UniDic after changing build.xml. I left some comments in build.xml.  If we use UniDic, we should change stoptags.txt and also add an example for part of speech filter. UniDic has different part of speech tags from ipadic.  # Solution  Added some logic for adjusting UniDic csv and unk.def. And also changing build.xml for downloading and building UniDic.  # Tests  Unfortunately there is no tests for DictionaryBuilder.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","johtani","2019-10-09T15:30:46Z","2020-06-12T09:08:17Z"
"","956","SOLR-13848 tweak documentation to be clearer about uprefix","# Description  Tweak How to use Solr Cell doc  # Solution  Simplifying parameters in initial example query.  # Tests  Tested in Solr 8.2  # Checklist  Please review the following and check all that apply:  - [x ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x ] I have created a Jira issue and added the issue ID to my pull request title. - [x ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-10-15T20:41:25Z","2019-10-21T03:50:43Z"
"","1009","SOLR-13926","# Description  Trying to improve documentation around mystical classes  # Solution  javadoc  # Tests  Committers eyes  # Checklist  Please review the following and check all that apply:  - [ x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x ] I have created a Jira issue and added the issue ID to my pull request title. - [x ] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x ] I have developed this patch against the `master` branch. - [x ] I have run `ant precommit` and the appropriate test suite. - [Z ] I have added tests for my changes. - [X ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","millerjeff0","2019-11-13T20:15:09Z","2019-12-03T22:44:32Z"
"","966","LUCENE-9024 Optimize IntroSelector to use median of medians","# Description  Today, `IntroSelector.slowSelect` falls back to a heap sort; This PR moves that implementation to use a median of medians approach, suggested by a TODO in the code.   # Solution  Implemented a median of medians algorithm.  # Tests  I didn't add any new tests for this as I believe the current test cases, `TestIntroSelector`, should cover the basic functionality. However, one thing I'd like input on: Today, `select(int from, int to, int k)`, the `to` parameter is _exclusive_. Since it's also a specified parameter in the test, the test never checks the case where the full array should be used. `doTestSelect` has a call to `Arrays.sort(expected, from, to);`, but `to` will never be the last index in the array because the size of the array is allocated to `from + to + random().nextInt(5)`, so the actual size of the array will be a minimum of `from + to`.  I'd be happy to address this problem in this PR, or in a subsequent PR if that's preferred.   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","pcsanwald","2019-10-23T00:43:58Z","2019-10-29T08:21:53Z"
"","814","SOLR-13666: Pull Request Template to sign-post to the Solr Ref Guide source","# Description  To make the ""I have added documentation for the Ref Guide (for Solr changes only)"" checklist item slightly more easy to action-and-tick-yes.  # Solution  Include a sign-posting link to where the ref guide source can be found and how it may be edited.","closed","","cpoerschke","2019-07-31T11:13:32Z","2019-08-02T17:34:12Z"
"","1207","LUCENE-9169: Add Github Workflow for Gradle Wrapper Validation","# Description  This workflow validates the checksums of Gradle Wrapper JAR files present in the source tree and fails if unknown Gradle Wrapper JAR files are found.  # Solution  See https://github.com/marketplace/actions/gradle-wrapper-validation  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch.","closed","","eskatos","2020-01-24T17:45:20Z","2020-01-25T12:04:15Z"
"","1220","SOLR-13996: Refactor HttpShardHandler.prepDistributed method","# Description  This PR refactors the huge HttpShardHandler.prepDistributed method into smaller pieces.  # Solution  It separates the logic for cloud and non-cloud modes into separate classes which are implementations of a new (experimental/internal) interface named ReplicaSource.  # Tests  This PR passes all current tests and I'll add more tests before merging.","closed","","shalinmangar","2020-01-28T13:52:16Z","2020-02-10T14:27:20Z"
"","867","SOLR-13751: add BooleanSimilarityFactory","# Description  This PR exposes Lucene's BooleanSimilarity in Solr.  # Solution  The new BooleanSimilarityFactory exposes BooleanSimilarity in the same way other Lucenes similarities are exposed in Solr. Creating this PR was largely a cut-and-paste of existing files.  # Tests  The Tests confirm that a field can be created using the new similarity.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).  (I haven't updated the ref guide - the notes at the end of other-schema-elements.adoc cover the existence of other non-default similarities.)","closed","","andywebb1975","2019-09-10T12:39:09Z","2020-08-16T18:44:21Z"
"","1267","LUCENE-9201: Add missing javadocs check task","# Description  This PR adds missing Javadocs check to gradle build (i.e., an equivalent to Ant build's ""check-missing-javadocs"" macro).  # Note  Newly added ""checkMissingDocs"" task will fail for now because there are missing package summaries.","closed","","mocobeta","2020-02-20T17:04:23Z","2020-02-24T12:43:17Z"
"","1242","LUCENE-9201: Port documentation-lint task to Gradle build","# Description  This PR adds an equivalent to ""documentation-lint"" to Gradle build.  https://issues.apache.org/jira/browse/LUCENE-9201  # Solution  The `gradle/validation/documentation-lint.gradle` includes  - `documentationLint` task that supposed to be called from `precommit` task, - a root project level sub-task `checkBrokenLinks`,  - sub-project level sub-tasks `ecjJavadocLint`, `checkMissingJavadocs`  # Note  For now, Python linters - `checkBrokenLinks` and `checkMissingJavadocs` - will fail because the Gradle-generated Javadocs seems to be slightly different to Ant-generated ones. e.g.;  - Javadoc directory structure: ""ant documentation"" generates ""analyzers-common"" docs dir for ""analysis/common"" module, but ""gradlew javadoc"" generates ""analysis/common"" for the same module. I think we can adjust the structure, but where is the suitable place to do so?  - Package summary: ""ant documentation"" uses ""package.html"" as package summary description, but ""gradlew javadoc"" ignores ""package.html"" (so some packages lacks summary description in ""package-summary.html"" when building javadocs by Gradle). We might be able to make Gradle Javadoc task to properly handle ""package.html"" files with some options. Or, should we replace all ""package.html"" with ""package-info.java"" at this time?","closed","","mocobeta","2020-02-06T15:34:01Z","2020-02-12T11:24:54Z"
"","1468","LUCENE-9333: Add gradle task to compile changes.txt to a html","# Description  This PR adds ""documentation"" gradle task (equivalent of ant's ""documentation"") and its sub task ""changesToHtml"" (equivalent of ant's ""changes-to-html"" that compiles CHANGES.txt into Changes.html).  The output directories are  `lucene/build/documentation` (for lucene) and `solr/build/documentation` (for solr). Those are not used by ant; ant outputs docs into `lucene/build/docs` and `solr/build/docs`.  ""documentation"" task is incomplete for now; please see the TODO comment.  ## Gradle command  ``` # root project task that collects all sub tasks $ ./gradlew documentation ``` or ``` # sub task for each project $ ./gradlew :lucene:changesToHtml $ ./gradlew :solr:changesToHtml ```  # Tests  I checked md5 hash for the compiled Changes.html to verify the gradle task generates exact same file as ant (when version property is set to `9.0.0`).  ``` lucene-solr $ md5sum lucene/build/docs/changes/Changes.html  # generate by ant e5ddb897e191915be0fe9f23bdd0edf0  lucene/build/docs/changes/Changes.html lucene-solr $ md5sum lucene/build/documentation/changes/Changes.html  # generated by gradle e5ddb897e191915be0fe9f23bdd0edf0  lucene/build/documentation/changes/Changes.html ```   ``` lucene-solr $ md5sum solr/build/docs/changes/Changes.html  # generate by ant 0076f4eb251c44a0b6effc9d8e958cd9  solr/build/docs/changes/Changes.html # Changes.html generated by gradle lucene-solr $ md5sum solr/build/documentation/changes/Changes.html  # generated by gradle 0076f4eb251c44a0b6effc9d8e958cd9  solr/build/documentation/changes/Changes.html ```  # Note about RDF processing  Not fully sure, but there is no equivalent gradle task for `ant.xmlproperties` as far as I know. We could remove the ant call by writing our custom XPath processor which will be built on groovy's [XmlSluper](https://docs.groovy-lang.org/latest/html/api/groovy/xml/XmlSlurper.html). I didn't do so here but just cloned the original Ant task. To me it would be preferable to replace the DOAP RDF with a JSON (or simply a CSV) instead of adding another XML processing code, if we want to completely drop the ant convention. ?","closed","","mocobeta","2020-04-29T18:05:07Z","2020-04-30T08:21:56Z"
"","1156","SOLR-13971: CVE-2019-17558: Velocity custom template RCE vulnerability","# Description  This is the fix for CVE-2019-17558: Velocity custom template RCE vulnerability, which was fixed for 8.3 and I provided the same fix for 7.7 as per the discussion here: https://issues.apache.org/jira/browse/SOLR-13971?focusedCommentId=17010489&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17010489  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","Sachpat","2020-01-08T11:52:09Z","2021-03-15T15:09:05Z"
"","1177","SOLR-13779: Use the safe fork of simple-xml for clustering contrib - for 7_7","# Description  This is the fix backported from 8.3 to 7.7 as implemented at https://github.com/apache/lucene-solr/commit/2a1d5eea42d2bb372245480dd2961baf6fa06469 as per the discussion at https://issues.apache.org/jira/browse/SOLR-13779?focusedCommentId=17016124&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17016124  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","Sachpat","2020-01-16T13:23:49Z","2020-01-16T17:51:34Z"
"","1123","LUCENE-9093: Unified highlighter with word separator never gives context to the left","# Description  This is an improved version of the patch submitted to the Jira issue about a week ago. A lot of information and discussion was posted on Jira. Here are my relevant comments: [Initial comment about possible solution](https://issues.apache.org/jira/browse/LUCENE-9093?focusedCommentId=16995738&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16995738) [Follow-up with more details](https://issues.apache.org/jira/browse/LUCENE-9093?focusedCommentId=16996736&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16996736)  # Solution [Submitted patch and some notes on the implementation](https://issues.apache.org/jira/browse/LUCENE-9093?focusedCommentId=16998660&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16998660)  This pull request is made so that the new parameter's default value is backward-compatible when merging to the 8.x and 7.x branches. @dsmiley suggested that for the 9.x (master) branch this should be changed in a second PR. In this 2nd phase I'll create another PR here with a `hl.fragalign` of `0.5` and updated docs. Regarding this 2nd PR I'd also suggest to change the default `hl.bs.type` to `WORD` as the new behavior will then be similar to the other highlighters' default behavior.  # Tests All related tests were updated and extended with a few new assertions to check the changes. I wrote one more comment clarifying how the changes work with different BreakIterator types: [Clarification on how SENTENCE BI will behave](https://issues.apache.org/jira/browse/LUCENE-9093?focusedCommentId=17002548&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17002548)  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","Traktormaster","2019-12-25T16:03:53Z","2020-01-01T05:57:17Z"
"","1595","SOLR-10059: Workaround for re-appended query params","# Description  This is a small yet simple fix for [SOLR-10059](https://issues.apache.org/jira/browse/SOLR-10059) to remove the re-appending of query params in the `SearchHandler` `appends` section on shards in a distributed request.  # Solution  The patch skips re-appending on the shards (`isShard=true`) if the parameter `shards.handler.skipAppends=true`. The latter defaults to `false`.   The fix is built against the `branch_7x` branch. I'll also check for `master` branch.  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","tboeghk","2020-06-19T07:35:53Z","2021-06-04T07:18:56Z"
"","799","LUCENE-8920: encapsulate FST.Arc data","# Description  This changes the visibility of FTS.Arc members to private and adds accessor methods for outside consumers of them. It also reduces the visibility of some other FST members.  Commit #2 then does some refactoring so we can eliminate the public Arc setters.  # Tests No new tests since there is no functional change","closed","","msokolov","2019-07-19T22:18:38Z","2019-07-20T17:20:27Z"
"","931","LUCENE-9001: Fix race condition in SetOnce","# Description  This change fixes race condition in SetOnce class that can cause code like below throw `NullPointerException` ```java SetOnce setOnce = new SetOnce<>(); new Thread(() -> setOnce.set(""thread"")).start(); try{     setOnce.set(""main""); } catch (SetOnce.AlreadySetException e){     setOnce.get().hashCode(); //possible NPE! } ``` It also adds `boolean trySet(T obj)` method that doesn't throw exception when something was set before, it returns false in that case.  # Solution  `AtomicBoolean` and `volatile T obj` was replaced with single `AtomicReference` that serves both purposes: checking if something was set and actually storing object. That way update is atomic - no race condition can occur. `Wrapper` is used to allow null object to be stored, the same way it was possible before.  # Tests  All tests from `TestSetOnce` pass, I've also added test for `trySet` method  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","probakowski","2019-10-08T20:23:45Z","2019-10-14T16:40:35Z"
"","809","LUCENE-8933: Validate JapaneseTokenizer user dictionary entry","# Description  This adds a format check to Kuromoji user dictionary if the concatenated segment is same as its surface form. See https://issues.apache.org/jira/browse/LUCENE-8933","closed","","mocobeta","2019-07-27T08:19:18Z","2019-08-16T09:23:58Z"
"","810","LUCENE-8933: Validate JapaneseTokenizer user dictionary entry (for 8.x)","# Description  This adds a format check to Kuromoji user dictionary if the concatenated segment is not longer than its surface form. (For backwards compatibility, we check only segmentation length in 8.x and will add more strict equality check as of 9.0.) See https://issues.apache.org/jira/browse/LUCENE-8933","closed","","mocobeta","2019-07-27T08:33:33Z","2019-08-16T09:23:02Z"
"","1344","SOLR-14316 Remove unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() squashed","# Description  there was an unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() method  # Solution fixed an unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() method  # Tests  added tests per @noblepaul 's comments on the previous pr [https://github.com/apache/lucene-solr/pull/1335]  - please do note that no tests existed for this api prior to this so I had to think how to set a baseline here - however now we have a a test case for this api oh JavaBinCodec, with decent boundary value code coverage - I tested for backward compatibility between the code change by running the same boundary value analysis of tests, with the new and old api implementation and the test results are the same  Some Observations - so from these tests, one can surmise that the JavaBinCodec.readMapEntry() method's current behavior is to create/expect new Objects to be created from binary streams. - What the tests do reveal is that its possible to use this api to read text streams too where the api does not generate object but uses primitive string comparisons. - Which is fine as I think this api was and is meant to be used for binary file renditions only. - however one may think that this should be more formally enforced ?  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have run `gradlew precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","aroopganguly","2020-03-11T18:41:52Z","2020-03-12T16:55:02Z"
"","1335","SOLR-14316 Remove unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() method","# Description  there was an unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() method  # Solution fixed an unchecked type conversion warning in JavaBinCodec's readMapEntry's equals() method  # Tests  added tests per @noblepaul 's comments  - please do note that no tests existed for this api prior to this so I had to think how to set a baseline here - however now we have a a test case for this api oh JavaBinCodec, with decent boundary value code coverage - I tested for backward compatibility between the code change by running the same boundary value analysis of tests, with the new and old api implementation and the test results are the same  Some Observations - so from these tests, one can surmise that the JavaBinCodec.readMapEntry() method's current behavior is to create/expect new Objects to be created from binary streams. - What the tests do reveal is that its possible to use this api to read text streams too where the api does not generate object but uses primitive string comparisons. - Which is fine as I think this api was and is meant to be used for binary file renditions only. - however one may think that this should be more formally enforced ?  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have run `gradlew precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","aroopganguly","2020-03-10T15:42:54Z","2020-03-11T19:34:07Z"
"","1372","SOLR-14358 Document how to use Processor in JavaDocs","# Description  There is an existing PR (https://github.com/apache/lucene-solr/pull/404) from 2018 that needed some updates to the JavaDoc formatting.  # Solution  I fixed the formatting.  # Tests  ran `ant documentation`  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of myx ability. - [ x] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-03-23T15:48:56Z","2020-06-30T15:40:50Z"
"","925","SOLR-13818: Upgrade jackson to 2.10.0","# Description  There are security risks with jackson databind 2.9.8 (CVE-2018-11307, CVE-2018-12022, CVE-2018-12023), the current version.   # Solution  This is a minor version upgrade to jackson core 2.10.0.  # Tests  `ant test` was run to ensure no breaking changes are being committed. No additional tests were written.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","marungo","2019-10-04T20:54:08Z","2020-01-10T15:19:41Z"
"","1297","SOLR-14253 Replace various sleep calls with ZK waits","# Description  There are lots of places in the code where we poll and sleep rather than using proper ZK callbacks.  # Solution  Replace poll loops with waitForState","closed","","madrob","2020-02-27T18:08:02Z","2021-02-01T19:25:21Z"
"","887","SOLR-13773: Prometheus Exporter GC and Heap options","# Description  The startup options for the Prometheus Exporter are pretty sparse compared to what the Solr start script offers.  # Solution  I've added some options that mirror what Solr offers, such as: - SOLR_HEAP - SOLR_JAVA_MEM - GC_TUNE  Having just the memory settings available would let us start the prometheus exporter with more than 500 Mb of heap, which right now isn't possible as the max heap is hard coded here.  # Tests  I have started the metrics-exporter locally with this script, although I do not have access to a windows host to test the `solr-exporter.cmd` script. That code came from `solr.cmd` so I would imagine that it works, however I would appreciate if someone on windows could make sure that it works as expected.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-09-17T17:29:47Z","2020-10-20T15:57:51Z"
"","1140","SOLR-14165 set SolrResponse's serialVersionUID explicitly","# Description  The serialVersionUID of SolrResponse changed in https://github.com/apache/lucene-solr/pull/929 making Solr nodes before/after that change mutually incompatible for some operations.  # Solution  This sets the serialVersionUID of SolrResponse explicitly to its previous value.  # Tests  I've run manual builds with/without this patch and seen that /solr/admin/collections?action=overseerstatus against stops giving the exception `java.io.InvalidClassException: org.apache.solr.client.solrj.SolrResponse; local class incompatible: stream classdesc serialVersionUID = -7931100103360242645, local class serialVersionUID = 2239939671435624715` for hybrid clusters when the patch is in place.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - (n/a) I have added tests for my changes. - (n/a) I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2020-01-03T21:22:16Z","2020-01-15T22:22:25Z"
"","924","SOLR-13820: Improve RBAP documentation","# Description  The Rule-Based Authorization page in the ref-guide is unclear, incomplete, and outright wrong in a few places.  # Solution  I took a shot at rewriting it.  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2019-10-04T19:36:57Z","2019-10-07T16:33:12Z"
"","1183","LUCENE-9134: Port ant-regenerate tasks to Gradle build","# Description  The only file that counts here really is lucene/queryparser/build.gradle. All the rest are a result of running the regenerate task.   There are a couple of dodgy bits, see //nocommit in the build.gradle file.  NOTES: - this does not address any of the other regenerate tasks yet. - I'm not going to untangle warnings until later.  # Tests  precommit and the full test suite (both gradle and ant) pass, so I think it's close for this part of the regenerate task","closed","","ErickErickson","2020-01-18T00:04:04Z","2020-01-18T22:49:25Z"
"","1247","SOLR-14252 use double rather than Double to avoid NPE","# Description  The getMax and getMin methods in AggregateMetric can throw an NPE if _only_ non-Number values are present when it tries to cast a null Double to a double.  # Solution  This PR switches to using primitive doubles, defaulting to zero, and logs when non-Number values are provided (we've seen `false` and `LocalStatsCache`).  # Tests  TBC  # Checklist  Please review the following and check all that apply:  - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2020-02-10T11:37:17Z","2020-02-18T16:48:19Z"
"","1265","SOLR-14252: avoid NPE in metric aggregation","# Description  The getMax and getMin methods in AggregateMetric can throw an NPE if _only_ non-Number values are present when it tries to cast a null Double to a double.  # Solution  This PR checks that the aggregation output is non-null before casting to a `double`. Null values become 0.  # Tests  Test suite has been updated to check that non-Number metrics can be aggregated (returning 0) and to ensure that negative values continue to be supported.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] (n/a) I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2020-02-18T15:54:43Z","2020-02-27T17:09:48Z"
"","1172","SOLR-14189 switch from String.trim() to StringUtils.isBlank()","# Description  The edismax and some other query parsers treat pure whitespace queries as empty queries, but they use Java's String.trim() method to normalise queries. That method only treats characters 0-32 as whitespace. Other whitespace characters exist - such as U+3000 IDEOGRAPHIC SPACE - which bypass the test and lead to 400 Bad Request responses.  # Solution  This PR switches from String.trim() to StringUtils.isBlank() which is aware of all whitespace characters.  # Tests  I've added a test which sends U+3000 IDEOGRAPHIC SPACE and checks that it behaves the same way as U+0020 SPACE.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [ ] (not sure!) I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] (n/a) I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2020-01-15T17:09:24Z","2020-01-26T15:18:15Z"
"","1512","SOLR-13325: Add a collection selector to ComputePlanAction","# Description  The ComputePlanAction supports a `collections` attribute that can be used to filter collections for which the plan is computed. This currently accepts a comma-separated `list` of collection names. If the attribute is empty or omitted then all collections are supposed to match.  However, it is difficult to maintain such a list in a system where collections are added and removed dynamically. It is easier to have a collection selector that matches zero or more collections by filtering them against a given set of collection (property, value) pairs. This can then be used to e.g. match all collections that use a specific policy etc.  # Solution  This PR adds collection selector support to the `ComputePlanAction` which is of the form `collections: {policy: my_policy}`. If more than one key/value pair is specified then all must match for a collection to be considered. The old way of specifying a string for `collections` is preserved for back-compatibility.  After this support, the `AutoAddReplicasPlanAction` becomes a thin shim over ComputePlanAction with a collection selector `collections: {autoAddReplicas: true}`.  The old implementation did not pass collection hints to the policy engine which meant that all possible operations were computed by the policy and then they were filtered out by the ComputePlanAction. In this PR, the collection hints are pushed down to the policy engine so operations for non-matching collections are not even computed in the first place.  # Tests  The ComputePlanActionTest.testSelectedCollections is now refactoted into two test methods viz. `testSelectedCollectionsByName` and `testSelectedCollectionsByPolicy`. The latter tests the collection selector by policy.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","shalinmangar","2020-05-12T05:13:39Z","2020-05-23T06:52:06Z"
"","999","SOLR-13782: Remove PDF Ref Guide build","# Description  The community recently decided to stop building the PDF in order to focus on the HTML and make it the ""official"" version of the Ref Guide. This PR pulls out all the PDF-related elements of the Ref Guide build.  # Solution  This PR makes several changes:  - Removes the PDF-related build targets from build.xml - Removes PDF-specific validation rules in tools classes - Changes the Jenkins build script for the Ref Guide to only build the HTML version - Updates Ref Guide publication process docs with new steps - Removes link to ""current PDF"" from Ref Guide nav - Miscellaneous other cleanups to excise PDF as an available version.  # Tests  I've built the HTML version of the Ref Guide and also precommit to be sure they pass.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","ctargett","2019-11-07T02:39:33Z","2019-11-19T21:54:42Z"
"","866","LUCENE-8964: Fix geojson shape parsing on string arrays in properties","# Description  The builtin geo shape parser throws an exception when the properties contains an array of strings.  # Solution  Allow parsing of such arrays by reading strings.  # Tests  Add unit test has been added for this case.  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes.","closed","","spinscale","2019-09-10T12:13:24Z","2019-09-10T12:15:29Z"
"","1133","SOLR-14157-backup-restore-docs-missing-parameter","# Description  The `name` parameter is madatory on BACKUP and RESTORE commands, but isn't listed, though it is in the example URLs...  # Solution  Added the parameter.  # Tests  n/a, though manually tested the commands to confirm needed, and reviewed the Java code.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-12-31T19:42:54Z","2020-01-03T15:18:28Z"
"","1151","SOLR-13890: Add ""top-level"" DVTQ implementation","# Description  The ""docValuesTermsFilter"" method for TermsQParserPlugin uses per-segment docvalue structures to find matches.  This works well when the number of query terms is small, but as it gets over a few hundred, this quickly becomes less efficient than using a ""top-level"" or global docvalues structure.  # Solution  This PR introduces a query that uses the top-level structure and takes advantage of the Two Phase Iterator (TPI) interface to handle queries with large numbers of terms much more efficiently than any of the existing methods.  # Tests  This PR adds `TestTermsQParserPlugin` with some tests to cover the new code added here as well as the rest of the terms qparser (which was largely untested).  I also wrote a JUnit performance driver to exhibit perf gains, but that no longer appears in this PR.  See patches on the JIRA issue for that.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-07T12:27:29Z","2020-01-13T11:43:28Z"
"","1633","SOLR-14422 hide alarming angular placeholders","# Description  Suppress alarming angular placeholders.   # Solution  Best practice is to apply ngCloak at a granular level to support progressive loading.  Thanks to Colvin Cowie for the patch that inspired this.  # Tests  Manually tested UI before and after to confirm.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X ] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [X ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-06-30T16:24:08Z","2020-07-06T18:29:22Z"
"","1171","SOLR-13892: Add 'top-level' docValues Join implementation","# Description  Some join usecases are better served (performance wise) by using top-level docValues structures, because they avoid some wasteful seeking and iteration that the per-segment alternatives do.  # Solution This PR introduces a ""TopLevelJoinQuery"" which can be run on join's where the fields in question have docValues enabled.  This scales much better with the number of ""from"" query matches than existing alternatives.  Since it's implemented as a Two-Phase Iterator it scales less well by the number of overall matches.  See SOLR-13892 for more in-depth performance discussion.  # Tests  Beefed up tests in `TestJoin` to prove correctness.  Wrote a performance test driver `TestJoinQueryPerformance` that allows reproduction of the perf results.  (Though this will not be committed in the final version)   # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-15T14:56:55Z","2020-01-31T16:21:08Z"
"","1525","SOLR-14443: Make SolrLogPostTool resilient to odd requests","# Description  SolrLogPostTool is relatively brittle in how it indexes log records to Solr.  A single error from Solr causes the tool to crash.  Further, when user requests have duplicate or unnecessary parameters, the tool attempts to record all of these (even when the field used is singlevalued).  As a result, a single ""oddball"" request crashes the whole ingestion run.  # Solution  This PR makes two related changes.  First, it changes SolrLogPostTool's batching code to log (but ultimately ignore) errors.  This allows an indexing run to survive malformed records or other indexing blips.  Second, it changes the record-parsing code to only record single values for single-valued log record fields.  This prevents a common cause of malformed records.     # Tests  Manual testing, in addition to a test case added to SolrLogPostToolTest  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-05-18T15:53:33Z","2020-05-22T14:08:33Z"
"","1162","SOLR-14186: Re-add CRLF line endings to solr.cmd","# Description  solr.cmd needs CRLF endings, but they were stripped out in a recent commit.  # Solution  I ran unix2dos to convert the file back to CRLF endings (and temporarily disabled my `autocrlf` setting so this change would actually get persisted on commit).  Shortly we need to add a `.gitattributes` file so that this doesn't happen again, but I want to get Windows devs/users unblocked while review of that happens separately.  # Tests N/A, whitespace changes only.  Though I did play with bin/solr.cmd a bit to make sure that the ""labels"" can be found with these new line endings.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-13T15:23:58Z","2020-01-13T15:40:50Z"
"","1542","SOLR-14347: fix cached session update to not depend on Zookeeper state","# Description  Session used for replica placement computation was wrongly built to only account for state already in Zookeeper, therefore ignoring all in progress past Collection API operations. The Session returned to the cache was also the wrong one not including the changes to the cluster state resulting from the placement computation.  # Solution  The Session used for computation is now cloned from the cached session without trying to validate state in Zookeeper where that state might not yet be.  # Tests  Ran solr tests.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","autoscaling,","murblanc","2020-05-28T22:08:15Z","2020-09-16T22:38:07Z"
"","1084","SOLR-14066 Deprecate DataImportHandler","# Description  See https://issues.apache.org/jira/browse/SOLR-14066","closed","","janhoy","2019-12-14T18:49:41Z","2019-12-16T14:16:25Z"
"","860","SOLR-13734 JWTAuthPlugin to support multiple issuers","# Description  See https://issues.apache.org/jira/browse/SOLR-13734  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-09-06T09:37:33Z","2019-09-19T07:50:26Z"
"","852","SOLR-13713 JWTAuthPlugin to support multiple jwks endpoints","# Description  See https://issues.apache.org/jira/browse/SOLR-13713  # Solution  Accept an array of urls in `jwkUrl` config parameter. Validate incoming JWT against all keys from all URLs.  # Tests  Added tests for checking config parsing and for validating signatures from multiple lists.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-09-03T13:57:10Z","2019-09-19T08:44:16Z"
"","834","SOLR-13702: Some components register twice their metric names","# Description  See https://issues.apache.org/jira/browse/SOLR-13702  # Solution  Remove explicit name registration.  # Tests  No tests failed, existing tests still pass after patch.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. (pre-existing) - [-] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-08-16T15:16:29Z","2019-10-08T10:52:39Z"
"","896","SOLR-10665: PF4J based plugin/package system (9x)","# Description  See https://issues.apache.org/jira/browse/SOLR-10665 This is a new PR which is up to date with current 9x master. Provided for more easily pull features from the POC for integration in Solr.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [-] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [-] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-09-21T22:05:55Z","2019-11-26T19:55:13Z"
"","958","LUCENE-8986: Add asf.yaml to our git repo","# Description  See https://issues.apache.org/jira/browse/LUCENE-8986  # Solution  Adding the .asf.yaml will edit GitHub project description, link and labels  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-10-16T19:47:27Z","2019-10-28T07:47:59Z"
"","1435","SOLR-14410: Switch from SysV init script to systemd service file","# Description  Remove the init.d/solr SysV init script and use a systemd service file instead.  # Solution  I've tried not to diverge as much as possible from the way the installation script used to work before.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","mhitza","2020-04-16T20:53:03Z","2021-10-21T14:36:03Z"
"","1373","SOLR-14340 - ZkStateReader.readConfigName is doing too much work","# Description  Reduced the scope of ZkStateReader.readConfigName() as described in the JIRA by David.   # Solution  * Removed the check to ensure that the Zk Node exists. * Removed the check for the existence of the Configset entry. In a local test with 600 collections, it saved 2/5 of the GETCLUSTERSTATUS  execution time.  # Tests  Removed an assert in TestCollectionAPI that tests that a collection with missing configset are not returned. That behavior will not happen anymore.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [  ] I have created a Jira issue and added the issue ID to my pull request title. - [  ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [  ] I have added tests for my changes. - [  ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","mariemat","2020-03-23T16:48:39Z","2020-03-24T19:37:47Z"
"","1138","LUCENE-9077 Print repro line for failed tests","# Description  Print the 'reproduce with' line for failed tests  # Tests  I did not test this with multiple tests running in parallel yet because I'm still figuring out all the right ways to do Gradle things.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ ] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","madrob","2020-01-03T20:40:11Z","2020-01-08T16:04:08Z"
"","1570","SOLR-14558: Record all log lines in SolrLogPostTool","# Description  Previously, SolrLogPostTool ignored all log-lines that didn't fall into a narrow handful of known ""types"".  This change adds a new ""other"" type, to hold all previously-ignored log records.  This allows all log records to be searched - not just the whitelisted cluster-traffic event types.(queries, commits, etc.)   # Solution  Straightforward implementation.  # Tests  New test to SolrLogPostToolTest.  Manual testing.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-06-11T13:13:08Z","2020-06-16T12:21:30Z"
"","761","SOLR-13593: Allow to use analyzer SPI names in schema definition.","# Description  Please see the issue description: https://issues.apache.org/jira/browse/SOLR-13593  # Solution  - Allow to load tokenizer/charFilter/tokenFilter factories via SPI name (""name"" attribute) in `FieldTypePluginLoader`. If both two attributes (""class"" and ""name"") are defined, ""class"" is preferred. - For `add-field-type` REST API, accept a field type definition if either ""class"" or ""name"" property is defined.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [] I have added documentation for the Ref Guide (for Solr changes only).","closed","","mocobeta","2019-07-04T06:28:19Z","2019-11-10T10:17:19Z"
"","910","SOLR-13661 :  SOLR-13661 A package management system for Solr","# Description  Please refer to the design doc for for details  https://docs.google.com/document/d/15b3m3i3NFDKbhkhX_BN0MgvPGZaBj34TKNF2-UNC3U8/edit?ts=5d86a8ad#    # Tests  TestPackages have the tests required for this   # Checklist  Please review the following and check all that apply:   - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability.  - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","noblepaul","2019-09-30T14:16:49Z","2020-03-02T19:49:52Z"
"","1408","LUCENE-8050 Ignore fields having no doc values when merging PerFieldDVFormat","# Description  PerFieldDocValuesFormat delegates the merge to the actual field DVF's merge. Great, but unfortunately it will call getDocValuesFormatForField on all fields (in FieldInfos) even those that have no DocValues (DocValuesType.NONE)  # Solution  Skip FieldInfo when getDocValuesType() == DocValuesType.NONE  # Tests  Test added using doc values in one segment and indexed fields in the other, we asses that the fields not having doc values are skipped while merging.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","juanka588","2020-04-06T10:23:46Z","2020-04-07T20:12:22Z"
"","1195","SOLR-13101: Log accurate file counts for Push and Pull in CorePushPull","# Description  Number of files and bytes pushed/pulled are logged when pushing to or pulling from blobstore, but values are only accurate when operation is successful. Log expected and actual file count, actual byte count, and add back boolean for whether operation was successful.  # Solution  Count number of files/bytes actually transferred  # Tests  Ran ant test, and working on adding additional unit tests.  Logline for successful push: `PushPullData=[collectionName=gettingstarted shardName=shard1 sharedStoreName=gettingstarted_shard1 coreName=gettingstarted_shard1_replica_s1] action=PUSH storageProvider=LOCAL_FILE_SYSTEM bucketRegion=N/A bucketName=N/A runTime=44325264 startLatency=44325264 bytesTransferred=2076 attempt=0 expectedFilesAffected=10 actualFilesAffected=10 isSuccessful=true localGeneration=2 blobGeneration=-1 `  Logline for successful pull: `PushPullData=[collectionName=pulltest2 shardName=shard2 sharedStoreName=pulltest2_shard2 coreName=pulltest2_shard2_replica_s6] action=PULL storageProvider=LOCAL_FILE_SYSTEM bucketRegion=N/A bucketName=N/A runTime=70088178 startLatency=70090032 bytesTransferred=9828 attempt=0 expectedFilesAffected=46 actualFilesAffected=46 isSuccessful=true localGeneration=1 blobGeneration=6`  Logline for exception thrown partway through push: `PushPullData=[collectionName=gettingstarted shardName=shard1 sharedStoreName=gettingstarted_shard1 coreName=gettingstarted_shard1_replica_s1] action=PUSH storageProvider=LOCAL_FILE_SYSTEM bucketRegion=N/A bucketName=N/A runTime=6363713 startLatency=6363713 bytesTransferred=1132 attempt=0 expectedFilesAffected=10 actualFilesAffected=6 isSuccessful=false localGeneration=2 blobGeneration=-1`  Logline for exception thrown partway through pull: `PushPullData=[collectionName=pulltest2 shardName=shard1 sharedStoreName=pulltest2_shard1 coreName=pulltest2_shard1_replica_s1] action=PULL storageProvider=LOCAL_FILE_SYSTEM bucketRegion=N/A bucketName=N/A runTime=9548098 startLatency=9548350 bytesTransferred=1404 attempt=0 expectedFilesAffected=19 actualFilesAffected=6 isSuccessful=false localGeneration=1 blobGeneration=3`","closed","","ebehrendt","2020-01-21T21:43:03Z","2020-02-03T17:46:43Z"
"","1159","SOLR-13892: Add top-level docValues ""join"" implementation","# Description  Many ""join"" use-cases can be made more performant by using ""top-level"" docValues data structures, instead of the per-segment structures that are used currently.  Users should have the ability to pick between top-level and per-segment, based on the particulars of their index and use case.    # Solution This PR introduces a ""top-level"" implementation in the form of a ""join"" postfilter.  Users get the ""top-level"" behavior by specifying `cache=false cost=101` as local params on their join.  We may decide to repackage this implementation as a Two-Phase Iterator before merging, though that is still up in the air.  # Tests Functional tests have been added in TestJoin.java.  Performance tests validating the improvement performance in select use-cases can be found in the comments on SOLR-13892.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-01-10T14:22:47Z","2020-01-15T14:51:02Z"
"","1561","SOLR-14546: OverseerTaskProcessor can process messages out of order","# Description  Make sure Collection API tasks are processed in enqueue order by the Overseer and OverseerCollectionMessageHandler  # Solution  The LockTree.Session class already existed to address that need but there were a few missing pieces preventing it from doing so. Added these pieces.  # Tests  A previously failing test MultiThreadedOCPTest (SOLR-14524) no longer fails. Also ran local massive enqueue tests into Collection API ZK queue and instrumented OverseerTaskProcessor to verify dequeue is in order (requires heavy handed code hacks so this was only run locally to help diagnose and verify).  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2020-06-10T01:34:01Z","2020-06-23T14:17:00Z"
"","1548","SOLR-14524: Harden MultiThreadedOCPTest testFillWorkQueue()","# Description  Make MultiThreadedOCPTest.testFillWorkQueue() less vulnerable to timing issues when test or overseer code are being slowed down by external factors (load, GC etc).  # Solution  A combination of verifying preconditions (to fail with a meaningful messages helping pinpoint issues for future test hardening), longer task execution time (that **does not delay** total test runtime) and do light synchronization between test steps and Overseeer processing progress.   # Tests  This _is_ a test.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","test-fix,","murblanc","2020-06-01T09:14:50Z","2020-09-16T22:38:05Z"
"","1581","SOLR-14572 document missing SearchComponents","# Description  Learned about a existing SearchComponent from 2012.  Went to look at the related Ref Guide page and saw it was missing, plus two other components.  # Solution  Add a table linking to the JavaDocs.  Also point to the http://solr.cool website.  # Tests  ant build of the ref guide.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X ] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-06-15T20:25:38Z","2020-06-17T15:01:50Z"
"","941","SOLR-13836: Add 'streaming_expression' QParser","# Description  It is currently possible to hit the search handler in a streaming expression (""search(...)""), but it is not currently possible to invoke a streaming expression from within a regular search within the search handler. In some cases, it would be useful to leverage the power of streaming expressions to generate a result set and then join that result set with a normal set of search results. This likely won't be particularly efficient for high cardinality streaming expression results, but it will be pretty powerful feature that could enable a bunch of use cases that aren't possible today within a normal search.  See https://issues.apache.org/jira/browse/SOLR-13836 for usage information.  # Solution  The current solution adds a StreamingExpressionQParserPlugin which executes a streaming expression and joins the tuples returned on an id field with the main docset. The field name from the streaming expression tuples can be overridden (""f"" param), as well as the method of joining (""method"" parameter).   # Usage *Docs:*  ``` curl -X POST -H ""Content-Type: application/json"" http://localhost:8983/solr/food_collection/update?commit=true  --data-binary ' [ {""id"": ""1"", ""name_s"":""donut"",""vector_fs"":[5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0]}, {""id"": ""2"", ""name_s"":""apple juice"",""vector_fs"":[1.0,5.0,0.0,0.0,0.0,4.0,4.0,3.0]}, {""id"": ""3"", ""name_s"":""cappuccino"",""vector_fs"":[0.0,5.0,3.0,0.0,4.0,1.0,2.0,3.0]}, {""id"": ""4"", ""name_s"":""cheese pizza"",""vector_fs"":[5.0,0.0,4.0,4.0,0.0,1.0,5.0,2.0]}, {""id"": ""5"", ""name_s"":""green tea"",""vector_fs"":[0.0,5.0,0.0,0.0,2.0,1.0,1.0,5.0]}, {""id"": ""6"", ""name_s"":""latte"",""vector_fs"":[0.0,5.0,4.0,0.0,4.0,1.0,3.0,3.0]}, {""id"": ""7"", ""name_s"":""soda"",""vector_fs"":[0.0,5.0,0.0,0.0,3.0,5.0,5.0,0.0]}, {""id"": ""8"", ""name_s"":""cheese bread sticks"",""vector_fs"":[5.0,0.0,4.0,5.0,0.0,1.0,4.0,2.0]}, {""id"": ""9"", ""name_s"":""water"",""vector_fs"":[0.0,5.0,0.0,0.0,0.0,0.0,0.0,5.0]}, {""id"": ""10"", ""name_s"":""cinnamon bread sticks"",""vector_fs"":[5.0,0.0,1.0,5.0,0.0,3.0,4.0,2.0]} ] ```     *Query:* ``` http://localhost:8983/solr/food/select?q=*:*&fq=\{!streaming_expression}top(select(search(food,%20q=%22*:*%22,%20fl=%22id,vector_fs%22,%20sort=%22id%20asc%22),%20cosineSimilarity(vector_fs,%20array(5.1,0.0,1.0,5.0,0.0,4.0,5.0,1.0))%20as%20cos,%20id),%20n=5,%20sort=%22cos%20desc%22)&fl=id,name_s ```     *Response:* ``` {   ""responseHeader"":{     ""zkConnected"":true,     ""status"":0,     ""QTime"":7,     ""params"":{       ""q"":""*:*"",       ""fl"":""id,name_s"",       ""fq"":""{!streaming_expression}top(select(search(food, q=\""*:*\"", fl=\""id,vector_fs\"", sort=\""id asc\""), cosineSimilarity(vector_fs, array(5.2,0.0,1.0,5.0,0.0,4.0,5.0,1.0)) as cos, id), n=5, sort=\""cos desc\"")""}},   ""response"":{""numFound"":5,""start"":0,""docs"":[       {         ""name_s"":""donut"",         ""id"":""1""},       {         ""name_s"":""apple juice"",         ""id"":""2""},       {         ""name_s"":""cheese pizza"",         ""id"":""4""},       {         ""name_s"":""cheese bread sticks"",         ""id"":""8""},       {         ""name_s"":""cinnamon bread sticks"",         ""id"":""10""}]   }} ```  # Tests  No tests written yet. First draft.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","treygrainger","2019-10-11T06:10:02Z","2019-10-11T06:10:02Z"
"","1571","SOLR-14560: Interleaving for Learning To Rank","# Description  Interleaving is an approach to Online Search Quality evaluation that can be very useful for Learning To Rank models: https://sease.io/2020/05/online-testing-for-learning-to-rank-interleaving.html Scope of this issue is to introduce the ability to the LTR query parser of accepting multiple models (2 to start with). If one model is passed, normal reranking happens. If two models are passed, reranking happens for both models and the final reranked list is the interleaved sequence of results coming from the two models lists. As a first step it is going to be implemented through: TeamDraft Interleaving with two models in input. In the future, we can expand the functionality adding the interleaving algorithm as a parameter.  # Solution  Change of core LTR classed and addition of a new rescorer  # Tests  Done  # Checklist  Please review the following and check all that apply:  - [X ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X ] I have created a Jira issue and added the issue ID to my pull request title. - [X ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","alessandrobenedetti","2020-06-11T18:49:28Z","2020-11-18T18:16:18Z"
"","1061","[SOLR-14029] Documentation updated","# Description  Incorrect documentation for handleSelect and qt param.  # Solution  Short documentation update  # Tests  N.A.  # Checklist  Please review the following and check all that apply:  - [ Y] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ Y] I have created a Jira issue and added the issue ID to my pull request title. - [ Y] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [Y ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [Y ] I have developed this patch against the `master` branch. - [ Y] I have run `ant precommit` and the appropriate test suite. - [ Y] I have added tests for my changes. - [ Y ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","alessandrobenedetti","2019-12-06T15:55:24Z","2019-12-10T04:28:46Z"
"","932","SOLR-13829: Stop converting continuous numbers to Longs in Streaming Expressions","# Description  In some cases, streaming expressions are taking continuous numeric values (double, float, etc.) and converting them to Longs if the scale of the number is zero (i.e. 1.0, 2.0, 3.0, etc.). This causes issues because steaming operations which try to compare different values may blow up because the types are incompatible.  For example, In trying to use the ""sort"" streaming evaluator on float field (pfloat), I am getting casting errors back based upon which values are calculated based upon underlying values in a field. For example:  **Docs:** (paste each into ""Documents"" pane in Solr Admin UI as type:""json"")  ``` {""id"": ""1"", ""name"":""donut"",""vector_fs"":[5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0]}  {""id"": ""2"", ""name"":""cheese pizza"",""vector_fs"":[5.0,0.0,4.0,4.0,0.0,1.0,5.0,2.0]} ```  **Streaming Expression:** ``` sort(select(search(food_collection, q=""*:*"", fl=""id,vector_fs"", sort=""id asc""), cosineSimilarity(vector_fs, array(5.0,0.0,1.0,5.0,0.0,4.0,5.0,1.0)) as sim, id), by=""sim desc"") ```  **Response:** ``` {    ""result-set"": {     ""docs"": [       {         ""EXCEPTION"": ""class java.lang.Double cannot be cast to class java.lang.Long (java.lang.Double and java.lang.Long are in module java.base of loader 'bootstrap')"",         ""EOF"": true,         ""RESPONSE_TIME"": 13       }     ]   } } ```  This is because in org.apache.solr.client.solrj.io.eval.RecursiveEvaluator, there is a line which examines a numeric (BigDecimal) value and - regardless of the type of the field the value originated from - converts it to a Long if it looks like a whole number. This is the code in question from that class:  ``` protected Object normalizeOutputType(Object value) {     if(null == value){       return null;     } else if (value instanceof VectorFunction) {       return value;     } else if(value instanceof BigDecimal){       BigDecimal bd = (BigDecimal)value;       if(bd.signum() == 0 || bd.scale() <= 0 || bd.stripTrailingZeros().scale() <= 0){         try{           return bd.longValueExact();         }         catch(ArithmeticException e){           // value was too big for a long, so use a double which can handle scientific notation         }       }              return bd.doubleValue();     } ... [other type conversions] ```  Because of the `return bd.longValueExact();` line, the calculated value for ""sim"" in doc 1 is ""Float(1)"", whereas the calculated value for ""sim"" for doc 2 is ""Double(0.88938313). These are coming back as incompatible data types, even though the source data is all of the same type and should be comparable.  Thus when the sort evaluator streaming expression (and probably others) runs on these calculated values and the list should contain [""0.88938313"", ""1.0""], an exception is thrown because the it's trying to compare incompatible data types [Double(""0.99""), Long(1)].  This bug is occurring on master currently, but has probably existed in the codebase since at least August 2017.  # Solution  Removing the code that converts incoming BigDecimals to a Long fixes the issue, and all Solr tests still pass, as well as `ant precommit` once that is done.  In reviewing various responses, I haven't thusfar been able to identify any problem resulting from this change. This is the solution I've implemented, but would appreciate review.  I'm not sure if this could potentially create any side effects (i.e. loss of precision on certain calculations that went through multiple data type transformations) that were intended to be avoided by that code. Perhaps @dennisgove or @joel-bernstein  may know the reason for that original conversion of BigDecimals to longs and could shed some light.  I guess the other way to solve this would be to modify every streaming expression (like the sort evaluator) to explicitly do type checking and conversions anytime incompatible types are being compared, but this feels inefficient and I'm assuming probably not like the right approach unless doing it that way was an intentional design decision in how the streaming expressions framework is intending to handle data types.  # Tests  `ant test` passes `ant precommit` passes I can add a test validating this works once the solution approach has been validated.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).  I don't believe any documentation changes are needed, as this is just fixing a bug. See notes on unit tests under the `Tests` section.","closed","","treygrainger","2019-10-08T22:44:52Z","2020-02-15T21:48:58Z"
"","991","SOLR-13887: Use default instead of idleTimeouts of 0 for HTTP2 requests","# Description  In Solr 7, and previous versions, the both the `socketTimeout` and `connTimeout` defaults in `solr.xml` have accepted 0 as values. This is even [documented in the ref guide](https://lucene.apache.org/solr/guide/8_2/format-of-solr-xml.html#defining-solr-xml). Using these same defaults with Solr 8 results in timeouts when trying to manually create replicas. The major change here seems to be that the Http2SolrClient is being used instead of the HttpSolrClient used in Solr 7 and previous versions.  After some digging, I think that the issue lies in the Http2SolrClient, specifically in how it enforces idleTimeouts. Since the idleTimeout is set to 0, since that is what solr pulls from the solr.xml, the listener immediately responds with a timeout.   # Solution  Set a default idleTimeout if 0 is provided. Basically treat an idleTimeout (or socketTimeout) of 0 the same as null. The ref-guide should also likely be updated with the same defaults as used in the solr.xml packaged in Solr.  # Tests  Added a test to make sure that the use of an idleTimeout of 0 does not immediately timeout requests, and that a default is used.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-11-01T20:30:44Z","2020-02-05T19:15:45Z"
"","1510","SOLR-14473: Improve Overseer Javadoc","# Description  Improve Overseer Javadoc.  # Solution  Added/modified Javadoc on Overseer, ClusterStateUpdater, ZkDistributedQueue and CollectionParams.CollectionAction  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2020-05-11T23:22:06Z","2020-05-18T21:06:20Z"
"","976","SOLR-13749: Implement support for joining across collections with multiple shards","# Description  Implements support for joining across collections with multiple shards.  # Solution  This PR includes two query parsers.  The XCJF query parser queries a remote collection to obtain a set of join keys to use as a filter against the local collection.  It retrieves the set of join keys from the remote collection using a streaming expression.  The Hash Range query parser takes a field name and a range, and matches documents whose hash for that field falls within the range.  If the local collection uses the join key field as its routing prefix with the compositeId router, applying this filter allows the XCJF query on each shard to request only the join keys that could match against that shard's documents.  # Tests  There is a test class XCJFQueryTest.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","danmfox","2019-10-24T23:05:55Z","2020-02-15T21:02:08Z"
"","1349","Document sort param tiebreak logic","# Description  I was trying to explain Solr's not-completely-deterministic `sort` param resolution to a coworker this week, and realized that this behavior wasn't documented afaict.  # Solution  A short ref-guide change to mention the tie-break behavior.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ ] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2020-03-13T19:41:40Z","2020-03-16T14:38:52Z"
"","1185","Jira/lucene 9151","# Description  I punted on the bits about FuzzyQuery deprecations and just added the SuppressWarnings tag.  # Tests  There are no substantive code changes here. This depends on LUCENE-9134","closed","","ErickErickson","2020-01-18T18:43:14Z","2020-01-31T22:10:09Z"
"","946","SOLR-13835 HttpSolrCall produces incorrect extra AuditEvent on AuthorizationResponse.PROMPT","# Description  HttpSolrCall produces incorrect extra AuditEvent on AuthorizationResponse.PROMPT This caused both 401 and 403 responses to have the ""Unauthorized request"" message returned to client.  # Solution  Do not fall-through to next code block but handle 401, 403, 5xx and  in separate code blocks.  # Tests  Fixed the auth() test that previously expected a 403 audit event to now expect 401. Earlier both a 401 and a 403 were emitted, and the test just verified the last one, which happened to be the 403. Now there is only one audit event and that is the 401.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-10-13T19:33:00Z","2019-10-16T22:44:40Z"
"","995","SOLR-13844: Fixing tests related to ShardTerms recovery removal","# Description  Fixes `ZkShardTermsTest.testCoreRemovalWhileRecovering()` test.  # Solution  Collection nam was shared with another test case in the class. Changed the test to use a unique collection name.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-11-04T19:58:33Z","2019-11-06T19:37:31Z"
"","985","LUCENE-9031: test unified highlighter on intervals queries","# Description  Fix UnsupportedOpExc on highlighting Interval queries.   # Solution  Caching and returning Interval queries.   # Tests  Cruel harm to TestUnifiedHighlighting","closed","","mkhludnev","2019-10-30T17:08:25Z","2019-11-24T15:55:11Z"
"","1302","SOLR-14294 fix typo in message","# Description  fix typo in message in response to querying /stream.  # Solution  Typo fix.  PLEASE NOTE that SOLR-14284 addes documentation on the /stream api, and has in the documentation the misspelled version of the word, which should also be updated.  # Tests  manual test.  # Checklist  Please review the following and check all that apply:  - [X ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-02-28T20:07:04Z","2020-04-02T15:38:15Z"
"","1575","SOLR-12823: fix TestZKPropertiesWriter","# Description  Fix TestZKPropertiesWriter that relied on legacy features of the SolrCloud cluster to work. These features were removed.  # Solution  Start a MiniSolrCloudCluster (implies config set and other test resources config) and have the test use the core of a created collection.  # Tests  Test fix.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2020-06-12T23:50:07Z","2020-06-22T16:02:45Z"
"","1565","SOLR-12823: fix test failures","# Description  Fix test failures in CloudHttp2SolrClientTest CloudSolrClientTest TestCloudSolrClientConnections due to previous SOLR-12823 commit.  # Solution  ZkStateReader returns a specific SolrException when expected nodes in the cluster created during cluster initialization do not exist.  # Tests  Fixes the tests mentioned above.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","murblanc","2020-06-10T20:56:50Z","2020-09-16T22:38:13Z"
"","1025","SOLR-13947 document how to load your own streaming plugins","# Description  Fix incorrect JavaDocs on `StreamHandler` on how to add your own expressions  # Solution  JavaDoc fixes and Ref Guide section.  # Tests  n/a  # Checklist  Please review the following and check all that apply:  - [X ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-11-21T12:19:05Z","2019-11-22T19:16:58Z"
"","1010","Update documentation to fix SOLR-13927","# Description  Fix api v2 sample urls to be solrcloud instead of standalone.  # Solution  text change.  # Tests n/a  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-11-13T20:24:22Z","2020-01-09T13:58:15Z"
"","822","SOLR-13680 Auto closed resources after use which implements Closeable or AutoCloseable interface.","# Description  Files, streams or connections which implements Closeable or AutoCloseable interface should be closed after use.  # Solution  Auto closed resources after use which implements Closeable or AutoCloseable interface.  # Tests  No need to write extra tests. Existing tests should not fail after this implementation.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","kamaci","2019-08-05T00:32:32Z","2019-08-10T08:31:54Z"
"","1019","SOLR-13947 stream plugin documentation","# Description  Document how to add your own streaming expressions to /stream request handler.  # Solution  JavaDocs and RefGuide  # Tests  No code changes.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-11-20T03:26:34Z","2019-11-21T02:27:15Z"
"","1164","SOLR-12930: Create developer docs in source repo","# Description  Docs for project contributors and committers has historically lived in the wiki (MoinMoin and now Confluence) but there has been discussion over the past 2 years to move this content into the source repository. We also have a low amount of developer-specific documentation for various features, creating barriers for new contributors to participate, and also for existing committers to learn features they are not yet familiar with.  # Solution  This PR creates a basic structure for project-level and Lucene and Solr specific developer-focused documentation. It includes the following:  - a new top-level directory `dev-docs`, and adds a README file and new documentation geared to PMC Chairs. - a `lucene/dev-docs` directory, and adds a README as a placeholder (to make Git add the directory) - a `solr/dev-docs` directory, and adds a README as a placeholder  We can choose to keep those READMEs or we can delete them/replace them.  # Tests  Not applicable","closed","","ctargett","2020-01-13T21:34:17Z","2020-02-05T12:48:51Z"
"","1238","SOLR-14240: Clean up znodes after shard deletion is invoked","# Description  Delete left over znodes on zookeeper that aren't removed after a delete shard command is issued.  # Solution  In DeleteShardCmd, use the zk client to issue a recursive delete from the specified root and its children (including that root).  # Tests  Added assertions to DeleteShardTest to verify the znodes no longer exist. Ran the full test suite via `ant test` and finally did a manual test via the same steps described in the jira description.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","andyvuong","2020-02-04T18:49:12Z","2020-02-24T21:18:42Z"
"","804","SOLR-13647 default solr.in.sh contains uncommented lines","# Description  default version of this file should be completely commented  ENABLE_REMOTE_JMX_OPTS had defaults  Issue: https://issues.apache.org/jira/browse/SOLR-13647  # Solution  Comment the line in question","closed","","jnyryan","2019-07-23T09:55:54Z","2019-08-12T11:57:05Z"
"","752","LUCENE-8894: Add APIs to look up SPI name for a concrete factory class","# Description  Currently, reflection tricks are needed to obtain SPI name (this is now stored in static NAME fields in each factory class) from a concrete factory class. While it is easy to implement that logic, it would be much better to provide unified APIs to look up SPI name for tokenizer/charfilter/tokenfilter factory classes.  # Solution  Add public static APIs which are signatured `String findSPIName(Class)` to `TokenizerFactory`, `CharFilterFactory`, and `TokenFilterFactory`.  # Tests  See `o.a.l.a.util.TestAbstractAnalysisFactory`.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","mocobeta","2019-06-29T08:42:25Z","2019-06-30T03:36:48Z"
"","951","SOLR-13844: Remove replica recovery terms with the replica term","# Description  Currently if a recovering replica is deleted, the term for that core in the shard's terms in ZK is removed. However the `_recovering` term is not removed as well. This can create clutter and confusion in the shard terms ZK node.  # Solution  When a core is removed from the shardTerms, the corresponding recovering term is removed if it exists.  # Tests  Added a test to make sure that corresponding terms are removed.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","HoustonPutman","2019-10-14T19:26:58Z","2019-11-01T16:40:18Z"
"","730","LUCENE-8868: New storing strategy for BKD tree leaves with low cardinality","# Description  Currently if a leaf on the BKD tree contains only few values, then the leaf is treated the same way as it all values are different. It many cases it can be much more efficient to store the distinct values with the cardinality.  # Solution  The strategy is the following: 1. When writing a leaf block the cardinality is computed. 2. Perform some naive calculation to compute if it is better to store the leaf as a low cardinality leaf. The storage cost are calculated as follows: low cardinality: leafCardinality * (packedBytesLength - prefixLenSum + 2) where two is the estimated size of storing the cardinality. This is an overestimation as in some cases you will only need one byte to store the cardinality. High cardinality: count * (packedBytesLength - prefixLenSum). We are not taking into account the runlen compression. 3. If the tree has low cardinality then we set the compressed dim to -2. Note that -1 is when all values are equal.  # Tests  BKD tree is extensively tested already so there is no need to add new tests.","closed","","iverase","2019-06-19T07:53:39Z","2019-06-26T08:16:13Z"
"","795","SOLR-13643: Create accessors/setters in ResponseBuilder with unit tests","# Description  Create accessors/setters in ResponseBuilder with unit tests.  Also migrate analytics rqeuest handling to use them.  # Solution  I added accessors/setters to ResponseBuilder for the analytics-related fields and unit tests.  # Tests  Unit tests are added to ResponseBuilderTest, and I create a local Solr installation with the changes and verified that analytics queries worked.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the Ref Guide (for Solr changes only).","closed","","nealsid","2019-07-18T07:37:12Z","2019-07-29T23:39:01Z"
"","1208","SOLR-13101: Convert nanotime to ms","# Description  Continue logging ms, but use System.nanoTime instead of System.currentTimeMillis for more precise measurement of elapsed time. Specifically updating references from this previous PR: https://github.com/apache/lucene-solr/pull/1117/  # Solution  Divide nanoseconds by 1000000 to get milliseconds. Converting nanoTime to milliseconds is more precise than using currentTimeMillis because it does not use the system clock time which could face disruptions  # Tests  Ran ant test and manually examined loglines","closed","","ebehrendt","2020-01-24T23:07:08Z","2020-02-05T05:52:49Z"
"","848","SOLR-13257: Cleanup code and make the AffinityReplicaTransformer constructors private","# Description  Cleanup code and make the AffinityReplicaTransformer  constructors private as the constructor is supposed to be called via the static getInstance method.","closed","","anshumg","2019-08-27T22:55:46Z","2019-08-27T23:27:21Z"
"","938","SOLR-13665: Added missing netty dependencies to solrJ","# Description  ClassNotFound error when attempting SSL connection with the new Zookeeper 3.5.5.  # Solution  Added missing netty dependencies to solrJ and upgraded netty to v 4.1.29.Final which is the version ZK uses.  See also https://issues.apache.org/jira/browse/ZOOKEEPER-3494. Probably the new Solr gradle build will ""just work"" from ZK 3.5.6 and later (for 3.5.5 we'd pull in netty-all which is unnecessarily large).  # Tests  Hard to test since we do not have any ZK SSL tests in the code base  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-10-10T10:17:20Z","2019-10-15T08:35:10Z"
"","1577","LUCENE-9390: JapaneseTokenizer discards token that is all punctuation characters only","# Description  Check and omit token that has all punctuation characters when discard punctuation flag is true. Currently, JapaneseTokenizer discards token that has punctuation at first character only.  # Solution  Add isAllPunctuation method for testing token.  # Tests  Ensure to discard if token is all punctuation characters. And not discard if token that start punctuation character and has non-punctuation character.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","johtani","2020-06-13T15:00:48Z","2020-06-18T14:59:22Z"
"","1330","LUCENE-9267 Replace getQueryBuildTime time unit from ms to ns","# Description  Change the documentation of getQueryBuildTime to report nanoseconds instead of milliseconds.  see https://issues.apache.org/jira/browse/LUCENE-9267  # Solution  Update the javadoc.  # Tests  N/A  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","plperron","2020-03-07T17:53:47Z","2020-04-25T10:59:36Z"
"","1342","SOLR-14317: HttpClusterStateProvider throws exception when only one node down","# Description  Catch SolrServerException as well since SolrServerException may throw from fetchLiveNodes(initialClient)  # Solution Catch SolrServerException  # Tests  Provide solrUrls list with two nodes, and shutdown the first node in the list, CloudSolrClient can created successful.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","lyle-w","2020-03-11T14:24:30Z","2020-03-29T01:50:49Z"
"","891","SOLR-13775: Add note about permissions to ""PR Template""","# Description  By default, when a Solr contributor creates their own fork, only they have access to that fork.  This creates a bit of a roadblock when users contribute PRs from these forks - others can't build off of their work - something that is done often with patches.  An initial user might contribute a feature, a committer might add tests or change formatting, etc.  # Solution  This change introduces a bullet point to our PR checklist to nudge users towards making their PR branches more open.  # Tests N/A  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2019-09-19T19:12:43Z","2019-09-19T19:52:54Z"
"","782","LUCENE-8911: Backport LUCENE-8778 (improved analysis SPI name handling) to 8.x","# Description  Backport LUCENE-8778 to 8.x and also keep legacy look-up algorithm for backwards compatibility. See also: https://issues.apache.org/jira/browse/LUCENE-8911  # Solution  Make it possible to load analysis factories by both of new/legacy SPI names.  # Tests  I added tests for the following cases: - A factory lacks the NAME field: the loader registers the legacy name only. - A factory has the NAME field but it does not match to the legacy name, which is derived from the class name: the loader registers both of NAME and legacy name.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - ~~[] I have developed this patch against the `master` branch.~~ - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - ~~[] I have added documentation for the Ref Guide (for Solr changes only).~~","closed","","mocobeta","2019-07-13T09:45:22Z","2019-11-10T10:16:28Z"
"","1103","LUCENE-9102: add maxQueryLength option to DirectSpellChecker","# Description  Attempting to spellcheck some long query terms can trigger org.apache.lucene.util.automaton.TooComplexToDeterminizeException.  (See also https://github.com/apache/lucene-solr/pull/1107 for a corresponding Solr change.)  # Solution  This change adds a maxQueryLength option to DirectSpellchecker so that Lucene can be configured to not attempt to spellcheck terms over a specified length.  # Tests  A new test checks that terms longer than maxQueryLength are not spellchecked.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","andywebb1975","2019-12-19T17:27:21Z","2020-01-15T22:21:17Z"
"","1113","SOLR-14131: adds maxQueryLength option","# Description  Attempting to spellcheck some long query terms can trigger org.apache.lucene.util.automaton.TooComplexToDeterminizeException.  # Solution  This change (previously discussed in SOLR-13190, and dependent on LUCENE-9102) adds a maxQueryLength option to DirectSolrSpellChecker so that Lucene/Solr can be configured to not attempt to spellcheck terms over a specified length.  # Tests  A new test checks that a term is spellchecked before maxQueryLength is reduced, and not afterwards.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","andywebb1975","2019-12-23T15:55:37Z","2020-01-15T22:22:07Z"
"","1504","SOLR-14462: cache more than one autoscaling session","# Description  Allow caching and reusing multiple Sessions for computing replica placement. With a single cached session and fixed timeout, under high collection creation load multiple new sessions are created at once against the same cluster state and placement decisions do not take into account most other placement decisions. Moreover, existing code could not cache more than one session, so most sessions were used for a single placement decision.  # Solution  Multiple sessions can be cached and reused. Although not optimal (parallel sessions do not see the changes done by each other) this is still an improvement over existing implementation because more context is used for placement (sessions are reused). Also, less sessions are created because all created sessions are candidates for reuse (not only a single one).  # Tests  Tests in class TestPolicy were run and adapted to some method signature changes. A new test testMultiSessionsCache explicitly verifying multiple sessions can be cached was added.  # Checklist  Please review the following and check all that apply:  - [X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [X] I have created a Jira issue and added the issue ID to my pull request title. - [X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [X] I have developed this patch against the `master` branch. - [X] I have run `ant precommit` and the appropriate test suite. - [X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","autoscaling,","murblanc","2020-05-11T12:58:59Z","2020-06-24T20:02:49Z"
"","818","SOLR-13672: Zk Status page now parses response from Zookeeper 3.5.5 correctly","# Description  Admin UI/ZK Status page: Zookeeper 3.5 : 4lw.commands.whitellist error Turned out that there is a new line `membership:` in `conf` 4lw response for a quorum that does not follow the `key=value` format, so our parsing crashed.  # Solution  * Be more lenient when parsing zk response and disregard the known `membership:` line * Do not stop parsing when one error occurs, but continue reading response from other ZK hosts, gathering up errors in the errors array to display on top  # Tests  Added a new test that mocks the raw response for `ruok`, `mntr` and `conf` from zk, so we can test how the handler parses the response and maps them to error messages etc.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the Ref Guide (for Solr changes only).","closed","","janhoy","2019-08-02T13:45:52Z","2019-08-07T07:13:50Z"
"","894","SOLR-13638: Add debug,trace RBAP logging","# Description  Adds log messages to RuleBasedAuthorizationPlugin to aid in debugging. Increase log level to DEBUG or TRACE for org.apache.solr.security.RuleBasedAuthorizationPlugin for more helpful output.  # Tests  N/A - logging change only  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","gerlowskija","2019-09-20T18:19:50Z","2019-09-20T18:51:03Z"
"","1632","SOLR-14358 respond to feedback on URLClassifyProcessorFactory","# Description  Addressing feedback from @janhoy originally provided as part of https://github.com/apache/lucene-solr/pull/1372.  The PR #1372 was partially committed, without this feedback being addressed.  # Solution  Add a unit test to confirm single value field and tweak the docs.  # Tests added a test  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ X] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-06-30T15:37:32Z","2020-07-02T19:16:25Z"
"","1033","SOLR-13965: Use Plugin to add new expressions to GraphHandler","# Description  Adding new streaming expression functions is handled differently in StreamHandler from GraphHandler, and I believe they should be handled the same way.  # Solution  I removed the old `` configuration in favor of using plugins in GraphHandler, which was how StreamHandler did it.  # Tests No test changes, as there were none around this previously.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ X] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2019-11-24T19:48:36Z","2020-03-24T14:08:29Z"
"","1161","SOLR-12325 Added uniqueBlockQuery(parent:true) aggregation for JSON Facet","# Description  Added uniqueBlockQuery(parent:true) aggregation for JSON Facet, which is an optimized alternative to uniqueBlock(_root_).   # Solution  Is implemented following the same type of strategy as uniqueBlock, but in this case we check next bit in parents' bitset  - which is faster than getting _root_ values.   # Tests  Covered by the tests analogous to uniqueBlock tests.   # Checklist  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","glincow","2020-01-13T12:37:30Z","2020-04-28T12:19:36Z"
"","1122","SOLR-12490 Added parsing of json.queries for referring in JSON facets","# Description  Added DSL support for queries - to have the ability to refer from domain.filter. It supports both single and multi-value query parameters. Support forexclusion is not yet implemented.  # Solution  Intoduced json.queries field, which is translated with query DSL (see [comment](https://issues.apache.org/jira/browse/SOLR-12490?focusedCommentId=16514420&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16514420)).  # Checklist  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","glincow","2019-12-25T09:47:50Z","2020-04-28T12:19:19Z"
"","1292","SOLR-14284 add expressible support to list, and add example of removing a component","# Description  Add expressible to the list of config supported methods.  # Solution  updated docs.    # Tests  Locally built ref guide.  I manually tested the commands.  # Checklist  Please review the following and check all that apply:  - [ X] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [ X] I have created a Jira issue and added the issue ID to my pull request title. - [ X] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [ X] I have developed this patch against the `master` branch. - [ ] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ X] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","epugh","2020-02-26T21:59:41Z","2020-03-25T19:50:10Z"
"","1607","SOLR-14546: add a Bug Fixes section for Solr 9.0.0 in CHANGES.txt","# Description  Add a ""Bug Fixes"" section in the Solr 9.0.0 section in CHANGES.txt and move the fix to SOLR-14546 into it.  # Solution  Text edit.","closed","","murblanc","2020-06-23T16:28:27Z","2020-06-23T16:54:09Z"
"","1480","SOLR-14456: Fix Content-Type header forwarding on compressed requests","# Description  `HttpSolrCall` and `HttpSolrClient` were using `Content-Encoding` header incorrectly, as they were setting the charset as the encoding for the request. This bug surfaces only when a Solr node is forwarding a request to another node and tries to forward the original headers.  # Solution  Adjusted the behaviour to set the charset encoding as part of the `Content-Type` (as is specified by the HTTP standard) and `Content-Encoding` is now properly forwarded when available.  # Tests  Run the Solr test suites successfully but only from IntelliJ as from the terminal was failing everywhere (from lucene tests to some other random failures, even with badapples set to false).  Regardin new/modified unit tests, none. Since the `HttpSolrCall` has no tests and this takes a really complicated setup to test it, I just wanted to verify that the introduced changes won't break existing suites. Of course I'm open to add them but I'd appreciate some direction on how to build such a test.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","samuelgmartinez","2020-05-03T09:53:13Z","2020-05-15T15:04:40Z"
"","921","SOLR-13816: Move eDisMax params from private interface to public constants","# Description  `DisMaxParams` contains many eDisMax query string parameters and makes them publicly available so that consuming code does not need to rely on ""magic strings"".  Several of these parameters are missing (but are currently being supplied to the `ExtendedDismaxQParser` by a private interface).  # Solution  Rename the private static interface `DMP` to be `ExtendedDisMaxParams`, move it to its own file, and make it public so that these param constants are publicly accessible to consuming code.  # Tests  None.  This is simply an implementation detail - it is renaming an existing private interface and making it public.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","open","","TheSench","2019-10-04T03:17:45Z","2019-10-10T02:42:22Z"
"","1436","SOLR-14413: allow timeAllowed and cursorMark parameters","# Description  [SOLR-14413](https://issues.apache.org/jira/browse/SOLR-14413): allow timeAllowed and cursorMark parameters.  Ever since cursorMarks were introduced in SOLR-5463 in 2014, cursorMark and timeAllowed parameters were not allowed in combination (""Can not search using both cursorMark and timeAllowed""), from [QueryComponent.java](https://github.com/apache/lucene-solr/blob/03363f413f2134594b012175deb3f10ec9384400/solr/core/src/java/org/apache/solr/handler/component/QueryComponent.java#L358): ```java  if (null != rb.getCursorMark() && 0 < timeAllowed) {   // fundamentally incompatible   throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, ""Can not search using both "" + CursorMarkParams.CURSOR_MARK_PARAM + "" and "" + CommonParams.TIME_ALLOWED); }  ``` While theoretically impure to use them in combination, it is often desirable to support cursormarks-style deep paging and attempt to protect Solr nodes from runaway queries using timeAllowed, in the hopes that most of the time, the query completes in the allotted time, and there is no conflict.    However if the query takes too long, it may be preferable to end the query and protect the Solr node and provide the user with a somewhat inaccurate sorted list. As noted in SOLR-6930, SOLR-5986 and others, timeAllowed is frequently used to prevent runaway load.  In fact, cursorMark and shards.tolerant are allowed in combination, so any argument in favor of purity would be a bit muddied in my opinion.     This was discussed once in the mailing list that I can find: https://mail-archives.apache.org/mod_mbox/lucene-solr-user/201506.mbox/%3C5591740B.4080807@elyograg.org%3E It did not look like there was strong support for preventing the combination.     I have tested cursorMark and timeAllowed combination together, and even when partial results are returned because the timeAllowed is exceeded, the cursorMark response value is still valid and reasonable.  # Solution  Remove the parameter check and assume that the implementer submitting the query accepts the tradeoff between accurate sorting, and timebounded queries.  # Tests  I removed the corresponding tests that checked the parameter combination - I didn't see any further insight in those tests as to why the combination wasn't allowed, so I assume they were just providing coverage of that check.  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes. - [x] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","slackhappy","2020-04-17T18:19:09Z","2020-12-23T19:17:55Z"
"","886","SOLR-13767: Upgrade jackson to 2.9.9","# Description  2.9.9 was released in May 2019  # Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [ ] I have added tests for my changes. - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).","closed","","janhoy","2019-09-16T22:54:42Z","2019-09-16T23:13:30Z"
"","957","SOLR-13822: Isolated Classloading from packages","# Description   load plugins from packages  main features:  - A new file for packages definition (/packages.json) in ZK - Public APIs to edit/read the file - The APIs are registered at /api/cluster/package - Classes can be loaded from the package classloader using the : syntax   # Checklist  Please review the following and check all that apply:   - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","noblepaul","2019-10-16T07:18:32Z","2020-03-02T19:49:57Z"
"","928","SOLR-13787: An annotation based system to write v2 only APIs","# Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended) - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","noblepaul","2019-10-05T22:36:39Z","2019-11-08T01:16:14Z"
"","827","SOLR-13682: command line option to export data to a file","# Checklist  Please review the following and check all that apply:  - [x] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability. - [x] I have created a Jira issue and added the issue ID to my pull request title. - [x] I am authorized to contribute this code to the ASF and have removed any code I do not have a license to distribute. - [x] I have developed this patch against the `master` branch. - [x] I have run `ant precommit` and the appropriate test suite. - [x] I have added tests for my changes.","closed","","noblepaul","2019-08-10T06:04:30Z","2019-08-10T06:34:24Z"
"","1665","SOLR-11262: XML writer implements writeMap and writeIterator","","closed","","munendrasn","2020-07-11T07:18:38Z","2020-07-13T06:53:38Z"
"","1663","SOLR-14641: PeerSync, remove canHandleVersionRanges check","","closed","","CaoManhDat","2020-07-10T08:11:42Z","2020-08-10T08:38:04Z"
"","1661","SOLR-14404: test fix","","closed","","noblepaul","2020-07-10T03:44:24Z","2020-11-10T22:45:44Z"
"","1655","SOLR-14634: Limit the HTTP security headers to ""/solr"" end point","","closed","","noblepaul","2020-07-07T12:33:41Z","2020-07-15T06:50:13Z"
"","1649","SOLR-14610: ReflectMapWriter to use VarHandle instead of reflection for branch_8x","","closed","","noblepaul","2020-07-04T03:01:53Z","2020-07-06T00:56:50Z"
"","1642","SOLR-14618 fix link and wordsmith a bit","","closed","","epugh","2020-07-02T19:41:24Z","2020-08-13T14:51:45Z"
"","1641","SOLR-14561: Adding upgrade notes for allowPaths","","closed","","janhoy","2020-07-02T12:00:39Z","2020-07-02T13:39:50Z"
"","1635","SOLR-14610: ReflectMapWriter to use VarHandle instead of old legacy reflection","","closed","","noblepaul","2020-07-01T00:35:16Z","2020-07-04T09:53:31Z"
"","1628","SOLR-14539: Ref Guide changes","","closed","","mkhludnev","2020-06-28T21:40:03Z","2020-07-01T20:37:44Z"
"","1621","SOLR-14404: support for openResource() in PackageResourceLoader","","closed","","noblepaul","2020-06-27T07:44:10Z","2020-07-04T09:53:43Z"
"","1611","Change JoinQuery class's visibility to package again","","closed","","atris","2020-06-24T16:56:35Z","2020-06-24T17:11:34Z"
"","1609","SOLR-14591: Move JoinQuery To Its Own File","","closed","","atris","2020-06-24T15:31:47Z","2020-06-24T16:06:47Z"
"","1599","SOLR-14586: replace the second function parameter in computeIfAbsent …","","closed","optimization,","noblepaul","2020-06-22T13:49:48Z","2020-09-23T08:02:11Z"
"","1598","SOLR-14409: Existing violations allow bypassing policy rules when add…","","closed","autoscaling,","noblepaul","2020-06-22T02:58:17Z","2020-06-25T15:40:36Z"
"","1596","Include delegate in AssertingSimilarity toString","","closed","","tflobbe","2020-06-20T00:11:38Z","2020-06-22T23:38:09Z"
"","1586","SOLR-14576 : Do not use SolrCore as keys in a WeakHashMap","","closed","perf,","noblepaul","2020-06-17T00:26:43Z","2020-10-08T11:10:46Z"
"","1564","LUCENE-9397: UniformSplit supports encodable fields metadata.","","closed","","bruno-roustant","2020-06-10T14:12:23Z","2020-08-31T06:41:45Z"
"","1563","LUCENE-9394: fix and suppress warnings","","closed","","msokolov","2020-06-10T11:27:08Z","2020-06-12T11:25:32Z"
"","1559","SOLR-14552: Add BMW support to ReRank queries","","closed","","tflobbe","2020-06-09T18:41:54Z","2020-06-10T21:59:55Z"
"","1556","SOLR-8392 type safety on SolrParam","","closed","","madrob","2020-06-08T18:15:36Z","2020-06-11T19:21:55Z"
"","1554","LUCENE-9352: Add iteratorCost function to Scorable","","open","","mayya-sharipova","2020-06-05T14:50:48Z","2020-06-05T14:51:46Z"
"","1539","Fix typos in release wizard","","closed","","madrob","2020-05-27T18:29:59Z","2020-06-03T20:27:41Z"
"","1491","LUCENE-9366: Remove unused maxDoc parameter from DocValues.emptySortedNumeric()","","closed","","romseygeek","2020-05-07T11:10:08Z","2020-05-07T13:28:06Z"
"","1478","SOLR-14454: support for UTF-8 (string) types with DocValuesType.BINARY","","open","","magibney","2020-05-01T21:20:36Z","2020-05-01T21:20:36Z"
"","1469","SOLR-14770 Avoid reregistering JVM Guage as well","","closed","","madrob","2020-04-30T02:42:43Z","2020-09-08T15:18:25Z"
"","1462","LUCENE-9328: open group.sort docvalues once per segment.","","open","","mkhludnev","2020-04-27T20:57:50Z","2020-05-14T23:19:32Z"
"","1452","Move audit logging docs under AAA section","","closed","","madrob","2020-04-23T17:56:14Z","2020-04-24T14:52:14Z"
"","1432","SOLR-14404 CoreContainer level custom requesthandlers","","closed","packages,","noblepaul","2020-04-14T13:32:29Z","2020-06-25T05:55:23Z"
"","1422","LUCENE-9311: detect intellij reimport and modify sourceset to exclude…","","closed","","dweiss","2020-04-09T11:35:24Z","2020-04-09T11:55:52Z"
"","1420","package store PUT should be idempotent","","closed","packages,","noblepaul","2020-04-09T03:30:18Z","2020-11-10T22:45:25Z"
"","1395","SOLR-14365: CollapsingQParser - Avoiding always allocate int[] and float[] with size equals to number of unique values (WIP)","","closed","","CaoManhDat","2020-04-02T08:04:33Z","2020-04-10T13:58:29Z"
"","1356","LUCENE-9279: Update dictionary version for Ukrainian analyzer","","closed","","arysin","2020-03-16T14:28:10Z","2020-03-17T21:01:54Z"
"","1355","LUCENE-9279: Update dictionary version for Ukrainian analyzer","","closed","","arysin","2020-03-16T14:14:46Z","2020-04-24T15:25:36Z"
"","1354","LUCENE-9279: Update dictionary version for Ukrainian analyzer","","closed","","arysin","2020-03-15T18:55:56Z","2020-03-16T20:48:38Z"
"","1337","Add 8.6 section to solr CHANGES.txt","","closed","","anshumg","2020-03-11T07:15:34Z","2020-03-11T07:16:28Z"
"","1333","LUCENE-9266 Update smoke test for gradle","","closed","","madrob","2020-03-09T22:24:53Z","2020-03-26T18:47:29Z"
"","1328","LUCENE-9257: Always keep FST off-heap. Remove SegmentReadState.openedFromWriter.","","closed","","bruno-roustant","2020-03-06T12:57:51Z","2020-08-31T06:56:58Z"
"","1327","SOLR-13942: /api/cluster/zk/* to fetch raw ZK data","","closed","","noblepaul","2020-03-06T12:14:45Z","2020-04-27T12:16:07Z"
"","1323","LUCENE-9265: Deprecate SimpleFSDirectory in favor of NIOFSDirectory","","closed","","ywelsch","2020-03-06T09:53:32Z","2020-03-06T10:45:49Z"
"","1321","LUCENE-9264: Remove SimpleFSDirectory in favor of NIOFSDirectory","","closed","","ywelsch","2020-03-06T09:24:30Z","2020-03-06T10:43:13Z"
"","1309","SOLR-13942: /api/cluster/zk/* to fetch raw ZK data","","closed","","noblepaul","2020-03-03T11:21:22Z","2020-03-04T08:29:49Z"
"","1308","SOLR-13942 /api/cluster/zk/* to fetch raw ZK data","","closed","","noblepaul","2020-03-03T05:05:20Z","2020-03-03T19:24:05Z"
"","1305","LUCENE-9257: Make FSTLoadMode enum not BlockTree specific.","","closed","","bruno-roustant","2020-03-02T13:28:05Z","2020-08-31T06:56:35Z"
"","1301","LUCENE-9254: UniformSplit supports FST off-heap.","","closed","","bruno-roustant","2020-02-28T17:15:07Z","2020-08-31T06:42:14Z"
"","1299","SOLR-14274 Do not register multiple sets of JVM metrics","","closed","","madrob","2020-02-27T20:12:41Z","2020-03-25T18:48:24Z"
"","1298","SOLR-14289 Skip ZkChroot check when not necessary","","closed","","madrob","2020-02-27T20:10:41Z","2020-03-13T18:56:23Z"
"","1281","LUCENE-9245: Reduce AutomatonTermsEnum memory usage.","","closed","","bruno-roustant","2020-02-24T09:27:50Z","2020-08-31T06:54:11Z"
"","1275","SOLR-14271: Delete CollectionsAPIAsyncDistributedZkTest.testAsyncIdBackCompat as that is no longer required","","closed","","anshumg","2020-02-21T17:38:14Z","2020-02-21T17:53:22Z"
"","1271","LUCENE-9155: Add Apache License header to the Kuromoji dictionary compilation","","closed","","anshumg","2020-02-20T22:30:29Z","2020-02-21T06:54:35Z"
"","1269","SOLR-14272: Remove autoReplicaFailoverBadNodeExpiration and autoReplicaFailoverWorkLoopDelay for 9.0 as it was deprecated in 7.1","","closed","","anshumg","2020-02-20T20:44:43Z","2020-02-24T18:09:39Z"
"","1268","SOLR-14271: Remove duplicate async id check meant for pre Solr 8 versions","","closed","","anshumg","2020-02-20T20:24:57Z","2020-02-20T23:13:05Z"
"","1266","SOLR-14270 export command to have an option to write to a zip file","","closed","","noblepaul","2020-02-20T15:00:25Z","2020-03-03T19:34:03Z"
"","1254","SOLR-14259: Back porting SOLR-14013 to Solr 7.7","","closed","","noblepaul","2020-02-12T18:30:29Z","2020-04-21T18:23:50Z"
"","1237","SOLR-14238: Fix HdfsDirectory to no longer overwrite existing files.","","closed","","jpountz","2020-02-04T09:56:45Z","2020-02-04T18:35:28Z"
"","1225","SOLR-14233: JsonSchemaCreator should support Map payloads","","closed","","noblepaul","2020-01-30T16:13:15Z","2020-09-16T07:46:24Z"
"","1203","SOLR-14206: Annotate HttpSolrCall as thread-safe","","closed","","anshumg","2020-01-22T22:41:55Z","2020-01-23T16:37:04Z"
"","1201","SOLR-14206: Annotate HttpSolrCall as thread-safe","","closed","","anshumg","2020-01-22T21:05:19Z","2020-01-22T22:39:36Z"
"","1194","LUCENE-9098 Use multibyte code-points for complex fuzzy query","","closed","","madrob","2020-01-21T17:53:10Z","2020-01-21T18:16:50Z"
"","1168","LUCENE-9135: Make uniformsplit.FieldMetadata counters long.","","closed","","bruno-roustant","2020-01-14T15:52:04Z","2020-01-21T10:27:18Z"
"","1147","SOLR-14163: SOLR_SSL_CLIENT_HOSTNAME_VERIFICATION needs to work with Jetty server/client SSL contexts","","closed","","risdenk","2020-01-06T15:04:12Z","2020-01-09T15:29:00Z"
"","1127","Minor fixes to the release wizard.","","closed","","jpountz","2019-12-29T21:16:32Z","2019-12-30T16:21:19Z"
"","1124","SOLR-14151 :Schema components to be loadable from packages","","closed","packages,","noblepaul","2019-12-26T01:40:05Z","2020-08-19T02:20:06Z"
"","1108","SOLR-14125 : Streaming expressions to be loadable from packages","","closed","","noblepaul","2019-12-20T22:15:15Z","2020-03-02T19:49:39Z"
"","1102","SOLR-9039: Enable clientAuth on OSX","","open","","risdenk","2019-12-19T03:32:33Z","2019-12-19T03:32:33Z"
"","1101","SOLR-9093: Enable clientAuth on OSX","","closed","","risdenk","2019-12-19T02:16:52Z","2019-12-19T03:32:27Z"
"","1092","SOLR-14054 -- upgrade to Tika 1.23 (and its dependencies)","","closed","","tballison","2019-12-17T20:37:51Z","2019-12-17T21:09:14Z"
"","1088","SOLR-14101: Move org.apache.solr.util.TimeOut to SolrJ","","closed","","noblepaul","2019-12-16T19:32:45Z","2020-08-05T08:16:58Z"
"","1071","SOLR-14003: Refactor code to avoid reading state from Replica/Slic","","closed","solrcloud,","noblepaul","2019-12-11T11:29:40Z","2020-12-15T03:22:46Z"
"","1068","SOLR-14033: Fix Hadoop tests with security manager","","closed","","risdenk","2019-12-08T01:06:18Z","2019-12-10T21:16:14Z"
"","1057","SOLR-13998: Add thread safety annotations to classes (#1053)","","closed","","anshumg","2019-12-04T06:38:56Z","2019-12-04T06:39:23Z"
"","1052","SOLR-13996: Refactoring HttpShardHandler#prepDistributed() into smaller pieces","","closed","","chatman","2019-12-03T01:51:49Z","2020-03-01T01:29:15Z"
"","1051","SOLR-13992: Refactor code to have collection, shard name in Replica,Slice","","closed","","noblepaul","2019-12-02T22:05:39Z","2020-03-02T19:49:59Z"
"","1039","SOLR-13968: Support postingsFormat and docValuesFormat in schema fields","","closed","","bruno-roustant","2019-11-25T10:59:43Z","2019-12-02T10:24:38Z"
"","1031","LUCENE-9059: Reduce garbage created by ByteBuffersDataOutput.","","closed","","jpountz","2019-11-22T18:46:56Z","2019-11-26T10:41:15Z"
"","1024","LUCENE-9050: MultiTermIntervalsSource should call visitLeaf() in visit","","closed","","romseygeek","2019-11-21T11:14:11Z","2019-11-22T10:29:05Z"
"","1012","LUCENE-8920: Fix flapping TestFstDirectAddressing.testDeDupTails","","closed","","bruno-roustant","2019-11-15T08:35:44Z","2019-12-02T10:19:22Z"
"","992","SOLR-13841: removed jackson dependencies from SolrJ and provided a mapping to our annotation","","closed","","noblepaul","2019-11-02T04:07:46Z","2019-11-08T01:15:24Z"
"","987","SOLR-13884: add ConcurrentCreateCollectionTest test","","closed","","yonik","2019-10-30T19:25:22Z","2020-10-21T11:37:27Z"
"","986","SOLR-13880: Add test that creates/reloads/deletes collections","","open","","tflobbe","2019-10-30T19:04:06Z","2019-10-31T21:45:11Z"
"","982","SOLR-13101: merge in 8.3 changes","","closed","","yonik","2019-10-29T00:50:50Z","2019-10-29T16:35:23Z"
"","965","SOLR-13860: Enable back TestTlogReplica","","closed","","tflobbe","2019-10-22T22:11:20Z","2019-10-23T17:46:41Z"
"","945","SOLR-9985: add docFreq for PointFields","","open","","barrotsteindev","2019-10-13T11:58:53Z","2019-10-13T11:58:53Z"
"","920","SOLR-13815: add simple live split test to help debugging possible issue","","closed","","yonik","2019-10-03T14:30:45Z","2019-10-13T20:13:29Z"
"","918","SOLR-13813: SHARED: add basic test for online shard splitting","","closed","","yonik","2019-10-02T18:07:47Z","2019-11-22T16:25:56Z"
"","916","LUCENE-8213: Asynchronous Caching in LRUQueryCache","","closed","","atris","2019-10-02T12:48:37Z","2019-11-26T06:32:04Z"
"","903","SOLR-13399: add SPLITSHARD splitByPrefix docs","","closed","","yonik","2019-09-27T02:40:05Z","2019-09-27T17:54:23Z"
"","900","test 13722 . branch","","closed","","noblepaul","2019-09-25T21:22:45Z","2019-11-06T00:59:19Z"
"","892","LUCENE-8972: Add ICUTransformCharFilter, to support pre-tokenizer ICU text transformation","","closed","","magibney","2019-09-20T03:31:18Z","2022-04-08T04:34:30Z"
"","882","Add versions.lock entry for ""org.apache.yetus:audience-annotations""","","closed","","anshumg","2019-09-15T19:05:17Z","2019-12-04T06:38:09Z"
"","876","Use The Passed In Threshold Value in doConcurrentSearchWithThreshold","","closed","","atris","2019-09-12T09:36:18Z","2019-09-13T13:18:55Z"
"","863","SOLR-13677: reverting the last commit","","closed","","noblepaul","2019-09-08T07:14:21Z","2019-11-08T01:16:18Z"
"","859","SOLR-13731: javabin must support a 1:1 mapping of the JSON update format","","closed","","noblepaul","2019-09-05T22:19:01Z","2019-11-08T01:16:08Z"
"","847","SOLR-13723 JettySolrRunner should support /api/* (the v2 end point)","","closed","","noblepaul","2019-08-27T09:54:55Z","2019-11-08T01:16:20Z"
"","846","SOLR-13710: Persist package jars locally & expose them over http","","closed","","noblepaul","2019-08-26T12:17:37Z","2019-11-08T01:16:22Z"
"","843","SOLR-13714: Correct refguide regarding shardHandlerFactory solrconfig…","","closed","","magibney","2019-08-23T14:16:57Z","2019-09-12T18:18:36Z"
"","837","SOLR-13650: Support for named global classloaders","","closed","","noblepaul","2019-08-19T10:22:33Z","2019-11-08T01:16:26Z"
"","836","SOLR-13677 All Metrics Gauges should be unregistered by the objects that registered them","","closed","","noblepaul","2019-08-19T09:46:03Z","2019-11-08T01:16:28Z"
"","831","LUCENE-8949: Allow LeafFieldComparators to publish Feature Values","","open","","atris","2019-08-14T06:27:43Z","2019-09-26T07:10:43Z"
"","825","SOLR-13677 All Metrics Gauges should be unregistered by the objects that registered them","","closed","","noblepaul","2019-08-07T12:54:10Z","2020-03-03T19:24:41Z"
"","820","SOLR-13677: All Metrics Gauges should be unregistered by the objects that registered them","","closed","","noblepaul","2019-08-02T21:25:12Z","2020-08-13T15:59:55Z"
"","817","SOLR-13655:Upgrade Collections.unModifiableSet to Set.of and Set.copyOf","","closed","","atris","2019-08-02T10:51:23Z","2019-08-24T03:18:31Z"
"","816","LUCENE-8942: Tighten Up LRUQueryCache's Methods","","closed","","atris","2019-08-01T09:00:07Z","2019-09-05T07:44:04Z"
"","815","LUCENE-8213: Introduce Asynchronous Caching in LRUQueryCache","","closed","","atris","2019-07-31T12:36:21Z","2019-09-28T13:29:01Z"
"","813","SOLR-13659: Refactor Cache config to lazily load the the class","","closed","","noblepaul","2019-07-30T23:04:15Z","2020-07-15T06:47:09Z"
"","806","LUCENE-8931: Remove Custom ScoreDoc Equality Method","","closed","","atris","2019-07-24T06:51:55Z","2019-07-25T17:59:41Z"
"","801","SOLR-13637 Enable loading of plugins from the corecontainer memclassloader","","closed","","noblepaul","2019-07-21T18:22:01Z","2020-07-15T06:45:55Z"
"","798","LUCENE-8906: Lucene50PostingsFormat.IntBlockTermState becomes public","","closed","","bruno-roustant","2019-07-19T13:54:40Z","2019-12-02T10:20:02Z"
"","797","LUCENE-8921: IndexSearcher.termStatistics requires docFreq totalTermFreq instead of TermStates","","closed","","bruno-roustant","2019-07-19T12:39:46Z","2019-12-02T10:25:53Z"
"","796","LUCENE-8927: Set.copyOf and Set.of instead of Collections.unmodifiabl…","","closed","","atris","2019-07-19T08:10:00Z","2019-07-25T18:15:30Z"
"","789","LUCENE-8915 : Improve Javadocs for RateLimiter and SimpleRateLimiter","","closed","","atris","2019-07-16T08:42:34Z","2019-07-26T09:29:22Z"
"","769","LUCENE-8905: Better Error Handling For Illegal Arguments","","closed","","atris","2019-07-08T11:39:55Z","2019-09-05T17:28:53Z"
"","763","SOLR-13608: Incremental backup for Solr","","closed","","CaoManhDat","2019-07-05T03:33:11Z","2021-01-26T18:43:42Z"
"","745","SOLR-13122: Ability to query aliases in Solr Admin UI","","closed","","janhoy","2019-06-26T13:21:34Z","2019-08-30T12:17:32Z"
"","744","LUCENE-8856: Promote intervals queries from sandbox to queries module","","closed","","romseygeek","2019-06-26T10:24:59Z","2019-07-05T08:02:12Z"
"","735","SOLR-13569: AdminUI visual indication of prod/test/dev environment","","closed","","janhoy","2019-06-21T13:45:20Z","2019-06-26T10:27:51Z"
"","727","SOLR-13329: change put: on-each to put: on-each-node","","closed","","noblepaul","2019-06-18T07:57:15Z","2020-11-10T18:22:09Z"
"","719","LUCENE-8861: Script to find open PRs that needs attention","","closed","","janhoy","2019-06-14T11:25:39Z","2019-06-14T11:30:17Z"
"","712","SOLR-13534: Dynamic loading of jars from a remote url","","closed","","noblepaul","2019-06-11T19:33:11Z","2020-07-15T06:43:49Z"
"","710","LUCENE-8852 ReleaseWizard tool","","closed","","janhoy","2019-06-11T08:35:27Z","2019-06-20T12:45:23Z"
"","702","LUCENE-8837 smokeTestRelease.py option --download-only","","closed","","janhoy","2019-06-06T22:29:30Z","2019-06-11T08:17:19Z"
"","701","LUCENE-8836 Optimize DocValues TermsDict to continue scanning from the last position when possible","","open","","bruno-roustant","2019-06-06T12:54:53Z","2022-04-21T17:19:01Z"
"","698","LUCENE-8834: Cache the SortedNumericDocValues.docValueCount() value whenever it is used in a loop","","closed","","tpunder","2019-06-05T17:28:54Z","2019-06-10T06:56:22Z"
"","694","SOLR-13504: In autoscaling policies, use an explicit 'put : on-each'   to specify the the rules is applied on each node","","closed","","noblepaul","2019-06-03T07:49:15Z","2020-07-15T06:44:47Z"
"","693","SOLR-13504 improve autoscaling syntax by adding a nodeset attribute","","closed","","noblepaul","2019-06-02T08:53:15Z","2020-07-15T06:45:02Z"
"","692","SOLR-13504 improve autoscaling syntax by adding a nodeset attribute","","closed","","noblepaul","2019-06-02T08:29:47Z","2019-06-02T08:42:13Z"
"","691","SOLR-13504: improve autoscaling syntax by adding a nodeset attribute","","closed","","noblepaul","2019-05-31T07:34:11Z","2019-05-31T07:45:56Z"
"","686","SOLR-13493: /autoscaling/suggestions to be able to filter by type","","closed","","noblepaul","2019-05-27T19:57:40Z","2019-05-27T21:59:57Z"
"","685","SOLR-13434: OpenTracing support for Solr","","closed","","CaoManhDat","2019-05-23T14:51:48Z","2019-06-04T19:04:20Z"
"","683","SOLR-13484: CHANGES.txt","","closed","","noblepaul","2019-05-23T07:14:25Z","2019-05-23T07:16:20Z"
"","679","LUCENE-8802: buildAndPushRelease --logfile arg","","closed","","janhoy","2019-05-16T23:04:20Z","2019-06-06T19:47:54Z"
"","678","SOLR-13468: autoscaling/suggestions should be able to give suggestions from config sent as a payload","","closed","","noblepaul","2019-05-15T06:58:02Z","2019-05-15T21:36:11Z"
"","672","SOLR-13465 CoreContainer.auditloggerPlugin should be volatile","","closed","","janhoy","2019-05-11T10:10:24Z","2019-11-23T00:22:01Z"
"","671","SOLR-13461: Update Parallel SQL docs to be very clear select * isn't supported","","closed","","epugh","2019-05-10T09:58:39Z","2019-06-21T02:07:09Z"
"","670","Solr 13461","","closed","","epugh","2019-05-10T09:54:24Z","2019-05-10T09:54:39Z"
"","669","Facet2d","","open","missing Jira,","joel-bernstein","2019-05-09T18:08:06Z","2020-03-06T11:18:09Z"