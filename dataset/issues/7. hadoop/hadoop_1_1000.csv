"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","4550","HADOOP-18074 - Partial/Incomplete groups list can be returned in LDAP…","…… (#4503)    ### Description of PR  Description of PR LdapGroupsMapping could return a partial list of group names due to encountering a NamingException while acquiring the RDN for a DN. This was due to not clearing the partially built list which results in the secondary query not being attempted. This PR clears the partially built list and forces the secondary query to be called.  How was this patch tested? Existing unit tests were run and a new unit test added to insure that the secondary query is indeed being called.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","lmccay","2022-07-11T20:28:54Z","2022-07-14T12:54:15Z"
"","4347","HDFS-16586. Purge FsDatasetAsyncDiskService threadgroup; it causes BP…","…… (#4338)  * HDFS-16586. Purge FsDatasetAsyncDiskService threadgroup; it causes BPServiceActor IllegalThreadStateException 'fatal exception and exit'     ### Description of PR  Remove the ThreadGroup used by executor factories; they are unused and ThreadGroups auto-destroy when their Thread-member count goes to zero. This behavior is incompatible with the configuration we have on the per-volume executor which is set to let all threads die if no use inside the keepalive time.  Signed-off-by: Hexiaoqiao   Backport to branch-3.3.  ### How was this patch tested?  By running a downstream test that was failing when this PR was not in place.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","saintstack","2022-05-23T15:56:57Z","2022-05-26T00:02:28Z"
"","4105","YARN-11088. Introduce the config to control the AM allocated to non-e…","…xclusive nodes    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-24T14:35:06Z","2022-03-31T11:55:59Z"
"","4663","HDFS-16699: RBF: Update Observer NameNode state to Active when failover","…ver because of sockeTimeOut Exception    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","SanQiMax","2022-07-31T03:46:41Z","2022-08-02T16:22:10Z"
"","3773","YARN-11014. YARN incorrectly validates maximum capacity resources on …","…the validation API. Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-08T17:47:44Z","2022-03-02T13:23:35Z"
"","3772","YARN-11014. YARN incorrectly validates maximum capacity resources on …","…the validation API. Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-08T17:45:31Z","2022-03-02T13:22:08Z"
"","3715","YARN-11014. YARN incorrectly validates maximum capacity resources on …","…the validation API    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-11-23T15:55:47Z","2021-12-07T07:44:42Z"
"","3734","YARN-11020. [UI2] No container is found for an application attempt with a single AM container","…th a single AM container    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-11-29T10:31:09Z","2021-12-07T07:51:57Z"
"","4427","HADOOP-18106: Handle memory fragmentation in S3A Vectored IO","…tation.  part of HADOOP-18103. Handling memoroy fragmentation in S3A vectored IO implementation by allocating smaller user range requested size buffers and directly filling them from the remote S3 stream and skipping undesired data in between ranges. This patch also adds aborting active vectored reads when stream is closed or unbuffer is called.    ### Description of PR   ### How was this patch tested? Added new tests, ran all existing tests.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-06-10T21:04:21Z","2022-07-11T18:23:33Z"
"","3785","YARN-11042. Fix testQueueSubmitWithACLsEnabledWithQueueMapping in Tes…","…tAppManager  Change-Id: I8ba1586d38610e6f737b3cb3a26a222f62889950    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-12-10T12:52:56Z","2022-03-02T15:20:02Z"
"","4289","YARN-11123. ResourceManager webapps test failures due to org.apache.hadoop.metrics2.MetricsException and subsequent java.net.BindException: Address already in use","…stem.setMiniClusterMode(true) in setup to prevent throwing an exception when registering the metrics source more than once  ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-05-08T21:07:04Z","2022-05-13T11:20:38Z"
"","3948","YARN-7898. [FederationStateStore] Create a proxy chain for Federation…","…StateStore API in the Router    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2022-01-31T19:33:47Z","2022-02-01T17:58:06Z"
"","3926","HDFS-16437 ReverseXML processor doesn't accept XML files without the …","…SnapshotDiffSection    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-01-25T10:14:09Z","2022-02-06T05:05:58Z"
"","4338","HDFS-16586. Purge FsDatasetAsyncDiskService threadgroup; it causes BP…","…ServiceActor IllegalThreadStateException 'fatal exception and exit'      ### Description of PR  Remove the ThreadGroup used by executor factories; they are unused and ThreadGroups auto-destroy when their Thread-member count goes to zero. This behavior is incompatible with the configuration we have on the per-volume executor which is set to let all threads die if no use inside the keepalive time.  ### How was this patch tested?  Yes. The downstream project test that was failing passes with this PR in place  Adding a test as part of this PR would be awkward.  ### For code changes:  - [x ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","saintstack","2022-05-20T22:29:06Z","2022-05-23T15:45:27Z"
"","3919","YARN-11067. Resource overcommitment due to incorrect resource normali…","…sation logical order    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-24T09:00:49Z","2022-03-10T21:23:55Z"
"","4439","YARN-11182. Refactor TestAggregatedLogDeletionService: 2nd phase","…s to variables  ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-06-14T09:19:00Z","2022-06-20T12:14:10Z"
"","4430","YARN-11176. Refactor TestAggregatedLogDeletionService","…s to variables  ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-06-11T15:06:20Z","2022-06-14T14:15:33Z"
"","4200","YARN-11111. Recovery failure when node-label configure-type transit f…","…rom delegated-centralized to centralized    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-04-19T10:59:18Z","2022-04-22T02:38:43Z"
"","3797","HDFS-16382. RBF: getContentSummary RPC compute sub-directory repeatedly","…rectory repeatedly    ### Description of PR  Router getContentSummary rpc compute sub-directory repeatedly when a direactory and its ancestor directory are both mounted  in the form of original src path.  For example, suppose we have mount table entries below:  /A--ns1--/A  /A/B—ns1,ns2—/A/B  we put a file test.txt to directory /A/B in namepsace ns1, then execute `hdfs dfs -count  hdfs://router:8888/A`,  the result is wrong, because we compute /A/B/test.txt repeatedly   ### How was this patch tested?  construct mount table entries in real cluster   ### For code changes: https://issues.apache.org/jira/browse/HDFS-16382","open","","hfutatzhanghb","2021-12-14T09:46:51Z","2021-12-20T12:16:39Z"
"","4180","YARN-11110. An implementation for using CGroups to control the numbe…","…r of the process in container    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ChaosJu","2022-04-15T14:29:45Z","2022-04-18T05:09:20Z"
"","3735","YARN-10975 EntityGroupFSTimelineStore#ActiveLogParser parses already …","…processed files  ### Description of PR EntityGroupFSTimelineStore#ActiveLogParser parses already processed files again and again even though there is no change in the file. This leads to unnecessary load on DFS where summary files reside and Timeline Store where timeline entities are present.  ### How was this patch tested? Manually on clusters and also running UTs","closed","","Sushmasree-28","2021-11-29T10:43:00Z","2021-11-29T18:30:25Z"
"","3896","YARN-10995. Move PendingApplicationComparator from GuaranteedOrZeroCa…","…pacityOverTimePolicy    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-01-17T18:22:18Z","2022-02-17T18:44:20Z"
"","3893","YARN-10580. Fix some issues in TestRMWebServicesCapacitySchedDynamicC…","…onfig  Change-Id: I4f42bcb2946aa21d8adebe19faeaf63403fd2484    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-01-17T08:50:02Z","2022-02-17T18:18:09Z"
"","4163","HADOOP-18199 Fix ZStandardCompressor bytesRead meta when using BlockC…","…ompressorStream","closed","","lichaojacobs","2022-04-12T08:43:06Z","2022-04-12T12:12:20Z"
"","4060","YARN-11084. Introduce new config to specify AM default node-label whe…","…n not specified    ### Description of PR #### What When submitting application to Yarn and user don't specify any node-label on AM request and ApplicationSubmissionContext, we hope that Yarn could provide the default AM node-label.    #### Why Yarn cluster in our internal company exists on-premise NodeManagers and elastic NodeManagers (which is built on K8s). To prevent application instability due to elastic NM decommission, we hope that the AM of job can be allocated to on-premise NMs.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-10T08:30:42Z","2022-03-24T15:02:18Z"
"","4066","YARN-11087. Introduce the config to control the refresh interval in R…","…MDelegatedNodeLabelsUpdater    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-13T08:37:52Z","2022-03-24T02:19:05Z"
"","4121","YARN-11106. Fix the test failure due to missing conf of yarn.resource…","…manager.node-labels.am.default-node-label-expression    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-30T13:58:38Z","2022-03-30T17:45:28Z"
"","4230","HDFS-16558 Consider changing the lock of delegation token from write …","…lock to read lock    ### Description of PR   In a very busy authed cluster, renewing/caneling/getting delegation token get slow and it will slow down the speed of handling rpcs from client. Since AbstractDelegationTokenSecretManager is a thread-safe manager, we propose to change the fs lock from write lock to read lock(protect editlog rolling)","open","","yuanboliu","2022-04-24T07:54:33Z","2022-04-28T02:42:47Z"
"","4255","HADOOP-18217. ExitUtil synchronized blocks reduced to avoid exit bloc…","…king halt + enlarged catches (robustness) to all Throwables (not just Exceptions)    ### Description of PR I've reduced the synchronized blocks scope so System.exit and Runtime.halt calls aren't within their boundaries, so ExitUtil wrappers do not block each others (System.exit never returns if called and no SecurityException is raised, so ExitUtil.terminate was never releasing the acquired lock, thus forbidding ExitUtil.halt to be able to halt the JVM even when in the middle of a graceful shutdown, thus it was not behaving like the 2 wrapped java's methods System.exit/Runtime.halt which do not block each other) I've altered throwable handling:   - what is catched: was nothing or only Exception, now all Throwables are catched (even ThreadDeath)   - what is rethrown: when exit/halt has been disabled, if what was catched is an Error it will be rethrown rather than the initial ExitException/HaltException. Other Throwables will be added as suppressed to the Exit/HaltException   - what wasn't catched: if not disabled, even is something was raised that wasn't catched before, it is now catched and System.exit/Runtime.halt is always called   - what is suppressed: if the what needs to be rethrown is changed on the way, the newly to-be-thrown will have the old one as a suppressed Throwable. I've also done this for the Exit/Halt Exception that can supress Throwables that are not Error (might not be a so good idea)  ### How was this patch tested? No more tests than the existing ones (if any). This case is not really hard to reproduce but the test would need to exit a JVM. I've not added such tests because if unit does not fork, it would kills the test suite (thus impacting all tests). I think developing a robust test for this specific case is way more hard and  dangerous to offset the cost of a review, the risk of what could be missed by this review.  Easiest way can be reproduced the initial bug: having a shutdown hook call ExitUtil.terminate, have another thread that will call ExitUtil.halt after (use pauses to ensure it calls it after the hook), witness the JVM not stopping and needing either an external kill or a internal Runtime.halt call, maybe check the JVM threads' stacks too to view the ExitUtil.terminate call stuck on System.exit, and ExitUtil.halt call stuck on ExitUtil.terminate.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","HerCath","2022-05-02T08:39:20Z","2022-07-13T11:41:50Z"
"","3737","YARN-11023. Extend the root QueueInfo with max-parallel-apps in Capac…","…ityScheduler.  Change-Id: I2235e5a6b45d469ad39e2006a82caf4c13d42ec4    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-11-30T15:32:12Z","2021-12-07T14:27:15Z"
"","3868","YARN-10590. Fix legacy auto queue creation absolute resource calculat…","…ion loss    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-06T12:52:33Z","2022-02-22T11:28:02Z"
"","4624","HADOOP-18364. All method metrics related to the RPC protocol should be …","…initialized.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zhangshuyan0","2022-07-25T10:23:28Z","2022-08-03T15:38:36Z"
"","4233","HDFS-16559 Seperate IBR thread waiting time from heart beat waiting t…","…ime.    ### Description of PR  IBR opeeration is seperated from heartbeat thread since [HDFS-16016](https://issues.apache.org/jira/browse/HDFS-16016), which is great.   But IBR thread is using the expireTime of heart beat to deside whether wait or not. In a high load DataNode, IBR thread becomes an infinit loop without sleeping and consuming 100% cpu because of the latency of heartbeat reporting.","open","","yuanboliu","2022-04-24T10:11:06Z","2022-04-24T19:05:45Z"
"","3814","YARN-10503. Support queue capacity in terms of absolute resources wit…","…h custom  resourceType. Contributed by Qi Zhu.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-17T16:52:52Z","2021-12-20T16:59:36Z"
"","4468","YARN-11188. Only files belong to the first file controller are removed even if multiple log aggregation file controllers are configured","…gationFilesBuilder  ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-06-20T09:11:23Z","2022-06-22T12:40:44Z"
"","3763","YARN-10850 Fix the application attempt ID query with the correspondin…","…g filter.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ghost","2021-12-08T09:04:14Z","2022-04-12T08:46:16Z"
"","4170","HDFS-16540 Data locality is lost when DataNode pod restarts in kubern…","…etes    ### Description of PR When Dn with the same uuid is registered with a different ip, host2DatanodeMap needs to be updated accordingly.  ### How was this patch tested? Tested 3.3.2 with the patch on a eks cluster, restarted the pod hosting DataNode and HBase region server. After that, doing a major compaction of Hbase region, made sure that locality is kept.  There is also a new unittest case added.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","huaxiangsun","2022-04-13T19:51:21Z","2022-05-18T13:02:42Z"
"","4443","YARN-11174. Setting user limit factor on dynamic queues at runtime do…","…es not propagate to ConfigurationProperties    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","9uapaw","2022-06-15T12:06:01Z","2022-06-17T11:49:30Z"
"","3892","YARN-10894. Follow up YARN-10237: fix the new test case in TestRMWebS…","…ervicesCapacitySched.  Change-Id: Idb6c31647f7e547fa9235117b34ec0d7cb6172c3    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-01-17T08:38:42Z","2022-02-17T18:19:22Z"
"","4092","HADOOP-18167. Add metrics to track delegation token secret manager op…","…erations    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","hchaverri","2022-03-22T17:09:53Z","2022-05-05T23:20:25Z"
"","4447","HDFS-16591. Setup JaasConfiguration in ZKCuratorManager when SASL is …","…enabled    ### Description of PR Setting up the JaasConfiguration when creating a new ZKCuratorManager, to allow ZK connections via SASL. Also removing duplicated classes of JaasConfiguration.  ### How was this patch tested? Ran the following unit tests: TestJaasConfiguration TestZKCuratorManager TestZKSignerSecretProvider TestZKDelegationTokenSecretManager TestMicroZookeeperService  Created a TestDelegationTokenSecretManager to replace the default ZKDelegationTokenSecretManagerImpl and deployed to an RBF router. Without these changes, the router initialization will fail with the error described on HDFS-16591. Initialization succeeds with this patch.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","hchaverri","2022-06-16T17:46:06Z","2022-06-28T23:46:21Z"
"","3741","YARN-11026. Make default AppPlacementAllocator configurable in AppSch…","…edulingInfo    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","minni31","2021-12-01T08:24:27Z","2022-02-01T06:48:54Z"
"","4068","HDFS-16504. Add parameter for NameNode to process getBloks request","…eckOperation when NN process getBlocks    ### Description of PR https://issues.apache.org/jira/browse/HDFS-16504  ### How was this patch tested? no new test  ### For code changes: add new config `dfs.namenode.get-blocks.check.operation`.    Whether enable checkOperation when Namenode process getBlocks.  It is enabled (true) by default.","closed","","Neilxzn","2022-03-14T03:02:26Z","2022-03-20T06:18:01Z"
"","3820","HDFS-16391. Avoid evaluation of LOG.debug statement in NameNodeHeartbeatService","…eatService    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?  JIRA: https://issues.apache.org/jira/browse/HDFS-16391?filter=-2","closed","","wzhallright","2021-12-20T06:05:21Z","2021-12-21T11:28:58Z"
"","3709","YARN-9532. SLSRunner.run() throws a YarnException when it fails to cr…","…eate the output directory    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-11-23T10:04:43Z","2022-02-01T01:32:05Z"
"","3748","HADOOP-16905. Update jackson-databind to 2.10.3 to relieve us from th…","…e endless CVE patches. (#1876)  (cherry picked from commit 69faaa1d58ad7de18a8dfa477531653a2c061568)   Conflicts: 	hadoop-project/pom.xml    ### Description of PR  PR for testing by Jenkins. Backport HADOOP-16905 to branch-3.2. I'll backport HADOOP-17534 next.  ### How was this patch tested?  Not tested.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-03T15:14:27Z","2021-12-10T07:24:24Z"
"","4313","YARN-11152. QueueMetrics is leaking memory when creating a new queue …","…during reinitialisation    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-05-16T08:41:19Z","2022-05-18T16:29:46Z"
"","4198","YARN-11112 Avoid renewing delegation token when app is first submitte…","…d to RM    ### Description of PR When auth is enabled by NameNode, then delegation token is required if application needs to acess files/directoies. We find that when app is first submitted to RM, RM renewer will renew app token no matter whether token is expired or not. Renewing token is a bit heavy since it uses global write lock. Here is the result when delegation token is required in a very busy cluster.   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","yuanboliu","2022-04-19T03:25:39Z","2022-07-01T04:35:01Z"
"","4324","YARN-11126. ZKConfigurationStore Java deserialisation vulnerability. …","…Contributed by Tamas Domok  Change-Id: Ie5f8c1c14f57e209c5d99756f4c75aa039780f4e    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-05-18T06:57:47Z","2022-05-18T12:25:58Z"
"","4325","YARN-11126. ZKConfigurationStore Java deserialisation vulnerability. …","…Contributed by Tamas Domok  Change-Id: Ibfdada7371e6b152eb9dcba8e22e62f5ae64b0aa    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-05-18T07:17:05Z","2022-05-18T12:24:16Z"
"","4442","YARN-11185. Pending app metrics are increased doubly when a queue rea…","…ches its max-parallel-apps limit    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-06-15T12:01:19Z","2022-06-20T13:04:38Z"
"","4021","YARN-10565. Refactor CS queue initialization to simplify weight mode …","…calculation    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-23T16:25:44Z","2022-03-21T13:57:02Z"
"","4019","YARN-11022. Fix the documentation for max-parallel-apps in CS. Contri…","…buted by Tamas Domok    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-02-23T08:50:50Z","2022-03-02T15:13:50Z"
"","4301","YARN-11147. ResourceUsage and QueueCapacities classes provide node la…","…bel iterators that are not thread safe    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-05-11T16:08:59Z","2022-05-18T16:20:24Z"
"","4242","YARN-11116. Migrate Times util from SimpleDateFormat to thread-safe D…","…ateTimeFormatter class    ### Description of PR Upgrading to the recommended thread safe java.time libraries for formatting and parsing with a little better performance  ### How was this patch tested? Attached a performance test patch to the jira and added unit test to ensure compatible behavior  ### For code changes:  - [ x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jteagles","2022-04-28T02:57:57Z","2022-05-02T18:36:53Z"
"","4005","HDFS-16429. Add DataSetLockManager to manage fine-grain locks for FsD…","…ataSetImpl. (#3900). Contributed by limingxiang.  Signed-off-by: He Xiaoqiao     ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","yuanboliu","2022-02-21T07:48:02Z","2022-02-21T13:31:12Z"
"","4622","YARN-11063. Support auto queue creation template wildcards for arbitr…","…ary queue depths    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","K0K0V0K","2022-07-25T09:55:47Z","2022-07-28T15:33:13Z"
"","4599","YARN-11211. QueueMetrics leaks Configuration objects when validation …","…API is called multiple times    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-07-20T14:38:09Z","2022-07-21T12:20:44Z"
"","4173","HADOOP-15133. [JDK9] Ignore com.sun.javadoc.* and com.sun.tools.* in …","…animal-sniffer-maven-plugin to compile with Java 9.  (cherry picked from commit d2d8f4aeb3e214d1a96eeaf96bbe1e9301824ccd)    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mccormickt12","2022-04-14T22:39:47Z","2022-04-15T12:24:15Z"
"","4137","HADOOP-16104. Wasb tests to downgrade to skip when test a/c is namesp…","…ace enabled. Contributed by Masatake Iwasaki.  (cherry picked from commit aa3ad3660506382884324c4b8997973f5a68e29a)    ### Description of PR cherry-pick of aa3ad3660506382884324c4b8997973f5a68e29a from trunk to branch-2.10 The unit tests that fail in this PR are also failing in the vanilla branch-2.10  @omalley  can you please review it?  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","arjun4084346","2022-04-04T18:44:37Z","2022-04-14T00:29:33Z"
"","4312","YARN-11141. Capacity Scheduler does not support ambiguous queue names…","… when moving application across queues    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-05-16T08:22:22Z","2022-05-18T12:34:45Z"
"","4297","YARN-11141. Capacity Scheduler does not support ambiguous queue names…","… when moving application across queues    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-05-10T12:31:49Z","2022-05-11T12:29:42Z"
"","3869","YARN-10944. AbstractCSQueue: Eliminate code duplication in overloaded…","… versions of setMaxCapacity    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-06T13:19:36Z","2022-02-22T10:56:26Z"
"","4102","YARN-10720. (branch-3.2) YARN WebAppProxyServlet should support connection timeout…","… to prevent proxy server from hanging. Contributed by Qi Zhu.  (cherry picked from commit a0deda1a777d8967fb8c08ac976543cda895773d)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java    ### Description of PR  Backport YARN-10720 to branch-3.2. I had to fix the conflicts by partiallly backporting YARN-8448, so I want to run precommit job for this change. If it ran successfully, I will push it.  ### How was this patch tested?  Compiled and ran TestWebAppProxyServlet locally.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-03-24T06:35:07Z","2022-03-24T08:36:47Z"
"","4103","YARN-10720. (branch-2.10) YARN WebAppProxyServlet should support connection timeout…","… to prevent proxy server from hanging. Contributed by Qi Zhu.  (cherry picked from commit a0deda1a777d8967fb8c08ac976543cda895773d)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/resources/yarn-default.xml 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java  (cherry picked from commit dbeb41b46a2cef9a48983add9c3b191915bd6de5)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java    ### Description of PR  Backport YARN-10720 to branch-2.10. This PR is to run the precommit job for this backport. I also opened #4102 to backport to branch-3.2.  ### How was this patch tested?  Ran TestWebAppProxyServlet locally.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-03-24T07:00:27Z","2022-03-24T08:37:12Z"
"","4042","HADOOP-18149 Allow S3 and other potential fs implementations to bypass verifyAndDownload IO Exception","… time behaviors that would cause verifyAndCopy to throw an IO exception    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cbevard1","2022-03-01T15:51:31Z","2022-03-04T22:06:30Z"
"","3808","YARN-11033. isAbsoluteResource is not correct for dynamically created…","… queues. Contributed by Tamas Domok    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-12-16T14:37:16Z","2022-03-02T13:45:56Z"
"","4150","YARN-11107:When NodeLabel is enabled for a YARN cluster, AM blacklist…","… program does not work properly.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zhangxiping1","2022-04-08T07:41:20Z","2022-04-15T03:32:06Z"
"","4139","HDFS-16524. Add configuration to control blocks deletion asynchronous…","… or synchronous    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ted12138","2022-04-05T07:27:46Z","2022-07-31T16:21:02Z"
"","3760","YARN-11037. Add configurable logic to split resource request to least…","… loaded SC    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-12-07T14:33:58Z","2022-02-04T15:31:15Z"
"","4503","HADOOP-18074 - Partial/Incomplete groups list can be returned in LDAP…","… groups lookup  ### Description of PR LdapGroupsMapping could return a partial list of group names due to encountering a NamingException while acquiring the RDN for a DN. This was due to not clearing the partially built list which results in the secondary query not being attempted. This PR clears the partially built list and forces the secondary query to be called.  ### How was this patch tested? Existing unit tests were run and a new unit test added to insure that the secondary query is indeed being called.  ### For code changes:  - [X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","lmccay","2022-06-25T18:08:44Z","2022-07-14T16:25:16Z"
"","4316","YARN-10850. TimelineService v2 lists containers for all attempts when…","… filtering for one. Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-05-16T12:58:46Z","2022-05-18T12:09:43Z"
"","4315","YARN-10850. TimelineService v2 lists containers for all attempts when…","… filtering for one. Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-05-16T12:44:56Z","2022-05-18T12:06:24Z"
"","4165","YARN-10850. TimelineService v2 lists containers for all attempts when…","… filtering for one    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-04-12T14:33:21Z","2022-05-11T12:40:26Z"
"","4014","YARN-11075. Explicitly declare serialVersionUID in LogMutation class.…","… Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-22T17:55:44Z","2022-03-01T17:09:09Z"
"","4013","YARN-11075. Explicitly declare serialVersionUID in LogMutation class.…","… Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-22T17:54:53Z","2022-03-01T17:08:08Z"
"","4012","YARN-11075. Explicitly declare serialVersionUID in LogMutation class.…","… Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-22T17:54:15Z","2022-03-01T17:09:25Z"
"","3771","YARN-6862. Nodemanager resource usage metrics sometimes are negative.…","… Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-08T17:39:58Z","2021-12-17T13:53:43Z"
"","3770","YARN-6862. Nodemanager resource usage metrics sometimes are negative.…","… Contributed by Benjamin Teke    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-08T17:36:24Z","2021-12-17T13:52:20Z"
"","4080","HADOOP-12760. sun.misc.Cleaner has moved to a new location in OpenJDK…","… 9. Contributed by Akira Ajisaka.  (cherry picked from commit 5d084d7eca32cfa647a78ff6ed3c378659f5b186)    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","mccormickt12","2022-03-17T20:11:32Z","2022-03-25T22:08:55Z"
"","3738","YARN-11024. Create an AbstractLeafQueue to store the common LeafQueue…","… + AutoCreatedLeafQueue functionality    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-11-30T18:19:54Z","2021-12-13T15:33:02Z"
"","4083","HADOOP-18129: Change URI to String in INodeLink to reduce memory footprint of ViewFileSystem","…  Fixes #3996  (cherry picked from commit da9970dd697752b4d00fe4e4760ea9cbf019ff2e)    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","abhishekdas99","2022-03-18T03:37:37Z","2022-03-30T22:22:11Z"
"","4184","HADOOP-18209. Fix links to DataNodes in NameNode UI to support IPv6","…    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-17T17:18:49Z","2022-05-03T16:38:41Z"
"","4632","YARN-5871. [RESERVATION] Add support for reservation-based routing.","YARN-5871. [RESERVATION] Add support for reservation-based routing.  When completing the pr of YARN-11177, I found that YARN Federation has limited support for Reservation System.  After looking at Jira, I found that YARN-5871 implements some of the functions of Router Reservation System, I continue to follow this pr. However, the original code of YARN-5871 has changed too much for the Router Policy And it can no longer be directly merged into the current version of trunk.  I refer to this part of the code to try to be compatible with the current version of the Router.  After reading the code of this PR, I will describe the design idea of ​​this PR. This part of the work is in progress, first ensure that the PR can be compiled normally and passed the unit test.","open","","slfan1989","2022-07-26T13:14:17Z","2022-07-28T15:54:19Z"
"","4510","YARN-11203. Fix typo in hadoop-yarn-server-router module.","YARN-11203. Fix typo in hadoop-yarn-server-router module.","closed","","slfan1989","2022-06-28T22:54:42Z","2022-07-24T02:08:17Z"
"","4388","YARN-11167. impove import * In YARN Project.","YARN-11167. impove import * In YARN Project.  Directly using * to reference does not conform to the code specification, adjust it and refer to the specified package.","closed","","slfan1989","2022-05-31T16:10:19Z","2022-06-01T16:18:38Z"
"","4540","YARN-11160. Support getResourceProfiles, getResourceProfile API's for Federation","YARN-11160. Support getResourceProfiles, getResourceProfile API's for Federation.","closed","","slfan1989","2022-07-09T02:38:32Z","2022-07-21T23:40:58Z"
"","4249","YARN-11120-Metrics for Federation getClusterMetrics","YARN-11120-Metrics for Federation getClusterMetrics  ### Description of PR Recently, I studied and researched Yarn's federation-related functions. I found that many methods have not been implemented. I chose the getClusterMetrics method, but found that this method has been merged into Trunk.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","slfan1989","2022-04-30T00:41:10Z","2022-05-06T14:10:31Z"
"","4175","YARN-11107. Addendum. When NodeLabel is enabled for a YARN cluster, A…","YARN-11107. Addendum. When NodeLabel is enabled for a YARN cluster, AM blacklist program does not work properly.","closed","","zhangxiping1","2022-04-15T02:03:26Z","2022-04-15T08:51:19Z"
"","4481","HADOOP-18307. Remove hadoop-cos as a dependency of hadoop-cloud-storage.","Workaround for HADOOP-18159; this ensures that projects declaring a dependency on hadoop-cloud-storage do _not_ have their s3 http connections broken by an out of date mozilla/public-suffix-list.txt resource on the classpath.  Contributed by Steve Loughran    ### Description of PR   ### How was this patch tested?  mvn clean install; verified dependencies of the module, full dist build and verification it is not in tools/lib  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-21T14:20:36Z","2022-06-27T10:48:05Z"
"","4453","WIP: Explore removing javax.ws.rs-api","WIP: Just to know what all break so that I can figure out a fix....","open","","ayushtkn","2022-06-17T18:24:41Z","2022-06-18T03:18:54Z"
"","4319","HADOOP-18241. Move to JAVA 11.","WIP","open","","ayushtkn","2022-05-17T09:58:12Z","2022-06-11T11:42:58Z"
"","3842","HDFS-16403. Improve FUSE IO performance by supporting FUSE parameter max_background","When we examining the FUSE IO performance on HDFS, we found that the simultaneous IO requests number are limited to a fixed number, like 12. This limitation makes the IO performance on FUSE client quite unacceptable. We did some research on this and inspired by the article  [Performance and Resource Utilization of FUSE User-Space File Systems](https://dl.acm.org/doi/fullHtml/10.1145/3310148), clearly the FUSE parameter `max_background` decides the simultaneous IO requests number, which is 12 by default.  We add `max_background` to fuse_dfs mount options,  the FUSE kernel will take effect when an option value is given.  To check it works: ``` cat /sys/fs/fuse/connections/55/max_background 100 ```","closed","","cndaimin","2021-12-30T08:23:09Z","2022-01-25T07:59:12Z"
"","4033","HDFS-16485. [SPS]: allow re-satisfy path after restarting sps process","When SPSPathIdProcessor thread call getNextSPSPath(), it get the pathId from namenode and namenode will also remove this pathId from pathsToBeTraveresed queue. ```java public Long getNextPathId() {   synchronized (pathsToBeTraveresed) {     return pathsToBeTraveresed.poll();   } }  ``` If SPS process restart, this path will not continue the move operation until namenode restart.  So we want to provide a way for the SPS to continue performing the move operation after SPS restart.  First solution:   1) When SPSPathIdProcessor thread call getNextSPSPath(), namenode return pathId and then move this pathId to a pathsBeingTraveresed queue;  2) After SPS finish a path movement operation, it call a rpc to namenode to remove this pathId from pathsBeingTraveresed queue;  3) If SPS restart, SPSPathIdProcessor thread should call a rpc to namenode to get all pathId from pathsBeingTraveresed queue;  Second solution:  We added timeout detection in the application layer, if a path does not complete the movement within the specified time, we can re-satisfy this path even though it has ""hdfs.sps"" xattr already.  We choose the second solution because the first solution will add more rpc operation and may affect namenode performance.","open","","liubingxing","2022-02-25T09:18:40Z","2022-02-28T08:16:13Z"
"","4621","Hadoop 18319: ABFS: ABFS: Fix FileSystemAlreadyExists error for AzureAD or other internal errors","When remote file system creation was enabled, any exception for getFileStatus on the root path was leading to filesystem create being called. This was leading to FILESYSTEM_ALREADY_EXISTS if it was trying to create an existing filesystem.  Added fix for the same.","open","","anmolanmol1234","2022-07-25T06:33:11Z","2022-07-25T12:13:11Z"
"","4155","HDFS-16533. COMPOSITE_CRC failed between replicated file and striped file due to invalid requested length","When I modifying some codes about `getFileChecksum` and found that `getFileChecksum` will return an error composite checksum with a special length.  The code that is suspected to have a bug is as follows:  ```java       long reportedLastBlockSize =           blockLocations.getLastLocatedBlock().getBlockSize();       long consumedLastBlockLength = reportedLastBlockSize;       if (length - sumBlockLengths < reportedLastBlockSize) {         LOG.warn(             ""Last block length {} is less than reportedLastBlockSize {}"",             length - sumBlockLengths, reportedLastBlockSize);         consumedLastBlockLength = length - sumBlockLengths;       } ```  Suppose there is a file with 4 blocks,  namely block1, block2, block3 and block4, and the size of each blocks is 10MB, 10MB, 10MB, 7MB respectively.    When I try to get a composite checksum by `getFileChecksum` with length 29MB, an incorrect checksum is returned. Because of 29MB, only involved block1(10MB), block2(10MB) and block3(9MB),  doesn't involve the last block4 of the file.  But code line323 ~ line331 is checking the last block4 of the file, caused an incorrect composite checksum. In theory, we just need to check the remaining length with block3 not last block of the file, block4.","closed","","ZanderXu","2022-04-09T15:31:15Z","2022-07-26T02:22:47Z"
"","4564","Hadoop 18325: ABFS: Add correlated metric support for ABFS operations","We have added support for collection of metrics at filesystem instance level. Whenever a filesystem close is called, we push metrics back to the store for all the operations that used that instance. The metrics collected include the no of succeeded requests without retrying, the no of requests which succeeded after x no of retries, the request count which was throttled, the count of requests which failed even after all the retries were exhausted.","open","","anmolanmol1234","2022-07-14T07:27:48Z","2022-08-03T06:31:16Z"
"","4077","HDFS-16509. Fix decommission UnsupportedOperationException","We encountered an `UnsupportedOperationException: Remove unsupported` error when some datanodes were in decommission. The reason of the exception is that `datanode.getBlockIterator()` returns an Iterator does not support remove, however `DatanodeAdminDefaultMonitor#processBlocksInternal` invokes `it.remove()` when a block not found, e.g, the file containing the block is deleted.","closed","","cndaimin","2022-03-17T04:23:47Z","2022-04-14T09:12:18Z"
"","4593","HADOOP-18079. Upgrade Netty to 4.1.77. (#3977)","Upgrade netty to address  CVE-2019-20444, CVE-2019-20445 CVE-2022-24823  Contributed by Wei-Chiu Chuang  cherrypicked from #3977 (cherry picked from commit a55ace7bc0c173f609b51e46cb0d4d8bcda3d79d) (cherry picked from commit c545341785186a9a3419396c0c1d843f19830b81)    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2022-07-19T22:20:52Z","2022-07-29T16:32:41Z"
"","4592","HADOOP-18079. Upgrade Netty to 4.1.77. (#3977)","Upgrade netty to address  CVE-2019-20444, CVE-2019-20445 CVE-2022-24823  Contributed by Wei-Chiu Chuang  cherrypicked from #3977 (cherry picked from commit a55ace7bc0c173f609b51e46cb0d4d8bcda3d79d)","closed","","jojochuang","2022-07-19T20:00:59Z","2022-07-26T19:10:21Z"
"","4482","HADOOP-18305. Preparing for 3.3.4 release","Updating the version of branch-3.3 to 3.3.9 pending some agreement on what number the next release off it should take.  Using 3.3.9 puts space in for other incremental releases, while avoiding crating JIRA release ordering and autocompletion confusion the way adding a 3.10 or higher would do.  Contributed by Steve Loughran  Change-Id: I7d3ca6410384c623f64ca35a2d051dbbf85f8a7f    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-21T18:20:15Z","2022-06-22T12:32:08Z"
"","4477","HADOOP-18308 - Update to Apache LDAP API 2.0.x","Update to Apache LDAP API 2.0.x","closed","","coheigea","2022-06-21T06:22:45Z","2022-06-27T10:29:04Z"
"","3757","HADOOP-17796. Upgrade jetty version to 9.4.43 for branch-3.2","Update Jetty to 9.4.41 or above in branch-3.2","closed","","AnanyaSingh2121","2021-12-07T10:10:25Z","2022-02-02T11:43:29Z"
"","4473","HADOOP-15072 - Update Apache Kerby to 2.0.2","Update Apache Kerby to 2.0.2.","open","","coheigea","2022-06-20T16:27:18Z","2022-07-05T00:02:35Z"
"","3900","HDFS-16429. Add DataSetLockManager to manage fine-grain locks for FsDataSetImpl","this pr is sub task of the https://github.com/apache/hadoop/pull/3889","closed","improvement,","MingXiangLi","2022-01-18T07:47:24Z","2022-01-27T08:53:21Z"
"","4441","HDFS-13522. IPC changes to support observer reads through routers.","This PR is about RBF IPC changes in order to support Observer-Read.  And this a draft PR, and relevant [PR](https://github.com/apache/hadoop/pull/4311) already exist.  Different point:  - Proxy always with AlignmentContext, like RouterGSIContext which contains a boolean flag, like enableObserverRead - Always update the lastSeenStateId from active - If enableObserverRead is false, updateRequestState will not set lastSeenStateId in RPCHeader - It's easily to dynamically reconfigure enableObserverRead.","closed","","ZanderXu","2022-06-15T03:43:23Z","2022-06-20T09:42:48Z"
"","4690","HADOOP-18181. Move prefetch common classes to hadoop common","This moves the ""common"" classes for the prefetch code from hadoop-aws to hadoop common under org.apache.hadoop.fs.impl.prefetch   ### How was this patch tested?  s3 london; all good apart from landsat scale test timeout which i am now seeing everywhere  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","steveloughran","2022-08-03T16:22:54Z","2022-08-03T21:40:19Z"
"","4132","HDFS-16384. Upgrade Netty to 4.1.72.Final","This is to pick up PR #3810  @tamaashu","closed","","jojochuang","2022-04-04T02:31:53Z","2022-04-08T01:23:10Z"
"","4512","HADOOP-18316: ABFS: Add support for cache handling when filesystem instance has clientCorrelationID enabled","This is to mainly handle the scenarios for cache enablement or disablement when the filesystem instance has clientCorrelationId as a part of its configuration.  If a filesystem instance has clientCorrelationId as a part of its configuration and if we enable cache for the same, there may be another job using a filesystem instance with same configuration and uri as the one cached, but the issue here is that since these 2 filesystem instances are part of 2 different running jobs, they can't share the same clientCorrelationId. So as a part of the fix, if a filesystem instance has clientCorrelationId we disable cache for that process, so that if any other job is running as a part of the same process with the same configuration and uri, the filesystem instance created as a part of it will be created as a new instance and hence will not report incorrect clientCorrelationId.   Our main aim here is to prevent incorrect clientCorrelationId from being reported.","open","","anmolanmol1234","2022-06-29T06:35:10Z","2022-07-14T07:27:06Z"
"","4075","MAPREDUCE-7341. Add an intermediate manifest committer for Azure and GCS","this is the patch of #2971 spilt into three subcommits on top of trunk for merging, one per module, to be applied in order  1. HADOOP-18162. hadoop-common support for MAPREDUCE-7341 Manifest Committer 2. MAPREDUCE-7341. Add an intermediate manifest committer for Azure and GCS 3. HADOOP-18163. hadoop-azure support for the Manifest Committer of MAPREDUCE-7341  once yetus is happy I will merge by cherrypicking each commit individually  tested: azure cardiff.","closed","","steveloughran","2022-03-16T16:39:08Z","2022-03-17T11:50:21Z"
"","4675","HADOOP-18028. Rebase S3A Prefetch feature branch to trunk","This is the commit chain of feature-HADOOP-18028-s3a-prefetch rebased to trunk and with a fixup at the end.  I intend to apply this chain to the feature branch with a goal of a squash and merge of the branch into trunk ASAP, with all final work done on trunk     ### How was this patch tested?  s3 london, params `-Dparallel-tests -DtestsThreadCount=8 -Dscale`  all new failures reported  * [HADOOP-18386](https://issues.apache.org/jira/browse/HADOOP-18386) ITestS3SelectLandsat timeout after 10 minutes. surfaces in trunk too. * [HADOOP-18384](https://issues.apache.org/jira/browse/HADOOP-18384) ITestS3AFileSystemStatistic failure in prefetch feature branch * [HADOOP-18385](https://issues.apache.org/jira/browse/HADOOP-18385) ITestS3ACannedACLs failure; not in a span.  I don't believe any are related, rather that changing the #of tests triggered latent issues surfacing from different states of the jvm before each suite, or, in the case of the landsat one, some change in the endpoint.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-08-02T17:12:30Z","2022-08-03T15:03:02Z"
"","4566","HADOOP-17461. IOStatisticsContext + committer integration","This is PR #4352 with a new patch to integrate with the MR committers, at least as far as collecting read stats from job commits and including in _SUCCESS the committer ITests collect these and print them.  * move reference map and lookup to a IOStatisticsContextIntegration class * static method in IOStatisticsContext to relay lookup * add method to switch a thread's context; needed to aggregate worker thread   IO in threads doing work for committers without the need to explicitly   collect and pass back the stats * production code moves to the new methods * tests move to this and away from looking up the fields in the streams * stats are reset in s3a test setup * s3a committers collect data read stats during job commit and include   in summary statistics. This is only the stats when reading manifest   files, not the actual work. * tests to print the aggregate of all loaded success files in the run.  ### How was this patch tested?  new/modified tests  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-07-14T19:48:59Z","2022-07-26T20:31:21Z"
"","4345","HADOOP-18305. Release Hadoop 3.3.4: minor update of hadoop-3.3.3","This is a cherrypick from branch-3.3 of security related changes applied to hadoop branch-3.3.  this lets yetus check they are all good  ## Tests known to fail  This branch isn't going to worry about test failures which have been fixed in PRs which we aren't going to backport.  Instead it is going to track what those known failures are, to help triage them.  | Test     |  Fix  | |-----|-----| | `yarn.applications.distributedshell.TestDistributedShell`| YARN-10553 | | `yarn.csi.client.TestCsiClient` | YARN-10788 | | `yarn.server.resourcemanager.scheduler.capacity.TestCapacitySchedulerAutoQueueCreation` | YARN-10506 | | `hdfs.server.datanode.TestDataNodeRollingUpgrade` | HDFS-15580 | | `fs.s3a.tools.ITestMarkerTool` | HADOOP-18168 |","open","","steveloughran","2022-05-23T13:38:23Z","2022-06-22T12:10:34Z"
"","4123","HADOOP-18144: getTrashRoot in ViewFileSystem should return a path in ViewFS","This is a cherry-pick commit from trunk to branch-3.3 for the viewFS getTrashRoot work.  Signed-off-by: Owen O'Malley  (cherry picked from commit 8b8158f02df18424b2406fd66b34c1bdf3d7ab55)","closed","","xinglin","2022-03-30T22:51:11Z","2022-03-31T20:26:09Z"
"","4124","HADOOP-18144: getTrashRoot in ViewFileSystem should return a path in ViewFS","This is a cherry-pick commit from trunk to 2.10.   Signed-off-by: Owen O'Malley  (cherry picked from commit 8b8158f02df18424b2406fd66b34c1bdf3d7ab55)","closed","","xinglin","2022-03-30T22:54:24Z","2022-03-31T20:58:30Z"
"","4569","HADOOP-18332. Remove rs-api dependency by downgrading jackson to 2.12.7. (#4552)","This downgrades jackson from the version switched to in  HADOOP-18033 (2.13.0), to Jackson 2.12.7. This removes the dependency on javax.ws.rs-api, so avoiding runtime problems with applications using jersey-core v1 and/or jsr311-api.  The 2.12.7 release still contains the fix for CVE-2020-36518.  Contributed by PJ Fanning   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-07-16T17:25:47Z","2022-07-17T16:03:12Z"
"","4302","ABANDONED - ABFS: Make provision for adding additional connections type","This commit aims to facilitate upcoming work as part of adding an alternate connection to store backend - [HADOOP-17853](https://issues.apache.org/jira/browse/HADOOP-17853)  The scope of the change is to make AbfsHttpOperation an abstract class and create a child class AbfsHttpConnection. Future connection types will be added as child of AbfsHttpOperation. Retaining the abstract class name to reduce any backport pain.  ABFS driver tests were run with HNS and non-HNS storage accounts over combinations of authentication types - OAuth and SharedKey. Tests results will be updated in conversation tab with each PR iteration.","closed","","snvijaya","2022-05-11T22:55:40Z","2022-05-12T00:50:16Z"
"","4239","HADOOP-18198. add -mvnargs option to create-release command line","This allows for release builds to be run with options like --mvnargs=""-Dhttp.keepAlive=false -Dmaven.wagon.http.pool=false""  Contributed by Ayush Saxena.","closed","","steveloughran","2022-04-27T14:22:38Z","2022-04-27T16:31:17Z"
"","4639","HADOOP-17461. Collect thread-level IOStatistics.","This adds a thread-level collector of IOStatistics, IOStatisticsContext, which can be: * Retrieved for a thread and cached for access from other   threads. * reset() to record new statistics. * Queried for live statistics through the   IOStatisticsSource.getIOStatistics() method. * Queries for a statistics aggregator for use in instrumented   classes. * Asked to create a serializable copy in snapshot()  The goal is to make it possible for applications with multiple threads performing different work items simultaneously to be able to collect statistics on the individual threads, and so generate aggregate reports on the total work performed for a specific job, query or similar unit of work.  Some changes in IOStatistics-gathering classes are needed for  this feature * Caching the active context's aggregator in the object's   constructor * Updating it in close()  Slightly more work is needed in multithreaded code, such as the S3A committers, which collect statistics across all threads used in task and job commit operations.  Currently the IOStatisticsContext-aware classes are: * The S3A input stream, output stream and list iterators. * RawLocalFileSystem's input and output streams. * The S3A committers. * The TaskPool class in hadoop-common, which propagates   the active context into scheduled worker threads.  Collection of statistics in the IOStatisticsContext is disabled process-wide by default until the feature  is considered stable.  To enable the collection, set the option fs.thread.level.iostatistics.enabled to ""true"" in core-site.xml; 	 Contributed by Mehakmeet Singh and Steve Loughran    ### Description of PR   ### How was this patch tested? Region: `ap-south-1` All tests ran successfully.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mehakmeet","2022-07-27T06:55:20Z","2022-07-27T10:23:07Z"
"","4638","HADOOP-15980. Securing Hadoop RPC using SSL","The work done in this pull request implements SSL support for Hadoop RPC using the following steps,  1. Add SSL support to the RPC server    a. Creates a RPC server implementation (Server.java) to use Netty    b. Juxtaposes the Netty Implementation alongside the Native Java NIO APIs.    c. Add SSL Handlers to the Netty Pipeline    d. Parameterizes the RPC unit tests to run with and without SSL.    e. Split the new classes into the constituent source files to reduce the clutter in Server.java.  2. Add Netty support to the RPC client    a. Creates a RPC Client implementation (Client.java) to use Netty    b. Juxtaposes the Netty Implementation alongside the Native Java NIO APIs.    c. Add SSL Handlers to the Netty Pipeline    d. Parameterizes the RPC unit tests to run with and without Netty.    e. Split the new classes into the constituent source files to reduce the clutter in Client.java.  3. Add configuration to turn Netty on and off.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","vnhive","2022-07-27T03:25:18Z","2022-07-27T14:18:15Z"
"","4187","HDFS-16546. Fix UT TestOfflineImageViewer#testReverseXmlWithoutSnapshotDiffSection to branch branch-3.2","The test fails due to incorrect layoutVersion.","closed","","cndaimin","2022-04-18T11:27:18Z","2022-04-22T06:34:29Z"
"","4295","HDFS-16575. [SPS]Should use real replication num instead getReplicati…","The SPS may have misjudged in the following scenario: 1. Create a file with one block and this block have 3 replication with **DISK** type [DISK, DISK, DISK]. 2. Set this file with **ALL_SSD** storage policy. 3. The replication of this file may become [DISK, DISK, **SSD**, DISK] with **decommission**. 4. Set this file with **HOT** storage policy and satisfy storage policy on this file. 5. The replication finally look like [DISK, DISK, SSD] not  [DISK, DISK, DISK] after decommissioned node offline. 6. The reason is that SPS get the block replications by   FileStatus.getReplication() which is not the real num of the block. ![image](https://user-images.githubusercontent.com/2844826/167540656-7191cf54-22f0-47c4-a1d3-2e648672224b.png)    So this block will be ignored, because it have 3 replications with DISK type already ( one replication in a decommissioning node)   ![image](https://user-images.githubusercontent.com/2844826/167540674-6a926ba0-f4ad-4e4c-910f-6dacb2e9a955.png)   I think we can use blockInfo.getLocations().length to count the replication of block instead of FileStatus.getReplication().","open","","liubingxing","2022-05-10T04:09:50Z","2022-05-10T11:38:13Z"
"","4269","HDFS-16570 RBF: The router using MultipleDestinationMountTableResolve…","The router using MultipleDestinationMountTableResolver remove Multiple subcluster data under the mount point failed","open","","zhangxiping1","2022-05-06T10:24:02Z","2022-06-10T03:29:19Z"
"","3914","HADOOP-18080. ABFS: Skip testEtagConsistencyAcrossRename for Non-HNS accounts","The rename operation is not supported for non-HNS accounts. Hence, tests verifying matching etag for file before and after rename should not be run against non-HNS accounts. PR modifies the `hasPathCapability` method to return true only for HNS-enabled accounts for the ETAGS_PRESERVED_IN_RENAME probe.","open","","sumangala-patki","2022-01-21T11:06:51Z","2022-01-21T12:34:44Z"
"","4081","HDFS-13248: Namenode needs to use the actual client IP when going through RBF proxy.","The NN makes decisions based on the client machine that control the locality of data access. Currently that is done by finding the ip address using the rpc connection, however in the RBF configuration, that will always be one of the router's ip address.  We'd added the client's ip to the caller context in the router, so now the NN has the information. This patch makes the NN use the caller context information.  From a security point of view, this patch adds a new configuration knob (dfs.namenode.ip-proxy-users) on the NN that defines the list of users that can set their client ip address. Sites should add ""hdfs"" (or the account that runs the routers) to ""dfs.namenode.ip-proxy-users"" on the NN to enable this feature.  Note that the audit log does NOT currently use this information, so the client ip in the audit log will be the RBF proxy. Sites should turn on caller context logging so that the client ip addresses are captured.   ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","omalley","2022-03-17T22:04:23Z","2022-03-21T16:31:51Z"
"","4558","HDFS-16657 Changing pool-level lock to volume-level lock for invalida…","The key code is:  // code placeholder try {   File blockFile = new File(info.getBlockURI());   if (blockFile != null && blockFile.getParentFile() == null) {     errors.add(""Failed to delete replica "" + invalidBlks[i]         +  "". Parent not found for block file: "" + blockFile);     continue;   } } catch(IllegalArgumentException e) {   LOG.warn(""Parent directory check failed; replica "" + info       + "" is not backed by a local file""); }  DN is trying to locate parent path of block file, thus there is a disk I/O in pool-level lock. When the disk becomes very busy with high io wait, All the pending threads will be blocked by the pool-level lock, and the time of heartbeat is high. We proposal to change the pool-level lock to volume-level lock for block invalidation","open","","yuanboliu","2022-07-13T03:40:59Z","2022-07-19T11:59:02Z"
"","4078","HDFS-16510. Fix EC decommission when rack is not enough","The decommission always fail when we start decommission multiple nodes on a cluster whose racks is not enough, a cluster with 6 racks to deploy RS-6-3, for example.  We find that those decommission nodes cover at least a rack, it's actulaly like we are decommission one or more racks. And rack decommission is not well supported currently, especially for cluster whose racks is not enough already.  In this patch, we add `numOfExcludedRacks` to indicate how many racks are in decommission(excluded) and fix the calculation of `BlockPlacementStatusDefault#getAdditionalReplicasRequired`. And in `ErasureCodingWork#addTaskToDatanode`, we adjust the process order as we should take care of decommission first, especially when rack is not enough.","closed","","cndaimin","2022-03-17T09:15:29Z","2022-04-18T10:10:38Z"
"","4177","HDFS-16542. Fix failed unit tests in branch branch-3.2","Tests fail in branch branch-3.2: ``` hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewer hadoop.hdfs.TestReconstructStripedFileWithValidator hadoop.hdfs.server.namenode.TestFsck hadoop.hdfs.server.blockmanagement.TestBlockManager ```","closed","","cndaimin","2022-04-15T07:02:07Z","2022-04-22T07:03:50Z"
"","4296","HDFS-16573. Fix TestDFSStripedInputStreamWithRandomECPolicy in branch-3.3","TestDFSStripedInputStreamWithRandomECPolicy fails due to test from [HDFS-16520](https://issues.apache.org/jira/browse/HDFS-16520)","closed","","cndaimin","2022-05-10T08:37:27Z","2022-05-10T23:56:53Z"
"","4291","HDFS-16573. Fix TestDFSStripedInputStreamWithRandomECPolicy","TestDFSStripedInputStreamWithRandomECPolicy fails due to test from [HDFS-16520](https://issues.apache.org/jira/browse/HDFS-16520)","closed","","cndaimin","2022-05-09T10:02:21Z","2022-05-10T08:39:32Z"
"","3696","HDFS-16345. Fix test case fail in TestBlockStoragePolicy","test class ``TestBlockStoragePolicy` ` fail frequently for the `BindException`, it block all normal source code build. we can improve it.  [ERROR] Tests run: 26, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 49.295 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestBlockStoragePolicy [ERROR] testChooseTargetWithTopology(org.apache.hadoop.hdfs.TestBlockStoragePolicy) Time elapsed: 0.551 s <<< ERROR! java.net.BindException: Problem binding to [localhost:43947] java.net.BindException: Address already in use; For more details see: http://wiki.apache.org/hadoop/BindException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:931) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:827) at org.apache.hadoop.ipc.Server.bind(Server.java:657) at org.apache.hadoop.ipc.Server$Listener.(Server.java:1352) at org.apache.hadoop.ipc.Server.(Server.java:3252) at org.apache.hadoop.ipc.RPC$Server.(RPC.java:1062) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.(ProtobufRpcEngine2.java:468) at org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371) at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.(NameNodeRpcServer.java:466) at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:860) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:766) at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:1017) at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:992) at org.apache.hadoop.hdfs.TestBlockStoragePolicy.testChooseTargetWithTopology(TestBlockStoragePolicy.java:1275) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) at sun.nio.ch.Net.bind(Net.java:461) at sun.nio.ch.Net.bind(Net.java:453) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85) at org.apache.hadoop.ipc.Server.bind(Server.java:640) ... 40 more   ","closed","","GuoPhilipse","2021-11-21T17:49:24Z","2021-12-08T11:25:52Z"
"","4470","HADOOP-17833. Improve Magic Committer performance (#3289)","Speed up the magic committer with key changes being  * Writes under __magic always retain directory markers  * File creation under __magic skips all overwrite checks,   including the LIST call intended to stop files being         created over dirs. * mkdirs under __magic probes the path for existence   but does not look any further.  Extra parallelism in task and job commit directory scanning Use of createFile and openFile with parameters which all for HEAD checks to be skipped.  The committer can write the summary _SUCCESS file to the path `fs.s3a.committer.summary.report.directory`, which can be in a different file system/bucket if desired, using the job id as the filename.  Also: HADOOP-15460. S3A FS to add `fs.s3a.create.performance`  Application code can set the createFile() option fs.s3a.create.performance to true to disable the same safety checks when writing under magic directories. Use with care.  The createFile option prefix `fs.s3a.create.header.` can be used to add custom headers to S3 objects when created.  Contributed by Steve Loughran.  Change-Id: I9e086423f02eb25b6e70fc1c12a13e0a5afe9cb9    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-20T13:33:17Z","2022-06-21T09:49:38Z"
"","4190","HADOOP-18202. create-release fails fatal: unsafe repository","Since CVE-2022-24765 in April 2022, git refuses to work in directories whose owner != the current user, unless explicitly told to trust it.  This patches the create-release script to trust the /build/source dir mounted from the hosting OS, whose userid is inevitably different from that of the account in the container running git.  ### Description of PR  pr #4188 against branch-3.3.3  ### How was this patch tested?  testing building releases  ### For code changes:  - [X ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-18T15:38:06Z","2022-04-18T18:48:18Z"
"","4188","HADOOP-18202. create-release fails fatal: unsafe repository","Since CVE-2022-24765 in April 2022, git refuses to work in directories whose owner != the current user, unless explicitly told to trust it.  This patches the create-release script to trust the /build/source dir mounted from the hosting OS, whose userid is inevitably different from that of the account in the container running git.  ### Description of PR   ### How was this patch tested?  going to test the PR on my build machine (a different laptop)  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-18T14:51:09Z","2022-04-18T18:24:36Z"
"","3933","HADOOP-18093. Better exception handling for testFileStatusOnMountLink…","Signed-off-by: Ayush Saxena  (cherry picked from commit 0d17b629ffee2f645f405ad46b0afa65224f87d5)    ### Description of PR  HADOOP-18093. Better exception handling for testFileStatusOnMountLink() in ViewFsBaseTest.java (#3918). Contributed by Xing Lin.  ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs","closed","","xinglin","2022-01-25T20:13:12Z","2022-01-26T16:24:43Z"
"","3932","HADOOP-18093. Better exception handling for testFileStatusOnMountLink…","Signed-off-by: Ayush Saxena  (cherry picked from commit 0d17b629ffee2f645f405ad46b0afa65224f87d5)    ### Description of PR  Better exception handling for testFileStatusOnMountLink() in ViewFsBaseTest.java (#3918).   ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs","closed","","xinglin","2022-01-25T20:11:50Z","2022-01-26T16:24:17Z"
"","3931","HADOOP-18093. Better exception handling for testFileStatusOnMountLink…","Signed-off-by: Ayush Saxena  (cherry picked from commit 0d17b629ffee2f645f405ad46b0afa65224f87d5)    ### Description of PR   ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs","closed","","xinglin","2022-01-25T19:31:23Z","2022-01-26T16:23:51Z"
"","3929","HADOOP-18093. Better exception handling for testFileStatusOnMountLink…","Signed-off-by: Ayush Saxena  (cherry picked from commit 0d17b629ffee2f645f405ad46b0afa65224f87d5)   ### Description of PR   ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs  [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.viewfs.TestViewFsLocalFs [INFO] Tests run: 66, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.765 s - in org.apache.hadoop.fs.viewfs.TestViewFsLocalFs","closed","","xinglin","2022-01-25T19:14:04Z","2022-01-26T16:25:33Z"
"","4350","YARN-11162 Set the zk acl for nodes created by YARN's ZKConfigurationStore.","Set the zk acl for nodes created by Yarn's ZKConfigurationStore.","closed","","omalley","2022-05-24T00:33:12Z","2022-05-24T05:07:20Z"
"","3916","HADOOP-18094. Disable S3A auditing by default.","See HADOOP-18091. S3A auditing leaks memory through ThreadLocal references  * Adds a new option fs.s3a.audit.enabled to controls whether or not auditing is enabled. This is false by default.  * When false, the S3A auditing manager is NoopAuditManagerS3A, which was formerly only used for unit tests and during filsystem initialization.  * When true, ActiveAuditManagerS3A is used for managing auditing, allowing auditing events to be reported.  * updates documentation and tests.  This patch does not fix the underlying leak. When auditing is enabled, long-lived threads will retain references to the audit managers of S3A filesystem instances which have already been closed.  Contributed by Steve Loughran.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-01-23T19:16:34Z","2022-01-24T14:00:13Z"
"","4597","HDFS-16671. RBF: RouterRpcFairnessPolicyController supports configurable permit acquire timeout","RouterRpcFairnessPolicyController supports configurable permit acquire timeout. Hardcode 1s is very long, and it has caused an incident in our prod environment when one nameserivce is busy.  And the optimal timeout maybe should be less than p50(avgTime).  And all handlers in RBF is waiting to acquire the permit of the busy ns.  ``` ""IPC Server handler 12 on default port 8888"" #? daemon prio=? os_prio=0 tid=? nid=?  waiting on condition [?]    java.lang.Thread.State: TIMED_WAITING (parking) 	at sun.misc.Unsafe.park(Native Method) 	- parking to wait for   (a java.util.concurrent.Semaphore$NonfairSync) 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037) 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328) 	at java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:409) 	at org.apache.hadoop.hdfs.server.federation.fairness.AbstractRouterRpcFairnessPolicyController.acquirePermit(AbstractRouterRpcFairnessPolicyController.java:56) 	at org.apache.hadoop.hdfs.server.federation.fairness.DynamicRouterRpcFairnessPolicyController.acquirePermit(DynamicRouterRpcFairnessPolicyController.java:123) 	at org.apache.hadoop.hdfs.server.federation.router.RouterRpcClient.acquirePermit(RouterRpcClient.java:1500) ```","closed","","ZanderXu","2022-07-20T12:35:27Z","2022-07-28T07:44:58Z"
"","4276","YARN-11130. removed unused import","RouterClientRMService Has Unused import, keep the code clean and remove unused. JIRA:[YARN-11130](https://issues.apache.org/jira/browse/YARN-11130)","closed","","slfan1989","2022-05-07T05:17:38Z","2022-05-11T06:45:12Z"
"","4206","HADOOP-17650. Bump solr to unblock build failure with Maven 3.8.1 (#2939)","Reviewed-by: Siyao Meng    ### Description of PR  patch 538ce9c / #2939 on branch-3.3; if good will add to 3.3.3  without this you can only build hadoop with a proxy nexus server  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-20T12:41:42Z","2022-04-20T15:36:57Z"
"","3924","HDFS-16423. Balancer should not get blocks on stale storages (#3883)","Reviewed-by: litao  Signed-off-by: Takanobu Asanuma  (cherry picked from commit db2c3200e63a4377331643eddedcab79d4a51468)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestGetBlocks.java 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithHANameNodes.java","closed","","jojochuang","2022-01-25T03:13:23Z","2022-01-26T03:54:21Z"
"","4306","Revert ""HDFS-14750. RBF: Support dynamic handler allocation in routers""","Reverts apache/hadoop#4199","closed","","ferhui","2022-05-13T06:44:26Z","2022-05-13T06:47:22Z"
"","4232","Revert ""HDFS-16488. [SPS]: Expose metrics to JMX for external SPS""","Reverts apache/hadoop#4035","closed","","ferhui","2022-04-24T09:18:11Z","2022-04-24T09:21:03Z"
"","4038","Revert ""HDFS-16458. [SPS]: Fix bug for unit test of reconfiguring SPS mode""","Reverts apache/hadoop#3998","closed","","ferhui","2022-02-28T13:04:50Z","2022-02-28T13:07:45Z"
"","4544","Rev: HADOOP-18178 & HADOOP-18033 to Fix Class conflicts in downstreams","Reverting the original commits upgrading Jackson, Fixed the conflicts in revert.   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ayushtkn","2022-07-11T08:06:31Z","2022-07-17T16:12:37Z"
"","4131","HDFS-16529. Remove unnecessary setObserverRead in TestConsistentReadsObserver","Remove unnecessary setObserverRead in TestConsistentReadsObserver JIRA: https://issues.apache.org/jira/browse/HDFS-16529   ### Description of PR   ### How was this patch tested?   ### For code changes:","closed","","wzhallright","2022-04-02T09:55:35Z","2022-04-06T08:29:58Z"
"","4320","HADOOP-18236. Remove duplicate locks in NetworkTopology","Remove duplicate locks in NetworkTopology","closed","","ZanderXu","2022-05-17T14:22:59Z","2022-05-28T01:39:36Z"
"","3830","HADOOP-18083 ABFS: Toggle Store Mkdirs request overwrite parameter with default value","Reference :- https://github.com/apache/hadoop/pull/2729  In this PR,  mkdirs config is set to overwrite=false as default as the related backend deployment is completed.","open","","anmolanmol1234","2021-12-27T13:19:41Z","2022-01-17T12:37:12Z"
"","4529","HDFS-16648. Add isDebugEnabled check for debug logs in some classes","Refer to [PR4480](https://github.com/apache/hadoop/pull/4480#event-6923776715) to uniformly the debug logs in HDFS core classes.  And the detailed performance test, please refer to [debugLogPerformanceTest](https://github.com/apache/hadoop/pull/4480#issuecomment-1166936626).","open","","ZanderXu","2022-07-04T13:54:59Z","2022-08-03T15:49:07Z"
"","4445","HADOOP-18106: Handle memory fragmentation in S3A Vectored IO.","Rebased the feature branch. Old pr link https://github.com/apache/hadoop/pull/4427  ### Description of PR part of HADOOP-18103. Handling memoroy fragmentation in S3A vectored IO implementation by allocating smaller user range requested size buffers and directly filling them from the remote S3 stream and skipping undesired data in between ranges. This patch also adds aborting active vectored reads when stream is closed or unbuffer is called.  ### How was this patch tested? Added new test and re-ran existing tests.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-06-15T22:36:02Z","2022-06-21T18:53:37Z"
"","3954","Rebase Access Point feature onto branch-3.3","Rebased Access Point feature (the 2 new commits `648a7ff329d7458d97e8b04461ccdf56eab3456a` and `e234d31af24a375f3942e5ac147ed54e6587dc0f`) onto `branch-3.3` right before the AWS SDK upgrade. Was hoping for a quick `cherry-pick` but there's changes that come into effect with removal S3 guard which conflict so placed it before that.   Apologies if this isn't the right way to do it! Let me know if there's anything else I can do.  Also created a PR that picks this AcessPoint feature straight into 3.3.2, which might be of help https://github.com/apache/hadoop/pull/3955   @sunchao @steveloughran","closed","","bogthe","2022-02-02T17:17:52Z","2022-02-04T16:25:17Z"
"","3881","HDFS-16422. Fix thread safety of EC decoding during concurrent preads","Reading data on an erasure-coded file with missing replicas(internal block of block group) will cause online reconstruction: read `dataUnits` part of data and decode them into the target missing data. Each `DFSStripedInputStream` object has a `RawErasureDecoder` object, and when we doing pread concurrently, `RawErasureDecoder.decode` will be invoked concurrently too. `RawErasureDecoder.decode` is not thread safe, as a result of that we get wrong data from pread occasionally.","closed","","cndaimin","2022-01-13T06:38:41Z","2022-02-18T15:33:20Z"
"","4229","HDFS-16453. Upgrade okhttp from 2.7.5 to 4.9.3","Raising PR to check `okhttp` upgrade possibility to 4.9.3   JIRA: HDFS-16453","closed","","ashutoshcipher","2022-04-23T23:23:31Z","2022-07-11T04:48:50Z"
"","4153","HADOOP-18178. Upgrade jackson to 2.13.2 and jackson-databind to 2.13.2.2 (#4111)","PR of #4111 on branch-3.3 to see what yetus says. if it is happy/no new regressions, will merge","closed","","steveloughran","2022-04-08T13:55:02Z","2022-04-09T04:46:41Z"
"","3699","HADOOP-17873. ABFS: Fix transient failures in ITestAbfsStreamStatistics and ITestAbfsRestOperationException","PR addresses transient failures in the following test classes:  ITestAbfsStreamStatistics: Uses a static instance to record read/write statistics, which also tracks these operations in other tests running parallelly. To be marked for sequential-only run to avoid transient failure ITestAbfsRestOperationException: The use of a static member to track retry count causes transient failures when two tests of this class happen to run together. Switch to non-static variable for assertions on retry count  Fixes import of VisibleForTesting in the [previously reviewed PR](https://github.com/apache/hadoop/pull/3341)","open","","sumangala-patki","2021-11-22T06:05:45Z","2022-05-13T16:46:09Z"
"","3955","HADOOP-17198. Support S3 Access Points  (#3260) (branch-3.3.2)","Picking the added changes straight into `branch-3.3.2` for ease.  Idea is we can release this feature in 3.3.2 and then can clean up `branch-3.3` by cherry-picking these changes on top of it. See rebase [PR](https://github.com/apache/hadoop/pull/3954).  @sunchao @steveloughran","closed","","bogthe","2022-02-02T17:26:36Z","2022-02-07T09:03:53Z"
"","4570","HADOOP-18074 - Partial/Incomplete groups list can be returned in LDAP. (#4503)","Partial/Incomplete groups list can be returned in LDAP groups lookup.  Backported in #4550; minor tuning of parameters needed.  Contributed by larry mccay","closed","","steveloughran","2022-07-16T17:34:24Z","2022-07-17T16:06:42Z"
"","4636","HADOOP-18227. Add input stream IOStats for vectored IO api in S3A.","part of HADOOP-18103.    ### Description of PR Adding input stream IOStats for vectored IO api in S3A.  ### How was this patch tested? Added new tests to assert the newly added stats. Ran the whole S3A IT suite using us-east-2 bucket.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-07-26T21:15:23Z","2022-07-28T16:27:38Z"
"","4647","HADOOP-18355. Update previous index properly while validating overlapping ranges.","part of HADOOP-18103.    ### Description of PR   ### How was this patch tested? Added a new UT and re-ran the vectored io related test suites using a us-west-1 endpoint.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","mukund-thakur","2022-07-27T21:32:25Z","2022-08-03T16:05:47Z"
"","4273","HADOOP-18107 Adding scale test for vectored reads for large file","part of HADOOP-18103.  ### Description of PR   ### How was this patch tested? Reran the changed tests.   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-05-06T19:40:28Z","2022-06-01T22:05:54Z"
"","3977","HADOOP-18079. Upgrade Netty to 4.1.77.","Output of mvn dependency:tree after the change:  ``` [INFO] +- io.netty:netty-all:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-buffer:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-dns:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-haproxy:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-http:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-http2:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-memcache:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-mqtt:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-redis:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-smtp:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-socks:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-stomp:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-codec-xml:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-common:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-handler:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-tcnative-classes:jar:2.0.48.Final:compile [INFO] |  +- io.netty:netty-handler-proxy:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-resolver:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-resolver-dns:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-rxtx:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-sctp:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-udt:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-classes-epoll:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-native-unix-common:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-classes-kqueue:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-resolver-dns-classes-macos:jar:4.1.74.Final:compile [INFO] |  +- io.netty:netty-transport-native-epoll:jar:linux-x86_64:4.1.74.Final:runtime [INFO] |  +- io.netty:netty-transport-native-epoll:jar:linux-aarch_64:4.1.74.Final:runtime [INFO] |  +- io.netty:netty-transport-native-kqueue:jar:osx-x86_64:4.1.74.Final:runtime [INFO] |  +- io.netty:netty-transport-native-kqueue:jar:osx-aarch_64:4.1.74.Final:runtime [INFO] |  +- io.netty:netty-resolver-dns-native-macos:jar:osx-x86_64:4.1.74.Final:runtime [INFO] |  \- io.netty:netty-resolver-dns-native-macos:jar:osx-aarch_64:4.1.74.Final:runtime ```","closed","","jojochuang","2022-02-10T08:17:08Z","2022-07-18T10:11:36Z"
"","4534","HADOOP-15789. DistCp does not clean staging folder if class extends DistCp. Contributed by Lawrence Andrews.","Original Patch in the Jira, Contributor not available now, & Patches don't run CI now, so Copied it to PR to run the build before merge  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2022-07-07T20:02:53Z","2022-07-08T11:34:20Z"
"","4504","HADOOP-18312. Yarn WebApps to support IPv6 address.","org.apache.hadoop.yarn.webapp.WebApps.Builder#at(java.lang.String)   This fails to parse an IPV6 address. Only IPv4 supported by the builder.","open","","prasad-acit","2022-06-25T22:15:51Z","2022-06-26T02:08:15Z"
"","4472","MAPREDUCE-7391. TestLocalDistributedCacheManager failing after HADOOP-16202","Mocking broke with the move to the builder.  Took the opportunity to clean up the test class with * use of lambda expressions for answers where they were short enough * constant for file contents * return that data's length in the getFileStatus calls * javadocs for the tests ca!ses which didn't have them.","closed","","steveloughran","2022-06-20T15:56:28Z","2022-06-22T11:52:42Z"
"","4026","MAPREDUCE-7372 MapReduce set permission too late in copyJar method","MAPREDUCE-7372.MapReduce set permission too late in copyJar method   ### Description of PR setReplication needs write permission , if umask too restrict , the project will fail, so we need to adjust the order.  ### How was this patch tested? Any MapReduce task with a strict umask (such as user set umask with 0600) can trigger it.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","skysiders","2022-02-24T05:32:10Z","2022-07-25T18:38:59Z"
"","3791","HADOOP-18044. Hadoop - Upgrade to jQuery 3.6.0","jQuery 3.6.0 has been released few months ago - http://blog.jquery.com/2021/03/02/jquery-3-6-0-released/   We can upgrade jquery-3.5.1.min.js to jquery-3.6.0.min.js in hadoop project.","closed","","luoyuan3471","2021-12-13T03:45:09Z","2022-02-11T15:32:46Z"
"","4281","YARN-11131. FlowRunCoprocessor Scan Used Deprecated Method","JIRA：[YARN-11131:FlowRunCoprocessor Scan Used Deprecated Method](https://issues.apache.org/jira/browse/YARN-11131)  In the Scan code of Hbase2.X setMaxVersions is an Deprecated code, it will be removed in Hbase3.x and replaced with readAllVersions.","closed","","slfan1989","2022-05-07T23:14:17Z","2022-05-09T08:25:13Z"
"","4292","HADOOP-18229. Fix Hadoop-Common JavaDoc Error","JIRA：[HADOOP-18229.  Fix Hadoop Common Java Doc Error](https://issues.apache.org/jira/browse/HADOOP-18229)  It is found that the JavaDoc Error problem blocks the merge of HADOOP-18222. HADOOP-18222 will cause the RM startup problem, which needs to be solved as soon as possible.","closed","","slfan1989","2022-05-09T10:49:18Z","2022-05-28T07:55:13Z"
"","4285","Yarn 11134. Support getNodeToLabels API in FederationClientInterceptor","JIRA： [YARN-11134 . Support getNodeToLabels API in FederationClientInterceptor](https://issues.apache.org/jira/browse/YARN-11134) YARN Federation is a very useful architecture, the getNodeToLabels method is not implemented, implement this method。","closed","","slfan1989","2022-05-08T05:07:00Z","2022-05-10T23:15:25Z"
"","4656","YARN-11235. Refactor Policy Code and Define getReservationHomeSubcluster","JIRA:YARN-11235. Refactor Policy Code and Define getReservationHomeSubcluster.","open","","slfan1989","2022-07-28T15:43:03Z","2022-08-03T20:41:51Z"
"","4682","YARN-11230. [Federation] Add getContainer, signalToContainer  REST APIs for Router","JIRA:YARN-11230. [Federation] Add getContainer, signalToContainer  REST APIs for Router","closed","","slfan1989","2022-08-03T09:05:39Z","2022-08-03T15:34:51Z"
"","4614","YARN-11212. [Federation] Add getNodeToLabels REST APIs for Router.","JIRA:YARN-11212. [Federation] Add getNodeToLabels REST APIs for Router.","closed","","slfan1989","2022-07-23T13:11:55Z","2022-07-29T01:21:53Z"
"","4463","YARN-11187. Remove WhiteBox in yarn module.","JIRA:YARN-11187. Remove WhiteBox in yarn module.","open","","slfan1989","2022-06-19T00:41:15Z","2022-08-03T01:51:10Z"
"","4380","YARN-11166. BackPort YARN-10788 To Branch-3.3.3","JIRA:YARN-11166. BackPort YARN-10788 To Branch-3.3.3  BackPort [YARN-10788](https://issues.apache.org/jira/browse/YARN-10788) To Branch-3.3.3, Used to solve hadoop.yarn.csi.client.TestCsiClient Junit Test failed.  The error message is as follows: ``` Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: File name too long  ```","closed","","slfan1989","2022-05-30T14:21:28Z","2022-05-30T20:06:25Z"
"","4610","YARN-11161. Support getAttributesToNodes, getClusterNodeAttributes, getNodesToAttributes API's for Federation","JIRA:YARN-11161. Support getAttributesToNodes, getClusterNodeAttributes, getNodesToAttributes API's for Federation.","closed","","slfan1989","2022-07-22T12:29:39Z","2022-07-25T17:05:46Z"
"","4595","YARN-11158. Support getDelegationToken, renewDelegationToken, cancelDelegationToken API's for Federation","JIRA:YARN-11158. Support getDelegationToken, renewDelegationToken, cancelDelegationToken API's for Federation.","closed","","slfan1989","2022-07-20T11:46:11Z","2022-08-03T05:42:46Z"
"","4421","YARN-10122. Support signalToContainer API for Federation.","JIRA:YARN-10122. Support signalToContainer API for Federation.","closed","","slfan1989","2022-06-09T15:38:11Z","2022-06-17T23:40:57Z"
"","4217","HDFS-16556.Fixtypos in distcp","JIRA:HDFS-HDFS-16556","closed","","GuoPhilipse","2022-04-22T01:47:38Z","2022-04-24T02:06:51Z"
"","4438","HDFS-16631. Enable dfs.datanode.lockmanager.trace In Test.","JIRA:HDFS-16631. Enable dfs.datanode.lockmanager.trace In Test.  In Jira [HDFS-16600](https://issues.apache.org/jira/browse/HDFS-16600). Fix deadlock on DataNode side. We discussed the issue of deadlock, this is a very meaningful discussion, I was reading the log and found the following:  PR:https://github.com/apache/hadoop/pull/4367  ``` 2022-05-27 07:39:47,890 [Listener at localhost/36941] WARN datanode.DataSetLockManager (DataSetLockManager.java:lockLeakCheck(261)) -  not open lock leak check func. ```  Looking at the code, I found that there is such a parameter: ```      dfs.datanode.lockmanager.trace     false            If this is true, after shut down datanode lock Manager will print all leak       thread that not release by lock Manager. Only used for test or trace dead lock       problem. In produce default set false, because it's have little performance loss.          ```  I think this parameter should be added in the test environment, so that if there is a DN deadlock, the cause can be quickly located.  If my understanding is correct, if a thread needs both read locks and write locks, if this parameter is true, relevant thread information can be printed.","closed","","slfan1989","2022-06-14T08:32:35Z","2022-08-03T07:16:57Z"
"","4419","HDFS-16627. Improve BPServiceActor#register log to add NameNode address.","JIRA:HDFS-16627. improve BPServiceActor#register Log Add NN Addr.  When I read the log, I think the Addr information of NN should be added to make the log information more complete. The log is as follows:  ``` 2022-06-06 06:15:32,715 [BP-1990954485-172.17.0.2-1654496132136  heartbeating to localhost/127.0.0.1:42811] INFO  datanode.DataNode (BPServiceActor.java:register(819)) - Block pool BP-1990954485-172.17.0.2-1654496132136 (Datanode Uuid 7d4b5459-6f2b-4203-bf6f-d31bfb9b6c3f) service to localhost/127.0.0.1:42811  beginning handshake with NN.  2022-06-06 06:15:32,717 [BP-1990954485-172.17.0.2-1654496132136  heartbeating to localhost/127.0.0.1:42811] INFO  datanode.DataNode (BPServiceActor.java:register(847)) - Block pool BP-1990954485-172.17.0.2-1654496132136 (Datanode Uuid 7d4b5459-6f2b-4203-bf6f-d31bfb9b6c3f) service to localhost/127.0.0.1:42811  successfully registered with NN.  ```","closed","","slfan1989","2022-06-09T00:20:15Z","2022-06-11T12:36:43Z"
"","4412","HDFS-16624. Fix flaky unit test TestDFSAdmin#testAllDatanodesReconfig","JIRA:HDFS-16624. Fix org.apache.hadoop.hdfs.tools.TestDFSAdmin#testAllDatanodesReconfig ERROR.  (https://github.com/apache/hadoop/pull/4406) found an error message during Junit unit testing, as follows:  ``` expected:<[SUCCESS: Changed property dfs.datanode.peer.stats.enabled]> but was:<[ From: ""false""]> ```  After code debugging, it was found that there was an error in the selection outs.get(2) of the assertion(1208), index should be equal to 1.  Please see jira for details.  Introduce code in https://github.com/apache/hadoop/pull/4264","closed","","slfan1989","2022-06-07T00:22:54Z","2022-06-09T01:34:26Z"
"","4387","HDFS-16611. impove TestSeveralNameNodes#testCircularLinkedListWrites Params","JIRA:HDFS-16611. impove TestSeveralNameNodes#testCircularLinkedListWrites Params.  When I was dealing with [HDFS-16590](https://issues.apache.org/jira/browse/HDFS-16590) JIRA, Junit Tests often reported errors,  I found that the following error messages often appear  org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes#  testCircularLinkedListWrites  This method runs very close to success. It can be found that the current item is approximately equal to the target length in 3 runs. I think it can reduce the length of LIST_LENGTH and prolong the RUNTIME time, which can effectively increase the success rate of this Test.  Reducing LIST_LENGTH does not change the running purpose of Test, and it can also test Circular Writes in the case of NN failover.  - 1st run ``` [ERROR] testCircularLinkedListWrites(org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes)  Time elapsed: 114.252 s  <<< FAILURE! java.lang.AssertionError:  Some writers didn't complete in expected runtime! Current writer state:[Circular Writer: 	 directory: /test-0 	 target length: **50** 	 current item: **43** 	 done: false , Circular Writer: 	 directory: /test-1 	 target length: **50** 	 current item: **47** 	 done: false , Circular Writer: 	 directory: /test-2 	 target length: **50** 	 current item: **42** 	 done: false ] expected:<0> but was:<3> ```  - 2st run ``` [ERROR] testCircularLinkedListWrites(org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes)  Time elapsed: 110.349 s  <<< FAILURE! java.lang.AssertionError:  Some writers didn't complete in expected runtime! Current writer state:[Circular Writer: 	 directory: /test-0 	 target length: **50** 	 current item: **50** 	 done: false , Circular Writer: 	 directory: /test-1 	 target length: **50** 	 current item: **49** 	 done: false , Circular Writer: 	 directory: /test-2 	 target length: **50** 	 current item: **49** 	 done: false ] expected:<0> but was:<3> ```  - 3rd run ``` [ERROR] testCircularLinkedListWrites(org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes)  Time elapsed: 109.364 s  <<< FAILURE! java.lang.AssertionError:  Some writers didn't complete in expected runtime! Current writer state:[Circular Writer: 	 directory: /test-0 	 target length: **50** 	 current item: **47** 	 done: false , Circular Writer: 	 directory: /test-1 	 target length: **50** 	 current item: **47** 	 done: false , Circular Writer: 	 directory: /test-2 	 target length: **50** 	 current item: **46** 	 done: false ] expected:<0> but was:<3> ```","closed","","slfan1989","2022-05-31T15:29:48Z","2022-06-06T16:13:10Z"
"","4375","HDFS-16605. Improve Code With Lambda in hadoop-hdfs-rbf module.","JIRA:HDFS-16605. Improve Code With Lambda in hadoop-hdfs-rbf moudle.","closed","","slfan1989","2022-05-30T05:22:10Z","2022-07-22T01:45:50Z"
"","4160","HDFS-16537.Fix oev decode xml error","JIRA:HDFS-16537","open","","GuoPhilipse","2022-04-11T11:11:53Z","2022-04-24T22:00:46Z"
"","3767","HDFS-16374 Fix error log format in AbfsListStatusRemoteIterator","JIRA:HDFS-16374 Fix error log format in AbfsListStatusRemoteIterator","closed","","GuoPhilipse","2021-12-08T12:09:20Z","2021-12-08T13:44:07Z"
"","4457","HADOOP-18302. Remove WhiteBox in hadoop-common module.","JIRA:HADOOP-18302. Remove WhiteBox in hadoop-common module.  In HADOOP-18277 Jira, the removal of Whitebox was discussed, aajisaka suggested that the relevant code needs to be refactored to completely remove the Whitebox class, I tried to refactor the code in this pr.","open","","slfan1989","2022-06-18T03:54:06Z","2022-08-03T03:33:15Z"
"","4422","HADOOP-18284. Remove Unnecessary semicolon ';'","JIRA:HADOOP-18284. Remove Unnecessary semicolon ';'   while reading the code, I found a very tiny optimization point, part of the code contains 2 semicolons at the end, I will fix it. Because this change is simple, I fixed it in One JIRA.  The code looks like this: ``` private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();; ```","closed","","slfan1989","2022-06-09T23:47:53Z","2022-06-29T09:50:41Z"
"","4277","YARN-11131:use readAllVersions replace setMaxVersions","JIRA:[YARN-11131:FlowRunCoprocessor Scan Used Deprecated Method](https://issues.apache.org/jira/browse/YARN-11131) I Found Some deprecated methods are used in [Atsv2] module, try to replace related methods","closed","","slfan1989","2022-05-07T06:11:46Z","2022-05-07T07:55:47Z"
"","4664","YARN-8973. [Router] Add missing methods in RMWebProtocol.","JIRA: YARN-8973. [Router] Add missing methods in RMWebProtocol.  1. YARN-8559 and YARN-5952 added 2 REST methods in RMWebService without inserting them in the RMWebServiceProtocol. 2. This pr defines two methods, `updateSchedulerConfiguration` and `getSchedulerConfiguration`, but does not implement specific functions. 3. The personal idea is to complete the method definition first, and then submit the new pr completion method.","closed","","slfan1989","2022-07-31T12:08:48Z","2022-08-02T22:44:39Z"
"","4543","YARN-8900. [Router] Federation: routing getContainers REST invocations transparently to multiple RMs","JIRA: YARN-8900. [Router] Federation: routing getContainers REST invocations transparently to multiple RMs","closed","","slfan1989","2022-07-10T05:04:59Z","2022-07-23T00:09:35Z"
"","4673","YARN-6972. Adding RM ClusterId in AppInfo.","JIRA: YARN-6972. Adding RM ClusterId in AppInfo.  The content completed by this pr is relatively simple. I refer to the code of YARN-6973 and the Patch of YARN-6972.   YARN-6972 does the following: When queries appInfo, it will try to read rmClusterId from the current YARN configuration file to identify this application which Cluster does it belong to.","closed","","slfan1989","2022-08-02T13:49:13Z","2022-08-03T16:35:40Z"
"","4678","YARN-11240. Fix incorrect placeholder in yarn-module.","JIRA: YARN-11240. Fix incorrect placeholder in yarn-module.","open","","slfan1989","2022-08-02T22:24:35Z","2022-08-03T00:48:30Z"
"","4689","YARN-11230. [Federation] Add getContainer, signalToContainer REST APIs for Router.","JIRA: YARN-11230. [Federation] Add getContainer, signalToContainer REST APIs for Router.","closed","","slfan1989","2022-08-03T16:08:10Z","2022-08-03T18:21:48Z"
"","4688","YARN-11230. [Federation] Add getContainer, signalToContainer REST APIs for Router","JIRA: YARN-11230. [Federation] Add getContainer, signalToContainer REST APIs for Router.","closed","","slfan1989","2022-08-03T16:00:44Z","2022-08-03T16:03:37Z"
"","4657","YARN-11220. [Federation] Add getLabelsToNodes, getClusterNodeLabels, getLabelsOnNode REST APIs for Router","JIRA: YARN-11220. [Federation] Add getLabelsToNodes, getClusterNodeLabels, getLabelsOnNode REST APIs for Router.","closed","","slfan1989","2022-07-29T08:47:40Z","2022-08-02T22:46:57Z"
"","4484","YARN-11192. TestRouterWebServicesREST failing after YARN-9827.","JIRA: YARN-11192. TestRouterWebServicesREST failing after YARN-9827.  In [YARN-9827](https://issues.apache.org/jira/browse/YARN-9827), the following modifications: ``` GenericExceptionHandler should respond with SERVICE_UNAVAILABLE in case of connection and service unavailable exception instead of INTERNAL_SERVICE_ERROR.  ```  This modification caused some of YARN Federation's TestRouterWebServicesREST unit tests to fail ``` [ERROR] Tests run: 201, Failures: 15, Errors: 0, Skipped: 0, Flakes: 2 ..... [ERROR] org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST.testUpdateAppStateXML(org.apache.hadoop.yarn.server.router.webapp.TestRouterWebServicesREST) [ERROR]   Run 1: TestRouterWebServicesREST.testUpdateAppStateXML:774 expected:<500> but was:<503> [ERROR]   Run 2: TestRouterWebServicesREST.testUpdateAppStateXML:774 expected:<500> but was:<503> [ERROR]   Run 3: TestRouterWebServicesREST.testUpdateAppStateXML:774 expected:<500> but was:<503>  ```  Report-URL:  https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4464/5/artifact/out/patch-unit-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-router.txt","closed","","slfan1989","2022-06-21T22:35:46Z","2022-06-23T07:51:37Z"
"","4618","YARN-11180. Refactor some code of getNewApplication, submitApplication etc.","JIRA: YARN-11180. Refactor some code of getNewApplication, submitApplication, forceKillApplication, getApplicationReport.  **1) FederationClientInterceptor#getNewApplication** 1.Increase request is empty check 2.Use RouterServerUtil.logAndThrowException instead of throw YarnRuntime Exception.  **2) FederationClientInterceptor#submitApplication/forceKillApplication/getApplicationReport/getApplications** 1.Use RouterServerUtil.logAndThrowException instead of throw YarnRuntime Exception. 2.Use string.format instead of + 3.Fix Code Style.  **3) FederationClientInterceptor#getClusterMetrics** 1.Increase request is empty check  **4) FederationClientInterceptor#getClusterNodes/getQueueUserAcls/listReservations/getNodeToLabels/getLabelsToNodes/getClusterNodeLabels** 1.Use RouterServerUtil.logAndThrowException instead of throw YarnRuntime Exception.","closed","","slfan1989","2022-07-24T02:56:10Z","2022-07-29T16:00:44Z"
"","4464","YARN-11169. Support moveApplicationAcrossQueues, getQueueInfo API's for Federation.","JIRA: YARN-11169. Support moveApplicationAcrossQueues, getQueueInfo API's for Federation.","closed","","slfan1989","2022-06-19T02:36:21Z","2022-07-05T18:24:29Z"
"","4396","YARN-11159. Support failApplicationAttempt, updateApplicationPriority, updateApplicationTimeouts API's for Federation","JIRA: YARN-11159. Support failApplicationAttempt, updateApplicationPriority, updateApplicationTimeouts API's for Federation","closed","","slfan1989","2022-06-03T03:24:14Z","2022-06-09T00:49:20Z"
"","4650","YARN-11029. Refactor AMRMProxy Service code and Added Some Metrics.","JIRA: YARN-11029. Improve logs to print askCount, allocatedCount in AMRMProxy service.  the original title of this jira  Improve logs to print askCount, allocatedCount in AMRMProxy service  In the process of completion, it is found that it is better to display the reuqestcount in a metric way, which is better  This pr does the following work: 1. AMRMProxy service log information optimization. 2. Added 2 metrics to display updateAMRMTokens, stopApplication. 3. Added statistical requestCount and allocatedCount variables and displayed them in the log. 4. Supplement Metric's Junit Test. 5. Fix Some Typo Error.","closed","","slfan1989","2022-07-28T07:04:22Z","2022-08-03T16:38:00Z"
"","4426","YARN-10883. [Router] Router Audit Log Add Client IP Address.","JIRA: YARN-10883. [Router] Router Audit Log Add Client IP Address.  Yarn Federation Router Audit Log Need Client IP Address.  Now the log information is printed as follows: ``` 2022-06-10 08:06:26,322 INFO  [main] router.RouterAuditLogger (RouterAuditLogger.java:logSuccess(89)) -  USER=test-user    OPERATION=Submit New App    TARGET=RouterClientRMService    RESULT=SUCCESS    APPID=application_1654873569440_0001    SUBCLUSTERID=2 ```  The log of adding IP information is as follows: ``` 2022-06-10 08:09:05,392 INFO  [main] router.RouterAuditLogger (RouterAuditLogger.java:logSuccess(89)) -  USER=test-user    IP=127.0.0.1    OPERATION=Submit New App    TARGET=RouterClientRMService    RESULT=SUCCESS    APPID=application_1654873732359_0001    SUBCLUSTERID=3  ```","closed","","slfan1989","2022-06-10T15:01:28Z","2022-07-25T18:55:41Z"
"","4687","YARN-10846. Add dispatcher metrics to NM.","JIRA: YARN-10846. Add dispatcher metrics to NM.","open","","slfan1989","2022-08-03T15:15:36Z","2022-08-03T18:57:07Z"
"","4341","YARN-10487. Support getQueueUserAcls, listReservations, getApplicationAttempts, getContainerReport, getContainers, getResourceTypeInfo API's for Federation","JIRA: YARN-10487.  Support getQueueUserAcls, listReservations, getApplicationAttempts, getContainerReport, getContainers, getResourceTypeInfo API's for Federation.  There are 6 methods that pr needs to implement this time.  Step 1, I need to improve the log message first(https://github.com/apache/hadoop/pull/4336)  Step 2, This method has the following points that need to be improved org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor.java#getApplicationAttemptReport  - **metric name is inaccurate**  ``` if (request == null || request.getApplicationAttemptId() == null             || request.getApplicationAttemptId().getApplicationId() == null) {       // this method should be called incrAppAttemptReportFailedRetrieved       routerMetrics.incrAppAttemptsFailedRetrieved();        ..... } ```  - **call the getApplicationHomeSubCluster method without considering that the applicationId cannot be found in the Home SubCluster. At this time, all SubClusters should be traversed.**  ``` // If applicationId is not found in HomeCluster, it should be looked up in all subclusters  try {       subClusterId = federationFacade               .getApplicationHomeSubCluster(                       request.getApplicationAttemptId().getApplicationId());     } catch (YarnException e) {       .....     } ```  - **need to improve indentation issues**  Step3, Junit Test occasionally reports errors as follows:  - need to look up ApplicationAttempt in RM. - need to wait until the app starts.  ``` org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: ApplicationAttempt with id 'appattempt_1653443553199_0001_000001' doesn't exist in RM. at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationAttemptReport(ClientRMService.java:456) at org.apache.hadoop.yarn.server.router.clientrm.FederationClientInterceptor.getApplicationAttemptReport(FederationClientInterceptor.java:1030) at org.apache.hadoop.yarn.server.router.clientrm.TestFederationClientInterceptor.testGetApplicationAttemptReport(TestFederationClientInterceptor.java:458) ```","closed","","slfan1989","2022-05-23T06:46:27Z","2022-06-02T19:54:29Z"
"","4462","MAPREDUCE-7390 Remove WhiteBox in mapreduce module.","JIRA: MAPREDUCE-7390 Remove WhiteBox in mapreduce module.","open","","slfan1989","2022-06-18T23:02:16Z","2022-08-03T01:32:49Z"
"","4428","MAPREDUCE-7387. Fix TestJHSSecurity#testDelegationToken AssertionError due to HDFS-16563","JIRA: MAPREDUCE-7387. Fix TestJHSSecurity#testDelegationToken AssertionError due to HDFS-16563.  During the processing of [HADOOP-18284](https://issues.apache.org/jira/browse/HADOOP-18284). Fix Repeated Semicolons., PR#4422 was submitted, and an error was reported in hadoop.mapreduce.security.TestJHSSecurity#testDelegationToken in the test report.  ``` [ERROR] testDelegationToken(org.apache.hadoop.mapreduce.security.TestJHSSecurity)  Time elapsed: 16.344 s  <<< FAILURE! java.lang.AssertionError 	at org.junit.Assert.fail(Assert.java:87) 	at org.junit.Assert.assertTrue(Assert.java:42) 	at org.junit.Assert.assertTrue(Assert.java:53) 	at org.apache.hadoop.mapreduce.security.TestJHSSecurity.testDelegationToken(TestJHSSecurity.java:163) ```  It can be found that [HDFS-16563](https://issues.apache.org/jira/browse/HDFS-16563) is causing this problem.  The reason is because [HDFS-16563](https://issues.apache.org/jira/browse/HDFS-16563) changed error msg, which made MR's Jinit Test assertion fail.","closed","","slfan1989","2022-06-11T08:16:34Z","2022-06-20T06:44:04Z"
"","4403","MAPREDUCE-7385. improve JobEndNotifier#httpNotification With recommended methods","JIRA: MAPREDUCE-7385. impove JobEndNotifier#httpNotification With recommended methods.  JobEndNotifier#httpNotification's DefaultHttpClient has been Deprecated, use the recommended method instead CoreConnectionPNames.SO_TIMEOUT  - use RequestConfig.setSocketTimeout instead ``` Deprecated. Defines the socket timeout (SO_TIMEOUT) in milliseconds,  which is the timeout for waiting for data or, put differently,  a maximum period inactivity between two consecutive data packets).  A timeout value of zero is interpreted as an infinite timeout.  ``` ClientPNames.CONN_MANAGER_TIMEOUT  - use RequestConfig.setConnectionRequestTimeout instead ``` Deprecated.  Defines the timeout in milliseconds  used when retrieving an instance of ManagedClientConnection from the ClientConnectionManager.  ```","open","","slfan1989","2022-06-05T06:50:56Z","2022-08-02T14:47:32Z"
"","4390","MAPREDUCE-7384. impove import * In MapReduce Project.","JIRA: MAPREDUCE-7384. impove import * In MapReduce Project. Directly using * to reference does not conform to the code specification, adjust it and refer to the specified package.","closed","","slfan1989","2022-05-31T16:30:22Z","2022-06-01T16:18:07Z"
"","4556","HDFS-16656. Fixed some incorrect descriptions in SPS.","jira: https://issues.apache.org/jira/browse/HDFS-16656: Fixed some incorrect descriptions in SPS.","open","","whbing","2022-07-12T16:20:44Z","2022-07-12T22:04:08Z"
"","3879","HADOOP-16223. Remove misleading fs.s3a.delegation.tokens.enabled prompt","Jira: https://issues.apache.org/jira/browse/HADOOP-16223  ### Description of PR  This PR removes `fs.s3a.delegation.tokens.enabled` option from `core-default.xml` as this option is not used.  ### How was this patch tested?  Tested in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-01-12T13:33:11Z","2022-01-12T17:41:21Z"
"","4602","HDFS-16673. Fix usage of chown","JIRA: HDFS-16673. actually `chown`  command can be used for the owner of the files or the super user, we need to correct the doc","open","","GuoPhilipse","2022-07-21T08:12:34Z","2022-07-21T16:59:56Z"
"","4423","HDFS-16629. [JDK 11] Fix javadoc  warnings in hadoop-hdfs module.","JIRA: HDFS-16629. [JDK 11] Fix javadoc  warnings in hadoop-hdfs module.  During compilation of the most recently committed code, a java doc waring appeared and I will fix it.  the error in the compilation report([PR-4419](https://github.com/apache/hadoop/pull/4419)、[PR-4406](https://github.com/apache/hadoop/pull/4406))  ``` 1 error 100 warnings [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time:  37.132 s [INFO] Finished at: 2022-06-09T17:07:12Z [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.0.1:javadoc-no-fork (default-cli) on project hadoop-hdfs: An error has occurred in Javadoc report generation:  [ERROR] Exit code: 1 - javadoc: warning - You have specified the HTML version as HTML 4.01 by using the -html4 option. [ERROR] The default is currently HTML5 and the support for HTML 4.01 will be removed [ERROR] in a future release. To suppress this warning, please ensure that any HTML constructs  ```  this repair will take a long time.","closed","","slfan1989","2022-06-10T00:20:48Z","2022-06-17T12:50:17Z"
"","4406","HDFS-16619. Fix HttpHeaders.Values And HttpHeaders.Names Deprecated Import","JIRA: HDFS-16619. Fix HttpHeaders.Values And HttpHeaders.Names Deprecated Import.  HttpHeaders.Values ​​and HttpHeaders.Names are deprecated, use HttpHeaderValues ​​and HttpHeaderNames instead.  HttpHeaders.Names Deprecated.  Use HttpHeaderNames instead. Standard HTTP header names. ``` /** @deprecated */ @Deprecated public static final class Names {   public static final String ACCEPT = ""Accept"";   public static final String ACCEPT_CHARSET = ""Accept-Charset"";   public static final String ACCEPT_ENCODING = ""Accept-Encoding"";   public static final String ACCEPT_LANGUAGE = ""Accept-Language"";   public static final String ACCEPT_RANGES = ""Accept-Ranges"";   public static final String ACCEPT_PATCH = ""Accept-Patch"";   public static final String ACCESS_CONTROL_ALLOW_CREDENTIALS = ""Access-Control-Allow-Credentials"";   public static final String ACCESS_CONTROL_ALLOW_HEADERS = ""Access-Control-Allow-Headers"";  ... ```  HttpHeaders.Values Deprecated.  Use HttpHeaderValues instead. Standard HTTP header values. ``` /** @deprecated */ @Deprecated public static final class Values {   public static final String APPLICATION_JSON = ""application/json"";   public static final String APPLICATION_X_WWW_FORM_URLENCODED = ""application/x-www-form-urlencoded"";   public static final String BASE64 = ""base64"";   public static final String BINARY = ""binary"";   public static final String BOUNDARY = ""boundary"";   public static final String BYTES = ""bytes"";   public static final String CHARSET = ""charset"";   public static final String CHUNKED = ""chunked"";   public static final String CLOSE = ""close"";  ```","closed","","slfan1989","2022-06-06T00:59:00Z","2022-07-28T08:52:23Z"
"","4382","HDFS-16609. Fix Flakes Junit Tests that often report timeouts.","JIRA: HDFS-16609. Fix Flakes Junit Tests that often report timeouts.  When I was dealing with [HDFS-16590](https://issues.apache.org/jira/browse/HDFS-16590) JIRA, Junit Tests often reported errors, I found that one type of problem is TimeOut problem, these problems can be avoided by adjusting TimeOut time.  The modified method is as follows:  1.org.apache.hadoop.hdfs.TestFileCreation#testServerDefaultsWithMinimalCaching ``` [ERROR] testServerDefaultsWithMinimalCaching(org.apache.hadoop.hdfs.TestFileCreation)  Time elapsed: 7.136 s  <<< ERROR! java.util.concurrent.TimeoutException:  Timed out waiting for condition.  Thread diagnostics:   [WARNING] org.apache.hadoop.hdfs.TestFileCreation.testServerDefaultsWithMinimalCaching(org.apache.hadoop.hdfs.TestFileCreation) [ERROR]   Run 1: TestFileCreation.testServerDefaultsWithMinimalCaching:277 Timeout Timed out ... [INFO]   Run 2: PASS ```  2.org.apache.hadoop.hdfs.TestDFSShell#testFilePermissions ``` [ERROR] testFilePermissions(org.apache.hadoop.hdfs.TestDFSShell)  Time elapsed: 30.022 s  <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds 	at java.lang.Thread.dumpThreads(Native Method) 	at java.lang.Thread.getStackTrace(Thread.java:1549) 	at org.junit.internal.runners.statements.FailOnTimeout.createTimeoutException(FailOnTimeout.java:182) 	at org.junit.internal.runners.statements.FailOnTimeout.getResult(FailOnTimeout.java:177) 	at org.junit.internal.runners.statements.FailOnTimeout.evaluate(FailOnTimeout.java:128) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748)  [WARNING] org.apache.hadoop.hdfs.TestDFSShell.testFilePermissions(org.apache.hadoop.hdfs.TestDFSShell) [ERROR]   Run 1: TestDFSShell.testFilePermissions TestTimedOut test timed out after 30000 mil... [INFO]   Run 2: PASS  ``` 3.org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testSPSWhenFileHasExcessRedundancyBlocks ``` [ERROR] testSPSWhenFileHasExcessRedundancyBlocks(org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier)  Time elapsed: 67.904 s  <<< ERROR! java.util.concurrent.TimeoutException:  Timed out waiting for condition.   [WARNING] org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testSPSWhenFileHasExcessRedundancyBlocks(org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier) [ERROR]   Run 1: TestExternalStoragePolicySatisfier.testSPSWhenFileHasExcessRedundancyBlocks:1379 Timeout [ERROR]   Run 2: TestExternalStoragePolicySatisfier.testSPSWhenFileHasExcessRedundancyBlocks:1379 Timeout [INFO]   Run 3: PASS  ```","closed","","slfan1989","2022-05-30T20:49:17Z","2022-06-11T12:41:19Z"
"","4372","HDFS-16603. Improve DatanodeHttpServer With Netty recommended method.","JIRA: HDFS-16603. Improve DatanodeHttpServer With Netty recommended method.  When reading the code, I found that some usage methods are outdated due to the upgrade of netty components. 1.DatanodeHttpServer#Constructor ``` @Deprecated public static final ChannelOption WRITE_BUFFER_HIGH_WATER_MARK = valueOf(""WRITE_BUFFER_HIGH_WATER_MARK"");  Deprecated. Use WRITE_BUFFER_WATER_MARK  @Deprecated public static final ChannelOption WRITE_BUFFER_LOW_WATER_MARK = valueOf(""WRITE_BUFFER_LOW_WATER_MARK""); Deprecated. Use WRITE_BUFFER_WATER_MARK ----- this.httpServer.childOption(           ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK,           conf.getInt(               DFSConfigKeys.DFS_WEBHDFS_NETTY_HIGH_WATERMARK,               DFSConfigKeys.DFS_WEBHDFS_NETTY_HIGH_WATERMARK_DEFAULT));  this.httpServer.childOption(           ChannelOption.WRITE_BUFFER_LOW_WATER_MARK,           conf.getInt(               DFSConfigKeys.DFS_WEBHDFS_NETTY_LOW_WATERMARK,               DFSConfigKeys.DFS_WEBHDFS_NETTY_LOW_WATERMARK_DEFAULT));  ``` 2.Duplicate code ``` ChannelFuture f = httpServer.bind(infoAddr); try {  f.syncUninterruptibly(); } catch (Throwable e) {   if (e instanceof BindException) {    throw NetUtils.wrapException(null, 0, infoAddr.getHostName(),    infoAddr.getPort(), (SocketException) e);  } else {    throw e;  } } httpAddress = (InetSocketAddress) f.channel().localAddress(); ``` 3.io.netty.bootstrap.ChannelFactory Deprecated use io.netty.channel.ChannelFactory instead.","closed","","slfan1989","2022-05-29T03:43:54Z","2022-05-31T22:45:08Z"
"","4362","HDFS-16597. RBF: Improve RouterRpcServer#reload With Lambda","JIRA: HDFS-16597 Improve RouterRpcServer#reload With Lambda  When reading the code, I found that RouterRpcServer#reload uses the following method to submit threads RouterRpcServer#reload ``` public ListenableFuture reload(         final DatanodeReportType type, DatanodeInfo[] oldValue)         throws Exception {       return executorService.submit(new Callable() {         @Override         public DatanodeInfo[] call() throws Exception {           return load(type);         }       });     }  ```  This place is better to use lambda .","closed","","slfan1989","2022-05-27T06:46:49Z","2022-05-28T04:26:07Z"
"","4349","HDFS-16590. Fix Junit Test Deprecated assertThat","JIRA: HDFS-16590. Fix Junit Test Deprecated assertThat  javac will give a warning for compilation, as follows:  ``` TestIncrementalBrVariations.java:141:4:[deprecation] assertThat(T,Matcher) in Assert has been deprecated. ```  by looking at the API documentation, the following can be found.  ``` org.junit.Assert.assertThat Deprecated. use org.hamcrest.MatcherAssert.assertThat() ```","open","","slfan1989","2022-05-24T00:27:18Z","2022-06-08T22:45:11Z"
"","4310","HDFS-16579. Fix build failure for TestBlockManager on branch-3.2","JIRA: HDFS-16579.  Fix build failure for TestBlockManager on branch-3.2. See [HDFS-16552](https://issues.apache.org/jira/browse/HDFS-16552).","closed","","tomscut","2022-05-14T00:46:35Z","2022-05-15T12:25:12Z"
"","4322","HDFS-16574. Reduces the time it takes once to hold FSNamesystem write lock to remove blocks associated with dead datanodes","JIRA: HDFS-16574  Reduces the time it takes once to hold FSNamesystem write lock to remove blocks associated with dead datanodes.","open","","Happy-shi","2022-05-17T16:56:51Z","2022-06-28T22:22:22Z"
"","4231","HDFS-16560. [SPS]: Duplicate method names in TestExternalStoragePolic…","JIRA: HDFS-16560.  Duplicate method names in TestExternalStoragePolicySatisfier.","closed","","tomscut","2022-04-24T09:09:20Z","2022-04-24T14:28:25Z"
"","4219","HDFS-16557. BootstrapStandby failed because of checking gap for inprogress EditLogInputStream","JIRA: HDFS-16557.  The lastTxId of an inprogress EditLogInputStream lastTxId isn't necessarily HdfsServerConstants.INVALID_TXID. We can determine its status directly by EditLogInputStream#isInProgress.  We introduced [SBN READ], and set `dfs.ha.tail-edits.in-progress=true`. Then bootstrapStandby a new Namenode, the EditLogInputStream of inProgress is misjudged, resulting in a gap check failure, which causes bootstrapStandby to fail.  `hdfs namenode -bootstrapStandby` ![image](https://user-images.githubusercontent.com/55134131/164676951-686f46ae-9b89-4be8-8d3c-41a08bb432ae.png) ![image](https://user-images.githubusercontent.com/55134131/164676977-bd3ece9d-3ffc-406f-8c06-aacdeac0dee8.png)","open","","tomscut","2022-04-22T09:21:28Z","2022-06-10T03:46:32Z"
"","4210","HDFS-16552. Fix NPE for TestBlockManager","JIRA: HDFS-16552.  Fix NPE for BlockManager#scheduleReconstruction.  There is a NPE in BlockManager when run TestBlockManager#testSkipReconstructionWithManyBusyNodes2. Because NameNodeMetrics is not initialized in this unit test.  Related ci link, see [this](https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4209/1/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt).","closed","","tomscut","2022-04-21T10:04:51Z","2022-04-23T03:20:37Z"
"","4209","HDFS-16550. [SBN read] Improper cache-size for journal node may cause cluster crash","JIRA: HDFS-16550.  For details, please refer to the JIRA.","open","","tomscut","2022-04-21T02:17:09Z","2022-06-10T03:53:20Z"
"","4201","HDFS-16547. [SBN read] Namenode in safe mode should not be transfered to observer state","JIRA: HDFS-16547.  Currently, when a Namenode is in safemode(under starting or enter safemode manually), we can transfer this Namenode to Observer by command. This Observer node may receive many requests and then throw a SafemodeException, this causes unnecessary failover on the client.  So Namenode in safe mode should not be transfer to observer state.","open","","tomscut","2022-04-19T11:27:28Z","2022-07-18T23:49:49Z"
"","4234","HDFS-16488. [SPS]: Expose metrics to JMX for external SPS","JIRA: HDFS-16488.  Since this PR #4035 has been revert due to some problems, I submit a new one.","closed","","tomscut","2022-04-24T11:30:55Z","2022-04-26T05:02:55Z"
"","4679","HADOOP-18387. Fix incorrect placeholder in hadoop-module.","JIRA: HADOOP-18387. Fix incorrect placeholder in hadoop-module.","open","","slfan1989","2022-08-02T23:32:44Z","2022-08-03T16:35:23Z"
"","4334","HADOOP-18248. Fix Junit Test Deprecated assertThat.","JIRA: HADOOP-18248 Fix Junit Test Deprecated assertThat  javac will give a warning for compilation, as follows: TestIncrementalBrVariations.java:141:4:[deprecation] assertThat(T,Matcher) in Assert has been deprecated.  by looking at the API documentation, the following can be found.  **org.junit.Assert.assertThat Deprecated.  use org.hamcrest.MatcherAssert.assertThat()**","closed","","slfan1989","2022-05-20T03:21:38Z","2022-05-24T08:25:06Z"
"","4293","YARN-11137. Improve log message in FederationClientInterceptor#submitApplication","JIRA: [YARN-11137-Improve log message in FederationClientInterceptor#submitApplication](https://issues.apache.org/jira/browse/YARN-11137)   Fix Slf4j Log CodeStyle","closed","","slfan1989","2022-05-09T11:13:10Z","2022-05-20T08:59:16Z"
"","4429","MAPREDUCE-7388. Remove unused variable _eof in GzipCodec.cc","JIRA: [MAPREDUCE-7388](https://issues.apache.org/jira/browse/MAPREDUCE-7388) remove unused variable _eof in GzipCodec.cc","closed","","cfg1234","2022-06-11T09:22:47Z","2022-07-08T18:05:30Z"
"","4407","HDFS-16622. addRDBI in IncrementalBlockReportManager may remove the b…","JIRA: [HDFS-16622](https://issues.apache.org/jira/browse/HDFS-16622).  addRDBI in IncrementalBlockReportManager may remove the block with bigger GS. I suspect there is a bug in function addRDBI(ReceivedDeletedBlockInfo rdbi,DatanodeStorage storage)(line 250). Bug code in the for loop: ``` synchronized void addRDBI(ReceivedDeletedBlockInfo rdbi,       DatanodeStorage storage) {     // Make sure another entry for the same block is first removed.     // There may only be one such entry.     for (PerStorageIBR perStorage : pendingIBRs.values()) {       if (perStorage.remove(rdbi.getBlock()) != null) {         break;       }     }     getPerStorageIBR(storage).put(rdbi);   } ``` Removed the GS of the Block in ReceivedDeletedBlockInfo may be greater than the GS of the Block in rdbi. And NN will invalidate the Replicate will small GS when complete one block.","open","","ZanderXu","2022-06-06T02:53:04Z","2022-06-09T13:35:03Z"
"","4176","HDFS-16541. Fix a typo in NameNodeLayoutVersion","JIRA: [HDFS-16541](https://issues.apache.org/jira/browse/HDFS-16541).  Fix a typo in NameNodeLayoutVersion.","closed","","Happy-shi","2022-04-15T02:57:55Z","2022-04-18T17:24:05Z"
"","4129","HDFS-16527. Add global timeout rule for TestRouterDistCpProcedure","JIRA: [HDFS-16527](https://issues.apache.org/jira/browse/HDFS-16527).  As @ayushtkn mentioned [here](https://issues.apache.org/jira/browse/HDFS-16527#pullrequestreview-925554297].) TestRouterDistCpProcedure failed many times because of timeout. I will add a global timeout rule for it. This makes it easy to set the timeout.","closed","","tomscut","2022-04-01T02:25:14Z","2022-04-06T05:34:43Z"
"","4122","HDFS-16525.System.err should be used when error occurs in multiple methods in DFSAdmin class","JIRA: [HDFS-16525](https://issues.apache.org/jira/browse/HDFS-16525).","closed","","singer-bin","2022-03-30T14:08:47Z","2022-05-12T09:16:29Z"
"","4088","HDFS-16514. Reduce the failover sleep time if multiple namenode are c…","JIRA: [HDFS-16514](https://issues.apache.org/jira/browse/HDFS-16514) Recently, we used the [Standby Read] feature in our test cluster, and deployed 4 namenode as follow: node1 -> active nn node2 -> standby nn node3 -> observer nn node3 -> observer nn  If we set ’dfs.client.failover.random.order=true‘, the client may failover twice and wait a long time to send msync to active namenode.   ![image](https://user-images.githubusercontent.com/2844826/159257471-4398ae11-fad3-4aee-8f56-1b89bef2f611.png)   I think we can reduce the sleep time of the first several failover based on the number of namenode. For example, if 4 namenode are configured, the sleep time of first three failover operations is set to zero.","open","","liubingxing","2022-03-21T12:06:00Z","2022-04-15T12:39:29Z"
"","4087","HDFS-16513. [SBN read] Observer Namenode should not trigger the edits rolling of active Namenode","JIRA: [HDFS-16513](https://issues.apache.org/jira/browse/HDFS-16513).  To avoid frequent edtis rolling, we should disable OBN from triggering the edits rolling of active Namenode.   It is sufficient to retain only the `triggering of SNN` and the `auto rolling of ANN`.","open","","tomscut","2022-03-21T03:33:45Z","2022-04-14T01:57:37Z"
"","4082","HDFS-16507. [SBN read] Avoid purging edit log which is in progress","JIRA: [HDFS-16507](https://issues.apache.org/jira/browse/HDFS-16507).  Avoid purging edit log which is in progress. This is described in detail in JIRA.  The stack: ``` java.lang.Thread.getStackTrace(Thread.java:1552)     org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032)     org.apache.hadoop.hdfs.server.namenode.FileJournalManager.purgeLogsOlderThan(FileJournalManager.java:185)     org.apache.hadoop.hdfs.server.namenode.JournalSet$5.apply(JournalSet.java:623)     org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:388)     org.apache.hadoop.hdfs.server.namenode.JournalSet.purgeLogsOlderThan(JournalSet.java:620)     org.apache.hadoop.hdfs.server.namenode.FSEditLog.purgeLogsOlderThan(FSEditLog.java:1512) org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.purgeOldStorage(NNStorageRetentionManager.java:177)     org.apache.hadoop.hdfs.server.namenode.FSImage.purgeOldStorage(FSImage.java:1249)     org.apache.hadoop.hdfs.server.namenode.ImageServlet$2.run(ImageServlet.java:617)     org.apache.hadoop.hdfs.server.namenode.ImageServlet$2.run(ImageServlet.java:516)     java.security.AccessController.doPrivileged(Native Method)     javax.security.auth.Subject.doAs(Subject.java:422)     org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)     org.apache.hadoop.hdfs.server.namenode.ImageServlet.doPut(ImageServlet.java:515)     javax.servlet.http.HttpServlet.service(HttpServlet.java:710)     javax.servlet.http.HttpServlet.service(HttpServlet.java:790)     org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)     org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)     org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1604)     org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)     org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)     org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)     org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)     org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)     org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)     org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)     org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)     org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)     org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)     org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)     org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)     org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)     org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)     org.eclipse.jetty.server.Server.handle(Server.java:539)     org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)     org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)     org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)     org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)     org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)     org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)     org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)     org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)     org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)     org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)     java.lang.Thread.run(Thread.java:745) ```","closed","","tomscut","2022-03-18T00:15:45Z","2022-03-31T06:03:15Z"
"","4079","HDFS-16507. Avoid purging edit log which is in progress","JIRA: [HDFS-16507](https://issues.apache.org/jira/browse/HDFS-16507).  Avoid purging edit log which is in progress. This is described in detail in JIRA.","closed","","tomscut","2022-03-17T15:20:21Z","2022-03-18T00:07:58Z"
"","4071","HDFS-16505. Setting safemode should not be interrupted by abnormal nodes","JIRA: [HDFS-16505](https://issues.apache.org/jira/browse/HDFS-16505).  Setting safemode should not be interrupted by abnormal nodes.   For example, we have four namenodes configured in the following order: NS1 -> active NS2 -> standby NS3 -> observer NS4 -> observer.  When the `NS1` process exits, setting the states of safemode, `NS2`, `NS3`, and `NS4` fails. Similarly, when the `NS2` process exits, only the safemode state of `NS1` can be set successfully.  When the `NS1` process exits: Before the change: ![image](https://user-images.githubusercontent.com/55134131/158288412-b0915dba-aa69-4e71-a47d-1d7728ba673d.png)  After the change: ![image](https://user-images.githubusercontent.com/55134131/158288423-f7de2689-ad0b-455e-9717-1b5ff877f20f.png)","open","","tomscut","2022-03-15T01:35:25Z","2022-03-29T03:00:23Z"
"","4067","HDFS-16503. Should verify whether the path name is valid in the WebHDFS","JIRA: [HDFS-16503](https://issues.apache.org/jira/browse/HDFS-16503).   When creating a file using WebHDFS, there are two main steps: 1. Obtain the location of the Datanode to be written. 2. Put the file to this location.  Currently `NameNodeRpcServer` verifies that pathName is valid, but `NamenodeWebHdfsMethods` and `RouterWebHdfsMethods` do not.  So if we use an invalid path(such as duplicated slash), the first step returns success, but the second step throws an `InvalidPathException`.  IMO, we should also do the validation in WebHdfs, which is consistent with the `NameNodeRpcServer`. ![image](https://user-images.githubusercontent.com/55134131/158090641-88a81a28-d700-44c4-90e1-142be5b448cc.png)  The same WebHDFS operations are: `CREATE`, `APPEND`, `OPEN`, `GETFILECHECKSUM`. So we can add `DFSUtil.isValidName` to `redirectURI` for `NamenodeWebHdfsMethods` and `RouterWebHdfsMethods`.","closed","","tomscut","2022-03-14T01:43:45Z","2022-03-21T07:20:56Z"
"","4058","HDFS-16499. [SPS]: Should not start indefinitely while another SPS process is running","JIRA: [HDFS-16499](https://issues.apache.org/jira/browse/HDFS-16499).  Normally, we can only start one SPS process at a time. When one process is running, start another process and retry indefinitely. I think, in this case, we should exit immediately.","closed","","tomscut","2022-03-10T02:25:06Z","2022-03-18T00:32:09Z"
"","4057","HDFS-16498. Fix NPE for checkBlockReportLease","JIRA: [HDFS-16498](https://issues.apache.org/jira/browse/HDFS-16498).  During the restart of Namenode, a Datanode is not registered, but this Datanode triggers FBR, which causes NPE.  ![image](https://user-images.githubusercontent.com/55134131/157443088-fddf4452-f783-47d3-9e92-b38c9a97c412.png)","closed","","tomscut","2022-03-09T12:38:07Z","2022-03-30T07:01:29Z"
"","4035","HDFS-16488. [SPS]: Expose metrics to JMX for external SPS","JIRA: [HDFS-16488](https://issues.apache.org/jira/browse/HDFS-16488).  Currently, external SPS has no monitoring metrics. We do not know how many blocks are waiting to be processed, how many blocks are waiting to be retried, and how many blocks have been migrated.  We can expose these metrics in JMX for easy collection and display by monitoring systems. ![sps-jmx-metrics](https://user-images.githubusercontent.com/55134131/155846364-241bbd40-f506-4421-bf06-5e70a637b424.jpg)  For example, in our cluster, we exposed these metrics to JMX, collected by JMX-Exporter and combined with Prometheus, and finally display by Grafana.","closed","","tomscut","2022-02-26T14:15:00Z","2022-04-24T09:22:14Z"
"","4032","HDFS-16484. [SPS]: Fix an infinite loop bug in SPSPathIdProcessor thread","JIRA: [HDFS-16484](https://issues.apache.org/jira/browse/HDFS-16484)  Currently, we ran SPS in our cluster and found this log. The SPSPathIdProcessor thread enters an infinite loop and prints the same log all the time. ![image](https://user-images.githubusercontent.com/2844826/159828355-7bad2cb5-4b11-40a3-8854-666262c2ab32.png)  The reason is that #ctxt.getNextSPSPath() get a inodeId which path does not exist. The inodeId will not be set to null, causing the thread hold this inodeId forever. ```java public void run() {   LOG.info(""Starting SPSPathIdProcessor!."");   Long startINode = null;   while (ctxt.isRunning()) {     try {       if (!ctxt.isInSafeMode()) {         if (startINode == null) {           startINode = ctxt.getNextSPSPath();         } // else same id will be retried         if (startINode == null) {           // Waiting for SPS path           Thread.sleep(3000);         } else {           ctxt.scanAndCollectFiles(startINode);           // check if directory was empty and no child added to queue           DirPendingWorkInfo dirPendingWorkInfo =               pendingWorkForDirectory.get(startINode);           if (dirPendingWorkInfo != null               && dirPendingWorkInfo.isDirWorkDone()) {             ctxt.removeSPSHint(startINode);             pendingWorkForDirectory.remove(startINode);           }         }         startINode = null; // Current inode successfully scanned.       }     } catch (Throwable t) {       String reClass = t.getClass().getName();       if (InterruptedException.class.getName().equals(reClass)) {         LOG.info(""SPSPathIdProcessor thread is interrupted. Stopping.."");         break;       }       LOG.warn(""Exception while scanning file inodes to satisfy the policy"",           t);       try {         Thread.sleep(3000);       } catch (InterruptedException e) {         LOG.info(""Interrupted while waiting in SPSPathIdProcessor"", t);         break;       }     }   } }  ```","closed","","liubingxing","2022-02-25T06:46:39Z","2022-04-13T03:22:31Z"
"","4009","HDFS-16477. [SPS]: Add metric PendingSPSPaths for getting the number of paths to be processed by SPS","JIRA: [HDFS-16477](https://issues.apache.org/jira/browse/HDFS-16477).  Currently we have no idea how many paths are waiting to be processed when using the SPS feature. We should add metric `PendingSPSPaths` for getting the number of paths to be processed by SPS in NameNode.","closed","","tomscut","2022-02-22T08:16:49Z","2022-04-03T02:28:45Z"
"","4001","HDFS-16460. [SPS]: Handle failure retries for moving tasks","JIRA: [HDFS-16460](https://issues.apache.org/jira/browse/HDFS-16460).  Handle failure retries for moving tasks.","closed","","tomscut","2022-02-19T13:08:11Z","2022-04-08T04:56:02Z"
"","3998","HDFS-16458. [SPS]: Fix bug for unit test of reconfiguring SPS mode","JIRA: [HDFS-16458](https://issues.apache.org/jira/browse/HDFS-16458).  TestNameNodeReconfigure#verifySPSEnabled was compared with itself(`isSPSRunning`) at assertEquals.  In addition, after an `internal SPS `has been removed, `spsService daemon` will not start within StoragePolicySatisfyManager. I think the relevant code can be removed to simplify the code.  IMO, after reconfig SPS mode, we just need to confirm whether the mode is correct and whether spsManager is NULL.","closed","","tomscut","2022-02-18T08:47:34Z","2022-03-01T01:36:25Z"
"","4041","HDFS-16458. [SPS]: Fix bug for unit test of reconfiguring SPS mode","JIRA: [HDFS-16458](https://issues.apache.org/jira/browse/HDFS-16458).  **_I resubmitted a PR, because the previous PR [#3998](https://github.com/apache/hadoop/pull/3998) forgot to set the `issue ID`._**  TestNameNodeReconfigure#verifySPSEnabled was compared with itself(`isSPSRunning`) at assertEquals.  In addition, after an `internal SPS `has been removed, `spsService daemon` will not start within StoragePolicySatisfyManager. I think the relevant code can be removed to simplify the code.  IMO, after reconfig SPS mode, we just need to confirm whether the mode is correct and whether spsManager is NULL.","closed","","tomscut","2022-03-01T01:33:10Z","2022-03-02T03:26:11Z"
"","4069","HDFS-16457.Make fs.getspaceused.classname reconfigurable","JIRA: [HDFS-16457](https://issues.apache.org/jira/browse/HDFS-16457). Make fs.getspaceused.classname reconfigurable","closed","","singer-bin","2022-03-14T09:52:04Z","2022-04-08T01:49:01Z"
"","3960","HDFS-16446. Consider ioutils of disk when choosing volume","JIRA: [HDFS-16446](https://issues.apache.org/jira/browse/HDFS-16446).  Consider ioutils of disk when choosing volume to avoid busy disks.  Document: https://docs.google.com/document/d/1Ko1J7shz8hVLnNACT6PKVQ_leIHf_YaIFA2s3yJMZHQ/edit?usp=sharing  Architecture is as follows: ![image](https://user-images.githubusercontent.com/55134131/159827737-f4ca4d66-c2f2-4bef-901b-6d2bc7bdda9a.png)","open","","tomscut","2022-02-05T02:40:36Z","2022-07-14T03:56:42Z"
"","3943","HDFS-16444. Show start time of JournalNode on Web","JIRA: [HDFS-16444](https://issues.apache.org/jira/browse/HDFS-16444).  Before: ![image](https://user-images.githubusercontent.com/55134131/151638000-2c6a3a5b-0a13-4312-bfd2-acee589faa47.png)  After: ![image](https://user-images.githubusercontent.com/55134131/151638006-2d847bfb-1fdf-4db1-a156-7559702c5e31.png)","closed","","tomscut","2022-01-29T00:13:04Z","2022-01-30T15:15:03Z"
"","3971","HDFS-16440. RBF: Support router get HAServiceStatus with Lifeline RPC address","JIRA: [HDFS-16440](https://issues.apache.org/jira/browse/HDFS-16440)  NamenodeHeartbeatService gets HAServiceStatus using NNHAServiceTarget.getProxy. When we set a special dfs.namenode.lifeline.rpc-address , NamenodeHeartbeatService may get HAServiceStatus using NNHAServiceTarget.getHealthMonitorProxy.","closed","","yulongz","2022-02-09T11:48:39Z","2022-02-15T16:44:17Z"
"","3928","HDFS-16438. Avoid holding read locks for a long time when scanDatanodeStorage","JIRA: [HDFS-16438](https://issues.apache.org/jira/browse/HDFS-16438).  At the time of decommission, if use `DatanodeAdminBackoffMonitor`, there is a heavy operation: `scanDatanodeStorage`. If the number of blocks on a storage is large(more than 5 hundred thousand), and GC performance is also poor, it may hold **read lock** for a long time, we should optimize it.  ![image](https://user-images.githubusercontent.com/55134131/151006515-c47ad5d9-96a6-44d1-9244-5b74a4f11f45.png)  ``` 2021-12-22 07:49:01,279 INFO  namenode.FSNamesystem (FSNamesystemLock.java:readUnlock(220)) - FSNamesystem scanDatanodeStorage read lock held for 5491 ms via java.lang.Thread.getStackTrace(Thread.java:1552) org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032) org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.readUnlock(FSNamesystemLock.java:222) org.apache.hadoop.hdfs.server.namenode.FSNamesystem.readUnlock(FSNamesystem.java:1641) org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor.scanDatanodeStorage(DatanodeAdminBackoffMonitor.java:646) org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor.checkForCompletedNodes(DatanodeAdminBackoffMonitor.java:417) org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor.check(DatanodeAdminBackoffMonitor.java:300) org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminBackoffMonitor.run(DatanodeAdminBackoffMonitor.java:201) java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) java.lang.Thread.run(Thread.java:745)     Number of suppressed read-lock reports: 0     Longest read-lock held interval: 5491  ```","open","","tomscut","2022-01-25T15:29:52Z","2022-01-28T19:41:31Z"
"","3912","HDFS-16435. Remove no need TODO comment for ObserverReadProxyProvider","JIRA: [HDFS-16435](https://issues.apache.org/jira/browse/HDFS-16435).  Based on discussion in [HDFS-13923](https://issues.apache.org/jira/browse/HDFS-13923), we don't think need to add a configuration to turn on/off observer reads.  So I suggest removing the `TODO comment` that are not needed.","closed","","tomscut","2022-01-21T07:30:33Z","2022-02-03T04:25:43Z"
"","3915","HDFS-16434. Add opname to read/write lock for remaining operations","JIRA: [HDFS-16434](https://issues.apache.org/jira/browse/HDFS-16434).  In this issue at [HDFS-10872](https://issues.apache.org/jira/browse/HDFS-10872), we add opname to read and write locks. However, there are still many operations that have not been completed. When analyzing some operations that hold locks for a long time, we can only find specific methods through stack. I suggest that these remaining operations be completed to facilitate later performance optimization.","closed","","tomscut","2022-01-22T14:00:26Z","2022-03-25T11:26:57Z"
"","3907","HDFS-16432. Namenode block report add yield to avoid holding write lo…","JIRA: [HDFS-16432](https://issues.apache.org/jira/browse/HDFS-16432)  ![image](https://user-images.githubusercontent.com/2844826/150293279-07d7bbf0-1471-464f-af81-7d5c23aeadcd.png)  In our cluster, namenode block report will held write lock for a long time if the storage block number more than 100000. So we want to add a yield mechanism in block reporting process to avoid holding write lock too long.  1. Ensure that the processing of the same block is in the same write lock. 2. Because StorageInfo.addBlock will moves the block to the head of blockList, so we can collect blocks that have not been reported by delimiter block.","open","","liubingxing","2022-01-20T07:32:03Z","2022-02-09T09:07:32Z"
"","3888","HDFS-16427. Add debug log for BlockManager#chooseExcessRedundancyStriped","JIRA: [HDFS-16427](https://issues.apache.org/jira/browse/HDFS-16427).  To solve this issue [HDFS-16420](https://issues.apache.org/jira/browse/HDFS-16420) , we added some debug logs, which were also necessary. If there are other problems, we set the log level to DEBUG, which is convenient to analyze it.","closed","","tomscut","2022-01-13T15:28:39Z","2022-01-27T04:43:06Z"
"","3887","HDFS-16426. Fix nextBlockReportTime when trigger full block report force","jira: [HDFS-16426](https://issues.apache.org/jira/browse/HDFS-16426)  When we trigger full block report force by command line, the next block report time will be set like this:  nextBlockReportTime.getAndAdd(blockReportIntervalMs);  nextBlockReportTime will larger than blockReportIntervalMs.  If we trigger full block report twice, the nextBlockReportTime will larger than 2 * blockReportIntervalMs. This is obviously not what we want.  We fix the nextBlockReportTime = now + blockReportIntervalMs after full block report trigger by command line.","closed","","liubingxing","2022-01-13T13:25:46Z","2022-01-19T09:33:28Z"
"","3883","HDFS-16423. Balancer should not get blocks on stale storages","JIRA: [HDFS-16423](https://issues.apache.org/jira/browse/HDFS-16423) We have met a problems as described in [HDFS-16420](https://issues.apache.org/jira/browse/HDFS-16420)  We found that balancer copied a block multi times without deleting the source block if this block was placed in a stale storage. And resulting a block with many copies, but these redundant copies are not deleted until the storage become not stale. ![image](https://user-images.githubusercontent.com/2844826/149301502-cf60927a-fc4b-4089-a856-ced5e90151bc.png)","closed","","liubingxing","2022-01-13T09:19:28Z","2022-01-19T09:34:03Z"
"","3863","HDFS-16413. Reconfig dfs usage parameters for datanode","JIRA: [HDFS-16413](https://issues.apache.org/jira/browse/HDFS-16413).  Reconfig dfs usage parameters for datanode.","closed","","tomscut","2022-01-05T15:47:49Z","2022-03-31T01:23:07Z"
"","3878","HDFS-16411 RBF: RouterId is NULL when disable RourterRpcServer","JIRA: [HDFS-16411](https://issues.apache.org/jira/browse/HDFS-16411)  When dfs.federation.router.rpc.enable=false, routerid is null, but RouterHeartbeatService need updateStateStore() with routerId.","closed","","yulongz","2022-01-12T05:18:49Z","2022-02-08T16:40:57Z"
"","3844","HDFS-16404. Fix typo for CachingGetSpaceUsed","JIRA: [HDFS-16404](https://issues.apache.org/jira/browse/HDFS-16404).  Fix typo for CachingGetSpaceUsed.","closed","","tomscut","2022-01-01T00:44:15Z","2022-01-09T08:41:10Z"
"","3839","HDFS-16402. Improve HeartbeatManager logic to avoid incorrect stats.","JIRA: [HDFS-16402](https://issues.apache.org/jira/browse/HDFS-16402).  After reconfig `dfs.datanode.data.dir`, we found that the stats of the Namenode Web became **negative** and there were many NPE in namenode logs. This problem has been solved by [HDFS-14042](https://issues.apache.org/jira/browse/HDFS-14042). ![namenode-web](https://user-images.githubusercontent.com/55134131/147616636-e57990d3-3611-4c92-93b1-219d8a346ddf.jpg) ![npe-log](https://user-images.githubusercontent.com/55134131/147616641-89b4c7eb-8e7e-43f0-9840-447ae01f8b05.jpg)   However, if `HeartbeatManager#updateHeartbeat` and `HeartbeatManager#updateLifeline` throw other exceptions, stats errors can also occur. We should ensure that `stats.subtract()` and `stats.add()` are transactional.","closed","","tomscut","2021-12-29T00:23:11Z","2022-01-24T06:26:30Z"
"","3843","HDFS-16400. Reconfig DataXceiver parameters for datanode","JIRA: [HDFS-16400](https://issues.apache.org/jira/browse/HDFS-16400).  To avoid frequent rolling restarts of the DN, we should make DataXceiver parameters reconfigurable.","closed","","tomscut","2021-12-31T01:15:37Z","2022-01-19T09:44:19Z"
"","3841","HDFS-16399. Reconfig cache report parameters for datanode","JIRA: [HDFS-16399](https://issues.apache.org/jira/browse/HDFS-16399).  Reconfig cache report parameters for datanode.","closed","","tomscut","2021-12-30T01:31:45Z","2022-01-19T09:44:58Z"
"","3831","HDFS-16398. Reconfig block report parameters for datanode","JIRA: [HDFS-16398](https://issues.apache.org/jira/browse/HDFS-16398).  Reconfig block report parameters for datanode. These configurations are included: `dfs.blockreport.intervalMsec` (see [HDFS-16331](https://issues.apache.org/jira/browse/HDFS-16331)) `dfs.blockreport.initialDelay`, `dfs.blockreport.split.threshold`.","closed","","tomscut","2021-12-28T01:36:57Z","2022-01-26T08:38:10Z"
"","3828","HDFS-16397. Reconfig slow disk parameters for datanode","JIRA: [HDFS-16397](https://issues.apache.org/jira/browse/HDFS-16397).  In large clusters, rolling restart datanodes takes long time. We can make slow peers parameters and slow disks parameters in datanode reconfigurable to facilitate cluster operation and maintenance.","closed","","tomscut","2021-12-27T01:42:06Z","2022-02-25T00:28:03Z"
"","3827","HDFS-16396. Reconfig slow peer parameters for datanode","JIRA: [HDFS-16396](https://issues.apache.org/jira/browse/HDFS-16396).  In large clusters, rolling restart datanodes takes a long time. We can make slow peers parameters and slow disks parameters in datanode reconfigurable to facilitate cluster operation and maintenance.","closed","","tomscut","2021-12-26T04:31:33Z","2022-02-15T04:41:11Z"
"","3787","HDFS-16379. Reset fullBlockReportLeaseId after any exceptions","JIRA: [HDFS-16379](https://issues.apache.org/jira/browse/HDFS-16379).  Recently we encountered FBR-related problems in the production environment, which were solved by introducing HDFS-12914 and HDFS-14314.  But there may be situations like this: 1 DN got `fullBlockReportLeaseId` via heartbeat.  2 DN trigger a blockReport, but some exception occurs (this may be rare, but it may exist), and then DN does multiple retries without resetting fullBlockReportLeaseId. Because fullBlockReportLeaseId is reset only if it succeeds currently.  3 After a while, the exception is cleared, but the `fullBlockReportLeaseId` has expired. Since NN did not throw an exception after the lease expired, the DN considered that the blockReport was successful. So the blockReport was not actually executed this time and needs to wait until the next time.  Therefore, should we consider resetting the `fullBlockReportLeaseId` in the finally block? The advantage of this is that lease expiration can be avoided. The downside is that each heartbeat will apply for a new `fullBlockReportLeaseId` during the exception, but I think this cost is negligible.","open","","tomscut","2021-12-11T02:50:26Z","2021-12-11T08:18:12Z"
"","3786","HDFS-16378. Add datanode address to BlockReportLeaseManager logs","JIRA: [HDFS-16378](https://issues.apache.org/jira/browse/HDFS-16378).  We should add datanode address to `BlockReportLeaseManager` logs. Because the `datanodeuuid` is not convenient for tracking.  ![image](https://user-images.githubusercontent.com/55134131/145660149-8b215301-753a-49b8-b471-65f574589032.png)","closed","","tomscut","2021-12-11T02:02:21Z","2021-12-15T03:22:42Z"
"","3784","HDFS-16377. Should CheckNotNull before access FsDatasetSpi","JIRA: [HDFS-16377](https://issues.apache.org/jira/browse/HDFS-16377).  When starting the DN, we found NPE in the **staring DN's log**, as follows: ![image](https://user-images.githubusercontent.com/55134131/145567811-be577f4a-169b-40a8-b1b0-90ef69cc38ba.png)  The logs of the **upstream DN** are as follows: ![image](https://user-images.githubusercontent.com/55134131/145567828-fe5677f3-6929-46b3-a095-9f0b10287961.png)  This is mainly because `FsDatasetSpi` has not been initialized at the time of access.   I noticed that checkNotNull is already done in these two method(`DataNode#getBlockLocalPathInfo` and `DataNode#getVolumeInfo`). So we should add it to other places(interfaces that clients and other DN can access directly) so that we can add a message(`Storage not yet initialized`) when throwing exceptions.  Therefore, the client and the upstream DN know that `FsDatasetSpi` has not been initialized, rather than blindly unaware of the specific cause of the NPE.","closed","","tomscut","2021-12-10T11:34:02Z","2021-12-16T04:50:20Z"
"","3778","HDFS-16376. Expose metrics of NodeNotChosenReason to JMX","JIRA: [HDFS-16376](https://issues.apache.org/jira/browse/HDFS-16376).  In our cluster, we can see logs for nodes that are not chosen. But it's hard to see the percentages in each reason from the logs. It is best to add relevant metrics to monitor the entire cluster.  ![image](https://user-images.githubusercontent.com/55134131/145429562-88b28729-4486-44fd-8d47-09b6a5c94864.png)  **JMX metrics:** ![block-placement-metrics](https://user-images.githubusercontent.com/55134131/145430372-d0282da3-586e-443a-87b9-7a079f31c675.jpg)","open","","tomscut","2021-12-09T15:56:58Z","2022-01-01T18:30:59Z"
"","3769","HDFS-16375. The FBR lease ID should be exposed to the log","JIRA: [HDFS-16375](https://issues.apache.org/jira/browse/HDFS-16375).  Our Hadoop version is 3.1.0. We encountered [HDFS-12914](https://issues.apache.org/jira/browse/HDFS-12914) and [HDFS-14314](https://issues.apache.org/jira/browse/HDFS-14314) in the production environment.  When locating the problem, the `fullBrLeaseId` was not exposed in the log, which caused some difficulties. We should add it to the log.","closed","","tomscut","2021-12-08T15:39:12Z","2021-12-16T04:29:57Z"
"","3753","HDFS-16371. Exclude slow disks when choosing volume","JIRA: [HDFS-16371](https://issues.apache.org/jira/browse/HDFS-16371).  Currently, the datanode can detect slow disks. See [HDFS-11461](https://issues.apache.org/jira/browse/HDFS-11461).  And after [HDFS-16311](https://issues.apache.org/jira/browse/HDFS-16311), the slow disks metrics we collected is more accurate.  So we can exclude these slow disks according to some rules when choosing volume. This will prevents some slow disks from affecting the throughput of the whole datanode.","closed","","tomscut","2021-12-06T14:52:28Z","2022-01-06T00:33:42Z"
"","3747","HDFS-16370. Fix assert message for BlockInfo","JIRA: [HDFS-16370](https://issues.apache.org/jira/browse/HDFS-16370).  In both methods BlockInfo#getPrevious and BlockInfo#getNext, the assert message is wrong. This may cause some misunderstanding and needs to be fixed.","closed","","tomscut","2021-12-03T11:45:57Z","2021-12-05T01:05:24Z"
"","3732","HDFS-16361. Fix log format for QueryCommand","JIRA: [HDFS-16361](https://issues.apache.org/jira/browse/HDFS-16361).  Fix log format for QueryCommand of disk balancer.","closed","","tomscut","2021-11-29T00:28:26Z","2021-12-01T07:54:08Z"
"","3731","HDFS-16359. RBF: RouterRpcServer#invokeAtAvailableNs does not take effect when retrying","JIRA: [HDFS-16359](https://issues.apache.org/jira/browse/HDFS-16359).  RouterRpcServer#invokeAtAvailableNs does not take effect when retrying. See [HDFS-15543](https://issues.apache.org/jira/browse/HDFS-15543).  The original code of RouterRpcServer#getNameSpaceInfo looks like follwings, and `namespaceInfos` is always empty here. ``` private Set getNameSpaceInfo(String nsId) {   Set namespaceInfos = new HashSet<>();   for (FederationNamespaceInfo ns : namespaceInfos) {     if (!nsId.equals(ns.getNameserviceId())) {       namespaceInfos.add(ns);     }   }   return namespaceInfos; }  ```","closed","","tomscut","2021-11-27T16:11:40Z","2021-12-02T13:40:53Z"
"","3714","HDFS-16352. return the real datanode numBlocks in #getDatanodeStorage…","JIRA: [HDFS-16352](https://issues.apache.org/jira/browse/HDFS-16352)  #getDatanodeStorageReport will return the array of DatanodeStorageReport which contains the DatanodeInfo in each DatanodeStorageReport, but the numBlocks in DatanodeInfo is always zero, which is confusing  ![image](https://user-images.githubusercontent.com/2844826/143049720-8822dd30-9cb0-4991-ab03-ae66a09e9692.png)   Or we can return the real numBlocks in DatanodeInfo when we call #getDatanodeStorageReport","closed","","liubingxing","2021-11-23T14:13:37Z","2021-12-17T05:31:29Z"
"","3695","HDFS-16344. Improve DirectoryScanner.Stats#toString","JIRA: [HDFS-16344](https://issues.apache.org/jira/browse/HDFS-16344).  Improve DirectoryScanner.Stats#toString.  ![scan](https://user-images.githubusercontent.com/55134131/142760163-e1d4704f-3c73-483d-9b28-bded8b423dfd.jpg)","closed","","tomscut","2021-11-21T11:36:38Z","2021-11-29T08:48:40Z"
"","3777","HDFS-16333. fix balancer bug when transfer an EC block","JIRA: [HDFS-16333](https://issues.apache.org/jira/browse/HDFS-16333)","closed","","liubingxing","2021-12-09T09:54:27Z","2021-12-10T03:16:48Z"
"","3716","HDFS-16327. Make dfs.namenode.max.slowpeer.collect.nodes reconfigurable","JIRA: [HDFS-16327](https://issues.apache.org/jira/browse/HDFS-16327).  As the HDFS cluster expands or shrinks, the number of slow nodes to be filtered must be dynamically adjusted. So we should make `DFS_NAMENODE_MAX_SLOWPEER_COLLECT_NODES_KEY` reconfigurable. This parameter was introduced in [HDFS-15879](https://issues.apache.org/jira/browse/HDFS-15879).","closed","","tomscut","2021-11-24T12:10:26Z","2021-12-14T02:43:20Z"
"","3721","HADOOP-18023. Allow cp command to run with multi threads.","JIRA:  https://issues.apache.org/jira/browse/HADOOP-18023  ### Description of PR  Allow hadoop fs -cp command to  run with multi-thread to improve copy speed.  ### How was this patch tested? It's useful to allow  -cp command to  run with multi-thread,  like the improvement we done for -put/-get commands.  It would reduce about 90% time cost when run with 10 threads in my test cases.  #### Source dir:  1 dir  401 files  2.3G  ##### Test 1: run with single thread ```   time hadoop fs -cp /tmp/data/test /tmp/data/t1   real    1m9.394s   user    0m16.688s   sys    0m5.331s ``` ##### Test 2: run with 10 threads ```   time hadoop fs -cp -t 10 /tmp/data/test /tmp/data/t2   real    0m8.217s   user    0m19.864s   sys    0m8.776s ```","closed","","smarthanwang","2021-11-25T06:45:31Z","2021-11-30T11:35:33Z"
"","4409","HDFS-16623. Avoid IllegalArgumentException in LifelineSender","Jira:  [HDFS-16623](https://issues.apache.org/jira/browse/HDFS-16623), fix bug to avoid IllegalArgumentException in LifelineSender.  In our production environment, an IllegalArgumentException occurred in the LifelineSender at one DataNode which was undergoing GC at that time.","closed","","ZanderXu","2022-06-06T12:45:06Z","2022-06-11T14:35:24Z"
"","4640","HADOOP-18370. Fix missing package-info in hadoop-common moudle.","JIRA. HADOOP-18370. Fix missing package-info in hadoop-common moudle.","open","","slfan1989","2022-07-27T07:04:13Z","2022-07-27T14:39:43Z"
"","4683","HADOOP-18361. Update commons-net from 3.6 to 3.8.0.","JIRA. HADOOP-18361. Update commons-net from 3.6 to 3.8.0.  Current version 3.6 is almost ~5 years old  Upgrading to new release to keep up for new features and bug fixes.","open","","slfan1989","2022-08-03T09:12:29Z","2022-08-03T09:12:29Z"
"","4619","HADOOP-18358. Update commons-math3 from 3.1.1 to 3.6.1.","JIRA. HADOOP-18358. Update commons-math3 from 3.1.1 to 3.6.1. I found that commons-math3 can be upgraded from 3.1.1 to 3.6.1. Try to upgrade, local compilation and verification are correct.","closed","","slfan1989","2022-07-24T05:25:29Z","2022-08-01T23:40:40Z"
"","4613","HADOOP-18356. Update jackson from 2.12.7 to 2.13.3.","JIRA. HADOOP-18356. Update jackson from 2.12.7 to 2.13.3.","closed","","slfan1989","2022-07-23T09:57:28Z","2022-07-24T07:38:59Z"
"","3909","HDFS-16431. Truncate CallerContext in client side.","JIRA https://issues.apache.org/jira/browse/HDFS-16431  ### Description of PR  The context of CallerContext would be truncated  when  it exceeds the maximum allowed length in server side. I think it's better to do check and truncate in client side to reduce the unnecessary overhead of network and memory for NN.  ### How was this patch tested?  Add some UTs to test the context would be truncated correctly .  ### For code changes:  1. Refactor CallerContext.Builder constrctor to simplify usage 2. Do context truncate before CallerContext.Builder.build()","open","","smarthanwang","2022-01-20T12:06:39Z","2022-04-08T07:34:50Z"
"","4286","YARN-11137. Slf4j log codeStyle fix","JIRA :[ YARN-11137-Improve log message in FederationClientInterceptor#submitApplication](https://issues.apache.org/jira/browse/YARN-11137)  Part of the logging code is adjusted to adapt to the SLF4J logging style.","closed","","slfan1989","2022-05-08T07:16:17Z","2022-05-09T11:07:08Z"
"","4594","YARN-6572. Refactoring Router services to use common util classes for pipeline creations.","Jira : YARN-6572. Refactoring Router services to use common util classes for pipeline creations.  When reading the code, I found that RouterClientRMService.java, RouterRMAdminService.java, RouterWebServices.java have a lot of repetitive code in the method of calling createRequestInterceptorChain, and I extracted the methods in RouterServerUtil.","open","","slfan1989","2022-07-20T05:15:42Z","2022-08-03T18:40:01Z"
"","3866","HADOOP-18071. ABFS: Set driver global timeout for ITestAzureBlobFileSystemBasics","ITestAzureBlobFileSystemBasics has a global timeout of 30s due to the timeout inherited from FileSystemContractBaseTest. All other ABFS driver tests have a timeout of 15min to account for retries. Setting a 15min timeout for this test will ensure sufficient time to allow retries and avoid transient failures.","closed","","sumangala-patki","2022-01-06T11:46:46Z","2022-02-23T19:38:39Z"
"","3901","Hadoop-18032: ABFS: Support for secondary accounts on ABFS Driver for SharedKey Auth","In this PR, we have added support for secondary accounts on ABFS Driver mainly for SharedKey Authorization Type. So we have created a separate field as primaryAccountName for getting the sharedKey credentials which is the same as accountName for primary accounts but we replace ""-secondary"" string from the accountName for the secondary accounts,","open","","anmolanmol1234","2022-01-18T11:19:21Z","2022-01-19T06:49:44Z"
"","4360","HDFS-16596. Improve the processing capability of FsDatasetAsyncDiskSe…","In our production environment, when DN needs to delete a large number blocks, we find that many deletion tasks are backlogged in the queue of threadPoolExecutor in FsDatasetAsyncDiskService. We can't improve its throughput because the number of core threads is hard coded.  So DN needs to support the number of core threads of FsDatasetAsyncDiskService can be configured.","open","","ZanderXu","2022-05-26T14:08:28Z","2022-07-20T16:18:42Z"
"","4567","HDFS-16663. Allow block reconstruction pending timeout refreshable to increase decommission performance","In [HDFS-16613](https://issues.apache.org/jira/browse/HDFS-16613), increase the value of dfs.namenode.replication.max-streams-hard-limit would maximize the IO performance of the decommissioning DN, which has a lot of EC blocks. Besides this, we also need to decrease the value of dfs.namenode.reconstruction.pending.timeout-sec, default is 5 minutes, to shorten the interval time for checking pendingReconstructions. Or the decommissioning node would be idle to wait for copy tasks in most of this 5 minutes.  In decommission progress, we may need to reconfigure these 2 parameters several times. In [HDFS-14560](https://issues.apache.org/jira/browse/HDFS-14560), the dfs.namenode.replication.max-streams-hard-limit can already be reconfigured dynamically without namenode restart. And the dfs.namenode.reconstruction.pending.timeout-sec parameter also need to be reconfigured dynamically.","open","","lfxy","2022-07-16T09:24:14Z","2022-07-25T01:04:17Z"
"","4179","HDFS-16544. EC decoding failed due to invalid buffer","In [HDFS-16538](https://issues.apache.org/jira/browse/HDFS-16538) , we found an EC file decoding bug if more than one data block read failed.   Currently, we found another bug trigger by #StatefulStripeReader.decode.  If we read an EC file which **length more than one stripe**, and this file have **one data block** and **the first parity block** corrupted, this error will happen.  ```java org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null     at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)     at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.(ByteBufferDecodingState.java:48)     at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)     at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)     at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:435)     at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)     at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:392)     at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:315)     at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:408)     at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:918)  ```  Let's say we use ec(6+3) and the data block[0] and the first parity block[6] are corrupted. 1. The readers for block[0] and block[6] will be closed after reading the first stripe of an EC file; 2. When the client reading the second stripe of the EC file, it will trigger #prepareParityChunk for block[6].  3. The decodeInputs[6] will not be constructed because the reader for block[6] was closed.  ```java boolean prepareParityChunk(int index) {   Preconditions.checkState(index >= dataBlkNum       && alignedStripe.chunks[index] == null);   if (readerInfos[index] != null && readerInfos[index].shouldSkip) {     alignedStripe.chunks[index] = new StripingChunk(StripingChunk.MISSING);     // we have failed the block reader before     return false;   }   final int parityIndex = index - dataBlkNum;   ByteBuffer buf = dfsStripedInputStream.getParityBuffer().duplicate();   buf.position(cellSize * parityIndex);   buf.limit(cellSize * parityIndex + (int) alignedStripe.range.spanInBlock);   decodeInputs[index] =       new ECChunk(buf.slice(), 0, (int) alignedStripe.range.spanInBlock);   alignedStripe.chunks[index] =       new StripingChunk(decodeInputs[index].getBuffer());   return true; }  ```","closed","","liubingxing","2022-04-15T09:31:26Z","2022-04-20T06:25:08Z"
"","4085","HDFS-16511. Improve lock type for ReplicaMap under fine-grain lock mode.","In [HDFS-16429](https://issues.apache.org/jira/browse/HDFS-16429) we make LightWeightResizableGSet to be thread safe, and  In [HDFS-15382](https://issues.apache.org/jira/browse/HDFS-15382) we have split lock to block pool grain locks.After these improvement, we can change some method to acquire read lock replace to acquire write lock. The lock now in ReplicaMap is protect  replicaInfo message.But now it have been protect by lock in LightWeightResizableGSet.","closed","","MingXiangLi","2022-03-19T12:54:34Z","2022-03-31T06:02:34Z"
"","3870","HADOOP-14334. S3 SSEC tests to downgrade when running against a mandatory encryption object store","If you run the S3A tests against a bucket with mandatory encryption, you need to set the encryption in auth keys. This breaks the SSEC tests because the encryption.key property being set breaks them.   This changes catch `AccessDeniedException` in the setup step of SSE tests, if it occurs then the test will be skipped. This exception is thrown by S3A when using encryption method that is not allowed by the bucket.  Tested in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify`  ``` [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   ITestS3AContractUnbuffer>AbstractContractUnbufferTest.testMultipleUnbuffers:100->AbstractContractUnbufferTest.validateFullFileContents:132->AbstractContractUnbufferTest.validateFileContents:139->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 failed to read expected number of bytes from stream. This may be transient expected:<1024> but was:<605> [ERROR]   ITestS3AInconsistency.testGetFileStatus:123->Assert.fail:89 getFileStatus should fail due to delayed visibility. [INFO]  [ERROR] Tests run: 1475, Failures: 2, Errors: 0, Skipped: 509 ```  Rerun failures individually ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.contract.s3a.ITestS3AContractUnbuffer [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.894 s - in org.apache.hadoop.fs.contract.s3a.ITestS3AContractUnbuffer [INFO]  [INFO] Results: [INFO]  [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0 ```  ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.s3a.ITestS3AInconsistency [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.108 s - in org.apache.hadoop.fs.s3a.ITestS3AInconsistency [INFO]  [INFO] Results: [INFO]  [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0 ```","closed","","monthonk","2022-01-06T17:13:40Z","2022-01-09T18:02:02Z"
"","3729","HDFS-16357. Fix log format in DFSUtilClient","if address is local ,there will be additional space in the log .we can improve it to look proper","closed","","GuoPhilipse","2021-11-26T11:38:16Z","2021-12-04T13:05:35Z"
"","3871","HDFS-16417. RBF: StaticRouterRpcFairnessPolicyController init fails with division by 0 if concurrent ns handler count is configured","If `dfs.federation.router.fairness.handler.count.concurrent` is configured, `unassignedNS` is thus empty and `handlerCount % unassignedNS.size()` will throw a /0 exception.  Changed it to assigning extra handlers to concurrent ns in case it's configured.","closed","","kokonguyen191","2022-01-07T02:41:17Z","2022-01-10T10:29:01Z"
"","4494","fix bug for MAPREDUCE-7392 bug for GzipCodec in native task","I found that inflateReset is not called after inflate return Z_STREAM_END.When Z_STREAM_END is returned after an inflate() method is called,the next inflate() call does not consume any input data and dost not produce any output data,thus the loop cannot exit(This bug is trigged by some special compressed data).So I fix the problem.","open","","cfg1234","2022-06-23T10:32:47Z","2022-07-11T01:51:32Z"
"","4666","HDFS-16708. RBF: Support transmit state id from client in router.","https://issues.apache.org/jira/browse/HDFS-16708","open","","zhengchenyu","2022-08-01T11:53:02Z","2022-08-01T21:18:06Z"
"","4061","HDFS-16500. Make asynchronous blocks deletion lock and unlock durtion threshold configurable.","https://issues.apache.org/jira/browse/HDFS-16500  I have backport the nice feature [HDFS-16043](https://issues.apache.org/jira/browse/HDFS-16043) to our internal branch, it works well in our testing cluster.  I think it's better to make the fields **_deleteBlockLockTimeMs_** and **_deleteBlockUnlockIntervalTimeMs_** configurable, so that we can control the lock and unlock duration.  ``` private final long deleteBlockLockTimeMs = 500; private final long deleteBlockUnlockIntervalTimeMs = 100; ``` And we should set the default value smaller to avoid blocking other requests long time when deleting some  large directories.","closed","","smarthanwang","2022-03-10T11:38:24Z","2022-04-22T01:59:33Z"
"","3726","HDFS-16356. JournalNode short name missmatch","https://issues.apache.org/jira/browse/HDFS-16356","closed","","FliegenKLATSCH","2021-11-25T20:20:50Z","2022-05-14T02:59:22Z"
"","3740","HDFS-16354. Add description of GETSNAPSHOTDIFFLISTING to WebHDFS doc.","https://issues.apache.org/jira/browse/HDFS-16354  [HDFS-16091](https://issues.apache.org/jira/browse/HDFS-16091) (#3374) added GETSNAPSHOTDIFFLISTING op leveraging ClientProtocol#getSnapshotDiffReportListing.","closed","","iwasakims","2021-12-01T03:10:22Z","2021-12-07T12:39:18Z"
"","4141","HDFS-16534. Split FsDatasetImpl from block pool locks to volume grain locks.","https://issues.apache.org/jira/browse/HDFS-15382 have split lock to block pool grain and do some prepare.This pr is the last part of volume lock.","closed","","MingXiangLi","2022-04-05T12:57:46Z","2022-04-17T11:23:34Z"
"","4652","HADOOP-18375. Fix failure of shelltest for hadoop_add_ldlibpath.","https://issues.apache.org/jira/browse/HADOOP-18375  The hadoop-functions_test_helper.bash assumes that the value of LD_LIBRARY_PATH is blank.   It is not true in the environment like [RHEL 8 with GCC 9](https://github.com/apache/hadoop/blob/06ac327e881a10f193b1fff14a1eaadf16e08e44/BUILDING.txt#L458-L460).  ``` $ grep LD_LIBRARY_PATH /opt/rh/gcc-toolset-9/enable export LD_LIBRARY_PATH=/opt/rh/gcc-toolset-9/root$rpmlibdir$rpmlibdir32${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} export LD_LIBRARY_PATH=/opt/rh/gcc-toolset-9/root$rpmlibdir$rpmlibdir32:/opt/rh/gcc-toolset-9/root$rpmlibdir/dyninst$rpmlibdir32/dyninst${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} ```  Resetting the value of LD_LIBRARY_PATH in the helper script should fix this. The update of environment variable in the tests does not affect outside.","open","","iwasakims","2022-07-28T07:33:21Z","2022-07-28T09:01:17Z"
"","4554","HADOOP-18334. Fix create-release to address removal of GPG_AGENT_INFO in branch-3.2.","https://issues.apache.org/jira/browse/HADOOP-18334  gpg v2.1 and above does not export GPG_AGENT_INFO. create-release script need to export the info by itself to make `--sign` work. It was addressed as part of [HADOOP-16797](https://issues.apache.org/jira/browse/HADOOP-16797) in branch-3.3 and trunk. Since we can not backport aarch64 support to branch-3.2, I filed this issue for branch-3.2 only.","closed","","iwasakims","2022-07-12T04:21:53Z","2022-07-12T11:05:42Z"
"","4548","Hadoop-18330","https://issues.apache.org/jira/browse/HADOOP-18330   ### Description of PR Added extra client attribute to the S3ClientCreation Parameters so that the full s3a path can be accessed!  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshpant","2022-07-11T16:18:53Z","2022-07-12T13:16:58Z"
"","4456","HADOOP-18299. Fix unexpected contents in hadoop-client-check-test-invariants on aarch64.","https://issues.apache.org/jira/browse/HADOOP-18299  Build on aarch64 failed due to banned classes. com.github.stephenc.findbugs:findbugs-annotations seems to be pulled somewhere.  ``` [ERROR] Found artifact with unexpected contents: '/home/ubuntu/srcs/bigtop/output/hadoop/hadoop-3.3.3/hadoop-client-modules/hadoop-client-minicluster/target/hadoop-client-minicluster-3.3.3.jar'     Please check the following and either correct the build or update     the allowed list with reasoning.      edu/     edu/umd/     edu/umd/cs/     edu/umd/cs/findbugs/     edu/umd/cs/findbugs/annotations/     edu/umd/cs/findbugs/annotations/NonNull.class     edu/umd/cs/findbugs/annotations/CheckForNull.class     edu/umd/cs/findbugs/annotations/DefaultAnnotationForFields.class ```  I can reproduce this only in branch-3.3 by the following cmdline.  ``` $ mvn -Pdist -Psrc -Dtar -Dzookeeper.version=3.5.9 -DskipTests -DskipTest -DskipITs install ```  Since the value of zookeeper.version in hadoop-project/pom.xml is 3.5.6, the issue looks only related to newer zookeeper 3.5.x. There is no issue in trunk in which the zookeeper.version is 3.6.3.","closed","","iwasakims","2022-06-18T03:41:27Z","2022-06-18T06:35:32Z"
"","4401","HADOOP-18276. Rename CHANGES to CHANGELOG in branch-2.10.","https://issues.apache.org/jira/browse/HADOOP-18276  The file names of changelog generated by releasedocmaker of Yetus was changed from CHANGES.md to CHANGELOG.md. Hyperlinks in the generated index.html of ""Changelog and Release Notes"" page were also affected. The CHANGES*.md generated in previous releases must be renamed to avoid 404.","closed","","iwasakims","2022-06-05T01:29:16Z","2022-06-06T23:32:12Z"
"","4376","HADOOP-18267. Fix failure of build of hadoop-hdfs-bkjournal in branch-2.10 without ptotoc in the PATH.","https://issues.apache.org/jira/browse/HADOOP-18267  This is the follow-up of [HADOOP-16598](https://issues.apache.org/jira/browse/HADOOP-16598) to leverage protobuf-maven-plugin for bkjournal code.","open","","iwasakims","2022-05-30T10:40:24Z","2022-06-02T13:35:19Z"
"","4344","HADOOP-18251. Fix failure of extracting JIRA id from commit message in git_jira_fix_version_check.py.","https://issues.apache.org/jira/browse/HADOOP-18251  git_jira_fix_version_check.py is confused by commit message like `""YARN-1151. Ability to configure auxiliary services from HDFS-based JAR files.""` which contains both `YARN-` and `HDFS-`. The latter `HDFS-` is unexpectedly picked as JIRA issue id then 404 is thrown on accessing invalid URL like ""https://issues.apache.org/jira/rest/api/2/issue/HDFS-"".  ``` Traceback (most recent call last):   File ""/home/centos/srcs/hadoop/dev-support/git-jira-validation/git_jira_fix_version_check.py"", line 87, in      issue = jira.issue(ACTUAL_PROJECT_JIRA + JIRA_NUM)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/client.py"", line 1404, in issue     issue.find(id, params=params)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/resources.py"", line 288, in find     self._load(url, params=params)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/resources.py"", line 458, in _load     r = self._session.get(url, headers=headers, params=params)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/resilientsession.py"", line 195, in get     return self.__verb(""GET"", str(url), **kwargs)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/resilientsession.py"", line 189, in __verb     raise_on_error(response, verb=verb, **kwargs)   File ""/home/centos/venv/lib64/python3.6/site-packages/jira/resilientsession.py"", line 70, in raise_on_error     **kwargs, jira.exceptions.JIRAError: JiraError HTTP 404 url: https://issues.apache.org/jira/rest/api/2/issue/HDFS-         text: Issue Does Not Exist ```  If commit message contains multiple match on 'HADOOP-|HDFS-|YARN-|MAPREDUCE-', choosing the first one should be the fix.","closed","","iwasakims","2022-05-23T10:17:48Z","2022-05-26T03:23:31Z"
"","4136","HADOOP-18192. Fix multiple_bindings warning about slf4j-reload4j.","https://issues.apache.org/jira/browse/HADOOP-18192  Jar of slf4j-relad4j exists in libdirs of both common and hdfs.  ``` $ ~/dist/hadoop-3.2.4-SNAPSHOT/bin/hdfs dfs -ls / SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/ext/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/ext/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-reload4j-1.7.35.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]   $ find ~/dist/hadoop-3.2.4-SNAPSHOT/ -name 'slf4j-reload4j-*.jar' /home/centos/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar /home/centos/dist/hadoop-3.2.4-SNAPSHOT/share/hadoop/hdfs/lib/slf4j-reload4j-1.7.35.jar ```  We need to update the part referring to slf4j-log4j12 in files under hadoop-assemblies too.","closed","","iwasakims","2022-04-04T16:51:24Z","2022-04-06T05:13:36Z"
"","4055","HADOOP-18158. Fix failure of create-release script due to releasedocmaker changes in branch-2.10","https://issues.apache.org/jira/browse/HADOOP-18158  The file name generated by releasedocmaker of Yetus was changed from CHANGES.md to CHANGELOG.md. dev-support scripts should be updated along with the change.","closed","","iwasakims","2022-03-09T05:34:17Z","2022-03-09T15:35:47Z"
"","3996","HADOOP-18129: Change URI to String in INodeLink to reduce memory footprint of ViewFileSystem","https://issues.apache.org/jira/browse/HADOOP-18129    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","abhishekdas99","2022-02-18T04:21:48Z","2022-03-18T00:26:57Z"
"","3953","HADOOP-18109. Ensure that default permissions of directories under internal ViewFS directories are the same as directories on target filesystems","https://issues.apache.org/jira/browse/HADOOP-18109  ### Description of PR - Ensure that default permissions of directories under internal ViewFS directories are the same as directories on target filesystems - Add new unit test  ### How was this patch tested? - mvn test -Dtest=TestViewFileSystem*  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cheyu2022","2022-02-01T18:47:21Z","2022-02-15T23:38:33Z"
"","3950","HADOOP-18100: Change scope of inner classes in InodeTree to make them accessible outside package","https://issues.apache.org/jira/browse/HADOOP-18100    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","abhishekdas99","2022-01-31T23:21:23Z","2022-02-04T00:28:34Z"
"","4151","HADOOP-18088. Replace log4j 1.x with reload4j.","https://issues.apache.org/jira/browse/HADOOP-18088  This is backport to branch-2.10 of HADOOP-18088.","closed","","iwasakims","2022-04-08T11:19:37Z","2022-04-14T00:33:25Z"
"","3801","HADOOP-18048. [branch-3.3] Dockerfile_aarch64 build fails with fatal error: Python.h: No such file or directory","https://issues.apache.org/jira/browse/HADOOP-18048  Explicitly include package `python3-dev` to ensure that `Python.h` would be in the header search path.","closed","","smengcl","2021-12-14T23:03:26Z","2021-12-15T01:57:13Z"
"","4424","HDFS-16628 RBF: Correct target directory when move to trash for kerberos login user.","HDFS-16628 RBF: kerbose user remove Non-default namespace data failed","closed","","zhangxiping1","2022-06-10T02:48:04Z","2022-06-15T13:17:19Z"
"","4154","HDFS-16533. COMPOSITE_CRC failed between replicated file and striped file","HDFS-16533. COMPOSITE_CRC failed between replicated file and striped file","closed","","ZanderXu","2022-04-09T00:44:40Z","2022-04-09T15:02:15Z"
"","3823","HDFS-16395. Remove useless NNThroughputBenchmark#dummyActionNoSynch().","HDFS-16395  ### Description of PR It looks like NNThroughputBenchmark#dummyActionNoSynch() has been deprecated and it should be deleted.  ### How was this patch tested? For testing, there is not much pressure.","closed","","jianghuazhu","2021-12-22T14:01:26Z","2021-12-24T08:35:25Z"
"","3724","HDFS-16355. Improve block scanner desc","HDFS-16355  datanode block scanner will be disabled if `dfs.block.scanner.volume.bytes.per.second` is configured less then or equal to zero, we can improve the desciption","closed","","GuoPhilipse","2021-11-25T12:57:34Z","2022-03-27T13:24:07Z"
"","4410","HDFS-16064. Determine when to invalidate corrupt replicas based on number of usable replicas","HDFS-16064. Determine when to invalidate corrupt replicas based on number of usable replicas  ### Description of PR  Bug fix for a re-occurring HDFS bug which can result in datanodes being unable to complete decommissioning indefinitely. In short, the bug is a chicken & egg problem where: - in order for a datanode to be decommissioned its blocks must be sufficiently replicated (which is not true if liveReplicas < ""dfs.replication"") - datanode cannot sufficiently replicate some block(s) because of corrupt block replicas on target datanodes (i.e. https://issues.apache.org/jira/browse/HDFS-16064) - corrupt block replicas will never be invalidated by the Namenode because the block(s) are not minimally replicated (because liveReplicas < ""dfs.namenode.replication.min"")  In this scenario, the block(s) are minimally replicated but the logic the Namenode uses to determine if a block is minimally replicated is flawed. Should be checking if usableReplicas < ""dfs.namenode.replication.min"" because decommissioning & entering maintenance nodes can have valid block replicas which should be replicated to the other datanodes with corrupt replicas if liveReplicas < ""dfs.replication"".  To understand the bug further we must first establish some background information.  #### Background Information  Givens: - FSDataOutputStream is being used to write the HDFS file, under the hood this uses a class DataStreamer - for the sake of example we will say the HDFS file has a replication factor of 2, though this is not a requirement to reproduce the issue - the file is being appended to intermittently over an extended period of time (in general, this issue needs minutes/hours  to reproduce because corrupt replicas need to be generated over time) - HDFS is configured with typical default configurations  Under certain scenarios the DataStreamer client can detect a bad link when trying to append to the block pipeline, in this case the DataStreamer client can shift the block pipeline by replacing the bad link with a new datanode. When this happens the replica on the datanode that was shifted away from becomes corrupted because it no longer has the latest generation stamp for the block. As a more concrete example: - DataStreamer client creates block pipeline on datanodes A & B, each have a block replica with generation stamp 1 - DataStreamer client tries to append the block pipeline by sending block transfer (with generation stamp 2) to datanode A - Datanode A succeeds in writing the block transfer & then attempts to forward the transfer to datanode B - Datanode B fails the transfer for some reason and responds with a PipelineAck failure code - Datanode A sends a PipelineAck to DataStreamer indicating datanode A succeeded in the append & datanode B failed in the append. The DataStreamer detects datanode B as a bad link which will be replaced before the next append operation - at this point datanode A has live replica with generation stamp 2 & datanode B has corrupt replica with generation stamp 1 - the next time DataStreamer tries to append the block it will call Namenode ""getAdditionalDatanode"" API which returns some other datanode C - DataStreamer sends data transfer (with generation stamp 3) to the new block pipeline containing datanodes A & C, the append succeeds to both datanodes - end state is that:   - datanodes A & C have live replicas with latest generation stamp 3   - datanode B has a corrupt replica because its lagging behind with generation stamp 1  The key behavior being highlighted here is that when the DataStreamer client shifts the block pipeline due to append failures on a subset of the datanode(s) in the pipeline, a corrupt block replica gets leftover on the datanode(s) that were shifted away from.  This corrupt block replica makes the datanode ineligible as a replication target for the block because of the following exception described in [https://issues.apache.org/jira/browse/HDFS-16064]:  ``` 2021-06-06 10:38:23,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode (DataXceiver for client  at /DN2:45654 [Receiving block BP-YYY:blk_XXX]): DN3:9866:DataXceiver error processing WRITE_BLOCK operation  src: /DN2:45654 dst: /DN3:9866; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-YYY:blk_XXX already exists in state FINALIZED and thus cannot be created. ```  What typically will occur is that these corrupt block replicas will be invalidated by the Namenode which will cause the corrupt replica to the be deleted on the datanode, thus allowing the datanode to once again be a replication target for the block. Note that the Namenode will not identify the corrupt block replica until the datanode sends its next block report, this can take up to 6 hours with the default block report interval.  As long as there is 1 live replica of the block, all the corrupt replicas should be invalidated based on this condition: https://github.com/apache/hadoop/blob/7bd7725532fd139d2e0e1662df7700f7ab95067a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L1928  When there are 0 live replicas, the corrupt replicas are not invalidated; I assume the reasoning behind this is that its better to have some corrupt copies of the block than to have no copies at all.  #### Description of Problem  The problem comes into play when the aforementioned behavior is coupled with decommissioning and/or entering maintenance.  Consider the following scenario: - block has replication factor of 2 - there are 3 datanodes A, B, & C - datanode A has decommissioning replica - datanodes B & C have corrupt replicas  This scenario is essentially a replication/decommissioning deadlock because: - corrupt replicas on B & C will not be invalidated because there are 0 live replicas (as per Namenode logic) - datanode A cannot finish decommissioning until the block is replicated to datanodes B & C - the block cannot be replicated to datanodes B & C until their corrupt replicas are invalidated  This does not need to be a deadlock, the corrupt replicas could be invalidated & the live replica could be transferred from A to B & C.  The same problem can exist on a larger scale, the requirements are that: - liveReplicas < ""dfs.namenode.replication.min"" = 1 by default - decommissioningAndEnteringMaintenanceReplicas > 0 - liveReplicas + decommissioningAndEnteringMaintenanceReplicas + corruptReplicas = numberOfDatanodes  In this case the corrupt replicas will not be invalidated by the Namenode which means that the decommissioning and entering maintenance replicas will never be sufficiently replicated and therefore will never finish decommissioning or entering maintenance.  The symptom of this issue in the logs is that right after a node with a corrupt replica sends its block report, rather than the block being invalidated it just gets counted as a corrupt replica:  ``` 2021-12-04 17:20:17,059 INFO BlockStateChange (DatanodeAdminMonitor-0): Block: blk_XYZ_123, Expected Replicas: 2, live replicas: 0, corrupt replicas: 0, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 10.0.0.1:15076 , Current Datanode: 10.0.0.1:15076, Is current datanode decommissioning: true, Is current datanode entering maintenance: false 2021-12-04 17:20:43,533 INFO BlockStateChange (Block report processor): BLOCK* processReport 0x456: from storage DS-XYZ node DatanodeRegistration(10.0.0.2:15076, ...), blocks: 57, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0 2021-12-04 17:20:47,059 INFO BlockStateChange (DatanodeAdminMonitor-0): Block: blk_XYZ_123, Expected Replicas: 2, live replicas: 0, corrupt replicas: 1, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 10.0.0.1:15076 10.0.0.2:15076 , Current Datanode: 10.0.0.1:15076, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... 2021-12-04 17:44:47,059 INFO BlockStateChange (DatanodeAdminMonitor-0): Block: blk_XYZ_123, Expected Replicas: 2, live replicas: 0, corrupt replicas: 1, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 10.0.0.1:15076 10.0.0.2:15076 , Current Datanode: 10.0.0.1:15076, Is current datanode decommissioning: true, Is current datanode entering maintenance: false 2021-12-04 17:44:47,293 INFO BlockStateChange (Block report processor): BLOCK* processReport 0x789: from storage DS-XYZ node DatanodeRegistration(10.0.0.3:15076, ...), blocks: 2014, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0 2021-12-04 17:45:17,059 INFO BlockStateChange (DatanodeAdminMonitor-0): Block: blk_XYZ_123, Expected Replicas: 2, live replicas: 0, corrupt replicas: 2, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 10.0.0.1:15076 10.0.0.2:15076 10.0.0.3:15076 , Current Datanode: 10.0.0.1:15076, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... ```  #### Proposed Solution  Rather than checking if ""dfs.namenode.replication.min"" is satisfied based on liveReplicas, check based on usableReplicas where ""usableReplicas = liveReplicas + decommissioningReplicas + enteringMaintenanceReplicas"". This will allow the corrupt replicas to be invalidated so that the live replica(s) on the decommissioning node(s) to be sufficiently replicated.  The only perceived risk here would be that the corrupt blocks are invalidated at around the same time the decommissioning and entering maintenance nodes are decommissioned. This could in theory bring the overall number of replicas below the ""dfs.namenode.replication.min"" (i.e. to 0 replicas in the worst case). This is however not an actual risk because the decommissioning and entering maintenance nodes will not finish decommissioning until their is a sufficient number of liveReplicas; so there is no possibility that the decommissioning and entering maintenance nodes will be decommissioned prematurely.  If the decommissioningReplicas are in fact corrupt, then because liveReplicas=0 there are no more uncorrupted replicas & the block cannot be recovered. So the additional corrupt replicas will be invalidated leaving only the decommissioning corrupt replicas which will contain the corrupted blocks data.  ### How was this patch tested?  Added a unit test ""testDeleteCorruptReplicaForUnderReplicatedBlock""  This test reproduces a scenario where: - there are 3 datanodes in the cluster - there is a blk_X with replication factor 2 - blk_X has:   - 2 corrupt replicas   - 1 decommissioning replica  The test then validates that:  - with the BlockManager changes the 2 corrupt replicas are invalidated which allows the decommissioning replica to be sufficiently replicated & the datanode to be decommissioned   - note that in a MiniDFSCluster when restartDatanode is used, the datanode comes up with a different port number  ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.hdfs.TestDecommission [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 46.006 s - in org.apache.hadoop.hdfs.TestDecommission [INFO]  [INFO] Results: [INFO]  [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 ```  ``` 2022-06-06 16:59:40,925 [Listener at localhost/64061] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2078)) - Block now has 2 corrupt replicas on [127.0.0.1:64049 , 127.0.0.1:64053] and 1 decommissioning replica on 127.0.0.1:64058 ... 2022-06-06 16:59:40,925 [Listener at localhost/64061] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2082)) - Restarting stopped nodes 127.0.0.1:64049 , 127.0.0.1:64053 ... 2022-06-06 16:59:42,466 [DataXceiver for client  at /127.0.0.1:64091 [Receiving block BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005]] INFO  datanode.DataNode (DataXceiver.java:run(308)) - 127.0.0.1:64087:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:64091 dst: /127.0.0.1:64087; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005 already exists in state FINALIZED and thus cannot be created. ... 2022-06-06 16:59:43,173 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(375)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 0, corrupt replicas: 0, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, replicas on stale nodes: 0, readonly replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:64058 , Current Datanode: 127.0.0.1:64058, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... 2022-06-06 16:59:54,490 [DataXceiver for client  at /127.0.0.1:64093 [Receiving block BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(935)) - Received BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005 src: /127.0.0.1:64093 dest: /127.0.0.1:64083 volume: /Users/wikak/apache/hadoop/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 of size 1536 2022-06-06 16:59:54,492 [DataXceiver for client  at /127.0.0.1:64094 [Receiving block BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(935)) - Received BP-73610852-192.168.2.16-1654549156791:blk_1073741825_1005 src: /127.0.0.1:64094 dest: /127.0.0.1:64087 volume: /Users/wikak/apache/hadoop/hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3 of size 1536 ... 2022-06-06 16:59:58,188 [DatanodeAdminMonitor-0] INFO  blockmanagement.DatanodeAdminManager (DatanodeAdminManager.java:setDecommissioned(301)) - Decommissioning complete for node 127.0.0.1:64058 ... 2022-06-06 17:00:01,098 [Listener at localhost/64090] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2147)) - Block now has 2 live replicas on [127.0.0.1:64083 , 127.0.0.1:64087] and 1 decommissioned replica on 127.0.0.1:64058 ```  - without the BlockManager changes the 2 corrupt replicas are never invalidated which prevents the decommissioning replica from being sufficiently replicated & prevents the datanode from even being decommissioned  ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.hdfs.TestDecommission [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 66.916 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestDecommission [ERROR] testDeleteCorruptReplicaForUnderReplicatedBlock(org.apache.hadoop.hdfs.TestDecommission)  Time elapsed: 66.771 s  <<< FAILURE! java.lang.AssertionError: Node 127.0.0.1:64157 failed to complete decommissioning. numTrackedNodes=1 , numPendingNodes=0 , adminState=Decommission In Progress , nodesWithReplica=[127.0.0.1:64157]     at org.junit.Assert.fail(Assert.java:89)     at org.apache.hadoop.hdfs.TestDecommission.testDeleteCorruptReplicaForUnderReplicatedBlock(TestDecommission.java:2132)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:498)     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)     at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)     at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)     at java.util.concurrent.FutureTask.run(FutureTask.java:266)     at java.lang.Thread.run(Thread.java:750)  [INFO]  [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestDecommission.testDeleteCorruptReplicaForUnderReplicatedBlock:2132 Node 127.0.0.1:64157 failed to complete decommissioning. numTrackedNodes=1 , numPendingNodes=0 , adminState=Decommission In Progress , nodesWithReplica=[127.0.0.1:64157] [INFO]  [ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0 ```  ``` 2022-06-06 17:03:05,174 [Listener at localhost/64165] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2078)) - Block now has 2 corrupt replicas on [127.0.0.1:64153 , 127.0.0.1:64162] and 1 decommissioning replica on 127.0.0.1:64157 ... 2022-06-06 17:03:05,174 [Listener at localhost/64165] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2082)) - Restarting stopped nodes 127.0.0.1:64153 , 127.0.0.1:64162 ... 2022-06-06 17:03:05,971 [RedundancyMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(489)) - Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]} 2022-06-06 17:03:06,977 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(375)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 0, corrupt replicas: 2, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, replicas on stale nodes: 0, readonly replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:64157 127.0.0.1:64183 127.0.0.1:64187 , Current Datanode: 127.0.0.1:64157, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... 2022-06-06 17:03:39,026 [RedundancyMonitor] WARN  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseTarget(489)) - Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]} 2022-06-06 17:03:42,007 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(375)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 0, corrupt replicas: 2, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, replicas on stale nodes: 0, readonly replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:64157 127.0.0.1:64183 127.0.0.1:64187 , Current Datanode: 127.0.0.1:64157, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... 2022-06-06 17:03:45,336 [Listener at localhost/64190] ERROR hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2131)) - Node 127.0.0.1:64157 failed to complete decommissioning. numTrackedNodes=1 , numPendingNodes=0 , adminState=Decommission In Progress , nodesWithReplica=[127.0.0.1:64157] ```  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-06-06T14:22:11Z","2022-06-20T02:20:45Z"
"","3899","HDFS-16430. Validate maximum blocks in EC group when adding an EC policy","HDFS EC adopts the last 4 bits of block ID to store the block index in EC block group. Therefore maximum blocks in EC block group is `2^4=16`, and which is defined here: `HdfsServerConstants#MAX_BLOCKS_IN_GROUP`.  Currently there is no limitation or warning when adding a bad EC policy with `numDataUnits + numParityUnits > 16`. It only results in read/write error on EC file with bad EC policy. To users this is not very straightforward.","closed","","cndaimin","2022-01-18T07:35:29Z","2022-01-24T06:34:44Z"
"","4104","HDFS-16520. Improve EC pread: avoid potential reading whole block","HDFS client 'pread' represents 'position read', this kind of read just need a range of data instead of reading the whole file/block. By using BlockReaderFactory#setLength, client tells datanode the block length to be read from disk and sent to client. To EC file, the block length to read is not well set, by default using 'block.getBlockSize() - offsetInBlock' to both pread and sread. Thus datanode read much more data and send to client, and abort when client closes connection. There is a lot waste of resource to this situation.","closed","","cndaimin","2022-03-24T07:04:20Z","2022-05-06T17:30:33Z"
"","4101","HDFS-16519. Add throttler to EC reconstruction","HDFS already have throttlers for data transfer(replication) and balancer, the throttlers reduce the impact of these background procedures to user read/write. We should add a throttler to EC background reconstruction too.","closed","","cndaimin","2022-03-24T03:51:06Z","2022-04-23T03:45:56Z"
"","4433","HADOOP-18289. Remove WhiteBox in hadoop-kms module.","HADOOP-18289. Remove WhiteBox in hadoop-kms module.  WhiteBox is deprecated, try to remove this method in hadoop-kms.","closed","","slfan1989","2022-06-12T10:42:39Z","2022-06-17T03:08:30Z"
"","3958","HADOOP-17198. Support S3 Access Points","HADOOP-17198. Support S3 Access Points (#3260)  Add support for S3 Access Points. This provides extra security as it ensures applications are not working with buckets belong to third parties.  To bind a bucket to an access point, set the access point (ap) ARN, which must be done for each specific bucket, using the pattern  fs.s3a.bucket.$BUCKET.accesspoint.arn = ARN  * The global/bucket option `fs.s3a.accesspoint.required` to mandate that buckets must declare their access point. * This is not compatible with S3Guard.  Consult the documentation for further details.  Contributed by Bogdan Stolojan  (this commit contains the changes to TestArnResource from HADOOP-18068,  ""upgrade AWS SDK to 1.12.132"" so that it works with the later SDK.)     ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-02-03T19:10:23Z","2022-02-04T18:40:32Z"
"","3966","[WIP] HADOOP-15980 : Enable TLS in RPC client/server","HADOOP-15980 : Enable TLS in RPC client/server  1 HADOOP-15980 : Enable TLS in RPC client/server ================================================    This pull request integrates the work done in the JIRAs for,    - HADOOP-15978 : Add Netty support to the RPC server   - HADOOP-15979 : Add Netty support to the RPC client    and then creates a prototype for enabling the SSL Handler over the   channel pipeline created in the above JIRAs. Specifically the   following work has been done,   1.1 HADOOP-15978 : Add Netty Support to the RPC Server ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    - Integrating the initial patch submitted by Daryn   - Integrating the patch for JAR shading provided by Wei-Chiu Chuang.   - Fixes for unit test failures   - Adding comments and Javadoc.   1.2 HADOOP-15979 : Add Netty support to the RPC client ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    - Integrating the initial patch submitted by Daryn.   - Addressing the initial comments given by Wei-Chiu Chuang on the      patch.   - Enabling the Netty Client flag in the unit tests.   1.3 HADOOP-15980 : Enable TLS in RPC client/server ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    - Post the above changes we added the SSLHandlers to the Server and      Client Channel pipelines and building the SSLContext from a      SelfSignedCertificate class.   - We had to change NettyOutputStream::write to use      Channel::writeAndFlush. We had to do this because the connection      context packet was not being flushed when a tez session was being      opened. The packet sent after the connection context packet was      being parsed as the header packet causing an error.   - We have a prototype internally using which Hive insert queries are      running successfully.   - There are unit test failures that we are working on fixing.   - I have discussed the changes with Akira offline. I highly respect      his opinion to share early versions of the patches and solicit      feedback on the changes continuously. Although the current patch      is not complete I am submitting it to solicit feedback. I will also keep      posting patches continuously.   - I am also taking Akira's advice and tagging @daryn-sharp and @jojochuang .","open","","vnhive","2022-02-08T20:14:36Z","2022-03-04T10:52:22Z"
"","3972","HADOOP-18118. Fix KMS Accept Queue Size default value to 500","From HADOOP-15638,`hadoop.http.socket.backlog.size` was set as 500 by default ,we can change code default value to keep consistent.","open","","GuoPhilipse","2022-02-09T11:53:05Z","2022-02-23T10:14:01Z"
"","4645","HADOOP-18344. Upgrade AWS SDK to 1.12.262","Fixes CVE-2018-7489 in shaded jackson.  +Add more commands in testing.md  to the CLI tests needed when qualifying  a release   ### Description of PR  #4637 on 3.3  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-07-27T16:13:56Z","2022-07-28T10:42:44Z"
"","4646","HADOOP-18344. Upgrade AWS SDK to 1.12.262","Fixes CVE-2018-7489 in shaded jackson.  +Add more commands in testing.md  to the CLI tests needed when qualifying  a release   ### Description of PR  #4637 / #4645 on branch-3.3.3  ### How was this patch tested?  tests in progress  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-07-27T16:18:23Z","2022-07-28T10:42:17Z"
"","3994","HADOOP-18110. ViewFileSystem: Add Support for Localized Trash Root","Fixes #3956  (cherry picked from commit ca8ba24051b7fca4612c9c182cb49f5183ce33ba)    ### Description of PR  Cherry-pick the patch from trunk to branch-2.10, with a few other changes, to make it work for branch-2.10.  ### How was this patch tested?  Manually run the new test cases from Intellij.","closed","","xinglin","2022-02-16T00:55:14Z","2022-02-18T19:23:18Z"
"","4090","HDFS-16516. Fix Fsshell wrong params","Fix wrong param name in FileSystemShell","closed","","GuoPhilipse","2022-03-21T13:12:11Z","2022-04-11T08:19:32Z"
"","3751","YARN-11032. Correct RMAppImpl log format","Fix Wrong format in RMAPPImpl and small fix to clean the code","open","","GuoPhilipse","2021-12-06T12:37:53Z","2021-12-07T14:49:14Z"
"","3783","HADOOP-18042. Fix jetty version in LICENSE-binary","Fix jetty version in LICENSE-binary,  related issue: https://github.com/apache/hadoop/pull/3700","closed","","luoyuan3471","2021-12-10T10:50:14Z","2021-12-13T01:46:01Z"
"","4172","Backport HDFS-16509 to branch branch-3.2","Fix cherry-pick conflicts.  Tested by `TestDecommission#testDecommissionWithUnknownBlock` and `mvn clean install -DskipTests`","closed","","cndaimin","2022-04-14T09:08:13Z","2022-04-18T02:40:13Z"
"","4557","HADOOP-18330 S3AFileSystem removes Path when calling createS3Client","First of all, sorry for the multiple PR's, it's because i cant push from my device because of security reasons and have to use https://github.dev/   ### Description of PR Added a new parameter object (pathUrl) that holds the full s3a path   ### How was this patch tested? - Ran all the tests successfully using `mvn clean compile package`. - Used the jar from above step to successfully read/write to an S3 bucket in us-east. Repeated this 3 times.   ### For code changes:  - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshpant","2022-07-12T21:48:55Z","2022-07-16T20:56:00Z"
"","3758","HADOOP-18036 : Backport HADOOP-17653 for branch-3.2","Do not use guava's Files.createTempDir()","open","","AnanyaSingh2121","2021-12-07T12:25:56Z","2021-12-22T05:50:42Z"
"","4369","HDFS-16601. Failed to replace a bad datanode on the existing pipeline…","Detail info please refer to [HDFS-16601](https://issues.apache.org/jira/browse/HDFS-16601).   Bug stack like: ``` java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:59687,DS-b803febc-7b22-4144-9b39-7bf521cdaa8d,DISK], DatanodeInfoWithStorage[127.0.0.1:59670,DS-0d652bc2-1784-430d-961f-750f80a290f1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:59670,DS-0d652bc2-1784-430d-961f-750f80a290f1,DISK], DatanodeInfoWithStorage[127.0.0.1:59687,DS-b803febc-7b22-4144-9b39-7bf521cdaa8d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration. 	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1418) 	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1478) 	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1704) 	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1605) 	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1587) 	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1371) 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:674) ```","open","","ZanderXu","2022-05-28T04:32:52Z","2022-07-28T12:23:43Z"
"","4367","HDFS-16600. Fix deadlock of fine-grain lock for FsDatastImpl of DataNode.","Detail info please refer to [HDFS-16600](https://issues.apache.org/jira/browse/HDFS-16600).  The UT org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaPlacement.testSynchronousEviction failed, because happened deadlock, which is introduced by [HDFS-16534](https://issues.apache.org/jira/browse/HDFS-16534).  ``` // org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.createRbw line 1588 need a read lock try (AutoCloseableLock lock = lockManager.readLock(LockLevel.BLOCK_POOl,         b.getBlockPoolId())) // org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.evictBlocks line 3526 need a write lock try (AutoCloseableLock lock = lockManager.writeLock(LockLevel.BLOCK_POOl, bpid)) ```","closed","","ZanderXu","2022-05-28T03:56:20Z","2022-06-18T09:24:22Z"
"","4366","HDFS-16598. Fix DataNode FsDatasetImpl lock issue without GS checks.","Detail info please refer [HDFS-16598](https://issues.apache.org/jira/browse/HDFS-16598).  All datanodes [DatanodeInfoWithStorage[127.0.0.1:57448,DS-1b5f7e33-a2bf-4edc-9122-a74c995a99f5,DISK]] are bad. Aborting...","closed","","ZanderXu","2022-05-28T01:29:37Z","2022-06-14T14:07:00Z"
"","3728","YARN-9063. ATS 1.5 fails to start if RollingLevelDb files are corrupt…","Description of PR Jira: https://issues.apache.org/jira/browse/YARN-9063 ATS 1.5 fails to start if RollingLevelDb files are corrupt or missing  How was this patch tested? Wrote a unit test case to check and manually tested it.","closed","","ashutoshcipher","2021-11-26T08:49:29Z","2021-12-06T10:45:55Z"
"","3725","YARN-9063. ATS 1.5 fails to start if RollingLevelDb files are corrupt or missing","Description of PR Jira: https://issues.apache.org/jira/browse/YARN-9063 ATS 1.5 fails to start if RollingLevelDb files are corrupt or missing  How was this patch tested? Wrote a unit test case to check and manually tested it.","closed","","ashutoshcipher","2021-11-25T14:20:35Z","2021-11-26T07:35:00Z"
"","3793","YARN-8234. Improve RM system metrics publisher's performance by pushi…","Description of PR Jira: https://issues.apache.org/jira/browse/YARN-8234 Improve RM system metrics publisher's performance by pushing events to timeline server in batch.  The patch is tested by applying it in-house clusters.","closed","","ashutoshcipher","2021-12-13T13:11:25Z","2021-12-23T08:17:18Z"
"","4185","HDFS-16632. modify block reader finish condition","Description of PR bytes_to_read_ is used to calculate all the data that client had received（include DATA and META length）. while currently we initialize it to data length，so the stop condition should be:  bytes_to_read_  < 0  JIRA: [HADOOP-18204](https://issues.apache.org/jira/browse/HADOOP-18204)","open","","chunie","2022-04-18T04:17:42Z","2022-06-16T04:24:19Z"
"","4018","YARN-10894. Follow up YARN-10237: fix the new test case in TestRMWebServicesCapacitySched.","Depends on: https://github.com/apache/hadoop/pull/3808  YARN-11033","closed","","tomicooler","2022-02-23T08:41:02Z","2022-03-02T17:20:07Z"
"","3755","YARN-11033. isAbsoluteResource is not correct for dynamically created queues.","Depends on YARN-11031.","closed","","tomicooler","2021-12-07T08:36:46Z","2021-12-10T14:10:38Z"
"","4167","HDFS-16538. EC decoding failed due to not enough valid inputs","Currently, we found this error if the #StripeReader.readStripe() have more than one block read failed. We use the EC policy ec(6+3) in our cluster.  ``` Caused by: org.apache.hadoop.HadoopIllegalArgumentException: No enough valid inputs are provided, not recoverable         at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkInputBuffers(ByteBufferDecodingState.java:119)         at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.(ByteBufferDecodingState.java:47)         at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)         at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)         at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:462)         at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)         at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:406)         at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:327)         at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:420)         at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:892)         at java.base/java.io.DataInputStream.read(DataInputStream.java:149)         at java.base/java.io.DataInputStream.read(DataInputStream.java:149)  ```  ```java while (!futures.isEmpty()) {   try {     StripingChunkReadResult r = StripedBlockUtil         .getNextCompletedStripedRead(service, futures, 0);     dfsStripedInputStream.updateReadStats(r.getReadStats());     DFSClient.LOG.debug(""Read task returned: {}, for stripe {}"",         r, alignedStripe);     StripingChunk returnedChunk = alignedStripe.chunks[r.index];     Preconditions.checkNotNull(returnedChunk);     Preconditions.checkState(returnedChunk.state == StripingChunk.PENDING);      if (r.state == StripingChunkReadResult.SUCCESSFUL) {       returnedChunk.state = StripingChunk.FETCHED;       alignedStripe.fetchedChunksNum++;       updateState4SuccessRead(r);       if (alignedStripe.fetchedChunksNum == dataBlkNum) {         clearFutures();         break;       }     } else {       returnedChunk.state = StripingChunk.MISSING;       // close the corresponding reader       dfsStripedInputStream.closeReader(readerInfos[r.index]);        final int missing = alignedStripe.missingChunksNum;       alignedStripe.missingChunksNum++;       checkMissingBlocks();        readDataForDecoding();       readParityChunks(alignedStripe.missingChunksNum - missing);     }  ```  This error can be trigger by #StatefulStripeReader.decode.  The reason is that: 1. If there are more than one **data block** read failed, the #readDataForDecoding will be called multiple times; 2. The **decodeInputs array** will be initialized repeatedly. 3. The **parity data** in **decodeInputs array** which filled by #readParityChunks previously will be set to null.","closed","","liubingxing","2022-04-13T09:13:13Z","2022-04-19T08:35:13Z"
"","4089","HDFS-16515. Improve ec exception message","Currently, if we set an erasure coding policy for a file,it only shows the following message, which is not that clear to user, we can improve it.  `RemoteException: Attempt to set an erasure coding policy for a file /ns-test1/test20211026/testec`","open","","GuoPhilipse","2022-03-21T12:35:06Z","2022-03-23T01:50:39Z"
"","4339","HDFS-16587. Allow configuring the number of JournalNodeRPCServer Hand…","Currently the number of JournalNodeRpc Handlers is hard-code 5, we need to support this parameter configurable.","closed","","ZanderXu","2022-05-22T00:11:26Z","2022-05-28T04:18:07Z"
"","4216","HADOOP-18269. Misleading method name in DistCpOptions.","Currently method in DistCpOptions withCRC was used as the following ``` withCRC(true) means check without crc withCRC(false) means check with crc ```  which mislead the developer when we pass the paramter, we can rename the method to clear that.after that it should be: ``` withSkipCRC(true) means check without crc withSkipCRC(false) means check with crc ```  so it will be more understandable.","closed","","GuoPhilipse","2022-04-22T01:25:19Z","2022-05-31T06:55:12Z"
"","4353","HDFS-16593. Correct inaccurate BlocksRemoved metric on DataNode side","Correct inaccurate BlocksRemoved metric on DataNode side.  When I read the code about `processCommandFromActive`, I guess there might be a bug.   ``` case DatanodeProtocol.DNA_INVALIDATE:    //    // Some local block(s) are obsolete and can be     // safely garbage-collected.    //    Block toDelete[] = bcmd.getBlocks();    try {       // using global fsdataset       dn.getFSDataset().invalidate(bcmd.getBlockPoolId(), toDelete);     } catch(IOException e) {       // Exceptions caught here are not expected to be disk-related.       throw e;     }     dn.metrics.incrBlocksRemoved(toDelete.length);     break; ```  `dn.metrics.incrBlocksRemoved(toDelete.length)` might be imprecise, because DN may not successfully remove some blocks in some abnormal case, such as some blocks have be deleted before.  So I think we should increase the metric after DN successfully deleting the block, like: ``` removing = volumeMap.remove(bpid, invalidBlks[i]); addDeletingBlock(bpid, removing.getBlockId()); LOG.debug(""Block file {} is to be deleted"", removing.getBlockURI()); datanode.getMetrics().incrBlocksRemoved(1); if (removing instanceof ReplicaInPipeline) {     ((ReplicaInPipeline) removing).releaseAllBytesReserved(); } ```","open","","ZanderXu","2022-05-25T02:55:10Z","2022-06-24T14:17:45Z"
"","4635","HADOOP-18354. Upgrade reload4j to 1.22.2 due to XXE vulnerability (#4607).","Contributed by PJ Fanning.","closed","","steveloughran","2022-07-26T19:46:41Z","2022-07-27T15:05:08Z"
"","4171","HADOOP-16965. Refactor abfs stream configuration. (#1956)","Contributed by Mukund Thakur.  (cherry picked from commit 8031c66295b530dcaae9e00d4f656330bc3b3952)    ### Description of PR It is an almost clean cherry pick of commit 8031c66295b530dcaae9e00d4f656330bc3b3952   ### How was this patch tested? Ran `mvn test -pl hadoop-tools/hadoop-azure` No new unit tests fail.   Ran all integration abfs tests using mvn -T 1C -Dparallel-tests=abfs clean verify with my storage account arjundev.dfs.core.windows.net No tests failed. There are 3 errors; these were negative test cases where error was expected. ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestAzureBlobFileSystemCreate.testFilterFSWriteAfterClose:182 » IO java.io.Fi... [ERROR]   ITestAzureBlobFileSystemE2E.testFlushWithFileNotFoundException:224 » IO java.i... [ERROR]   ITestAzureBlobFileSystemE2E.testWriteWithFileNotFoundException:204 » IO java.i... [INFO]  [ERROR] Tests run: 413, Failures: 0, Errors: 3, Skipped: 253 ```  Same three errors are found in the current branch-2.10 code as well.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","arjun4084346","2022-04-14T00:16:26Z","2022-05-03T17:06:52Z"
"","4017","YARN-11080. Improve debug log in ContainerTokenIdentifier when writin…","ContainerTokenIdentifier  debug information takes up too much CPU time, Just add debug level check.","open","","ChaosJu","2022-02-23T06:51:43Z","2022-02-23T08:28:03Z"
"","3897","HADOOP-17409. Remove s3guard from S3A module (#3534)","Completely removes S3Guard support from the S3A codebase.  If the connector is configured to use any metastore other than the null and local stores (i.e. DynamoDB is selected) the s3a client will raise an exception and refuse to initialize.  This is to ensure that there is no mix of S3Guard enabled and disabled deployments with the same configuration but different hadoop releases -it must be turned off completely.  The ""hadoop s3guard"" command has been retained -but the supported subcommands have been reduced to those which are not purely S3Guard related: ""bucket-info"" and ""uploads"".  This is major change in terms of the number of files changed; before cherry picking subsequent s3a patches into older releases, this patch will probably need backporting first.  Goodbye S3Guard, your work is done. Time to die.  Contributed by Steve Loughran.","closed","","steveloughran","2022-01-17T21:55:22Z","2022-01-18T18:52:30Z"
"","4495","HADOOP-18044. Hadoop - Upgrade to jQuery 3.6.0 (#3791)","Co-authored-by: luoyuan  (cherry picked from commit e2d620192aa0b712d05e4092eb63ef2ccdcd8220)","closed","","steveloughran","2022-06-23T11:12:10Z","2022-06-24T03:26:52Z"
"","4134","cherrypick hadoop 3.2.4 changes to branch 3.3.2","cherrypick all the changes in hadoop 3.2.4 on top of branch 3.3.2, so they are in sync.  This avoids us having a 3.2.x release which is ahead of the 3.3.x release line.  1. all the relevant patches were in branch-3.3. which is where I cherry picked them from 2. This excludes the move off log4j, which is yet to go in to 3.3  If we do a release of this, it should be as a separate chain of patches; this is the wrap up PoC  ### How was this patch tested?  waiting to see what yetus says   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-04T12:51:34Z","2022-04-19T11:59:42Z"
"","3882","HDFS-16043. Add markedDeleteBlockScrubberThread to delete blocks asyn…","cherry-pick to branch-3.3","closed","","zhuxiangyi","2022-01-13T08:49:18Z","2022-01-15T17:22:00Z"
"","4156","HDFS-16457.Make fs.getspaceused.classname reconfigurable (apache#4069)","Cherry-pick https://github.com/apache/hadoop/pull/4069 to branch-3.3.","closed","","singer-bin","2022-04-10T14:31:02Z","2022-04-11T06:39:18Z"
"","4031","HDFS-16371. Exclude slow disks when choosing volume (#3753)","Cherry-pick [#3753](https://github.com/apache/hadoop/pull/3753) to branch-3.3 .","closed","","tomscut","2022-02-25T05:03:29Z","2022-02-25T17:21:28Z"
"","4125","HDFS-16413. Reconfig dfs usage parameters for datanode (#3863)","Cherry-pick #3863 to branch-3.3.","closed","","tomscut","2022-03-31T01:21:02Z","2022-03-31T11:13:23Z"
"","4600","HADOOP-18333. Upgrade jetty version to 9.4.48.v20220622","Cherry picked #4553 to branch-3.3","open","","jojochuang","2022-07-20T16:27:33Z","2022-07-21T10:23:11Z"
"","3949","HADOOP-18114. Documentation Correction","Changed reference of s.s3a.assumed.role.credentials.provider to fs.s3a.assumed.role.credentials.provider","closed","","KraFusion","2022-01-31T23:05:07Z","2022-02-09T10:35:11Z"
"","3952","YARN-11071. AutoCreatedQueueTemplate incorrect wildcard level.","Change-Id: Iea0dd6b54d7e1e0d906928b768668a753900f81c    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-02-01T08:06:07Z","2022-02-17T18:19:33Z"
"","3759","YARN-11038. Fix testQueueSubmitWithACL* tests in TestAppManager.","Change-Id: I8a2e83468d211c32eba64742b6ba468517d38105    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-12-07T14:05:42Z","2021-12-08T15:42:11Z"
"","3752","YARN-11031. Improve the maintainability of RM webapp tests.","Change-Id: I6e39402dafc78e2a10bf0329c9e7619ffbbb9e1c    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-12-06T13:39:54Z","2021-12-08T15:05:14Z"
"","4508","YARN-11202. Optimize ClientRMService.getApplications.","Change-Id: I55ddb46fd0e4cdb644747d6d43083215f10861b5","closed","","tomicooler","2022-06-28T08:30:29Z","2022-07-01T09:02:07Z"
"","3895","YARN-11036. Do not inherit from TestRMWebServicesCapacitySched","Change-Id: I48e5b1e020d5cc6fb5ae4a7edc38e9212f4f069b    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-01-17T12:26:26Z","2022-03-10T13:51:59Z"
"","3894","YARN-11022. Fix the documentation for max-parallel-apps in CS","Change-Id: I29685db5cf298d9b25a0e80ed0b96be4f16d68c3    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2022-01-17T11:33:49Z","2022-02-17T18:37:51Z"
"","3865","HADOOP-18066: AbstractJavaKeyStoreProvider: need a way to read credential store password from Configuration","Change-Id: I272dd387ecb52eccd8035661cfe35edcdb29840c    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","abstractdog","2022-01-06T11:02:54Z","2022-01-10T14:54:01Z"
"","4106","HADOOP-18172: Changed scope for isRootInternalDir/getRootFallbackLink for InodeTree","Change scope for two functions. No tests is needed.","closed","","xinglin","2022-03-24T22:17:16Z","2022-04-20T05:47:03Z"
"","4532","Bump select2 from 4.0.0 to 4.0.6 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [select2](https://github.com/select2/select2) from 4.0.0 to 4.0.6.  Release notes Sourced from select2's releases.  Select2 4.0.6 New features/improvements  Add style property to package.json (#5019) Implement clear and clearing events (#5058) Add scrollAfterSelect option (#5150) Add missing diacritics (#4118, #4337, #5464)  Bug fixes  Fix up arrow error when there are no options in dropdown (#5127) Add ; before beginning of factory wrapper (#5089) Fix IE11 issue with select losing focus after selecting an item (#4860) Clear tooltip from select2-selection__rendered when selection is cleared (#4640, #4746) Fix keyboard not closing when closing dropdown on iOS 10 (#4680) User-defined types not normalized properly when passed in as data (#4632) Perform deep merge for Defaults.set() (#4364) Fix ""the results could not be loaded"" displaying during AJAX request (#4356) Cache objects in Utils.__cache instead of using $.data (#4346, #5486) Removing the double event binding registration of selection:update (#4306)  Accessibility  Improve .select2-hidden-accessible (#4908) Add role and aria-readonly attributes to single selection dropdown value (#4881)  Translations  Add Turkmen translations (tk) (#5125) Fix error in French translations (#5122) Add Albanian translation (sq) (#5199) Add Georgian translation (ka) (#5179) Add Nepali translation (ne) (#5295) Add Bangla translation (bn) (#5248) Add removeAllItems translation for clear ""x"" title (#5291) Fix wording in Vietnamese translations (#5387) Fix error in Russian translation (#5401)  Miscellaneous  Remove duplicate CSS selector in classic theme (#5115)  Select2 4.0.6-rc.1 Bug fixes  Fix up arrow error when there are no options in dropdown (#5127) Fix IE11 issue with select losing focus after selecting an item (#4860) Reinstate backwards-compatible support for data('select2') (#4014)  Translations  Add Turkmen translations (tk) (#5125) Fix error in French translations (#5122)  Miscellaneous  Remove duplicate CSS selector in classic theme (#5115)    ... (truncated)   Changelog Sourced from select2's changelog.  4.0.6 New features/improvements  Add style property to package.json (#5019) Implement clear and clearing events (#5058) Add scrollAfterSelect option (#5150) Add missing diacritics (#4118, #4337, #5464)  Bug fixes  Fix up arrow error when there are no options in dropdown (#5127) Add ; before beginning of factory wrapper (#5089) Fix IE11 issue with select losing focus after selecting an item (#4860) Clear tooltip from select2-selection__rendered when selection is cleared (#4640, #4746) Fix keyboard not closing when closing dropdown on iOS 10 (#4680) User-defined types not normalized properly when passed in as data (#4632) Perform deep merge for Defaults.set() (#4364) Fix ""the results could not be loaded"" displaying during AJAX request (#4356) Cache objects in Utils.__cache instead of using $.data (#4346, #5486) Removing the double event binding registration of selection:update (#4306)  Accessibility  Improve .select2-hidden-accessible (#4908) Add role and aria-readonly attributes to single selection dropdown value (#4881)  Translations  Add Turkmen translations (tk) (#5125) Fix error in French translations (#5122) Add Albanian translation (sq) (#5199) Add Georgian translation (ka) (#5179) Add Nepali translation (ne) (#5295) Add Bangla translation (bn) (#5248) Add removeAllItems translation for clear ""x"" title (#5291) Fix wording in Vietnamese translations (#5387) Fix error in Russian translation (#5401)  Miscellaneous  Remove duplicate CSS selector in classic theme (#5115)  4.0.5 Bug fixes  Replace autocapitalize=off with autocapitalize=none (#4994)  Translations  Vietnamese: remove an unnecessary quote mark (#5059) Czech: Add missing commas and periods (#5052) Spanish: Update the 'errorLoading' message (#5032) Fix typo in Romanian (#5005) Improve French translation (#4988) Add Pashto translation (ps) (#4960)    ... (truncated)   Commits  5dcc102 Merge pull request #5488 from select2/develop 3e9809d Update changelog for 4.0.6 release a2bfa6c Recompile dist for 4.0.6 release a8ea4cc Bump versions for 4.0.6 release b4aa352 Removed unused files 650035c Restore compatibility with data-* attributes in jQuery 2.x (#5486) 9f8b6ff [WIP] Get Grunt consistently working again (#5466) 5977856 minor fix (greek omega used has no diacritic) (#5464) a9c1b61 Update composer to remove deprecated dependency (#5165) 9032705 More suitable spelling ещё instead of еще (#5401) Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=select2&package-manager=npm_and_yarn&previous-version=4.0.0&new-version=4.0.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","javascript,","dependabot[bot]","2022-07-06T20:26:14Z","2022-07-06T21:47:22Z"
"","3706","HADOOP-18034. Bump mina-core from 2.0.16 to 2.1.5 in /hadoop-project","Bumps [mina-core](https://github.com/apache/mina) from 2.0.16 to 2.1.5.  Commits  bc9bb23 [maven-release-plugin] prepare release 2.1.5 dfd80f1 workout for failing unit test 3bca0bc Adds malformed HTTP request check 7dc266a Fixes HTTP pipeline processing issue d90bbb9 o Simplified the check for messages that can bypass encryption f6f9795 o Removed unused imports 01e0497 Adds hex dump length safety check 04d121f Merge branch '2.1.X' of http://gitbox.apache.org/repos/asf/mina into 2.1.X 5156438 Fixed the xbean plugin error in eclipse 6540022 HexDump improvements Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.apache.mina:mina-core&package-manager=maven&previous-version=2.0.16&new-version=2.1.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","java,","dependabot[bot]","2021-11-23T06:36:57Z","2021-12-08T18:39:05Z"
"","4475","Bump kotlin-stdlib from 1.4.10 to 1.6.0 in /hadoop-project","Bumps [kotlin-stdlib](https://github.com/JetBrains/kotlin) from 1.4.10 to 1.6.0.  Release notes Sourced from kotlin-stdlib's releases.  Kotlin 1.6.0 Changelog Android  KT-48019 Bundle Kotlin Tooling Metadata into apk artifacts KT-47733 JVM / IR: Android Synthetic don't generate _findCachedViewById function  Compiler New Features  KT-47984 In-place arguments inlining for @​InlineOnly functions KT-12794 Allow runtime retention repeatable annotations when compiling under Java 8 KT-43714 Support annotations on class type parameters (AnnotationTarget.TYPE_PARAMETER) KT-45949 Kotlin/Native: Improve bound check elimination KT-43919 Support loading Java annotations on base classes and implementing interfaces'  type arguments KT-48194 Try to resolve calls where we don't have enough type information, using the builder inference despite the presence of the annotation KT-47736 Support conversion from regular functional types to suspending ones in JVM IR KT-39055 Support property delegate created via synthetic method instead of field  Performance Improvements  KT-45185 FIR2IR: get rid of IrBuiltIns usages KT-47918 JVM / IR: Performance degradation with const-bound for-cycles KT-33835 Bytecode including unnecessary null checks for safe calls where left-hand side is non-nullable KT-41510 Compilation of kotlin html DSL is still too slow KT-48211 We spend a lot of time in ExpectActual declaration checker when there is very small amount of actual/expect declaration KT-39054 Optimize delegated properties which call get/set on the given KProperty instance on JVM KT-46615 Don't generate nullability assertions in methods for directly invoked lambdas  Fixes  KT-49613 JVM / IR: ""Exception during IR lowering"" with java fun interface and it's non-trivial usage KT-49548 ""ClassCastException: java.util.ArrayList$Itr cannot be cast to kotlin.collections.IntIterator"" with Iterable inside let KT-22562 Deprecate calls to ""suspend"" named functions with single dangling lambda argument KT-47120 JVM IR: NoClassDefFoundError when there are an extension and a regular function with the same name KT-49477 Has ran into recursion problem with two interdependant delegates KT-49442 ClassCastException on reporting [EXPOSED_FROM_PRIVATE_IN_FILE] Deprecation: private-in-file class should not expose 'private-in-class' KT-49371 JVM / IR: ""NoSuchMethodError"" with multiple inheritance KT-44843 PSI2IR: ""org.jetbrains.kotlin.psi2ir.generators.ErrorExpressionException: null: KtCallExpression"" with delegate who has name or parameter with the same name as a property KT-49294 Turning FlowCollector into 'fun interface' leads to AbstractMethodError KT-18282 Companion object referencing it's own method during construction compiles successfully but fails at runtime with VerifyError KT-25289 Prohibit access to class members in the super constructor call of its companion and nested object KT-32753 Prohibit @​JvmField on property in primary constructor that overrides interface property KT-43433 Suspend conversion is disabled message in cases where it is not supported and quickfix to update language version is suggested KT-49399 Building repeatable annotation with Container nested class fails with ISE: ""Repeatable annotation class should have a container generated"" KT-49209 Default upper bound for type variables should be non-null KT-49335 NPE in RepeatedAnnotationLowering.wrapAnnotationEntriesInContainer when using @Repeatable annotation from different file KT-48876 java.lang.UnsupportedOperationException: org.jetbrains.kotlin.ir.expressions.impl.IrReturnableBlockImpl@4a729df2    ... (truncated)   Changelog Sourced from kotlin-stdlib's changelog.  1.6.0 Android  KT-48019 Bundle Kotlin Tooling Metadata into apk artifacts KT-47733 JVM / IR: Android Synthetic don't generate _findCachedViewById function  Compiler New Features  KT-47984 In-place arguments inlining for @​InlineOnly functions KT-12794 Allow runtime retention repeatable annotations when compiling under Java 8 KT-43714 Support annotations on class type parameters (AnnotationTarget.TYPE_PARAMETER) KT-45949 Kotlin/Native: Improve bound check elimination KT-43919 Support loading Java annotations on base classes and implementing interfaces'  type arguments KT-48194 Try to resolve calls where we don't have enough type information, using the builder inference despite the presence of the annotation KT-47736 Support conversion from regular functional types to suspending ones in JVM IR KT-39055 Support property delegate created via synthetic method instead of field  Performance Improvements  KT-45185 FIR2IR: get rid of IrBuiltIns usages KT-47918 JVM / IR: Performance degradation with const-bound for-cycles KT-33835 Bytecode including unnecessary null checks for safe calls where left-hand side is non-nullable KT-41510 Compilation of kotlin html DSL is still too slow KT-48211 We spend a lot of time in ExpectActual declaration checker when there is very small amount of actual/expect declaration KT-39054 Optimize delegated properties which call get/set on the given KProperty instance on JVM KT-46615 Don't generate nullability assertions in methods for directly invoked lambdas  Fixes  KT-49613 JVM / IR: ""Exception during IR lowering"" with java fun interface and it's non-trivial usage KT-49548 ""ClassCastException: java.util.ArrayList$Itr cannot be cast to kotlin.collections.IntIterator"" with Iterable inside let KT-22562 Deprecate calls to ""suspend"" named functions with single dangling lambda argument KT-47120 JVM IR: NoClassDefFoundError when there are an extension and a regular function with the same name KT-49477 Has ran into recursion problem with two interdependant delegates KT-49442 ClassCastException on reporting [EXPOSED_FROM_PRIVATE_IN_FILE] Deprecation: private-in-file class should not expose 'private-in-class' KT-49371 JVM / IR: ""NoSuchMethodError"" with multiple inheritance KT-44843 PSI2IR: ""org.jetbrains.kotlin.psi2ir.generators.ErrorExpressionException: null: KtCallExpression"" with delegate who has name or parameter with the same name as a property KT-49294 Turning FlowCollector into 'fun interface' leads to AbstractMethodError KT-18282 Companion object referencing it's own method during construction compiles successfully but fails at runtime with VerifyError KT-25289 Prohibit access to class members in the super constructor call of its companion and nested object KT-32753 Prohibit @​JvmField on property in primary constructor that overrides interface property KT-43433 Suspend conversion is disabled message in cases where it is not supported and quickfix to update language version is suggested KT-49399 Building repeatable annotation with Container nested class fails with ISE: ""Repeatable annotation class should have a container generated"" KT-49209 Default upper bound for type variables should be non-null KT-49335 NPE in RepeatedAnnotationLowering.wrapAnnotationEntriesInContainer when using @Repeatable annotation from different file KT-48876 java.lang.UnsupportedOperationException: org.jetbrains.kotlin.ir.expressions.impl.IrReturnableBlockImpl@4a729df2 KT-48131 IAE ""Repeatable annotation container value must be a class reference"" on using Kotlin-repeatable annotation from dependency    ... (truncated)   Commits  829d1d8 Add changelog for 1.6.0 99b69ae Merge KT-MR-4942: Mark packages for relocation to fix classpath interferring ... 583488e [scripting] Fix NPE in aether.kt 0d1f362 Fix PureAndroidAndJavaConsumeMppLibIT working with test project 46af453 JVM KT-49613 don't generate indy reference to protected constructor d5275aa Mark packages for relocation to fix classpath interferring in main-kts a3820d4 JVM KT-49548 progression iterators can be tainted 63044b1 Update -Xjvm-default description e8e3c72 Update INTERFACE_CANT_CALL_DEFAULT_METHOD_VIA_SUPER message ddd02fe JvmDefault. Allow non default inheritance with special flag Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.jetbrains.kotlin:kotlin-stdlib&package-manager=maven&previous-version=1.4.10&new-version=1.6.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","java,","dependabot[bot]","2022-06-20T22:48:03Z","2022-06-21T00:36:40Z"
"","4535","Bump jetty-server from 9.4.44.v20210927 to 10.0.10 in /hadoop-project","Bumps [jetty-server](https://github.com/eclipse/jetty.project) from 9.4.44.v20210927 to 10.0.10.  Release notes Sourced from jetty-server's releases.  10.0.10 Special Thanks to the following Eclipse Jetty community members  @​jianglai (Lai Jiang) @​markslater (markslater) @​prenagha (Padraic Renaghan)  Changelog  #8136 - Cherry-pick of Improvements to PathSpec for Jetty 10.0.x #8134 - Improve cleanup of deflater/inflater pools for PerMessageDeflateExtension #8088 - Add option to configure exitVm on ShutdownMonitor from System properties #8067 - Wall time usage in DoSFilter RateTracker results in false positive alert #8057 - Support Http Response 103 (Early Hints) #8014 - Review HttpRequest URI construction #8008 - Add compliance mode for LEGACY multipart parser in Jetty 10+ #7994 - Ability to construct a detached client Request #7981 - Add TRANSFER_ENCODING violation for MultiPart RFC7578 parser. (#7976) #7977 - UpgradeHttpServletRequest.setAttribute & UpgradeHttpServletRequest.removeAttribute can throw NullPointerException #7975 - ForwardedRequestCustomizer setters do not clear existing handlers #7953 - Fix StatisticsHandler in the case a Handler throws exception. #7935 - Review HTTP/2 error handling #7929 - Correct requestlog formatString commented default (@​prenagha) #7924 - Fix a typo in Javadoc (@​jianglai) #7918 - PathMappings.asPathSpec does not allow root ServletPathSpec #7891 - Better Servlet PathMappings for Regex #7880 - DefaultServlet should not overwrite programmatically configured precompressed formats with defaults (@​markslater) #7863 - Default servlet drops first accept-encoding header if there is more than one. (@​markslater) #7858 - GZipHandler does not play nice with other handlers in HandlerCollection #7818 - Modifying of HTTP headers in HttpChannel.Listener#onResponseBegin is no longer possible with Jetty 10 #7808 - Jetty 10.0.x 7801 duplicate set session cookie #7802 - HTTP/3 QPACK - do not expect section ack for zero required insert count #7754 - jetty.sh ignores JAVA_OPTIONS environment variable #7748 - Allow overriding of url-pattern mapping in ServletContextHandler to allow for regex or uri-template matching #7635 - QPACK decoder should fail connection if the encoder blocks more than SETTINGS_QPACK_BLOCKED_STREAMS #4414 - GZipHandler not excluding inflation for specified paths #1771 - Add module for SecuredRedirect support  Dependencies  #8083 - Bump asciidoctorj to 2.5.4 #8077 - Bump asciidoctorj-diagram to 2.2.3 #7839 - Bump asm.version to 9.3 #8142 - Bump biz.aQute.bndlib to 6.3.1 #8075 - Bump checkstyle to 10.3 #8056 - Bump error_prone_annotations to 2.14.0 #8109 - Bump google-cloud-datastore to 2.7.0 #8100 - Bump grpc-core to 1.47.0 #7987 - Bump hawtio-default to 2.15.0    ... (truncated)   Commits  de73e94 Updating to version 10.0.10 1b4f941 RegexPathSpec documentation and MatchedPath improvements (#8163) 1f902f6 Disable H3 tests by default with a system property to explicitly enable them ... 7cc461b Fixing javadoc build errors (#8173) d63569d Migrate code from jetty-util Logger to slf4j Logger (#8162) 66de7ba Improve ssl buffers handling (#8165) 0699bc5 Use static exceptions for closing websocket flushers and in ContentProducer (... b1c19c0 Merge pull request #8134 from eclipse/jetty-10.0.x-websocketPermessageDeflate... 23948f1 no more profile IT tests runs per default (#8138) 0d13cbe change-dependabot-interval-to-monthly (#8140) Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=org.eclipse.jetty:jetty-server&package-manager=maven&previous-version=9.4.44.v20210927&new-version=10.0.10)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","java,","dependabot[bot]","2022-07-07T21:03:02Z","2022-07-20T16:16:43Z"
"","4146","Bump follow-redirects from 1.14.7 to 1.14.9 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.14.7 to 1.14.9.  Commits  13136e9 Release version 1.14.9 of the npm package. 2ec9b0b Keep headers when upgrading from HTTP to HTTPS. 5fc74dd Reduce nesting. 3d81dc3 Release version 1.14.8 of the npm package. 62e546a Drop confidential headers across schemes. See full diff in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.14.7&new-version=1.14.9)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","javascript,","dependabot[bot]","2022-04-07T07:20:52Z","2022-04-07T09:00:56Z"
"","3890","YARN-11065. Bump follow-redirects from 1.13.3 to 1.14.7 in hadoop-yarn-ui","Bumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.13.3 to 1.14.7.  Commits  2ede36d Release version 1.14.7 of the npm package. 8b347cb Drop Cookie header across domains. 6f5029a Release version 1.14.6 of the npm package. af706be Ignore null headers. d01ab7a Release version 1.14.5 of the npm package. 40052ea Make compatible with Node 17. 86f7572 Fix: clear internal timer on request abort to avoid leakage 2e1eaf0 Keep Authorization header on subdomain redirects. 2ad9e82 Carry over Host header on relative redirects (#172) 77e2a58 Release version 1.14.4 of the npm package. Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=follow-redirects&package-manager=npm_and_yarn&previous-version=1.13.3&new-version=1.14.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2022-01-17T07:33:19Z","2022-01-20T12:44:15Z"
"","4467","HDFS-16634. Dynamically adjust slow peer report size on JMX metrics","branch-3.3 backport PR of #4448","open","","virajjasani","2022-06-20T04:51:11Z","2022-06-22T04:14:03Z"
"","4259","HDFS-16521. DFS API to retrieve slow datanodes (#4107)","branch-3.3 backport PR of #4107","closed","","virajjasani","2022-05-03T01:18:05Z","2022-05-05T20:55:47Z"
"","4251","HDFS-16528. Reconfigure slow peer enable for Namenode (#4186)","branch-3.3 backport of PR https://github.com/apache/hadoop/pull/4186","closed","","virajjasani","2022-04-30T06:09:02Z","2022-05-02T00:03:02Z"
"","4084","HADOOP-18088. Replace log4j 1.x with reload4j.","backporting [HADOOP-18088](https://issues.apache.org/jira/browse/HADOOP-18088) to branch-3.2.","closed","","iwasakims","2022-03-18T14:34:19Z","2022-03-23T03:59:39Z"
"","4405","HDFS-16595. Slow peer metrics - add median, mad and upper latency limits","Backport PR from (#4357)  Reviewed-by: Tao Li  Signed-off-by: Wei-Chiu Chuang","closed","","virajjasani","2022-06-05T18:35:31Z","2022-06-06T22:41:16Z"
"","4692","HADOOP-18365. Update the remote address when a change is detected","Avoid reconnecting to the old address after detecting that the address has been updated.  ### Description of PR  When the IPC Client recognizes that an IP address has changed, it updates the server field and logs a message:  `Address change detected. Old: journalnode-1.journalnode.hdfs.svc.cluster.local/10.1.0.178:8485 New: journalnode-1.journalnode.hdfs.svc.cluster.local/10.1.0.182:8485`  Although the change is detected, the client will continue to connect to the old IP address, resulting in repeated log messages.  This is seen in managed environments when JournalNode syncing is enabled and a JournalNode is restarted, with the remaining nodes in the set repeatedly logging this message when syncing to the restarted JournalNode.  The source of the problem is that the remoteId.address is not updated.  ### How was this patch tested?  HA configuration deployed on a Kubernetes cluster, using Java 11.  1. Deleted an individual JournalNode pod 2. Looked for ""Address change detected"" messages from each other JournalNode 3. Looked for ""Address change detected"" messages from each NameNode (Active and Standby) 4. Waiting long enough for the `JounalNodeSyncer` to  wrap back around to the Restarted JournalNode to ensure that the message wasn't repeated and communications went directly to the new instance 5. Waited long enough for the NameNodes to push to the restarted JournalNode, restarting other JournalNodes if necessary as a forcing function, to ensure that the message wasn't repeated and communications went directly to the new instance  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","snmvaughan","2022-08-03T19:11:16Z","2022-08-03T19:11:16Z"
"","3885","HDFS-16214. Asynchronously collect blocks and update quota when deleting","Asynchronously collect blocks and update quota when deleting  [HDFS-16214](https://issues.apache.org/jira/browse/HDFS-16214)","open","","zhuxiangyi","2022-01-13T09:47:32Z","2022-03-14T12:53:04Z"
"","4202","HADOOP-18198. Add release 3.3.2 diff files","Adds the 3.3.2 jdiff files which were left out of the release commit...the ones the release doc didn't mention.  This is needed for the 3.3.3 build to complete.     ### How was this patch tested?  this is a working branch to sync between my dev machine and the machine running create-release; testing in progress.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-19T13:05:12Z","2022-04-20T20:11:46Z"
"","3930","HADOOP-18091. S3A auditing leaks memory through ThreadLocal references","Adds a new map type WeakReferenceMap, which stores weak references to values, and a WeakReferenceThreadMap subclass to more closely resemble a thread local type, as it is a map of thread id to value.  construct it with a factory method and optional callback for notification on loss and regeneration. ```  WeakReferenceThreadMap activeSpan =       new WeakReferenceThreadMap<>(           (k) -> getUnbondedSpan(),           this::noteSpanReferenceLost); ``` This is used in ActiveAuditManagerS3A for span tracking.  If a calling method has a reference to the span then even with gc the reference will be valid, it's only if no more references are held that there will be problems.  This does mean that if S3 a code keeps references around then back references to the auditor are retained.  but those classes which can get returned and which do have spans (list iterators, input and output streams, ...) And all have callbacks into the main S3a file system anyway.   Testing this has been fun; about as hard as the production code.  The good news: We can do this in a unit test relatively quickly. We just create a sequence of audit managers and in each one Schedule tasks across a thread pool to create spans. By providing an auditor implementation whose class and spans use lots of memory We can trigger OOM fast on the original code.  With the new structure this doesn't happen. Instead, and after 100+ iterations GC calls trigger removal of the only-weakly-referenced spans.  I have run the Itest suites against s3 london and all is well; retesting.  my system is set to fail if any operation is ever executed out of a span, other than those which happened during copy operations when the S3 SDK transfer manager invokes them.  That is the sole risk I can see in this world: that if and external reference is not held then the thread reference will be discarded prematurely.  I don't see it happening in this case but will review carefully just to make sure.   ### How was this patch tested?  New test which triggers oom on the old code, but works now.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-01-25T19:18:49Z","2022-02-10T12:31:41Z"
"","4506","YARN-11199. Replace htrace-core with hbase-noop-htrace","Addresses the following CVEs: - CVE-2018-7489 - CVE-2020-35491 - CVE-2020-35490 - CVE-2020-36518  ### Description of PR  Distributions of Hadoop still contain `htrace-core`, which is associated with 4 CVEs concerning FasterXML `jackson-databind`.  This can be addressed by replacing `htrace-core` with `hbase-noop-htrace` in Hadoop builds.  ### How was this patch tested?  The existence of `htrace-core` was verified in published distributions from the Apache Hadoop site, as well as by running a build against `trunk`.  Once the patch was applied `htrace-core` was replaced with `hbase-noop-htrace` in subsequent distribution builds, and all related tests passed as expected.  ### For code changes:  - [] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","snmvaughan","2022-06-27T20:08:23Z","2022-06-28T15:18:01Z"
"","4037","ABFS: Added changes for expect hundred continue header with append re…","Added expect hundred continue header with append requests.","closed","","anmolanmol1234","2022-02-28T09:02:18Z","2022-02-28T10:30:40Z"
"","3903","HADOOP-18084 ABFS: Add testfilePath while verifying test contents are read correctly","Add testfilePath as a param to get the file path for which the contents were not read correctly or the buffer size didn't match the expected one.","closed","","anmolanmol1234","2022-01-19T06:27:40Z","2022-01-19T10:13:13Z"
"","3713","HDFS-16351. add path exception information in FSNamesystem","add path information in exception message to make message more clear in FSNamesystem","closed","","GuoPhilipse","2021-11-23T13:44:09Z","2021-12-07T08:49:36Z"
"","4331","HADOOP-18242. ABFS Rename Failure when tracking metadata is in an incomplete state","ABFS rename fails intermittently when the Storage-blob tracking  metadata is in an incomplete state. This surfaces as the error code 404 and an error message of ""RenameDestinationParentPathNotFound""  To mitigate this issue, when a request fails with this response. the ABFS client issues a HEAD call on the source file and then retry the rename op.  ABFS filesystem statistics track when this occurs with new counters   rename_recovery   metadata_incomplete_rename_failures   rename_path_attempts  This is very rare occurrence and appears to be triggered under certain heavy load conditions, just as HADOOP-18163 is.  Contributed by Mehakmeet Singh.","closed","","mehakmeet","2022-05-19T06:28:03Z","2022-06-27T18:07:00Z"
"","4517","HADOOP-18242. ABFS Rename Failure when tracking metadata is in an incomplete state","ABFS rename fails intermittently when the Storage-blob tracking metadata is in an incomplete state. This surfaces as the error code 404 and an error message of ""RenameDestinationParentPathNotFound""  To mitigate this issue, when a request fails with this response. the ABFS client issues a HEAD call on the source file and then retries the rename operation again  ABFS filesystem statistics track when this occurs with new counters   rename_recovery   metadata_incomplete_rename_failures   rename_path_attempts  This is very rare occurrence and appears to be triggered under certain heavy load conditions, just as with HADOOP-18163.  Contributed by Mehakmeet Singh.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mehakmeet","2022-06-30T04:47:02Z","2022-07-01T20:19:15Z"
"","3876","HADOOP-18075. ABFS: Fix failure caused by listFiles() in ITestAbfsRestOperationException","ABFS driver test `testAbfsRestOperationExceptionFormat` in `ITestAbfsRestOperationException` fails due to the wrong exception format of the FileNotFound exception. The test invokes the Filesystem method listFiles(), and the exception thrown is found to be of the GetPathStatus format instead of ListStatus (difference in number of error fields in response).  The Filesystem implementation of listFiles() calls listLocatedStatus(), which then makes a listStatus call. A recent check-in that added implementation for listLocatedStatus() in ABFS driver led to a GetFileStatus request before ListStatus api are invoked, leading to the aberrant FNF exception format. The fix eliminates the GetPathStatus request before ListStatus is called.","closed","","sumangala-patki","2022-01-11T16:18:50Z","2022-03-01T13:52:06Z"
"","4577","HDFS-16668. Clean up moverExecutor after each iterations.","A quick fix to clean up moverExecutor after each iterations to avoid thread leaking. Testcases is working on.","open","","www49195","2022-07-18T07:50:51Z","2022-07-18T16:25:13Z"
"","4178","HDFS-16543. Keep default value of dfs.datanode.directoryscan.throttle.limit.ms.pe…","`WARN datanode.DirectoryScanner (DirectoryScanner.java:(300)) - dfs.datanode.directoryscan.throttle.limit.ms.per.sec set to value above 1000 ms/sec. Assuming default value of -1` A warning log like above will be printed when datanode is starting. The reason of that is the default value of `dfs.datanode.directoryscan.throttle.limit.ms.per.sec` is 1000 in hdfs-site.xml, and is -1 in `DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT`. We should try to keep it consistent, and value -1 looks is better to 1000.  The code segment that print the warning log: ``` if (throttle >= TimeUnit.SECONDS.toMillis(1)) {   LOG.warn(       ""{} set to value above 1000 ms/sec. Assuming default value of {}"",       DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY,       DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT);   throttle =       DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT; } ```","open","","cndaimin","2022-04-15T09:00:17Z","2022-05-19T04:14:56Z"
"","3723","HADOOP-18026. Fix default value of Magic committer","`fs.s3a.committer.magic.enabled` was set to true by default after HADOOP-17483, we can improve the doc","closed","","GuoPhilipse","2021-11-25T12:23:56Z","2021-12-11T13:25:06Z"
"","3703","HDFS-16347. Fix directory scan throttle default value","`dfs.datanode.directoryscan.throttle.limit.ms.per.sec` was changed from `1000` to `-1` by default after HDFS-13947, we can improve the doc","open","","GuoPhilipse","2021-11-22T12:41:40Z","2022-03-31T14:13:50Z"
"","4551","HADOOP-18330","]  ### Description of PR Added path to client creation parameters  ### How was this patch tested? I just have an Enterprise restricted device with me so could not clone repo for testing purposes! Used git dev for PR  ### For code changes:  - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshpant","2022-07-11T23:04:41Z","2022-07-12T21:49:32Z"
"","4420","HDFS-16626. Under replicated blocks in dfsadmin report should contain pendingReconstruction‘s blocks","[HDFS-16626](https://issues.apache.org/jira/browse/HDFS-16626) In the output of command 'hdfs dfsadmin -report', the value of Under replicated blocks and ec Low redundancy block groups only contains the block number in BlockManager::neededReconstruction. It should also contain the block number in BlockManager::pendingReconstruction, include the timeout items. Specially, in some scenario, for example, decommission a dn with a lot of ec blocks, there would be a lot blocks in pendingReconstruction at a long time but neededReconstruction's size may be 0. That will confuse user and they can't access the real decommissioning progress.  Configured Capacity: 1036741707829248 (942.91 TB) Present Capacity: 983872491622400 (894.83 TB) DFS Remaining: 974247450424426 (886.07 TB) DFS Used: 9625041197974 (8.75 TB) DFS Used%: 0.98% Replicated Blocks: Under replicated blocks: 0 Blocks with corrupt replicas: 0 Missing blocks: 0 Missing blocks (with replication factor 1): 0 Low redundancy blocks with highest priority to recover: 0 Pending deletion blocks: 0 Erasure Coded Block Groups: Low redundancy block groups: 3481 Block groups with corrupt internal blocks: 0 Missing block groups: 0 Low redundancy blocks with highest priority to recover: 0 Pending deletion blocks: 245","open","","lfxy","2022-06-09T05:44:54Z","2022-06-11T20:37:27Z"
"","4416","Under replicated blocks in dfsadmin report should contain pendingReconstruction‘s blocks","[HDFS-16626](https://issues.apache.org/jira/browse/HDFS-16626) In the output of command 'hdfs dfsadmin -report', the value of Under replicated blocks and ec Low redundancy block groups only contains the block number in BlockManager::neededReconstruction. It should also contain the block number in BlockManager::pendingReconstruction, include the timeout items. Specially, in some scenario, for example, decommission a dn with a lot of ec blocks, there would be a lot blocks in  pendingReconstruction at a long time but neededReconstruction's size may be 0. That will confuse user and they can't access the real decommissioning progress.  Configured Capacity: 1036741707829248 (942.91 TB) Present Capacity: 983872491622400 (894.83 TB) DFS Remaining: 974247450424426 (886.07 TB) DFS Used: 9625041197974 (8.75 TB) DFS Used%: 0.98% Replicated Blocks:     **Under replicated blocks: 0**     Blocks with corrupt replicas: 0     Missing blocks: 0     Missing blocks (with replication factor 1): 0     Low redundancy blocks with highest priority to recover: 0     Pending deletion blocks: 0 Erasure Coded Block Groups:     **Low redundancy block groups: 3481**     Block groups with corrupt internal blocks: 0     Missing block groups: 0     Low redundancy blocks with highest priority to recover: 0     Pending deletion blocks: 245","closed","","lfxy","2022-06-08T09:52:07Z","2022-06-10T02:17:53Z"
"","4391","HDFS-16613. EC: Improve performance of decommissioning dn with many ec blocks","[HDFS-16613](https://issues.apache.org/jira/browse/HDFS-16613)  In a hdfs cluster with a lot of EC blocks, decommission a dn is very slow. The reason is unlike replication blocks can be replicated from any dn which has the same block replication, the ec block have to be replicated from the decommissioning dn.  The configurations dfs.namenode.replication.max-streams and dfs.namenode.replication.max-streams-hard-limit will limit the replication speed, but increase these configurations will create risk to the whole cluster's network. So it should add a new configuration to limit the decommissioning dn, distinguished from the cluster wide max-streams limit.","closed","","lfxy","2022-05-31T16:30:36Z","2022-06-17T02:46:41Z"
"","4398","HDFS-16613. EC: Improve performance of decommissioning dn with many ec blocks","[HDFS-16613](https://issues.apache.org/jira/browse/HDFS-16613)  In a hdfs cluster with a lot of EC blocks, decommission a dn is very slow. The reason is unlike replication blocks can be replicated from any dn which has the same block replication, the ec block have to be replicated from the decommissioning dn. The configurations dfs.namenode.replication.max-streams and dfs.namenode.replication.max-streams-hard-limit will limit the replication speed, but increase these configurations will create risk to the whole cluster's network. So it should add a new configuration to limit the decommissioning dn, distinguished from the cluster wide max-streams limit.","closed","","lfxy","2022-06-03T15:26:05Z","2022-06-17T02:46:12Z"
"","4126","HDFS-16456. EC: Decommission a rack with only on dn will fail when the rack number is equal with replication","[HDFS-16456](https://issues.apache.org/jira/browse/HDFS-16456)  In below scenario, decommission will fail by TOO_MANY_NODES_ON_RACK reason: 1. Enable EC policy, such as RS-6-3-1024k. 2. The rack number in this cluster is equal with or less than the replication number(9) 3. A rack only has one DN, and decommission this DN. The root cause is in BlockPlacementPolicyRackFaultTolerant::getMaxNodesPerRack() function, it will give a limit parameter maxNodesPerRack for choose targets. In this scenario, the maxNodesPerRack is 1, which means each rack can only be chosen one datanode.  int maxNodesPerRack = (totalNumOfReplicas - 1) / numOfRacks + 1; here will be called, where totalNumOfReplicas=9 and  numOfRacks=9    When we decommission one dn which is only one node in its rack, the chooseOnce() in BlockPlacementPolicyRackFaultTolerant::chooseTargetInOrder() will throw NotEnoughReplicasException, but the exception will not be caught and fail to fallback to chooseEvenlyFromRemainingRacks() function.  When decommission, after choose targets, verifyBlockPlacement() function will return the total rack number contains the invalid rack, and BlockPlacementStatusDefault::isPlacementPolicySatisfied() will return false and it will also cause decommission fail. `  public boolean isPlacementPolicySatisfied() {     return requiredRacks <= currentRacks || currentRacks >= totalRacks;   }` According to the above description, we should make the below modify to fix it: 1. In startDecommission() or stopDecommission(), we should also change the numOfRacks in class NetworkTopology. Or choose targets may fail for the maxNodesPerRack is too small. And even choose targets success, isPlacementPolicySatisfied will also return false cause decommission fail. 2. In BlockPlacementPolicyRackFaultTolerant::chooseTargetInOrder(), the first chooseOnce() function should also be put in try..catch..., or it will not fallback to call chooseEvenlyFromRemainingRacks() when throw exception. 3. In verifyBlockPlacement, we need to remove invalid racks from total numOfRacks, or isPlacementPolicySatisfied() will return false and cause fail to reconstruct data.","closed","","lfxy","2022-03-31T04:33:21Z","2022-04-18T02:42:48Z"
"","4243","YARN-11117 check permission for LeveldbRMStateStore","YARN-11117 check permission for LeveldbRMStateStore ### Description of PR LeveldbRMStateStore use fs.mkdirs(root,new FsPermission((short)0700));  to create root directory with permission 700.BUT if umask is too strict such as 0777, this directory will have wrong permission with 000. So it should check if umask can affect permission, if true , use setPermission to fix it  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","skysiders","2022-04-28T04:37:48Z","2022-05-17T13:03:02Z"
"","4236","MAPREDUCE-7375 set permission for JobSubmissionFiles","MAPREDUCE-7375 set permission for JobSubmissionFiles ### Description of PR JobSubmissionFiles provide getStagingDir to get Staging Directory.If stagingArea missing, method will create new directory with this. fs.mkdirs(stagingArea, new FsPermission(JOB_DIR_PERMISSION)); It seems create new directory with JOB_DIR_PERMISSION,but this permission will be apply by umask.If umask too strict , this permission may be 000(if umask is 700).So we should change permission after create.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","skysiders","2022-04-27T05:07:54Z","2022-07-26T13:25:48Z"
"","4237","MAPREDUCE-7375 JobSubmissionFiles don't set right permission after mkdirs","MAPREDUCE-7375 JobSubmissionFiles don't set right permission after mkdirs ### Description of PR JobSubmissionFiles provide getStagingDir to get Staging Directory.If stagingArea missing, method will create new directory with this. fs.mkdirs(stagingArea, new FsPermission(JOB_DIR_PERMISSION)); It seems create new directory with JOB_DIR_PERMISSION,but this permission will be apply by umask.If umask too strict , this permission may be 000(if umask is 700).So we should change permission after create.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","skysiders","2022-04-27T09:25:51Z","2022-07-29T21:07:10Z"
"","4629","YARN-11231 modify destinationTmp permission from 755 to 777","### Description of PR YARN-11231 FSDownload calls createDir in the call method to create the destinationTmp directory, which is later used as the parent directory to create the directory dFinal, which is used in doAs to perform operations such as path creation and path traversal. doAs cannot determine the user's identity, so there is a problem with setting 755 permissions for destinationTmp here, I think it should be set to 777 permissions here.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","skysiders","2022-07-26T06:45:00Z","2022-07-26T08:40:03Z"
"","4250","YARN-11121:Check GetClusterMetrics Request parameter is null","### Description of PR YARN-11121 -  Conditional judgment Add in getClusterMetrics  https://issues.apache.org/jira/browse/YARN-11121  I added a judgment condition to ensure that when the request is empty, the metrics can be effectively counted   ### How was this patch tested? Added new unit test to check if request is empty  ### For code changes: The original code logic does not judge that the request is empty. In this case, add a judgment condition to ensure that when it is empty, it can be effectively processed","closed","","slfan1989","2022-04-30T01:56:36Z","2022-05-06T22:55:11Z"
"","3982","HDFS-16454:fix inconsistent comments in DataNode","### Description of PR While we fix    https://issues.apache.org/jira/browse/HDFS-7932, we change some value in code , but forget the value in comments.   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","lujiefsi","2022-02-11T09:10:40Z","2022-04-11T06:30:49Z"
"","4571","HDFS-16464. Create only libhdfspp static libraries for Windows","### Description of PR While building dynamic libraries (.dll) on Windows, there's a constraint that all the dependent libraries be linked dynamically. This poses an issue since Protobuf (which is an HDFS native client dependency) runs into build issues when linked dynamically. There are a few [warning notes](https://github.com/protocolbuffers/protobuf/blob/9ebb31726cef11e4e940b50ec751df4e863e3d2a/cmake/README.md#dlls-vs-static-linking) on the Protobuf repository's build instructions page as well.  Thus, to keep things simple, we can resort to do only static linking and thereby only produce statically linked libraries on Windows. In summary, we'll be providing only Hadoop .lib files initially. We can aim to produce Hadoop DLL on Windows eventually once we're able to resolve Protobuf's DLL issues.  ### How was this patch tested? Hadoop Jenkins CI validation ensures that the Linux components aren't affected. Verified that the static library is produced as a result of successful build on my local Windows system.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-16T18:21:33Z","2022-07-19T16:07:26Z"
"","4036","HADOOP-18145.Decompress the ZIP file and retain the original file per…","### Description of PR When we use the unzip method of Hadoop common to decompress, the UNIX permission cannot be reserved, so we use the ziparchiveentry of Apache common to preserve the permission information. When the file has UNIX permission, the unzip decompression method will preserve the permission first, and then set the original permission information to the new file.  ### How was this patch tested? Unit tests were conducted.","closed","","smallzhongfeng","2022-02-27T09:18:35Z","2022-03-30T13:52:48Z"
"","4053","HADOOP-18155. Refactor tests in TestFileUtil","### Description of PR We need to ensure that we check the results of file operations whenever we invoke mkdir, deleteFile etc. and assert them right there before proceeding on. Also, we need to ensure that some of the relevant FileSystem APIs don't return null.  ### How was this patch tested? Unit tests on CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","java,","GauthamBanasandra","2022-03-07T17:16:36Z","2022-03-10T16:32:45Z"
"","4625","HADOOP-18362. Solve ZKFailoverController throw ambiguous exception","### Description of PR We modified the Groups.java file, manually checked the cache validity time in the initialization function of this file, and then threw an exception at an earlier position. Because what is thrown in ZKFailoverController.java is the cause of the exception information, which is RuntimeException.getCause(). Also, the cache validity time is used when creating a cacheBuilder. However, the check information in this cacheBuilder class is imperfect, so it leads to imperfection of the thrown information.  ### How was this patch tested? we don't need more tests, because we just add some codes to check configuration.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","MEILIDEKCL","2022-07-25T11:25:03Z","2022-07-27T12:47:13Z"
"","4648","HADOOP-18362. Solve ZKFailoverController throw ambiguous exception","### Description of PR We modified the Groups.java file, manually checked the cache validity time in the initialization function of this file, and then threw an exception at an earlier position. Because what is thrown in ZKFailoverController.java is the cause of the exception information, which is RuntimeException.getCause(). Also, the cache validity time is used when creating a cacheBuilder. However, the check information in this cacheBuilder class is imperfect, so it leads to imperfection of the thrown information.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","MEILIDEKCL","2022-07-28T04:27:54Z","2022-08-01T13:22:36Z"
"","4643","HADOOP-18362. Solve ZKFailoverController throw ambiguous exception","### Description of PR We modified the Groups.java file, manually checked the cache validity time in the initialization function of this file, and then threw an exception at an earlier position. Because what is thrown in ZKFailoverController.java is the cause of the exception information, which is RuntimeException.getCause(). Also, the cache validity time is used when creating a cacheBuilder. However, the check information in this cacheBuilder class is imperfect, so it leads to imperfection of the thrown information.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","MEILIDEKCL","2022-07-27T12:51:33Z","2022-07-28T04:11:19Z"
"","4050","HADOOP-17526 Use Slf4jRequestLog for HttpRequestLog","### Description of PR Use Slf4jRequestLog in HttpRequestLog, so we do not need to depend on actual log appender implementations in code. For keeping the rolling way the safe, I have to use DailyRollingFileAppender but the DRFA in log4j1 does not support removing old files, so we loss the ability of 'RetainDays'. Anyway, later after we upgrade to log4j2, we can add this ability back.  ### How was this patch tested? Introduced a new UT. Better to deploy a hadoop cluster, enable request log and check the output.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Apache9","2022-03-06T16:14:13Z","2022-03-10T02:15:10Z"
"","3810","HDFS-16384. Upgrade Netty to 4.1.72.Final","### Description of PR Upgrade Netty to version 4.1.72.Final  ### How was this patch tested? Built locally, dependency tree generated, no conflict seen. Unit tests ran for affected components.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tamaashu","2021-12-16T18:23:34Z","2022-05-06T22:24:39Z"
"","4479","HADOOP-18309.Upgrade bundled Tomcat to 8.5.81","### Description of PR Upgrade bundled Tomcat to 8.5.81. Current version 8.5.75 is affected by CVE-2022-25762  More details - https://lists.apache.org/thread/qzkqh2819x6zsmj7vwdf14ng2fdgckw7  * JIRA: HADOOP-18309   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-21T10:47:58Z","2022-06-24T00:53:42Z"
"","3761","HDFS-15788. Correct the statement for pmem cache to reflect cache persistence support","### Description of PR Update document to reflect code changes we made previously.  ### How was this patch tested? N/A  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ N/A] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [N/A ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ N/A] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","PHILO-HE","2021-12-08T07:22:24Z","2021-12-08T12:28:28Z"
"","4501","YARN-11195. Adding document to enable numa","### Description of PR This pr is a documentation steps to enable numa for the cluster running on instances like m5.24x large which internally has 2 chip .  Relevent JIRA :- [YARN-11195](https://issues.apache.org/jira/browse/YARN-11195)  ### How was this patch tested? This working steps has been tested in EMR cluster with 1 master node and 5 core nodes of instance types (m5.24xlarge)  Allignment     ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-06-25T10:29:45Z","2022-06-28T12:39:57Z"
"","3705","HDFS-16322. Fix that ClientProtocol.truncate(...) can cause data loss.","### Description of PR This PR fix [HDFS-16322](https://issues.apache.org/jira/browse/HDFS-16322).  The NameNode implementation of ClientProtocol.truncate(...) can cause data loss. If dfsclient drops the first response of a truncate RPC call, the retry by retry cache will truncate the file again and cause data loss. Specifically, under concurrency, after the first execution of truncate(...), concurrent requests from other clients may append new data and change the file length. When truncate(...) is retried after that, it will truncate the file again, which causes data loss.  This patch utilized retry cache to avoid such data loss. When the truncate operation is applied for the first time, the status of this operation and the return value from server is recorded in retry cache. If this truncate is retried, server will directly read from retry cache and perform no operation that may cause non-idempotence.  See [HDFS-16322](https://issues.apache.org/jira/browse/HDFS-16322) description for more details.  ### How was this patch tested?  We added a new unit test for the idempotency of truncate operation under hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java. This test will issue a truncate operation on an existing file, remove the whole file and then issue a retry of previous operation. If the truncate is idempotent, the retry should return successfully and does not throw an exception saying it truncates on a non-existing file.","open","","Nsupyq","2021-11-22T17:28:38Z","2021-12-04T06:28:24Z"
"","4355","YARN-11163. use relative path to fix reverse proxy image display","### Description of PR this pr corresponds to [YARN-11163](https://issues.apache.org/jira/browse/YARN-11163)  ### How was this patch tested? this patch has unit test added and tried in local docker environment  ### For code changes:  - [x ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-05-25T10:44:25Z","2022-05-25T13:05:52Z"
"","4509","remove duplicated conf.set() in TestScriptBasedMapping","### Description of PR There is a duplicated `conf.set()` in `TestScriptBasedMapping#testNoArgsMeansNoResult`. This PR removes the duplicated line.  ### How was this patch tested? Rerun `TestScriptBasedMapping#testNoArgsMeansNoResult` and pass without any failure/error.","open","","shuaiwang516","2022-06-28T18:16:13Z","2022-06-30T16:41:29Z"
"","4616","HADOOP-18357. Retarget solution file to VS2019","### Description of PR The Visual Studio version used by `winutils` and `native` components in Hadoop common are quite old. We need to retarget the solution and vcxproj files to use the latest version (Visual Studio 2019 as of this writing).  With this PR, we can finally build Hadoop on Windows without any hacks.  ## Hadoop tar ![image](https://user-images.githubusercontent.com/10280768/180619082-d0544f46-dc23-4585-a1b1-0c6a3a1609b5.png)  ## Hadoop distribution ![image](https://user-images.githubusercontent.com/10280768/180619024-956805b6-3eaa-427b-bc60-db6d5f2fca1f.png)  ## Hadoop native libraries ![image](https://user-images.githubusercontent.com/10280768/180619073-a54a568d-3e60-4d79-97e2-b5e646de13f7.png)  ### How was this patch tested? 1. Tested by building locally on my Windows 10 PC. 2. Hadoop Jenkins CI validation.   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-07-23T18:54:52Z","2022-07-28T12:08:31Z"
"","4615","HDFS-16681. Do not pass GCC flags for MSVC in libhdfspp","### Description of PR The tests in HDFS native client uses the `-Wno-missing-field-initializers` flag to ignore warnings about uninitialized members - https://github.com/apache/hadoop/blob/8f83d9f56d775c73af6e3fa1d6a9aa3e64eebc37/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tests/CMakeLists.txt#L27-L28   This leads to the following error on Visual C++. ``` [exec] ""E:\hadoop-hdfs-project\hadoop-hdfs-native-client\target\native\ALL_BUILD.vcxproj"" (default target) (1) -> [exec] ""E:\hadoop-hdfs-project\hadoop-hdfs-native-client\target\native\main\native\libhdfspp\tests\x-platform\x_platform_dirent_test_obj.vcxproj"" (default target) (24) -> [exec]   cl : command line error D8021: invalid numeric argument '/Wno-missing-field-initializers' [E:\hadoop-hdfs-project\hadoop-hdfs-native-client\target\native\main\native\libhdfspp\tests\x-platform\x_platform_dirent_test_obj.vcxproj] ```  Thus, we need to pass this flag only when the compiler isn't Visual C++.  ### How was this patch tested? 1. Tested by building locally on my Windows 10 PC. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-23T18:36:57Z","2022-07-25T17:25:41Z"
"","4076","HDFS-16470. Make HDFS find tool cross platform","### Description of PR The source files for hdfs_find uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make these tools cross platform.   ### How was this patch tested? ## hdfs_find I ran hdfs_find with root, it does a traversal of the whole filesystem from the root and displays the same. ```bash $ ./hdfs_find / ``` ![Find root](https://user-images.githubusercontent.com/10280768/158857624-16312155-874c-4a67-92ff-2db8b515200f.jpg)   I then ran the same command with the -m option which limits the depth of the traversal. ```bash $ ./hdfs_find / -m 2 ``` ![Find with max depth](https://user-images.githubusercontent.com/10280768/158858449-07fa502c-68a5-4342-84b0-f20b286bfde0.jpg)   I ran the tool with the -n option, specifying the pattern of the file to find. ```bash $ ./hdfs_find / -n *.txt ``` ![Find with pattern](https://user-images.githubusercontent.com/10280768/158857961-34dffa2b-b60b-48f8-bc3b-f4ec8a695357.jpg)   I then ran the tool with both the options - limiting the traversal depth and by the pattern to find. ```bash $ ./hdfs_find / -m 2 -n *.txt ``` ![Find with max depth and pattern](https://user-images.githubusercontent.com/10280768/158858747-db2fe0fd-10c8-4f55-967b-4b340cee03f5.jpg)  ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-03-16T18:15:28Z","2022-03-18T16:11:05Z"
"","3848","HDFS-16407. Make hdfs_du tool cross platform","### Description of PR The source files for hdfs_du uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make this cross platform.  ### How was this patch tested? I ran the `hdfs_du` tool by exercising the supported options - ```bash $ ./hdfs_du / ``` ![image](https://user-images.githubusercontent.com/10280768/147869414-a049d70a-91ca-442d-815e-5ea37b104c9b.png)  ```bash $ ./hdfs_du -R / ``` ![image](https://user-images.githubusercontent.com/10280768/147869427-a9b0486e-96e0-4a7b-aafe-930c1c03ce4f.png)  ### Confirmation from UI The home directory and its subdirectory contain a file of size 5 bytes each. We can correlate this with the above output for verification.  #### Home directory ![image](https://user-images.githubusercontent.com/10280768/147869564-5bcd0087-7d4b-48f2-8771-3a0018e7bef9.png)  #### Subdirectory of home ![image](https://user-images.githubusercontent.com/10280768/147869537-774db59d-6bf2-4510-9fb8-0b13c295402a.png)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-01-02T07:33:08Z","2022-01-04T17:00:18Z"
"","3873","HDFS-16419. Make HDFS data transfer tools cross platform","### Description of PR The source files for hdfs_copyToLocal and hdfs_moveToLocal uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make these tools cross platform.  ### How was this patch tested? Both of the tools were tested by running locally as follows - ## hdfs_copyToLocal ```bash $ ./hdfs_copyToLocal ~/data.txt /tmp/data.txt ```  ![Command](https://user-images.githubusercontent.com/10280768/148806803-764ac13e-c8e4-4e1e-90a9-5328c4b13bc5.jpg)  ## hdfs_moveToLocal ```bash $ ./hdfs_moveToLocal ~/data.txt /tmp/data.txt ```  ![Command](https://user-images.githubusercontent.com/10280768/148806945-4cadb3b6-117c-41a9-ba22-4d7f2ca76536.jpg)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-01-09T17:54:26Z","2022-01-12T14:27:33Z"
"","4157","HDFS-16474. Make HDFS tail tool cross platform","### Description of PR The source files for `hdfs_tail` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make these tools cross platform.   ### How was this patch tested? # hdfs_tail  I tested this on a Hadoop cluster setup locally in pseudo-distributed mode for various scenarios. I ran `hdfs_tail` on a file whose size was more than 1 KB.  ```bash $ ./hdfs_tail ~/some-dir/Lorem-ipsum.txt ```  ![HDFS tail on lorem ipsum](https://user-images.githubusercontent.com/10280768/162765303-27874fac-89c1-473a-9f98-a6b99c43ae74.jpg)  We note that it correctly displays the last 1 KB of data as verified with piping to `wc`.  ![HDFS tail on lorem ipsum with wc](https://user-images.githubusercontent.com/10280768/162766296-06754f4b-2977-4708-9221-429d019ee8c2.jpg)  I then ran this tool with the `follow` option.  ```bash $ ./hdfs_tail -f ~/some-dir/Lorem-ipsum.txt ```  It again displayed the same last 1 KB of the data in the file and I had to press `Ctrl + C` to exit the program.  ![HDFS tail with follow option on lorem ipsum](https://user-images.githubusercontent.com/10280768/162767087-53d87d5f-f846-4c2d-b199-a092f9e5bdef.jpg)  I also tested it by giving a non-existent path.  ```bash $ ./hdfs_tail ~/some-non-existent-path ```  ![HDFS tail on a non-existent path](https://user-images.githubusercontent.com/10280768/162767242-71d796bc-2bf8-46dc-92cd-780f724646eb.jpg)  We note that it provides an appropriate error message and exits.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-04-10T15:09:33Z","2022-04-12T15:01:31Z"
"","4145","HDFS-16473. Make HDFS stat tool cross platform","### Description of PR The source files for `hdfs_stat` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make this tool cross platform.  ### How was this patch tested?  # hdfs_stat This tool was testing locally with the Hadoop cluster setup in pseudo-distributed mode. I ran the tool by providing the path to a file as the argument.  ```bash $ ./hdfs_stat ~/some-dir/some-data.txt ```  ![Stat on a file](https://user-images.githubusercontent.com/10280768/162121024-9a2d8f96-d34d-4636-8ace-70f00e3152f9.jpg)  I verified the output against the information displayed in the UI.  ![Confirmation from UI - stat on a file](https://user-images.githubusercontent.com/10280768/162121126-c4133834-0f45-4b25-be78-ee2c8892eb84.jpg)   I then ran the tool by providing the path to a directory as the argument.  ```bash $ ./hdfs_stat ~/some-dir ```  ![Stat on a directory](https://user-images.githubusercontent.com/10280768/162121241-8157480e-8f8e-4fd7-af47-503bd2e9e866.jpg)  I verified the output against the information displayed in the UI.  ![Confirmation from UI - stat on a directory](https://user-images.githubusercontent.com/10280768/162121280-d72b72ee-99f0-4167-8cb3-ea3543c60194.jpg)  I then ran the tool by providing a non-existent path as the argument.  ```bash $ ./hdfs_stat ~/some-non-existent-path ```  ![Stat on a non-existent path](https://user-images.githubusercontent.com/10280768/162121415-66ffcd06-1117-43b3-a763-2bdcb9ba460f.jpg)  We note that an appropriate error message is reported in the above output.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-04-07T04:40:44Z","2022-04-08T17:36:42Z"
"","4130","HDFS-16472. Make HDFS setrep tool cross platform","### Description of PR The source files for `hdfs_setrep` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make this tool cross platform.  ### How was this patch tested? # hdfs_setrep This tool was tested by running against a Hadoop cluster setup in pseduo-distributed mode. The details are as follows - There's a file initially in the HDFS path `~/some-dir/some-data.txt` that has an initial replication factor of 1.  ![Initial](https://user-images.githubusercontent.com/10280768/161321159-fec83b22-3aa7-4c9d-9593-bab0ae3ce95a.jpg)   We change it to 4 by running the following command. ```bash $ ./hdfs_setrep 4 ~/some-dir/some-data.txt ``` ![Set replication factor of 4](https://user-images.githubusercontent.com/10280768/161321196-b42c79df-f6fa-418d-aa58-688f37ae3cc9.jpg)  Confirmation from UI. ![After setting replication factor of 4](https://user-images.githubusercontent.com/10280768/161321236-5122e58a-9143-45de-b805-a18df3636a11.jpg)  If the path provided to `hdfs_setrep` happens to be a directory, it recursively sets the replication factor to all its children. Here, we set the replication factor to 6 from the root. ```bash $ ./hdfs_setrep 6 / ``` ![Set replication factor 6 recursively](https://user-images.githubusercontent.com/10280768/161321444-e75cbee5-a846-41cb-b4f8-b1a0c4158e98.jpg)  Confirmation from UI. ![After setting replication factor of 6 recursively](https://user-images.githubusercontent.com/10280768/161321484-c1b1ec6e-83f5-4b21-98e6-0cff7d6864fe.jpg)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-04-01T18:22:45Z","2022-04-05T17:29:19Z"
"","4086","HDFS-16471. Make HDFS ls tool cross platform","### Description of PR The source files for `hdfs_ls` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make this tool cross platform.  ### How was this patch tested? ## hdfs_ls I ran this tool locally with root as the parameter, where it lists the contents of the root directory - ```bash ./hdfs_ls / ``` ![Ls on root](https://user-images.githubusercontent.com/10280768/159152685-59f16900-231b-4f7a-b34b-6fccfb07015c.jpg)  I then ran it with the recursive option (`-R`) with root as the argument, where it lists all the files recursively from the root - ```bash ./hdfs_ls -R / ``` ![Ls recursive on root](https://user-images.githubusercontent.com/10280768/159152705-38bb24cc-0668-4960-911e-e8b6cda885e7.jpg)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-03-20T07:31:11Z","2022-03-22T17:42:11Z"
"","4003","HDFS-16462. Make HDFS get tool cross platform","### Description of PR The source files for `hdfs_get` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make these tools cross platform.  `hdfs_get` has the same functionality as `hdfs_copyToLocal`. Thus, we're reusing the same to implement `hdfs_get`.  ### How was this patch tested? Given that `hdfs_get` has the same functionality as `hdfs_copyToLocal`, we've implemented `hdfs::tools::Get` by deriving from `hdfs::tools::CopyToLocal`. Since we've already tested `hdfs::tools::CopyToLocal` for functional correctness, we don't need to do any additional testing to validate the same for `hdfs::tools::Get`.  However, we do ensure that the correct tool name shows up for `hdfs_copyToLocal` and `hdfs_get`.  ## hdfs_copyToLocal  ```bash $ ./hdfs_copyToLocal -h ```  ![image](https://user-images.githubusercontent.com/10280768/154832281-ac4e52fa-c799-4f52-ac63-dca8797da2b9.png)  ## hdfs_get  ```bash $ ./hdfs_get -h ```  ![image](https://user-images.githubusercontent.com/10280768/154832302-05e0b5d3-c96f-4dd4-ad3e-d2074aaa5fcc.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-02-20T07:07:52Z","2022-03-05T17:35:51Z"
"","3945","HDFS-16445. Make HDFS count, mkdir, rm cross platform","### Description of PR The source files for `hdfs_count`, `hdfs_mkdir` and `hdfs_rm` uses `getopt` for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with `boost::program_options` to make these tools cross platform.  ### How was this patch tested? This PR was tested by running the above tools locally.  ## hdfs_mkdir  Ran the tool to create a directory.  ```bash $ ./hdfs_mkdir /home/gautham/new-dir ```  ![image](https://user-images.githubusercontent.com/10280768/151838650-55a1fc41-8d4c-4d72-9a3b-563cfd37857d.png)  The tool is then run with the `-p` option to exercise the creation of intermediate directories.  ```bash $ ./hdfs_mkdir -p /home/gautham/a/b/c ```  The trailing `a/b/c` directories are created under the path `/home/gautham`.  ![image](https://user-images.githubusercontent.com/10280768/151836730-3f228afc-720f-4518-be93-0e656cb6969a.png)  The tool is then run with the `-m` option with `700` as the mode. This turns all the permission bits on for user and turns off all the permission bits for group and other users.  ```bash $ ./hdfs_mkdir -p -m 700 /home/gautham/d/e/f ```  The trailing `d/e/f` directories are created under the path `/home/gautham` with the permission of `700`, even for the intermediate directories.  ![image](https://user-images.githubusercontent.com/10280768/151837549-a435e367-c513-45ed-8e38-4591e020fe68.png)  Confirmation from browser.  ![Browser](https://user-images.githubusercontent.com/10280768/151837719-dcf51ce5-6b94-4796-9abb-347b2a727460.jpg)    ## hdfs_count  Ran the tool on one of the HDFS paths.  ```bash $ ./hdfs_count /home/gautham ``` It shows the count of files and directories contained within this path.  ![Basic](https://user-images.githubusercontent.com/10280768/151835301-951491c0-be68-49bc-983c-d3b50031061a.jpg)  The tool is then run with the quota option.  ```bash $ ./hdfs_count -q /home/gautham ```  It displays the quota information additionally.  ![Quota](https://user-images.githubusercontent.com/10280768/151835658-102d3fe8-7288-43b8-9b44-dea766e629c8.jpg)    ## hdfs_rm  Ran the tool to delete a file.  ```bash $ ./hdfs_rm /home/gautham/data.txt ```  ![Basic](https://user-images.githubusercontent.com/10280768/151837956-bb41a77f-5486-4568-8744-9d20dfde2bb1.jpg)  The tool was then run with the `-R` option to recursively delete a directory and its sub-directories.  ```bash $ ./hdfs_rm -R /home/gautham/a ```  The directory `a` and its sub-directories were deleted.  Confirmation from browser.  ![Delete directory](https://user-images.githubusercontent.com/10280768/151838226-86f1fb90-b3ab-46dd ![Browser](https://user-images.githubusercontent.com/10280768/151838314-c0fc7aac-6cf0-4270-a92d-d073dd322dfa.jpg) -b8ab-28c8cfd19c76.jpg)    ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-01-30T18:32:46Z","2022-02-02T05:42:26Z"
"","4261","Backport Hadoop 17215 branch 2.10","### Description of PR The reason for this cherry pick is to backport fix for https://issues.apache.org/jira/browse/HADOOP-17215 .The following commits were cherry picked in order to do it cleanly  8031c66295b530dcaae9e00d4f656330bc3b3952 459eb2ad6d5bc6b21462e728fb334c6e30e14c39 7f486f0258943f1dbda7fe5c08be4391e284df28 3472c3efc0014237d0cc4d9a989393b8513d2ab6 0d855159f0956af3070a1a173c8a6cb2c71a1ea3 e31a636e922a8fdbe0aa7cca53f6de7175e97254 07b7d073884720d8423be1dfdcfa60ec8833d2ae  The second last commit e31a636e922a8fdbe0aa7cca53f6de7175e97254 was the intended one and the last one was cherry picked because it fixed some test failures.   ### How was this patch tested? Ran `mvn test -pl hadoop-tools/hadoop-azure`  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 277, Failures: 0, Errors: 0, Skipped: 4 [INFO]  [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  06:09 min [INFO] Finished at: 2022-05-03T09:43:05-07:00 [INFO] ------------------------------------------------------------------------ ```  Ran all integration abfs tests using `mvn -T 1C -Dparallel-tests=abfs clean verify` with my storage account arjundev.dfs.core.windows.net Storage account's Primary location: East US, Secondary location: West US  ``` [INFO] Results: [INFO]  [INFO] Tests run: 42, Failures: 0, Errors: 0, Skipped: 0 ```  ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestAzureBlobFileSystemCreate.testFilterFSWriteAfterClose:210 » IO java.io.Fi... [ERROR]   ITestAzureBlobFileSystemCreate.testNegativeScenariosForCreateOverwriteDisabled:363 » TokenAccessProvider [ERROR]   ITestAzureBlobFileSystemE2E.testFlushWithFileNotFoundException:224 » IO java.i... [ERROR]   ITestAzureBlobFileSystemE2E.testWriteWithFileNotFoundException:204 » IO java.i... [INFO]  [ERROR] Tests run: 426, Failures: 0, Errors: 4, Skipped: 253 ```  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 24 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","arjun4084346","2022-05-03T16:17:45Z","2022-05-11T21:19:13Z"
"","4573","HDFS-16665. Fix duplicate sources for HDFS test","### Description of PR The library target `hdfspp_test_shim_static` is built using the following sources, which causes duplicate symbols to be defined - 1. hdfs_shim.c 2. ${LIBHDFSPP_BINDING_C}/hdfs.cc https://github.com/apache/hadoop/blob/8774f178686487007dcf8c418c989b785a529000/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tests/CMakeLists.txt#L153  And fails the compilation - ``` H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\hdfspp_test_shim_static.lib(hdfs_shim.obj) : error LNK2005: hdfsAllowSnapshot already defined in hdfspp_test_shim_static.lib(hdfs.obj)  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\hdfspp_test_shim_static.lib(hdfs_shim.obj) : error LNK2005: hdfsAvailable already defined in hdfspp_test_shim_static.lib(hdfs.obj)  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\hdfspp_test_shim_static.lib(hdfs_shim.obj) : error LNK2005: hdfsBuilderConfSetStr already defined in hdfspp_test_shim_static.lib(hdfs.obj)  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\hdfspp_test_shim_static.lib(hdfs_shim.obj) : error LNK2005: hdfsBuilderConnect already defined in hdfspp_test_shim_static.lib(hdfs.obj) Duplicate symbols defined by hdfs_shim.c - https://github.com/apache/hadoop/blob/440f4c2b28515d2007b81ac00b549bbf14fa9f64/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tests/hdfs_shim.c#L583-L585. ```  Adding the source `${LIBHDFSPP_BINDING_C}/hdfs.cc` is redundant here since this file is transitively included in `hdfs_shim.c` through `libhdfspp_wrapper.h` - https://github.com/apache/hadoop/blob/440f4c2b28515d2007b81ac00b549bbf14fa9f64/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tests/hdfs_shim.c#L20 Thus, we need to exclude `${LIBHDFSPP_BINDING_C}/hdfs.cc` to fix this issue.  ### How was this patch tested? 1. Tested this locally by building on my Windows system. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-17T16:50:03Z","2022-07-19T16:09:23Z"
"","4373","HADOOP-18268. Install maven from Apache archives","### Description of PR The Jenkins CI for Hadoop is failing to build since it's unable to download and install maven -  ``` 22:38:13  #11 [ 7/16] RUN pkg-resolver/install-maven.sh centos:7 22:38:13  #11 sha256:8b1823a6197611693af5daa2888f195db76ae5e9d0765f799becc7e7d5f7b019 22:40:25  #11 131.5 curl: (7) Failed to connect to 2403:8940:3:1::f: Cannot assign requested address 22:40:25  #11 ERROR: executor failed running [/bin/bash --login -c pkg-resolver/install-maven.sh centos:7]: exit code: 7 ```  Jenkins run - https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4370/4/console We need to switch to using Maven from Apache archives to prevent such issues.   ### How was this patch tested? CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2022-05-29T11:56:34Z","2022-06-02T11:35:46Z"
"","4611","HDFS-16680. Skip libhdfspp Valgrind tests on Windows","### Description of PR The CMake test `libhdfs_mini_stress_valgrind` requires Valgrind - https://github.com/apache/hadoop/blob/221eb2d68d5b52e4394fd36cb30d5ee9ffeea7f0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tests/CMakeLists.txt#L172-L175  We need to skip this test on Windows since we don't have Valgrind on Windows.  ### How was this patch tested? 1. Tested this locally by building on my Windows system. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-22T17:15:39Z","2022-07-23T17:52:16Z"
"","4434","HDFS-16469. Locate protoc-gen-hrpc across platforms","### Description of PR The `protoc-gen-hrpc` executable is supposed to be found at `${CMAKE_CURRENT_BINARY_DIR}/protoc-gen-hrpc`.  https://github.com/apache/hadoop/blob/652b257478f723a9e119e5e9181f3c7450ac92b5/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/proto/CMakeLists.txt#L70  This works so long as we're building the `Release` build. Since we can only build `RelWithDebInfo` on Windows, the protoc-gen-hrpc binary will be placed at ${CMAKE_CURRENT_BINARY_DIR}/RelWithDebInfo/protoc-gen-hrpc.exe. Hadoop would need to locate this binary in order to generate the Protobuf headers.  ### Solution We resolve this issue by using the `$` CMake generator expression to get the path to the `protoc-gen-hrpc` CMake target.  ### How was this patch tested? Hadoop Jenkins CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2022-06-12T17:30:10Z","2022-06-15T09:58:13Z"
"","4371","HDFS-16602. Use ""defined"" directive along with #if","### Description of PR The `#if` preprocessor directive expects a boolean expression. Thus, we need to use the `defined` directive as well to check if the macro has been defined.   ### How was this patch tested? Unit tests run by Jenkins CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-05-28T07:43:44Z","2022-06-03T16:35:23Z"
"","4228","HDFS-16468. Define ssize_t for Windows","### Description of PR Some C/C++ files use `ssize_t` data type. This isn't available for Windows and we need to define an alias for this and set it to an appropriate type to make it cross platform compatible.  ### How was this patch tested? The existing unit tests exercises usage of ssize_t. The passing of these unit tests are sufficient to ensure that the XPlatforms' ssize_t has been substituted correctly.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-04-23T17:09:14Z","2022-04-29T16:55:17Z"
"","4287","HDFS-16561. Handle error returned by strtol","### Description of PR Solves the bug mentioned here :  https://issues.apache.org/jira/browse/HDFS-16561   ### How was this patch tested? gtests in c++  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","rishabh1704","2022-05-08T10:44:12Z","2022-05-26T07:52:42Z"
"","4684","HDFS-16714: Repalce okhttp by apache http client","### Description of PR Repalce okhttp by apache http client, and remove kotlin dependencies  ### How was this patch tested? Existing test.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","pan3793","2022-08-03T11:40:17Z","2022-08-03T19:49:43Z"
"","4547","HADOOP-18332: remove rs-api dependency as it conflicts with jsr311-api","### Description of PR Relates to https://issues.apache.org/jira/browse/HADOOP-18332 - rs-api jar seems to cause conflicts with jsr311-api jar (latter is needed by jersey-core 1.19)  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-11T14:33:25Z","2022-07-17T16:07:55Z"
"","3750","HDFS-16372. refreshNodes should not use System.out but System.err when exceptions…","### Description of PR RefreshNodes should not use System.out but System.err when exceptions occur  ### How was this patch tested? No needs to test  ### For code changes: - https://issues.apache.org/jira/browse/HDFS-16372","open","","hfutatzhanghb","2021-12-06T05:09:50Z","2021-12-07T11:50:48Z"
"","4608","HADOOP-18340 deleteOnExit does not work with S3AFileSystem","### Description of PR processDeleteOnExit() is overiden in S3AFilesystem, it skips exist() check and delete objects without checking if FileSystem is closed.   ### How was this patch tested? A new unitest case is added. And all unittest cases under hadoop-tools/hadoop-aws passed. mvn -Dparallel-tests clean test  Did S3A Integration tests against us-west-2 region and there are a few failures/errors. Run the trunk code without the patch, there are same errors/failures. The errors/failures are not caused by the patch, probably due to misconfiguration ( I could not figure out) mvn -Dparallel-tests clean verify  The result is ` Tests | Errors | Failures | Skipped | Success Rate | Time -- | -- | -- | -- | -- | -- 1252 | 6 | 1 | 270 | 77.875% | 3,627.473  ` The errors are  ` org.apache.hadoop.fs.s3a.auth.delegation.ITestDelegatedMRJob#testCommonCrawlLookup[1] + [ Detail ] | 0.324    | s3a://hbase-test-data/fork-0001/test: getFileStatus on s3a://hbase-test-data/fork-0001/test: com.amazonaws.services.s3.model.AmazonS3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: XJ3DCCR6Q7SXTJDW; S3 Extended Request ID: fRmP3m1lThWxhj3s9VkSNEtuBz1JeBWYw65aRajrSg/H7IN+muB7d8PavSeqJ2urvLZtguTbnlc=; Proxy: null), S3 Extended Request ID: fRmP3m1lThWxhj3s9VkSNEtuBz1JeBWYw65aRajrSg/H7IN+muB7d8PavSeqJ2urvLZtguTbnlc=:InvalidAccessKeyId |     |   |     | testJobSubmissionCollectsTokens[1] + [ Detail ] | 0.329   | s3a://hbase-test-data/fork-0001/test: getFileStatus on s3a://hbase-test-data/fork-0001/test: com.amazonaws.services.s3.model.AmazonS3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: XJ35WHK7X6EMP9B6; S3 Extended Request ID: rtWEsDYcGqNiaoKy2D5EQQqN+O7MbYe1bYbiSmkF+FOz9/wb6+t+dQooqj7ppCSCZMBgC3PeEw4=; Proxy: null), S3 Extended Request ID: rtWEsDYcGqNiaoKy2D5EQQqN+O7MbYe1bYbiSmkF+FOz9/wb6+t+dQooqj7ppCSCZMBgC3PeEw4=:InvalidAccessKeyId `  ` org.apache.hadoop.fs.s3a.ITestS3AEndpointRegion#testBlankRegionTriggersSDKResolution + [ Detail ] | 2.817    | [Client region name] expected:<""[mars-north]-2""> but was:<""[us-west]-2""> ` and  ` org.apache.hadoop.fs.s3a.ITestS3ATemporaryCredentials#estSTS    | : request session credentials: com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: Cannot call GetSessionToken with session credentials (Service: AWSSecurityTokenService; Status Code: 403; Error Code: AccessDenied; Request ID: d935696d-7d98-4a1c-825f-22bf3d28ed9d; Proxy: null):AccessDenied `  ### For code changes:  - [x ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","huaxiangsun","2022-07-21T23:27:21Z","2022-08-03T00:14:46Z"
"","3818","HDFS-16390 Enhance ErasureCodeBenchmarkThroughput for support random read and make buffer size customizable","### Description of PR nhance ErasureCodeBenchmarkThroughput for support random read and make buffer size customizable  ### How was this patch tested? Test in our production cluster  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zhengzhuobinzzb","2021-12-20T05:47:41Z","2021-12-28T09:13:13Z"
"","4574","HDFS-16666. Pass CMake args for Windows in pom.xml","### Description of PR Need to pass the required HDFS CMake related argument for building on Windows. Currently, these are passed - https://github.com/apache/hadoop/blob/34e548cb62ed21c5bba7a82f5f1489ca6bdfb8c4/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml#L150  We need to pass these arguments as well for Windows - https://github.com/apache/hadoop/blob/34e548cb62ed21c5bba7a82f5f1489ca6bdfb8c4/hadoop-hdfs-project/hadoop-hdfs-native-client/pom.xml#L219-L223  ### How was this patch tested? 1. Tested this locally by building on my Windows system. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-17T17:07:05Z","2022-07-19T05:16:03Z"
"","4575","MAPREDUCE-7396 Job History File Permissions should not have exec perm.","### Description of PR MAPREDUCE-7396 Job History File Permissions now set with 77X permission and can config by user.But we are creating a file instead of a path, as a file does not need executable permissions  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","skysiders","2022-07-18T04:16:34Z","2022-07-18T06:40:53Z"
"","4252","HDFS-16566 Erasure Coding: Recovery may cause excess replicas when busy DN exsits","### Description of PR Jira:[HDFS-16566](https://issues.apache.org/jira/browse/HDFS-16566)  Simple case:  RS3-2 ,[0(busy),2,3,4] (1 missing),0 is busy. We can get liveblockIndice=[2,3,4], additionalRepl=1.So the DN will get the LiveBitSet=[2,3,4] and targets.length=1. According to StripedWriter.initTargetIndices(), 0 will get recovered instead of 1. So the internal blocks will become [0(busy),2,3,4,0'(excess)].Although NN will detect, delete the excess replicas and recover the missing block(1) correctly after the wrong recovery of 0', I don't think this process is expected and the recovery of 0' is obviously wrong and not necessary.  ### How was this patch tested? RS6-3, [0(busy),1(busy),3,4,5,6,7,8] ,2 is missing.Test the recovery of this block group to verify there is no excess during the recovery.  ### For code changes: The most important modification is in BlockManager.chooseSourceDatanodes( ). Add ""ExcludeReconstructed"" to save the block in busy DN. StripedWriter.initTargetIndices() will exclude the blocks which are in ""ExcludeReconstructed"". The changes also refer to BlockECReconstructionInfo, so the modification of erasurecoding.proto is necessary.","closed","","RuinanGu","2022-04-30T10:48:07Z","2022-07-16T18:22:03Z"
"","3811","HADOOP-18214. Update BUILDING.txt","### Description of PR java-8-openjdk become openjdk-8-jdk (see both ubuntu and debian package's name)  ### How was this patch tested? with simple command issued on command line   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","gvieri","2021-12-16T21:54:47Z","2022-04-28T21:22:35Z"
"","3776","HADOOP-13500. Synchronizing iteration of Configuration properties object","### Description of PR It is possible to encounter a ConcurrentModificationException while trying to iterate a Configuration object. The iterator method tries to walk the underlying Property object without proper synchronization, so another thread simultaneously calling the set method can trigger it. setProperty method on Property object is also synchronized on the same property object.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dbadaya1","2021-12-09T08:03:03Z","2021-12-17T16:07:57Z"
"","3775","HADOOP-13500. Synchronizing iteration of Configuration properties object","### Description of PR It is possible to encounter a ConcurrentModificationException while trying to iterate a Configuration object. The iterator method tries to walk the underlying Property object without proper synchronization, so another thread simultaneously calling the set method can trigger it. setProperty method on Property object is also synchronized on the same property object.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dbadaya1","2021-12-09T07:28:32Z","2021-12-17T07:05:47Z"
"","4280","YARN-11133. YarnClient gets the wrong EffectiveMinCapacity value","### Description of PR It calls the QueueConfigurations#getEffectiveMinCapacity to get the wrong value when I use the YarnClient. I found some bugs with QueueConfigurationsPBImpl#mergeLocalToBuilder. ConfiguredMinResource was incorrectly assigned to effMinResource. This causes the real effMinResource to be overwritten and configuredMinResource is null.  JIRA: YARN-11133  ### How was this patch tested? Test in a production environment  ### For code changes: Call the correct method","closed","","zhuzilong2013","2022-05-07T16:37:49Z","2022-05-16T16:42:13Z"
"","4437","YARN-11179. Show more detailed info when container token is expired","### Description of PR ISSUE: https://issues.apache.org/jira/browse/YARN-11179  I found in our nm logs, there is no appid in log about failing on starting containers when container token is expired. This will make hard to troubleshoot.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zuston","2022-06-14T03:36:15Z","2022-06-17T01:39:23Z"
"","3963","YARN-11068. Update transitive log4j2 dependency to 2.17.1","### Description of PR Instead of excluding log4j2, perhaps we should try force its version to 2.17.1, which is not vulnerable to the log4shell issues.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2022-02-07T15:53:55Z","2022-02-22T02:39:36Z"
"","4093","YARN-11093: fix for reading files in timeline server in chronological order in case of fs-support-append to false","### Description of PR In our setup, we are using Hive and Tez and using offline tez-ui mode, where we copy the ATS events files to other place and start tez-ui. To keep the event files small, we are using   fs-support-append = false, and creating a new file for writing the events. We can see that, in this mode, file name contains [suffix](https://github.com/apache/hadoop/blob/1d5650c4d0acf33f141d593762682b3603523104/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/FileSystemTimelineWriter.java#L382) as timestamp.  But at the time of [read](https://github.com/apache/hadoop/blob/1d5650c4d0acf33f141d593762682b3603523104/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java#L844), we are relying on the FS to provide correct file order. If that fails, we are adding event out of order, leading to event being discarded, or incorrect information.  Fix could be sorting of the file names, based on suffix if append mode is not used.  sample file names:  summarylog-appattempt_1647348120288_0001_000001_460237 entitylog-timelineEntityGroupId_1647348120288_1_dag_1647348120288_0001_1_673147  ### How was this patch tested? 1. Added conf `yarn.timeline-service.fs-support-append` to false. 2. Ran a job, which created small files for domain, entity and summary log. 3. Ran ATS by copying all the entity files to custom location, and checked tez-ui with and without the change. Was able to see latest status of the task correctly after the change.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","guptashailesh92","2022-03-22T17:33:56Z","2022-03-23T09:14:16Z"
"","3803","HDFS-16385. Fix Datanode retrieve slownode information bug.","### Description of PR in HDFS-16320, the DataNode will retrieve the SLOW status from each NameNode. But it didn't set isSlowNode to HeartbeatResponseProto in DatanodeProtocolServerSideTranslatorPB#sendHeartbeat.  ### How was this patch tested? I constructed some slownode scenarios to test getting the slownode results correctly.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Jackson-Wang-7","2021-12-15T09:48:22Z","2021-12-21T02:04:47Z"
"","3976","HDFS-16452. msync RPC should send to acitve namenode directly","### Description of PR In current ObserverReadProxyProvider implementation,   we use the following code to  invoke msync RPC.  ```java getProxyAsClientProtocol(failoverProxy.getProxy().proxy).msync();  ``` But msync RPC maybe send to Observer NameNode in this way, and then failover to Active NameNode.   This can be avoid by applying this patch.","open","","hfutatzhanghb","2022-02-10T02:56:53Z","2022-06-02T13:47:01Z"
"","3880","HDFS-16420. Avoid deleting unique data blocks when deleting redundancy striped blocks.","### Description of PR if there are two or more blocks exist in a same rack, it may cause unique data block is added to exactlyOne processing list when choosing redundancy stripted block to delete. `      storages.remove(cur);     if (storages.isEmpty()) {       rackMap.remove(rack);     }     if (moreThanOne.remove(cur)) {       if (storages.size() == 1) {         **          final DatanodeStorageInfo remaining = storages.get(0);         moreThanOne.remove(remaining);         exactlyOne.add(remaining);          **       }     } else {       exactlyOne.remove(cur);     }  `  In this case, moreThanOne list may not contain the remaining block. The remaining block shouldn’t be deleted, but it is added to exactlyOne list. And then it will be deleted.  ### How was this patch tested? The testcase is that:(EC 6+3) blk_-xxx009 in rack /d1/r1 blk_-xxx008 in rack /d1/r1 blk_-xxx008 in rack /d1/r2 blk_-xxx008 in rack /d1/r3 blk_-xxx007 in rack /d1/r4 blk_-xxx006 in rack /d2/r1 blk_-xxx005 in rack /d2/r2 blk_-xxx004 in rack /d2/r3 blk_-xxx003 in rack /d2/r4 blk_-xxx002 in rack /d2/r5 blk_-xxx001 in rack /d2/r6 After the FBR is triggered and redundant data blocks are added to invalidate list, blk_-xxx008 in rack /d1/r1 and blk_-xxx008 in rack /d1/r2 need to be deleted, blk_-xxx009 is HEALTHY.","closed","","Jackson-Wang-7","2022-01-13T03:26:03Z","2022-01-14T13:39:03Z"
"","4493","[Do not commit] Testing cross platform builds","### Description of PR I'm triggering this CI run just to make sure that the cross platform builds are running fine.  ### How was this patch tested? Hadoop Jenkins CI.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","GauthamBanasandra","2022-06-23T06:25:36Z","2022-07-01T05:10:56Z"
"","4029","[Do not commit] Debug CI issues","### Description of PR I'm reverting the changes done till the last known successful run. I'll abandon this PR once my debugging is complete. Please don't merge this.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-02-24T15:28:02Z","2022-03-05T01:46:31Z"
"","4514","YARN-11200. Backport Numa patch to 2.10","### Description of PR https://issues.apache.org/jira/browse/YARN-5764  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-06-29T12:17:41Z","2022-07-18T17:51:18Z"
"","4361","YARN-11165. Use default java.policy when no group policy is set.","### Description of PR https://issues.apache.org/jira/browse/YARN-11165  When JavaSandboxLinuxContainerRuntime is used, we can specify yarn.nodemanager.runtime.linux.sandbox-mode.policy to use self-provided java.policy file. When this setting is not specified, JavaSandboxLinuxContainerRuntime will use the default java.policy file.  However, when user belongs to a group (or more groups), and yarn.nodemanager.runtime.linux.sandbox-mode.policy.group.$groupName setting is not specified, JavaSandboxLinuxContainerRuntime still skips the default java.policy file, resulting in a final policy which looks like this:  ``` grant codeBase ""file:/usr/local/hadoop/-"" {   permission java.security.AllPermission; }; grant {    permission java.io.FilePermission ""/tmp/hadoop-yarn/nm-local-dir/usercache/yarn/appcache/application_1653546011283_0006//-"", ""read"";    permission java.io.FilePermission ""/tmp/hadoop-yarn/nm-local-dir/usercache/yarn/appcache/application_1653546011283_0006/filecache/13/-"", ""read"";    permission java.io.FilePermission ""/tmp/hadoop-yarn/nm-local-dir/usercache/yarn/appcache/application_1653546011283_0006/filecache/11/-"", ""read"";    permission java.io.FilePermission ""/tmp/hadoop-yarn/nm-local-dir/usercache/yarn/appcache/application_1653546011283_0006/filecache/12/-"", ""read"";    permission java.io.FilePermission ""/tmp/hadoop-yarn/nm-local-dir/usercache/yarn/appcache/application_1653546011283_0006/filecache/10/-"", ""read""; };  ``` which will cause problem running applications.   This PR ensures that the default java.policy will still be used when no group policy is set.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","bambrow","2022-05-27T03:49:30Z","2022-05-27T05:59:20Z"
"","4605","HDFS-16677. Add OP_SWAP_BLOCK_LIST as an operation code in FSEditLogOpCodes.","### Description of PR https://issues.apache.org/jira/browse/HDFS-16677  sub task of https://issues.apache.org/jira/browse/HDFS-14978  ### How was this patch tested?  add unit test testSwapBlockListEditLog ### For code changes: add swapBlockListEditLog & Feature.SWAP_BLOCK_LIST for NameNodeLayoutVersion","closed","","Neilxzn","2022-07-21T12:55:16Z","2022-07-26T07:22:26Z"
"","4541","HDFS-16655. OIV: print out erasure coding policy name in oiv Delimited output","### Description of PR https://issues.apache.org/jira/browse/HDFS-16655 By adding erasure coding policy name to oiv output, it will help with oiv post-analysis to have a overview of all folders/files with specified ec policy and to apply internal regulation based on this information. In particular, it wiil be convenient for the platform to calculate the real storage size of the ec file.  ### How was this patch tested? add test class: org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForErasureCodingPolicy  ### For code changes:  add oiv cmd option ""-ec"" , used by delimiter only","closed","","Neilxzn","2022-07-09T07:08:09Z","2022-07-25T09:40:36Z"
"","4400","HDFS-16616. remove use of org.apache.hadoop.util.Sets","### Description of PR https://issues.apache.org/jira/browse/HDFS-16616  As part of removing guava dependencies  [HADOOP-17115](https://issues.apache.org/jira/browse/HADOOP-17115), [HADOOP-17721](https://issues.apache.org/jira/browse/HADOOP-17721), [HADOOP-17722](https://issues.apache.org/jira/browse/HADOOP-17722) and [HADOOP-17720](https://issues.apache.org/jira/browse/HADOOP-17720) are fixed,  Currently the code call util function to create HashSet and TreeSet in the repo . These function calls dont have much importance as it is calling internally new HashSet<> / new TreeSet<> from java.utils   This task is to clean up all the function calls to create sets which is redundant   Before moving to java8 , sets were created using guava functions and API , now since this is moved away and util code in the hadoop now looks like  ``` public static  TreeSet newTreeSet() { return new TreeSet();} ```  2.  ``` public static  HashSet newHashSet()  { return new HashSet(); } ``` These interfaces dont do anything much just a extra layer of function call   please refer to the task  https://issues.apache.org/jira/browse/HADOOP-17726  Can anyone review if this ticket add some value in the code.  Looking forward to some input/ thoughts . If not adding any value we can close it and not move forward with changes !  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","Samrat002","2022-06-03T22:16:38Z","2022-06-22T04:47:37Z"
"","3999","HDFS-16459. RBF: register RBFMetrics in MetricsSystem for promethuessink","### Description of PR https://issues.apache.org/jira/browse/HDFS-16459 Router' RBFMetrics was not register  in MetricsSystem. We can't find these metrics from PrometheusSink. Maybe we should fix it.      After fix it ,  some  RBFMetrics will export like this  ``` # HELP rbf_metrics_current_tokens_count Number of router's current tokens # TYPE rbf_metrics_current_tokens_count gauge rbf_metrics_current_tokens_count{processname=""Router"",context=""dfs"",hostname=""xxxx""} 2  ```  ### How was this patch tested? no new test  ### For code changes: 1. DefaultMetricsSystem.instance().register  2. use `@Metric` to export metrics","closed","","Neilxzn","2022-02-18T09:27:16Z","2022-02-21T19:32:35Z"
"","4590","HDFS-15006. Add OP_SWAP_BLOCK_LIST as an operation code in FSEditLogOpCodes.","### Description of PR https://issues.apache.org/jira/browse/HDFS-15006  sub task of https://issues.apache.org/jira/browse/HDFS-14978  ### How was this patch tested?  add unit test testSwapBlockListEditLog ### For code changes: add swapBlockListEditLog & Feature.SWAP_BLOCK_LIST for NameNodeLayoutVersion","open","","Neilxzn","2022-07-19T10:34:53Z","2022-07-26T07:29:03Z"
"","4589","HADOOP-18348. Change hadoop_start_daemon function $ to BASHPID","### Description of PR https://issues.apache.org/jira/browse/HADOOP-18348  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","Pr-Jiang","2022-07-19T08:15:27Z","2022-07-19T09:51:48Z"
"","4181","HADOOP-18193:Support nested mount points in INodeTree","### Description of PR https://issues.apache.org/jira/browse/HADOOP-18193  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","li-leyang","2022-04-15T20:48:38Z","2022-05-12T18:59:46Z"
"","4158","HDFS-16535. SlotReleaser should reuse the domain socket based on socket paths","### Description of PR HDFS-13639 (#1885) improves the performance of short-circuit shm slot releasing by reusing the domain socket that the client previously used to send release request to the DataNode.  This is good when there are only one DataNode locates with the client (truth in most of the production environment). However, if we launch multiple DataNodes on a machine (usually for testing, e.g. Impala's end-to-end tests), the request could be sent to the wrong DataNode. See an example in IMPALA-11234.  We should only reuse the domain socket when it corresponds to the same socket path. This change introduces a map in ShortCircuitCache to track the mapping from socket path to the domain socket. SlotReleaser will pick the correct domain socket (if exists) based on the socket path.  ### How was this patch tested?  I deploy the fix in my Impala minicluster with multiple DataNodes launched on my machine. Verified that the error logs of slot release failures disappeared.  Also add a regression test: TestShortCircuitCache#testDomainSocketClosedByMultipleDNs().  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","stiga-huang","2022-04-11T03:44:05Z","2022-04-18T02:32:29Z"
"","4395","HADOOP-18274. Use CMake 3.19.0 in Debian 10","### Description of PR HDFS Native Client fails to build on Debian 10 due to the following error -  ``` [WARNING] CMake Error at main/native/libhdfspp/CMakeLists.txt:68 (FetchContent_MakeAvailable): [WARNING]   Unknown CMake command ""FetchContent_MakeAvailable"". [WARNING]  [WARNING]  [WARNING] -- Configuring incomplete, errors occurred! ``` Jenkins run - https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4371/2/artifact/out/branch-compile-hadoop-hdfs-project_hadoop-hdfs-native-client.txt  This cause for this issue is that the version of CMake on Debian 10 (which is installed through apt) is 3.13 and FetchContent_MakeAvailable was [introduced in CMake 3.14](https://cmake.org/cmake/help/v3.14/module/FetchContent.html)  Thus, we upgrade CMake by installing through the [install-cmake.sh](https://github.com/apache/hadoop/blob/34a973a90ef89b633c9b5c13a79aa1ac11c92eb5/dev-support/docker/pkg-resolver/install-cmake.sh) script from pkg-resolver which installs CMake 3.19.0, instead of installing CMake through apt on Debian 10.  ### How was this patch tested? Hadoop Jenkins CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2022-06-02T11:47:33Z","2022-06-03T04:33:47Z"
"","3762","HDFS-16014: Fix an issue in checking native pmdk lib by 'hadoop check native' command","### Description of PR Fix an issue: 'hadoop check native' command doesn't correctly reflect the loading state of native pmdk lib when this lib is removed after building.  ### How was this patch tested? We tested the validity of the patch on our lab machine. The test depends on the installation of pmdk lib.  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [N/A] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [N/A] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [N/A] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","PHILO-HE","2021-12-08T07:26:39Z","2021-12-14T09:15:58Z"
"","3936","YARN-11068. Exclude transitive log4j2 dependency coming from solr 8.","### Description of PR Exclude transitive log4j2 dependency coming from solr 8  ### How was this patch tested? manually verified the output of 'mvn dependency:tree'  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2022-01-26T05:38:29Z","2022-01-27T15:37:39Z"
"","4584","HADOOP-18345: Enhance client protocol to propagate last seen state IDs for multiple nameservices.","### Description of PR Enhance client protocol to propagate last seen state IDs for multiple nameservices.  ### How was this patch tested? This is a prerequisite for HDFS-13522. There are tests in the that JIRA.  ### For code changes:  - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","simbadzina","2022-07-18T17:00:52Z","2022-08-03T16:35:57Z"
"","4127","HDFS-13522. RBF: Support observer node from Router-Based Federation","### Description of PR Enables routers to direct read calls to observer namenodes. This is a refresh of a patch by Zhuobin Zhang to make it apply on current trunk. https://issues.apache.org/jira/secure/attachment/13027012/HDFS-13522.002.patch  ### How was this patch tested? New unit tests Deployed this in our test cluster and saw read operations in the observer audit log.  ### For code changes:  - [ x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","simbadzina","2022-03-31T17:16:55Z","2022-07-28T15:14:52Z"
"","4379","HDFS-16608. Fix the link in the doc to point correct function","### Description of PR Doc in the code refer to the variable in the class `DataStreamer. pipelineRecoveryCount` which is a private variable and link do not refer to it .   It has been modified to refer link to `DataStreamer. getPipelineRecoveryCount` function (getter function existing in the class DataStreamer) [HDFS-16608](https://issues.apache.org/jira/browse/HDFS-16608) ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","Samrat002","2022-05-30T14:01:51Z","2022-06-06T08:58:49Z"
"","4022","[Do not commit] Debug CI issues","### Description of PR Do not commit this PR. I've created this to debug some issues in the Precommit CI.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-02-23T16:52:33Z","2022-03-12T15:17:39Z"
"","4576","HDFS-16667. Use malloc for buffer allocation in uriparser2","### Description of PR Currently, a variable is used to specify the array size in uriparser2 - https://github.com/apache/hadoop/blob/34e548cb62ed21c5bba7a82f5f1489ca6bdfb8c4/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/third_party/uriparser2/uriparser2/uriparser2.c#L71-L79  This results in the following error on Windows - ``` H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\cl : command line warning D9025: overriding '/W4' with '/w'      uriparser2.c H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\main\native\libhdfspp\third_party\uriparser2\uriparser2\uriparser2.c(74,23): error C2057: expected constant expression  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\main\native\libhdfspp\third_party\uriparser2\uriparser2\uriparser2.c(74,23): error C2466: cannot allocate an array of constant size 0  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\main\native\libhdfspp\third_party\uriparser2\uriparser2\uriparser2.c(74,24): error C2133: 'buffer': unknown size  ```  I've used malloc to fix this.  ### How was this patch tested? 1. Tested this locally by building on my Windows system. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-18T05:33:39Z","2022-07-20T16:27:32Z"
"","3743","HDFS-16368. DFSadmin supports refresh topology info without restarting namenode","### Description of PR Currently in HDFS, if we update the rack info for rack-awareness, we may need to rolling restart namenodes to let it be effective. If cluster is large, the cost time of rolling restart namenodes is very log. So, we develope a method to refresh topology info without rolling restart namenodes.  ### How was this patch tested? 1. hdfs dfsamin -printTopology 2. upload a file to HDFS, then use `fsck` to check its block location and rack info.  ### For code changes:  https://issues.apache.org/jira/browse/HDFS-16368","open","","hfutatzhanghb","2021-12-02T08:49:59Z","2021-12-20T15:19:04Z"
"","4374","HDFS-16604. Install gtest via FetchContent_Declare in CMake","### Description of PR CMake is unable to checkout `release-1.10.0` version of GoogleTest - ``` [WARNING] -- Build files have been written to: /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-4370/centos-7/src/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/libhdfspp/googletest-download [WARNING] Scanning dependencies of target googletest [WARNING] [ 11%] Creating directories for 'googletest' [WARNING] [ 22%] Performing download step (git clone) for 'googletest' [WARNING] Cloning into 'googletest-src'... [WARNING] fatal: invalid reference: release-1.10.0 [WARNING] CMake Error at googletest-download/googletest-prefix/tmp/googletest-gitclone.cmake:40 (message): [WARNING]   Failed to checkout tag: 'release-1.10.0' [WARNING]  [WARNING]  [WARNING] gmake[2]: *** [CMakeFiles/googletest.dir/build.make:111: googletest-prefix/src/googletest-stamp/googletest-download] Error 1 [WARNING] gmake[1]: *** [CMakeFiles/Makefile2:95: CMakeFiles/googletest.dir/all] Error 2 [WARNING] gmake: *** [Makefile:103: all] Error 2 [WARNING] CMake Error at main/native/libhdfspp/CMakeLists.txt:68 (message): [WARNING]   Build step for googletest failed: 2 ```  Jenkins run - https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-4370/6/artifact/out/branch-compile-hadoop-hdfs-project_hadoop-hdfs-native-client.txt  We need to use `FetchContent_Declare` since we're getting the source code exactly at the given commit SHA. This avoids the checkout step altogether and solves the above issue.  ### How was this patch tested? Hadoop Jenkins CI run.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-05-29T14:06:07Z","2022-06-01T05:06:37Z"
"","4538","HDFS-16654. Link OpenSSL lib for CMake deps check","### Description of PR CMake checks whether the required components of OpenSSL are available prior to building HDFS native client - https://github.com/apache/hadoop/blob/fac895828f714b5587b57900d588acac69880c1e/hadoop-hdfs-project/hadoop-hdfs-native-client/src/CMakeLists.txt#L130  This check compiles but fails while linking on Windows - ``` src.obj : error LNK2019: unresolved external symbol EVP_aes_256_ctr referenced in function main [H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\CMakeFiles\CMakeTmp\cmTC_e391b.vcxproj]  H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\CMakeFiles\CMakeTmp\Debug\cmTC_e391b.exe : fatal error LNK1120: 1 unresolved externals [H:\hadoop-hdfs-project\hadoop-hdfs-native-client\src\out\build\x64-Debug\CMakeFiles\CMakeTmp\cmTC_e391b.vcxproj]  Source file was: #include  int main(int argc, char **argv) { return !EVP_aes_256_ctr; } ```  Thus, we need to link to the OpenSSL library prior to running this check. Please note that this check doesn't fail on Linux since CMake is able to pick it up from the standard location where libs are installed.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-08T18:19:41Z","2022-07-17T15:17:37Z"
"","4288","HDFS-16572. Fix typo in readme of hadoop-project-dist","### Description of PR Change `not` to `no`.  ### How was this patch tested? Not needed.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-05-08T11:32:26Z","2022-05-10T16:15:08Z"
"","4047","HADOOP-18151. Switch the baseurl for Centos 8","### Description of PR Centos 8 has reached its End-of-Life and thus its packages are no longer accessible from mirror.centos.org. We need to switch the baseurl to vault.centos.org where the packages are archived.  Please see https://www.centos.org/centos-linux-eol/ for more details.  ### How was this patch tested? Through CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2022-03-03T17:26:56Z","2022-03-05T01:41:26Z"
"","4329","HADOOP-18245. Extend KMS related exceptions that get mapped to ConnectException","### Description of PR Based on production workload, we found that it is not enough to map just SSLHandshakeException to ConnectException in Loadbalancing KMS Client but that needs to be extended to SSLExceptions and SocketExceptions.  ### How was this patch tested? Updated existing unit tests.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","kerneltime","2022-05-18T22:27:37Z","2022-05-19T20:20:24Z"
"","4604","HDFS-16674 : Improve TestDFSIO to support more filesystems","### Description of PR Added support for more filesystems. This will let the same TestDFSIO to be used for testing different filesystem other than HDFS  ### How was this patch tested? Tested with OCI filesystem after building the jar with new changes  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","gvijay452","2022-07-21T10:19:34Z","2022-07-26T10:59:40Z"
"","4465","YARN-11139. Wait one millisecond in DelegationTokenRenewerPoolTracker","### Description of PR Added a wait statement to reduce the CPU usage  ### How was this patch tested? The issue owner already tested it. You can see the results over [there](https://issues.apache.org/jira/browse/YARN-11139)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","akin-tekeoglu","2022-06-19T11:10:36Z","2022-06-19T21:41:24Z"
"","4641","HDFS-16697.Randomly setting “dfs.namenode.resource.checked.volumes.minimum” will always prevent safe mode from being turned off","### Description of PR Add Precondition.checkArgument() for minimumRedundantVolumes to ensure that the value is greater than the number of  NameNode storage volumes to avoid never being able to turn off safe mode afterwards.  JIRA:[HDFS-16697](https://issues.apache.org/jira/browse/HDFS-16697)  ### How was this patch tested?  It is found that “dfs.namenode.resource.checked.volumes.minimum” lacks a condition check and an associated exception handling mechanism, which makes it impossible to find the root cause of the impact when a misconfiguration occurs. This patch provides a check of the configuration items，it will throw a IllegalArgumentException and detailed error message when the value of ""dfs.namenode.resource.checked.volumes.minimum"" is set greater than the number of  NameNode storage volumes to avoid the misconfiguration from affecting the subsequent operation of the program.","closed","","Likkey","2022-07-27T12:30:13Z","2022-07-28T05:52:43Z"
"","4653","HDFS-16697.Randomly setting “dfs.namenode.resource.checked.volumes.minimum” will always prevent safe mode from being turned off","### Description of PR Add Precondition.checkArgument() for minimumRedundantVolumes to ensure that the value is greater than the number of NameNode storage volumes to avoid never being able to turn off safe mode afterwards.  JIRA:[[HDFS-16697](https://issues.apache.org/jira/browse/HDFS-16697)]  ### How was this patch tested? It is found that “dfs.namenode.resource.checked.volumes.minimum” lacks a condition check and an associated exception handling mechanism, which makes it impossible to find the root cause of the impact when a misconfiguration occurs. This patch provides a check of the configuration items，it will throw a IllegalArgumentException and detailed error message when the value of ""dfs.namenode.resource.checked.volumes.minimum"" is set greater than the number of NameNode storage volumes to avoid the misconfiguration from affecting the subsequent operation of the program.","open","","Likkey","2022-07-28T08:13:02Z","2022-07-31T10:43:59Z"
"","4649","HDFS-16697.Randomly setting “dfs.namenode.resource.checked.volumes.minimum” will always prevent safe mode from being turned off","### Description of PR Add Precondition.checkArgument() for minimumRedundantVolumes to ensure that the value is greater than the number of NameNode storage volumes to avoid never being able to turn off safe mode afterwards.  JIRA:[[HDFS-16697](https://issues.apache.org/jira/browse/HDFS-16697)]  ### How was this patch tested? It is found that “dfs.namenode.resource.checked.volumes.minimum” lacks a condition check and an associated exception handling mechanism, which makes it impossible to find the root cause of the impact when a misconfiguration occurs. This patch provides a check of the configuration items，it will throw a IllegalArgumentException and detailed error message when the value of ""dfs.namenode.resource.checked.volumes.minimum"" is set greater than the number of NameNode storage volumes to avoid the misconfiguration from affecting the subsequent operation of the program.","closed","","Likkey","2022-07-28T05:51:09Z","2022-07-28T08:13:32Z"
"","3886","HDFS-16425. Add debug log when requeue observer read call","### Description of PR Add debug log when requeue observer read call  ### How was this patch tested? no need to test.","open","","hfutatzhanghb","2022-01-13T09:59:22Z","2022-04-26T01:25:39Z"
"","3970","HADOOP-18117. Add an option to preserve root directory permissions","### Description of PR Add an option to preserve root directory permissions when using **distcp**  ### How was this patch tested? dev-support/bin/test-patch against trunk and unittest   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mohan3d","2022-02-09T10:52:19Z","2022-02-18T12:30:20Z"
"","4043","YARN-11082 AbstractCSQueue#canAssignToThisQueue DRF should use node partition reource as denominator","### Description of PR AbstractCSQueue#canAssignToThisQueue will check current queue useage and limit, and DRF will use cluster resource as denominator to check which resource is dominated and comapre the ratio however if our cluster's nodes resource are not blance such as there is larger proportion of memory/vores, then DRF will chose wrong dominated resource. For Example our cluster's total resouce are  the ratio is 1 vores : 4.25 GB, and the ratio changed to 1 : 4.8 under node label x.  ```java 2021-12-09 10:24:37,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_xxx_xxx container=xxx queue=root.a.a1.a2 clusterResource= type=RACK_LOCAL requestedPartition=x 2021-12-09 10:24:37,069 DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.AbstractCSQueue: Used resource= exceeded maxResourceLimit of the queue =  2021-12-09 10:24:37,069 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Failed to accept allocation proposal ``` clusterResource =  usedExceptKillable =  currentLimitResource =   currentLimitResource: memory : 3381248/175117312 = 0.01930847362 vCores : 687/40222 = 0.01708020486  usedExceptKillable: memory : 3384320/175117312 = 0.01932601615 vCores : 688/40222 = 0.01710506687  DRF will think memory is dominated resource and compare the ratio of memeory  in this scenario","open","","BrightK7","2022-03-02T04:18:50Z","2022-03-08T03:30:52Z"
"","4279","HDFS-16465. Remove redundant strings.h inclusions","### Description of PR `strings.h` was included in a bunch of C/C++ files and are redundant in its usage. Also, `strings.h` is not available on Windows and thus isn't cross-platform compatible. Thus, these inclusions of `strings.h` must be removed.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-05-07T15:08:01Z","2022-05-11T17:34:22Z"
"","4245","HDFS-16564. Use uint32_t for hdfs_find","### Description of PR `hdfs_find` uses `u_int32_t` type for storing the value for the `max-depth` command line argument - https://github.com/apache/hadoop/blob/a631f45a99c7abf8c9a2dcfb10afb668c8ff6b09/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/tools/hdfs-find/hdfs-find.cc#L43. The type `u_int32_t` isn't standard, isn't available on Windows and thus breaks cross-platform compatibility. We need to replace this with `uint32_t` which is available on all platforms since it's part of the C++ standard.  ### How was this patch tested? The existing unit tests exercise this PR sufficiently.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-04-28T17:07:31Z","2022-05-04T14:54:12Z"
"","4587","YARN-11200 numa support in branch-2.10","### Description of PR [YARN-11200](https://issues.apache.org/jira/browse/YARN-11200)  ### How was this patch tested? tested patch in the EMR cluster (5.x series)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-07-18T19:28:05Z","2022-07-28T09:20:20Z"
"","4586","YARN-11200. Yarn numa backport to branch-2.10","### Description of PR [YARN-11200](https://issues.apache.org/jira/browse/YARN-11200)  ### How was this patch tested? tested patch in the EMR cluster (5.x series)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-07-18T19:27:36Z","2022-07-18T19:28:23Z"
"","4546","YARN-11198. clean up numa resources from statestore","### Description of PR [YARN-11198](https://issues.apache.org/jira/browse/YARN-11198)  ### How was this patch tested? created and tested in emr   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-07-11T12:31:53Z","2022-07-14T05:54:35Z"
"","4526","HDFS-16466. Implement Linux permission flags on Windows","### Description of PR [statinfo.cc](https://github.com/apache/hadoop/blob/869317be0a1fdff23be5fc500dcd9ae4ecd7bc29/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/common/statinfo.cc#L41-L49) uses POSIX permission flags. These flags aren't available for Windows. We need to implement the equivalent flags on Windows to make this cross platform compatible.  ### How was this patch tested? Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-01T16:26:38Z","2022-07-08T03:59:19Z"
"","4370","HDFS-16463. Make dirent cross platform compatible","### Description of PR [jnihelper.c](https://github.com/apache/hadoop/blob/1fed18bb2d8ac3dbaecc3feddded30bed918d556/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/jni_helper.c#L28) in HDFS native client uses dirent.h. This header file isn't available on Windows. Thus, we need to replace this with a cross platform compatible implementation for dirent.   ### How was this patch tested? In progress.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-05-28T07:32:23Z","2022-06-10T15:19:49Z"
"","4411","HADOOP-18272. Removing util.set from yarn project","### Description of PR [HADOOP-18272](https://issues.apache.org/jira/browse/HADOOP-18272)  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","Samrat002","2022-06-06T20:01:35Z","2022-06-09T01:09:08Z"
"","3872","HADOOP-18046. TestIPC#testIOEOnListenerAccept fails","### Description of PR [HADOOP-18046](https://issues.apache.org/jira/browse/HADOOP-18046)  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","functioner","2022-01-08T03:30:15Z","2022-02-08T14:41:19Z"
"","4340","HDFS-15878 RBF: Flaky test TestRouterWebHDFSContractCreate>AbstractCo…","### Description of PR  WebHdfsFileSystem is not syncable, it should not fail the test. [https://issues.apache.org/jira/browse/HDFS-15878](https://issues.apache.org/jira/browse/HDFS-15878) ### How was this patch tested? Updated existing unit tests.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","HanleyYang","2022-05-22T02:33:19Z","2022-05-28T04:26:26Z"
"","4343","Update ojAlgo to latest version (v51.3.0)","### Description of PR  You're using a very old version of ojAlgo, and there has been significant improvements to the solvers over the years.  ### How was this patch tested?  1. Code compiles without any changes 2. The unit test works   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","apete","2022-05-23T09:25:53Z","2022-06-29T09:57:51Z"
"","4607","HADOOP-18354: Upgrade reload4j to 1.22.2 due to XXE vulnerability","### Description of PR  XXE issue in reload4j (probably not very exploitable)   ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-21T22:36:49Z","2022-07-24T10:39:25Z"
"","3935","HADOOP-18087. fix bugs when looking up record from upstream DNS servers.","### Description of PR  When query A record which is chained by CNAME, YARN Registry DNS Server does not properly respond. Some CNAME records are missing.  For example, ""repo.maven.apache.org"" is chaned as follows:  repo.maven.apache.org.	21317	IN	CNAME	repo.apache.maven.org. repo.apache.maven.org.	20114	IN	CNAME	maven.map.fastly.net. maven.map.fastly.net.	7	IN	A	199.232.192.215 maven.map.fastly.net.	7	IN	A	199.232.196.215  If ask A record for ""repo.maven.apache.org"" using ""dig"" or ""nslookup"", YARN Registry DNS Server will give answers similar to this: (10.1.2.3, 10.8.8.8 IP is virtual)   ``` $ nslookup repo.maven.apache.org 10.1.2.3 Server:		10.1.2.3 Address:	10.1.2.3#53  Non-authoritative answer: repo.maven.apache.org	canonical name = repo.apache.maven.org. Name:	maven.map.fastly.net Address: 151.101.196.215 ** server can't find repo.apache.maven.org: NXDOMAIN ```  The reason why you can see ""NXDOMAIN"", ""nslookup"" will query ""A"" & ""AAAA"" records. If there is no answer from other dns server, ""answers == null"" but YARN Registry DNS Server has a bug. There is no null handling.   ```java     // Forward lookup to primary DNS servers     Record[] answers = getRecords(name, type);     try {       for (Record r : answers) {         if (!response.findRecord(r)) {           if (r.getType() == Type.SOA) {             response.addRecord(r, Section.AUTHORITY);           } else {             response.addRecord(r, Section.ANSWER);           }         }         if (r.getType() == Type.CNAME) {           Name cname = r.getName();           if (iterations < 6) {             remoteLookup(response, cname, type, iterations + 1);           }         }       }     } catch (NullPointerException e) {       return Rcode.NXDOMAIN;     } catch (Throwable e) {       return Rcode.SERVFAIL;     }     return Rcode.NOERROR; ```      It should be like this:  ``` nslookup repo.maven.apache.org 10.8.8.8 Server:		10.8.8.8 Address:	10.8.8.8#53  Non-authoritative answer: repo.maven.apache.org	canonical name = repo.apache.maven.org. repo.apache.maven.org	canonical name = maven.map.fastly.net. Name:	maven.map.fastly.net Address: 151.101.196.215 ```    ### How was this patch tested?  - apply this patch to YARN Registry DNS Server in our cluster & test with `dig` and `nslookup`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","eubnara","2022-01-26T01:19:13Z","2022-02-28T09:29:27Z"
"","3911","HADOOP-18087. fix bug when getting CNAME chain records and no record from remote dns server","### Description of PR  When query A record which is chained by CNAME, YARN Registry DNS Server does not properly respond. Some CNAME records are missing.  For example, ""repo.maven.apache.org"" is chaned as follows:  repo.maven.apache.org.	21317	IN	CNAME	repo.apache.maven.org. repo.apache.maven.org.	20114	IN	CNAME	maven.map.fastly.net. maven.map.fastly.net.	7	IN	A	199.232.192.215 maven.map.fastly.net.	7	IN	A	199.232.196.215  If ask A record for ""repo.maven.apache.org"" using ""dig"" or ""nslookup"", YARN Registry DNS Server will give answers similar to this: (10.1.2.3, 10.8.8.8 IP is virtual)   ``` $ nslookup repo.maven.apache.org 10.1.2.3 Server:		10.1.2.3 Address:	10.1.2.3#53  Non-authoritative answer: repo.maven.apache.org	canonical name = repo.apache.maven.org. Name:	maven.map.fastly.net Address: 151.101.196.215 ** server can't find repo.apache.maven.org: NXDOMAIN ```  The reason why you can see ""NXDOMAIN"", ""nslookup"" will query ""A"" & ""AAAA"" records. If there is no answer from other dns server, ""answers == null"" but YARN Registry DNS Server has a bug. There is no null handling.   ```java     // Forward lookup to primary DNS servers     Record[] answers = getRecords(name, type);     try {       for (Record r : answers) {         if (!response.findRecord(r)) {           if (r.getType() == Type.SOA) {             response.addRecord(r, Section.AUTHORITY);           } else {             response.addRecord(r, Section.ANSWER);           }         }         if (r.getType() == Type.CNAME) {           Name cname = r.getName();           if (iterations < 6) {             remoteLookup(response, cname, type, iterations + 1);           }         }       }     } catch (NullPointerException e) {       return Rcode.NXDOMAIN;     } catch (Throwable e) {       return Rcode.SERVFAIL;     }     return Rcode.NOERROR; ```      It should be like this:  ``` nslookup repo.maven.apache.org 10.8.8.8 Server:		10.8.8.8 Address:	10.8.8.8#53  Non-authoritative answer: repo.maven.apache.org	canonical name = repo.apache.maven.org. repo.apache.maven.org	canonical name = maven.map.fastly.net. Name:	maven.map.fastly.net Address: 151.101.196.215 ```    ### How was this patch tested?  - apply this patch to YARN Registry DNS Server in our cluster & test with `dig` and `nslookup`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","eubnara","2022-01-21T01:36:06Z","2022-01-26T01:21:08Z"
"","3956","HADOOP-18110. ViewFileSystem: Add Support for Localized Trash Root","### Description of PR  We modified getTrashRoot()/getTrashRoots() in ViewFileSystem to return a localized trash root that is within a mount point when CONFIG_VIEWFS_MOUNT_POINT_LOCAL_TRASH is set.   ### How was this patch tested?  mvn test -Dtest=TestView*  Manually ran three newly added unit tests using intellij.","closed","","xinglin","2022-02-02T17:42:50Z","2022-02-11T05:16:20Z"
"","3817","HADOOP-18052. Support Apple Silicon in start-build-env.sh","### Description of PR  Use Dockerfile_aarch64 in Apple Silicon.  ### How was this patch tested?  Manually tested in M1 Pro MBP. However, Docker for Mac with Apple Silicon is too slow to develop.  ``` bash-3.2$ echo $MACHTYPE arm64-apple-darwin21 bash-3.2$ echo ""$MACHTYPE"" | cut -d- -f1 arm64 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-19T15:50:32Z","2021-12-23T09:13:28Z"
"","3988","HADOOP-15983. Use jersey-json that is built to use jackson2","### Description of PR  Use a jersey-json fork that supports jackson 2. This jar is still a prototype. This PR is experimental and not yet for merging.   ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-02-11T19:01:09Z","2022-04-28T05:19:12Z"
"","3852","HADOOP-18062. Update the year to 2022","### Description of PR  Update the release year to 2022.  ### How was this patch tested?  Not tested.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-01-04T02:11:34Z","2022-01-04T03:32:22Z"
"","4253","HADOOP-18219. Revert ""HADOOP-15983. Use jersey-json that is built to use jackson2 (#3988)""","### Description of PR  This reverts commit 63187083cc3b9bb1c1e90e692e271958561f9cc8. This commit seems to break the shadedclient tests on Debian 10 - https://github.com/apache/hadoop/pull/4245#issuecomment-1114026936.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2022-05-01T17:42:44Z","2022-05-03T03:46:29Z"
"","4414","HDFS-16625.","### Description of PR  There are unit tests that require native PMDK libraries which aren't checking if the library is available, resulting in unsuccessful test.  This patch checks the assumption about PMDK availability.  The same changes have been applied and tested against trunk (3.4.0-SNAPSHOT), branch-3.3 (3.3.4-SNAPSHOT), and branch-3.3.3.  ### How was this patch tested?  This patch has been applied to a local build that runs in the Hadoop development environment, which doesn't include the PMDK shared libraries.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","snmvaughan","2022-06-07T15:11:39Z","2022-06-14T14:14:22Z"
"","4537","HADOOP-18329 - Support for IBM Semeru JVM v>11.0.15.0 Vendor Name Changes","### Description of PR  There are checks within the PlatformName class that use the Vendor property of the provided runtime JVM specifically looking for `IBM` within the name. Whilst this check worked for IBM's [java technology edition](https://www.ibm.com/docs/en/sdk-java-technology) it fails to work on [Semeru](https://developer.ibm.com/languages/java/semeru-runtimes/) since 11.0.15.0 due to the following change:  **java.vendor system property** In this release, the java.vendor system property has been changed from ""International Business Machines Corporation"" to ""IBM Corporation"".  Modules such as the below are not provided in these runtimes. com.ibm.security.auth.module.JAASLoginModule  This change attempts to use reflection to ensure that a class common to IBM JT runtimes exists, extending upon the vendor check, since IBM vendored JVM's may not actually require special logic to use custom security modules. The same 3.3.3 versions were working correctly until the vendor name change was observed during routine upgrades by internal CI.  ### How was this patch tested?  CI + Unit test, some seemingly unrelated failures were observed relating to `java.lang.NoSuchMethodError: java.nio.ByteBuffer.limit(I)Ljava/nio/ByteBuffer;`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","JackBuggins","2022-07-08T12:55:30Z","2022-07-14T12:49:24Z"
"","4490","HADOOP-18311. Upgrade dependencies to address several CVEs","### Description of PR  The following CVEs can be addressed by upgrading dependencies within the build.  This includes a replacement of HTrace with a noop implementation. - CVE-2018-7489 - CVE-2020-10663 - CVE-2020-28491 - CVE-2020-35490 - CVE-2020-35491 - CVE-2020-36518 - PRISMA-2021-0182  This addresses all of the CVEs from `branch-3.3` except for the kotlin library associated with okhttp and the ones that would require upgrading Netty to 4.x.  ### How was this patch tested?  Tested using a local build of `branch-3.3` along with this patch.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","snmvaughan","2022-06-22T19:42:48Z","2022-06-28T00:19:22Z"
"","4491","HADOOP-18311. Upgrade dependencies to address several CVEs","### Description of PR  The following CVEs can be addressed by upgrading dependencies within the build.  This includes a replacement of HTrace with a noop implementation.    - CVE-2018-7489    - CVE-2020-10663    - CVE-2020-28491    - CVE-2020-35490    - CVE-2020-35491    - CVE-2020-36518    - PRISMA-2021-0182    This addresses all of the CVEs from `branch-3.3.4` except for the kotlin library associated with okhttp and the ones that would require upgrading Netty to 4.x.  This is a backport specifically targeted at 3.3.4  ### How was this patch tested?  Tested using a local build of `branch-3.3.4` along with this patch.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","snmvaughan","2022-06-22T22:05:53Z","2022-07-18T11:01:58Z"
"","4256","[DO NOT MERGE] Test YETUS-1060. github status should use htmlreport","### Description of PR  Testing https://github.com/apache/yetus/pull/261 in Apache Hadoop repo for upcoming Yetus 0.14.0 release.  ### How was this patch tested?  Not tested locally.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-05-02T18:04:39Z","2022-05-07T22:42:32Z"
"","4290","[DO NOT MERGE] Use Yetus 0.14.0-RC1 commit hash in Jenkinsfile","### Description of PR  Test Yetus 0.14.0 release candidate","closed","","aajisaka","2022-05-09T00:49:25Z","2022-05-16T15:18:56Z"
"","4348","HDFS-16586. Purge FsDatasetAsyncDiskService threadgroup; it causes BP…","### Description of PR  Remove the ThreadGroup used by executor factories; they are unused and ThreadGroups auto-destroy when their Thread-member count goes to zero. This behavior is incompatible with the configuration we have on the per-volume executor which is set to let all threads die if no use inside the keepalive time.  Signed-off-by: Hexiaoqiao [hexiaoqiao@apache.org](mailto:hexiaoqiao@apache.org)  Backport to branch-3.2.   ### How was this patch tested?  By running a downstream test that was failing when this PR was not in place.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","saintstack","2022-05-23T15:59:33Z","2022-05-25T23:35:57Z"
"","4498","HADOOP-18246. Update prefetching block size lower bound to 1 byte","### Description of PR  Remove the lower limit of 8MB on prefetching feature block size. While a value of 1 is likely not very useful, there is no other reason to limit it.  ### How was this patch tested?  Running integration tests. All passing except those expected to fail mentioned in HADOOP-18247.  Also tested by setting values 0 and 1 and checking expected range header.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","dannycjones","2022-06-24T14:10:10Z","2022-08-03T12:46:26Z"
"","4545","HADOOP-18332: remove jsr311-api dependency","### Description of PR  Relates to https://issues.apache.org/jira/browse/HADOOP-18033 - js311-api jar seems to cause conflicts with rs-api jar   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-11T12:00:29Z","2022-07-11T16:55:39Z"
"","4432","YARN-11078. Set env vars in a cross platform compatible way","### Description of PR  Prior to running a node.js command, a `TMPDIR` environment variable is set - https://github.com/apache/hadoop/blob/11d144d2284be29da1f49e163db0763636dcf058/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp/package.json#L11-L13  This is bash syntax and thus fails on Windows. This PR sets the `TMPDIR` environment variable in a cross-platform compatible way.  ### How was this patch tested? Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2022-06-12T06:06:02Z","2022-06-15T09:59:58Z"
"","4149","HADOOP-18195 make jackson 1 a runtime scope dependency","### Description of PR  pom changes  ### How was this patch tested?  CI build - relates to HADOOP-18195  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-04-07T14:14:34Z","2022-04-08T10:07:40Z"
"","4415","MAPREDUCE-7386. Maven parallel builds (skipping tests) fail","### Description of PR  Parallel builds fail because the assembly plugin does not understand the required dependencies.  ### How was this patch tested?  This patch has been applied to a local build that runs in the Hadoop development environment, and is used to build a distribution.  The same changes have been applied to trunk, branch-3.3, and branch-3.3.3.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","snmvaughan","2022-06-07T19:35:05Z","2022-08-03T13:31:33Z"
"","4459","YARN-11186: Upgrade frontend toolchains used by YARN application catalog webapp","### Description of PR  NodeJS 12 is EOL, and starting from NodeJS 16, it supports Apple Sillion natively.  This PR proposes to  - upgrade `frontend-maven-plugin` from 1.11.2 to 1.12.1, because of https://github.com/eirslett/frontend-maven-plugin/pull/970 - upgrade NodeJS from v12.22.1(EOL) to the latest LTS v16.15.1, see https://nodejs.org/en/about/releases/ - upgrade Yarn from v1.22.5 to the latest v1.22.19, see https://www.npmjs.com/package/yarn  ### How was this patch tested?  Build on M1 chip machine.  ``` [INFO] --- frontend-maven-plugin:1.12.1:install-node-and-yarn (install node and yarn) @ hadoop-yarn-applications-catalog-webapp --- [INFO] testFailureIgnore property is ignored in non test phases [INFO] Installing node version v16.15.1 [INFO] Downloading https://nodejs.org/dist/v16.15.1/node-v16.15.1-darwin-arm64.tar.gz to /Users/chengpan/.m2/repository/com/github/eirslett/node/16.15.1/node-16.15.1-darwin-arm64.tar.gz [INFO] No proxies configured [INFO] No proxy was configured, downloading directly [INFO] Unpacking /Users/chengpan/.m2/repository/com/github/eirslett/node/16.15.1/node-16.15.1-darwin-arm64.tar.gz into /Users/chengpan/Projects/apache-hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/node/tmp [INFO] Copying node binary from /Users/chengpan/Projects/apache-hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/node/tmp/node-v16.15.1-darwin-arm64/bin/node to /Users/chengpan/Projects/apache-hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/node/node [INFO] Installed node locally. [INFO] Installing Yarn version v1.22.19 [INFO] Unpacking /Users/chengpan/.m2/repository/com/github/eirslett/yarn/1.22.19/yarn-1.22.19.tar.gz into /Users/chengpan/Projects/apache-hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target/node/yarn [INFO] Installed Yarn locally. [INFO] [INFO] --- frontend-maven-plugin:1.12.1:yarn (yarn install) @ hadoop-yarn-applications-catalog-webapp --- [INFO] testFailureIgnore property is ignored in non test phases [INFO] Running 'yarn ' in /Users/chengpan/Projects/apache-hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/target [INFO] yarn install v1.22.19 [INFO] [1/4] Resolving packages... [INFO] [2/4] Fetching packages... [INFO] [3/4] Linking dependencies... [INFO] [4/4] Building fresh packages... [INFO] Done in 0.44s. ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","pan3793","2022-06-18T19:31:22Z","2022-07-19T07:33:30Z"
"","4111","HADOOP-18178. Upgrade jackson to 2.13.2 and jackson-databind to 2.13.2.2","### Description of PR  New CVE in jackson-databind  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-03-26T12:06:11Z","2022-04-08T13:57:03Z"
"","4138","HDFS-16479. EC: NameNode should not send a reconstruction work when the source datanodes are insufficient","### Description of PR  NameNode should not send a reconstruction work when the source datanodes are insufficient. Otherwise, DataNodes receive the order and throw the following exception. ``` java.lang.IllegalArgumentException: No enough live striped blocks.         at com.google.common.base.Preconditions.checkArgument(Preconditions.java:141)         at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReader.(StripedReader.java:128)         at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedReconstructor.(StripedReconstructor.java:135)         at org.apache.hadoop.hdfs.server.datanode.erasurecode.StripedBlockReconstructor.(StripedBlockReconstructor.java:41)         at org.apache.hadoop.hdfs.server.datanode.erasurecode.ErasureCodingWorker.processErasureCodingTasks(ErasureCodingWorker.java:133)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:796)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processCommand(BPServiceActor.java:1314)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.lambda$enqueue$2(BPServiceActor.java:1360)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1287) ```  ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tasanuma","2022-04-05T03:04:02Z","2022-04-14T02:23:45Z"
"","4034","HADOOP-18144. getTrashRoot/s in ViewFileSystem return a viewFS path","### Description of PR  Modified getTrashRoot/s to return viewFS path for the new trash location.    If CONFIG_VIEWFS_TRASH_ROOT_UNDER_MOUNT_POINT_ROOT is not set, return the default trash root (a targetFS path) from targetFS.    When CONFIG_VIEWFS_TRASH_ROOT_UNDER_MOUNT_POINT_ROOT is set to true,    1) If path p is in a snapshot or encryption zone, or when it is in the        fallback FS, return the default trash root (a targetFS path) from targetFS.   2) else, return a viewFS path for the trash root under the root of the        mount point (/mntpoint/.Trash/{user}).  ### How was this patch tested?  Run unit tests manually from intellij.","closed","","xinglin","2022-02-26T01:28:44Z","2022-03-14T18:32:12Z"
"","3917","HADOOP-18092. Exclude log4j2 dependency from hadoop-huaweicloud module","### Description of PR  JIRA: HADOOP-18092  Exclude log4j2 dependency from hadoop-huaweicloud module.  ### How was this patch tested?  Manually checked that the dependencies are excluded.  ``` % mvn clean dependency:tree -pl hadoop-cloud-storage-project/hadoop-huaweicloud/ (snip) [INFO] +- com.huaweicloud:esdk-obs-java:jar:3.20.4.2:compile [INFO] |  +- com.jamesmurty.utils:java-xmlbuilder:jar:1.2:compile [INFO] |  +- com.squareup.okhttp3:okhttp:jar:3.14.2:compile [INFO] |  +- com.fasterxml.jackson.core:jackson-core:jar:2.13.0:compile [INFO] |  \- com.fasterxml.jackson.core:jackson-annotations:jar:2.13.0:compile ```  I didn't run integration tests.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-01-24T06:06:10Z","2022-01-24T07:47:25Z"
"","4502","HADOOP-18313: AliyunOSSBlockOutputStream should not mark the temporary file for deletion","### Description of PR  Jira issue: https://issues.apache.org/jira/browse/HADOOP-18313  ### How was this patch tested?  New unit tests.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","wujinhu","2022-06-25T11:36:12Z","2022-07-06T06:44:57Z"
"","4007","HADOOP-18138. UserGroupInformation shouldn't log exception unnecessarily","### Description of PR  In UserGroupInformation.doAs, we currently create a new Exception and log it in LOG.debug. This doesn't look necessary:  ```       if (LOG.isDebugEnabled()) {         LOG.debug(""PrivilegedAction [as: {}][action: {}]"", this, action,             new Exception());       } ```  ### How was this patch tested?  Existing tests.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","sunchao","2022-02-21T21:20:04Z","2022-02-22T15:40:03Z"
"","3704","HDFS-16348. Mark slownode as badnode to recover pipeline","### Description of PR  In HDFS-16320, the DataNode can retrieve the SLOW status from each NameNode.   This ticket is to send this information back to Clients who are writing blocks. If a Clients noticed the pipeline is build on a slownode, he/she can choose to mark the slownode as a badnode to exclude the node or rebuild a pipeline.  In order to avoid the false positives, we added a config of ""threshold"", only clients continuously receives slownode reply from the same node will the node be marked as SLOW.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16348  ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-11-22T14:50:06Z","2021-12-29T15:28:07Z"
"","3782","YARN-10880: nodelabels update log is too noisy","### Description of PR  https://issues.apache.org/jira/browse/YARN-10880  when use YARN Distributed NodeLabel setup, every time the node update, RM will print INFO log “INFO org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager: No Modified Node label Mapping to replace”，the log is too noisy, see the attachment pic, so can we just change to DEBUG or remove it.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","luoge457","2021-12-10T08:02:32Z","2021-12-16T12:33:21Z"
"","3781","YARN-10863: CGroupElasticMemoryController is not work","### Description of PR  https://issues.apache.org/jira/browse/YARN-10863  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","luoge457","2021-12-10T07:40:49Z","2022-04-20T12:19:03Z"
"","4100","HDFS-16518: Add shutdownhook in KeyProviderCache to invalidate cache at jvm shutdown","### Description of PR  https://issues.apache.org/jira/browse/HDFS-16518  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","li-leyang","2022-03-23T18:34:34Z","2022-03-30T21:00:10Z"
"","4582","HADOOP-18343: upgrade jetty","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-18343  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-18T11:00:38Z","2022-07-19T09:12:24Z"
"","4581","HADOOP-18342: upgrade to avro 1.11.0","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-18342   ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-18T10:42:20Z","2022-07-25T16:22:13Z"
"","4578","HADOOP-18341: upgrade commons-configuration2 to 2.8.0","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-18341  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","pjfanning","2022-07-18T09:33:27Z","2022-07-20T12:59:24Z"
"","4583","HADOOP-18333: upgrade to jetty 9.4.48 (3.3 branch)","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-18333  Relates to https://github.com/apache/hadoop/pull/4553  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-18T11:07:38Z","2022-07-19T02:29:04Z"
"","3990","[HADOOP-13386] upgrade to avro 1.9.2","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-13386  ### How was this patch tested?  mvn build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-02-13T21:43:21Z","2022-03-26T11:31:56Z"
"","3992","HDFS-15745. Make DataNodePeerMetrics#LOW_THRESHOLD_MS and MIN_OUTLIER_DETECTION_NODES configurable. Contributed by Haibin Huang.","### Description of PR  HDFS-15745. Make DataNodePeerMetrics#LOW_THRESHOLD_MS and MIN_OUTLIER_DETECTION_NODES configurable.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tasanuma","2022-02-15T13:15:51Z","2022-02-16T00:42:47Z"
"","4115","HADOOP-18180. Replace use of twitter util-core with java futures","### Description of PR  HADOOP-18180 replace use of twitter util-core - it uses scala 2.11 and will cause issue for Spark and other scala based tools that use different versions of scala  ### How was this patch tested?  * CI build * local build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-03-28T13:15:56Z","2022-04-04T14:48:33Z"
"","3993","HADOOP-18126. update junit 5 due to build issues","### Description of PR  HADOOP-18126 Fix for broken tests  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-02-15T22:35:50Z","2022-02-18T10:34:27Z"
"","3792","HADOOP-18043. Use mina-core 2.0.22 to fix LDAP unit test failures","### Description of PR  HADOOP-18043 Use mina-core 2.0.22 to fix unit test failures in hadoop-auth module.  ### How was this patch tested?  Ran unit tests manually  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-13T03:54:31Z","2021-12-13T08:49:42Z"
"","3774","HADOOP-18040. Use maven.test.failure.ignore instead of ignoreTestFailure","### Description of PR  HADOOP-18040 Use `maven.test.failure.ignore` instead of using a new variable `ignoreTestFailure`.  ### How was this patch tested?  1. Create a unit test to fail. 1. The test failure is ignored as expected. 1. Add `-Dmaven.test.failure.ignore=false` from command line. Verify that the test failure is not ignored.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-09T01:07:41Z","2021-12-09T16:36:48Z"
"","4254","HADOOP-18219. Fix shadedclient test failure","### Description of PR  Fix the following ClassNotFoundException in shadedclient test: ``` Caused by: java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522) 	at org.apache.hadoop.shaded.org.eclipse.jetty.webapp.WebAppClassLoader.loadClass(WebAppClassLoader.java:538) ``` JIRA: HADOOP-18219  ### How was this patch tested?  Ran the following command manually ``` mvn clean verify -fae -am -pl hadoop-client-modules/hadoop-client-check-invariants -pl hadoop-client-modules/hadoop-client-check-test-invariants -pl hadoop-client-modules/hadoop-client-integration-tests -Dtest=NoUnitTests -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-05-01T21:43:10Z","2022-05-02T18:50:10Z"
"","4112","HDFS-16523. Fix dependency error in hadoop-hdfs on M1 Mac","### Description of PR  Fix dependency error in hadoop-hdfs by fixing the version of hawtjni-runtime  JIRA: HDFS-16523  ### How was this patch tested?  Manually tested.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-03-26T15:35:34Z","2022-03-29T18:28:18Z"
"","3829","HADOOP-18045. Disable TestDynamometerInfra","### Description of PR  Disable TestDynamometerInfra. JIRA: HADOOP-18045  ### How was this patch tested?  Ran the following command after applying the patch: ``` % mvn clean test -Dtest=TestDynamometerInfra (snip) [INFO] Running org.apache.hadoop.tools.dynamometer.TestDynamometerInfra [WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 0.007 s - in org.apache.hadoop.tools.dynamometer.TestDynamometerInfra [INFO]  [INFO] Results: [INFO]  [WARNING] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-27T09:04:44Z","2021-12-28T04:22:26Z"
"","3989","YARN-10788. TestCsiClient fails","### Description of PR  Create unix domain socket in java.io.tmpdir instead of test.build.dir to avoid 'File name too long' error.  ### How was this patch tested?  Locally ran the test and it passed.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-02-13T21:12:44Z","2022-02-14T16:13:28Z"
"","4246","HDFS-16540. Data locality is lost when DataNode pod restarts in kubernetes (#4170)","### Description of PR  Cherry-pick of 9ed8d60511dccf96108239c5c96e108a7d4bc975  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","saintstack","2022-04-28T17:50:45Z","2022-05-18T13:11:45Z"
"","3934","HADOOP-18093. Better exception handling for testFileStatusOnMountLink…","### Description of PR  Better exception handling for testFileStatusOnMountLink() in ViewFsBaseTest.java (#3918).  ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs","closed","","xinglin","2022-01-25T20:15:15Z","2022-01-26T16:25:05Z"
"","4147","HADOOP-18178. Upgrade jackson to 2.13.2 and jackson-databind to 2.13.…","### Description of PR  Backport HADOOP-18178 to branch-3.3 to upgrade jackson  ### How was this patch tested?  No manual testing  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-04-07T07:31:54Z","2022-04-11T05:58:33Z"
"","4478","HADOOP-18304. Improve user-facing S3A committers documentation","### Description of PR  As noted in the ticket, this PR attempts to improve the committer docs given a fresh pair of eyes from someone who has not worked with the committers before.  I've tried to ensure that the Table of Contents makes more sense too.  ### How was this patch tested?  Reading :)  I did try and build the HTML but I couldn't get it to pickup the new markdown.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","dannycjones","2022-06-21T09:31:48Z","2022-06-23T16:08:19Z"
"","4161","remove explicit dependency on jackson 1","### Description of PR  An alternative to https://github.com/apache/hadoop/pull/3988  Both jersey-json 1.19 and avro 1.7.x imported jackson v1 transitively. Avro has been upgraded so now jersey-json needs jackson v1.  In theory, hadoop itself only needs to let jersey-json import jackson transitively. I prefer https://github.com/apache/hadoop/pull/3988 but this alternative means that hadoop doesn't advertise its indirect dependency on jackson 1.    ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-04-11T18:55:09Z","2022-04-18T20:22:59Z"
"","4533","HADOOP-18328. S3A supports S3 on Outposts","### Description of PR  [HADOOP-18328](https://issues.apache.org/jira/browse/HADOOP-18328) S3A supports S3 on Outposts  I have fixed the endpoint to support [S3 on Outposts](https://aws.amazon.com/blogs/aws/amazon-s3-on-outposts-now-available/). I have also added tests to cover the fixes accordingly.  ### How was this patch tested?  I ran `mvn test` and verified that the test passed.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","suzusuzu","2022-07-07T03:14:05Z","2022-07-07T04:52:16Z"
"","4364","HADOOP-18265. use HashSet/ TreeSet Constructor for hadoop-hdfs-project","### Description of PR  [HADOOP-18265](https://issues.apache.org/jira/browse/HADOOP-18265)  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-05-27T19:54:49Z","2022-06-03T22:07:23Z"
"","3980","Hadoop-17563 bouncycastle 1.68","### Description of PR  [HADOOP-17563](https://issues.apache.org/jira/browse/HADOOP-17563). BouncyCastle to 1.70  CVEs are reported for releases lower than 1.66  ### How was this patch tested?  CI build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-02-10T20:42:57Z","2022-03-07T22:24:32Z"
"","3742","HDFS-16364. Remove unnecessary brackets in NameNodeRpcServer#L453","### Description of PR   Remove unnecessary brackets in NameNodeRpcServer#L453  ### JIRA https://issues.apache.org/jira/browse/HDFS-16364?filter=-2","closed","","wzhallright","2021-12-01T12:42:18Z","2021-12-03T11:21:04Z"
"","4346","HDFS-16586. Purge FsDatasetAsyncDiskService threadgroup","### Description of PR      Remove the ThreadGroup used by executor factories; they are unused     and ThreadGroups auto-destroy when their Thread-member count goes to zero.     This behavior is incompatible with the configuration we have on the per-volume     executor which is set to let all threads die if no use inside the     keepalive time.  Backport from trunk.  ### How was this patch tested?  By running a downstream test that was broken w/o this PR  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","saintstack","2022-05-23T15:50:34Z","2022-05-23T15:51:34Z"
"","4099","Hdfs 16518","### Description of PR   https://issues.apache.org/jira/browse/HDFS-16518  ### How was this patch tested?  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","li-leyang","2022-03-23T17:32:28Z","2022-03-23T18:32:19Z"
"","4120","MAPREDUCE-7373. Building MapReduce NativeTask fails on Fedora 34+","### Description of PR   Building MapReduce NativeTask with GCC 11, in which C++17 features are enabled by default, leads to the following error. This PR fixes this problem.  ``` error: ISO C++17 does not allow dynamic exception specifications ```  ### How was this patch tested?  I ran the following command locally and confirmed that it succeeded.  ``` $ mvn package -Pdist,native -DskipTests -Dtar -Dmaven.javadoc.skip=true ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","sekikn","2022-03-30T02:01:42Z","2022-03-30T14:22:40Z"
"","3813","HADOOP-18049. Pin python lazy-object-proxy to 1.6.0 in Docker file as newer versions are incompatible with python2.7","### Description of PR   ### How was this patch tested? `./start-build-env.sh` executes without failure.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dbadaya1","2021-12-17T08:37:35Z","2021-12-17T09:02:38Z"
"","4637","HADOOP-18344. Upgrade AWS SDK to 1.12.262","### Description of PR   ### How was this patch tested?   object store test in progress. first run had two failures I'd not see before  * landsat test timeout * oom in a test writing data. I'd set my connector to use bytebuffer for block buffering and it ran out. But why now? SDK update *or* just upload delays triggering it?","closed","","steveloughran","2022-07-26T21:22:38Z","2022-07-28T12:04:38Z"
"","4365","HADOOP-18266. Using HashSet/ TreeSet Constructor for hadoop-common","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Samrat002","2022-05-27T20:34:18Z","2022-06-20T06:41:05Z"
"","4552","HADOOP-18332. remove rs-api dependency (3.3 branch)","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-07-11T23:30:55Z","2022-07-16T17:57:27Z"
"","4549","Hadoop 18074 branch 3.3","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","lmccay","2022-07-11T19:03:14Z","2022-07-11T20:13:30Z"
"","3997","YARN-11077. containersToClean not need to use TreeSet In RMNodeImpl","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ChaosJu","2022-02-18T06:57:07Z","2022-02-18T12:24:24Z"
"","4685","footer metrics","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","anmolanmol1234","2022-08-03T13:52:41Z","2022-08-03T15:35:09Z"
"","4655","YARN-11216. Avoid unnecessary reconstruction of ConfigurationProperties","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","K0K0V0K","2022-07-28T13:51:11Z","2022-08-01T15:15:17Z"
"","4651","Hadoop 18325 temp","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","anmolanmol1234","2022-07-28T07:18:52Z","2022-07-28T11:19:24Z"
"","4633","HDFS-16693. RBF: fix NPE in Quota","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","WeisonWei","2022-07-26T13:30:04Z","2022-07-26T17:54:21Z"
"","4631","YARN-11216. Avoid unnecessary reconstruction of ConfigurationProperties","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","K0K0V0K","2022-07-26T12:09:37Z","2022-08-01T13:35:37Z"
"","4620","update cloud 123","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","Summar9786","2022-07-24T14:54:46Z","2022-07-25T18:14:53Z"
"","4617","update cloud 123","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","haider11225","2022-07-23T20:03:25Z","2022-07-23T21:04:34Z"
"","4591","update cloud 123","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","raheeltariq","2022-07-19T15:00:10Z","2022-07-19T16:01:39Z"
"","4513","YARN-11204. Various MapReduce tests fail with NPE in AggregatedLogDeletionService.stopRMClient","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-06-29T10:02:39Z","2022-06-29T13:12:52Z"
"","4471","YARN-11190. CS Mapping rule bug: User matcher does not work correctly for usernames with dot","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","szilard-nemeth","2022-06-20T15:43:30Z","2022-06-20T19:23:13Z"
"","4425","YARN-11175. Refactor LogAggregationFileControllerFactory","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-06-10T14:35:06Z","2022-06-13T11:59:25Z"
"","4359","YARN-11164. PartitionQueueMetrics support more metrics","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zuston","2022-05-26T08:14:12Z","2022-06-14T03:21:43Z"
"","4266","HADOOP-18222. Prevent DelegationTokenSecretManagerMetrics from registering multiple times","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","hchaverri","2022-05-05T19:43:34Z","2022-05-10T20:59:13Z"
"","4241","HDFS-16563. Namenode WebUI prints sensitive information on Token expiry","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-27T17:55:21Z","2022-06-07T04:07:41Z"
"","4235","YARN-11114. RMWebServices returns only apps matching exactly the submitted queue name","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-04-26T13:39:23Z","2022-05-11T16:05:47Z"
"","4227","HDFS-16551. CryptoInputStream#close() should be syncronized","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-23T16:21:45Z","2022-04-24T18:27:08Z"
"","4214","HDFS-16551. CryptoInputStream#close() should be syncronized","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-21T14:49:10Z","2022-04-21T21:13:06Z"
"","4207","HDFS-16545. Provide an option to balance rack level in Balancers","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","prasad-acit","2022-04-20T20:50:58Z","2022-04-22T11:48:24Z"
"","4191","HDFS-16526. Addendum Add metrics for slow DataNode","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-18T15:39:07Z","2022-04-20T17:23:06Z"
"","4174","HADOOP-18203. Backport HADOOP-15033 to branch-2.10.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","mccormickt12","2022-04-14T22:41:35Z","2022-04-20T06:27:35Z"
"","4162","HDFS-16526. Add metrics for slow DataNode","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2022-04-12T05:32:16Z","2022-04-17T03:13:37Z"
"","4143","Hadoop 16612 branch 2.10","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","arjun4084346","2022-04-05T22:40:06Z","2022-04-12T17:20:36Z"
"","4119","YARN-11103. SLS cleanup after previously merged SLS refactor jiras","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-03-29T19:52:11Z","2022-03-31T13:04:51Z"
"","4117","YARN-11102. Fix spotbugs error in hadoop-sls module","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-03-29T15:36:53Z","2022-04-01T16:47:20Z"
"","4116","YARN-10550. Decouple NM runner logic from SLSRunner","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-03-29T15:22:16Z","2022-03-30T17:57:18Z"
"","4113","YARN-10549. Decouple RM runner logic from SLSRunner","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2022-03-28T09:25:40Z","2022-03-29T07:58:13Z"
"","4072","YARN-11089. Fix typo in rm audit log","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-15T04:24:37Z","2022-03-21T13:39:05Z"
"","4065","YARN-11086. Add space in debug log of ParentQueue","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zuston","2022-03-13T08:03:28Z","2022-03-24T02:18:46Z"
"","4030","HADOOP-18143. toString method of RpcCall throws IllegalArgumentException","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","9uapaw","2022-02-24T17:29:51Z","2022-03-02T20:33:20Z"
"","4025","HADOOP-18128. Fix typo issues of outputstream.md","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ted12138","2022-02-24T01:58:57Z","2022-03-02T10:25:56Z"
"","4023","YARN-10983. Follow-up changes for YARN-10904","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-23T16:56:16Z","2022-03-02T10:25:07Z"
"","4020","HDFS-16480. Fix typo: indicies -> indices","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","qijiale76","2022-02-23T10:48:08Z","2022-02-28T13:12:54Z"
"","4011","YARN-10945. Add javadoc to all methods of AbstractCSQueue","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-02-22T15:49:55Z","2022-03-09T18:41:54Z"
"","3995","YARN-10049. FIFOOrderingPolicy Improvements","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-17T16:35:00Z","2022-03-10T21:16:11Z"
"","3986","YARN-11075. Explicitly declare serialVersionUID in LogMutation class","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-11T14:27:50Z","2022-02-17T18:49:05Z"
"","3985","trunk","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-11T14:26:35Z","2022-02-17T13:28:25Z"
"","3984","YARN-11075","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2022-02-11T14:22:47Z","2022-02-11T17:34:02Z"
"","3975","HDFS-16450.Give priority to releasing DNs with less free space","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-02-09T14:15:31Z","2022-03-14T06:19:05Z"
"","3974","Hdfs 16450","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-02-09T13:05:00Z","2022-02-09T13:07:37Z"
"","3973","HDFS-16450.Give priority to releasing DNs with less free space","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-02-09T12:11:34Z","2022-02-09T12:12:59Z"
"","3969","HDFS-16450.Give priority to releasing DNs with less free space","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-02-09T10:33:49Z","2022-02-09T10:47:23Z"
"","3946","YARN-11070. Minimum resource ratio is overridden by subsequent labels","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-31T11:20:36Z","2022-02-17T18:58:42Z"
"","3925","HDFS-16437 ReverseXML processor doesn't accept XML files without the SnapshotDiffSection","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2022-01-25T08:54:21Z","2022-01-25T09:04:43Z"
"","3874","YARN-11017. Unify node label access in queues","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-10T06:39:44Z","2022-02-22T14:04:55Z"
"","3867","YARN-10947. Simplify AbstractCSQueue#initializeQueueState","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-06T12:50:31Z","2022-02-17T20:05:27Z"
"","3859","YARN-10632. Make auto queue creation maximum allowed depth configurable","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-05T09:06:19Z","2022-01-11T06:39:00Z"
"","3858","YARN-10918. Simplify method: CapacitySchedulerQueueManager#parseQueue","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2022-01-05T09:05:47Z","2022-03-09T18:37:46Z"
"","3840","YARN-11025. [WIP] Implement distributed decommissioning","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-12-29T07:08:45Z","2021-12-29T12:10:21Z"
"","3812","YARN-11052. Improve code quality in TestRMWebServicesNodeLabels","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-16T21:54:54Z","2022-03-10T07:41:37Z"
"","3809","YARN-10427. Duplicate Job IDs in SLS output","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-16T15:07:01Z","2021-12-16T23:35:01Z"
"","3805","YARN-11050. Typo in method name: RMWebServiceProtocol#removeFromCluserNodeLabels","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-15T23:45:42Z","2021-12-20T16:23:14Z"
"","3804","YARN-11049. MutableConfScheduler is referred as plain String instead of class name","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-15T23:39:41Z","2022-03-07T08:02:15Z"
"","3800","YARN-10951. CapacityScheduler: Move all fields and initializer code that belongs to async scheduling to a new class","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-14T22:11:03Z","2021-12-16T23:18:42Z"
"","3799","YARN-11048. Add tests that shows how to delete config values with Mutation API","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-12-14T20:46:24Z","2021-12-16T17:44:34Z"
"","3795","YARN-11044. Fix TestApplicationLimits.testLimitsComputation() ineffective asserts.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-13T17:12:26Z","2021-12-15T23:01:32Z"
"","3794","YARN-11043. Clean up checkstyle warnings from YARN-11024/10907/10929","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-13T16:45:56Z","2021-12-14T21:01:32Z"
"","3766","YARN-11034. Add enhanced headroom in AllocateResponse","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","minni31","2021-12-08T11:41:52Z","2022-01-25T17:25:18Z"
"","3754","YARN-11028. Add metrics for container allocation latency","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-12-06T18:50:16Z","2022-02-01T18:59:55Z"
"","3744","YARN-10929. Do not use a separate config in legacy CS AQC.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-12-02T13:37:32Z","2021-12-14T13:58:00Z"
"","3727","YARN-11016. Queue weight is incorrectly reset to zero","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-11-26T08:41:33Z","2021-12-07T14:56:12Z"
"","3710","HADOOP-18021. Provide a public wrapper of Configuration#substituteVars","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-11-23T11:00:31Z","2021-12-03T16:44:58Z"
"","4039","HADOOP-18146: ABFS: Added changes for expect hundred continue header with append re…","1. Added Expect Hundred Continue Header with all append requests   -> Heavy load from a Hadoop cluster lead to high resource utilization at FE nodes. Investigations from the server side indicate payload buffering at Http.Sys as the cause. Payload of requests that eventually fail due to throttling limits are also getting buffered, as its triggered before FE could start request processing.  Approach: Client sends Append Http request with Expect header, but holds back on payload transmission until server replies back with HTTP 100. We add this header for all append requests so as to reduce.    2. Added account wise statistics collection in case of throttling  -> Currently the throttling statistics collection was not done at an account level, so if one account would get throttled, it might lead to some requests getting throttled for the other accounts as well. So to remove this kind of confusion, we have added support to collect throttling statistics at account level.","open","","anmolanmol1234","2022-02-28T13:21:00Z","2022-08-02T08:00:24Z"
"","3832","HADOOP-16748. Migrate to Python 3 and upgrade Yetus to 0.13.0 in branch-2.10","- Upgrade Yetus to 0.13.0 to support Python 3 for the release scripts. - Removed determine-flaky-tests-hadoop.py. - Temporarily disabled shelldocs check due to YETUS-1099.  Reviewed-by: Inigo Goiri  Reviewed-by: Mingliang Liu  (cherry picked from commit b9b49ed956e6fa9b55758f3d2c1b92ae2597cdbb)   Conflicts: 	dev-support/Jenkinsfile 	dev-support/bin/yetus-wrapper 	dev-support/docker/Dockerfile 	dev-support/docker/Dockerfile_aarch64    ### Description of PR  Backport HADOOP-16748 to branch-2.10.  ### How was this patch tested?  Not tested. The jenkins job will do.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-28T03:44:31Z","2021-12-28T07:05:59Z"
"","3906","HADOOP-18088. Replace log4j 1.x with reload4j.","- Replaced direct dependency on log4j/slf4j-log4j12 with reload4j/slf4j-reload4j. - Exclude any transitive dependencies on log4j/slf4j-log4j12 - Ban any log4j/slf4j-log4j12     ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","jojochuang","2022-01-20T03:05:04Z","2022-01-28T06:06:08Z"
"","3889","HDFS-15180.Split FsDatasetImpl lock to block Pool level","- DataNode performance bottleneck is mainly in datasetLock.DatasetLock not only influence performance and also influence stable. - We have split datasetLock to block pool level and volume level.And have stable run in 30000+ DataNode server in our cluster over than one year.It has big improvement for our client read write performance and stable. - This pr is split lock to block pool level.Another pr will split to volume level when this pr is merge.  Design doc https://drive.google.com/file/d/1eaE8vSEhIli0H3j2eDiPJNYuKAC0MFgu/view?usp=sharing https://issues.apache.org/jira/browse/HDFS-15382 https://issues.apache.org/jira/browse/HDFS-15382","closed","","MingXiangLi","2022-01-16T10:38:17Z","2022-01-23T07:42:02Z"
"","3941","HDFS-15382. Split one FsDatasetImpl lock to block pool grain locks.","- DataNode performance bottleneck is mainly in datasetLock.DatasetLock not only influence performance and also influence stable. - We have split datasetLock to block pool level and volume level.And have stable run in 30000+ DataNode server in our cluster over than one year.It has big improvement for our client read write performance and stable. - This pr is split lock to block pool level.Another pr will split to volume level when this pr is merge.  Design doc https://drive.google.com/file/d/1eaE8vSEhIli0H3j2eDiPJNYuKAC0MFgu/view?usp=sharing https://issues.apache.org/jira/browse/HDFS-15382","closed","","MingXiangLi","2022-01-28T13:04:56Z","2022-03-12T10:45:28Z"
"","3857","YARN-10922. Verify if legacy AQC works as documented","- Absolute resource can be configured for dynamically created queues with Legacy AQC, see the new test case in (YARN-11033). It is possible to configure the parent with Absolute resource and the child with Percentage mode using the leaf-queue-template, that leads to this bug (YARN-11010).    - QueueACL are partially supported, they can be configured for dynamically created queues with Legacy AQC leaf-queue-template. If a dynamically created queue exists already, then the ACLs configured with the leaf-queue-template applies for that queue. However at queue creation those ACLs are not in effect, that's why I call it partial support. Also the scheduler response doesn't show the ACLs in auto created queue's configuration, which is misleading.  Absolute Resource:    Question#1: Should I report a bug for a missing config validation for the Absolute+Percentage mix?  QueueACL:    Question#2-a: Should the documentation be updated with the QueueACLs current behaviour? IMHO it's hard to explain and I don't think this behaviour was intended.    Question#2-b: Should I report a bug to eliminate this partial support for QueueACL? The new Flexible Auto Queue Creation doesn't support it either, that works as it is documented.    Question#3b: Should I report bugs to improve the QueueACL support on the Legacy AQC? One for the queue creation and one for the scheduler response? IMHO if this is a Legacy feature then it would make sense to introduce this feature in the Flexible AQC instead.","closed","","tomicooler","2022-01-05T08:49:47Z","2022-03-07T10:09:02Z"
"","3708","HADOOP-17995. Stale record should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson","**Description of PR** As HADOOP-16947 problem with description. Stale SumAndCount also should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson. Ensure the DataNode JMX get SendPacketDownstreamAvgInfo Metrics is accurate.  Details: HADOOP-17995","closed","","haiyang1987","2021-11-23T10:04:00Z","2021-11-25T02:21:18Z"
"","3707","HADOOP-17995. Stale record should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson","**Description of PR** As HADOOP-16947 problem with description. Stale SumAndCount also should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson. Ensure the DataNode JMX get SendPacketDownstreamAvgInfo Metrics is accurate.  Details: HADOOP-17995","closed","","haiyang1987","2021-11-23T08:56:10Z","2021-11-23T09:58:07Z"
"","4601","HDFS-16467. Ensure Protobuf generated headers are included first","* This PR ensures that the Protobuf generated headers   are always included first, even when these headers   are included transitively. * This problem is specific to Windows only.    ### Description of PR We need to ensure that the Protobuf generated headers ([such as ClientNamenodeProtocol.pb.h](https://github.com/apache/hadoop/blob/cce5a6f6094cefd2e23b73d202cc173cf4fc2cc5/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/lib/connection/datanodeconnection.h#L23)) are included at the top. In other words, `*.ph.h` should be the first header files to be included in any of the .c/.cc/.h files. Otherwise, it causes symbol redefinition errors during compilation. Also, we need to ensure that Protobuf generated header files are the first ones to be included even in the case of `transitive inclusion` of header files.  This issue seems to be specific to Windows only. Not sure why the other platforms aren't running into this.  ### How was this patch tested? 1. Tested this locally by building on my Windows system. 2. Hadoop Jenkins CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2022-07-20T17:12:10Z","2022-07-23T17:50:21Z"
"","4004","HADOOP-18007. Use platform specific endpoints for CI report","* This is to find out the build   URL pattern.    ### Description of PR WIP  ### How was this patch tested? WIP  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","GauthamBanasandra","2022-02-20T14:49:02Z","2022-02-20T14:49:02Z"
"","4063","HADOOP-18155. Refactor tests in TestFileUtil (#4053)","(cherry picked from commit d0fa9b5775185bd83e4a767a7dfc13ef89c5154a)   Conflicts: 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java 	hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFileUtil.java    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2022-03-11T05:40:24Z","2022-03-14T00:40:17Z"
"","4304","HDFS-16456. EC: Decommission a rack with only one dn will fail when the rack number is equal with replication (#4126)","(cherry picked from commit cee8c62498f55794f911ce62edfd4be9e88a7361)   Conflicts: 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetworkTopology.java  Change-Id: I5da7693d6176315f126f1b5107de2bb100ee9778    ### Description of PR (HDFS-16456 was merged in trunk. It has conflict in branch-3.3 because of HDFS-15263. It looks like HDFS-15263 could potentially change the behavior and surprise users, so decided to backport without HDFS-15263)  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2022-05-12T05:25:11Z","2022-05-26T16:50:48Z"
"","4218","HADOOP-15783. [JDK10] TestSFTPFileSystem.testGetModifyTime fails.","(cherry picked from commit 93b0f540ed52f31c53fe4afc6d423e64c57c0af2)   ### Description of PR  trivial clean cherry-pick  ### How was this patch tested?  mvn test -Dtest=""TestSFTPFileSystem""  ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.fs.sftp.TestSFTPFileSystem [INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.105 s - in org.apache.hadoop.fs.sftp.TestSFTPFileSystem [INFO] [INFO] Results: [INFO] [INFO] Tests run: 11, Failures: 0, Errors: 0, Skipped: 0 ```","closed","","xinglin","2022-04-22T03:38:48Z","2022-04-22T06:43:01Z"
"","4159","YARN-10553. Refactor TestDistributedShell (#2581)","(cherry picked from commit 890f2da624465473a5f401a3bcfc4bbd068289a1)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDistributedShell.java    ### Description of PR  Backport YARN-10553 to branch-3.3. Fixed conflicts in TestDSWithMultipleNodeManager because YARN-10360 is only in trunk.  ### How was this patch tested?  Manually ran the related tests but TestDSTimelineV20 failed. I'll investigate why the test is failing:  ``` [ERROR] Tests run: 6, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 422.012 s <<< FAILURE! - in org.apache.hadoop.yarn.applications.distributedshell.TestDSTimelineV20 [ERROR] testDSShellWithoutDomain(org.apache.hadoop.yarn.applications.distributedshell.TestDSTimelineV20)  Time elapsed: 73.413 s  <<< FAILURE! java.lang.AssertionError 	at org.junit.Assert.fail(Assert.java:87) 	at org.junit.Assert.assertTrue(Assert.java:42) 	at org.junit.Assert.assertTrue(Assert.java:53) 	at org.apache.hadoop.yarn.applications.distributedshell.TestDSTimelineV20.verifyEntityTypeFileExists(TestDSTimelineV20.java:478) ... ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2022-04-11T07:37:15Z","2022-04-13T03:22:21Z"
"","3833","HADOOP-16054. Update Dockerfile to use Bionic (branch-2.10)","(cherry picked from commit 81d8b71534645a2109a037115fb955351edfbf64)   Conflicts: 	dev-support/docker/Dockerfile    ### Description of PR  HADOOP-16054: Upgrade the build environment from Xenial to Bionic.  ### How was this patch tested?  Ran `mvn install -Pnative -Dskiptests` successfully.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-28T05:19:14Z","2021-12-28T06:25:48Z"
"","4579","HADOOP-13386. Upgrade Avro to 1.9.2 (#3990)","#3990 on branch-3.3","closed","","steveloughran","2022-07-18T10:04:46Z","2022-07-19T09:41:39Z"
"","4580","HADOOP-13386. Upgrade Avro to 1.9.2 (#3990)","#3990 for 3.3.4","closed","","steveloughran","2022-07-18T10:06:45Z","2022-07-19T03:23:27Z"
"","4046","YARN-11076. Upgrade jQuery version in Yarn UI2.","### Notes  1. bower-shrinkwrap.json is outdated  I updated the parts in the **bower-shrinkwrap.json** manually as described in the README.md.  I had to add the following to the .bowerrc ``` ""resolvers"": [     ""bower-shrinkwrap-resolver""   ] ``` Unfortunately the other packages are outdated in that json, so the build fail.   2. jquery-ui is updated to 1.12.1 (the same version is in the hadoop-common repo), the 1.13.1 version is not available in that jquery-ui shims repo   3. bootstrap 3.3.6 depended on older jQuery so it had to be updated   ### How was this patch tested?  I did a **short** smoke test on the UI2.","closed","","tomicooler","2022-03-03T13:32:59Z","2022-03-03T15:12:55Z"
"","4110","YARN-11073. Avoid unnecessary preemption for tiny queues under certain corner cases","### Description of PR (YARN-11073) When running a Hive job in a low-capacity queue on an idle cluster, preemption kicked in to preempt job containers even though there's no other job running and competing for resources.   Let's take this scenario as an example: ``` cluster resource :  queue_low: min_capacity 1% queue_mid: min_capacity 19% queue_high: min_capacity 80% CapacityScheduler with DRF ``` During the fifo preemption candidates selection process, the preemptableAmountCalculator needs to first ""computeIdealAllocation"" which depends on each queue's guaranteed/min capacity. A queue's guaranteed capacity is currently calculated as ""Resources.multiply(totalPartitionResource, absCapacity)"", so the guaranteed capacity of queue_low is: `queue_low:  = `, but since the Resource object takes only Long values, these Doubles values get casted into Long, and then the final result becomes ``  Because the guaranteed capacity of queue_low is 0, its normalized guaranteed capacity based on active queues is also 0 based on the current algorithm in ""resetCapacity"". This eventually leads to the continuous preemption of job containers running in queue_low.   In order to work around this corner case, ""resetCapacity"" needs to consider a couple new scenarios:   if the sum of absoluteCapacity/minCapacity of all active queues is zero, we should normalize their guaranteed capacity evenly: `1.0f / num_of_queues`  if the sum of pre-normalized guaranteed capacity values (MB or VCores) of all active queues is zero, meaning we might have several queues like queue_low whose capacity value got casted into 0, we should normalize evenly as well like the first scenario (if they are all tiny, it really makes no big difference, for example, 1% vs 1.2%).  if one of the active queues has a zero pre-normalized guaranteed capacity value but its absoluteCapacity/minCapacity is not zero, then we should normalize based on the weight of their configured queue absoluteCapacity/minCapacity. This is to make sure queue_low gets a small but fair normalized value when queue_mid is also active.  `minCapacity / (sum_of_min_capacity_of_active_queues)`   ### How was this patch tested? The patch was tested on a small cluster with queues configured to be same as the scenario described above, verified that  - containers running in a low-capacity queue didn't get preempted when the cluster is idle - preemption kicked in properly in the low-capacity queue when cluster is busy (heavy usage in high-capacity queues)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?  @aajisaka","closed","","jchenjc","2022-03-26T11:06:49Z","2022-05-13T16:13:02Z"
"","3845","HADOOP-18061. Update the year to 2022.","### Description of PR Year change in POM:2022","closed","","ayushtkn","2022-01-01T11:29:34Z","2022-01-04T02:42:32Z"
"","3802","YARN-11047. ResourceManager and NodeManager unable to connect to Hbase when ATSv2 is enabled","### Description of PR Yarn ResourceManager and NodeManager in the current state are not able to interact with HBase cluster because of the incompatible guava and guice dependencies being used in the classpath.  ``` 2021-12-14 19:26:30,511 INFO  [IPC Server listener on 8020] ipc.Server (Server.java:run(1479)) - IPC Server listener on 8020: starting 2021-12-14 19:26:30,960 INFO  [Listener at 0.0.0.0/8020] webproxy.ProxyCA (ProxyCA.java:createCert(183)) - Created Certificate for OU=YARN-3eb03ade-472e-45ec-a274-7a11be9c791e 2021-12-14 19:26:30,992 INFO  [Listener at 0.0.0.0/8020] recovery.RMStateStore (RMStateStore.java:transition(647)) - Storing CA Certificate and Private Key 2021-12-14 19:26:30,992 INFO  [Listener at 0.0.0.0/8020] resourcemanager.ResourceManager (ResourceManager.java:transitionToActive(1525)) - Transitioned to active state 2021-12-14 19:26:33,345 WARN  [pool-28-thread-1] storage.TimelineStorageMonitor (TimelineStorageMonitor.java:run(95)) - Got failure attempting to read from HBase, assuming Storage is down java.lang.RuntimeException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.client.AbstractClientScanner$1.hasNext(AbstractClientScanner.java:95)         at org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader.readEntities(TimelineEntityReader.java:283)         at org.apache.hadoop.yarn.server.timelineservice.storage.HBaseStorageMonitor.healthCheck(HBaseStorageMonitor.java:77)         at org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor$MonitorThread.run(TimelineStorageMonitor.java:89)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:260)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:233)         at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:394)         at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:368)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:143)         at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)         ... 3 more Caused by: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.net.Address.getHostName(Address.java:72)         at org.apache.hadoop.hbase.net.Address.toSocketAddress(Address.java:57)         at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:576)         at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:37250)         at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:405)         at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)         at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:231)         ... 7 more ```  Once we allow RM and NM to use Timeline service specific guava version, it fails due to incompatibilities b/ guava and guice versions because hbase compatible guava is not guice compatible guava for Yarn:  ``` 2021-12-14 20:28:10,405 ERROR [main] resourcemanager.ResourceManager (MarkerIgnoringBase.java:error(159)) - Error starting ResourceManager java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V 	at com.google.inject.TypeLiteral.getParameterTypes(TypeLiteral.java:269) 	at com.google.inject.spi.InjectionPoint.forMember(InjectionPoint.java:113) 	at com.google.inject.spi.InjectionPoint.(InjectionPoint.java:72) 	at com.google.inject.spi.InjectionPoint.forMethod(InjectionPoint.java:316) 	at com.google.inject.internal.ProviderMethodsModule.createProviderMethod(ProviderMethodsModule.java:293) 	at com.google.inject.internal.ProviderMethodsModule.getProviderMethods(ProviderMethodsModule.java:135) 	at com.google.inject.internal.ProviderMethodsModule.configure(ProviderMethodsModule.java:105) 	at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:347) 	at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:356) 	at com.google.inject.AbstractModule.install(AbstractModule.java:103) 	at com.google.inject.servlet.ServletModule.configure(ServletModule.java:49) 	at com.google.inject.AbstractModule.configure(AbstractModule.java:61) 	at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:347) 	at com.google.inject.spi.Elements.getElements(Elements.java:104) 	at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:137) 	at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:105) 	at com.google.inject.Guice.createInjector(Guice.java:87) 	at com.google.inject.Guice.createInjector(Guice.java:69) 	at com.google.inject.Guice.createInjector(Guice.java:59) 	at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:420) 	at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:468) 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1443) 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1552) 	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:195) 	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1753) ```  Hence, we need to keep both guice and guava compatible with each other and ensure guava is also compatible with HBase. Changes on this PR would allow Yarn to use guice and guava versions compatible with HBase and not the ones being derived from hadoop-project.  ### How was this patch tested? Tested locally. Triggered MapReduce sample WordCount job. No issues encountered with RM and NM. Data is also flowing from Yarn to HBase tables:       ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-15T09:14:35Z","2021-12-20T10:02:58Z"
"","4436","YARN-9971.YARN Native Service HttpProbe logs THIS_HOST in error messages","### Description of PR YARN Native Service HttpProbe logs THIS_HOST in error messages * JIRA: YARN-9971   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-06-13T12:50:19Z","2022-06-22T04:08:20Z"
"","4152","HADOOP-18196. Remove replace-guava from replacer plugin","### Description of PR While running the build, realized that all replacer plugin executions run only after ""banned-illegal-imports"" enforcer plugin.  For instance,  ``` [INFO] --- maven-enforcer-plugin:3.0.0:enforce (banned-illegal-imports) @ hadoop-cloud-storage --- [INFO]  [INFO] --- replacer:1.5.3:replace (replace-generated-sources) @ hadoop-cloud-storage --- [INFO] Skipping [INFO]  [INFO] --- replacer:1.5.3:replace (replace-sources) @ hadoop-cloud-storage --- [INFO] Skipping [INFO]  [INFO] --- replacer:1.5.3:replace (replace-guava) @ hadoop-cloud-storage --- [INFO] Replacement run on 0 file. [INFO] ```    Hence, if our source code uses com.google.common, `banned-illegal-imports` will cause the build failure and replacer plugin would not even get executed.  We should remove it as it is only redundant execution step.  ### How was this patch tested? Local build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-04-08T13:10:09Z","2022-04-15T14:24:36Z"
"","4630","HDFS-16692. Add detailed scope info in NotEnoughReplicas Reason log","### Description of PR When we write some ec data from clients that are not in hdfs cluster, there is a lot of INFO log output, as blew: ``` 2022-07-26 15:50:40,973 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=17} 2022-07-26 15:50:40,974 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=18} 2022-07-26 15:50:40,974 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=17} 2022-07-26 15:50:40,975 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=18} 2022-07-26 15:50:40,975 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=17} 2022-07-26 15:50:40,976 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=18} 2022-07-26 15:50:40,976 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=18} 2022-07-26 15:50:40,977 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=18} 2022-07-26 15:50:40,977 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=19} 2022-07-26 15:50:40,977 INFO  blockmanagement.BlockPlacementPolicy (BlockPlacementPolicyDefault.java:chooseRandom(912)) - Not enough replicas was chosen. Reason: {NO_REQUIRED_STORAGE_TYPE=1, TOO_MANY_NODES_ON_RACK=3} ```  I feel that we should add detailed scope info in this log to show the scope that we cannot select any good nodes from.   It will be convenient for us to quickly locate the scope in which the node cannot be selected for what reason from the log.","open","","ZanderXu","2022-07-26T11:41:47Z","2022-08-01T23:34:53Z"
"","3819","HDFS-16389.Improve NNThroughputBenchmark test mkdirs.","### Description of PR When using the NNThroughputBenchmark test to create a large number of directories, an exception message that the queue is out of bounds will be prompted.  ### How was this patch tested? For testing, there is not much pressure.","closed","","jianghuazhu","2021-12-20T06:04:13Z","2022-04-18T03:21:52Z"
"","4048","HDFS-16494.Removed reuse of AvailableSpaceVolumeChoosingPolicy#initLocks().","### Description of PR When using the default constructor to build the AvailableSpaceVolumeChoosingPolicy, initLocks() is used twice, which is actually unnecessary, the purpose of this pr is to avoid this from happening. Details: HDFS-16494  ### How was this patch tested? Not too stressful for testing. Because this class is relatively mature.","closed","","jianghuazhu","2022-03-04T10:40:08Z","2022-03-16T12:45:13Z"
"","4333","HDFS-16584.Record StandbyNameNode information when Balancer is running.","### Description of PR When the Balancer is running, it is allowed to get block data from the StandbyNameNode, we should record more detailed information, such as the host related to the StandbyNameNode. Details: HDFS-16584  ### How was this patch tested? When the Balancer is running, the Standby NameNode information should be checked.","closed","","jianghuazhu","2022-05-19T13:06:15Z","2022-05-23T02:08:30Z"
"","3701","YARN-11011. Make YARN Router throw Exception to client clearly","### Description of PR When router submits job to yarn, the job is rejected by yarn for some reason. Yarn router just throw exception to log and return not enough info to client.  The router always throws the following non-obvious error to client: Exception in thread ""main"" org.apache.hadoop.yarn.server.federation.policies.exceptions.FederationPolicyException: No active SubCluster available to submit the request.  To make it easy for the user to locate the cause of a job submission failure, we need to make Router throw the true cause of the error to the client clearly.","open","","luoyuan3471","2021-11-22T08:55:31Z","2021-11-22T10:25:55Z"
"","4660","HDFS-16703. Enable RPC Timeout for some protocols of NameNode.","### Description of PR When I read some code about protocol, I found that only ClientNamenodeProtocolPB proxy with RPC timeout, other protocolPB proxy without RPC timeout, such as RefreshAuthorizationPolicyProtocolPB, RefreshUserMappingsProtocolPB, RefreshCallQueueProtocolPB, GetUserMappingsProtocolPB and NamenodeProtocolPB.  If proxy without rpc timeout,  it will be blocked for a long time if the NN machine crash or bad network during writing or reading with NN.   So I feel that we should enable RPC timeout for all ProtocolPB.","open","","ZanderXu","2022-07-30T03:10:35Z","2022-08-03T08:21:30Z"
"","4670","HDFS-16710. Remove redundant throw exceptions in org.apache.hadoop.hdfs.server.namenode package","### Description of PR When I read some class about HDFS NameNode, I found there are many redundant throw exception in org.apahce.hadoop.hdfs.server.namenode package, such as: ``` public synchronized void transitionToObserver(StateChangeRequestInfo req)     throws ServiceFailedException, AccessControlException, IOException {   checkNNStartup();   nn.checkHaStateChange(req);   nn.transitionToObserver(); }  ```  Because ServiceFailedException and AccessControlException is subClass of IOException, so I feel that ServiceFailedException and AccessControlException are redundant, so we can remove it to make code clearer, such as: ``` public synchronized void transitionToObserver(StateChangeRequestInfo req)     throws IOException {   checkNNStartup();   nn.checkHaStateChange(req);   nn.transitionToObserver(); }   ```","open","","ZanderXu","2022-08-01T15:54:56Z","2022-08-03T01:44:36Z"
"","4662","HDFS-16705. RBF: Support healthMonitor timeout configurable and cache NN and client proxy in NamenodeHeartbeatService","### Description of PR When I read NamenodeHeartbeatService.class of RBF, I feel that there are somethings we can do for NamenodeHeartbeatService.class. - Cache NameNode Protocol and Client Protocol to avoid creating a new proxy every time - Supports healthMonitorTimeout configuration - Format code of getNamenodeStatusReport to make it clearer","open","","ZanderXu","2022-07-30T09:27:29Z","2022-08-02T19:04:36Z"
"","3806","HDFS-16386.Reduce DataNode load when FsDatasetAsyncDiskService is working.","### Description of PR When FsDatasetAsyncDiskService is working, if the DataNode has a lot of disks, this will cause a higher load on the DataNode, for example, a lot of memory is used. This phenomenon will affect the stability of the DataNode. Details: HDFS-16386  ### How was this patch tested? For testing, there is little pressure.","closed","","jianghuazhu","2021-12-16T10:00:50Z","2022-05-25T05:00:51Z"
"","3807","HDFS-16387.[FGL]Access to Create File is more secure.","### Description of PR When creating files through RPC, sometimes a deadlock is triggered. The purpose of this pr is to make it more secure. Details: HDFS-16387  ### How was this patch tested? Use the existing test, that's ok.","open","","jianghuazhu","2021-12-16T12:49:11Z","2021-12-17T02:00:13Z"
"","4303","MAPREDUCE-7378. Change job temporary dir name to avoid delete by other jobs","### Description of PR when concurrent write data to a path , the finished job will delete the ""_temporary"" dir cause the others failed, so we need to make they have a separate dir to write datas to a path  JIRA: MAPREDUCE-7378  ### How was this patch tested? passed the all the hadoop tests  ### For code changes: add a new param as the job temporary dir with the job id","closed","","klaus-xiong","2022-05-12T02:46:33Z","2022-05-24T08:51:19Z"
"","3861","HDFS-16316.Improve DirectoryScanner: add regular file check related block.","### Description of PR When blk_xxxx and blk_xxxx.meta are not regular files, they will have adverse effects on the cluster, such as errors in the calculation space and the possibility of failure to read data. For this type of block, it should not be used as a normal block file. Details: HDFS-16316   ### How was this patch tested? Need to verify whether a file is a real regular file.","closed","","jianghuazhu","2022-01-05T10:22:45Z","2022-02-22T02:16:00Z"
"","4659","HDFS-16700. RBF: Record the real client IP carried by the Router in the NameNode log","### Description of PR When applying RBF, the ip recorded in the log file saved in the NameNode is still the Router. The real client ip should be logged. Details: HDFS-16700  ### How was this patch tested? When hadoop.caller.context.enabled=true, you should see the client ip recorded in the NameNode log file.","open","","jianghuazhu","2022-07-29T14:57:13Z","2022-08-02T21:38:14Z"
"","4323","HDFS-16582. Expose aggregate latency of slow node as perceived by the reporting node","### Description of PR When any datanode is reported to be slower by another node, we expose the slow node as well as the reporting nodes list for the slow node. However, we don't provide latency numbers of the slownode as reported by the reporting node. Having the latency exposed in the metrics would be really helpful for operators to keep a track of how far behind a given slow node is performing compared to the rest of the nodes in the cluster.  The operator should be able to gather aggregated latencies of all slow nodes with their reporting nodes in Namenode metrics.  ### How was this patch tested? Dev cluster and UT.     ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-05-18T04:27:33Z","2022-05-21T01:45:09Z"
"","4028","HDFS-16481. Provide support to set Http and Rpc ports in MiniJournalCluster","### Description of PR We should provide support for clients to set Http and Rpc ports of JournalNodes in MiniJournalCluster.  ### How was this patch tested? Using a downstreamer application as well as using UT.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-02-24T07:44:07Z","2022-03-04T16:48:26Z"
"","3944","HADOOP-18098. Basic verification for the release candidate vote","### Description of PR We should provide script for the basic sanity of Hadoop release candidates. It should include:  - Signature - Checksum - Rat check - Build from src - Build tarball from src  Although we can include unit test as well, but overall unit test run is going to be significantly higher, and precommit Jenkins builds provide better view of UT sanity.  ### How was this patch tested? Locally  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-01-29T06:59:04Z","2022-02-07T01:34:58Z"
"","4002","HDFS-16461. Expose JournalNode storage info in the jmx metrics","### Description of PR We should expose the list of storage info of JournalNode's journals (including layout version, namespace id, cluster id and creation time of the FS state) in the jmx metrics.  ### How was this patch tested? Local dev cluster and UT.    Updated screenshot:     ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-02-20T05:58:33Z","2022-02-22T04:04:48Z"
"","3711","HDFS-16350. Datanode start time should be set after RPC server starts successfully","### Description of PR We set start time of Datanode when the class is instantiated but it should be ideally set only after RPC server starts and RPC handlers are initialized to serve client requests.  ### How was this patch tested? Tested locally. UI screenshot:     ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-23T11:16:15Z","2021-11-25T02:23:26Z"
"","4118","YARN-11104. Support application resource limit when scheduling in FairScheduler","### Description of PR We can enable this feature by setting `yarn.scheduler.fair.app-submission-limit-enabled` to true, and control the maximum resources ratio an application can use by adjusting `` of each queue.   It resolves the problem applications starve when a single application takes all resources of the queue.  It can be quite effective when all tasks of a queue need to be treated fairly.  ### How was this patch tested? UTs and manual","open","","Deegue","2022-03-29T17:07:03Z","2022-03-29T23:10:01Z"
"","3987","HADOOP-18122. ViewFileSystem fails on determining owning group when primary group doesn't exist for user","### Description of PR ViewFileSystem should not fail on determining owning group when primary group doesn't exist for user  ### How was this patch tested? new unit test; run existing unit tests `mvn test -Dtest=TestViewFileSystem*`  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cheyu2022","2022-02-11T17:47:52Z","2022-04-22T17:56:13Z"
"","4215","HADOOP-18215. Enhance WritableName to be able to return aliases for classes that use serializers","### Description of PR Very simple one-line change that allows users to optionally call addName for key/value classes that may not inherit Writable (instead relying on serialization).   ### How was this patch tested? Added two tests: 1. Simple unit test in TestWritableName 2. More of an integration test in TestSequenceFile which verifies that it works to read and write with shimmed serializable classes.  This patch has also been deployed in our environment for 2 months.","open","","bbeaudreault","2022-04-21T23:44:38Z","2022-07-16T20:10:45Z"
"","4097","YARN-11097. Support refresh load limit on ResourceManager","### Description of PR Use `yarn rmadmin -refreshLoadLimitConfiguration` to refresh load limit related configurations.  ### How was this patch tested? UTs and manual","open","","Deegue","2022-03-23T15:17:15Z","2022-03-23T18:18:12Z"
"","4318","HADOOP-18237. Upgrade Apache Xerces Java to 2.12.2","### Description of PR Upgraded Apache Xerces Java to 2.12.2 due to handle vulnerability [CVE-2022-23437](https://nvd.nist.gov/vuln/detail/CVE-2022-23437) * JIRA: HADOOP-18237   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-17T03:55:57Z","2022-06-20T17:10:54Z"
"","4328","HADOOP-18240. Upgrade Yetus to 0.14.0","### Description of PR Upgrade Yetus to 0.14.0 * JIRA: HADOOP-18240   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-18T17:18:08Z","2022-05-25T08:31:32Z"
"","3798","HDFS-16384. Upgrade Netty to 4.1.72.Final","### Description of PR Upgrade Netty to newest available version  ### How was this patch tested? Ran unit tests  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tamaashu","2021-12-14T20:13:33Z","2021-12-16T13:26:05Z"
"","4196","HADOOP-17551. Upgrade maven-site-plugin to 3.11.0","### Description of PR Upgrade maven-site-plugin to 3.11.0  * JIRA: HADOOP-17551","closed","","ashutoshcipher","2022-04-18T17:52:02Z","2022-04-21T13:16:13Z"
"","4260","YARN-11092. Upgrade jquery ui to 1.13.1","### Description of PR Upgrade jquery ui to 1.13.1 due to handle vulnerabilities CVE-2021-41182, CVE-2021-41183, CVE-2021-41184 * JIRA: YARN-11092   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-03T13:34:36Z","2022-05-16T14:11:18Z"
"","4562","HDFS-16652. Upgrade jquery datatable version references to v1.10.19","### Description of PR Upgrade jquery datatable version references to v1.10.19  ### How was this patch tested? Verified NameNode dfshealth.html and explorer.html ![image](https://user-images.githubusercontent.com/13732639/178783155-8ed87302-f8a9-4992-958b-2b64c0562893.png) ![image](https://user-images.githubusercontent.com/13732639/178783362-80982ea7-43a8-4b29-8424-c1c0b0f939c7.png)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dmmkr","2022-07-13T16:53:25Z","2022-07-14T12:57:07Z"
"","3722","HADOOP-18025. Upgrade HBase version to 1.7.1 for hbase1 profile","### Description of PR Upgrade HBase version to 1.7.1 for hbase1 profile. The current version is quite old. Also, replace old deprecated APIs of Scan with better alternatives.  ### How was this patch tested? Verified with some local testing. All CRUD unit tests are tested.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-11-25T11:46:17Z","2021-12-02T02:40:39Z"
"","3749","HADOOP-18033. Upgrade fasterxml Jackson to 2.13.0","### Description of PR Upgrade fasterxml Jackson to 2.13.0.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-12-04T15:39:51Z","2021-12-08T07:52:22Z"
"","4449","HADOOP-18297. Upgrade dependency-check-maven to 7.1.1","### Description of PR Upgrade dependency-check-maven to 7.1.1 The OWASP dependency-check-maven Plugin version has corrected various false positives in 7.1.1. We can upgrade to it.  https://github.com/jeremylong/DependencyCheck/milestone/45?closed=1  * JIRA: HADOOP-18297   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-06-17T01:33:33Z","2022-07-05T09:00:10Z"
"","4203","HADOOP-18210. Upgrade cglib to 3.3.0","### Description of PR Upgrade cglib to 3.3.0  * JIRA: HADOOP-18210","closed","","ashutoshcipher","2022-04-19T15:58:20Z","2022-05-11T00:09:24Z"
"","3947","HADOOP-18099. Upgrade bundled Tomcat to 8.5.75","### Description of PR Upgrade bundled Tomcat to 8.5.75  * JIRA: HADOOP-18099","closed","","ashutoshcipher","2022-01-31T14:40:02Z","2022-02-01T05:27:27Z"
"","4182","HDFS-16536. Updating NameNodeLayoutVersion to -66 in TestOfflineImage…","### Description of PR Updating NameNodeLayoutVersion to -66 in TestOfflineImageViewer  * JIRA: HDFS-16536","closed","","ashutoshcipher","2022-04-16T21:58:04Z","2022-04-18T07:05:55Z"
"","4515","YARN-10287.Update scheduler-conf corrupts the CS configuration when removing queue which is referred in queue mapping","### Description of PR Update scheduler-conf corrupts the CS configuration when removing queue which is referred in queue mapping  JIRA: YARN-10287  ### How was this patch tested?  Added Unit test and updated current unit tests   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-29T23:28:07Z","2022-07-02T16:28:57Z"
"","3836","HDFS-14099. Unknown frame descriptor when decompressing multiple frames","### Description of PR Unknown frame descriptor when decompressing multiple frames in ZStandardDecompressor  * JIRA: https://issues.apache.org/jira/browse/HDFS-14099","closed","","ashutoshcipher","2021-12-28T08:20:24Z","2021-12-28T15:51:51Z"
"","4492","YARN-9822.TimelineCollectorWebService#putEntities blocked when ATSV2 HBase is down","### Description of PR TimelineCollectorWebService#putEntities blocked when ATSV2 HBase is down  JIRA: YARN-9822  ### How was this patch tested? Unit test is added  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-23T01:10:08Z","2022-06-28T04:02:07Z"
"","4059","HADOOP-16298. Manage/Renew delegation tokens for externally scheduled jobs","### Description of PR This PR provides a means to reload authentication credentials from updated Hadoop delegation tokens similar to how one would reload credentials from an updated kerberos ticket.  ### How was this patch tested? This patch has been tested via: * The unit tests * Via long-running (modified to use the refresh) HBase 1.x clients. Long-running means through many delegation token refresh cycles * Further it has been used in non-long running Spark workloads which were delegation token based * TODO: I would like to figure out how I can write a unit-test to validate [the reload for an HA HDFS|https://github.com/apache/hadoop/compare/trunk...cbaenziger:HADOOP-16298?expand=1#diff-e160685d647b1420f6c56264302c0e1c82ead52e75a31d91fb2a12f0ce9261c4R465-R481] but I can not find what seems like the correct layer to put such a test. I do not expect HDFS should be used in a `hadoop-common` layer test; is it okay to test UGI's behavior in `hadoop-hdfs-project`?  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [N/A] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [N/A] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cbaenziger","2022-03-10T06:17:09Z","2022-03-10T09:53:00Z"
"","4262","HADOOP-17725. Keep MSI tenant ID and client ID optional (ADDENDUM)","### Description of PR This PR is the same as https://github.com/apache/hadoop/pull/3788 but rebased onto a more recent version of the trunk branch.  This make tenant and client ID optional when getting an Azure token from the Azure Metadata Service since they can be inferred from the VM the calls is made from.   For some reason the integration tests were failing in @virajjasani 's version (couldn't get a useful error message) but once I rebased onto `trunk` it worked. I ran the integration tests from an azure VM so that Azure's metadata service can infer the tenant and client ID (which is why they are optional in `MsiTokenProvider`)  ### How was this patch tested?  I ran the integration tests:  ``` /hadoop/hadoop-tools/hadoop-azure# mvn -T 1C clean verify -Dtest=none -Dit.test=ITest* [...] [INFO] Results: [INFO] [WARNING] Tests run: 1559, Failures: 0, Errors: 0, Skipped: 1542 [INFO] [INFO] [INFO] --- maven-enforcer-plugin:3.0.0:enforce (depcheck) @ hadoop-azure --- [INFO] [INFO] --- maven-failsafe-plugin:3.0.0-M1:verify (default) @ hadoop-azure --- [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  36.325 s [INFO] Finished at: 2022-05-06T15:23:32Z [INFO] ------------------------------------------------------------------------ ```  Let me know if something's missing, this is the first PR I do in this repo.","open","","CLevasseur","2022-05-04T10:00:55Z","2022-05-06T15:51:27Z"
"","4342","HDFS-16588.Backport HDFS-16584 to branch-3.3.","### Description of PR This issue has been dealt with in trunk and needs to be backported again to branch 3.3 or another active branch. This is the purpose of this pr. Details: HDFS-16588  ### How was this patch tested? This can be based on the tests of branch-3.3.","closed","","jianghuazhu","2022-05-23T07:53:27Z","2022-05-24T15:48:20Z"
"","4404","HDFS-16621.Remove unused JNStorage#getCurrentDir()","### Description of PR There is no use of getCurrentDir() anywhere in JNStorage, we should remove it. Details: HDFS-16621  ### How was this patch tested? For testing, there is not much pressure.","closed","","jianghuazhu","2022-06-05T14:21:55Z","2022-06-10T06:42:43Z"
"","4609","YARN-11213. DecommissioningNodesWatcher#logDecommissioningNodesStatus…","### Description of PR There is a calculation in DecommissioningNodesWatcher#logDecommissioningNodesStatus to calculate app elapsed time: ``` (mclock.getTime() - rmApp.getStartTime()) / 1000 ``` But mclock.getTime() is generated from System.nanoTime() / NANOSECONDS_PER_MILLISECOND,  rmApp.getStartTime() is generated from System.currentTimeMillis(), they cannot be compared.  This calculation should be: ``` (System.currentTimeMillis() - rmApp.getStartTime()) / 1000 ```","open","","liubin101","2022-07-22T09:22:48Z","2022-07-22T12:54:52Z"
"","4480","HDFS-16638. Add isDebugEnabled check for debug blockLogs in BlockManager","### Description of PR There are lots of concatenating Strings using `blockLog.debug` in `BlockManager`.  ### How was this patch tested? exist UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","cxzl25","2022-06-21T13:55:30Z","2022-07-03T21:06:29Z"
"","4272","HADOOP-18228. Update hadoop-vote to use HADOOP_RC_VERSION dir","### Description of PR The recent changes in release script requires a minor change in hadoop-vote to use Hadoop RC version dir before verifying signature and checksum of .tar.gz files.  ### How was this patch tested? Tested locally against Hadoop 3.3.3 RC0.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-05-06T18:51:03Z","2022-05-16T14:34:40Z"
"","4070","HADOOP-18154. S3A Authentication to support WebIdentity","### Description of PR The PR addresses a requirement to comply with AWS security concept [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA) while operating a service that isn't based on Apache Spark and that runs inside Amazon Elastic Kubernetes Service (EKS).  The code change consists in adding a new credentials provider class `org.apache.hadoop.fs.s3a.OIDCTokenCredentialsProvider` to the module [hadoop-aws](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).  ### How was this patch tested? No new unit-test or integration-test was created on-purpose. The patch was ""only"" tested based on [Hadoop release 2.10.1](https://github.com/apache/hadoop/tree/rel/release-2.10.1), as part of our specific use-case based on [Delta sharing service](https://github.com/delta-io/delta-sharing) v0.4.0 along with the following Hadoop configuration (core-site.xml): ```           fs.s3a.aws.credentials.provider     org.apache.hadoop.fs.s3a.OIDCTokenCredentialsProvider           fs.s3a.jwt.path     /var/run/secrets/eks.amazonaws.com/serviceaccount/token           fs.s3a.assumed.role.arn     my_iam_role_arn           fs.s3a.assumed.role.session.name     my_iam_session_name             fs.s3a.server-side-encryption-algorithm       SSE-KMS             fs.s3a.server-side-encryption.key       my_kms_key_id           ```  ### For code changes: - [X] The title or this PR starts with the corresponding JIRA issue 'HADOOP-18154' - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] No new dependency was added to the code.","open","","jclarysse","2022-03-14T15:33:48Z","2022-07-20T14:28:41Z"
"","4049","Hadoop 18154 branch 2.10.1","### Description of PR The PR addresses a requirement to comply with AWS security concept [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA) while operating [Delta sharing](https://github.com/delta-io/delta-sharing) in Amazon Elastic Kubernetes Service (EKS). The code change consists in adding a new credentials provider class `org.apache.hadoop.fs.s3a.OIDCTokenCredentialsProvider` to the module [hadoop-aws](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html) in [Hadoop release 2.10.1](https://github.com/apache/hadoop/tree/rel/release-2.10.1). In addition, the dependency aws-java-sdk-bundle-1.11.271 was upgraded to its latest version 1.12.167 as [AWS WebIdentityTokenCredentialsProvider class](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/WebIdentityTokenCredentialsProvider.html%E2%80%A6) was not yet available in original version.  ### How was this patch tested? No new unit-test or integration-test was created on-purpose. The patch was ""only"" tested as part of our specific use-case, using Delta sharing server 0.4.0 with the following Hadoop configuration (core-site.xml): ```           fs.s3a.aws.credentials.provider     org.apache.hadoop.fs.s3a.OIDCTokenCredentialsProvider           fs.s3a.jwt.path     /var/run/secrets/eks.amazonaws.com/serviceaccount/token           fs.s3a.role.arn     my_iam_role_arn           fs.s3a.session.name     my_iam_session_name             fs.s3a.server-side-encryption-algorithm       SSE-KMS             fs.s3a.server-side-encryption.key       my_kms_key_id           ```  ### For code changes: - [X] The title or this PR starts with the corresponding JIRA issue 'HADOOP-18154' - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] No new dependency was added to the code, however one dependency was upgraded.","closed","","jclarysse","2022-03-05T15:37:49Z","2022-04-27T18:20:33Z"
"","3979","HADOOP-18119. ViewFileSystem#getUri should return a URI with an empty path component","### Description of PR The goal here is that the semantics of FileSystem#getURI() remains the same as before when using ViewFileSystem.   We should assume that the URI returned by FileSystem#getURI() doesn't have a Path component (i.e., path component is null). DistributedFileSystem follows this pattern. However, ViewFileSystem add ""/"" as the path component in the URI returned.  ### How was this patch tested? mvn test -Dtest=TestViewFileSystem*  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cheyu2022","2022-02-10T19:14:26Z","2022-02-14T23:27:38Z"
"","4027","YARN-11081. TestYarnConfigurationFields consistently keeps failing","### Description of PR TestYarnConfigurationFields consistently keeps failing: ``` Error  Messageclass org.apache.hadoop.yarn.conf.YarnConfiguration has 1 variables missing in yarn-default.xml Entries:   yarn.scheduler.app-placement-allocator.class expected:<0> but was:<1>Stacktracejava.lang.AssertionError: class org.apache.hadoop.yarn.conf.YarnConfiguration has 1 variables missing in yarn-default.xml Entries:   yarn.scheduler.app-placement-allocator.class expected:<0> but was:<1> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.apache.hadoop.conf.TestConfigurationFieldsBase.testCompareConfigurationClassAgainstXml(TestConfigurationFieldsBase.java:493) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498)  ```  ### How was this patch tested? Locally  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-02-24T07:36:19Z","2022-03-08T12:32:52Z"
"","3720","HDFS-16171. De-flake testDecommissionStatus (#3280)","### Description of PR testDecommissionStatus keeps failing intermittently. ``` [ERROR] testDecommissionStatus(org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor)  Time elapsed: 3.299 s  <<< FAILURE! java.lang.AssertionError: Unexpected num under-replicated blocks expected:<4> but was:<3> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.checkDecommissionStatus(TestDecommissioningStatus.java:169) 	at org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor.testDecommissionStatus(TestDecommissioningStatusWithBackoffMonitor.java:136) ```  ### How was this patch tested? Local run of unit test  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-25T06:22:17Z","2021-11-25T13:18:01Z"
"","4283","YARN-10080. Support show app id on localizer thread pool","### Description of PR Support show app id on localizer thread pool * JIRA: YARN-10080  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-08T03:02:39Z","2022-05-13T16:41:07Z"
"","4628","HDFS-16689. NameNode may crash when transitioning to Active with in-progress tailer if there are some abnormal JNs.","### Description of PR Standby NameNode may crash when transitioning to Active with a in-progress tailer if there are some abnormal JNs. And the crashed error message as blew: ```java Caused by: java.lang.IllegalStateException: Cannot start writing at txid X when there is a stream available for read: ByteStringEditLog[X, Y], ByteStringEditLog[X, 0] 	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:344) 	at org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.openForWrite(FSEditLogAsync.java:113) 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1423) 	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:2132) 	... 36 more ```  After tracing and found there is a critical bug in `EditlogTailer#catchupDuringFailover()` with `DFS_HA_TAILEDITS_INPROGRESS_KEY=true`.  `catchupDuringFailover()` may cannot replay all edits from JournalNodes with `onlyDurableTxns=true`  if there are some abnormal JournalNodes, because maybe JournalNodes will return a empty response caused `maxAllowedTxns=0` in `QuorumJournalManager#selectRpcInputStreams`.   Reproduce method, suppose:  - There are 2 namenode, namely NN0 and NN1, and the status of echo namenode is Active, Standby respectively. And there are 3 JournalNodes, namely JN0, JN1 and JN2.  - NN0 try to sync 3 edits to JNs with started txid 3, but only successfully synced them to JN1 and JN3. And JN0 is abnormal, such as GC, bad network or restarted. - NN1's lastAppliedTxId is 2, and at the moment, we are trying failover active from NN0 to NN1.  - NN1 only got two responses from JN0 and JN1 when it try to selecting inputStreams with `fromTxnId=3`  and `onlyDurableTxns=true`, and the count txid of response is 0, 3 respectively. JN2 is abnormal, such as GC,  bad network or restarted. - NN1 will cannot replay any Edits with `fromTxnId=3` from JournalNodes because the `maxAllowedTxns` is 0.   So I think Standby NameNode should `catchupDuringFailover()` with `onlyDurableTxns=false` , so that it can replay all missed edits from JournalNode.","open","","ZanderXu","2022-07-25T16:03:55Z","2022-07-28T03:39:48Z"
"","4505","HADOOP-18314. Add some description for PowerShellFencer.","### Description of PR Some descriptions related to PowerShellFencer are missing here, such as: core-default.xml, HDFSHighAvailabilityWithQJM.md, HDFSHighAvailabilityWithNFS.md, NodeFencer.java. The purpose of this PR is to perfect them. Details: HADOOP-18314  ### How was this patch tested? Here is mainly to update some documents, for testing, there is not much pressure.","closed","","jianghuazhu","2022-06-27T12:16:16Z","2022-06-29T02:06:39Z"
"","4357","HDFS-16595. Slow peer metrics - add median, median absolute deviation and upper latency limits","### Description of PR Slow datanode metrics include slow node and it's reporting node details. With HDFS-16582, we added the aggregate latency that is perceived by the reporting nodes.  In order to get more insights into how the outlier slownode's latencies differ from the rest of the nodes, we should also expose median, median absolute deviation and the calculated upper latency limit details.  ### How was this patch tested? UTs and Dev cluster testing:     ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-05-26T02:26:26Z","2022-06-03T22:10:15Z"
"","3902","HADOOP-18085. S3 SDK Upgrade causes AccessPoint ARN endpoint mistranslation","### Description of PR Since upgrading the SDK to 1.12.132 the access point endpoint translation was broken. Correct endpoints should start with ""s3-accesspoint."", after SDK upgrade they start with ""s3.accesspoint-"" which messes up tests + region detection by the SDK.  - Fixed endpoint translation with a .replace, if SDK will fix the bug   then this replace will remain harmless; - Fixed the TestArnResource tests;  ### How was this patch tested? Tested in `eu-west-1` by running  ``` mvn -Dparallel-tests -DtestsThreadCount=32 clean verify ```  with output: ``` [INFO] Results: [INFO] [WARNING] Tests run: 1061, Failures: 0, Errors: 0, Skipped: 181 -------- [INFO] [INFO] Results: [INFO] [WARNING] Tests run: 108, Failures: 0, Errors: 0, Skipped: 68 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?  @steveloughran","closed","","bogthe","2022-01-18T16:51:29Z","2022-02-04T15:37:08Z"
"","4431","HADOOP-18288. Total requests and total requests per sec served by RPC servers","### Description of PR RPC Servers provide bunch of useful information like num of open connections, slow requests, num of in-progress handlers, RPC processing time, queue time etc, however so far it doesn't provide accumulation of all requests as well as current snapshot of requests per second served by the server. Exposing them would benefit from operational viewpoint in identifying how busy the servers have been and how much load they are currently serving in the presence of cluster wide high load.  ### How was this patch tested? Local dev cluster and UT.      ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-06-12T01:26:53Z","2022-06-18T04:26:19Z"
"","3702","YARN-11012. Add version and process startup time on Router web page","### Description of PR Router web page should show hadoop version and startup time like other YARN componment, so we can add a tab 'ServerInfo' to show these informations.","open","","luoyuan3471","2021-11-22T11:08:24Z","2022-02-14T03:23:06Z"
"","4321","HDFS-16581. Print node status when executing printTopology","### Description of PR Right now we can't directly see the status of some DataNodes, it would be helpful to see this information through the dfsadmin tool. Details: HDFS-16581  ### How was this patch tested? Need to test, when nodes are in DECOMMISSION_INPROGRESS, DECOMMISSION_INPROGRESS or alive state, we should correctly identify them.","closed","","jianghuazhu","2022-05-17T15:43:07Z","2022-06-16T11:43:15Z"
"","4096","YARN-11096. Support node load based scheduling","### Description of PR ResourceManager can scheduled according to the node load reported by NodeManager through heartbeat. We can config the threshold and skip the nodes with high load when scheduling.  ### How was this patch tested? UTs and manual","open","","Deegue","2022-03-23T15:13:57Z","2022-04-01T19:07:02Z"
"","4452","HDFS-16633. Fixing when Reserved Space For Replicas is not released on some cases","### Description of PR Reserved Space For Replicas is not released on some cases  Have found the Reserved Space For Replicas is not released on some cases in a Cx Prod cluster. We can fix the issue completely by releasing any remaining reserved space from BlockReceiver#close which is initiated by DataXceiver#writeBlock finally.  * JIRA: HDFS-16633   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-06-17T16:32:17Z","2022-06-24T13:05:01Z"
"","4222","YARN-10187. Removing hadoop-yarn-project/hadoop-yarn/README as it is …","### Description of PR Removing hadoop-yarn-project/hadoop-yarn/README as it is no longer maintained. * JIRA: YARN-10187","closed","","ashutoshcipher","2022-04-22T16:37:47Z","2022-05-02T18:08:41Z"
"","4326","HDFS-16540. Addendum: Data locality is lost when DataNode pod restarts in kubernetes.","### Description of PR remove unusually introduce file '.BUILDING.txt.swp'.  ### How was this patch tested? No adding test since no source code changes.  ### For code changes: - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [N] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [N] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [N] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Hexiaoqiao","2022-05-18T13:19:35Z","2022-05-18T15:29:17Z"
"","4381","YARN-11142.Remove unused Imports in Hadoop YARN project","### Description of PR Remove unused Imports in Hadoop YARN project * JIRA: YARN-11142   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-30T16:47:10Z","2022-05-31T17:47:11Z"
"","4284","YARN-11135. Remove unused Imports in hadoop project","### Description of PR Remove unused Imports in hadoop project * JIRA: YARN-11135  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-08T03:33:56Z","2022-05-11T09:49:24Z"
"","4299","MAPREDUCE-7377. Remove unused imports in MapReduce project","### Description of PR Remove unused Imports in Hadoop MAP/REDUCE project * JIRA: MAPREDUCE-7377  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-11T01:21:37Z","2022-05-13T16:37:00Z"
"","4389","HDFS-16576.Remove unused Imports in Hadoop HDFS project","### Description of PR Remove unused Imports in Hadoop HDFS project * JIRA: HDFS-16576   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-31T16:28:10Z","2022-06-09T13:41:18Z"
"","4392","HADOOP-18271.Remove unused Imports in Hadoop Common project","### Description of PR Remove unused Imports in Hadoop Common project * JIRA: HADOOP-18271   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-31T17:12:09Z","2022-06-23T07:00:29Z"
"","3789","HADOOP-16908. Prune Jackson 1 from the codebase and restrict it's usage for future","### Description of PR Prune Jackson1 usages from entire Hadoop codebase. Restrict the imports from org.codehaus.jackson. mvn dependency shows that only avro 1.7.7 needs codehaus jackson dependency. From Jersey 1.x, we are excluding codehaus jackson explicitly. We can upgrade avro to the version not using codehaus jackson in a separate Jira once this PR merged. Also, this PR is anyways going to ensure that codehaus jackson is never used in the codebase again.  ### How was this patch tested? Local testing. Full QA results are also available from Jenkins builds.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-11T15:33:22Z","2021-12-20T07:01:35Z"
"","4107","HDFS-16521. DFS API to retrieve slow datanodes","### Description of PR Providing DFS API to retrieve slow nodes would help add an additional option to ""dfsadmin -report"" that lists slow datanodes info for operators to take a look, specifically useful filter for larger clusters.  The other purpose of such API is for HDFS downstreamers without direct access to namenode http port (only rpc port accessible) to retrieve slownodes.  Created follow-up Jira HDFS-16528 to support enabling slow peer stats without having to restart Namenode.  [FanOutOneBlockAsyncDFSOutput](https://github.com/apache/hbase/blob/master/hbase-asyncfs/src/main/java/org/apache/hadoop/hbase/io/asyncfs/FanOutOneBlockAsyncDFSOutput.java) in HBase currently has to rely on it's own way of marking and excluding slow nodes while 1) creating pipelines and 2) handling ack, based on factors like the data length of the packet, processing time with last ack timestamp, whether flush to replicas is finished etc. If it can utilize slownode API from HDFS to exclude nodes appropriately while writing block, a lot of it's own post-ack computation of slow nodes can be saved or improved or based on further experiment, we could find better solution to manage slow node detection logic both in HDFS and HBase. However, in order to collect more data points and run more POC around this area, HDFS should provide API for downstreamers to efficiently utilize slownode info for such critical low-latency use-case (like writing WALs).  ### How was this patch tested? Dev cluster:      ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-03-25T16:34:21Z","2022-05-18T23:53:40Z"
"","4108","HDFS-16522. Set Http and Ipc ports for Datanodes in MiniDFSCluster","### Description of PR Provide options to set Http and Ipc ports for Datanodes in MiniDFSCluster.  ### How was this patch tested? UT  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-03-25T16:49:48Z","2022-04-06T09:17:12Z"
"","3875","HADOOP-18077. ProfileOutputServlet unable to proceed due to NPE","### Description of PR ProfileOutputServlet context doesn't have Hadoop configs available and hence async profiler redirection to output servlet is failing to identify if admin access is allowed: ``` HTTP ERROR 500 java.lang.NullPointerException URI:	/prof-output-hadoop/async-prof-pid-98613-cpu-2.html STATUS:	500 MESSAGE:	java.lang.NullPointerException SERVLET:	org.apache.hadoop.http.ProfileOutputServlet-58c34bb3 CAUSED BY:	java.lang.NullPointerException Caused by: java.lang.NullPointerException 	at org.apache.hadoop.http.HttpServer2.isInstrumentationAccessAllowed(HttpServer2.java:1619) 	at org.apache.hadoop.http.ProfileOutputServlet.doGet(ProfileOutputServlet.java:51) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) 	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) 	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799) 	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:550) 	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233) 	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1434) 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188) 	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501) 	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186) 	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1349) 	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) 	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234) 	at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:146) 	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:179) 	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127) 	at org.eclipse.jetty.server.Server.handle(Server.java:516) 	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:400) 	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:645) 	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:392) 	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277) 	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311) 	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105) 	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104) 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338) 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315) 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173) 	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) 	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409) 	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883) 	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034) 	at java.lang.Thread.run(Thread.java:748) ```  ### How was this patch tested? Locally.  Few screenshots:     ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-01-10T18:29:46Z","2022-01-12T08:20:35Z"
"","4263","HADOOP-18105 Implement buffer pooling with weak references","### Description of PR part of HADOOP-18103. Required for vectored IO feature. None of current buffer pool implementation is complete. ElasticByteBufferPool doesn't use weak refrences and could lead to memory leak errors and DirectBufferPool doesn't support caller prefrences of direct and heap buffers and has only fixed length buffer implementation.  ### How was this patch tested? Added new unit tests and tested through vectored read api integration test.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-05-04T21:54:07Z","2022-06-01T22:08:07Z"
"","3964","HADOOP-18104: S3A: Add configs to configure minSeekForVectorReads and maxReadSizeForVectorReads","### Description of PR Part of HADOOP-18103. Introducing fs.s3a.min.seek.vectored.read and fs.s3a.max.readsize.vectored.read to configure min seek and max read during a vectored IO operation in S3A connector. These propeties actually define how the ranges will be merged. To completely disable merging set fs.s3a.max.readsize.vectored.read to 0.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-02-08T09:06:49Z","2022-04-29T22:47:33Z"
"","4248","MAPREDUCE-7370. Parallelize MultipleOutputs#close call","### Description of PR Parallelize MultipleOutputs#close call  * JIRA: MAPREDUCE-7370","open","","ashutoshcipher","2022-04-29T12:47:15Z","2022-07-13T22:27:06Z"
"","4448","HDFS-16634. Dynamically adjust slow peer report size on JMX metrics","### Description of PR On a busy cluster, sometimes it takes bit of time for deleted node(from the cluster)'s ""slow node report"" to get removed from slow peer json report on Namenode JMX metrics. In the meantime, user should be able to browse through more entries in the report by adjusting i.e. reconfiguring ""dfs.datanode.max.nodes.to.report"" so that the list size can be adjusted without user having to bounce active Namenode just for this purpose.  ### How was this patch tested? Dev cluster and using UT.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-06-17T01:13:38Z","2022-06-20T01:25:26Z"
"","4010","HDFS-16476.Increase the number of metrics used to record PendingRecoveryBlocks.","### Description of PR Now we don't know how many blocks are happening or are about to recover, the purpose of this pr is to record them through metrics. Details: HDFS-16476  ### How was this patch tested? When some blocks are recovering or are about to recover, you can view the number through metrics.","open","","jianghuazhu","2022-02-22T13:51:44Z","2022-03-05T17:43:11Z"
"","3822","HADOOP-18060.RPCMetrics increases the number of handlers in processing.","### Description of PR Now we can't see how many Handlers in RPC are actually being used. It would be very helpful to see this information directly through RPCMetrics. The purpose of this pr is to solve this problem. Details: HDFS-16394  ### How was this patch tested? This needs to be tested. When accessing RPC, you need to know how many handlers are being used based on RPCMetrics.","closed","","jianghuazhu","2021-12-22T06:30:32Z","2021-12-31T08:40:57Z"
"","4460","HADOOP-18033. [WIP] Remove jsr311-api dependency","### Description of PR More discussion on HADOOP-18033","open","","virajjasani","2022-06-18T20:33:57Z","2022-07-11T23:10:21Z"
"","4192","HDFS-16256. Minor fix in HDFSFederationBalance document","### Description of PR Minor fix in HDFSFederationBalance document  * JIRA: HDFS-16256","closed","","ashutoshcipher","2022-04-18T16:45:40Z","2022-05-02T00:08:54Z"
"","4383","HADOOP-18258. Merging of S3A Audit Logs","### Description of PR Merging audit log files containing huge number of audit logs collected from a job like Hive or Spark job containing various S3 requests like list, head, get and put requests.  ### How was this patch tested? Region : AP-South-1 Command used : mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale Getting these two errors while testing ``` [ERROR] testSTS(org.apache.hadoop.fs.s3a.ITestS3ATemporaryCredentials)  Time elapsed: 7.931 s  <<< ERROR! com.amazonaws.SdkClientException: Unable to find a region via the region provider chain. Must provide an explicit region in the builder or setup environment to supply a region. 	at com.amazonaws.client.builder.AwsClientBuilder.setRegion(AwsClientBuilder.java:462) 	at com.amazonaws.client.builder.AwsClientBuilder.configureMutableProperties(AwsClientBuilder.java:424) 	at com.amazonaws.client.builder.AwsSyncClientBuilder.build(AwsSyncClientBuilder.java:46) 	at org.apache.hadoop.fs.s3a.ITestS3ATemporaryCredentials.testSTS(ITestS3ATemporaryCredentials.java:130) ``` ``` [ERROR] testDTUtilShell(org.apache.hadoop.fs.s3a.auth.delegation.ITestSessionDelegationInFileystem)  Time elapsed: 1.861 s  <<< FAILURE! java.lang.AssertionError: expected:<0> but was:<1> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at org.apache.hadoop.fs.s3a.auth.delegation.ITestSessionDelegationInFileystem.dtutil(ITestSessionDelegationInFileystem.java:739) 	at org.apache.hadoop.fs.s3a.auth.delegation.ITestSessionDelegationInFileystem.testDTUtilShell(ITestSessionDelegationInFileystem.java:750) ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","sravanigadey","2022-05-31T08:23:41Z","2022-07-25T04:57:22Z"
"","4000","HADOOP-18131. Upgrade maven enforcer plugin and relevant dependencies","### Description of PR Maven enforcer plugin's latest version 3.0.0 has some noticeable improvements (e.g. MENFORCER-350, MENFORCER-388, MENFORCER-353) and fixes for us to incorporate. Besides, some of the relevant enforcer dependencies (e.g. extra enforcer rules and restrict import enforcer) too have good improvements.  We should upgrade maven enforcer plugin and the relevant dependencies.  ### How was this patch tested? Locally  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-02-18T19:04:03Z","2022-03-08T08:28:15Z"
"","3733","HADOOP-18027. Include static imports in the maven plugin rules","### Description of PR Maven enforcer plugin to ban illegal imports require explicit mention of static imports in order to evaluate whether any publicly accessible static entities from the banned classes are directly imported by Hadoop code.  ### How was this patch tested? Tested locally.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-29T10:03:19Z","2021-12-01T07:58:29Z"
"","3891","HADOOP-18082.Add debug log when RPC#Reader gets a Call.","### Description of PR Log some information from the moment Call enters the RPC inside, which will help us understand more about Call. The records here should be in the form of logs. The priority of the logs should not be too high, and debugging is the best. Details: HADOOP-18082  ### How was this patch tested? Corresponding to the test, the pressure is not large.","closed","","jianghuazhu","2022-01-17T08:25:49Z","2022-02-16T20:20:05Z"
"","3788","HADOOP-17725. Keep MSI tenant ID and client ID optional (ADDENDUM)","### Description of PR Keep MSI tenant ID and client ID optional.","open","","virajjasani","2021-12-11T06:08:24Z","2022-03-14T06:14:59Z"
"","4560","HDFS-16659. JournalNode should throw CacheMissException when SinceTxId is bigger than HighestWrittenTxId","### Description of PR JournalNode should throw `CacheMissException` if `sinceTxId` is bigger than `highestWrittenTxId` during handling `getJournaledEdits` rpc from NNs.  Current logic may cause in-progress EditlogTailer cannot replay any Edits from JournalNodes in some corner cases, resulting in ObserverNameNode cannot handle requests from clients.  Suppose there are 3 journalNodes, JN0 ~ JN1. * JN0 has some abnormal cases when Active Namenode is syncing 10 Edits with first txid 11 * NameNode just ignore the abnormal JN0 and continue to sync Edits to Journal 1 and 2 * JN0 backed to health * NameNode continue sync 10 Edits with first txid 21. * At this point, there are no Edits 11 ~ 30 in the cache of JN0 * Observer NameNode try to select EditLogInputStream through `getJournaledEdits` with since txId 21 * Journal 2 has some abnormal cases and caused a slow response  The expected result is: Response should contain 20 Edits from txId 21 to txId 30 from JN1 and JN2. Because Active NameNode successfully write these Edits to JN1 and JN2 and failed write these edits to JN0.  But in the current implementation,  the response is [Response(0) from JN0, Response(10) from JN1], because  there are some abnormal cases in  JN2, such as GC, bad network,  cause a slow response. So the `maxAllowedTxns` will be 0, NameNode will not replay any Edits.   As above, the root case is that JournalNode should throw Miss Cache Exception when `sinceTxid` is more than `highestWrittenTxId`.  And the bug code as blew: ``` if (sinceTxId > getHighestWrittenTxId()) {     // Requested edits that don't exist yet; short-circuit the cache here     metrics.rpcEmptyResponses.incr();     return GetJournaledEditsResponseProto.newBuilder().setTxnCount(0).build();  } ```","open","","ZanderXu","2022-07-13T14:56:21Z","2022-07-28T03:39:06Z"
"","4336","YARN-11137. Improve log message in FederationClientInterceptor","### Description of PR JIRA:YARN-11137. Improve log message in FederationClientInterceptor.  ### For code changes: The log methods are inconsistent, some use the splicing method, and some use the placeholder method.  1.org.apache.hadoop.yarn.server.router.clientrmsubmit.FederationClientInterceptor#getNewApplication  ``` for (int i = 0; i < numSubmitRetries; ++i) {    SubClusterId subClusterId = getRandomActiveSubCluster(subClustersActive);    LOG.debug(        ""getNewApplication try #{} on SubCluster {}"", i, subClusterId);    ApplicationClientProtocol clientRMProxy =         getClientRMProxyForSubCluster(subClusterId);   ... } ```  2.org.apache.hadoop.yarn.server.router.clientrmsubmit.FederationClientInterceptor#submitApplication  ``` for (int i = 0; i < numSubmitRetries; ++i) {                SubClusterId subClusterId = policyFacade.getHomeSubcluster(           request.getApplicationSubmissionContext(), blacklist);       LOG.info(""submitApplication appId"" + applicationId + "" try #"" + i           + "" on SubCluster "" + subClusterId);    ... }  ```  I think the first way is better.","closed","","slfan1989","2022-05-20T11:59:11Z","2022-05-25T00:21:04Z"
"","4368","HDFS-16599. Fix typo in hadoop-hdfs-rbf module","### Description of PR JIRA:HDFS-16599. Fix typo in RouterRpcClient.","closed","","slfan1989","2022-05-28T04:24:54Z","2022-05-30T20:29:17Z"
"","3718","YARN-9063. ATS 1.5 fails to start if RollingLevelDb files are corrupt or missing","### Description of PR Jira: https://issues.apache.org/jira/browse/YARN-9063 ATS 1.5 fails to start if RollingLevelDb files are corrupt or missing  ### How was this patch tested? Wrote a unit test case to check and manually tested it.  ``` [INFO] --- maven-surefire-plugin:2.21.0:test (default-test) @ hadoop-yarn-server-applicationhistoryservice --- [INFO]  [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.735 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices [INFO] Tests run: 36, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.125 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.204 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore [INFO] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.878 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.653 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.438 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.748 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer [INFO] Running org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerOnTimelineStore [INFO] Tests run: 40, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.455 s - in org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerOnTimelineStore [INFO] Running org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.89 s - in org.apache.hadoop.yarn.server.timeline.TestRollingLevelDB [INFO] Running org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.059 s - in org.apache.hadoop.yarn.server.timeline.TestTimelineDataManager [INFO] Running org.apache.hadoop.yarn.server.timeline.security.TestTimelineACLsManager [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.553 s - in org.apache.hadoop.yarn.server.timeline.security.TestTimelineACLsManager [INFO] Running org.apache.hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.173 s - in org.apache.hadoop.yarn.server.timeline.security.TestTimelineAuthenticationFilterForV1 [INFO] Running org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.847 s - in org.apache.hadoop.yarn.server.timeline.recovery.TestLeveldbTimelineStateStore [INFO] Running org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.554 s - in org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL [INFO] Running org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices [INFO] Tests run: 28, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.214 s - in org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices [INFO] Running org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore [INFO] Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.338 s - in org.apache.hadoop.yarn.server.timeline.TestRollingLevelDBTimelineStore [INFO] Running org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore [INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.999 s - in org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore [INFO] Running org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore [INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.397 s - in org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore [INFO] Running org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.38 s - in org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper [INFO]  [INFO] Results: [INFO]  [INFO] Tests run: 208, Failures: 0, Errors: 0, Skipped: 0 [INFO]  [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  02:45 min [INFO] Finished at: 2021-11-25T10:05:41+05:30 [INFO] ------------------------------------------------------------------------  ```","closed","","ashutoshcipher","2021-11-24T20:05:23Z","2021-11-26T07:12:29Z"
"","4317","YARN-10465. Support getNodeToLabels, getLabelsToNodes, getClusterNodeLabels API's for Federation","### Description of PR JIRA: https://issues.apache.org/jira/browse/YARN-10465 getNodeToLabels, getLabelsToNodes, getClusterNodeLabels API's are not implemented under the Yarn Federation framework, implement the methods.  ### How was this patch tested? Added Junit Test to test normal requests and NULL requests, all as expected  ### For code changes: 1.Use the loop method to call the getNodeToLabels, getLabelsToNodes, getClusterNodeLabels method of the subcluster one by one 2.Complete the Metric indicator counting function","closed","","slfan1989","2022-05-16T23:40:19Z","2022-05-20T20:19:42Z"
"","3978","HADOOP-13704. Optimised getContentSummary()","### Description of PR JIRA: https://issues.apache.org/jira/browse/HADOOP-13704  This PR implements an optimised version of getContentSummary which uses the result from the listFiles iterator.  Explanation of new `buildDirectorySet` method added:  Since the listFiles operation can return the directory `a/b/c` as a single object, we need to recurse over the path `a/b/c` to ensure we have counted all directories. We do this by keeping two sets, dirSet (Set of all directories under the base path) and pathTraversed (Set of paths we have recursed over so far).  Iterating over directory structure `basePath/a/b/c`, `basePath/a/b/d`, we will first find all the directories in `basePath/a/b/c`. Once this is completed, the pathTraversed set will have `{basePath/a/b}` and dirSet will have `{basePath/a, basePath/a/b, basePath/a/b/c}`.  Then for `basePath/a/b/d`, just add `basePath/a/b/d` to the dirSet and don't do any additional work as path `basePath/a/b` has already been traversed.  The Jira ticket mentions that we should add in some instrumentation to measure usage. T's already code that does this   [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java#L3256) and usage is tested in an integration test [here](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/performance/ITestS3AMiscOperationCost.java#L144) .  ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-02-10T15:21:19Z","2022-03-22T13:47:17Z"
"","4335","HADOOP-18249. Fix getUri() in HttpRequest has been deprecated.","### Description of PR JIRA: HADOOP-18249. Fix getUri() in HttpRequest has been deprecated.  When reading the code, I found that the method used has been deprecated due to the upgrade of the netty component. The main methods are as follows:  io.netty.handler.codec.http#HttpRequest ``` @Deprecated HttpMethod getMethod(); Deprecated. Use method() instead.  @Deprecated String getUri() Deprecated. Use uri() instead. ```  io.netty.handler.codec.http#DefaultHttpResponse ``` @Deprecated public HttpResponseStatus getStatus() {         return this.status(); } Deprecated. Use status()  instead. ```","closed","","slfan1989","2022-05-20T05:27:09Z","2022-05-24T21:15:06Z"
"","4530","HDFS-15079. RBF: Namenode needs to use the actual client Id and callId when going through RBF proxy.","### Description of PR Jira: [HDFS-15079](https://issues.apache.org/jira/browse/HDFS-15079)  Similarly with [HDFS-13248](https://issues.apache.org/jira/browse/HDFS-13248),  RBF adds the actual client Id and client call Id in CallerContext and carries them to NameNode. Then nameNode try to obtain the actual client Id and client call Id to ensure CacheEntry mechanism.","closed","","ZanderXu","2022-07-04T14:04:15Z","2022-07-23T14:46:31Z"
"","4531","HDFS-13274. RBF: Extend RouterRpcClient to use multiple sockets","### Description of PR Jira: [HDFS-13274](https://issues.apache.org/jira/browse/HDFS-13274) [HADOOP-13144](https://issues.apache.org/jira/browse/HADOOP-13144) introduces the ability to create multiple connections for the same user and use different sockets. The RouterRpcClient should use this approach to get a better throughput.  In my practice, I configured `dfs.federation.router.connection.pool-size=6` and found some abnormal cases: - RBF frequently to create socket with NN during forwarding request. - There are a large number of connection threads with name node. (There are 70+ downstream name services)  After tracing the code and found that we can optimize the mechanism of round-robin obtaining connection.  - One `ConnectionContext` can support handler some concurrent requests at the same time, such as 5 - We can mark one `ConnectionContext` usable when the `numThreads` is less than `maxConcurrencyPerConn` - About `getConnection`, we can returns the previous usable connection in order - If there no usable connections, we can return one connection round-robin  After using the above solution,  we effectively controlled the number of connections between RBF and NameNode, and there is almost no the phenomenon of creating sockets during forwarding requests.","open","","ZanderXu","2022-07-05T04:20:32Z","2022-07-22T16:54:59Z"
"","4691","HADOOP-18389. Limit stacked call of one connection in client to avoid possible OOM in server","### Description of PR Jira link: [HADOOP-18389](https://issues.apache.org/jira/browse/HADOOP-18389) In our prod environment, we encountered an accident that JN OOM because Server#Connection#responseQueue used 97% memory.  After analyzed the memory of JN and found that there are 2w+ called stacked in one Server#Connection#responseQueue, because the network between NN and JN jitters with some tcp packet loss.  We can refer to some screenshots in [HADOOP-18389](https://issues.apache.org/jira/browse/HADOOP-18389)  In this case, I think Client.java should support limit the stacked calls of one connection to avoid the possible OOM in Server.  When the number of stacked calls is more than the limit size, we can just throw one IOException to the method caller.","open","","ZanderXu","2022-08-03T16:33:01Z","2022-08-03T20:24:11Z"
"","4327","HADOOP-18244. Fix Hadoop-Common JavaDoc Error on branch-3.3","### Description of PR JIRA : [HADOOP-18244. Fix Hadoop-Common JavaDoc Error on branch-3.3](https://issues.apache.org/jira/browse/HADOOP-18244)  ### For code changes:  In the PR(https://github.com/apache/hadoop/pull/4267) of HADOOP-18224. Upgrade maven compiler plugin to 3.10.1, I found that hadoop-common has a lot of javadoc compilation errors. I fixed it on the trunk, hoping to backport these changes to branch 3.3. These changes will ensure that javadoc will pass in JDK11 compilation.","closed","","slfan1989","2022-05-18T13:46:24Z","2022-05-29T06:01:17Z"
"","4588","Advanced PR for https://github.com/apache/hadoop/pull/3440","### Description of PR Its for the fixture of https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3440/5/artifact/out/new-spotbugs-hadoop-tools_hadoop-azure.html  PR created to check result of Jenkins job. Shall be deleted once job's results are checked.  ### How was this patch tested? Zone: US-EAST HNS endpoint: pranavsaxenahns.dfs.core.windows.net NON-HNS endpoint: pranavsaxenanonhns.dfs.core.windows.net   Ran integeration test on the code-change. Following are the logs of the tests: ``` Combination specific property setting: [ key=fs.azure.account.auth.type , value=OAuth ]   Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: pranavsaxenahns for combination HNS-OAuth Running test for combination HNS-OAuth on account pranavsaxenahns [ProcessCount=8] Result can be seen in dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-HNS-OAuth.txt  ----- Test results ----- [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"" [INFO]  [ERROR] Tests run: 107, Failures: 1, Errors: 0, Skipped: 2 [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 26 [INFO] Results: [INFO]  [WARNING] Tests run: 332, Failures: 0, Errors: 0, Skipped: 41  Time taken: 14 mins 51 secs. Find test logs for the combination (HNS-OAuth) in: dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-HNS-OAuth.txt Find consolidated test results in: dev-support/testlogs/2022-07-12_12-35-00/Test-Results.txt ------------------------   Combination specific property setting: [ key=fs.azure.account.auth.type , value=SharedKey ]   Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: pranavsaxenahns for combination HNS-SharedKey Running test for combination HNS-SharedKey on account pranavsaxenahns [ProcessCount=8] Result can be seen in dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-HNS-SharedKey.txt  ----- Test results ----- [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"" [INFO]  [ERROR] Tests run: 107, Failures: 1, Errors: 0, Skipped: 2 [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   ITestAbfsRestOperationException.testCustomTokenFetchRetryCount:95->testWithDifferentCustomTokenFetchRetry:123->Assert.assertTrue:42->Assert.fail:89 Number of token fetch retries (7) done, does not match with fs.azure.custom.token.fetch.retry.count configured (3): SUCCESSFUL WHEN RAN INDIVIDUALLY [INFO]  [ERROR] Tests run: 574, Failures: 1, Errors: 0, Skipped: 26 [INFO] Results: [INFO]  [WARNING] Tests run: 332, Failures: 0, Errors: 0, Skipped: 41  Time taken: 14 mins 42 secs. Find test logs for the combination (HNS-SharedKey) in: dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-HNS-SharedKey.txt Find consolidated test results in: dev-support/testlogs/2022-07-12_12-35-00/Test-Results.txt ------------------------   Combination specific property setting: [ key=fs.azure.account.auth.type , value=SharedKey ]   Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: pranavsaxenanonhns for combination NonHNS-SharedKey Running test for combination NonHNS-SharedKey on account pranavsaxenanonhns [ProcessCount=8] Result can be seen in dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-NonHNS-SharedKey.txt  ----- Test results ----- [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"" [INFO]  [ERROR] Tests run: 107, Failures: 1, Errors: 0, Skipped: 2 [INFO] Results: [INFO]  [WARNING] Tests run: 559, Failures: 0, Errors: 0, Skipped: 268 [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   ITestAbfsRenameStageFailure>TestRenameStageFailure.testResilienceAsExpected:126 [resilient commit support] expected:<[tru]e> but was:<[fals]e> [ERROR]   ITestAbfsTerasort.test_110_teragen:244->executeStage:211->Assert.assertEquals:647->Assert.failNotEquals:835->Assert.fail:89 teragen(1000, abfs://testcontainer@pranavsaxenanonhns.dfs.core.windows.net/ITestAbfsTerasort/sortin) failed expected:<0> but was:<1> [ERROR] Errors:  [ERROR]   ITestAbfsJobThroughManifestCommitter.test_0420_validateJob » OutputValidation ... [ERROR]   ITestAbfsManifestCommitProtocol.testCommitLifecycle » OutputValidation `abfs:/... [ERROR]   ITestAbfsManifestCommitProtocol.testCommitterWithDuplicatedCommit » OutputValidation [ERROR]   ITestAbfsManifestCommitProtocol.testConcurrentCommitTaskWithSubDir » OutputValidation [ERROR]   ITestAbfsManifestCommitProtocol.testMapFileOutputCommitter » OutputValidation ... [ERROR]   ITestAbfsManifestCommitProtocol.testOutputFormatIntegration » OutputValidation [ERROR]   ITestAbfsManifestCommitProtocol.testParallelJobsToAdjacentPaths » OutputValidation [ERROR]   ITestAbfsManifestCommitProtocol.testTwoTaskAttemptsCommit » OutputValidation `... [INFO]  [ERROR] Tests run: 332, Failures: 2, Errors: 8, Skipped: 46: FAILURES SAME IN CI-JENKINS REPORT  Time taken: 14 mins 41 secs. Find test logs for the combination (NonHNS-SharedKey) in: dev-support/testlogs/2022-07-12_12-35-00/Test-Logs-NonHNS-SharedKey.txt Find consolidated test results in: dev-support/testlogs/2022-07-12_12-35-00/Test-Results.txt   AppendBlob-HNS-OAuth ======================== [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestAccountConfiguration.testConfigPropNotFound:386->testMissingConfigKey:399 Expected a org.apache.hadoop.fs.azurebfs.contracts.exceptions.TokenAccessProviderException to be thrown, but got the result: : ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"" [INFO]  [ERROR] Tests run: 107, Failures: 1, Errors: 0, Skipped: 2 [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 26 [INFO] Results: [INFO]  [WARNING] Tests run: 332, Failures: 0, Errors: 0, Skipped: 41  ```  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pranavsaxena-microsoft","2022-07-19T04:36:18Z","2022-07-19T09:24:42Z"
"","3937","HDFS-16439.Makes calculating maxNodesPerRack simpler.","### Description of PR It should be simpler to compute numOfReplicas when BlockPlacementPolicyDefault#getMaxNodesPerRack() is executed. This is the purpose of this pr. Details: HDFS-16439  ### How was this patch tested? For testing, there is not much pressure.","open","","jianghuazhu","2022-01-26T10:25:58Z","2022-01-27T14:32:41Z"
"","3838","HDFS-16401.Remove the worthless DatasetVolumeChecker#numAsyncDatasetChecks.","### Description of PR It seems that cleaning up DatasetVolumeChecker#numAsyncDatasetChecks was ignored before this. Details: HDFS-16401  ### How was this patch tested? For testing, there is not much pressure.","closed","","jianghuazhu","2021-12-28T10:05:26Z","2022-01-25T13:51:40Z"
"","4525","HDFS-16647.Delete unused NameNode#FS_HDFS_IMPL_KEY.","### Description of PR It looks like NameNode#FS_HDFS_IMPL_KEY is not being used anywhere, it would be cleaner to remove it. Details: HDFS-16647  ### How was this patch tested? Here is just to delete some unused code, it is not too stressful for testing.","closed","","jianghuazhu","2022-07-01T10:17:38Z","2022-07-02T20:17:02Z"
"","4311","HDFS-13522: IPC changes to support observer reads through routers.","### Description of PR IPC changes so that clients send their txn number to routers and in turn routers will forward this to the namenodes. The code to direct routers to the observer namenodes will be in a follow up PR.  ### How was this patch tested? Ran unit tests in module in Intellij  ### For code changes:  - [ x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","simbadzina","2022-05-16T02:26:48Z","2022-07-28T12:18:09Z"
"","3834","YARN-11054. Alleviate LocalJobRunnerMetricName Conflicts","### Description of PR In some scenarios, Sqoop will use LocalJobRuner (YarnLocal mode) to run massive jobs,  assuming  2 million jobs have been run. Considering LocalJobRunner MetricName generated by nextInt function is in the range of (0, 2147483647), The conflict probability will be about 2000000/2147483647 = 1/1000, which means that an average of one fail task for every more 1000 jobs run. If LocalJobRunner MetricName is generated by nextLong() whose range is (0, 9223372036854775807), considering that Long's range is 1 billion times that of Int, the conflict probability is also decreased by one trillionth times. the conflict probability goes to 1/1000000000 from 1/1000, which alleviate LocalJobRunnerMetricName conflicts a lot.  ### How was this patch tested? With long-time test, the conflict probability decreases a lot  ### For code changes:  - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [Y] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [Y] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [y] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","marchpure","2021-12-28T06:40:05Z","2021-12-28T08:10:27Z"
"","3846","HADOOP-18063. Remove unused import AbstractJavaKeyStoreProvider in Shell class.","### Description of PR In Shell, there are some invalid introductions, which are unnecessary. Details: HADOOP-18063  ### How was this patch tested? For testing, there is little pressure.","closed","","jianghuazhu","2022-01-01T15:05:36Z","2022-01-04T02:25:31Z"
"","4606","HDFS-16678. RBF should supports disable getNodeUsage() in RBFMetrics","### Description of PR In our prod environment, we try to collect RBF metrics every 15s through jmx_exporter. And we found that collection task often failed.   After tracing and found that the collection task is blocked at getNodeUsage() in RBFMetrics, because it will collect all datanode's usage from downstream nameservices.    This is a very expensive and almost useless operation. Because in most scenarios, each downstream nameserivce contains almost the same DNs. We can get the data usage's from any one nameservices if need, not from RBF.  So I feel that RBF should supports disable getNodeUsage() in RBFMetrics.","open","","ZanderXu","2022-07-21T14:30:58Z","2022-07-30T02:29:57Z"
"","4644","HDFS-16698. Add a metric to sense possible MaxDirectoryItemsExceededException in time.","### Description of PR In our prod environment, we occasionally encounter MaxDirectoryItemsExceededException caused job failure. ``` org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException):  The directory item limit of /user/XXX/.sparkStaging is exceeded: limit=1048576 items=1048576 ```  In order to avoid it, we add a metric to sense possible MaxDirectoryItemsExceededException in time. So that we can process it in time to avoid job failure.","open","","ZanderXu","2022-07-27T14:53:59Z","2022-07-27T21:41:04Z"
"","4496","HDFS-16641. [SBN read] Add a metric to quantify the number of re-enter rpcs","### Description of PR In our prod environment, Observer NameNode also have high QueueTime, but with low processingTime. After tracing and found the root cause maybe is that calls from clients frequently re-enter the Queue because the client call's state id is bigger than the server state id.  So I think maybe we should add a metric to quantify the number of re-enter rpcs to help us locate this anomaly easily.","open","","ZanderXu","2022-06-24T06:45:58Z","2022-07-25T02:00:08Z"
"","4497","HDFS-16642. [SBN read] Moving the selecting inputstream from journalnode in EditLogTailer out of FSNLock","### Description of PR In our prod environment, 8020 Handler of Observer NameNode also blocked by EditlogTailer.   And EditlogTailer cost a long time for selecting InputStreams from Journalnode with holding the FSNLock.   During this period, 8020 handlers of Observer NameNode will be blocked by the FSN Lock.  In theory, selecting inputstreams from JournalNode does not involve changing memory information in NameNode, so we can move the selecting out of the FSN Lock, and it can improve the throughput of Observer NameNode.  After reading the relevant code in depth, I found that it is feasible to move the selecting inputStream out of the FSNLock.","open","","ZanderXu","2022-06-24T07:38:59Z","2022-08-03T16:43:29Z"
"","4223","MAPREDUCE-7246. In MapredAppMasterRest#Mapreduce_Application_Master_I…","### Description of PR In MapredAppMasterRest#Mapreduce_Application_Master_Info_API, updating the datatype of appId to ""string"". * JIRA: MAPREDUCE-7246","closed","","ashutoshcipher","2022-04-22T16:42:54Z","2022-04-25T05:29:36Z"
"","4674","HDFS-16713. Improve code with Lambda in org.apache.hadoop.hdfs.server.namenode sub packages","### Description of PR Improve Code with Lambda in org.apahce.hadoop.hdfs.server.namenode sub packages.  For example: Current logic: ``` public ListenableFuture getJournaledEdits(       long fromTxnId, int maxTransactions) {     return parallelExecutor.submit(         new Callable() {           @Override           public GetJournaledEditsResponseProto call() throws IOException {             return getProxy().getJournaledEdits(journalId, nameServiceId,                 fromTxnId, maxTransactions);           }         });   }  ```  Improved Code with Lambda: ``` public ListenableFuture getJournaledEdits(       long fromTxnId, int maxTransactions) {     return parallelExecutor.submit(() -> getProxy().getJournaledEdits(         journalId, nameServiceId, fromTxnId, maxTransactions));   }  ```","open","","ZanderXu","2022-08-02T15:53:25Z","2022-08-03T13:57:52Z"
"","4668","HDFS-16695. Improve code with Lambda in org.apache.hadoop.hdfs.servernamenode package","### Description of PR Improve Code with Lambda in org.apahce.hadoop.hdfs.server.namenode package.  For example: Current logic: ``` public ListenableFuture getJournaledEdits(       long fromTxnId, int maxTransactions) {     return parallelExecutor.submit(         new Callable() {           @Override           public GetJournaledEditsResponseProto call() throws IOException {             return getProxy().getJournaledEdits(journalId, nameServiceId,                 fromTxnId, maxTransactions);           }         });   } ```  Improved Code with Lambda: ``` public ListenableFuture getJournaledEdits(       long fromTxnId, int maxTransactions) {     return parallelExecutor.submit(() -> getProxy().getJournaledEdits(         journalId, nameServiceId, fromTxnId, maxTransactions));   } ```","open","","ZanderXu","2022-08-01T14:56:03Z","2022-08-02T23:29:12Z"
"","4045","HADOOP-18112: Rename operation fails during multi object delete of size more than 1000.","### Description of PR Implementing batching of requests during bulk delete operation based on page size.  ### How was this patch tested? Added new test. and re-ran the existing integration test.   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-03-03T09:43:57Z","2022-03-11T08:28:48Z"
"","4488","HDFS-16640. RBF: Show datanode IP list when click DN histogram in Router","### Description of PR I'm not sure if need to show a list of ip's when click dn histogram. I just saw the open_hostip_list() was called, but not implemented. So I ported the code from NN UI. JIRA: https://issues.apache.org/jira/browse/HDFS-16640  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","wzhallright","2022-06-22T08:17:47Z","2022-07-21T23:21:31Z"
"","3736","HADOOP-18028. improve S3 read speed using prefetching & caching","### Description of PR I work for Pinterest. I developed a technique for vastly improving read throughput when reading from the S3 file system. It not only helps the sequential read case (like reading a SequenceFile) but also significantly improves read throughput of a random access case (like reading Parquet). This technique has been very useful in significantly improving efficiency of the data processing jobs at Pinterest.    I would like to contribute that feature to Apache Hadoop. More details on this technique are available in this blog I wrote recently: https://medium.com/pinterest-engineering/improving-efficiency-and-reducing-runtime-using-s3-read-optimization-b31da4b60fa0  ### How was this patch tested? Tested against S3 in us-east-1 region.  There are a small number of test failures (most within s3guard). I can attach test output if it helps.","closed","","bhalchandrap","2021-11-29T17:26:31Z","2022-03-29T12:20:44Z"
"","4623","HDFS-16682. [SBN Read] make estimated transactions configurable","### Description of PR https://issues.apache.org/jira/browse/HDFS-16682","open","","zhengchenyu","2022-07-25T10:10:07Z","2022-07-25T19:16:31Z"
"","4213","HDFS-16554 Remove unused configuration dfs.namenode.block.deletion.increment.","### Description of PR https://issues.apache.org/jira/browse/HDFS-16554  The configuration dfs.namenode.block.deletion.increment will not be used after the feature [HDFS-16043](https://issues.apache.org/jira/browse/HDFS-16043) that do block deletetion asynchronously. So it's better to remove it.  ### How was this patch tested?  ### For code changes:","closed","","smarthanwang","2022-04-21T11:28:09Z","2022-04-27T03:46:19Z"
"","4211","HDFS-16553 Fix checkstyle for the length of BlockManager construction method over limit.","### Description of PR https://issues.apache.org/jira/browse/HDFS-16553  The length  of BlockManager construction method is 156 lines which is over 150 limit for BlockManager, do refactor the method to fix the checkstyle.  ### How was this patch tested?  ### For code changes:","closed","","smarthanwang","2022-04-21T10:36:56Z","2022-04-29T15:07:02Z"
"","3983","HDFS-16455. RBF: Add `zk-dt-secret-manager.jute.maxbuffer` property for Router's ZKDelegationTokenManager","### Description of PR https://issues.apache.org/jira/browse/HDFS-16455  ### How was this patch tested? no test  ### For code changes: add `zk-dt-secret-manager.jute.maxbuffer`property to default value `20000000`,  20MB","open","","Neilxzn","2022-02-11T11:51:24Z","2022-02-15T16:42:19Z"
"","3968","HDFS-16451. RBF: Add search box for Router's tab-mounttable web page","### Description of PR https://issues.apache.org/jira/browse/HDFS-16451 In our cluster, we have mount many paths in HDFS Router and it may lead to take some time to load the mount-table page of Router when we open it  in the browser.  In order to use the mount-table page more conveniently, maybe we should add a search box style, just like the screenshot below ![image](https://user-images.githubusercontent.com/10757009/153178234-dbf59390-6a6b-4be7-a5b9-5d0d33b59057.png) ![image](https://user-images.githubusercontent.com/10757009/153178258-21220620-291c-4bdb-b09f-5e57ddc0b125.png)   ### How was this patch tested? no new test  ### For code changes: none","closed","","Neilxzn","2022-02-09T10:20:56Z","2022-02-11T18:26:27Z"
"","3965","HDFS-16447. RBF: Registry HDFS Router's RPCServer & RPCClient metrics for PrometheusSink","### Description of PR https://issues.apache.org/jira/browse/HDFS-16447 When we enable PrometheusSink for HDFS Router,  Router' prometheus sink miss some metrics, for example `RpcClientNumActiveConnections` and so on.  We need  registry some  Router's rpcserver & rpcclient metrics for PrometheusSink.  ### How was this patch tested?  no new test  ### For code changes: use `@Metric` to registry metrics.","closed","","Neilxzn","2022-02-08T10:34:22Z","2022-02-11T19:12:24Z"
"","3730","HDFS-16358. HttpFS implementation for getSnapshotDiffReportListing","### Description of PR HttpFS should support getSnapshotDiffReportListing API for improved snapshot diff.  ### How was this patch tested? Local testing. Added some tests in httpfs module.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-26T19:22:16Z","2021-12-03T04:25:33Z"
"","4186","HDFS-16528. Reconfigure slow peer enable for Namenode","### Description of PR HDFS-16396 provides reconfig options for several configs associated with slownodes in Datanode. Similarly, HDFS-16287 and HDFS-16327 have added some slownodes related configs as the reconfig options in Namenode. The purpose of this PR is to add DFS_DATANODE_PEER_STATS_ENABLED_KEY as reconfigurable option for Namenode.  ### How was this patch tested? Local env, and using UT.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-04-18T10:53:23Z","2022-04-30T08:16:25Z"
"","3796","YARN-11045. ATSv2 storage monitor fails to read from hbase cluster","### Description of PR HBase compatible guava dependency is bit messed up i.e. timelineservice-hbase modules are still being built with Hadoop's guava version (defined in hadoop-project) and this creates issues with HBaseStorageMonitor reading records from hbase cluster: ``` java.lang.RuntimeException: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.client.AbstractClientScanner$1.hasNext(AbstractClientScanner.java:95)         at org.apache.hadoop.yarn.server.timelineservice.storage.reader.TimelineEntityReader.readEntities(TimelineEntityReader.java:283)         at org.apache.hadoop.yarn.server.timelineservice.storage.HBaseStorageMonitor.healthCheck(HBaseStorageMonitor.java:77)         at org.apache.hadoop.yarn.server.timelineservice.storage.TimelineStorageMonitor$MonitorThread.run(TimelineStorageMonitor.java:89)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:260)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:233)         at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:394)         at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:368)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:143)         at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)         ... 3 more Caused by: java.lang.NoSuchMethodError: com.google.common.net.HostAndPort.getHostText()Ljava/lang/String;         at org.apache.hadoop.hbase.net.Address.getHostName(Address.java:72)         at org.apache.hadoop.hbase.net.Address.toSocketAddress(Address.java:57)         at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:576)         at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:37250)         at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:405)         at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:274)         at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:62)         at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:231) ```  TimelineReaderServer logs after fixing the issue:      ### How was this patch tested? 1. Tested locally 2. Verified guava jar present in timelineservice/lib 3. Run all hbase UTs 4. Checked timelinereader logs to confirm the storage monitor is not failing   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-13T19:42:58Z","2021-12-15T08:47:51Z"
"","4402","HDFS-16618. sync_file_range error should include more volume/file info","### Description of PR Having seen multiple sync_file_range errors recently with Bad file descriptor, it would be good to include more volume stats as well as file offset/length info with the error log to get some more insights.","closed","","virajjasani","2022-06-05T04:17:31Z","2022-06-07T08:54:21Z"
"","4489","HADOOP-18292. Fix s3 select tests when running against unsupported storage class","### Description of PR HADOOP-18292. when you set your fs client to work with unsupported storage classes, the s3 select tests fail.  Fix by removing storage class option before running S3 Select tests.  ### How was this patch tested? Tested with a bucket in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","monthonk","2022-06-22T13:25:49Z","2022-06-23T10:32:03Z"
"","4226","HADOOP-16515. Update the link to compatibility guide","### Description of PR HADOOP-16515. Update the link to compatibility guide  * JIRA: HADOOP-16515","closed","","ashutoshcipher","2022-04-23T12:37:51Z","2022-05-07T22:38:42Z"
"","4224","HADOOP-16515. Update the link to compatibility guide","### Description of PR HADOOP-16515. Update the link to compatibility guide  * HADOOP-16515","closed","","ashutoshcipher","2022-04-22T17:52:07Z","2022-04-23T20:00:21Z"
"","4528","HDFS-16650.Optimize the cost of obtaining timestamps in Centralized cache management.","### Description of PR Getting timestamps in Centralized cache management is done in the following ways: long now = new Date().getTime(); This way doesn't seem optimal. The purpose of pr here is to simplify them. Details: HDFS-16650  ### How was this patch tested? Here is just a change of method, for the test, the pressure is not too big.","open","","jianghuazhu","2022-07-04T13:04:39Z","2022-07-07T05:59:17Z"
"","4516","YARN-9403.GET /apps/{appid}/entities/YARN_APPLICATION accesses application table instead of entity table","### Description of PR GET /apps/{appid}/entities/YARN_APPLICATION accesses application table instead of entity table  JIRA: YARN-9403   ### How was this patch tested? Added Unit Test  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-29T23:40:18Z","2022-07-02T16:29:29Z"
"","4378","HADOOP-18255. Fix fsdatainputstreambuilder.md reference to hadoop bra…","### Description of PR fsdatainputstreambuilder.md refers to hadoop 3.3.3, when it means whatever ships off hadoop branch-3.3 * JIRA: HADOOP-18255   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-30T12:23:34Z","2022-06-21T10:13:43Z"
"","4225","YARN-9429. Fixing status code error in ResourceManager REST api doc","### Description of PR Fixing status code error in ResourceManager REST api doc * JIRA: YARN-9429","closed","","ashutoshcipher","2022-04-23T12:32:39Z","2022-04-26T01:58:58Z"
"","4221","YARN-10303. Fixing rest api example, status code error and other typos in ResourceManagerRest.md","### Description of PR Fixing rest api example, status code error and other typos in ResourceManagerRest.md * JIRA: YARN-10303 * JIRA: YARN-9429","closed","documentation,","ashutoshcipher","2022-04-22T12:38:00Z","2022-04-26T02:01:57Z"
"","3745","HDFS-16369. RBF: Fix the retry logic of RouterRpcServer#invokeAtAvailableNs.","### Description of PR Fixes the retry the logic of invokeAtAvailableNs (RBF  ### How was this patch tested? Through UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2021-12-02T14:10:27Z","2021-12-04T05:24:46Z"
"","3756","HDFS-16373. Fix MiniDFSCluster restart in case of multiple namenodes.","### Description of PR Fixes MiniDFSCluster namenode restart in case of multiple namenodes.  ### How was this patch tested? Added UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2021-12-07T08:46:44Z","2021-12-14T08:22:37Z"
"","3940","HADOOP-18096. Distcp: Sync moves filtered file to home directory rather than deleting.","### Description of PR Fixes creation of Delete Diff entry, the target in the delete diff should be null.  ### How was this patch tested? Added a UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2022-01-27T18:23:18Z","2022-02-10T21:00:27Z"
"","4351","HDFS-16592.Fix typo for BalancingPolicy.","### Description of PR Fixed some typos related to BalancingPolicy.  ### How was this patch tested? Not too stressful for testing.","closed","","jianghuazhu","2022-05-24T03:37:13Z","2022-05-26T02:50:36Z"
"","4309","HADOOP-18234. Fix s3a access point xml examples","### Description of PR Fixed s3a access point xml examples * JIRA: HADOOP-18234  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-13T11:21:37Z","2022-05-16T16:47:15Z"
"","4247","MAPREDUCE-7369. Fixed MapReduce tasks timing out when spends more time on MultipleOutputs#close","### Description of PR Fixed MapReduce tasks timing out when spends more time on MultipleOutputs#close  * JIRA: MAPREDUCE-7369","closed","","ashutoshcipher","2022-04-29T01:14:02Z","2022-06-20T08:17:11Z"
"","4282","MAPREDUCE-7379. Fixed log message in RMContainerRequestor#makeRemoteRequest","### Description of PR Fixed log message in RMContainerRequestor#makeRemoteRequest * JIRA: MAPREDUCE-7379  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-08T02:56:41Z","2022-05-11T16:42:32Z"
"","4451","HDFS-16635.Fixed javadoc error in Java 11","### Description of PR Fixed javadoc error in Java 11  * JIRA: HDFS-16635   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-06-17T09:27:28Z","2022-06-20T05:44:32Z"
"","3826","HADOOP-18057. Fix typo: validateEncrytionSecrets -> validateEncryptio…","### Description of PR Fix typo: `validateEncrytionSecrets` -> `validateEncryptionSecrets`  * JIRA: https://issues.apache.org/jira/browse/HADOOP-18057","closed","","ashutoshcipher","2021-12-24T14:10:21Z","2021-12-27T08:51:45Z"
"","3835","HDFS-16409. Fix typo: testHasExeceptionsReturnsCorrectValue -> test…","### Description of PR Fix typo: `testHasExeceptionsReturnsCorrectValue` -> `testHasExceptionsReturnsCorrectValue `  * JIRA: HDFS-16409","closed","","ashutoshcipher","2021-12-28T07:32:02Z","2022-01-04T04:27:04Z"
"","4195","HADOOP-17564. Fix typo in UnixShellGuide.html","### Description of PR Fix typo in UnixShellGuide.html  * JIRA: HADOOP-17564","closed","","ashutoshcipher","2022-04-18T17:34:42Z","2022-04-22T16:59:42Z"
"","3849","HDFS-16393. RBF: Fix TestRouterRPCMultipleDestinationMountTableResolver.","### Description of PR Fix the test  ### How was this patch tested? Ran test locally: Fails without fix & passes with the fix  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2022-01-03T09:00:32Z","2022-01-04T02:45:36Z"
"","4197","HADOOP-17479. Fix the examples of hadoop config prefix","### Description of PR Fix the examples of hadoop config prefix  * JIRA: HADOOP-17479","closed","","ashutoshcipher","2022-04-18T17:58:23Z","2022-05-07T23:09:25Z"
"","3862","HDFS-11107.TestStartup#testStorageBlockContentsStaleAfterNNRestart fails intermittently.","### Description of PR Fix TestStartup#testStorageBlockContentsStaleAfterNNRestart test failure.  ### How was this patch tested? UT.","open","","jianghuazhu","2022-01-05T14:59:09Z","2022-03-04T15:54:33Z"
"","4330","HADOOP-18238.Fix reentrancy check in SFTPFileSystem.close()","### Description of PR Fix reentrancy check in SFTPFileSystem.close() * JIRA: HADOOP-18238   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-19T01:24:37Z","2022-05-30T16:34:06Z"
"","4672","HDFS-16712. Fix incorrect placeholder in DataNode.java","### Description of PR Fix incorrect placeholder in DataNode.java ``` public String getDiskBalancerStatus() {   try {     return getDiskBalancer().queryWorkStatus().toJsonString();   } catch (IOException ex) {     // incorrect placeholder     LOG.debug(""Reading diskbalancer Status failed. ex:{}"", ex);     return """";   } }  ```","closed","","ZanderXu","2022-08-02T13:28:12Z","2022-08-03T08:08:43Z"
"","4393","YARN-9827.Fix Http Response code in GenericExceptionHandler","### Description of PR Fix Http Response code in GenericExceptionHandler * JIRA: YARN-9827   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-31T17:29:59Z","2022-06-20T05:14:48Z"
"","4193","HDFS-16255. RBF: Fix dead link to fedbalance document","### Description of PR Fix dead link to fedbalance document  * JIRA: HDFS-16255","closed","","ashutoshcipher","2022-04-18T16:50:42Z","2022-04-25T05:11:42Z"
"","4271","YARN-11128. Fix comments in TestProportionalCapacityPreemptionPolicy*","### Description of PR Fix comments in TestProportionalCapacityPreemptionPolicy* * JIRA: YARN-11128  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-06T14:47:40Z","2022-05-26T06:52:15Z"
"","4194","HDFS-16185. Fix comment in LowRedundancyBlocks.java","### Description of PR Fix comment in LowRedundancyBlocks.java  * JIRA: HDFS-16185","closed","","ashutoshcipher","2022-04-18T16:56:00Z","2022-05-07T23:05:34Z"
"","4658","YARN-11237. Fix Bug while disabling proxy failover with Federation","### Description of PR Fix Bug while disabling proxy failover with Federation  When one disables the use of RM fail over proxy with federation, there is a bug checking a wrong/parent flag `yarn.federation.enabled` whether the federation is used instead of the fail over feature flag `yarn.federation.failover.enabled` of federation. Without this change, when fail over feature is disabled, node manager cannot be started.  JIRA - YARN-11237  ### How was this patch tested? Tested on EMR cluster and modified existing test.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-29T14:37:17Z","2022-08-03T10:03:34Z"
"","4627","HADOOP-18363. Fix bug preventing hadoop-metrics2 from emitting metrics to > 1 Ganglia servers","### Description of PR Fix bug preventing hadoop-metrics2 from emitting metrics to > 1 Ganglia servers  JIRA - HADOOP-18363  The patch is tested internally in EMR cluster. (via internal integration test)  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-25T13:20:21Z","2022-07-28T08:40:36Z"
"","3825","HADOOP-18056. DistCp: Filter duplicates in the source paths.","### Description of PR Filter duplicates from source paths  ### How was this patch tested? UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2021-12-24T09:02:25Z","2022-01-05T18:23:08Z"
"","3957","HADOOP-18102. Extract method to check two URIs are from same hosts","### Description of PR Extracted the logic out to a new method. Ran existing unit tests. Added a new test  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","cheyu2022","2022-02-03T01:00:54Z","2022-02-03T18:27:44Z"
"","4585","HADOOP-18383. Codecs with @DoNotPool annotation are not closed causing memory leak","### Description of PR Explicitly call `end()` when returning `Compressor` or `Decompressor` implementations with `DoNotPool` annotation to the `CodecPool`.  ### How was this patch tested? I created the following [project](https://github.com/kevins-29/hadoop-gzip-memory-leak) to demo the leak. You can run the demo with   ``` shell ./gradlew run ```   and then monitor the memory usage using  ```shell while true; do echo \""$(date +%Y-%m-%d' '%H:%M:%S)\"",$(pmap -x  | grep ""total kB"" | awk '{print $4}'); sleep 10; done; ```  ### Results - Before Patch  ``` ""2022-07-18 03:21:49"",1113060 ""2022-07-18 03:22:00"",1126184 ""2022-07-18 03:22:10"",1126248 ""2022-07-18 03:22:20"",1126248 ""2022-07-18 03:22:30"",1130204 ""2022-07-18 03:22:40"",1130216 ""2022-07-18 03:22:50"",1130244 ""2022-07-18 03:23:00"",1130776 ""2022-07-18 03:23:10"",1130776 ""2022-07-18 03:23:20"",1130776 ""2022-07-18 03:23:30"",1130776 ""2022-07-18 03:23:40"",1130888 ""2022-07-18 03:23:50"",1130888 ""2022-07-18 03:24:00"",1130888 ""2022-07-18 03:24:10"",1130928 ""2022-07-18 03:24:20"",1130928 ""2022-07-18 03:24:30"",1130928 ""2022-07-18 03:24:40"",1131204 ""2022-07-18 03:24:50"",1131204 ""2022-07-18 03:25:00"",1131204 ""2022-07-18 03:25:10"",1131204 ""2022-07-18 03:25:20"",1139044 ""2022-07-18 03:25:30"",1140900 ""2022-07-18 03:25:40"",1140900 ""2022-07-18 03:25:50"",1140900 ""2022-07-18 03:26:00"",1140900 ""2022-07-18 03:26:10"",1141164 ""2022-07-18 03:26:20"",1141164 ""2022-07-18 03:26:30"",1141164 ""2022-07-18 03:26:40"",1141164 ""2022-07-18 03:26:50"",1141164 ""2022-07-18 03:27:00"",1141164 ""2022-07-18 03:27:10"",1141164 ```  ### Results - After Patch ``` ""2022-07-18 03:34:36"",1098112 ""2022-07-18 03:34:46"",1098112 ""2022-07-18 03:34:56"",1098204 ""2022-07-18 03:35:06"",1098152 ""2022-07-18 03:35:16"",1098152 ""2022-07-18 03:35:26"",1098172 ""2022-07-18 03:35:36"",1098172 ""2022-07-18 03:35:46"",1098172 ""2022-07-18 03:35:57"",1098172 ""2022-07-18 03:36:07"",1098268 ""2022-07-18 03:36:17"",1098268 ""2022-07-18 03:36:27"",1098268 ""2022-07-18 03:36:37"",1098292 ""2022-07-18 03:36:47"",1098292 ""2022-07-18 03:36:57"",1098292 ""2022-07-18 03:37:07"",1098320 ""2022-07-18 03:37:17"",1098320 ""2022-07-18 03:37:27"",1098320 ""2022-07-18 03:37:37"",1098320 ""2022-07-18 03:37:47"",1098320 ""2022-07-18 03:37:57"",1098340 ""2022-07-18 03:38:07"",1098340 ""2022-07-18 03:38:17"",1098340 ```","open","","kevins-29","2022-07-18T17:37:44Z","2022-08-02T16:47:17Z"
"","4446","HADOOP-18294.Ensure build folder exists before writing checksum file.…","### Description of PR Ensure build folder exists before writing checksum file.ProtocRunner#writeChecksums * JIRA: HADOOP-18294   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-06-16T13:36:17Z","2022-07-12T11:17:13Z"
"","4559","HDFS-16658. Change logLevel from DEBUG to INFO if logEveryBlock is true","### Description of PR During locating some abnormal cases about block replication in our prod environment, I found that BlockManager does not out put some logs in `addStoredBlock` even though `logEveryBlock` is true.  I feel that we need to change the log level from DEBUG to INFO when `logEveryBlock` is true. So that we can more easily locate some abnormal cases. ``` private Block addStoredBlock(final BlockInfo block,                                final Block reportedBlock,                                DatanodeStorageInfo storageInfo,                                DatanodeDescriptor delNodeHint,                                boolean logEveryBlock)   throws IOException {     ....       if (logEveryBlock) {         blockLog.debug(""BLOCK* addStoredBlock: {} is added to {} (size={})"",             node, storedBlock, storedBlock.getNumBytes());       }     ...   } ```","closed","","ZanderXu","2022-07-13T13:00:18Z","2022-07-28T07:31:11Z"
"","4661","HDFS-16704. Datanode return empty response instead of NPE for GetVolumeInfo during restarting","### Description of PR During datanode starting, I found some NPE in logs: ``` Caused by: java.lang.NullPointerException: Storage not yet initialized     at org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899)     at org.apache.hadoop.hdfs.server.datanode.DataNode.getVolumeInfo(DataNode.java:3533)     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:498)     at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)     at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)     at java.lang.reflect.Method.invoke(Method.java:498)     at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:276)     at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)     at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)  ```  Because the storage of datanode not yet initialized when we trying to get metrics of datanode, and related code as below: ``` @Override // DataNodeMXBean public String getVolumeInfo() {   Preconditions.checkNotNull(data, ""Storage not yet initialized"");   return JSON.toString(data.getVolumeInfoMap()); }  ```  The logic is ok, but I feel that the more reasonable logic should be return an empty response instead of NPE, because InfoServer will be started before initBlockPool.","open","","ZanderXu","2022-07-30T03:40:26Z","2022-08-03T16:42:17Z"
"","4183","HDFS-16035. Remove DummyGroupMapping as it is not longer used anywhere.","### Description of PR DummyGroupMapping class was added as part of [HDFS-2657](https://issues.apache.org/jira/browse/HDFS-2657) and it was only used in TestHttpFSServer as httpfs.groups.hadoop.security.group.mapping. However, TestHttpFSServer is no longer using DummyGroupMapping and hence, it can be removed completely as it is not used anywhere.  * JIRA: HDFS-16035","closed","","ashutoshcipher","2022-04-17T12:53:19Z","2022-04-19T05:35:32Z"
"","3860","HADOOP-18065 ExecutorHelper.logThrowableFromAfterExecute() is too noisy.","### Description of PR Downgrading warn logs to debug in case of InterruptedException   ### How was this patch tested? This is just a log level change. So no new tests required.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-01-05T10:15:19Z","2022-01-10T09:29:28Z"
"","3855","MAPREDUCE-7371. DistributedCache alternative APIs should not use DistributedCache APIs internally","### Description of PR DistributedCache has been deprecated long back and it's still being used as of today. However, all the alternative Job APIs (for deprecated DistributedCache APIs) still internally use DistributedCache APIs only, this is a deadlock and it leaves no room for removal of DistributedCache APIs in future. We should move core logic to Job or JobContext as required and let deprecated DistributedCache APIs point to the right alternatives internally.  ### How was this patch tested? Unit Tests  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-01-04T16:11:35Z","2022-06-22T11:58:08Z"
"","4461","HADOOP-18303. Remove shading exclusion of javax.ws.rs-api from hadoop-client-runtime","### Description of PR Discussion on HADOOP-18033","closed","","virajjasani","2022-06-18T22:19:59Z","2022-07-24T06:11:15Z"
"","4264","HDFS-16568. dfsadmin -reconfig option to start/query reconfig on all live datanodes","### Description of PR DFSAdmin provides option to initiate or query the status of reconfiguration operation on only specific host based on host:port provided by user. It would be good to provide an ability to initiate such operations in bulk, on all live datanodes.  ### How was this patch tested? Dev cluster and UT. Sample outputs:  ``` $ bin/hdfs dfsadmin -reconfig datanode livenodes start 2022-05-04 22:17:44,695 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Started reconfiguration task on node [host1:port1]. Started reconfiguration task on node [host2:port2]. Started reconfiguration task on node [host3:port3]. Started reconfiguration task on node [host4:port4]. Started reconfiguration task on node [host5:port5]. Starting of reconfiguration task successful on 5 nodes, failed on 0 nodes.   $ bin/hdfs dfsadmin -reconfig datanode livenodes status 2022-05-04 21:57:36,506 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Reconfiguring status for node [host1:port1]: started at Wed May 04 21:57:04 PDT 2022 and finished at Wed May 04 21:57:04 PDT 2022. SUCCESS: Changed property dfs.datanode.peer.stats.enabled 	From: ""false"" 	To: ""true"" Reconfiguring status for node [host2:port2]: started at Wed May 04 21:57:06 PDT 2022 and finished at Wed May 04 21:57:06 PDT 2022. SUCCESS: Change property dfs.datanode.peer.stats.enabled 	From: ""false"" 	To: ""true"" Reconfiguring status for node [host3:port3]: started at Wed May 04 21:57:08 PDT 2022 and finished at Wed May 04 21:57:08 PDT 2022. SUCCESS: Change property dfs.datanode.peer.stats.enabled 	From: ""false"" 	To: ""true"" Reconfiguring status for node [host4:port4]: started at Wed May 04 21:57:10 PDT 2022 and finished at Wed May 04 21:57:10 PDT 2022. SUCCESS: Change property dfs.datanode.peer.stats.enabled 	From: ""false"" 	To: ""true"" Reconfiguring status for node [host5:port5]: started at Wed May 04 21:57:12 PDT 2022 and finished at Wed May 04 21:57:12 PDT 2022. SUCCESS: Change property dfs.datanode.peer.stats.enabled 	From: ""false"" 	To: ""true"" Retrieval of reconfiguration status successful on 5 nodes, failed on 0 nodes. ```   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-05-05T05:44:31Z","2022-05-11T00:11:20Z"
"","3961","HADOOP-14451. Deadlock in NativeIO","### Description of PR Deadlock in NativeIO  * JIRA: HADOOP-14451","closed","","ashutoshcipher","2022-02-06T15:37:54Z","2022-02-07T05:36:19Z"
"","4267","HADOOP-18224. Upgrade maven compiler plugin to 3.10.1","### Description of PR Currently we are using maven-compiler-plugin 3.1 version, which is quite old (2013) and it's also pulling in vulnerable log4j dependency: ``` [INFO]    org.apache.maven.plugins:maven-compiler-plugin:maven-plugin:3.1:runtime [INFO]       org.apache.maven.plugins:maven-compiler-plugin:jar:3.1 [INFO]       org.apache.maven:maven-plugin-api:jar:2.0.9 [INFO]       org.apache.maven:maven-artifact:jar:2.0.9 [INFO]       org.codehaus.plexus:plexus-utils:jar:1.5.1 [INFO]       org.apache.maven:maven-core:jar:2.0.9 [INFO]       org.apache.maven:maven-settings:jar:2.0.9 [INFO]       org.apache.maven:maven-plugin-parameter-documenter:jar:2.0.9 ... ... ... [INFO]       log4j:log4j:jar:1.2.12 [INFO]       commons-logging:commons-logging-api:jar:1.1 [INFO]       com.google.collections:google-collections:jar:1.0 [INFO]       junit:junit:jar:3.8.2 ``` We should upgrade to 3.10.1 (latest Mar, 2022) version of maven-compiler-plugin.","closed","","virajjasani","2022-05-05T23:08:18Z","2022-05-20T18:21:41Z"
"","4135","HADOOP-18188. Support touch command for directory","### Description of PR Currently hadoop fs -touch command cannot update the mtime and the atime of directory. The feature would be useful when we check whether the filesystem is ready to write or not without creating any file.  ### How was this patch tested? Local dev cluster and UTs.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-04-04T13:04:56Z","2022-04-07T08:29:46Z"
"","4095","HADOOP-18170. Collect disk I/Os time on the node","### Description of PR Collect disk I/Os time on SysInfo, supported for Linux. Add IoTimeTracker, calculate a coefficient for each configured mounted disk to describe disk load in periods.   ### How was this patch tested? Manual","open","","Deegue","2022-03-23T14:57:00Z","2022-03-23T20:48:41Z"
"","4522","HADOOP-18323.Bump javax.ws.rs-api to 3.1.0","### Description of PR Bump javax.ws.rs-api To Version 3.1.0 to mitigate [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)  JIRA - HADOOP-18323  ### How was this patch tested? CI/Build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-06-30T22:42:06Z","2022-07-06T10:36:41Z"
"","4204","HADOOP-18211. Bump aliyun-sdk-oss to 3.14.0","### Description of PR Bump aliyun-sdk-oss to 3.14.0  * JIRA: HADOOP-18211","open","","ashutoshcipher","2022-04-19T16:21:02Z","2022-04-24T21:55:58Z"
"","3764","HADOOP-18033. Upgrade fasterxml Jackson to 2.13.0","### Description of PR branch-3.3 backport PR of #3749   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-08T10:16:27Z","2022-06-17T17:33:45Z"
"","4064","HDFS-16502. Reconfigure Block Invalidate limit","### Description of PR Based on the cluster load, it would be helpful to consider tuning block invalidate limit (dfs.block.invalidate.limit). The only way we can do this without restarting Namenode as of today is by reconfiguring heartbeat interval `Math.max(heartbeatInt*20, blockInvalidateLimit)`, this logic is not straightforward and operators are usually not aware of it (lack of documentation), also updating heartbeat interval is not desired in all the cases.  We should provide the ability to alter block invalidation limit without affecting heartbeat interval on the live cluster to adjust some load at Datanode level. We should also take this opportunity to keep (heartbeatInterval * 20) computation logic in a common method.  ### How was this patch tested? Locally and with UT.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-03-12T10:14:51Z","2022-03-16T01:32:30Z"
"","4258","YARN-11125. Backport YARN-6483 to branch-2.10","### Description of PR Backport YARN-6483 to branch-2.10 * JIRA: YARN-11125   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-03T01:07:41Z","2022-05-13T16:23:14Z"
"","4144","HADOOP-18172: Change scope of InodeTree and its member methods to make them accessible from outside package.","### Description of PR backport of HADOOP-18172 for branch-2.10  ### How was this patch tested?  mvn install -DskipTests","closed","","xinglin","2022-04-06T18:32:43Z","2022-04-20T23:26:49Z"
"","4015","HADOOP-18127. Backport HADOOP-13055 into branch-2.10","### Description of PR Back porting HADOOP-13055 needed two more commits. So this PR  includes three commits:  -  HADOOP-13722. Code cleanup -- ViewFileSystem and InodeTree. - HADOOP-12077. Provide a multi-URI replication Inode for ViewFs. - HADOOP-13055. Implement linkMergeSlash and linkFallback for ViewFileSystem  ### How was this patch tested? `mvn clean test -Dtest=Test*ViewF*`","closed","","shvachko","2022-02-22T18:02:41Z","2022-03-16T00:04:42Z"
"","3824","HADOOP-18055. Async Profiler endpoint for Hadoop daemons","### Description of PR Async profiler (https://github.com/jvm-profiling-tools/async-profiler) is a low overhead sampling profiler for Java that does not suffer from Safepoint bias problem. It features HotSpot-specific APIs to collect stack traces and to track memory allocations. The profiler works with OpenJDK, Oracle JDK and other Java runtimes based on the HotSpot JVM. Async profiler can also profile heap allocations, lock contention, and HW performance counters in addition to CPU.  This PR provides Async profiler endpoint for Hadoop daemons.  More analysis on CPU and memory overhead:  1. https://github.com/jvm-profiling-tools/async-profiler/issues/14 2. https://github.com/jvm-profiling-tools/async-profiler/issues/131  ### How was this patch tested? Locally. Screenshots:                               ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-22T15:18:54Z","2022-01-06T09:56:49Z"
"","4519","HDFS-16646. RBF: Support an elastic RouterRpcFairnessPolicyController","### Description of PR As we all known, `StaticRouterRpcFairnessPolicyController` is very helpfully for RBF to minimize impact of clients connecting to healthy vs unhealthy nameNodes.  But in prod environment, the traffic of clients accessing each NS and the pressure of downstream namenodes are dynamically changed. So if we only have one static permit conf, RBF cannot able to adapt to the changes in traffic to achieve optimal results.   So here I propose an elastic RouterRpcFairnessPolicyController to help RBF adapt to traffic changes to achieve an optimal result.  The overall idea is: - Each name service can configured the exclusive permits like `StaticRouterRpcFairnessPolicyController` - TotalPermits is more than sum(NsExclusivePermit) and mark TotalPermits - sum(NsExclusivePermit) as SharedPermits - Each name service can properly preempt the SharedPermits after it's own exclusive permits is used up. - But the maximum value of SharedPermits preempted by each nameservice should be limited. Such as 20% of SharedPermits.  Suppose we have 200 handlers and 5 name services, and each name services configured different exclusive Permits, like: | NS1 | NS2 | NS3 | NS4 | NS5 | Concurrent NS | |-- | -- | -- | -- | -- | -- | | 9 | 11 | 8 | 12 | 10 | 50 |  The `sum(NsExclusivePermit)` is 100, and the `SharedPermits = TotalPermits(200) - Sum(NsExclusivePermit)(100) = 100` Suppose we configure that each nameservice can preempt up to 20% of TotalPermits, marked as `elasticPercent`.  Then from the point view of a single NS, the permits it may be can use are as follow: - Exclusive Permits, which is cannot be used by other name services. - Limited SharedPermits, whether is can use so many shared permits depends on the remaining number of SharedPermits, because the SharedPermits is be preempted by all nameservices.  If we configure the `elasticPercent=100`, it means one nameservices can use up all SharedPermits. If we configure the `elasticPercent=0`, it means nameservice can only use it's exclusive Permits. If we configure the `elasticPercent=20`, it means that the RBF can tolerate 5 unhealthy name services at the same time.  In our prod environment, we configured as follow, and it works well: - RBF has 3000 handlers - Each nameservice has 10 exclusive permits - `elasticPercent` is 30%  Of course, we need to configure reasonable parameters according to the prod traffic.","open","","ZanderXu","2022-06-30T11:05:52Z","2022-07-11T03:55:05Z"
"","4056","HADOOP-18142. Increase precommit job timeout from 24 hr to 30 hr","### Description of PR As per some recent precommit build results, full build QA is not getting completed in 24 hr (recent example [here](https://github.com/apache/hadoop/pull/4000) where more than 5 builds timed out after 24 hr). We should increase it to 30 hr.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","virajjasani","2022-03-09T07:02:40Z","2022-03-09T08:00:09Z"
"","3991","HADOOP-18125. Utility to identify git commit / Jira fixVersion discrepancies for RC preparation","### Description of PR As part of RC preparation,  we need to identify all git commits that landed on release branch, however their corresponding Jira is either not resolved yet or does not contain expected fixVersions. Only when we have git commits and corresponding Jiras with expected fixVersion resolved, we get all such Jiras included in auto-generated CHANGES.md as per Yetus changelog generator.  Proposal of this Jira is to provide such script that can be useful for all upcoming RC preparations and list down all Jiras where we need manual intervention. This utility script should use Jira API to retrieve individual fields and use git log to loop through commit history.  The script should identify these issues:  1. commit is reverted as per commit message 2. commit does not contain Jira number format (e.g. HADOOP-XXXX / HDFS-XXXX etc) in message 3. Jira does not have expected fixVersion 4. Jira has expected fixVersion, but it is not yet resolved 5. Jira has release corresponding fixVersion and is resolved, but no corresponding commit yet found  It can take inputs as:  1. First commit hash to start excluding commits from history 2. Fix Version 3. JIRA Project Name 4. Path of project's working dir 5. Jira server url  ### How was this patch tested? Locally.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-02-14T09:47:13Z","2022-02-22T03:00:38Z"
"","4133","HADOOP-18191. Log retry count while handling exceptions in RetryInvocationHandler","### Description of PR As part of failure handling in RetryInvocationHandler, we log details of the Exception details with which API was invoked, failover attempts, delay.  For the purpose of better debugging as well as fine-tuning of retry params, it would be good to also log retry count that we already maintain in the Counter object.","closed","","virajjasani","2022-04-04T11:24:07Z","2022-04-08T12:45:55Z"
"","3768","HADOOP-18039. Upgrade hbase2 version and fix TestTimelineWriterHBaseDown","### Description of PR As mentioned on HADOOP-17668, we can't upgrade hbase2 profile version beyond 2.2.4 until we either have hbase 2 artifacts available that are built with hadoop 3 profile by default or hbase 3 is rolled out (hbase 3 is compatible with hadoop 3 versions only). This PR is to upgrade hbase2 profile version to 2.2.4 and also fix TestTimelineWriterHBaseDown to create connection only after mini cluster is up.  ### How was this patch tested? Local testing. For unit testing, here is the test result with hbase2 profile: ``` [INFO] ------------------------------------------------------- [INFO]  T E S T S [INFO] ------------------------------------------------------- [INFO] Running org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage [INFO] Tests run: 33, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.996 s - in org.apache.hadoop.yarn.server.timelineservice.reader.TestTimelineReaderWebServicesHBaseStorage [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.288 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineWriterHBaseDown [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageApps [INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.485 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageApps [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown [INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 201.434 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestTimelineReaderHBaseDown [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageSchema [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 59.86 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageSchema [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageDomain [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 27.672 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageDomain [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageEntities [INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 30.734 s - in org.apache.hadoop.yarn.server.timelineservice.storage.TestHBaseTimelineStorageEntities [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowActivity [INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.285 s - in org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowActivity [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRunCompaction [INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 38.799 s - in org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRunCompaction [INFO] Running org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRun [INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 68.664 s - in org.apache.hadoop.yarn.server.timelineservice.storage.flow.TestHBaseStorageFlowRun [INFO]  [INFO] Results: [INFO]  [INFO] Tests run: 103, Failures: 0, Errors: 0, Skipped: 0 [INFO]  [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  09:33 min [INFO] Finished at: 2021-12-08T19:32:42+05:30 [INFO] ------------------------------------------------------------------------ ```  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-12-08T14:55:13Z","2021-12-13T09:19:21Z"
"","3913","HADOOP-18089. Test coverage for Async profiler servlets","### Description of PR As discussed in HADOOP-18077, we should provide sufficient test coverage to discover any potential regression in async profiler servlets: ProfileServlet and ProfileOutputServlet.  ### How was this patch tested? UT  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-01-21T08:05:22Z","2022-01-26T03:24:17Z"
"","4523","[Draft] HDFS-13522: Allow routers to use observer namenode without an msync on even read. [Not ready for review]","### Description of PR Allow routers to use observer namenode without an msync on even read. Is layered on top of the following two  - https://github.com/apache/hadoop/pull/4311 - https://github.com/apache/hadoop/pull/4127 and   I'm still working on cleaning up this PR to add documentation, pick better variable names and remove unneeded features like ""disabling observer read from the client side"". I will also move IPC related changes to the first PR in the series (https://github.com/apache/hadoop/pull/4311)  ### How was this patch tested?  New unit tests.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","simbadzina","2022-06-30T23:51:31Z","2022-07-14T06:11:00Z"
"","4352","HADOOP-17461. Thread-level IOStatistics in S3A","### Description of PR Adding Thread-level IOStatsitics in hadoop-common and implementing it in S3A Streams.  ### How was this patch tested? Region: ap-south-1 `mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale`  All tests ran fine. ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mehakmeet","2022-05-24T13:38:57Z","2022-07-26T19:42:51Z"
"","3904","HADOOP-11867. Add a high-performance vectored read API.","### Description of PR Adding support for multiple ranged read async api in PositionedReadable. The default iterates through the ranges to read each synchronously, but the intent is that FSDataInputStream subclasses can make more efficient readers especially object stores implementation.  ### How was this patch tested? Added benchmarks. Added UT's Added new contract tests for new API spec.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-01-19T08:00:04Z","2022-02-01T14:22:38Z"
"","3851","YARN-11055. Add missing newline in cgroups-operations.c","### Description of PR Adding missing newline characters in the end of fprintf format strings in cgroups-operations.c  ### How was this patch tested? pseudo-distributed deploy https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/UsingGpus.md#distributed-shell--gpu-with-docker ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","yarn,","gerashegalov","2022-01-03T19:52:48Z","2022-01-18T18:33:20Z"
"","4572","HADOOP-18330-S3AFileSystem removes Path when calling createS3Client","### Description of PR added new parameter object in s3ClientCreationParameters that returns full s3a path  ### How was this patch tested? Tested against the following endpoint -> s3.us-east-1.amazonaws.com  ### For code changes:  - [ X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshpant","2022-07-16T20:54:35Z","2022-07-21T09:16:39Z"
"","4168","HDFS-16539. RBF: Support refreshing/changing router fairness policy controller without rebooting router","### Description of PR Add support for refreshing/changing router fairness policy controller without the need to shutdown and boot a router.  This patch makes use of the generic refresh feature on RouterAdmin. Usage: `hdfs dfsrouteradmin -refreshRouterArgs ROUTER_ADDR RefreshFairnessPolicyController`  ### How was this patch tested? Unit test and local deployment.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","kokonguyen191","2022-04-13T10:44:44Z","2022-04-27T06:42:43Z"
"","4356","HDFS-15225. RBF: Add snapshot counts to content summary in router.","### Description of PR Add snapshot counts in getContentSummary  ### How was this patch tested? UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2022-05-25T10:48:02Z","2022-05-27T03:36:08Z"
"","3712","HADOOP-18022. Add restrict-imports-enforcer-rule for Guava Preconditions and remove remaining usages","### Description of PR Add restrict-imports-enforcer-rule for Guava Preconditions in hadoop-main pom to restrict any new import in future. Remove any remaining usages of Guava Preconditions from the codebase.  ### How was this patch tested? Local testing.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-23T11:50:02Z","2021-11-29T08:38:57Z"
"","4244","YARN-11119. Backport YARN-10538 to branch-2.10","### Description of PR Add recommissioning nodes to the list of updated nodes returned to the AM  Backport YARN-10538 to branch-2.10  * JIRA: YARN-11119","closed","","ashutoshcipher","2022-04-28T15:30:43Z","2022-06-27T01:31:52Z"
"","3856","HDFS-16408. Negative LeaseRecheckIntervalMs will let LeaseMonitor loop forever and print huge amount of log","### Description of PR Add Preconditions.checkArgument() to ensure the leaseRecheckIntervalMs cannot get set to a value less than or equal to zero.   It avoids negative intervals that cause LeaseMonitor to be in a constant state of fast printing exception logs  JIRA: [HDFS-16408](https://issues.apache.org/jira/browse/HDFS-16408)  ### How was this patch tested? when the configuration item 'dfs.namenode.lease-recheck-interval-ms' is accidentally set to a negative number, origin code will print warning log again and again immediately. This problem can be eliminated after using this patch.","closed","","liever18","2022-01-05T05:39:01Z","2022-01-05T15:42:59Z"
"","4098","YARN-11098. Support automatically killing container when the container load is high","### Description of PR Add CPU monitor for containers in ContainerMonitor thread. Just like the memory limit, container will be killed after exceeding CPU threshold for configured times.  ### How was this patch tested? UTs and manual","open","","Deegue","2022-03-23T15:18:42Z","2022-03-23T18:33:33Z"
"","4377","YARN-11115. Add configuration to globally disable AM preemption for capacity scheduler","### Description of PR Add configuration to globally disable AM preemption for the capacity scheduler * JIRA: YARN-11115   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","ashutoshcipher","2022-05-30T12:16:43Z","2022-07-12T11:23:25Z"
"","4199","HDFS-14750. RBF: Support dynamic handler allocation in routers","### Description of PR Add a `DynamicRouterRpcFairnessPolicyController` class that resizes permit capacity periodically based on traffic to namespaces.  ### How was this patch tested? Unit tests and local deployment.  ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","kokonguyen191","2022-04-19T07:56:30Z","2022-05-13T06:28:53Z"
"","4307","HDFS-14750. RBF: Support dynamic handler allocation in routers","### Description of PR Add a `DynamicRouterRpcFairnessPolicyController` class that resizes permit capacity periodically based on traffic to namespaces + minor fixes to make it work with HDFS-16539  ### How was this patch tested? Unit tests, local deployment + modelling for performance improvement.  ### For code changes: Added a few fixes in comparison with https://github.com/apache/hadoop/pull/4168  - Fix dead executor when dynamic controller is created more than once - Controller refresh does not work with dynamic controller","open","","kokonguyen191","2022-05-13T06:54:11Z","2022-05-27T06:07:27Z"
"","4337","HDFS-16585.Add @VisibleForTesting in Dispatcher.java","### Description of PR Add @VisibleForTesting in Dispatcher.java * JIRA: HDFS-16585   - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ashutoshcipher","2022-05-20T18:34:58Z","2022-05-26T21:18:15Z"
"","4300","HADOOP-18120 Hadoop auth does not handle HTTP Headers in a case-insensitive way","### Description of PR According to RFC-2616 HTTP Headers are case-insensitive. There are proxies / load balancers (e.g.: newer versions of HA-proxy) which deliberately make some of the HTTP headers lower-case results in an authentication / authorization failure inside the Hadoop codebase.  This patch is responsible for resolving the authentication issue.   ### How was this patch tested? Tested on our test cluster where we could reproduce the original issue. I also ran the corresponding old and new unit tests.","closed","","jacktheone","2022-05-11T07:44:21Z","2022-05-20T08:52:17Z"
"","4518","HDFS-16645. [JN] Bugfix: java.lang.IllegalStateException: Invalid log manifest","### Description of PR ``` java.lang.IllegalStateException: Invalid log manifest (log [1-? (in-progress)] overlaps [6-? (in-progress)])[[6-? (in-progress)], [1-? (in-progress)]] CommittedTxId: 0      at org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest.checkState(RemoteEditLogManifest.java:62)     at org.apache.hadoop.hdfs.server.protocol.RemoteEditLogManifest.(RemoteEditLogManifest.java:46)     at org.apache.hadoop.hdfs.qjournal.server.Journal.getEditLogManifest(Journal.java:740) ```","open","","ZanderXu","2022-06-30T09:37:57Z","2022-07-28T03:40:23Z"
"","4466","HDFS-16637. TestHDFSCLI#testAll consistently failing","### Description of PR ``` 2022-06-19 15:41:16,183 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(146)) - Detailed results: 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(147)) - ----------------------------------  2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(156)) - ------------------------------------------- 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(157)) -                     Test ID: [629] 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(158)) -            Test Description: [printTopology: verifying that the topology map is what we expect] 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(159)) -  2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(163)) -               Test Commands: [-fs hdfs://localhost:51486 -printTopology] 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(167)) -  2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(174)) -  2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(178)) -                  Comparator: [RegexpAcrossOutputComparator] 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(180)) -          Comparision result:   [fail] 2022-06-19 15:41:16,184 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(182)) -             Expected output:   [^Rack: \/rack1\s*127\.0\.0\.1:\d+\s\([-.a-zA-Z0-9]+\)\s*127\.0\.0\.1:\d+\s\([-.a-zA-Z0-9]+\)] 2022-06-19 15:41:16,185 [Listener at localhost/51519] INFO  cli.CLITestHelper (CLITestHelper.java:displayResults(184)) -               Actual output:   [Rack: /rack1    127.0.0.1:51487 (localhost) In Service    127.0.0.1:51491 (localhost) In Service  Rack: /rack2    127.0.0.1:51500 (localhost) In Service    127.0.0.1:51496 (localhost) In Service    127.0.0.1:51504 (localhost) In Service  Rack: /rack3    127.0.0.1:51508 (localhost) In Service  Rack: /rack4    127.0.0.1:51512 (localhost) In Service    127.0.0.1:51516 (localhost) In Service  ] ```  ### How was this patch tested? UT  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2022-06-19T22:52:40Z","2022-06-21T09:17:32Z"
"","4275","YARN-11130:Rmove RouterClientRMService Has Unused import","### Description of PR [YARN-11130:org.apache.hadoop.yarn.server.router.clientrm#RouterClientRMService Has Unused import](https://issues.apache.org/jira/browse/YARN-11130) During code debugging, it was found that RouterClientRMService has unreferenced import, java.io.InputStream, remove it  ### How was this patch tested? Not  Need Test  ### For code changes: remove unused import","closed","","slfan1989","2022-05-07T03:26:40Z","2022-05-07T05:12:31Z"
"","4474","HADOOP-18306: Warnings should not be shown on cli console when linux user not present on client","### Description of PR [Warnings should not be shown on cli console when linux user not present on client](https://issues.apache.org/jira/browse/HADOOP-18306) ### How was this patch tested? Logging Changes","closed","","swamirishi","2022-06-20T17:45:05Z","2022-06-28T00:20:59Z"
"","4665","HDFS-16707. RBF: Expose RouterRpcFairnessPolicyController related request record metrics for each nameservice to Prometheus","### Description of PR [HDFS-16302](https://issues.apache.org/jira/browse/HDFS-16302) intoduced request record for each namespace, but it is only exposed in /jmx endpoint and in json format, not very convenient. this patch exposed these metrics in /prom endpoint for Prometheus  ### How was this patch tested? manual testing","open","","qijiale76","2022-08-01T11:15:28Z","2022-08-01T17:44:24Z"
"","4524","HDFS-16283. RBF: reducing the load of renewLease() RPC","### Description of PR [HDFS-16283](https://issues.apache.org/jira/browse/HDFS-16283): RBF: improve renewLease() to call only a specific NameNode rather than make fan-out calls  Currently RBF will forward the renewLease() rpc to all the available name services. So the forwarding efficiency will be affected by the unhealthy downstream name services. And along with as more as NSs are monitored by RBF, this problem will become more and more serious.   In our prod cluster, there are 70+ nameservices, the phenomenon that renewLease() rpc is blocked often occurs.   This patch is be used to fix this problem and work well on our cluster, and the main ideas is: - Carrying nsId to client when creating a new file - Store this nsId in DFSOutputStream - Client renewLease() rpc carries all nsIds of all DFSOutputStream to RBF - RBF parses the nsIds and forwards the renewLease rpc to the corresponding name services","closed","","ZanderXu","2022-07-01T08:44:28Z","2022-07-14T02:16:58Z"
"","4220","HADOOP-18216. io.file.buffer.size must be greater than zero","### Description of PR [HADOOP-18216](https://issues.apache.org/jira/browse/HADOOP-18216) Modify the description of ""io.file.buffer.size"" in the default configuration file to prevent io operations from failing or blocking due to incorrect settings of this configuration item.","closed","","liever18","2022-04-22T11:15:46Z","2022-04-27T14:37:37Z"
"","4671","HDFS-16702. MiniDFSCluster should report cause of exception in assertion error","### Description of PR - MiniDFSCluster should report cause of exception in assertion error - Improve message of ExitException to include cause","open","","virajjasani","2022-08-02T00:41:26Z","2022-08-03T18:44:19Z"
"","4686","HADOOP-17234. Addendum. Add .asf.yaml to allow github and jira integration.","### Description of PR **Addendum to the previous change** Disables WorkLog entries & TimeTracking, which we aren't using, and tough to follow and shoots redundant mails. Instead the Jira comments as well as the Github Pr comments can all be seen together on the Jira.  https://issues.apache.org/jira/browse/INFRA-23544?focusedCommentId=17574650&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17574650  ### How was this patch tested?  Saw an example posted in INFRA-23544, Rest will get to know post merge only. :-)   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","ayushtkn","2022-08-03T14:14:22Z","2022-08-03T15:19:59Z"
"","3717","YARN-11005. Implement the core QUEUE_LENGTH_THEN_RESOURCES OContainer allocation policy","### Description of PR * Implement the `QUEUE_LENGTH_THEN_RESOURCES` `LoadComparator` * Add `Resource` requested to select node methods when allocating `OPPORTUNISTIC` containers * Add node capacity and node allocated resources to `ClusterNode` * Extend `DominantResourceCalculator` to be able to compute min-share * Add tests and extend existing ones to consider 3 resource dimensions  ### How was this patch tested? * Unit tests * Deployment to a production cluster","closed","","afchung","2021-11-24T18:17:29Z","2021-12-08T18:02:10Z"
"","3779","YARN-11015. Decouple queue capacity with ability to run OPPORTUNISTIC container","### Description of PR * Adds queueing policies `BY_RESOURCES` and `BY_QUEUE_LEN` at the NM * If `BY_RESOURCES` is specified, the NM will queue as long as it has enough resources to run all pending + running containers, otherwise, it will reject the OPPORTUNISTIC container * If `BY_QUEUE_LEN` is specified, the NM will only accept as many containers as its queue capacity is configured * Restructure `TestContainerSchedulerQueueing` to accommodate different queueing policies at the NM  ### How was this patch tested? * Unit tests * Deployment to a production cluster of 300+ nodes and observed the expected behavior via logs for each setting of both NM queueing policies: For `BY_QUEUE_LEN`, the behavior is the same as before and for `BY_RESOURCES`, we observe that the NM does not accept more containers than its resources can handle","closed","","afchung","2021-12-09T16:00:18Z","2022-01-24T16:03:37Z"
"","3854","HDFS-16410. Fixing Insecure Xml parsing in OfflineEditsXmlLoader","### Description of PR  Fixing Insecure Xml parsing in OfflineEditsXmlLoader  * JIRA: HDFS-16410","closed","","ashutoshcipher","2022-01-04T12:47:15Z","2022-01-05T18:02:07Z"
"","4385","Hadoop 18231 prefetching stream test","### Description of PR  WIP  ### How was this patch tested?","closed","","ahmarsuhail","2022-05-31T12:42:08Z","2022-05-31T13:15:51Z"
"","4680","HDFS-16702. MiniDFSCluster should report cause of exception in assert…","### Description of PR  When the MiniDFSClsuter detects that an exception caused an exit, it should include that exception as the cause for the AssertionError that it throws.  The current AssertError simply reports the message ""Test resulted in an unexpected exit"" and provides a stack trace to the location of the check for an exit exception.  This patch adds the original exception as the cause of the AssertError.  ### How was this patch tested?  This was used in local unit test runs during debugging.  The updated exception was particularly useful in CI unit test reports.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","snmvaughan","2022-08-03T01:39:47Z","2022-08-03T11:15:10Z"
"","3884","HDFS-16424. Add a new requeue rpcmetric","### Description of PR  When observer namenode read enabled,  if `call.getClientStateId() > alignmentContext.getLastSeenStateId()`, the rpc call will be requeued to callQueue. We need to know rpc requeue metrics to monitor requeue call and its performance.  If we don't expose this requeue metric directly, we will find it in flame graph, that is too bothersome.  ### https://issues.apache.org/jira/browse/HDFS-16424","open","","hfutatzhanghb","2022-01-13T09:34:47Z","2022-01-18T01:56:34Z"
"","4667","HDFS-16709. Remove redundant cast in FSEditLogOp.class","### Description of PR  When I read some class about Edits of NameNode, I found that there are much redundant cast in FSEditLogOp.class, I feel that we should remove them.  Such as: ``` static UpdateBlocksOp getInstance(OpInstanceCache cache) {   return (UpdateBlocksOp)cache.get(OP_UPDATE_BLOCKS); } ```   Because cache.get() have cast the response to T, so we can remove the redundant cast. ``` @SuppressWarnings(""unchecked"") public  T get(FSEditLogOpCodes opCode) {   return useCache ? (T)CACHE.get().get(opCode) : (T)newInstance(opCode); }  ```","open","","ZanderXu","2022-08-01T13:18:10Z","2022-08-03T10:04:49Z"
"","4408","YARN-11172. Fix TestClientRMTokens#testDelegationToken introduced by HDFS-16563.","### Description of PR  UT fail after [HDFS-16563], other yarn PR is blocked.  ``` [ERROR] testDelegationToken(org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens)  Time elapsed: 17.379 s  <<< FAILURE! java.lang.AssertionError 	at org.junit.Assert.fail(Assert.java:87) 	at org.junit.Assert.assertTrue(Assert.java:42) 	at org.junit.Assert.assertTrue(Assert.java:53) 	at org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens.testDelegationToken(TestClientRMTokens.java:207) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ```","closed","","zhengchenyu","2022-06-06T12:01:46Z","2022-06-17T18:49:36Z"
"","3918","HADOOP-18093: Better exception handling for testFileStatusOnMountLink","### Description of PR  Use `@Test(expected = FileNotFoundException.class)` to indicate we are expecting a FileNotFound exception. This makes it consistent with how similar exceptions are handled in this class as well.  ### How was this patch tested?  mvn test -Dtest=TestViewFsLocalFs","closed","","xinglin","2022-01-24T06:12:07Z","2022-01-26T16:26:07Z"
"","4397","HADOOP-18275. Update os-maven-plugin to 1.7.0","### Description of PR  Upgrade the plugin version. no compelling reason, just noticed as part of https://github.com/apache/parquet-mr/pull/970 that we were some versions behind.  ### How was this patch tested?  did local build on macbook m1  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-03T10:46:35Z","2022-06-08T15:48:32Z"
"","4240","HDFS-16562. Upgrade moment.min.js to 2.29.2","### Description of PR  Upgrade moment.min.js to 2.29.2 to resolve  https://nvd.nist.gov/vuln/detail/CVE-2022-24785","closed","","dmmkr","2022-04-27T17:12:31Z","2022-05-03T12:12:45Z"
"","4603","YARN-10793. Upgrade Junit from 4 to 5 in hadoop-yarn-server-applicationhistoryservice","### Description of PR  Upgrade Junit from 4 to 5 in hadoop-yarn-server-applicationhistoryservice  JIRA - YARN-10793  ### How was this patch tested?  Ran tests in local as well.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-21T10:07:21Z","2022-07-22T11:36:49Z"
"","4553","HADOOP-18333.Upgrade jetty version to 9.4.48.v20220622","### Description of PR  Upgrade jetty version to 9.4.48.v20220622 to mitigate CVE-2022-2047  JIRA: HADOOP-18333   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-07-12T00:02:20Z","2022-07-21T06:23:40Z"
"","3700","HADOOP-18001. Upgrade jetty version to 9.4.44","### Description of PR  Upgrade jetty version to 9.4.44: https://www.eclipse.org/lists/jetty-dev/msg03562.html","closed","","luoyuan3471","2021-11-22T07:23:10Z","2021-12-10T10:54:26Z"
"","4454","HADOOP-18300. Upgrade Gson dependency to version 2.9.0","### Description of PR  Upgrade Gson dependency to version 2.9.0  ### How was this patch tested?  Presubmit/CI  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","medb","2022-06-17T23:09:06Z","2022-06-23T00:45:40Z"
"","4455","HADOOP-18301.Upgrade commons-io to 2.11.0","### Description of PR  Upgrade commons-io to 2.11.0  Upgrading to new release to keep up for new features and bug fixes.  JIRA: HADOOP-18301   ### How was this patch tested? CI/Build Check   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-17T23:27:26Z","2022-08-03T01:44:40Z"
"","4539","HADOOP-17649. Update wildfly openssl to 2.2.1.Final","### Description of PR  Update wildfly openssl to 2.2.1.Final.     https://nvd.nist.gov/vuln/detail/CVE-2020-25644  A memory leak flaw was found in WildFly OpenSSL in versions prior to 1.1.3.Final, where it removes an HTTP session. It may allow the attacker to cause OOM leading to a denial of service. The highest threat from this vulnerability is to system availability.   JIRA - HADOOP-17649   ### How was this patch tested?  1. CI/Build.  2. After setting this property ```            fs.s3a.ssl.channel.mode       openssl      ``` Added test results for hadoop-aws integration tests   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-09T02:12:26Z","2022-08-02T12:37:40Z"
"","4568","HDFS-16664. Use correct GenerationStamp when invalidating corrupt block replicas","### Description of PR  Under certain conditions the Namenode can send the incorrect generationStamp to a datanode when invalidating a corrupt block replica.  - the generationStamp sent in the DNA_INVALIDATE is based on the [generationStamp of the block sent in the block report](https://github.com/apache/hadoop/blob/8774f178686487007dcf8c418c989b785a529000/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java#L3733) - the problem is that the datanode with the corrupt block replica (that receives the DNA_INVALIDATE) is not necissarily the same datanode that sent the block report - this can cause the above exception when the corrupt block replica on the datanode receiving the DNA_INVALIDATE & the block replica on the datanode that sent the block report have different generationStamps  Results in the following datanode exception:  ``` 2022-07-16 08:07:52,041 [BP-958471676-X-1657973243350 heartbeating to localhost/127.0.0.1:61365] WARN  datanode.DataNode (BPServiceActor.java:processCommand(887)) - Error processing datanode Command java.io.IOException: Failed to delete 1 (out of 1) replica(s): 0) Failed to delete replica blk_1073741825_1005: GenerationStamp not matched, existing replica is blk_1073741825_1001         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2139)         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2034)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:735)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:883)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:678)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)         at java.lang.Thread.run(Thread.java:750) ```  See JIRA for additional details: https://issues.apache.org/jira/browse/HDFS-16664  ### How was this patch tested?  Validated the fix by leveraging the unit test ""TestDecommission#testDeleteCorruptReplicaForUnderReplicatedBlock""  ## Failed Test - Before this change  ``` > mvn test -Dtest=TestDecommission#testDeleteCorruptReplicaForUnderReplicatedBlock   [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   TestDecommission.testDeleteCorruptReplicaForUnderReplicatedBlock:2035 Node 127.0.0.1:61366 failed to complete decommissioning. numTrackedNodes=1 , numPendingNodes=0 , adminState=Decommission In Progress , nodesWithReplica=[127.0.0.1:61366, 127.0.0.1:61419] ```  ``` > cat target/surefire-reports/org.apache.hadoop.hdfs.TestDecommission-output.txt | grep 'Expected Replicas:\|XXX\|FINALIZED\|Block now\|Failed to delete'   2022-07-16 08:07:45,891 [Listener at localhost/61378] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(1942)) - Block now has 2 corrupt replicas on [127.0.0.1:61370 , 127.0.0.1:61375] and 1 live replica on 127.0.0.1:61366 2022-07-16 08:07:45,913 [Listener at localhost/61378] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(1974)) - Block now has 2 corrupt replicas on [127.0.0.1:61370 , 127.0.0.1:61375] and 1 decommissioning replica on 127.0.0.1:61366 XXX invalidateBlock dn=127.0.0.1:61415 , blk=1073741825_1001 XXX postponeBlock dn=127.0.0.1:61415 , blk=1073741825_1001 XXX invalidateBlock dn=127.0.0.1:61419 , blk=1073741825_1003 XXX addToInvalidates dn=127.0.0.1:61419 , blk=1073741825_1003 XXX addBlocksToBeInvalidated dn=127.0.0.1:61419 , blk=1073741825_1003 XXX rescanPostponedMisreplicatedBlocks blk=1073741825_1005 XXX DNA_INVALIDATE dn=/127.0.0.1:61419 , blk=1073741825_1003 XXX invalidate(on DN) dn=/127.0.0.1:61419 , invalidBlk=blk_1073741825_1003 , blkByIdAndGenStamp = FinalizedReplica, blk_1073741825_1003, FINALIZED 2022-07-16 08:07:49,084 [BP-958471676-X-1657973243350 heartbeating to localhost/127.0.0.1:61365] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(226)) - Scheduling blk_1073741825_1003 replica FinalizedReplica, blk_1073741825_1003, FINALIZED XXX addBlock dn=127.0.0.1:61419 , blk=1073741825_1005   <<<  block report is coming from 127.0.0.1:61419 which has genStamp=1005 XXX invalidateCorruptReplicas dn=127.0.0.1:61415 , reported_blk=1073741825_1005   <<<  corrupt replica is on 127.0.0.1:61415 which is expecting genStamp=1001 XXX addToInvalidates dn=127.0.0.1:61415 , blk=1073741825_1005 2022-07-16 08:07:49,431 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(417)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 1, corrupt replicas: 0, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:61366 127.0.0.1:61419 , Current Datanode: 127.0.0.1:61366, Is current datanode decommissioning: true, Is current datanode entering maintenance: false XXX addBlocksToBeInvalidated dn=127.0.0.1:61415 , blk=1073741825_1005   <<<  Namenode sends wrong genStamp to 127.0.0.1:61415 XXX DNA_INVALIDATE dn=/127.0.0.1:61415 , blk=1073741825_1005 XXX invalidate(on DN) dn=/127.0.0.1:61415 , invalidBlk=blk_1073741825_1005 , blkByIdAndGenStamp = null XXX invalidate(on DN) dn=/127.0.0.1:61415 , invalidBlk=blk_1073741825_1005 , blkById = FinalizedReplica, blk_1073741825_1001, FINALIZED 2022-07-16 08:07:52,041 [BP-958471676-X-1657973243350 heartbeating to localhost/127.0.0.1:61365] WARN  datanode.DataNode (BPServiceActor.java:processCommand(887)) - Error processing datanode Command java.io.IOException: Failed to delete 1 (out of 1) replica(s): 0) Failed to delete replica blk_1073741825_1005: GenerationStamp not matched, existing replica is blk_1073741825_1001         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2139)         at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.invalidate(FsDatasetImpl.java:2034)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:735)         at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:680)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:883)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:678)         at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:849)         at java.lang.Thread.run(Thread.java:750) 2022-07-16 08:07:52,384 [DataXceiver for client  at /127.0.0.1:61434 [Receiving block BP-958471676-X-1657973243350:blk_1073741825_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(939)) - opWriteBlock BP-958471676-X-1657973243350:blk_1073741825_1005 received exception org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-958471676-X-1657973243350:blk_1073741825_1005 already exists in state FINALIZED and thus cannot be created. 2022-07-16 08:07:52,385 [DataXceiver for client  at /127.0.0.1:61434 [Receiving block BP-958471676-X-1657973243350:blk_1073741825_1005]] INFO  datanode.DataNode (DataXceiver.java:run(307)) - 127.0.0.1:61415:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:61434 dst: /127.0.0.1:61415; org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException: Block BP-958471676-X-1657973243350:blk_1073741825_1005 already exists in state FINALIZED and thus cannot be created. 2022-07-16 08:07:54,422 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(417)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 1, corrupt replicas: 0, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:61366 127.0.0.1:61419 , Current Datanode: 127.0.0.1:61366, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ... 2022-07-16 08:08:24,426 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(417)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 1, corrupt replicas: 0, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:61366 127.0.0.1:61419 , Current Datanode: 127.0.0.1:61366, Is current datanode decommissioning: true, Is current datanode entering maintenance: false ```  Note the inline comments above which illustrate the bug   ## Successful Test - After this change  ``` > mvn test -Dtest=TestDecommission#testDeleteCorruptReplicaForUnderReplicatedBlock   [INFO] Results: [INFO]  [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 ```  Logs:  ``` > cat target/surefire-reports/org.apache.hadoop.hdfs.TestDecommission-output.txt | grep 'Expected Replicas:\|XXX\|FINALIZED\|Block now\|Failed to delete'   2022-07-16 07:54:30,648 [Listener at localhost/60376] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(1942)) - Block now has 2 corrupt replicas on [127.0.0.1:60364 , 127.0.0.1:60368] and 1 live replica on 127.0.0.1:60373 2022-07-16 07:54:30,669 [Listener at localhost/60376] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(1974)) - Block now has 2 corrupt replicas on [127.0.0.1:60364 , 127.0.0.1:60368] and 1 decommissioning replica on 127.0.0.1:60373 XXX invalidateBlock dn=127.0.0.1:60423 , blk=1073741825_1001 XXX postponeBlock dn=127.0.0.1:60423 , blk=1073741825_1001 XXX invalidateBlock dn=127.0.0.1:60427 , blk=1073741825_1003 XXX addToInvalidates dn=127.0.0.1:60427 , blk=1073741825_1003 XXX addBlocksToBeInvalidated dn=127.0.0.1:60427 , blk=1073741825_1003 XXX rescanPostponedMisreplicatedBlocks blk=1073741825_1005 XXX DNA_INVALIDATE dn=/127.0.0.1:60427 , blk=1073741825_1003 XXX invalidate(on DN) dn=/127.0.0.1:60427 , invalidBlk=blk_1073741825_1003 , blkByIdAndGenStamp = FinalizedReplica, blk_1073741825_1003, FINALIZED 2022-07-16 07:54:32,831 [BP-1469857843-X-1657972447604 heartbeating to localhost/127.0.0.1:60363] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(226)) - Scheduling blk_1073741825_1003 replica FinalizedReplica, blk_1073741825_1003, FINALIZED 2022-07-16 07:54:33,772 [DatanodeAdminMonitor-0] INFO  BlockStateChange (DatanodeAdminManager.java:logBlockReplicationInfo(417)) - Block: blk_1073741825_1005, Expected Replicas: 2, live replicas: 0, corrupt replicas: 1, decommissioned replicas: 0, decommissioning replicas: 1, maintenance replicas: 0, live entering maintenance replicas: 0, excess replicas: 0, Is Open File: false, Datanodes having this block: 127.0.0.1:60373 127.0.0.1:60423 , Current Datanode: 127.0.0.1:60373, Is current datanode decommissioning: true, Is current datanode entering maintenance: false XXX addBlock dn=127.0.0.1:60427 , blk=1073741825_1005 XXX invalidateCorruptReplicas dn=127.0.0.1:60423 , reported_blk=1073741825_1005 XXX getCorruptReplicaGenerationStamp dn=127.0.0.1:60423 , genStamp=1001 XXX addToInvalidates dn=127.0.0.1:60423 , blk=1073741825_1001 XXX addBlocksToBeInvalidated dn=127.0.0.1:60423 , blk=1073741825_1001 XXX DNA_INVALIDATE dn=/127.0.0.1:60423 , blk=1073741825_1001 XXX invalidate(on DN) dn=/127.0.0.1:60423 , invalidBlk=blk_1073741825_1001 , blkByIdAndGenStamp = FinalizedReplica, blk_1073741825_1001, FINALIZED 2022-07-16 07:54:35,796 [BP-1469857843-X-1657972447604 heartbeating to localhost/127.0.0.1:60363] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:deleteAsync(226)) - Scheduling blk_1073741825_1001 replica FinalizedReplica, blk_1073741825_1001, FINALIZED XXX addBlock dn=127.0.0.1:60423 , blk=1073741825_1005 2022-07-16 07:54:40,768 [Listener at localhost/60430] INFO  hdfs.TestDecommission (TestDecommission.java:testDeleteCorruptReplicaForUnderReplicatedBlock(2050)) - Block now has 2 live replicas on [127.0.0.1:60423 , 127.0.0.1:60427] and 1 decommissioned replica on 127.0.0.1:60373 ```  Using ""getCorruptReplicaGenerationStamp"" allows the Namenode to get the correct generationStamp for the corrupt block replica  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","KevinWikant","2022-07-16T16:50:33Z","2022-08-03T21:24:56Z"
"","4642","HADOOP-18372. ILoadTestS3ABulkDeleteThrottling failing.","### Description of PR  This test was failing because:  - delete page size was not being set in configuration properly. - fs.removeKeys() was not being audited and so giving `unaudited operation executing a request outside an audit span`. It was passing when I disabled auditing.   Have moved up setting config values to `createScaleConfiguration`, added auditing.   ### How was this patch tested?  Since it's only updating the test, ran `ILoadTestS3ABulkDeleteThrottling` on `eu-west-1`, all tests are passing.","closed","","ahmarsuhail","2022-07-27T12:45:34Z","2022-07-27T21:36:16Z"
"","4298","Hadoop-18184. Adds support for unbuffer","### Description of PR  This PR adds support for unbuffer.   Unbuffer is used by certain applications (eg: Impala) when they want to hold onto an input stream but free the resource it's using. This is useful as when it needs to read from the stream again, it doesn't have to open the stream again, and can save on HEAD calls.  For prefetching, unbuffer needs to free up the buffer pool, delete any local files, clear state about blocks in the file etc. Also, when reading after an unbuffer, the input stream should reinitialise all this state. It should also read from the last active position before the read.  ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`  `ITestS3AInputStreamPerformance` is failing, unrelated to this PR. Created issue: https://issues.apache.org/jira/browse/HADOOP-18231  `ITestS3AUnbuffer` fails. instance of assertion & `isObjectStreamOpen()` fails.  Similar to the above issue, there are a few different ways to fix this test. I'm not sure what the best way is. Parameterized tests and different assertions based on if prefetching is enabled/new tests? I've left it failing for now.   All unbuffer contract tests are passing now.   Also tested a few different read sequences, for eg seek should work after an unbuffer:  ``` in.read(buffer, 0, _1MB * 3); in.unbuffer(); in.seek(_10MB); in.read(buffer, 0, _1MB * 3); ```","closed","","ahmarsuhail","2022-05-10T13:18:11Z","2022-06-20T09:51:42Z"
"","4458","HADOOP-18190. Adds iostats for prefetching","### Description of PR  This PR adds in iostats for the prefetching input stream. This PR is dependent on https://github.com/apache/hadoop/pull/4386, which fixes issues after the rebase. Once that gets in merged in, this PR can also update `ITestS3PrefetchingInputStream` with assertions on more stats.  The following stats are added:  **Counters:**  `STREAM_READ_PREFETCH_OPERATIONS`: Total number of prefetch ops requested  **Duration:**  `STREAM_READ_REMOTE_BLOCK_READ`: Time taken to read a full block of data from S3 `STREAM_READ_BLOCK_ACQUIRE_AND_READ`: Time taken to acquire a buffer from the pool and read data into it. This is  not for prefetching ops, but when the first block of data is read (on first read or after a seek) `ACTION_EXECUTOR_ACQUIRED`: Time taken to acquire an executor. Either by the prefetching task or the caching task.   **Gauges:**  `STREAM_READ_BLOCKS_IN_FILE_CACHE`: Blocks currently in file cache `STREAM_READ_ACTIVE_PREFETCH_OPERATIONS`: Current active prefetch operations `STREAM_READ_ACTIVE_MEMORY_IN_USE`: Current active memory in use  There are still some more stats that can be added, especially around cache usage. I'll follow up with another PR for those if we're happy with the approach of this PR.   ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-06-18T13:39:39Z","2022-07-26T11:04:31Z"
"","4109","HADOOP-18028. High performance S3A input stream","### Description of PR  This is the PR of #3736 applied to a dedicated feature branch, with some minor pre-merge tuning with all subsequent changes to be their own PR  * rename test classes, have AbstractHadoopTestBase as the base * package info files for new packages * import ordering * move to intercept() for assertions; ExceptionAsserts is invoking it and can be removed in future.  this adds a dependency on a twitter lib which looks like scala code. that MUST be cut before we can merge.  ### How was this patch tested?  s3 london with `-Dparallel-tests -DtestsThreadCount=8  -Dmarkers=keep -Dscale`  there are some failing integration tests which will need to be fixed before the feature branch is merged to trunk.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-03-25T20:16:43Z","2022-03-28T10:57:40Z"
"","4654","HADOOP-18379 rebase feature/HADOOP-18028-s3a-prefetch to trunk","### Description of PR  This is the feature branch feature/HADOOP-18028-s3a-prefetch rebased on trunk to see how yetus likes it.  If all is good I will force push it to the asf branch, so things are up to date.   rebase merge issues  * Constants, docs * S3AReadOpsContext  some compilation errors to fix  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-07-28T13:28:15Z","2022-08-02T14:39:40Z"
"","4386","HADOOP-18231. Fixes failing tests & drain stream async.","### Description of PR  This is a combined PR for:  #### Add new test for Prefetching:   [JIRA](https://issues.apache.org/jira/browse/HADOOP-18231). Previous PR before rebase was opened [here](https://github.com/apache/hadoop/pull/4305).  `ITestS3AInputStreamPerformance` was failing when prefetching is enabled. This PR disables prefetching when running ITestS3AInputStreamPerformance and adds in a new test class `ITestS3PrefetchingInputStream` with tests specific for prefetching.   #### Drain stream async:   [JIRA](https://issues.apache.org/jira/browse/HADOOP-18230). Previous PR before rebase was opened [here](https://github.com/apache/hadoop/pull/4294).   If we close the prefetching input stream before prefetched blocks have finished reading the S3 input stream, the sdk repeatedly complains ""Not all bytes were read from the S3ObjectInputStream"". This happened on S3AInputStream as well, see [this](https://github.com/aws/aws-sdk-java/issues/1211) issue for more details. Closing the stream before draining it will abort the connection, so to allow for connection reuse we drain it asynchronously when remaining bytes is > async drain threshold.  #### Fix ITestS3AOpenCost:   [JIRA](https://issues.apache.org/jira/browse/HADOOP-18247)  This test was failing after the rebase.   ` testOpenFileShorterLength` fails because inputStreamStats need to be created before opening the stream so that the time to prepare to open the file is included.  `testOpenFileLongerLength` fails because in this [read method](https://github.com/apache/hadoop/blob/feature-HADOOP-18028-s3a-prefetch/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/read/S3Reader.java#L95) when for eg: file length is 8 and length to read is 18, it swallows the EOF exception and returns the num of successful bytes read which will be 8. So readFully doesn't throw an EOF immediately but then tries to read bytes 8-18 again, and then fails.  ### How was this patch tested? Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-05-31T12:43:53Z","2022-07-15T12:51:19Z"
"","3962","HADOOP-14661. Add S3 requester pays bucket support to S3A","### Description of PR  This change introduces support for ""requester pays"" S3 buckets (HADOOP-14661).  There was already some discussion and patches made available (e.g. #250), although the code base has changed a bit since then.  In summary:  - Add some information to the main S3A documentation - Add example of error and how to fix/configure to troubleshooting doc - Add options to configure requester pays in S3A (`fs.s3a.requester-pays.enabled`) when creating S3A filesystem, which uses S3ClientFactory to create a new S3Client also taking this option - Add the required header inside S3ClientFactory if option was configured - Remove unused configuration options for requester pays in RequestFactoryImpl - Add new integration test which talks to publicly available `usgs-landsat` bucket (new version, not old) which is configured to require requester pays (['usgs-landsat' on Registry of Open Data on AWS](https://registry.opendata.aws/usgs-landsat/index.html)). - Add information on how to configure the new tests in testing.md  ### How was this patch tested?  I created an EC2 instance and an S3 Bucket in eu-west-1 and ran the tests against the global S3 endpoint.  ``` mvn -Dparallel-tests -DtestsThreadCount=16 clean verify ```  Note, the request pays specific tests introduced do not use the developer supplied bucket.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dannycjones","2022-02-07T15:08:53Z","2022-03-24T09:01:45Z"
"","4024","HADOOP-18139. Allow configuration of zookeeper server principal.","### Description of PR  This change allows RBF routers to configure the principal the ZK servers run as.  ### How was this patch tested?  Running in an integration cluster.","closed","","omalley","2022-02-24T00:20:59Z","2022-02-24T23:02:43Z"
"","4006","HADOOP-13294. Test hadoop fs shell against s3a","### Description of PR  There's no tests of hadoop -fs commands against S3a. With some tests we can see problems when using these commands with S3a (HADOOP-13294).  From the command list in [FileSystemShell](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html), I added tests for each of them, verify expected output from stdout, stderr and return code. Some tests might be a bit long because I'm trying to group the commands and run them together so it won't take too long.  ### Changes - Adding hadoop -fs commands test for S3a  ### How was this patch tested?  Tested in ` eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 1070, Failures: 0, Errors: 0, Skipped: 185 [INFO] Results: [INFO]  [WARNING] Tests run: 108, Failures: 0, Errors: 0, Skipped: 68 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","monthonk","2022-02-21T10:58:49Z","2022-04-20T23:10:02Z"
"","4114","[HADOOP-18180] remove use of twitter util-core (scala jar)","### Description of PR  the scala 2.11 util-core jar will cause problems for anyone like Spark who use newer scala versions.  ### How was this patch tested?  CI build - local build  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pjfanning","2022-03-28T12:36:39Z","2022-04-04T15:44:02Z"
"","4091","HDFS-16517 Distance metric is wrong for non-DN machines in 2.10.","### Description of PR  The distance metric used for machines in 2.10 that aren't in the NetworkTopology, because they aren't running DataNodes, is wrong. It means that off-rack and on-rack, but off-node, are both given a weight of 2. In normal Hadoop clusters, this isn't a big problem because they don't have clients that are on-rack but without DataNodes. For clusters that are striped (federated HDFS going across racks) or separate compute and storage that share racks are both really bad with this bug.  ### How was this patch tested?  Unit test added.","closed","","omalley","2022-03-22T17:04:27Z","2022-03-23T21:54:30Z"
"","4435","YARN-11178. Avoid CPU busy idling and resource wasting in DelegationTokenRenewerPoolTracker thread","### Description of PR  The DelegationTokenRenewerPoolTracker thread is busy wasting CPU resource in empty poll iterate when there is no delegation token renewer event task in the futures map:  ```java // org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.DelegationTokenRenewerPoolTracker#run @Override public void run() {   // this while true loop is busy when the `futures` is empty   while (true) {     for (Map.Entry> entry : futures         .entrySet()) {       DelegationTokenRenewerEvent evt = entry.getKey();       Future future = entry.getValue();       try {         future.get(tokenRenewerThreadTimeout, TimeUnit.MILLISECONDS);       } catch (TimeoutException e) {          // Cancel thread and retry the same event in case of timeout         if (future != null && !future.isDone() && !future.isCancelled()) {           future.cancel(true);           futures.remove(evt);           if (evt.getAttempt() < tokenRenewerThreadRetryMaxAttempts) {             renewalTimer.schedule(                 getTimerTask((AbstractDelegationTokenRenewerAppEvent) evt),                 tokenRenewerThreadRetryInterval);           } else {             LOG.info(                 ""Exhausted max retry attempts {} in token renewer ""                     + ""thread for {}"",                 tokenRenewerThreadRetryMaxAttempts, evt.getApplicationId());           }         }       } catch (Exception e) {         LOG.info(""Problem in submitting renew tasks in token renewer ""             + ""thread."", e);       }     }   } } ```  A better way to avoid CPU idling is waiting for some time when the `futures` map is empty, and when the renewer task done or cancelled, we should remove the task future in `futures` map to avoid memory leak:  ```java @Override public void run() {   while (true) {     // waiting for some time when futures map is empty     if (futures.isEmpty()) {       synchronized (this) {         try {           // waiting for tokenRenewerThreadTimeout milliseconds           long waitingTimeMs = Math.min(10000, Math.max(500, tokenRenewerThreadTimeout));           LOG.info(""Delegation token renewer pool is empty, waiting for {} ms."", waitingTimeMs);           wait(waitingTimeMs);         } catch (InterruptedException e) {           LOG.warn(""Delegation token renewer pool tracker waiting interrupt occurred."");           Thread.currentThread().interrupt();         }       }       if (futures.isEmpty()) {         continue;       }     }     for (Map.Entry> entry : futures         .entrySet()) {       DelegationTokenRenewerEvent evt = entry.getKey();       Future future = entry.getValue();       try {         future.get(tokenRenewerThreadTimeout, TimeUnit.MILLISECONDS);       } catch (TimeoutException e) {          // Cancel thread and retry the same event in case of timeout         if (future != null && !future.isDone() && !future.isCancelled()) {           future.cancel(true);           futures.remove(evt);           if (evt.getAttempt() < tokenRenewerThreadRetryMaxAttempts) {             renewalTimer.schedule(                 getTimerTask((AbstractDelegationTokenRenewerAppEvent) evt),                 tokenRenewerThreadRetryInterval);           } else {             LOG.info(                 ""Exhausted max retry attempts {} in token renewer ""                     + ""thread for {}"",                 tokenRenewerThreadRetryMaxAttempts, evt.getApplicationId());           }         }       } catch (Exception e) {         LOG.info(""Problem in submitting renew tasks in token renewer ""             + ""thread."", e);       }       // remove done and cancelled task       if (future.isDone() || future.isCancelled()) {         try {           futures.remove(evt);           LOG.info(""Removed done or cancelled renew tasks of {} in token renewer thread."", evt.getApplicationId());         } catch (Exception e) {           LOG.warn(""Problem in removing done or cancelled renew tasks in token renewer thread."", e);         }       }     }   } }  ```  some screenshots and CPU profile as following:  > all screenshots images and CPU profile report HTML files were attached into issue: [YARN-11178](https://issues.apache.org/jira/browse/YARN-11178)  before optimized the **ACTIVE** ResourceManager process will occupy CPU core 100% continuous：  ![YARN-11178.CPU idling busy 100% before optimized](https://issues.apache.org/jira/secure/attachment/13045187/YARN-11178.CPU%20idling%20busy%20100%25%20before%20optimized.png)  after optimized the code, CPU occupation decreased to normal：  ![YARN-11178.CPU normal after optimized](https://issues.apache.org/jira/secure/attachment/13045189/YARN-11178.CPU%20normal%20after%20optimized.png)  for CPU profile, before optimized:  ![YARN-11178.CPU profile for idling busy 100% before optimized](https://issues.apache.org/jira/secure/attachment/13045191/YARN-11178.CPU%20profile%20for%20idling%20busy%20100%25%20before%20optimized.png)  after optimized:  ![YARN-11178.CPU profile for normal after optimized](https://issues.apache.org/jira/secure/attachment/13045192/YARN-11178.CPU%20profile%20for%20normal%20after%20optimized.png)","open","","LennonChin","2022-06-13T04:01:38Z","2022-06-17T05:26:40Z"
"","4164","HADOOP-18198. patch chain for Hadoop 3.3.3","### Description of PR  The chain of patches for 3.3.3 release, in the same order as the 3.2.3 release  ### How was this patch tested?  will see what yetus says first  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-12T14:12:42Z","2022-04-19T10:40:46Z"
"","4520","HADOOP-18180. Replaces ExecutorServicePool with ExecutorService","### Description of PR  The `ExecutorServicePool` class was only submitting tasks to the `ExecutorService`. This PR removes `ExecutorServicePool` and submits directly to the `ExecutorService`.  ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","open","","ahmarsuhail","2022-06-30T11:47:57Z","2022-08-03T12:46:26Z"
"","4555","HADOOP-18336.Tag FSDataInputStream.getWrappedStream() @Public/@Stable","### Description of PR  Tag FSDataInputStream.getWrappedStream() @Public/@Stable  JIRA - HADOOP-18336   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-07-12T11:26:30Z","2022-07-13T11:56:56Z"
"","3951","HADOOP-18101. Bump aliyun-sdk-oss to 3.13.2 and jdom2 to 2.0.6.1","### Description of PR  See discription: [HADOOP-18101](https://issues.apache.org/jira/browse/HADOOP-18101)  ### How was this patch tested?  Confirmed that it has been fixed in v3.13.1 with the following [patch](https://github.com/aliyun/aliyun-oss-java-sdk/pull/381)  Confirmed with maven dependency tree. ``` mvn dependency:tree | grep jdom [INFO] |  +- org.jdom:jdom2:jar:2.0.6.1:provided [INFO] |  +- org.jdom:jdom2:jar:2.0.6.1:compile [INFO] |     +- org.jdom:jdom2:jar:2.0.6.1:compile [INFO] |     +- org.jdom:jdom2:jar:2.0.6.1:compile ```","closed","","aswinshakil","2022-02-01T03:32:30Z","2022-02-07T16:47:01Z"
"","3959","YARN-11072. Cannot display hadoop-st.png when using reverse proxy and applying APPLICATION_WEB_PROXY_BASE","### Description of PR  Scenario:  When I want to use reverse proxy and apply APPLICATION_WEB_PROXY_BASE to change the base path, it can not find the hadoop-st.png.  (file: hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/view/HeaderBlock.java)  Issues: 1. Cannot display the image when you apply APPLICATION_WEB_PROXY_BASE=base_path/. The image path should be /base_path/static/hadoop-st.png. 2. Should not use a fixed path.  [JIRA YARN-11072](https://issues.apache.org/jira/browse/YARN-11072)  ### How was this patch tested?  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","bensonlin321","2022-02-04T22:15:39Z","2022-06-30T04:28:35Z"
"","4051","HDFS-16496. Snapshot diff on snapshotable directory fails with not snapshottable error","### Description of PR  Running a snapshot diff against some snapshotable folders gives an error:  ``` org.apache.hadoop.hdfs.protocol.SnapshotException: Directory is neither snapshottable nor under a snap root! 	at org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.checkAndGetSnapshottableAncestorDir(SnapshotManager.java:395) 	at org.apache.hadoop.hdfs.server.namenode.snapshot.SnapshotManager.diff(SnapshotManager.java:744) 	at org.apache.hadoop.hdfs.server.namenode.FSDirSnapshotOp.getSnapshotDiffReportListing(FSDirSnapshotOp.java:200) 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSnapshotDiffReportListing(FSNamesystem.java:6983) 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getSnapshotDiffReportListing(NameNodeRpcServer.java:1977) 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getSnapshotDiffReportListing(ClientNamenodeProtocolServerSideTranslatorPB.java:1387) 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:533) 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070) 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:989) 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:917) 	at java.security.AccessController.doPrivileged(Native Method) 	at javax.security.auth.Subject.doAs(Subject.java:422) 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1898) 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2894) ``` This is caused by [HDFS-15483](https://issues.apache.org/jira/browse/HDFS-15483) (in order snapshot delete), and the issue is in the following method in SnapshotManager: ```   public INodeDirectory getSnapshottableAncestorDir(final INodesInPath iip)       throws IOException {     final String path = iip.getPath();     final INode inode = iip.getLastINode();     final INodeDirectory dir;     if (inode instanceof INodeDirectory) { // THIS SHOULD BE TRUE - change to inode.isDirectory()       dir = INodeDirectory.valueOf(inode, path);     } else {       dir = INodeDirectory.valueOf(iip.getINode(-2), iip.getParentPath());     }     if (dir.isSnapshottable()) {       return dir;     }     for (INodeDirectory snapRoot : this.snapshottables.values()) {       if (dir.isAncestorDirectory(snapRoot)) {         return snapRoot;       }     }     return null;   } ``` After adding some debug, I found the directory which is the snapshot root is not an instance of INodeDirectory, but instead is an ""INodeReference$DstReference"". I think the directory becomes an instance of this class, if the directory is renamed and one of its children has been moved out of another snapshot.  The fix is simple - just check `inode.isDirectory()` instead.  ### How was this patch tested?  Manually against an affected image with the problem.  ### For code changes:","closed","","sodonnel","2022-03-06T22:03:28Z","2022-03-08T11:07:12Z"
"","4681","HDFS-16687. RouterFsckServlet replicates code from DfsServlet base class","### Description of PR  RouterFsckServlet replicates the method ""getUGI(HttpServletRequest request, Configuration conf)"" from DfsServlet instead of just extending DfsServlet.  This change is just a refactor, which replaces the copy with the original base class.  ### How was this patch tested?  The fsck endpoint was called using curl, verifying that the Kerberos checks performed as expected.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","snmvaughan","2022-08-03T01:57:05Z","2022-08-03T16:14:11Z"
"","4169","HADOOP-18201. Remove endpoint config overrides for ITestS3ARequesterPays","### Description of PR  Requester pays was added in #3962. The new tests remove overrides for requester pays enablement but do not account for developers changing the S3 endpoint.  To address this, we remove the override for endpoint.  Addresses HADOOP-18201.  ### How was this patch tested?  Patch will be tested against `eu-west-1` - specifically against its regional endpoint.","closed","","dannycjones","2022-04-13T14:41:28Z","2022-04-14T16:13:51Z"
"","4486","YARN-10320.Replace FSDataInputStream#read with readFully in Log Aggregation","### Description of PR  Replace FSDataInputStream#read with readFully in Log Aggregation  * JIRA : YARN-10320  ### How was this patch tested? Current Units will suffice, no new tests are required  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-22T02:15:57Z","2022-06-23T16:28:33Z"
"","4487","YARN-9874.Remove unnecessary LevelDb write call in LeveldbConfigurationStore#confirmMutation","### Description of PR  Remove unnecessary LevelDb write call in LeveldbConfigurationStore#confirmMutation  JIRA: YARN-9874  ### How was this patch tested? Current tests will suffice   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-22T02:49:56Z","2022-06-23T16:29:27Z"
"","4417","YARN-11173. remove redeclaration of os-maven-plugin.version from yarn-csi","### Description of PR  Remove an out of data maven extension which is not needed -the parent pom does this already.   ### How was this patch tested?  typed ""mvn install""  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-08T15:47:44Z","2022-06-09T12:12:34Z"
"","4016","HDFS-16478 Add thread id to command processor name","### Description of PR  Provides additional clarity in logs when debugging MiniDFSCluster with multiple in-process DataNodes.  ### How was this patch tested?  Visual inspection of log output.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","open","","madrob","2022-02-22T19:17:05Z","2022-03-01T06:48:27Z"
"","4166","HDFS-14478: Add libhdfs APIs for openFile","### Description of PR  pr #955 rebased to trunk  ### How was this patch tested?  there's a new test  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-12T17:16:11Z","2022-04-13T14:58:12Z"
"","4189","Releases/hadoop 18202 docker git branch 3.3.3","### Description of PR  pr #4188 against the branch-3.3.3 release branch  ### How was this patch tested?   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-18T15:36:15Z","2022-04-18T15:38:30Z"
"","4693","HDFS-4043. Namenode Kerberos Login does not use proper hostname for host qualified hdfs principal name.","### Description of PR  Perform a DNS reverse name lookup when getCanonicalHost() returns the IP address as a string.  The standard InetAddress getCanonicalHostName() has the following issues: 1. It reports any cached value if exists 2. It returns the IP address if it is unable to resolve a canonical name 3. It caches any returned values, which would include the IP address as a string  ### How was this patch tested?  An HA configuration was deployed to Kubernetes, running Java 11.  All services startup as expected.  Performing a rolling restart of the JournalNodes caused failures when the NameNode communicated with the newly started JournalNode, reporting an exception that included a principal with the IP address instead of the canonical host name.  Performing the same activity after applying the patch succeeds as the DNS reverse lookup is performed to acquire the correct calculated principal.    ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","snmvaughan","2022-08-03T20:49:59Z","2022-08-03T21:06:37Z"
"","4677","MAPREDUCE-7401. Optimize liststatus for better performance by using recursive listing","### Description of PR  Optimize liststatus for better performance by using recursive listing.  JIRA - MAPREDUCE-7401  ### How was this patch tested?  Unit tests   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-08-02T21:26:43Z","2022-08-03T21:02:24Z"
"","4499","HADOOP-18320. Fixes typos in Delegation Tokens documentation.","### Description of PR  Noticed some typos while reading through the documentation, this PR fixes those.  ### How was this patch tested?  Not required","open","","ahmarsuhail","2022-06-24T16:30:32Z","2022-06-30T14:49:26Z"
"","4536","HADOOP-18327.Fix eval expression in hadoop-functions.sh","### Description of PR  Need to fix the eval expression.  1. Prefix exec by eval in Hadoop bin scripts Prior to this change, if HADOOP_OPTS contains any arguments that include a space, the command is not parsed correctly. For example, if HADOOP_OPTS=""... -XX:OnOutOfMemoryError=\""kill -9 %p\"" ..."", the bin/hadoop script will fail with the error ""Unrecognized option: -9"". No amount of clever escaping of the quotes or spaces in the ""kill -9 %p"" command will fix this. The only alternative appears to be to use 'eval'. Switching to use 'eval' instead of 'exec' also works, but it results in an intermediate bash process being left alive throughout the entire lifetime of the Java proces being started. Using 'exec' prefixed by 'eval' as has been done in this commit gets the best of both worlds, in that options with spaces are parsed correctly, and you don't end up with an intermediate bash process as the parent of the Java process.  2. We can replace single quote with escape-char and a double quote.  JIRA - HADOOP-18327  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-07T21:07:36Z","2022-08-01T08:31:33Z"
"","3864","HADOOP-18068. upgrade AWS SDK to 1.12.132","### Description of PR  move to latest AWS SDK  ### How was this patch tested?  itests without s3guard ```  -Dparallel-tests -DtestsThreadCount=6  -Dmarkers=keep -Dscale ```  full manual qualification as covered in testing doc  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-01-05T19:16:45Z","2022-01-18T10:31:28Z"
"","4598","HDFS-16672. Fix lease interval comparison in BlockReportLeaseManager","### Description of PR  monotonicNowMs is generated by System.nanoTime(), direct comparison is not recommended.  org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager#pruneIfExpired ```java if (monotonicNowMs < node.leaseTimeMs + leaseExpiryMs) {   return false; }  ```  ### How was this patch tested? exist UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cxzl25","2022-07-20T13:50:37Z","2022-08-01T06:37:12Z"
"","4054","HDFS-16495: RBF should prepend the client ip rather than append it.","### Description of PR  Makes RBF prepends the client ip & port to the caller context and removes previous values. This avoids a couple problems: * User can't fake their network address to the NN. * It is less likely to have false positives (accidental conflicts), since the critical information that is under system control comes first.  ### How was this patch tested?  The relevant unit tests were fixed and run.","closed","","omalley","2022-03-08T22:04:24Z","2022-03-14T17:23:02Z"
"","4507","MAPREDUCE-7201.Make Job History File Permissions configurable","### Description of PR  Make Job History File Permissions configurable.  JIRA: MAPREDUCE-7201  Current unit tests will suffice.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-27T22:14:54Z","2022-07-12T11:17:52Z"
"","4634","HADOOP-18368. Fixes ITestCustomSigner for access point names with '-'","### Description of PR  Jira: https://issues.apache.org/jira/browse/HADOOP-18368  ITestCustomSigner was failing for access point names that had '-', eg: test-custom-signer. This PR updates the logic so access point name is now extracted using substring. Eg, if `bucketName = test-custom-signer-12345678` where `12345678` is the aws account id, we can get access point name by `bucketName.substring(0, bucketName.length() - (accountId.length() + 1))`, with the `+ 1`  being for removing  the`-` before the account id.   ### How was this patch tested?  Since it's only updating the `ITestCustomSigner`, I tested by running`ITestCustomSigner` with:  - Access point name test-custom-signer - Access point name testcustomsigner - Access points disabled","closed","","ahmarsuhail","2022-07-26T13:48:05Z","2022-08-01T20:44:59Z"
"","3905","HADOOP-16155. S3AInputStream read(bytes[]) to not retry on read failure","### Description of PR  Jira: https://issues.apache.org/jira/browse/HADOOP-16155  When an error happens on read, do not reopen the stream.  For the unit tests I've changed the order of super.read() and trigger failure. This is because previously the failure would happen after the read actually executed and so the buffer would not be empty and we couldn't assert on the buffer being empty in case of failures.  I've also added a new variable triggerGetObjectFailure. This will not trigger a get object failure in case of testInputStreamReadFullyRetryForException. This is because since we are removing retries from read(bytes[]), readFully() will call read(bytes[]) after each failure which will then call lazySeek() every time. The get object failure ends up being thrown in lazySeek() which does not retry and so readFully does not complete.  ### How was this patch tested?  Tested in eu-west-1 by running  ``` mvn -Dparallel-tests -DtestsThreadCount=16 clean verify ```","open","","ahmarsuhail","2022-01-19T13:36:40Z","2022-02-23T15:06:59Z"
"","3927","HADOOP-15245. S3AInputStream.skip() to use lazy seek","### Description of PR  Jira: https://issues.apache.org/jira/browse/HADOOP-15245  Implements an optimised skip() that calls lazySeek which will decided when to do a default skip vs do a new get. Also adds in instrumentation to count number of times skip is called.  ### How was this patch tested?  Tested in eu-west-1 by running ``` mvn -Dparallel-tests -DtestsThreadCount=16 clean verify ```","open","","ahmarsuhail","2022-01-25T15:18:03Z","2022-03-24T16:10:41Z"
"","3765","HADOOP-18035. Ignore unit test failures to run all the unit tests from root","### Description of PR  JIRA: HADOOP-18035  Backported `testFailureIgnore` related changes from HADOOP-16596.  ### How was this patch tested?  Not tested.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - n/a Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - n/a If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - n/a If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","aajisaka","2021-12-08T11:19:15Z","2021-12-09T16:27:31Z"
"","4542","HADOOP-13144. Enhancing IPC client throughput via multiple connections per user","### Description of PR  JIRA ID: [HADOOP-13144](https://issues.apache.org/jira/browse/HADOOP-13144)  The generic IPC client (org.apache.hadoop.ipc.Client) utilizes a single connection thread for each ConnectionId. The ConnectionId is unique to the connection's remote address, ticket and protocol. Each ConnectionId is 1:1 mapped to a connection thread by the client via a map cache.  The result is to serialize all IPC read/write activity through a single thread for a each user/ticket + address. If a single user makes repeated calls (1k-100k/sec) to the same destination, the IPC client becomes a bottleneck.","closed","","ZanderXu","2022-07-09T12:44:17Z","2022-07-15T21:18:46Z"
"","4208","HADOOP-16768. SnappyCompressor test cases wrongly assume that the compressed data is always smaller than the input data.","### Description of PR  It is a clean cherry pick from commit 328eae9a146b2dd9857a17a0db6fcddb1de23a0d from trunk.  However, we manually added dependency for assertj. assertj dependency was added as part of a not small patch in HADOOP-16287 and HDFS-12665, and subsequently modified again in HADOOP-16253. Too much overhead in bringing in these PRs and thus we decided to add the dependency manually ourselves.    ### How was this patch tested?  mvn test -Dtest=hadoop.io.compress.TestCompressorDecompressor mvn test -Dtest=hadoop.io.compress.snappy.TestSnappyCompressorDecompressor","closed","","xinglin","2022-04-21T00:32:25Z","2022-04-21T14:28:49Z"
"","4142","HDFS-16530. setReplication debug log creates a new string even if debug is disabled","### Description of PR  In FSDirAttrOp, [HDFS-14521](https://issues.apache.org/jira/browse/HDFS-14521) made a good change to move a noisy logger to debug:  ```       if (oldBR > targetReplication) {         FSDirectory.LOG.debug(""Decreasing replication from {} to {} for {}"",                              oldBR, targetReplication, iip.getPath());       } else if (oldBR < targetReplication) {         FSDirectory.LOG.debug(""Increasing replication from {} to {} for {}"",                              oldBR, targetReplication, iip.getPath());       } else {         FSDirectory.LOG.debug(""Replication remains unchanged at {} for {}"",                              oldBR, iip.getPath());       }     } ``` However the `iip.getPath()` method must be evaluated to pass the resulting string into the LOG.debug method, even if debug is not enabled:  This code may form a new string where it does not need to: ```   public String getPath() {     if (pathname == null) {       pathname = DFSUtil.byteArray2PathString(path);     }     return pathname;   } ``` We should wrap the entire logging block in `if LOG.debugEnabled()` to avoid any overhead when the logger is not enabled.  ### How was this patch tested?  No new tests - simple change.","closed","","sodonnel","2022-04-05T21:20:25Z","2022-04-06T07:49:52Z"
"","4384","HDFS-16610. Make fsck read timeout configurable","### Description of PR  In a cluster with a lot of small files, we encountered a case where fsck was very slow. I believe it is due to contention with many other threads reading / writing data on the cluster.  Sometimes fsck does not report any progress for more than 60 seconds and the client times out. Currently the connect and read timeout are hardcoded to 60 seconds. This change is to make them configurable.  ### How was this patch tested?  Tested manually by inserting a sleep into the fsck logic in the NN. I then adjusted the read timeout to validate I got a timeout or not depending on the timeout setting.","closed","","sodonnel","2022-05-31T12:01:31Z","2022-06-01T19:36:02Z"
"","4561","HDFS-16660. Improve Code With Lambda in IPCLoggerChannel class","### Description of PR  Improve Code With Lambda in IPCLoggerChannel class, make it more readable.","closed","","ZanderXu","2022-07-13T15:10:55Z","2022-07-28T01:53:06Z"
"","4596","HDFS-16670. Improve Code With Lambda in EditLogTailer class","### Description of PR  Improve Code With Lambda in EditLogTailer class","closed","","ZanderXu","2022-07-20T12:03:17Z","2022-08-01T23:36:34Z"
"","4565","HDFS-16661. Improve Code With Lambda in AsyncLoggerSet class","### Description of PR  Improve Code With Lambda in AsyncLoggerSet class","open","","ZanderXu","2022-07-14T15:25:16Z","2022-07-28T00:47:24Z"
"","4294","HADOOP-18221. Drains stream async before closing","### Description of PR  If close the prefetching input stream before prefetched blocks have finished reading the S3 input stream, the sdk repeatedly complains ""Not all bytes were read from the S3ObjectInputStream"". This happened on `S3AInputStream` as well, see [this](https://github.com/aws/aws-sdk-java/issues/1211) issue for more details.   Closing the stream before draining it will abort the connection, so to allow for connection reuse we drain it asynchronously.   ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-05-09T13:13:18Z","2022-06-09T16:21:07Z"
"","3739","HADOOP-18029: Update CompressionCodecFactory to handle uppercase file extensions","### Description of PR  I've updated the CompressionCodecFactory to be able to handle filenames with capitalized compression extensions. Two of the three existing internal maps used to store codecs have existing lowercase casts, but it is absent from the call inside getCodec() used for comparing path names. I updated the corresponding unit test in TestCodecFactory, to include intended use cases, and confirmed the test passes with the change.   ### Minor Fixes  - Updated the unit test error message in the case of a null from an NPE to a rich error message. - Fixed all checkstyle warnings inside the two changed files.  ### How was this patch tested?  I confirmed the updated TestCodecFactory:testFinding() test passes, and reran the test suite of hadoop-common.  ### For code changes:  - [Y] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [N/A] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [N/A] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [N/A] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","Desmeister","2021-12-01T00:25:25Z","2022-02-17T18:51:23Z"
"","4148","HDFS-16531. Avoid setReplication writing an edit record if old replication equals the new value","### Description of PR  I recently came across a NN log where about 800k setRep calls were made, setting the replication from 3 to 3 - ie leaving it unchanged. Obviously the application should be fixed, but we could have an optimisation for this.  When the replication is unchanged in a case like this, we log an edit record, an audit log, and perform some quota checks etc. I believe we should still log an audit in these sort of cases, but we can skip all the checks and avoid writing an edit.  ### How was this patch tested?  Added a new test and validated the code paths were correct around writing and syncing edits or not with some log messages I then removed.","closed","","sodonnel","2022-04-07T13:34:48Z","2022-04-17T12:06:58Z"
"","4527","HADOOP-18324. Interrupting RPC Client calls can lead to thread exhaustion.","### Description of PR  I modified the RPC Connection class to use a single thread that does the writes to the socket instead of a thread factory. (It already has a different thread that does the reads.) Java’s concurrency library has a SynchronousQueue that will simplify hand offs from the calling thread to the rpc sending thread.  As a result, we’ll end up with: * Exactly 1 sending thread per an RPC connection. * If the calling thread is interrupted before the socket write, it will be skipped instead of sending it anyways. * If the calling thread is interrupted during the socket write, the write will finish. * RPC requests will be written to the socket in the order received.  ### How was this patch tested?  Added a unit test.","open","","omalley","2022-07-01T23:37:14Z","2022-08-01T08:48:21Z"
"","4354","YARN-6539. Create SecureLogin inside Router.","### Description of PR  https://issues.apache.org/jira/browse/YARN-6539  This PR is contributed by Xie YiFan,  I transform from patch to PR, and fix some ut then add yarn-default.xml and license.   It is precondition so that we could use yarn federation in security mode.  ### How was this patch tested?  uni-test and test in our cluster manually.  ### For code changes:  support security for router","open","","zhengchenyu","2022-05-25T06:09:14Z","2022-07-01T05:51:27Z"
"","4450","YARN-11183. Federation: Remove outdated ApplicationHomeSubCluster in …","### Description of PR  https://issues.apache.org/jira/browse/YARN-11183  Nowadays, ApplicationHomeSubCluster in federation state store can't be removed automatically.  The key is when remove ApplicationHomeSubCluster. What's the end of ApplicationHomeSubCluster's lifecycle?  We know when called getApplicationReport, we must know ApplicationHomeSubCluster. It may be the last possible to get this application's home subcluster. So I think we should remove ApplicationHomeSubCluster when the application is removed from resourcemanager's memory.  ### How was this patch tested?  unit test and test on our cluster.  ### For code changes:  Here I add AsyncDispatcher for handle federation state store. Two key point:  * When application is removed from RM, will delete ApplicationHomeSubCluster from federation state store.  * When ResourceManager start, will check all application whether is removed from resourcemanager memory.","open","","zhengchenyu","2022-06-17T09:20:51Z","2022-06-27T12:33:55Z"
"","4308","YARN-11148. In federation and security mode, nm recover may fail.","### Description of PR  https://issues.apache.org/jira/browse/YARN-11148  ### How was this patch tested?  manual test in our cluster.  ### For code changes:  support security","open","","zhengchenyu","2022-05-13T09:20:53Z","2022-07-25T13:35:37Z"
"","4278","YARN-11132. RM failover may fail when Dispatcher stuck.","### Description of PR  https://issues.apache.org/jira/browse/YARN-11132  ### How was this patch tested?  unit test  ### For code changes:  - Add thread to detect dead lock.","open","","zhengchenyu","2022-05-07T06:27:59Z","2022-05-07T11:11:01Z"
"","4363","HADOOP-18263. Prometheus metrics tag changed unexpectedly.","### Description of PR  https://issues.apache.org/jira/browse/HADOOP-18263  ### How was this patch tested?  unitest and test in our cluster manully  ### For code changes:  After apply this PR, we can filter some metric which will transform to tag. For example, totalsynctimes.","open","","zhengchenyu","2022-05-27T13:42:03Z","2022-06-06T07:54:32Z"
"","4669","HADOOP-18339. fix storage class option doesn't work with heap and byte buffer","### Description of PR  HADOOP-18339. S3A storage class option only picked up when buffering writes to disk.  The problem is that there are two `createPutObjectRequest` classes, one for source file and another one for input stream. Previously, only the first one is picking up storage class option because I thought the second one is used only by creating directory marker which should not have any storage class.  This is fixed it by making both of `createPutObjectRequest` classes pick up the storage class option and updating `newDirectoryMarkerRequest` to create PUT request on its own then add parameterized test to verify it.  ### How was this patch tested?  Tested with a bucket in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 1149, Failures: 0, Errors: 0, Skipped: 146  [INFO] Results: [INFO]  [WARNING] Tests run: 124, Failures: 0, Errors: 0, Skipped: 10 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","monthonk","2022-08-01T15:30:58Z","2022-08-01T17:19:33Z"
"","4212","HADOOP-18175. fix test failures with prefetching s3a input stream","### Description of PR  HADOOP-18175 Fix many test failures on prefetching branch.  **ITestS3AContractUnbuffer** -> changed getPos() on a closed file to return 0  **ITestS3AUnbuffer** -> skipping unsupported input stream for now, HADOOP-18184 should address this test later  **ITestS3ARequesterPays** -> wired up `streamStatistics.streamOpened()` and changed assertion based on stream type  **ITestS3AFileContextStatistics** -> moved seek past EOF validation to read(), which is what S3AInputStream does   ### How was this patch tested?  Tested in `eu-west-1` with both `fs.s3a.prefetch.enabled=true` and `fs.s3a.prefetch.enabled=false`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","monthonk","2022-04-21T10:48:41Z","2022-05-04T18:47:50Z"
"","3877","HADOOP-12020. Add configurable storage class option to s3a files","### Description of PR  HADOOP-12020 Amazon S3 uses, by default, the NORMAL_STORAGE class for s3 objects. This offers 99.99999999% reliability.  For many applications, however, the 99.99% reliability offered by the REDUCED_REDUNDANCY storage class is amply sufficient, and comes with a significant cost saving.  This changes  - Add new configuration `fs.s3a.storage.class` for S3A - Add integration tests to verify objects can be created and copied with specified storage class - It also enable applications to write in other storage classes, including archive, which current behavior produces errors on read - Update documentation about the new config  ### How was this patch tested?  Run integration tests in `eu-west-1` passed with `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","monthonk","2022-01-11T18:12:21Z","2022-06-08T18:05:17Z"
"","3908","HADOOP-17386. Change default fs.s3a.buffer.dir to be under Yarn container path on yarn applications","### Description of PR  fs.s3a.buffer.dir defaults to hadoop.tmp.dir which is /tmp or similar. A lot of systems don't clean up /tmp until reboot -and if they stay up for a long time then they accrue files written through s3a staging committer from spark containers which fail.  Fix: use ${env.LOCAL_DIRS:-${hadoop.tmp.dir}}/s3a as the option so that if env.LOCAL_DIRS is set is used over hadoop.tmp.dir. YARN-deployed apps will use that for the buffer dir. When the app container is destroyed, so is the directory.  ### How was this patch tested?  Injected LOCAL_DIRS env and verified that it was picked up by S3A. Also when it is not set, verified that hadoop.tmp.dir would be used as a fallback.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","monthonk","2022-01-20T10:18:55Z","2022-02-23T19:22:19Z"
"","4257","MAPREDUCE-7376. AggregateWordCount fetches wrong results.","### Description of PR  Fixes AggregateWordCount  ### How was this patch tested? ``` hadoop-3.4.0-SNAPSHOT % bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0-SNAPSHOT.jar  aggregatewordcount /testData /testOut 1 textinputformat  hadoop-3.4.0-SNAPSHOT % bin/hdfs dfs -cat /testOut/part-r-00000                                                                                                    Bye	1 Goodbye	1 Hadoop	2 Hello	2 World	2 ``` ``/testData`` had two files: `wc01.txt:` Hello World Bye World  `wc02.txt:` Hello Hadoop Goodbye Hadoop  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2022-05-02T19:03:21Z","2022-05-09T17:26:14Z"
"","3746","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Fixes a bug in Hadoop HDFS where if more than ""dfs.namenode.decommission.max.concurrent.tracked.nodes"" datanodes are lost while in state decommissioning, then all forward progress towards decommissioning any datanodes (including healthy datanodes) is blocked  JIRA: https://issues.apache.org/jira/browse/HDFS-16303  ### Additional Details  To solve this HDFS bug, there are 2 different proposals: - the [original proposal in this PR](https://github.com/apache/hadoop/pull/3675) which continues to track a dead DECOMMISSION_INPROGRESS node in the DatanodeAdminManager until there are no LowRedundancy blocks on HDFS - this new PR which removes a dead DECOMMISSION_INPROGRESS node from the DatanodeAdminManager immediately leaving the node in DECOMMISSION_INPROGRESS regardless of whether or not there are any LowRedundancy blocks on HDFS  These 2 different implementations will largely behave the same from a user perspective. There is however 1 key difference: - in the original PR when there a no LowRedundancy blocks a dead DECOMMISSION_INPROGRESS node will be transitioned to DECOMMISSIONED. - in the new PR a dead DECOMMISSION_INPROGRESS node will remain in DECOMMISSION_INPROGRESS indefinitely regardless of if there are no LowRedundancy blocks. That is, unless the node either comes alive again or the user removes it from the exclude file.  ### How was this patch tested?  3 new unit tests added to both ""TestDecommission"" & ""TestDecommissionWithBackoffMonitor"": - testDeadNodesRemainDecommissionInProgress - testRevivedDeadNodeIsDecommissioned - testDeadNodeWithDecommissionInProgressRemovedFromExcludeFile  ### For code changes:  - [yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [no] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2021-12-02T21:40:33Z","2021-12-23T13:58:33Z"
"","4521","HADOOP-18321.Fix when to read an additional record from a BZip2 text file split","### Description of PR  Fix when to read an additional record from a BZip2 text file split  JIRA - HADOOP-18321  ### How was this patch tested?  Added Units   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-30T16:18:53Z","2022-08-03T21:42:23Z"
"","4694","HADOOP-18390. Fix out of sync imports for HADOOP-18321","### Description of PR  Fix out of sync imports for HADOOP-18321  JIRA - HADOOP-18390  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-08-03T21:41:05Z","2022-08-03T21:41:05Z"
"","4044","HADOOP-18150. Fix ITestAuditManagerDisabled test in S3A.","### Description of PR  Fix ITestAuditManagerDisabled test which verifies that auditing is disabled by default. In [HADOOP-18091](https://issues.apache.org/jira/browse/HADOOP-18091) we made it enabled by default.  ### How was this patch tested? Region: ap-south-1. Command: `mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale`  ``` [INFO] Running org.apache.hadoop.fs.s3a.audit.ITestAuditManagerDisabled [INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.275 s - in org.apache.hadoop.fs.s3a.audit.ITestAuditManagerDisabled ```  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mehakmeet","2022-03-03T09:27:26Z","2022-05-04T12:18:31Z"
"","4676","HADOOP-18181. move prefetch common classes to hadoop common","### Description of PR  extra changes on top of #4673   * moves the common classes to the impl.prefetch package in hadoop common * production code review; remove this. refs out of all but ctor * test code, directly move to intercept  not done: javadocing comments, reduce logging  ### How was this patch tested?  in progress against s3 london  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [.] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-08-02T18:37:10Z","2022-08-03T16:20:10Z"
"","4205","HADOOP-18177. Document prefetching architecture.","### Description of PR  Documents usage and architecture of the prefetching input stream.","closed","","ahmarsuhail","2022-04-20T11:10:01Z","2022-04-26T18:10:45Z"
"","4469","HADOOP-18254. Disable prefetching by default.","### Description of PR  Disables prefetching by default and updates documentation.   ### How was this patch tested?  Not required","closed","","ahmarsuhail","2022-06-20T09:48:26Z","2022-07-15T12:54:35Z"
"","4270","YARN-11127. Potential deadlock in AsyncDispatcher caused by RMNodeImp…","### Description of PR  detail message see:    https://issues.apache.org/jira/browse/YARN-11127   ### How was this patch tested?  uni-test in PR  ### For code changes:  * For thread ""RM Event dispatcher"", we have lock writelock twice. We could update aggregateLogReport async, then only lock writelock one time. * For thread ""IPC Server handler 264 on default port 8032"", I think getResourceUsageReport could use readLock.","open","","zhengchenyu","2022-05-06T11:50:57Z","2022-07-25T10:12:52Z"
"","4626","HDFS-16676. DatanodeAdminManager$Monitor reports a node as invalid continuously","### Description of PR  DatanodeAdminManager$Monitor reports a node as invalid continuously  JIRA - HDFS-16676   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","ashutoshcipher","2022-07-25T12:01:27Z","2022-07-27T07:18:01Z"
"","4128","HADOOP-18169. getDelegationTokens in ViewFs should also fetch the token from fallback FS","### Description of PR  cherry-pick of 15a5ea2c955a7d1b89aea0cb127727a57db76c76 from trunk to branch-2.10 and created TestViewFsLinkFallback.java file with one test case included.  All other test cases in TestViewFsLinkFallback.java from trunk are removed, as the implementation of InternalDirOfViewFs (createInternal function) is out of date and these test cases won't pass. Leave the fix and the inclusion of these other unit tests as a future pull request.  ### How was this patch tested?  mvn test -Dtest=""TestViewFsLinkFallback""","closed","","xinglin","2022-03-31T23:55:34Z","2022-04-16T01:00:35Z"
"","3698","HADOOP-18014. CallerContext should not include some characters.","### Description of PR  CallerContext should not include control characters. This PR is a different approach from #3683.  ### How was this patch tested?  I created a unit test and confirmed that it succeeded.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","tasanuma","2021-11-22T04:39:54Z","2021-11-25T05:05:43Z"
"","4444","HADOOP-18159. Bumping cos_api-bundle - fix `public-suffix-list.txt`","### Description of PR  Bumping cos_api-bundle  to `5.6.69`  ### How was this patch tested?  In my internal environment. The `public-suffix-list.txt` resource brought on the classpath by this jar is now consistent with other versions (avoiding cert issues in some cases).  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","andreAmorimF","2022-06-15T14:19:40Z","2022-06-15T19:03:26Z"
"","4485","HADOOP-18288. Total requests and total requests per sec served by RPC servers","### Description of PR  branch-3.3 backport PR of #4431","closed","","virajjasani","2022-06-21T23:46:15Z","2022-06-23T09:30:25Z"
"","4399","HADOOP-18088. Replace log4j 1.x with reload4j. (#4052)","### Description of PR  Branch 3.3 move to reload4j cherrypicked to trunk.  Even though we plan to move to log4j in trunk, applying the reload4j patch  * ensures that the branch is safe for us local developers * reduces difference between the branches * allows us to consider having an enforcement rule to keep log4j out of our    dependencies  ### How was this patch tested?  letting yetus do that  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-06-03T17:24:10Z","2022-06-03T17:34:35Z"
"","4500","YARN-11197.Backport YARN-9608 - DecommissioningNodesWatcher should get lists of running applications on node from RMNode.","### Description of PR  Backport YARN-9608 - DecommissioningNodesWatcher should get lists of running applications on node from RMNode.\  JIRA: YARN-11197  ### How was this patch tested?  Added unit test  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ashutoshcipher","2022-06-24T19:46:46Z","2022-06-28T04:03:55Z"
"","3921","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Backport of this [trunk commit](https://github.com/apache/hadoop/commit/d20b598f97e76c67d6103a950ea9e89644be2c41) to Hadoop 3.x  ### How was this patch tested?  All ""TestDecommission"", ""TestDecommissionWithBackoffMonitor"", & ""DatanodeAdminManager"" tests pass when run locally  Same manual testing from https://github.com/apache/hadoop/pull/3675 applies.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-01-24T19:32:04Z","2022-01-31T04:18:53Z"
"","3920","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Backport of this [trunk commit](https://github.com/apache/hadoop/commit/d20b598f97e76c67d6103a950ea9e89644be2c41) to Hadoop 2.x  ### How was this patch tested?  All ""TestDecommission"" & ""DatanodeAdminManager"" tests pass when run locally  Same manual testing from https://github.com/apache/hadoop/pull/3675 applies.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-01-24T17:47:17Z","2022-01-31T07:12:28Z"
"","4238","HADOOP-16202. Enhanced openFile() -branch-3.3 backport","### Description of PR  backport of HADOOP-16202. Enhanced openFile() to branch-3.3, plus a couple of other cherrypicks from trunk to ease the backporting.  if yetus is happy i wil merge the entire sequence in as the ordered chain of commits   ### How was this patch tested?  cloud store testing in progress against aws london and azure cardiff.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-04-27T11:45:44Z","2022-04-28T11:05:23Z"
"","4394","HDFS-16583. DatanodeAdminDefaultMonitor can get stuck in an infinite loop","### Description of PR  Avoid concurrently modifying the outOfServiceNodeBlocks map in the DatanodeAdminDefaultMonitor when ending maintenance. The concurrentModification can cause the monitor to get stuck in an infinite loop in some circumstances, while holding the NN write lock.  The issue does not affect the DatanodeAdminBackoffMonitor, as it is implemented slightly differently.  More details in the Jira https://issues.apache.org/jira/browse/HDFS-16583  This is a backport for branch-3.2 as the newer branches have diverged significantly.  ### How was this patch tested?  Existing tests cover it.","closed","","sodonnel","2022-05-31T20:27:35Z","2022-06-01T01:46:36Z"
"","4332","HDFS-16583. DatanodeAdminDefaultMonitor can get stuck in an infinite loop holding the write lock","### Description of PR  Avoid concurrently modifying the `outOfServiceNodeBlocks` map in the DatanodeAdminDefaultMonitor when ending maintenance. The concurrentModification can cause the monitor to get stuck in an infinite loop in some circumstances, while holding the NN write lock.  The issue does not affect the DatanodeAdminBackoffMonitor, as it is implemented slightly differently.  More details in the Jira  https://issues.apache.org/jira/browse/HDFS-16583  ### How was this patch tested?  Existing tests should cover it.","closed","","sodonnel","2022-05-19T11:16:08Z","2022-05-26T17:11:11Z"
"","3939","HADOOP-17415. Use S3 content-range header to update length of an object during reads","### Description of PR  As part of all the openFile work, knowing full length of an object allows for a HEAD to be skipped. But: code knowing only the splits don't know the final length of the file.  If the content-range header is used, then as soon as a single GET is initiated against an object, if the field is returned then we can update the length of the S3A stream to its real/final length  * Skip file status probe on openFile. Content length will be updated on first read. * As a side effect, modification time, etag and version id also be unknown as there is no probe on openFile. * Allow content length to be negative value, it means content length is unknown. * On read file, use information from Content-Range header to update content length. * If out of range exception occurs on read, ActualObjectSize from exception details can be used to update content length.  ### How was this patch tested?  Tested in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify` (AP tests failed from previous SDK upgrade)  ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3ABucketExistence.testAccessPointProbingV2:171->expectUnknownStore:103->lambda$testAccessPointProbingV2$12:172 » IllegalArgument [ERROR]   ITestS3ABucketExistence.testAccessPointRequired:188->expectUnknownStore:103->lambda$testAccessPointRequired$14:189 » IllegalArgument [INFO]  [ERROR] Tests run: 1063, Failures: 0, Errors: 2, Skipped: 186 [INFO] Results: [INFO]  [WARNING] Tests run: 108, Failures: 0, Errors: 0, Skipped: 68 ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","monthonk","2022-01-27T13:51:54Z","2022-02-24T16:42:52Z"
"","4563","YARN-11210. Fix YARN RMAdminCLI retry logic for non-retryable kerbero…","### Description of PR  Applications which call YARN RMAdminCLI (i.e. YARN ResourceManager client) synchronously can be blocked for up to 15 minutes with the default configuration of ""yarn.resourcemanager.connect.max-wait.ms""; this is not an issue in of itself, but there is a non-retryable IllegalArgumentException exception thrown within the YARN ResourceManager client that is getting swallowed & treated as a retryable ""connection exception"" meaning that it gets retried for 15 minutes.  The purpose of this JIRA (and PR) is to modify the YARN client so that it does not retry on this non-retryable exception.  See JIRA for additional details: https://issues.apache.org/jira/browse/YARN-11210  ### How was this patch tested?  - Create Kerberized YARN cluster  - Run YARN rmadmin client & validate it completes successfully  ``` > yarn rmadmin -refreshNodes;  22/07/13 15:30:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8033  >> Success is silent in client logs, but can be seen in the ResourceManager logs << ```  - Unset the value of ""yarn.resourcemanager.principal"" is ""yarn-site.xml""  - Run YARN rmadmin client & validate it retries for 15 minutes  ``` > yarn rmadmin -refreshNodes;  22/06/28 14:23:45 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8033 22/06/28 14:23:46 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) ... 22/06/28 14:23:55 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8033. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 22/06/28 14:23:56 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8033. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) ... 22/06/28 14:24:05 INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8033. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 22/06/28 14:24:05 INFO retry.RetryInvocationHandler: java.net.ConnectException: Your endpoint configuration is wrong; For more details see:  http://wiki.apache.org/hadoop/UnsetHostnameOrPort, while invoking ResourceManagerAdministrationProtocolPBClientImpl.refreshNodes over null after 1 failover attempts. Trying to failover after sleeping for 27166ms. 22/06/28 14:24:32 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name; Host Details : local host is: ""0.0.0.0/0.0.0.0""; destination host is: ""0.0.0.0"":8033; , while invoking ResourceManagerAdministrationProtocolPBClientImpl.refreshNodes over null after 2 failover attempts. Trying to failover after sleeping for 22291ms. ... 22/06/28 14:37:57 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name; Host Details : local host is: ""0.0.0.0/0.0.0.0""; destination host is: ""0.0.0.0"":8033; , while invoking ResourceManagerAdministrationProtocolPBClientImpl.refreshNodes over null after 28 failover attempts. Trying to failover after sleeping for 26721ms. 22/06/28 14:38:23 INFO retry.RetryInvocationHandler: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name; Host Details : local host is: ""0.0.0.0/0.0.0.0""; destination host is: ""0.0.0.0"":8033; , while invoking ResourceManagerAdministrationProtocolPBClientImpl.refreshNodes over null after 29 failover attempts. Trying to failover after sleeping for 27641ms. refreshNodes: Failed on local exception: java.io.IOException: Couldn't set up IO streams: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name; Host Details : local host is: ""0.0.0.0/0.0.0.0""; destination host is: ""0.0.0.0"":8033; ```  - Modify YARN client runtime classpath to contain the changes in this PR  - Run YARN rmadmin client & validate it fails after 1 try (tested in both federation enabled & non-federation enabled clusters)  ``` > yarn rmadmin -refreshNodes;  22/07/13 17:37:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8033 22/07/13 17:37:50 WARN ipc.Client: Exception encountered while connecting to the server javax.security.sasl.SaslException: Bad Kerberos server principal configuration [Caused by java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name]         at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:237)         at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:159)         at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:397)         at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:630)         at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:424)         at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:825)        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:821)         at java.security.AccessController.doPrivileged(Native Method)         at javax.security.auth.Subject.doAs(Subject.java:422)         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1926)         at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:821)         at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:424)         at org.apache.hadoop.ipc.Client.getConnection(Client.java:1612)         at org.apache.hadoop.ipc.Client.call(Client.java:1442)         at org.apache.hadoop.ipc.Client.call(Client.java:1395)         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)         at com.sun.proxy.$Proxy7.refreshNodes(Unknown Source)         at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceManagerAdministrationProtocolPBClientImpl.refreshNodes(ResourceManagerAdministrationProtocolPBClientImpl.java:145)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:498)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)         at com.sun.proxy.$Proxy8.refreshNodes(Unknown Source)         at org.apache.hadoop.yarn.client.cli.RMAdminCLI.refreshNodes(RMAdminCLI.java:349)         at org.apache.hadoop.yarn.client.cli.RMAdminCLI.refreshNodes(RMAdminCLI.java:423)         at org.apache.hadoop.yarn.client.cli.RMAdminCLI.handleRefreshNodes(RMAdminCLI.java:917)         at org.apache.hadoop.yarn.client.cli.RMAdminCLI.run(RMAdminCLI.java:816)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)         at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)         at org.apache.hadoop.yarn.client.cli.RMAdminCLI.main(RMAdminCLI.java:1027) Caused by: java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name         at org.apache.hadoop.security.SaslRpcClient.getServerPrincipal(SaslRpcClient.java:332)         at org.apache.hadoop.security.SaslRpcClient.createSaslClient(SaslRpcClient.java:233)         ... 35 more refreshNodes: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: Bad Kerberos server principal configuration [Caused by java.lang.IllegalArgumentException: Failed to specify server's Kerberos principal name]; Host Details : local host is: ""0.0.0.0/0.0.0.0""; destination host is: ""0.0.0.0"":8033; ```  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-07-13T21:25:16Z","2022-07-26T03:51:37Z"
"","3853","HDFS-16408. Negative LeaseRecheckIntervalMs will let LeaseMonitor loop forever and print huge amount of log","### Description of PR  Adjust the position of the try catch statement in the LeaseMonitor daemon by moving 'catch(Throwable e)' to the outside of the loop body to end the thread when an unexpected exception is caught.   ### How was this patch tested?  when the configuration item 'dfs.namenode.lease-recheck-interval-ms' is accidentally set to a negative number, origin code will print warning log again and again immediately. This problem can be eliminated after using this patch.","closed","","liever18","2022-01-04T10:34:34Z","2022-01-05T07:51:57Z"
"","4140","HADOOP-18168. Fix S3A ITestMarkerTool dep. on purged public bucket, introduce PublicDatasetTestUtils","### Description of PR  Addressing HADOOP-18168, this change replaces the use of `landsat-pds` with its successor [usgs-landsat](https://registry.opendata.aws/usgs-landsat/), another public dataset. `landsat-pds` recently was cleaned up excluding some specific files and is no longer suitable for this test case. `usgs-landsat` is a requester pays bucket, so costs will not be passed on to the owner of the bucket - instead, they will be paid by the test runner.  We introduce a new class `PublicDatasetTestUtils` which provides a simple interface for getting file systems for a specific purpose. This change allows the bucket used to be replaced or for the tests to be skipped - for example, for S3-compatible stores or non-`aws` partition S3 endpoints.  ### How was this patch tested?  It will be tested from a `eu-west-1` EC2 instance against a `eu-west-1` bucket.","closed","","dannycjones","2022-04-05T09:33:25Z","2022-05-03T15:01:51Z"
"","4476","HADOOP-18103. High performance vectored read API in Hadoop","### Description of PR  Add support for multiple ranged vectored read api in PositionedReadable. The default iterates through the ranges to read each synchronously, but the intent is that FSDataInputStream subclasses can make more efficient readers especially object stores implementation.   ### How was this patch tested? Added new tests. Ran older test suites in ap-south-1. All good.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mukund-thakur","2022-06-21T01:34:15Z","2022-06-22T17:50:31Z"
"","4483","HADOOP-18310 Add option and make 400 bad request retryable","### Description of PR  Add option and make 400 bad request retryable, added `fs.s3a.fail.on.aws.bad.request` and default to `true` such that it's acting the same behavior without turning it on.  ### How was this patch tested? Add a new unit test.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?   ### Integration Tests executed   I configured auth-keys.xml with using `AssumedRoleCredentialProvider` with a `TemporaryAWSCredentialsProvider` in `auth-keys.xml`. the region is us-west-2 and the endpoint is s3.us-west-2.amazonaws.com.  ```  % mvn -Dparallel-tests clean verify -DtestsThreadCount=12 | tee ~/s3-test.log % grep ""\[ERROR] Tests run"" ~/s3-test.log  ### default-integration-test [ERROR] Tests run: 14, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 222.383 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3ATemporaryCredentials [ERROR] Tests run: 11, Failures: 0, Errors: 11, Skipped: 0, Time elapsed: 9.573 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.auth.delegation.ITestSessionDelegationInFileystem [ERROR] Tests run: 11, Failures: 0, Errors: 11, Skipped: 0, Time elapsed: 10.08 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.auth.delegation.ITestRoleDelegationInFileystem [ERROR] Tests run: 7, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 21.432 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.auth.delegation.ITestRoleDelegationTokens [ERROR] Tests run: 20, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 40.107 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.ITestS3AConfiguration [ERROR] Tests run: 6, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 70.07 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.auth.delegation.ITestDelegatedMRJob  ### overall first set of tests for default-integration-test has 36 errors  [ERROR] Tests run: 1088, Failures: 0, Errors: 36, Skipped: 96    # sequential-integration-tests  [ERROR] Tests run: 9, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 546.824 s <<< FAILURE! - in org.apache.hadoop.fs.contract.s3a.ITestS3AContractRootDir [ERROR] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 15.9 s <<< FAILURE! - in org.apache.hadoop.fs.s3a.tools.ITestMarkerToolRootOperations  ### second set of tests for the sequential-integration-tests has 4 errors  [ERROR] Tests run: 117, Failures: 0, Errors: 4, Skipped: 77  ```   The default-integration-test failed because the following reasons, they looks fine to me. 1. `com.amazonaws.services.securitytoken.model.AWSSecurityTokenServiceException: Cannot call GetSessionToken with session credentials` 2. `ava.io.IOException: org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider instantiation exception: java.lang.IllegalArgumentException: Proxy error: fs.s3a.proxy.username or fs.s3a.proxy.password set without the other.` 3. `java.nio.file.AccessDeniedException: s3a://abc/fork-0006/test: getFileStatus on s3a://abc/fork-0006/test: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied;` 4.  `org.apache.hadoop.service.ServiceStateException: java.io.IOException: Unset property fs.s3a.assumed.role.arn`, this one is strange because I have set `fs.s3a.assumed.role.arn` in `auth-keys.xml`   For the sequential-integration-tests, the errors are.  1. `org.junit.runners.model.TestTimedOutException: test timed out after 180000 milliseconds`   - this time mainly caused by a the `getFileStatus` cannot be done, `ERROR contract.ContractTestUtils (ContractTestUtils.java:cleanup(383)) - Error deleting in TEARDOWN - /test: java.io.InterruptedIOException: getFileStatus on s3a://taklwu-thunderhead-dev/test: com.amazonaws.AbortedException`  2. `[ERROR] test_100_audit_root_noauth(org.apache.hadoop.fs.s3a.tools.ITestMarkerToolRootOperations)  Time elapsed: 7.774 s  <<< ERROR! 46: Marker count 2 out of range [0 - 0]`   I'm wondered if I have a clean simple credential without using assumeRole setup, all above tests could passed and would not get into permission and the special `GetSessionToken` problem.","open","","taklwu","2022-06-21T19:00:46Z","2022-06-24T11:04:36Z"
"","3942","HDFS-16443. Fix edge case where DatanodeAdminDefaultMonitor doubly en…","### Description of PR  A rare edge case was noticed in DatanodeAdminDefaultMonitor which causes a DatanodeDescriptor to be added twice to the pendingNodes queue.   - a [datanode is unhealthy so it gets added to ""unhealthyDns""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L227) - an exception is thrown which causes [this catch block](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L271) to execute - the [datanode is added to ""pendingNodes""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L276) - under certain conditions the [datanode can be added again from ""unhealthyDns"" to ""pendingNodes"" here](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L296)  ### How was this patch tested?  All ""TestDecommission"" & ""DatanodeAdminManager"" tests pass when run locally  Same manual testing from https://github.com/apache/hadoop/pull/3675 applies.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-01-28T15:17:34Z","2022-01-31T04:04:45Z"
"","3923","HDFS-16303. Fix edge case where DatanodeAdminDefaultMonitor doubly en…","### Description of PR  A rare edge case was noticed in DatanodeAdminDefaultMonitor which causes a DatanodeDescriptor to be added twice to the pendingNodes queue.   - a [datanode is unhealthy so it gets added to ""unhealthyDns""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L227) - an exception is thrown which causes [this catch block](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L271) to execute - the [datanode is added to ""pendingNodes""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L276) - under certain conditions the [datanode can be added again from ""unhealthyDns"" to ""pendingNodes"" here](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L296)  ### How was this patch tested?  All ""TestDecommission"" & ""DatanodeAdminManager"" tests pass when run locally  Same manual testing from https://github.com/apache/hadoop/pull/3675 applies.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-01-24T19:47:52Z","2022-01-28T15:18:26Z"
"","3922","Feature/hdfs 16303","### Description of PR  A rare edge case was noticed in DatanodeAdminDefaultMonitor which causes a DatanodeDescriptor to be added twice to the pendingNodes queue.   - a [datanode is unhealthy so it gets added to ""unhealthyDns""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L227) - an exception is thrown which causes [this catch block](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L271) to execute - the [datanode is added to ""pendingNodes""](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L276) - under certain conditions the [datanode can be added again from ""unhealthyDns"" to ""pendingNodes"" here](https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeAdminDefaultMonitor.java#L296)   ### How was this patch tested?  All ""TestDecommission"" & ""DatanodeAdminManager"" tests pass when run locally  Same manual testing from https://github.com/apache/hadoop/pull/3675 applies.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [n/a] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2022-01-24T19:42:54Z","2022-01-24T19:48:00Z"
"","4074","HADOOP-18160 Avoid shading wildfly.openssl runtime dependency","### Description of PR  `org.wildfly.openssl` is a runtime library and its references are being shaded on Hadoop, breaking the integration with other frameworks like Spark, whenever the ""fs.s3a.ssl.channel.mode"" is set to ""openssl"". The error produced in this situation is:  ``` Suppressed: java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/org/wildfly/openssl/OpenSSLProvider ```  This PR aims to exclude `org.wildfly.openssl` from the shade, keeping the original namespace to be referenced whenever this runtime dependency is added to the classpath.   ### How was this patch tested?  Used the patched jar on a Spark 3.2.1 application which now correctly references the `org.wildfly.openssl` dependency added to the classpath.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","andreAmorimF","2022-03-16T14:22:21Z","2022-03-22T21:46:35Z"
"","4305","HADOOP-18231. Adds in new test for S3PrefetchingInputStream","### Description of PR  `ITestS3AInputStreamPerformance` was failing when prefetching is enabled. This PR disables prefetching when running `ITestS3AInputStreamPerformance`.   It also adds in a new test class `ITestS3PrefetchingInputStream` with tests specific for prefetching. Once more stats are added to prefetching iostats, we can add in more tests + assertions. For eg it would be good to assert on if a file has been read via the cache or not on a backward seek.   ### How was this patch tested?  Tested in eu-west-1 by running  `mvn -Dparallel-tests -DtestsThreadCount=16 clean verify`","closed","","ahmarsuhail","2022-05-12T10:57:35Z","2022-06-09T16:21:30Z"
"","4274","YARN-11122. Support getClusterNodes API in FederationClientInterceptor","### Description of PR  [Yarn-11122] Support getClusterNodes In Federation architecture (https://issues.apache.org/jira/browse/YARN-11122)  In [YARN-10465](https://issues.apache.org/jira/browse/YARN-10465), some methods have been implemented, from a personal point of view, some doubts. Question 1: Without considering the metric implementation, it is impossible to understand the execution of related functions. Question 2: Using multi-threading and reflection implementation, the readability of the related logic is relatively poor, and there is not much performance difference between this method and the conventional loop method to obtain the theory. Question 3: The code is already 2 years old, merged into the local branch, there may be conflicts.  ### How was this patch tested?  Added Junit Test to test normal requests and NULL requests, all as expected  ### For code changes:  1.Use the loop method to call the getClusterNodes method of the subcluster one by one 2.Complete the Metric indicator counting function 3.Adapt to branch2.10 & branch3.3","closed","","slfan1989","2022-05-07T03:05:12Z","2022-05-15T16:16:07Z"
"","3816","YARN-11053. AuxService should not use class name as default system classes","### Description of PR  [YARN-11053] AuxService should not use class name as default system classes  Currently, if `yarn.nodemanager.aux-services.spark_shuffle.system-classes` is absent, yarn will treat `yarn.nodemanager.aux-services.spark_shuffle.class` as the system classes, and use system ClassLoader rather than AuxServiceClassLoader to load aux service entry class, then cause ClassNotFound exception.  ### How was this patch tested?  Existing UTs.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pan3793","2021-12-18T12:41:06Z","2021-12-24T02:17:15Z"
"","3719","HADOOP-18024. SocketChannel is not closed when IOException happens in Server$Listener.doAccept","### Description of PR  [HADOOP-18024](https://issues.apache.org/jira/browse/HADOOP-18024)  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","functioner","2021-11-25T05:42:34Z","2021-12-08T15:40:59Z"
"","3790","HDFS-11242. Support refreshTopology","### Description of PR  1. Support refreshing topology of clients/datanodes in NameNode 2. Usage: `hdfs dfsadmin -refreshTopology ` 3. Based on the patch `HDFS-11242.002.patch` at https://issues.apache.org/jira/browse/HDFS-11242 with the following changes     - Rebase to `trunk`     - Change from refreshing all nodes to a specific node, as the former holds the lock for a long time   ### How was this patch tested?  1. Tested with UT 2. Tested in a real HDFS cluster ``` $ hdfs dfsadmin -fs hdfs://test -refreshTopology 1.2.3.4 Refresh topology successful at active-namenode/1.1.1.1:8020 for 1.2.3.4 Refresh topology successful at standby-namenode/1.1.1.2:8020 for 1.2.3.4 Refresh topology successful at observer-namenode/1.1.1.3:8020 for 1.2.3.4 ```  ### Note  1. It is also ok to pass in a hostname instead of an IP address, as keys of the cache may be hostnames sometimes.","closed","","secfree","2021-12-12T05:12:06Z","2022-06-14T07:17:42Z"
"","3847","HDFS-16406. ReadsFromLocalClient counts short-circuit reads","### Description of PR  1. ReadsFromLocalClient counts short-circuit reads  ### How was this patch tested?  1. UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","secfree","2022-01-01T16:02:40Z","2022-02-09T02:07:31Z"
"","3837","HADOOP-18059. Make reloadCachedMappings and resolve consistent","### Description of PR  1. Make CachedDNSToSwitchMapping#reloadCachedMappings and CachedDNSToSwitchMapping#resolve consistent  ### How was this patch tested?  1. UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","secfree","2021-12-28T08:29:41Z","2022-01-31T06:30:09Z"
"","4008","HDFS-16475. Cleanup code in the write path","### Description of PR  1. In DFSOutputStream#newStreamForCreate(), `shouldRetry` is always true at the while loop. Remove this unnecessary varible. 2. Cleanup code in the write path.  ### How was this patch tested?  No code logic was changed, CI should be enough.","closed","","kaijchen","2022-02-22T03:57:08Z","2022-02-28T14:18:02Z"
"","3850","HDFS-16169. Fix TestBlockTokenWithDFSStriped#testEnd2End failure","### Description of PR  1. Fix TestBlockTokenWithDFSStriped#testEnd2End failure  ### How was this patch tested?  1. Run TestBlockTokenWithDFSStriped#testEnd2End 30 times, the `trunk` version failed `5` times, the version with this patch failed `0` times.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","secfree","2022-01-03T12:02:42Z","2022-01-28T08:04:12Z"
"","3821","HDFS-16392. Fix TestWebHdfsFileSystemContract#testResponseCode timeout","### Description of PR  1. Fix random timeout failures of TestWebHdfsFileSystemContract#testResponseCode  ### How was this patch tested?  1. UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","secfree","2021-12-20T13:52:52Z","2021-12-22T02:44:35Z"
"","3815","HDFS-16168. Fix TestHDFSFileSystemContract.testAppend timeout","### Description of PR  1. Fix random timeout failures of TestHDFSFileSystemContract.testAppend  ### How was this patch tested?  1. UT  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","secfree","2021-12-18T08:29:22Z","2021-12-20T13:56:27Z"
"","3981","HADOOP-18121: Change scope of fixFileStatus and Fix path in fileStatus for internal directories","### Description of PR  - Change scope of fixFileStatus - Fix path in fileStatus for internal directories  ### How was this patch tested? mvn test -Dtest=TestViewFileSystem*  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","cheyu2022","2022-02-11T00:52:24Z","2022-02-16T18:33:00Z"
"","4040","HADOOP-18075. ABFS: Fix failure caused by listFiles() in ITestAbfsRestOperationException","### Description of PR  #3876 rebased, with a minor change to the test, and tested locally  ### How was this patch tested?  against azure cardiff with `-Dparallel-tests=abfs -DtestsThreadCount=10`  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2022-02-28T15:56:36Z","2022-03-07T22:26:13Z"
"","4314","YARN-11153. Make proxy server support YARN federation.","### Description of PR   Can't get running appmaster's web app in federation mode.  (Detail message seen: https://issues.apache.org/jira/browse/YARN-10775) Two step to solve this problem :  (a) make proxy server support federation. (b) make router support proxy server.  It is the first step(a), detail message see: https://issues.apache.org/jira/browse/YARN-11153  ### How was this patch tested?  manually test in our cluster and unit test.  ### For code changes:  make proxyserver support federation","open","","zhengchenyu","2022-05-16T10:29:22Z","2022-08-01T18:03:10Z"
"","4094","HADOOP-18169. getDelegationTokens in ViewFs should also fetch the token from fallback FS","### Description of PR   ### How was this patch tested? mvn test -Dtest=""TestViewFs*"" in both hadoop-common-project and hadoop-hdfs-project.","closed","","xinglin","2022-03-22T17:54:46Z","2022-03-31T21:59:10Z"
"","4418","HADOOP-18197. Upgrade protobuf to 3.21.1","### Description of PR     This patch bumps up the protobuf version so that Hadoop is not a vulnerable to CVE-2021-22569.  Depends on a version of hadoop-shaded-protobuf_3_7 with the update; this PR does this by depending on 1.2.0-SNAPSHOT...this is only going to work for local builds  ### How was this patch tested?  non-native build on a local mac against a local build of the thirdparty jar.  none of the docker changes/build instructions have been tested yet  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [X] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [X] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","steveloughran","2022-06-08T16:42:27Z","2022-06-14T09:08:16Z"
"","3967","HDFS-16449. Fix hadoop web site release notes and changelog not available","![image](https://user-images.githubusercontent.com/46367746/153176207-f5d7cd22-505c-4962-8f3f-4498815d6403.png) ![image](https://user-images.githubusercontent.com/46367746/153176306-2c3aeea0-6470-401f-b7dc-725cd517e67e.png)","closed","","GuoPhilipse","2022-02-09T10:10:56Z","2022-02-13T20:38:40Z"
"","4062","HDFS-16501. Print the exception when reporting a bad block","![image](https://user-images.githubusercontent.com/2844826/157654307-18cec889-2b59-40c4-b40c-a7e0aacf73ef.png)  Currently, volumeScanner will find bad blocks and report them to namenode without printing the reason why they are bad blocks. I think we should be better print the exception in log file.","closed","","liubingxing","2022-03-10T11:39:16Z","2022-03-23T06:31:51Z"
"","4612","HADOOP-15984. Update jersey from 1.19 to 2.x (WIP)","","open","","virajjasani","2022-07-22T23:00:13Z","2022-07-23T13:48:16Z"
"","4511","HADOOP-18315. Fix 3.3 build problems caused by backport of HADOOP-11867.","","open","","omalley","2022-06-29T00:45:19Z","2022-07-11T18:06:36Z"
"","4440","MAPREDUCE-7389. Fix typo in description of property","","closed","","usev6","2022-06-14T16:37:38Z","2022-06-21T17:36:44Z"
"","4413","Metrics","","closed","","anmolanmol1234","2022-06-07T06:26:27Z","2022-06-07T13:57:58Z"
"","4358","HDFS-16456. EC: Decommission a rack with only on dn will fail when the rack number is equal with replication.","","closed","","jojochuang","2022-05-26T06:07:22Z","2022-06-03T22:56:08Z"
"","4268","[Test] Release 3.3.3 RC testing","","closed","","virajjasani","2022-05-05T23:11:20Z","2022-05-15T20:21:34Z"
"","4265","YARN-11126. ZKConfigurationStore Java deserialisation vulnerability","","closed","","tomicooler","2022-05-05T12:55:01Z","2022-05-12T11:42:35Z"
"","4073","[Test] Run full build for 3.2.3 RC","","closed","","virajjasani","2022-03-15T09:17:07Z","2022-03-28T07:00:20Z"
"","4052","HADOOP-18088. Replace log4j 1.x with reload4j.","","closed","","iwasakims","2022-03-07T16:44:32Z","2022-05-07T19:47:36Z"
"","3938","YARN-11069. Dynamic Queue ACL handling in Legacy and Flexible Auto Created Queues","","closed","","tomicooler","2022-01-27T10:16:55Z","2022-03-25T22:41:04Z"
"","3910","HDFS-16426. Fix nextBlockReportTime when trigger full block report force","","closed","","liubingxing","2022-01-20T13:08:39Z","2022-01-20T17:35:47Z"
"","3898","HDFS-16428. Source path with storagePolicy cause wrong typeConsumed while rename","","closed","","ThinkerLei","2022-01-18T03:37:38Z","2022-01-25T07:26:19Z"
"","3780","HDFS-16317 : Backport HDFS-14729 for branch-3.2.3","","closed","","AnanyaSingh2121","2021-12-10T05:48:37Z","2021-12-22T05:32:31Z"
"","3697","Revert ""HADOOP-17995. Stale record should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson""","","closed","","ferhui","2021-11-22T02:04:27Z","2021-11-22T02:07:11Z"