"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","3475","YARN-10953. Make CapacityScheduler#getOrCreateQueueFromPlacementConte…","…xt easier to comprehend    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-09-23T13:38:45Z","2021-10-07T18:08:04Z"
"","2984","HDFS-16010.Improve NamenodeBeanMetrics#getSafemode() to print IOException.","…tion.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-05-06T13:06:23Z","2021-05-06T14:52:42Z"
"","3257","YARN-8992. Fair scheduler can delete a dynamic queue while an applica…","…tion attempt is being added to the queue. (Contributed by Wilfred Spiegelenburg)  (cherry picked from commit a41b648e98b6a1c5a9cdb7393e73e576a97f56d4)  Backport YARN-8992 to branch-3.2. Let me check the result of precommit job before committing.","closed","backport,","aajisaka","2021-08-03T04:16:36Z","2021-08-08T05:24:24Z"
"","3295","HDFS-16162.Improve DFSUtil#checkProtectedDescendants() related parameter comments.","…ter comments.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","documentation,","jianghuazhu","2021-08-11T11:10:25Z","2021-08-17T08:47:24Z"
"","3402","YARN-10914. Simplify duplicated code for tracking ResourceUsage in AbstractCSQueue","…stractCSQueue.  Change-Id: Ib24e8c2dd610b6c3f96b2aabbb4531ee494af432    ### Description of PR   ### How was this patch tested?  Existing tests should cover this code path.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-08T09:16:33Z","2021-09-10T16:54:56Z"
"","3348","HADOOP-15129. Datanode caches namenode DNS lookup failure and cannot …","…startup  Co-authored-by: Karthik Palaniappan Change-Id: Id95de455979451c8bb15f47d7f1210f7aac150b9    ### Description of PR  In RPC connection, move failure handling of unresolved hostname into retry loop, so that if the server is temporarily unresolvable in DNS, the connection can still recover.  ### How was this patch tested?  In addition to new unit tests, this patch has been tested in Dataproc clusters and shown to fix a problem with sequencing of startup if a DataNode starts before the NameNode is registered in DNS.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","cnauroth","2021-08-28T03:53:09Z","2021-09-13T16:23:35Z"
"","2836","HDFS-15936.Solve BlockSender#sendPacket() does not record SocketTimeout exception.","…SocketTimeout exception.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-03-30T06:41:09Z","2021-07-29T04:57:29Z"
"","3186","HDFS-16118.Improve the number of handlers that initialize NameNodeRpcServer#clientRpcServer.","…Server#clientRpcServer.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-07-07T10:23:22Z","2022-01-16T01:00:35Z"
"","3458","YARN-10960. Extract test queues and related methods from TestCapacity…","…Scheduler.  Change-Id: I6fe1918c5776b44e824b39b8cde7b3114522107d    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-20T08:03:55Z","2021-10-01T02:06:53Z"
"","3339","YARN-10646. TestCapacitySchedulerWeightMode test descriptor comments doesnt reflect the correct scenario","…rentUsePctChildUseWeight test based on the description in comment.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-08-26T09:21:02Z","2021-09-08T14:11:32Z"
"","3510","YARN-10973. Remove Jersey version from application.wadl for Security …","…Reasons.  Change-Id: I1991763a7f78fb1c75e4a4c3f06374b8cfb4b2a5    ### Description of PR  Please read the associated Jira ticket, do we really need a workaround like this?  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-10-01T11:07:00Z","2021-10-04T09:52:18Z"
"","3155","HDFS-16095. Add lsQuotaList command and getQuotaListing api for hdfs …","…quota.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","zhuxiangyi","2021-06-29T05:16:46Z","2022-01-06T05:10:31Z"
"","3282","YARN-10814 3.3 backport","…pty.  The rest endpoint would be unusable with an empty secret file (throwing IllegalArgumentExceptions).  Any IO error would have resulted in the same fallback path.  Change-Id: Ieb147a0f6f8f628cdafb3c987d0c9a440fa8c6d0  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","backport,","tomicooler","2021-08-09T13:40:01Z","2021-08-10T09:25:36Z"
"","3206","YARN-10814. Fallback to RandomSecretProvider if the secret file is empty","…pty.  The rest endpoint would be unusable with an empty secret file (throwing IllegalArgumentExceptions).  Any IO error would have resulted in the same fallback path.  Change-Id: Ieb147a0f6f8f628cdafb3c987d0c9a440fa8c6d0  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-07-15T06:23:04Z","2021-07-30T13:15:03Z"
"","3333","YARN-10576. Update Capacity Scheduler documentation about JSON-based …","…placement mapping    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-08-25T15:07:23Z","2021-09-08T13:19:35Z"
"","3092","HDFS-16062.When a DataNode hot reload configuration, JMX will block for a long time.","…or a long time.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-06-10T09:32:41Z","2021-06-10T23:52:25Z"
"","2782","HDFS-15901.Solve the problem of DN repeated block reports occupying too many RPCs during Safemode.","…oo many RPCs during Safemode.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-03-17T13:40:32Z","2021-03-22T10:56:33Z"
"","2913","HDFS-15978.Solve DatanodeManager#getBlockRecoveryCommand() printing IOException.","…OException.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-04-15T07:26:17Z","2021-04-25T06:31:51Z"
"","3247","HDFS-16146. All three replicas are lost due to not adding a new DataN…","…ode in time.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhangshuyan0","2021-07-29T08:47:41Z","2021-08-03T16:24:46Z"
"","3453","HADOOP-17919. Fix command line example in Hadoop Cluster Setup documentation.","…ntation.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","rinikeda","2021-09-17T09:07:53Z","2021-09-17T13:24:45Z"
"","3652","YARN-11006. Allow overriding user limit factor and maxAMResourcePerce…","…nt with AQCv2 templates    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-11-12T16:41:07Z","2021-12-07T14:44:07Z"
"","3270","HDFS-16154. TestMiniJournalCluster failing intermittently because of …","…not reseting UserGroupInformation completely  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute JIRA: https://issues.apache.org/jira/browse/HDFS-16154","closed","","wzhallright","2021-08-05T11:14:09Z","2021-08-06T10:23:01Z"
"","2991","HDFS-16015. RBF: Read and write data through router select dn according to real user ip","…ng to real user ip","closed","","zhuxiangyi","2021-05-08T10:59:51Z","2021-05-08T18:56:10Z"
"","3365","YARN-10931.Remove some invalid characters in NMClientAsyncImpl#ContainerState.","…nerState.    jira:YARN-10931","closed","","jianghuazhu","2021-09-01T02:08:13Z","2021-10-29T05:42:12Z"
"","2994","HDFS-16018. Optimize the display of hdfs ""count -e"" or ""count -t"" com…","…mand  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","whbing","2021-05-10T06:50:35Z","2021-05-20T03:24:11Z"
"","3255","HDFS-16149.Improve the parameter annotation in FairCallQueue#priorityLevels.","…Levels.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-08-02T08:40:35Z","2021-08-03T08:53:25Z"
"","3632","YARN-10822. Containers going from New to Scheduled transition for kil…","…led container on recovery    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","minni31","2021-11-08T18:04:16Z","2022-02-01T17:35:59Z"
"","3491","HDFS-16239. XAttr#toString doesnt print the attribute value in readab…","…le format    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2021-09-27T18:32:14Z","2021-10-06T11:21:52Z"
"","2831","HDFS-15920.Solve the problem that the value of SafeModeMonitor#RECHECK_INTERVAL can be configured.","…K_INTERVAL can be configured.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-03-29T07:37:57Z","2021-09-14T08:54:36Z"
"","3290","HADOOP-17843. Support IPV6 with IP for internal and external communication","…ion.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","prasad-acit","2021-08-10T21:05:50Z","2022-04-21T17:36:55Z"
"","2741","HDFS-15855.Solve the problem of incorrect EC progress when loading FsImage.","…Image.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-03-04T07:13:25Z","2021-03-22T15:11:15Z"
"","3474","YARN-10954. Remove commented code block from CSQueueUtils#loadCapacit…","…iesByLabelsFromConf    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-09-23T13:38:29Z","2021-10-19T11:13:10Z"
"","3236","YARN-10856. Prevent ATS v2 health check REST API call if the ATS service itself is disabled.","…ice itself is disabled.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","brumi1024","2021-07-26T18:10:15Z","2021-07-29T17:15:27Z"
"","2912","YARN-10738: When multi thread scheduling with multi node, we should s…","…huffle to prevent hot accessing nodes.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","zhuqi-lucas","2021-04-15T06:05:55Z","2021-11-14T23:21:15Z"
"","3407","YARN-10910. AbstractCSQueue#setupQueueConfigs: Separate validation logic from initialization logic","…gic from initialization logic    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-09-08T17:27:16Z","2021-09-10T17:17:39Z"
"","3126","HADOOP-17714 ABFS: testBlobBackCompatibility, testRandomRead & WasbAb…","…fsCompatibility tests fail when triggered with default configs (#3035)  (cherry picked from commit 35e4c31fff9946d35a3543481585031558e9f5d5)","closed","","snehavarma","2021-06-21T07:57:34Z","2021-07-12T06:23:47Z"
"","2833","HDFS-15934: Make DirectoryScanner reconcile blocks batch size and int…","…erval between batch configurable.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhuqi-lucas","2021-03-29T14:44:05Z","2021-05-05T15:32:47Z"
"","3634","HADOOP-17997. RBF: Namespace usage of mount table with multi subclust…","…ers can exceed quota    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","xicm","2021-11-09T03:27:13Z","2021-11-17T09:44:28Z"
"","3553","HDFS-16273. RBF: RouterRpcFairnessPolicyController add availableHandl…","…eOnPerNs metrics    ### Description of PR [HDFS-16273](https://issues.apache.org/jira/browse/HDFS-16273) Add the availableHandlerOnPerNs metrics to monitor whether the number of handlers configured for each NS is reasonable when using RouterRpcFairnessPolicyController.","closed","","zhuxiangyi","2021-10-14T08:34:07Z","2021-11-08T00:31:50Z"
"","3390","YARN-10912. Refactor AbstractCSQueue#updateConfigurableResourceRequir…","…ement.   - capacityConfigType update is extracted to a separate method  - validation logic is extracted to a helper function  - min resource must not be greater than max resource is now checked    after the max resource is updated  Change-Id: I731c2639281721afed32c30854bafcf048d6ee28    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-06T09:55:38Z","2021-09-14T15:30:44Z"
"","2918","HDFS-15987. Improve oiv tool to parse fsimage file in parallel with d…","…elimited format  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","whbing","2021-04-16T09:18:36Z","2022-03-23T03:36:41Z"
"","3418","YARN-10915. AbstractCSQueue: Simplify complex logic in methods: deriveCapacityFromAbsoluteConfigurations and updateEffectiveResources","…eCapacityFromAbsoluteConfigurations and updateEffectiveResources    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-09-10T17:27:28Z","2021-09-14T17:35:49Z"
"","3256","HDFS-16151.Improve the parameter comments related to ProtobufRpcEngine2#Server().","…e2#Server().  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-08-03T01:56:22Z","2021-08-08T05:56:13Z"
"","3526","YARN-6862. Nodemanager resource usage metrics should not show negativ…","…e values.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-10-06T17:00:13Z","2021-10-12T15:36:11Z"
"","3517","YARN-10934. Fix LeafQueue#activateApplication NPE when the user of th…","…e pending application is missing from usersManager.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-10-04T16:33:45Z","2021-10-07T18:11:43Z"
"","3500","YARN-10949. Simplify AbstractCSQueue#updateMaxAppRelatedField and fin…","…d a more meaningful name for this method    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-09-29T13:54:51Z","2021-10-20T10:56:42Z"
"","2841","HDFS-15939.Solve the problem that DataXceiverServer#run() does not record SocketTimeout exception.","…cord SocketTimeout exception.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-03-31T03:30:18Z","2021-08-17T18:23:40Z"
"","3254","YARN-8990. Fix fair scheduler race condition in app submit and queue …","…cleanup. (Contributed by Wilfred Spiegelenburg)  (cherry picked from commit 524a7523c427b55273133078898ae3535897bada)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java  Backport YARN-8990 to branch-3.2.","closed","backport,","aajisaka","2021-08-02T06:54:16Z","2021-08-08T05:24:33Z"
"","3465","HADOOP-17926. Maven-eclipse-plugin is no longer needed since Eclipse …","…can import Maven projects by itself.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","rinikeda","2021-09-21T07:20:23Z","2021-09-22T06:49:46Z"
"","2720","YARN-10650: Create dispatcher metrics interface, and apply to RM asyn…","…c dispatcher.  Now there are no dispatcher detailed metrics about event type counters and processing time metrics. It will be very helpful to big cluster event monitor.","open","","zhuqi-lucas","2021-02-24T08:28:47Z","2021-02-24T08:34:02Z"
"","3175","HDFS-16111. Add a configuration to RoundRobinVolumeChoosingPolicy to …","…avoid failed volumes at datanodes.  Change-Id: Iead25812d4073e3980893e3e76f7d2b03b57442a  JIRA: https://issues.apache.org/jira/browse/HDFS-16111  there is a potential bug when picking a disk volume to write a new block file(replica). By default, Hadoop uses RoundRobinVolumeChoosingPolicy, The code to select a disk will check whether the available space on the selected disk is more than the size bytes of block file to store (https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/RoundRobinVolumeChoosingPolicy.java#L86) But when creating a new block, there will be two files created: one is the block file blk_XXXX, the other is block metadata file blk_XXXX_XXXX.meta, this is the code when finalizing a block, both block file size and meta data file size will be updated: https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java#L391 the current code only considers the size of block file and doesn't consider the size of block metadata file, when choosing a disk in RoundRobinVolumeChoosingPolicy. There can be a lot of on-going blocks received at the same time, the default maximum number of DataXceiver threads is 4096. This will underestimate the total size needed to write a block, which will potentially cause the disk full error(No space left on device) when writing a replica.  Since the size of the block metadata file is not fixed, I suggest to add a configuration(dfs.datanode.round-robin-volume-choosing-policy.additional-available-space) to safeguard the disk space when choosing a volume to write a new block data in RoundRobinVolumeChoosingPolicy.","closed","","zhihaixu2012","2021-07-04T22:36:30Z","2021-07-28T04:58:55Z"
"","3461","YARN-10823. Expose all node labels for root without explicit configurations","…ations    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-09-20T09:04:39Z","2021-10-01T02:20:37Z"
"","3502","YARN-10972. Remove stack traces from Jetty's response for Security Re…","…asons.  Change-Id: I0035f823b1f65648a4343a5f99064d1f869c4010    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","tomicooler","2021-09-30T08:48:54Z","2021-10-01T05:52:54Z"
"","3195","YARN-10459. containerLaunchedOnNode method not need to hold scheduler…","…Apptemt lock  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","minni31","2021-07-12T07:22:03Z","2022-02-01T17:37:41Z"
"","3307","HDFS-16175.Improve the configurable value of Server #PURGE_INTERVAL_NANOS.","…ANOS.    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jianghuazhu","2021-08-16T10:41:27Z","2021-08-25T09:34:45Z"
"","3196","HDFS-15160. branch-2.10 ReplicaMap, Disk Balancer, Directory Scanner …","…and various FsDatasetImpl methods should use datanode readlock. Contributed By Stephen O'Donnell and Ahmed Hussein   Backporting HDFS-15160 ReplicaMap, Disk Balancer, Directory Scanner and various FsDatasetImpl methods should use datanode readlock.  Also backported the two jiras that fix unit tests: - HDFS-15818 - HDFS-15457  The main Conflicts were as follow:    - main conflicts caused by [HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File.](https://issues.apache.org/jira/browse/HDFS-10636)     - `FsDatasetImpl.java#validateBlockFile` due to HDFS-10636     - `FsDatasetImpl.java#getFile` only exists in branch-2.10 and was removed in HDFS-10636     - `DiskBalancer` does not exist in branch-2.10     - `FsDatasetImpl.java#moveBlockAcrossStorage` is different in branch-2.10","open","backport,","amahussein","2021-07-12T20:44:31Z","2022-02-01T02:48:55Z"
"","2851","HDFS-15941.Solve the problem that the inspection period of HeartbeatManager#Monitor can be configured.","…anager#Monitor can be configured.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-04-02T02:25:22Z","2021-04-02T11:20:10Z"
"","3269","HDFS-16153. Avoid evaluation of LOG.debug statement in QuorumJournalM…","…anager  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute  JIRA: https://issues.apache.org/jira/browse/HDFS-16153","closed","","wzhallright","2021-08-05T08:34:26Z","2021-08-06T09:36:13Z"
"","3410","YARN-10901. Permission checking error on an existing directory in LogAggregationFileController#verifyAndCreateRemoteLogDir","…AggregationFileController#verifyAndCreateRemoteLogDir (#3355)  Co-authored-by: Tamas Domok  Change-Id: I9584b319cbffa3b0bafccddb6d53c05e92ea0d2c    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-09T08:11:15Z","2021-09-14T15:35:08Z"
"","3409","YARN-10901. Permission checking error on an existing directory in LogAggregationFileController#verifyAndCreateRemoteLogDir","…AggregationFileController#verifyAndCreateRemoteLogDir (#3355)  Co-authored-by: Tamas Domok     ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-09T07:11:35Z","2021-09-14T15:34:32Z"
"","3411","HADOOP-17901. Performance degradation in Text.append() after HADOOP-1…","…6951.  Change-Id: I628380b6d29a2796d9d67bd38b423cdb1830bf04    ### Description of PR  HADOOP-16951 introduced a performance regression to Text.append(). The backing array is not increased as intended, which resulted in a lot of unnecessary new arrays.  ### How was this patch tested?  Tried the change on a cluster, where a mapper successfully read a large text file (1.1GB) without slowing down. Executed unit tests, which all passed.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pbacsko","2021-09-09T09:41:48Z","2021-09-10T23:01:38Z"
"","3279","Add documentation for YARN-10623 auto refresh queue conf in CS","…. Contributed by Qi Zhu.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhuqi-lucas","2021-08-09T09:13:07Z","2021-09-08T15:01:16Z"
"","3218","HADOOP-17028. ViewFS should initialize mounted target filesystems lazily","…. Contributed by Abhishek Das (#2929   (cherry picked from commit 1dd03cc4b573270dc960117c3b6c74bb78215caa)  https://issues.apache.org/jira/browse/HADOOP-17028  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","abhishekdas99","2021-07-20T09:31:00Z","2021-07-21T01:23:20Z"
"","2822","HADOOP-17605. Change String.getBytes() to DFSUtil.string2Bytes(String…","…) to avoid Unsupported Encoding Exception  Hello, I found that DFSUtil.string2Bytes(String) can be used here instead of String.getBytes(). Otherwise, the API String.getBytes() may cause potential risk of UnsupportedEncodingException since the behavior of this method when the string cannot be encoded in the default charset is unspecified. One recommended API is DFSUtil.string2Bytes(String) which provides more control over the encoding process and can avoid this exception.","open","","dbgp2021","2021-03-27T09:28:22Z","2021-05-10T17:52:02Z"
"","2806","HADOOP-17138. Fix spotbugs warnings surfaced after upgrade to 4.0.6.","…(#2155)  (cherry picked from commit 1b29c9bfeee0035dd042357038b963843169d44c)  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tasanuma","2021-03-23T10:44:12Z","2021-03-24T08:09:49Z"
"","3403","YARN-10917. Investigate and simplify CapacitySchedulerConfigValidator…","…#validateQueueHierarchy.  Change-Id: I721799bd66df45303e324153d8d042fd6b1edc8e    ### Description of PR   ### How was this patch tested?   This was just a refactor, existing tests should cover this method.  I checked the if - elif statements, fortunately they did not utilise the previous if branch, proof:  ```   // if (oldQueue instanceof ParentQueue && !(oldQueue instanceof ManagedParentQueue) && newQueue instanceof ManagedParentQueue)   // a: oldQueue instanceof ParentQueue   // b: oldQueue instanceof ManagedParentQueue   // c: newQueue instanceof ManagedParentQueue   // a && !b && c                  --> a && !b && c    // else if (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))   // !(a && !b && c) && (b && !c)  --> b && !c   //                               --> (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))    // else if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue)   // d: oldQueue instanceof LeafQueue   // !(!(a && !b && c) && (b && !c)) && (d && c)  --> c && d   //                                              --> newQueue instanceof ManagedParentQueue && oldQueue instanceof LeafQueue     // else if (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   // e: newQueue instanceof LeafQueue   // !(!(!(a && !b && c) && (b && !c)) && (d && c)) && (a && e)  --> (a && !c && e) || (a && !d && e)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && !(oldQueue instanceof LeafQueue) && newQueue instanceof LeafQueue)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   //                                                             --> oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue ```  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-08T09:21:27Z","2021-09-14T15:54:25Z"
"","3395","YARN-10917. Investigate and simplify CapacitySchedulerConfigValidator…","…#validateQueueHierarchy.    ### Description of PR   ### How was this patch tested?  This was just a refactor, existing tests should cover this method.  I checked the if - elif statements, fortunately they did not utilise the previous if branch, proof: ```   // if (oldQueue instanceof ParentQueue && !(oldQueue instanceof ManagedParentQueue) && newQueue instanceof ManagedParentQueue)   // a: oldQueue instanceof ParentQueue   // b: oldQueue instanceof ManagedParentQueue   // c: newQueue instanceof ManagedParentQueue   // a && !b && c                  --> a && !b && c    // else if (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))   // !(a && !b && c) && (b && !c)  --> b && !c   //                               --> (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))    // else if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue)   // d: oldQueue instanceof LeafQueue   // !(!(a && !b && c) && (b && !c)) && (d && c)  --> c && d   //                                              --> newQueue instanceof ManagedParentQueue && oldQueue instanceof LeafQueue     // else if (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   // e: newQueue instanceof LeafQueue   // !(!(!(a && !b && c) && (b && !c)) && (d && c)) && (a && e)  --> (a && !c && e) || (a && !d && e)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && !(oldQueue instanceof LeafQueue) && newQueue instanceof LeafQueue)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   //                                                             --> oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue ```   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-07T14:07:11Z","2021-09-08T09:58:33Z"
"","3114","HDFS-13671. Namenode deletes large dir slowly caused by FoldedTreeSet#removeAndGet","…#removeAndGet (#3065)  Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/ReplicaMap.java 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockInfo.java 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-06-17T08:30:15Z","2021-06-18T13:04:32Z"
"","2721","HDFS-15856: Make recover the pipeline in same packet exceed times for…","… stream closed configurable.  @jojochuang Could you help review this? Thanks.","closed","","zhuqi-lucas","2021-02-24T14:01:51Z","2021-03-02T05:16:44Z"
"","3194","YARN-10841 Fix token reset synchronization by making sure for UAM response token…","… reset is done while in lock.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","minni31","2021-07-12T07:04:58Z","2021-07-29T09:25:40Z"
"","3504","YARN-10869. CS considers only the default maximum-allocation-mb/vcore property as a maximum when it creates dynamic queues","… property as a maximum when it creates dynamic queues (#3225)  Co-authored-by: Benjamin Teke     ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-09-30T13:54:32Z","2021-10-12T16:06:08Z"
"","3225","YARN-10869. CS considers only the default maximum-allocation-mb/vcore property as a maximum when it creates dynamic queues","… property as a maximum when it creates dynamic queues  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","brumi1024","2021-07-22T15:35:52Z","2021-08-02T15:17:05Z"
"","3355","YARN-10901. Permission checking error on an existing directory in LogAggregationFileController#verifyAndCreateRemoteLogDir","… LogAggregationFileController.  Create a temporary file with proper ownership to see if the underlying file system supports chmod.  When the log directory was already created, but with a different user, there was an exception in the log:    org.apache.hadoop.security.AccessControlException: Permission denied.   user=yarn is not the owner of inode=/tmp/logs  because the setPermission checked the ownership of the given path.  Change-Id: I7c7908b7ec815501337daaf7a2253a11d79afc8c    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-08-30T08:02:12Z","2021-09-09T06:34:41Z"
"","3423","HADOOP-17905. Modify Text.ensureCapacity() to efficiently max out the…","… backing array size  Change-Id: I3cfae85000c5fa7aa86c40c2d7efa282958178bf    ### Description of PR  Allow org.apache.hadoop.io.Text to expand the underlying byte array to a safe maximum.   ### How was this patch tested?  1. Ran unit tests 2. Ran a MapReduce job where a single mapper processed a text file with a size of 1.8G with no line feeds. Array expansion was verified with extra printouts.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","pbacsko","2021-09-11T12:19:11Z","2021-09-30T00:25:56Z"
"","3010","HADOOP-17284. Support BCFKS keystores for Hadoop Credential Provider.…","… (#2334)  (cherry picked from commit 4c5ad57818a7e894b5bf430358e02a0bb8618769)  Prepare for backport HADOOP-17284 to 3.3.1.","closed","","xiaoyuyao","2021-05-12T23:05:00Z","2021-05-13T23:58:23Z"
"","3619","HADOOP-17990. Fix failing concurrent FS.initialize commands when fs.azure.createRemoteFileSystemDuringInitialization is enabled.","…    ### Description of PR  When fs.azure.createRemoteFileSystemDuringInitialization is enabled, the filesystem will create a container if it does not already exist inside the initialize method. The current flow of creating the container will fail in the case of concurrent initialize methods being executed simultaneously (only one request can create the container, the rest will fail instead of moving on). This PR is fixing this issue by also catching org.apache.Hadoop.fs.FileAlreadyExistsException generated by the createFilesystem command.  ### How was this patch tested?  A new test in ITestAzureBlobFileSystemInitAndCreate is introduced which was breaking before the fox.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","majdyz","2021-11-04T16:57:33Z","2021-11-04T17:03:24Z"
"","3297","YARN-10882. Fix branch-3.1 build: zstd library is missing from the Dockerfile","zstd dependency was not installed in the docker environment and caused a build issue in hadoop-common  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-08-11T15:38:51Z","2021-08-12T04:32:47Z"
"","3413","HADOOP-17904. Test Result Not Working In Jenkins Result.","Working Example: https://github.com/apache/hadoop/pull/2773#issuecomment-916059913  The Test Report link works in the above build  https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2773/10/testReport/","closed","","ayushtkn","2021-09-09T12:51:21Z","2021-09-10T23:45:54Z"
"","3011","HADOOP-17666. Update LICENSE for 3.3.1","WIP.  The following steps are included: (1) Inspected the jar files in the produced tarball and updated LICENSE-binary accordingly (2) add LICENSE from hadoop-thirdparty jars. (3) remove any dependencies no longer in the tarball.  TODO:  (4) inspect hadoop's shading logic, find out packages that are shaded/excluded. (5) add javascript packages. (6) update NOTICE file","closed","","jojochuang","2021-05-13T15:34:05Z","2021-05-22T01:15:48Z"
"","2838","HDFS-15937. Reduce memory used during datanode layout upgrade","When the datanode block layout is upgrade from -56 (256x256) to -57 (32x32), we have found the datanode uses a lot more memory than usual.  For each volume, the blocks are scanned and a list is created holding a series of LinkArgs objects. This object contains a File object for the block source and destination. The file object stores the path as a string, eg:  /data01/dfs/dn/current/BP-586623041-127.0.0.1-1617017575175/current/finalized/subdir0/subdir0/blk_1073741825_1001.meta /data01/dfs/dn/current/BP-586623041-127.0.0.1-1617017575175/current/finalized/subdir0/subdir0/blk_1073741825  This is string is repeated for every block and meta file on the DN, and much of the string is the same each time, leading to a large amount of memory.  If we change the linkArgs to store:    *  Src Path without the block, eg /data01/dfs/dn/previous.tmp/BP-586623041-127.0.0.1-1617017575175/current/finalized/subdir0/subdir0   *  Dest Path without the block eg /data01/dfs/dn/current/BP-586623041-127.0.0.1-1617017575175/current/finalized/subdir0/subdir10    * Block / Meta file name, eg blk_12345678_1001 or blk_12345678_1001.meta  Then ensure were reuse the same file object for repeated src and dest paths, we can save most of the memory without reworking the logic of the code.  The current logic works along the source paths recursively, so you can easily re-use the src path object.  For the destination path, there are only 32x32 (1024) distinct paths, so we can simply cache them in a hashMap and lookup the re-useable object each time.  I tested locally by generating 100k block files and attempting the layout upgrade. A heap dump showed the 100k blocks using about 140MB of heap. That is close to 1.5GB per 1M blocks.  After the change outlined above the same 100K blocks used about 20MB of heap, so 200MB per million blocks.  A general DN sizing recommendation is 1GB of heap per 1M blocks, so the upgrade should be able to happen within the pre-upgrade heap.","closed","","sodonnel","2021-03-30T11:21:23Z","2021-04-08T10:59:03Z"
"","2916","HDFS-15981. Removing redundant block queues will slow down block reporting","When the block report satisfies the block distribution strategy, the block is removed from the lowredundancyBlocks. But removing the block from the lowredundancyBlocks is a redundant operation.  First, in the patch queue, the block removal operation will be performed in the method chooseSourceDatanodes and validateReconstructionWork.  second, the removal of the block report will only be at the QUEUE_REPLICAS_BADLY_DISTRIBUTED level, which is not an accurate operation.  Finally, when there is a large amount of data in the QUEUE_REPLICAS_BADLY_DISTRIBUTED queue, the processing efficiency of the block report will be reduced","open","","langlaile1221","2021-04-16T04:54:42Z","2021-04-16T13:08:19Z"
"","3185","HDFS-16119. start balancer with parameters -hotBlockTimeInterval xxx is invalid.","when start balancer with parameters -hotBlockTimeInterval xxx,  it is invalid.  but set hdfs-site.xml is valid.  dfs.balancer.getBlocks.hot-time-interval 1000","closed","","JiaguodongF","2021-07-07T10:05:48Z","2021-07-27T16:55:22Z"
"","2828","HADOOP-17608. Fix NPE in TestKMS","When running all the unit tests in TestKMS by `mvn test -Dtest=TestKMS` instead of specifying a test case (e.g., `mvn test -Dtest=TestKMS#testStartStopHttpsPseudo`) , NPE occurs because there may be some active null threads or some active threads are removed after allocating the active thread list.","closed","","aajisaka","2021-03-29T02:35:36Z","2021-03-31T19:28:30Z"
"","3222","HADOOP-17812. NPE in S3AInputStream read() after failure to reconnect to store","When IOException happens during ""wrappedStream.read"", onReadFailure->reopen will be called and reopen will try to re-open ""wrappedStream"", but what if exception happens during getting S3Object, then ""wrappedStream"" will be null, finally, the ""retry"" may re-execute the read block and cause the NPE.  See the issue https://issues.apache.org/jira/browse/HADOOP-17812","closed","","wbo4958","2021-07-22T01:06:48Z","2021-07-30T19:06:15Z"
"","3105","HDFS-16070. DataTransfer block storm when datanode's io is busy.","When I speed up the decommission, I found that some datanode's io is busy, then I found host's load is very high, and ten thousands data transfer thread are running. Then I find log like below. ``` # setup datatranfer log 2021-06-08 13:42:37,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.201.4.49:9866, datanodeUuid=6c55b7cb-f8ef-445b-9cca-d82b5b077ed1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-37e80bd5-733a-4d7b-ba3d-b46269573c72;nsid=215490653;c=1584525570797) Starting thread to transfer BP-852924019-10.201.1.32-1584525570797:blk_-9223372036449848858_30963611 to 10.201.7.52:9866 2021-06-08 13:52:36,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.201.4.49:9866, datanodeUuid=6c55b7cb-f8ef-445b-9cca-d82b5b077ed1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-37e80bd5-733a-4d7b-ba3d-b46269573c72;nsid=215490653;c=1584525570797) Starting thread to transfer BP-852924019-10.201.1.32-1584525570797:blk_-9223372036449848858_30963611 to 10.201.7.31:9866 2021-06-08 14:02:37,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(10.201.4.49:9866, datanodeUuid=6c55b7cb-f8ef-445b-9cca-d82b5b077ed1, infoPort=9864, infoSecurePort=0, ipcPort=9867, storageInfo=lv=-57;cid=CID-37e80bd5-733a-4d7b-ba3d-b46269573c72;nsid=215490653;c=1584525570797) Starting thread to transfer BP-852924019-10.201.1.32-1584525570797:blk_-9223372036449848858_30963611 to 10.201.16.50:9866 # datatranfer done log 2021-06-08 13:54:08,134 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at bd-tz1-hadoop-004049.zeus.lianjia.com:9866: Transmitted BP-852924019-10.201.1.32-1584525570797:blk_-9223372036449848858_30963611 (numBytes=7457424) to /10.201.7.52:9866 2021-06-08 14:10:47,170 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DataTransfer, at bd-tz1-hadoop-004049.zeus.lianjia.com:9866: Transmitted BP-852924019-10.201.1.32-1584525570797:blk_-9223372036449848858_30963611 (numBytes=7457424) to /10.201.16.50:9866 ``` You will see last datatranfser thread was done on 13:54:08, but next datatranfser was start at 13:52:36. If datatranfser was not done in 10min(pending timeout + check interval), then next datatranfser for same block will be running. Then disk and network are heavy.  > Note: decommission ec block will trigger this problem easily, becuase every ec internal block are unique.","open","","zhengchenyu","2021-06-15T03:42:05Z","2021-06-16T16:34:13Z"
"","3094","[YARN-9246][Test][followup] Add test for YARN-9246","When complete YARN-9246 , I found there are no test for this patch. it's better to add the test to make sure interface stable.","open","","yikf","2021-06-10T13:00:45Z","2021-06-16T08:34:49Z"
"","3287","YARN-10873: Account for scheduled AM containers before deactivating node","When a node heartbeats to the RM, the StatusUpdateWhenHealthyTransition checks if rmNode.runningApplications is empty before deactivating the node (killing all containers on the node).  The data structure rmNode.runningApplications is updated in the same transition but after the call to RMNodeImpl.deactivateNode.  This can lead a race condition when a node is gracefully decommissioned immediately after launching a container on a node with 0 containers.   The heartbeat received from the node just after the container gets launched will update rmNode.runningApplications after RMNodeImpl.deactivateNode is called, causing the node to be deactivated and all scheduled containers to be killed.   If the container was an AM container, a retry is attempted without counting towards application failure. But for cases where max attempts is set to 1, the application is never retried (YARN-5617) and hence fails.  This PR checks the scheduler if any application's AM are scheduled on the node before deactivating it.","closed","","srinivasst","2021-08-10T12:56:29Z","2021-08-17T08:49:48Z"
"","3649","HDFS-16318. Add exception blockinfo","we may suffer `Could not obtain the last block location` exception, but we may reading more than one file, the following exception cannnot guide us to find the problem block or dn info.   `2021-11-12 14:01:59,633 WARN [main] org.apache.hadoop.hdfs.DFSClient: Last block locations not available. Datanodes might not have reported blocks completely. Will retry for 3 times` `2021-11-12 14:02:03,724 WARN [main] org.apache.hadoop.hdfs.DFSClient: Last block locations not available. Datanodes might not have reported blocks completely. Will retry for 2 times` `2021-11-12 14:02:07,726 WARN [main] org.apache.hadoop.hdfs.DFSClient: Last block locations not available. Datanodes might not have reported blocks completely. Will retry for 1 times`   `Caused by: java.lang.reflect.InvocationTargetException 	at sun.reflect.GeneratedConstructorAccessor19.newInstance(Unknown Source) 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) 	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251) 	... 11 more` `Caused by: java.io.IOException: Could not obtain the last block locations. 	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:291) 	at org.apache.hadoop.hdfs.DFSInputStream.(DFSInputStream.java:264) 	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1535) 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304) 	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299) 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) 	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:312) 	at org.apache.hadoop.fs.FilterFileSystem.open(FilterFileSystem.java:162) 	at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.open(ChRootedFileSystem.java:261) 	at org.apache.hadoop.fs.viewfs.ViewFileSystem.open(ViewFileSystem.java:463) 	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:768) 	at org.apache.hadoop.mapred.LineRecordReader.(LineRecordReader.java:109) 	at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67) 	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.(CombineHiveRecordReader.java:66) 	... 15 more`","open","","GuoPhilipse","2021-11-12T08:42:47Z","2021-12-07T10:16:40Z"
"","3336","HADOOP-17868. Add more tests for BuiltInGzipCompressor","We added BuiltInGzipCompressor recently. It is better to add more compatibility tests for the compressor.","closed","","viirya","2021-08-26T07:43:24Z","2021-09-22T16:18:50Z"
"","2884","HADOOP-17331. [JDK 16] TestDNS fails","Using reflection to update a static final field is not allowed. Removed the final modifier and use a setter method to update the field.  This fix is similar to #2309","closed","","aajisaka","2021-04-09T09:50:44Z","2021-06-30T10:09:31Z"
"","2865","HADOOP-17621. hadoop-auth to remove jetty-server dependency.","Use setStatus() API defined in J2EE's HttpServletResponse. Jetty's Response#setStatus() calls setStatusWithReason() so it's all the same for Jetty.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jojochuang","2021-04-05T01:05:35Z","2021-04-07T04:55:01Z"
"","3660","YARN-10982: Replace all occurences of queuePath with the new QueuePath class","Use QueuePath class where it's reasonable.","closed","","ghost","2021-11-15T10:28:58Z","2021-12-09T19:59:47Z"
"","3229","HDFS-16140. TestBootstrapAliasmap fails by BindException.","Use an available port instead of the default 50200 port in the test.  JIRA: HDFS-16140","closed","","aajisaka","2021-07-24T16:31:11Z","2021-07-26T07:46:40Z"
"","3057","HADOOP-17735. Upgrade AWS SDK to 1.11.1026","Upgrade the AWS SDK to the latest version.  Tested: S3 london ``` -Dparallel-tests -DtestsThreadCount=8 -Dmarkers=keep -Ds3guard -Ddynamo  -Dparallel-tests -DtestsThreadCount=8 -Dmarkers=delete -Dscale -Ds3guard -Ddynamo ```  some intermittent underful buffers on read(); the usual  Also  * dependency review: no new spurious artifacts * built: release; playing with that right now","closed","build,","steveloughran","2021-05-26T18:22:12Z","2021-10-15T19:42:52Z"
"","2947","HADOOP-17655. Upgrade Jetty to 9.4.40.","Upgrade Jetty to 9.4.40 to fix http put error with SSL enabled.","closed","","aajisaka","2021-04-23T08:04:18Z","2021-04-23T10:52:52Z"
"","2835","HADOOP-17601. Upgrade Jackson databind in branch-2.10 to 2.9.10.7","Upgrade Jackson databind in branch-2.10 from 2.9.10.6 to 2.9.10.7: https://issues.apache.org/jira/browse/HADOOP-17601  Two known vulnerabilities found in Jackson-databind:  [CVE-2021-20190](https://nvd.nist.gov/vuln/detail/CVE-2021-20190) high severity [CVE-2020-25649](https://nvd.nist.gov/vuln/detail/CVE-2020-25649) high severity","closed","","amahussein","2021-03-29T22:35:44Z","2021-04-13T00:42:48Z"
"","3152","HADOOP-17777. Update clover-maven-plugin version from 3.3.0 to 4.4.1","Update clover-maven-plugin version to 4.4.1, some important changes are as follows:  - the license key is no longer required to run Clover - the Atlassian brand and logos were removed to avoid any trademark violations - the 'org.openclover.*' groupId is used for artifacts  The license key is no longer required is the key factor to do this update. So the developer can run and get the code coverage easier.  https://issues.apache.org/jira/browse/HADOOP-17777","closed","","jiwq","2021-06-28T01:44:11Z","2021-06-30T01:32:17Z"
"","3243","HDFS-14529. SetTimes to throw FileNotFoundException if inode is not found","Throw FileNotFoundException instead of NPE if inode is not found.  As explained in the corresponding jira, there are two possibilities for this to happen. This PR does not resolve the culprit. I just merely make the exception more graceful.  If this problem is ever hit during edit log loading, NameNode can work around by applying the -recover option.","closed","","jojochuang","2021-07-28T02:37:01Z","2021-07-30T15:30:14Z"
"","2761","HADOOP-17576. ABFS: Disable throttling update for auth failures","Throttling metrics are updated post the execution of each request. Failures related to fetching access tokens and signing requests do not occur at the Store. Hence, such operations should not contribute to the measured Store failures, and are therefore excluded from the metric update for throttling.","closed","","sumangala-patki","2021-03-11T09:35:17Z","2021-04-09T04:01:23Z"
"","2699","HADOOP-17529. Upgrade os-maven-plugin to 1.7.0","This would enable support for RISC-V arch.  CC @trustin  https://issues.apache.org/jira/browse/HADOOP-17529","open","","advancedwebdeveloper","2021-02-14T22:31:21Z","2021-02-14T23:48:09Z"
"","3585","HADOOP-17928. Syncable: S3A to warn and downgrade","This switches the default behavior of S3A output streams to warning that Syncable.hsync() or hflush() have been called; it's not considered an error unless the defaults are overridden.  This avoids breaking applications which call the APIs, at the risk of people trying to use S3 as a safe store of streamed data (HBase WALs, audit logs etc).  Contributed by Steve Loughran.  Change-Id: Id7f8fc65a32a5ab664ebbd68d92754d4c38e29d8   ### How was this patch tested?  New unit tests; cloud store test in progress  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","steveloughran","2021-10-25T17:28:56Z","2021-11-02T13:26:17Z"
"","3055","Revert ""HADOOP-17563. Update Bouncy Castle to 1.68. (#2740)""","This reverts commit 077411675679900f94adba5329d0e33a4a528793.  JIRA: HADOOP-17563","closed","","tasanuma","2021-05-26T11:28:25Z","2021-05-27T04:14:26Z"
"","3046","HDFS-16037. Possible Resource Leak in org.apache.hadoop.hdfs.tools.offlineImageViewer#FSImageLoader","This PR fixes the issue mentioned [here](https://issues.apache.org/jira/browse/HDFS-16037).","open","","Nargeshdb","2021-05-24T18:23:07Z","2021-05-25T02:01:54Z"
"","3027","HDFS-16031. Possible Resource Leak in org.apache.hadoop.hdfs.server.aliasmap#InMemoryAliasMap","This PR fixes the issue mentioned [here](https://issues.apache.org/jira/browse/HDFS-16031).","closed","","Nargeshdb","2021-05-19T18:41:26Z","2021-05-26T05:43:53Z"
"","2809","HDFS-15914. Possible Resource Leak in org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap","This PR fixes the issue mentioned [here](https://issues.apache.org/jira/browse/HDFS-15914).","open","","Nargeshdb","2021-03-23T21:25:47Z","2021-05-20T00:47:31Z"
"","2790","HDFS-15908. Possible Resource Leak in org.apache.hadoop.hdfs.qjournal.server.Journal","This PR fixes the issue mentioned [here](https://issues.apache.org/jira/browse/HDFS-15908).","closed","","Nargeshdb","2021-03-19T18:46:04Z","2021-03-23T21:22:12Z"
"","2736","HDFS-15868. Possible Resource Leak in EditLogFileOutputStream","This PR fixes the issue mentioned [here](https://issues.apache.org/jira/browse/HDFS-15868).","closed","","Nargeshdb","2021-03-02T19:57:47Z","2021-03-19T17:56:56Z"
"","3048","HADOOP-17711. Fix divide by zero in LoadBalancingKMSClientProvider.","This pr fixes the issue [here](https://issues.apache.org/jira/browse/HADOOP-17711).","open","","yiyuaner","2021-05-25T07:39:52Z","2021-05-26T08:10:58Z"
"","3442","HADOOP-17915. ABFS AbfsDelegationTokenManager to generate canonicalServiceName if DT plugin doesn't","This PR ensures that  * `FileSystem.getCanonicalServiceName()` is *never* used to build an abfs service name from the IPaddr of the storage account's endpoint. * Instead, if DTs are enabled, the `AbfsDelegationTokenManager` gets it from the DT plugin if it implements `getCanonicalServiceName()` & returns a non-null value, else derives it from the FS URI. * And if DTs are disabled: returns null.  The fallback calculation of the Canonical Service Name is `""abfs://"" + fsURI.getHost() + ""/""`  1. schema is always abfs, even for abfss stores 1. the container is stripped from the service name. 1. so all abfs containers for the same service a/c will have the same Canonical Service Name 1. share a single DT in job submission 1. *and*: if a DT is issued for one of the containers in job submission, all other containers for the same storage a/c will use that DT 1. Even if the caller didn't explicitly name it.  That is consistent with using the storage a/c's endpoint IPAddr to identify the storage account.  Today, `abfs://c1@storage1.dfs.core.windows.net/` and  `abfs://container2@storage1.dfs.core.windows.net/` will both have their CSN map to the same endpoint hostname *if the DT plugin doesn't return a schema*  If the DT plugin returns their own canonical name -which it should, this is all moot. This is a fallback if they don't.  Tests updated to match new behavior.  ### How was this patch tested?  azure cardiff  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","open","fs/azure,","steveloughran","2021-09-15T20:57:32Z","2021-10-07T05:22:27Z"
"","3215","YARN-10861. Make NodeHealthCheckerService pluggable in NodeManager","This PR aims to support NodeHealthCheckerService as pluggable interface,  The custom NodeHealthCheckerService can be used by setting `yarn.nodemanager.health-checker-service.class` in the configuration. If no class is set it falls back to the default class i.e `NodeHealthCheckerServiceImpl`  Linked Jira: [YARN-10861](https://issues.apache.org/jira/browse/YARN-10861)","open","","cyrus-jackson","2021-07-19T22:29:44Z","2021-10-07T19:12:23Z"
"","2889","HDFS-15963. Unreleased volume references cause an infinite loop.","This patch releases the three previously unreleased volume references.","closed","","zhangshuyan0","2021-04-10T15:48:00Z","2021-04-16T16:11:02Z"
"","3576","HDFS-16282. Remove duplicate generic usage information to hdfs debug command","This patch is tested by hdfs debug command: BEFORE: ``` ~ $ hdfs debug Usage: hdfs debug  [arguments]  These commands are for advanced users only.  Incorrect usages may result in data loss. Use at your own risk.  verifyMeta -meta  [-block ]  Generic options supported are: -conf         specify an application configuration file -D                define a value for a given property -fs  specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations. -jt   specify a ResourceManager -files                 specify a comma-separated list of files to be copied to the map reduce cluster -libjars                specify a comma-separated list of jar files to be included in the classpath -archives           specify a comma-separated list of archives to be unarchived on the compute machines  The general command line syntax is: command [genericOptions] [commandOptions]  computeMeta -block  -out   Generic options supported are: -conf         specify an application configuration file -D                define a value for a given property -fs  specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations. -jt   specify a ResourceManager -files                 specify a comma-separated list of files to be copied to the map reduce cluster -libjars                specify a comma-separated list of jar files to be included in the classpath -archives           specify a comma-separated list of archives to be unarchived on the compute machines  The general command line syntax is: command [genericOptions] [commandOptions]  recoverLease -path  [-retries ]  Generic options supported are: -conf         specify an application configuration file -D                define a value for a given property -fs  specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations. -jt   specify a ResourceManager -files                 specify a comma-separated list of files to be copied to the map reduce cluster -libjars                specify a comma-separated list of jar files to be included in the classpath -archives           specify a comma-separated list of archives to be unarchived on the compute machines  The general command line syntax is: command [genericOptions] [commandOptions]   Generic options supported are: -conf         specify an application configuration file -D                define a value for a given property -fs  specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations. -jt   specify a ResourceManager -files                 specify a comma-separated list of files to be copied to the map reduce cluster -libjars                specify a comma-separated list of jar files to be included in the classpath -archives           specify a comma-separated list of archives to be unarchived on the compute machines  The general command line syntax is: command [genericOptions] [commandOptions] ```  AFTER: ``` ~ $ hdfs debug Usage: hdfs debug  [arguments]  These commands are for advanced users only.  Incorrect usages may result in data loss. Use at your own risk.  verifyMeta -meta  [-block ] computeMeta -block  -out  recoverLease -path  [-retries ] verifyEC -file   Generic options supported are: -conf         specify an application configuration file -D                define a value for a given property -fs  specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations. -jt   specify a ResourceManager -files                 specify a comma-separated list of files to be copied to the map reduce cluster -libjars                specify a comma-separated list of jar files to be included in the classpath -archives           specify a comma-separated list of archives to be unarchived on the compute machines  The general command line syntax is: command [genericOptions] [commandOptions] ```","closed","","cndaimin","2021-10-22T03:46:21Z","2021-10-22T15:45:25Z"
"","3593","HDFS-16286. Add a debug tool to verify the correctness of erasure coding on file","This patch is tested by `hdfs debug` tool.  Check a good file: ``` $ hdfs debug verifyEC -file /dfsperf.0.0 Checking EC block group: blk_-9223372036854774784 Status: OK Checking EC block group: blk_-9223372036854774768 Status: OK Checking EC block group: blk_-9223372036854774752 Status: OK  All EC block group status: OK ```  Check a bad file: ``` $ hdfs debug verifyEC -file /sc.dat Checking EC block group: blk_-9223372036854774736 Status: ERROR, message: EC compute result not match. ```  Help message: ``` $ hdfs debug verifyEC verifyEC -file    Verify HDFS erasure coding on all block groups of the file. ```","closed","","cndaimin","2021-10-27T06:47:07Z","2021-11-03T20:19:56Z"
"","3364","HDFS-16200: Improve NameNode failover","This patch adds a configuration to skip resolving the topology for the client hosts, e.g., YARN hosts. Such topology info is useful in colocated environment but not in non-colocated env.    ### How was this patch tested? unit test","open","","aihuaxu","2021-09-01T00:16:04Z","2021-09-08T00:52:19Z"
"","2778","HADOOP-13551. AWS metrics wire-up","This moves to a parameter-class and builder pattern for passing in configuration parameters to the S3 client factory, so as to stop adding any future parameters from breaking external implementations.  Extracted from HADOOP-17511 for isolated review/commit","closed","","steveloughran","2021-03-15T18:26:10Z","2021-03-24T13:32:55Z"
"","3507","HADOOP-17871. S3A CSE: minor tuning (#3412)","This migrates the fs.s3a-server-side encryption configuration options   to a name which covers client-side encryption too.    fs.s3a.server-side-encryption-algorithm becomes fs.s3a.encryption.algorithm   fs.s3a.server-side-encryption.key becomes fs.s3a.encryption.key    The existing keys remain valid, simply deprecated and remapped   to the new values. If you want server-side encryption options   to be picked up regardless of hadoop versions, use   the old keys.    (the old key also works for CSE, though as no version of Hadoop   with CSE support has shipped without this remapping, it's less   relevant)     Contributed by: Mehakmeet Singh","closed","","mehakmeet","2021-10-01T08:22:35Z","2021-10-05T11:17:14Z"
"","2864","HADOOP-17614. Bump netty to the latest 4.1.61.","This is the branch-3.2 backport. There is a slight conflict so just in case here's the PR","closed","","jojochuang","2021-04-05T00:53:50Z","2021-04-06T05:44:48Z"
"","3520","HADOOP-17424. Replace HTrace with No-Op tracer","this is backport to branch-3.3 of [HADOOP-17424](https://issues.apache.org/jira/browse/HADOOP-17424).","closed","","iwasakims","2021-10-05T09:47:30Z","2021-10-11T15:07:09Z"
"","2849","HDFS-15621. Datanode DirectoryScanner uses excessive memory","This is a relatively simple change to reduce the memory used by the Directory Scanner and also simplify the logic in the ScanInfo object.  This change ensures the same File object is re-used for all blocks in a directory. Previously a large part of the path was repeated for each block file.  Aside from that, the logic of the directory scanner remains the same.  Comparing heap dumps, the memory used by 100K blocks goes from ~35MB to 19MB. Or 350MB per 1M blocks down to 190MB per 1M blocks. This is a reduction of about 46%.","closed","","sodonnel","2021-04-01T15:40:04Z","2021-04-26T10:00:23Z"
"","3441","HADOOP-17891. Fix compilation error under skipShade (ADDENDUM)","This is a follow up for #3385 to fix the compilation error under skipShade.","closed","","viirya","2021-09-15T20:53:55Z","2021-09-16T17:13:10Z"
"","3251","HADOOP-17812. NPE in S3AInputStream read() after failure to reconnect to store","This improves error handling after multiple failures reading data -when the read fails and attempts to reconnect() also fail.","closed","","wbo4958","2021-07-31T21:26:55Z","2021-08-03T11:56:50Z"
"","3309","HADOOP-17853. ABFS: Enable optional store connectivity over azure specific protocol for data egress","This commit provides an option to enable store access on read path over an Azure specific protocol. To enable the feature, config ""fs.azure.fastpath.enable""needs to be turned on. This will only work on Azure VMs and hence will be disabled by default.  With feature on, read requests will switch from HttpConnection to FastpathConnection. This will route the requests to APIs exposed by a new dependent artifact azure-storage-fastpath.  Feature flow: ABFS InputStream needs to cache a session token that it fetches from store backend, renew it close to its expiry and use it as an input to the API calls of artifact azure-storage-fastpath. On init, the inputStream needs to call upon the Open API to fetch file handle and use for the successive read calls. On inputStream close, the file handle created earlier needs to be closed too.   To unit test and inject failures, mock child classes of AzureBlobFileSystemStore, AbfsClient, AbfsRestOperation, AbfsInputStream are created in tests which mimic the instances/field values to test the feature even though the config is off. Existing test methods that do valid read tests are triggered seperately over mockFastpath mode to isolate test results from the existing tests.   The dependent artifact azure-storage-fastpath is a published artifact to public maven repository.","closed","","snvijaya","2021-08-17T16:45:57Z","2021-08-24T10:55:04Z"
"","3047","MAPREDUCE-7346. Fix divide by zero in AvgRecordFactory","This commit fixes the issue [here](https://issues.apache.org/jira/browse/MAPREDUCE-7346).","open","","yiyuaner","2021-05-25T07:12:31Z","2021-05-25T10:50:35Z"
"","3381","HADOOP-17890. ABFS: Http request handling code refactoring","This commit aims to refactor the Http request handling code.  ABFS driver tests were run with HNS and non-HNS storage accounts over combinations of authentication types - OAuth and SharedKey. Tests results will be updated in conversation tab with each PR iteration.","open","","snvijaya","2021-09-03T13:54:29Z","2022-05-02T23:49:43Z"
"","3344","HADOOP-17872. ABFS: Refactor read flow to include ReadRequestParameter","This commit aims to refactor the code that passes various parameters needed for read flow while the store requests are created. It adds a class ReadRequestParameter that will hold the inputs needed for the read request.  ABFS driver tests were run with HNS and non-HNS storage accounts over combinations of authentication types - OAuth and SharedKey. Tests results will be updated in conversation tab with each PR iteration.","open","","snvijaya","2021-08-27T05:27:03Z","2022-05-12T01:34:53Z"
"","3335","HADOOP-17864. ABFS: Make provision for adding additional connections type","This commit aims to facilitate upcoming work as part of adding an alternate connection to store backend - [HADOOP-17853](https://issues.apache.org/jira/browse/HADOOP-17853)  The scope of the change is to make AbfsHttpOperation an abstract class and create a child class AbfsHttpConnection. Future connection types will be added as child of AbfsHttpOperation. Retaining the abstract class name to reduce any backport pain.  ABFS driver tests were run with HNS and non-HNS storage accounts over combinations of authentication types - OAuth and SharedKey. Tests results will be updated in conversation tab with each PR iteration.","open","","snvijaya","2021-08-26T06:44:53Z","2022-07-29T10:38:59Z"
"","3067","HADOOP-17742. fix distcp fail when copying to ftp filesystem","There is more Detailed information. https://issues.apache.org/jira/browse/HADOOP-17742#","closed","","zhaomin1423","2021-06-02T12:23:42Z","2021-06-03T12:20:04Z"
"","3693","HDFS-16342. Improve code in KMS","There are duplicated code in KMS, we can do a little improve","closed","","GuoPhilipse","2021-11-20T16:05:01Z","2021-11-21T10:04:31Z"
"","2968","MAPREDUCE-7342. Stop RMService in TestClientRedirect.testRedirect()","The test `org.apache.hadoop.mapred.TestClientRedirect.testRedirect` is not idempotent and fail if run twice in the same JVM, because it pollutes some states shared among tests. It may be good to clean this state pollution so that some other tests do not fail in the future due to the shared state polluted by this test.  ### Detail Running `TestClientRedirect.testRedirect` twice would result in the second run failing due to the following assertion error: ``` INFO  [main] service.AbstractService (AbstractService.java:noteFailure(267)) - Service test failed in state STARTED org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.BindException:   Problem binding to [0.0.0.0:8054] java.net.BindException: Address already in use ``` The root cause is that the RM server listening on port 8054 is started in the first run of this test, but hasn't been stopped when the test finishes. In the second run, when the test is trying to start the RMService, it fails because port 8054 is already in use, leading to the exception.  The suggested fix is to stop the RM server in the added overridden method `RMService.serviceStop()`.  With the proposed fix, the test does not pollute the shared state (and passes when run twice in the same JVM).  JIRA link: [MAPREDUCE-7342](https://issues.apache.org/jira/browse/MAPREDUCE-7342)","closed","","lzx404243","2021-05-01T08:30:37Z","2021-08-29T23:40:37Z"
"","2724","HDFS-15862. Make TestViewfsWithNfs3.testNfsRenameSingleNN() idempotent","The test `org.apache.hadoop.hdfs.nfs.nfs3.TestViewfsWithNfs3.testNfsRenameSingleNN` is not idempotent and fails if run twice in the same JVM, because it pollutes state shared among tests. It may be good to clean this state pollution so that some other tests do not fail in the future due to the shared state polluted by this test.  ### Details  Running `TestViewfsWithNfs3.testNfsRenameSingleNN` twice would result in the second run failing with the a NullPointer exception: ``` [ERROR] Errors: [ERROR] TestViewfsWithNfs3.testNfsRenameSingleNN:317 NullPointer ```  The reason for this is that the `/user1/renameSingleNN` file is created in `setup()`, but gets renamed in`testNfsRenameSingleNN`. When the second run of `testNfsRenameSingleNN` tries to get info of the file by its original name, it returns a NullPointer since the file no longer exists.  The fix for this is to rename the file back to the original when the test is done.  With the proposed fix, the test does not pollute the shared state (and passes when run twice in the same JVM).  Link to JIRA issue: https://issues.apache.org/jira/browse/HDFS-15862","closed","","lzx404243","2021-02-26T06:54:33Z","2021-12-08T11:44:32Z"
"","2726","HDFS-13975. TestBalancer#testMaxIterationTime fails sporadically","The PR solves the flakiness of the unit test as follows. 1. Set dfs.datanode.balance.bandwidthPerSec to 4MB so that an iteration cannot move any block in 2 seconds. 2. Set maxIdleIteratons to 1 for the iteration to report NO_MOVE_PROGERSS when there is not block move. 3. Assert that the reported status is NO_MOVE_PROGERSS, since a 10MB block cannot be moved with 4MB/s bandwidth in 2 seconds (i.e, 4MB/s * 2s = 8MB < 10MB).","closed","","touchida","2021-02-27T20:10:21Z","2021-03-17T03:00:06Z"
"","3508","HADOOP-17922. move to fs.s3a.encryption.algorithm - JCEKS integration (#3466)","The ordering of the resolution of new and deprecated s3a encryption options & secrets is the same when JCEKS and other hadoop credentials stores are used to store them as when they are in XML files: per-bucket settings always take priority over global values, even when the bucket-level options use the old option names.  Contributed by Mehakmeet Singh and Steve Loughran","closed","","mehakmeet","2021-10-01T08:24:38Z","2021-10-05T11:17:56Z"
"","3026","HADOOP-17590 ABFS: Introduce Lease Operations with Append to provide single writer semantics","The lease operations have been introduced as part of Create, Append, Flush to ensure the single writer semantics.  Testing Done: 1. Introduced new set of tests in the ITestAzureBlobFileSystemBundleLease.java  2. Existing tests were run: Region: Canary EastUs2euap a. HNS account + OAuth config b. HNS account + Shared Key config c. Non-HNS account + SharedKey config d. AppendBlob+HNS+Oauth config  Failures seen - testReadAndWriteWithDifferentBufferSizesAndSeek, ITestAbfsFileSystemContractDistCp, ITestAbfsFileSystemContractSecureDistCp, TestAbfsStreamOps with appendblob, testBlobBackCompatibility, testRandomRead & WasbAbfsCompatibility with non-HNS account","open","","snehavarma","2021-05-19T10:25:56Z","2021-06-11T11:46:40Z"
"","2956","HADOOP-17661. mvn versions:set fails to parse pom.xml.","The last PR caused compilation error.  Trying again.","closed","","jojochuang","2021-04-26T09:07:46Z","2021-04-27T01:42:43Z"
"","3235","HDFS-16143. Add Timer in EditLogTailer and de-flake TestEditLogTailer#testStandbyTriggersLogRollsWhenTailInProgressEdits","The intention here is to de-flake test TestEditLogTailer#testStandbyTriggersLogRollsWhenTailInProgressEdits.  The purpose of this test is to ensure that EditLogTailer is able to perform log roll within a specific period of time. Test sets `dfs.ha.tail-edits.period` as 1 sec and `dfs.ha.log-roll.period` as 5 sec. Hence, the thread responsible for checking whether we should tail edit journals sleeps for 1 sec in an never-ending loop and it can trigger active log roll if `timer.monotonicNow() - lastRollTimeMs` is greater than 5 sec. Unless we can provide custom Timer to tweak `monotonicNow()`, it is almost impossible to guarantee that above condition will be true. So this test has been flaky.  In order to de-flake it, we have introduced `Timer` in `EditLogTailer` to replace all instances of `Time.monotonicNow()` to `timer.monotonicNow()` where timer is `Timer` instance. This part of the code doesn't change any behaviour in source code of `EditLogTailer`. On the other hand, making this change allows this test to provide it's own custom timer instance and properly deal with above condition (`(timer.monotonicNow() - lastRollTimeMs) > logRollPeriodMs`) such that it will be false when we set `inSufficientTimeForLogRoll` in `FakeTimer` and the condition will be true when we advance the timer with `sufficientTimeForLogRoll` and hence it will trigger the log rolling and we can assert the same.","closed","","virajjasani","2021-07-26T10:59:51Z","2021-08-26T10:42:23Z"
"","3372","HADOOP-17888. The error of Constant annotation in AzureNativeFileSystem…","The error annotation","closed","","guoxin12","2021-09-02T07:48:29Z","2021-10-18T10:07:32Z"
"","2795","HADOOP-17596. ABFS: Change default Readahead Queue Depth from num(processors) to const","The default value of readahead queue depth is currently set to the number of available processors. However, this can result in one inputstream instance consuming more processor time. To ensure equal thread allocation during read for all inputstreams created in a session, we change the default readahead queue depth to a constant (2).","closed","","sumangala-patki","2021-03-22T12:04:09Z","2021-06-09T05:35:52Z"
"","2707","HADOOP-17536. ABFS: Supporting customer provided encryption key","The data for a particular customer needs to be encrypted on account level. At server side the APIs will start accepting the encryption key as part of request headers. The data will be encrypted/decrypted with the given key at the server.   Since the ABFS FileSystem APIs are implementations for Hadoop FileSystem APIs there is no direct way with which customer can pass the key to ABFS driver. In this case driver should have the following capabilities so that it can accept and pass the encryption key as one of the request headers.   There should be a way to configure the encryption key for different accounts. If there is a key specified for a particular account, the same needs to be sent along with the request headers.  Config changes   They key for an account can be specified in the core-site as follows.   fs.azure.account.client-provided-encryption-key.{account name}.dfs.core.windows.net","closed","","bilaharith","2021-02-19T05:02:22Z","2021-04-27T10:16:11Z"
"","2867","HADOOP-17624. Remove any rocksdb exclusion code.","The code compiles locally. No additional tests added.","closed","","jojochuang","2021-04-06T03:17:43Z","2021-04-07T04:52:33Z"
"","3068","YARN-10803. [JDK 11] TestRMFailoverProxyProvider and TestNoHaRMFailoverProxyProvider fails by ClassCastException","The class definitions are wrong in some variables. Fixed.  JIRA: YARN-10803","closed","","aajisaka","2021-06-03T07:14:13Z","2021-06-10T05:30:22Z"
"","2780","HADOOP-17592. Fix the wrong CIDR range example in Proxy User documentation","The CIDR range example on the Proxy user description page is wrong.  In the Configurations section of Proxy user page, CIDR format 10.222.0.0/16 means 10.222.0.0-15.  But It's not true. the CIDR format 10.222.0.0/16 means 10.222.0.0-10.222.255.255.","closed","","nohkwangsun","2021-03-17T09:23:14Z","2021-03-22T02:44:14Z"
"","3141","HDFS-16087. Fix stuck issue in rbfbalance tool.","The balance process will be stuck at DisableWrite stage when running the rbfbalance command. This patch can solve this problem. Please review, thanks.","closed","","lipppppp","2021-06-25T05:58:42Z","2021-07-20T16:03:34Z"
"","3076","HADOOP-17745. Wrap IOException with InterruptedException cause properly","The Azure client sometimes throws an IOException with an InterruptedException cause which can be converted to an InterruptedIOException. This is important for downstream consumers that rely on an InterruptedIOException to gracefully close.","open","","eric-maynard","2021-06-03T18:57:25Z","2022-01-12T17:20:51Z"
"","2950","HADOOP-17471. ABFS to collect IOStatistics (#2731)","The ABFS Filesystem and its input and output streams now implement the IOStatisticSource interface and provide IOStatistics on their interactions with Azure Storage.  This includes the min/max/mean durations of all REST API calls.  Contributed by Mehakmeet Singh","closed","","mehakmeet","2021-04-24T03:08:03Z","2021-04-24T16:59:26Z"
"","3332","HADOOP-17863. ABFS: Fix compiler deprecation warning in TextFileBasedIdentityHandler","TextFileBasedIdentityHandler uses an instance of LineIterator whose closeQuietly method has been deprecated, resulting in compiler warnings during yetus runs. This PR fixes it by leveraging a try-catch block for close() to avoid the explicit call to closeQuietly by the LineIterator instance.  Testing: Verified change with existing test class TestTextFileBasedIdentityHandler; no new test required.","closed","","sumangala-patki","2021-08-25T11:48:39Z","2021-11-05T12:53:51Z"
"","2706","HADOOP-13887. Support S3 client side encryption (S3-CSE) using AWS-SDK","Tests: mvn -T 32 clean verify -Ddynamo -Dauth -Dscale -Dparallel-tests Region: ap-south-1  ``` [INFO] Results: [INFO] [WARNING] Tests run: 535, Failures: 0, Errors: 0, Skipped: 5 ```  ``` [INFO] Results: [INFO] [ERROR] Failures: [ERROR]   ITestS3AEncryptionCSEAsymmetric>ITestS3AEncryptionCSE.testEncryption:48->ITestS3AEncryptionCSE.validateEncryptionForFilesize:82->AbstractS3ATestBase.writeThenReadFile:196->AbstractS3ATestBase.writeThenReadFile:209->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0001/test/testEncryption0 status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0001/test/testEncryption0; isDirectory=false; length=16; replication=1; blocksize=33554432; modification_time=1613656460000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=91f7a9a65b91c2332c572ea76bdbe822 versionId=null expected:<0> but was:<16> [ERROR]   ITestS3AEncryptionCSEAsymmetric>ITestS3AEncryptionCSE.testEncryptionOverRename:63->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0001/test/testEncryptionOverRename status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0001/test/testEncryptionOverRename; isDirectory=false; length=1040; replication=1; blocksize=33554432; modification_time=1613656458000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=97712202251ecc080bbbf909c9000718 versionId=null expected:<1024> but was:<1040> [ERROR]   ITestS3AEncryptionCSEKms>ITestS3AEncryptionCSE.testEncryption:48->ITestS3AEncryptionCSE.validateEncryptionForFilesize:82->AbstractS3ATestBase.writeThenReadFile:196->AbstractS3ATestBase.writeThenReadFile:209->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0001/test/testEncryption0 status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0001/test/testEncryption0; isDirectory=false; length=16; replication=1; blocksize=33554432; modification_time=1613656468000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=74cb30cc19ee58be3bcd462120c16eb7 versionId=null expected:<0> but was:<16> [ERROR]   ITestS3AEncryptionCSEKms>ITestS3AEncryptionCSE.testEncryptionOverRename:63->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0001/test/testEncryptionOverRename status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0001/test/testEncryptionOverRename; isDirectory=false; length=1040; replication=1; blocksize=33554432; modification_time=1613656465000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d03375638396ae14794cde1a7e5fd4f8 versionId=null expected:<1024> but was:<1040> [ERROR]   ITestS3AEncryptionCSESymmetric>ITestS3AEncryptionCSE.testEncryption:48->ITestS3AEncryptionCSE.validateEncryptionForFilesize:82->AbstractS3ATestBase.writeThenReadFile:196->AbstractS3ATestBase.writeThenReadFile:209->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0002/test/testEncryption0 status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0002/test/testEncryption0; isDirectory=false; length=16; replication=1; blocksize=33554432; modification_time=1613656453000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ff01163e592622879cd0cdb7005618ca versionId=null expected:<0> but was:<16> [ERROR]   ITestS3AEncryptionCSESymmetric>ITestS3AEncryptionCSE.testEncryptionOverRename:63->Assert.assertEquals:645->Assert.failNotEquals:834->Assert.fail:88 Wrong file length of file s3a://mehakmeet-singh-data/fork-0002/test/testEncryptionOverRename status: S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0002/test/testEncryptionOverRename; isDirectory=false; length=1040; replication=1; blocksize=33554432; modification_time=1613656451000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1802986c25982cdacb58aff624626eeb versionId=null expected:<1024> but was:<1040> ``` When we do fs.getFileStatus().getLen() to get the content length of the encrypted file, it is not the same as the original length and hence, the tests are breaking.  ```[INFO] [ERROR] Tests run: 1427, Failures: 7, Errors: 21, Skipped: 458 ```  Failures other than S3-CSE tests are config related    ```  [ERROR] Errors: [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO] [ERROR] Tests run: 151, Failures: 0, Errors: 1, Skipped: 28 ```  When I remove the checks for file content Lengths from the tests, the tests run successfully, hence, the Key-wrap Algo and Content Encryption Algo are used as intended successfully.   One possible way we explored to tackle the padding issue was to tweak the s3GetFileStatus call to return a FileStatus with ""UNENCRYPTED_CONTENT_LENGTH"" header which comes in the user metadata, but this would still break where we don't get these headers to do our tasks. Hence, consistency of content Length needs to be maintained.  P.S: Need more tests to validate that even with tweaking, we are breaking some tests.  CC: @steveloughran","closed","","mehakmeet","2021-02-18T14:35:28Z","2021-08-21T20:34:20Z"
"","2963","HADOOP-17670. S3AFS and ABFS to log IOStats at DEBUG mode or optionally at INFO level in close()","Tested on S3AFS and ABFS.  Region for S3AFS: ap-south-1 Region for ABFS: East US  Don't know if we need any tests for these?  S3A(trunk also) showed these errors I haven't seen before:  ``` [ERROR] testListEmptyRootDirectory(org.apache.hadoop.fs.contract.s3a.ITestS3AContractRootDir)  Time elapsed: 16.79 s  <<< FAILURE! java.lang.AssertionError: Deleted file: unexpectedly found s3a://mehakmeet-singh-data/Users as  S3AFileStatus{path=s3a://mehakmeet-singh-data/Users; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null 	at org.junit.Assert.fail(Assert.java:89) 	at org.apache.hadoop.fs.contract.ContractTestUtils.assertPathDoesNotExist(ContractTestUtils.java:1004) 	at org.apache.hadoop.fs.contract.ContractTestUtils.assertDeleted(ContractTestUtils.java:733) 	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest.testListEmptyRootDirectory(AbstractContractRootDirectoryTest.java:196) 	at org.apache.hadoop.fs.contract.s3a.ITestS3AContractRootDir.testListEmptyRootDirectory(ITestS3AContractRootDir.java:82) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) 	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```  ``` [ERROR] testRmEmptyRootDirNonRecursive(org.apache.hadoop.fs.contract.s3a.ITestS3AContractRootDir)  Time elapsed: 30.584 s  <<< FAILURE! java.lang.AssertionError: After 12 attempts: listing after rm /* not empty final [00] S3AFileStatus{path=s3a://mehakmeet-singh-data/Users; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [01] S3AFileStatus{path=s3a://mehakmeet-singh-data/user; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null  deleted [00] S3AFileStatus{path=s3a://mehakmeet-singh-data/Users; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [01] S3AFileStatus{path=s3a://mehakmeet-singh-data/user; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null  original [00] S3AFileStatus{path=s3a://mehakmeet-singh-data/file.txt; isDirectory=false; length=0; replication=1; blocksize=33554432; modification_time=1619544154000; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d41d8cd98f00b204e9800998ecf8427e versionId=null [01] S3AFileStatus{path=s3a://mehakmeet-singh-data/Users; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [02] S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0001; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [03] S3AFileStatus{path=s3a://mehakmeet-singh-data/fork-0002; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [04] S3AFileStatus{path=s3a://mehakmeet-singh-data/path; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [05] S3AFileStatus{path=s3a://mehakmeet-singh-data/test; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [06] S3AFileStatus{path=s3a://mehakmeet-singh-data/tests3ascale; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [07] S3AFileStatus{path=s3a://mehakmeet-singh-data/user; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null  	at org.junit.Assert.fail(Assert.java:89) 	at org.apache.hadoop.fs.contract.AbstractContractRootDirectoryTest$1.call(AbstractContractRootDirectoryTest.java:109) ```","closed","","mehakmeet","2021-04-27T17:54:16Z","2021-05-24T12:02:11Z"
"","3020","HADOOP-17705. S3A to add Config to set AWS region","Tested by: mvn clean verify -Dparallel-tests -Dscale Region: ap-south-1  Test: ``` [INFO] Results: [INFO] [WARNING] Tests run: 537, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO] [WARNING] Tests run: 1430, Failures: 0, Errors: 0, Skipped: 462 ``` ``` [INFO] [ERROR] Tests run: 151, Failures: 2, Errors: 1, Skipped: 28 ``` Timeout and intermittent failures.   CC: @steveloughran @mukund-thakur","closed","","mehakmeet","2021-05-17T15:12:34Z","2021-05-24T12:12:54Z"
"","2731","HADOOP-17471. ABFS to collect IOStatistics","Tested by: mvn -T 16 -Dparallel-tests=abfs clean verify Region: East US  ``` [INFO] Results: [INFO] [INFO] Tests run: 95, Failures: 0, Errors: 0, Skipped: 0 ```  ``` [INFO] Results: [INFO] [ERROR] Errors: [ERROR]   ITestAzureBlobFileSystemCheckAccess.testCheckAccessForNonExistentFile »  Unexp... [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionALL:293->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionEXECUTE:218->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionNONE:204->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionREAD:233->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionREADEXECUTE:263->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionWRITE:248->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [ERROR]   ITestAzureBlobFileSystemCheckAccess.testFsActionWRITEEXECUTE:278->checkPrerequisites:306->setTestUserFs:88->checkIfConfigIsSet:319 » IllegalArgument [INFO] [ERROR] Tests run: 512, Failures: 0, Errors: 8, Skipped: 67 ``` Illegal argument due to not using oauth config for these tests. ``` [INFO] Results: [INFO] [WARNING] Tests run: 256, Failures: 0, Errors: 0, Skipped: 77 ```","closed","","mehakmeet","2021-03-01T14:13:04Z","2021-04-23T09:47:54Z"
"","3425","HDFS-16224. testBalancerWithObserverWithFailedNode times out","testBalancerWithObserverWithFailedNode fails intermittently.  Seems it is because of datanode cannot shutdown because we need to wait for datanodes to finish retries to failed observer.","closed","","LeonGao91","2021-09-13T06:39:37Z","2021-09-17T06:59:36Z"
"","3201","HDFS-16128: Added support for saving/loading an FS Image for fine-grain locking","Test: We create 1 million dirs and then stop and re-start the namenode. Namenode restarts successfully.   ``` xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> ./bin/hadoop org.apache.hadoop.hdfs.server.namenode.NNThroughputBenchmark -fs hdfs://localhost:9000 -op mkdirs -threads 50 -dirs 1000000 -dirsPerDir 100 -keepResults 2021-07-13 14:59:39,703 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 2021-07-13 14:59:40,264 INFO namenode.NNThroughputBenchmark: Starting benchmark: mkdirs 2021-07-13 14:59:40,412 INFO namenode.NNThroughputBenchmark: Generate 1000000 inputs for mkdirs 2021-07-13 14:59:40,669 INFO namenode.NNThroughputBenchmark: Log level = ERROR 2021-07-13 14:59:40,894 INFO namenode.NNThroughputBenchmark: Starting 1000000 mkdirs(s). 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: --- mkdirs inputs --- 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: nrDirs = 1000000 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: nrThreads = 50 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: nrDirsPerDir = 100 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: --- mkdirs stats  --- 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: # operations: 1000000 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: Elapsed Time: 33637 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark:  Ops per sec: 29729.16728602432 2021-07-13 15:00:15,524 INFO namenode.NNThroughputBenchmark: Average Time: 1 xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> ./bin/hdfs dfs -ls -R / | wc 2021-07-13 15:01:23,813 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  1010002 8080016 156814967 xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> ./sbin/stop-dfs.sh Stopping namenodes on [localhost] Stopping datanodes Stopping secondary namenodes [localhost] 2021-07-13 15:09:47,518 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> ./sbin/start-dfs.sh Starting namenodes on [localhost] Starting datanodes Starting secondary namenodes [localhost] 2021-07-13 15:10:05,055 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> jps 61745 DataNode 61635 NameNode 61878 SecondaryNameNode 95382 CmdLineAppRunner 65082 62030 Jps xinglin@xinglin-mn1 ~/p/h/h/t/hadoop-3.4.0-SNAPSHOT (fgl-saveloadfs)> ./bin/hdfs dfs -ls -R / | wc 2021-07-13 15:10:20,998 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  1010002 8080016 156814967  ```  Log output from the namenode. ``` xinglin@xinglin-mn1 ~/p/h/h/t/h/logs (fgl-saveloadfs)> grep LOADING hadoop-xinglin-namenode-xinglin-mn1.linkedin.biz.log INFO org.apache.hadoop.hdfs.server.namenode.FSImage: LOADING_FSIMAGE: loaded 1 inodes into inodeMap INFO org.apache.hadoop.hdfs.server.namenode.FSImage: LOADING_EDITS: loaded 1 inodes into inodeMap INFO org.apache.hadoop.hdfs.server.namenode.FSImage: LOADING_FSIMAGE: loaded 1010003 inodes into inodeMap INFO org.apache.hadoop.hdfs.server.namenode.FSImage: LOADING_EDITS: loaded 1010003 inodes into inodeMap ```","closed","","xinglin","2021-07-13T22:28:22Z","2021-07-31T21:02:04Z"
"","2842","HADOOP-17615. ADLS Gen1: Update adls SDK to 2.3.9","Test results are observed to be the same before and after the change.  [ERROR] Errors:  [ERROR]   TestAdlContractGetFileStatusLive>AbstractContractGetFileStatusTest.testComplexDirActions:153->AbstractContractGetFileStatusTest.checkListStatusIteratorComplexDir:196 » NoClassDefFound [ERROR]   TestAdlContractGetFileStatusLive>AbstractContractGetFileStatusTest.testListStatusIteratorFile:363->AbstractContractGetFileStatusTest.validateListingForFile:384 » NoClassDefFound [ERROR]   TestAdlContractRootDirLive>AbstractContractRootDirectoryTest.testSimpleRootListing:251 » NoClassDefFound [INFO]  [ERROR] Tests run: 886, Failures: 0, Errors: 3, Skipped: 3","closed","","bilaharith","2021-03-31T05:01:09Z","2021-05-13T06:34:17Z"
"","3144","HADOOP-17774. bytesRead FS statistic showing twice the correct value in S3A","Test command: ```mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale``` Region: ap-south-1  ``` INFO] Results: [INFO]  [WARNING] Tests run: 568, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1460, Failures: 0, Errors: 2, Skipped: 462 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 2, Errors: 1, Skipped: 28 ```  Seeing these errors:  ``` [ERROR] Failures:  [ERROR]   ITestS3AContractRootDir.testListEmptyRootDirectory:82->AbstractContractRootDirectoryTest.testListEmptyRootDirectory:196->Assert.fail:89 Deleted file: unexpectedly found s3a://mehakmeet-singh-data/user as  S3AFileStatus{path=s3a://mehakmeet-singh-data/user; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive:101->Assert.fail:89 After 20 attempts: listing after rm /* not empty final [00] S3AFileStatus{path=s3a://mehakmeet-singh-data/user; isDirectory=true; modification_time=0; access_time=0; owner=mehakmeet.singh; group=mehakmeet.singh; permission=rwxrwxrwx; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=null versionId=null  ``` have seen these errors intermittently, due to some issue with DynamoDB table.  @steveloughran suggested ```hadoop org.apache.hadoop.fs.s3a.s3guard.PurgeS3GuardDynamoTable  -force s3a://example-bucket/``` But that fails with an error:  ``` 2021-06-25 16:18:43,464 INFO service.AbstractService: Service PurgeS3GuardDynamoTable failed in state STARTED -1: Filesystem has no metadata store: s3a://mehakmeet-singh-data 	at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardDynamoDBDiagnostic.failure(AbstractS3GuardDynamoDBDiagnostic.java:115) 	at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardDynamoDBDiagnostic.require(AbstractS3GuardDynamoDBDiagnostic.java:94) 	at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardDynamoDBDiagnostic.bindStore(AbstractS3GuardDynamoDBDiagnostic.java:157) 	at org.apache.hadoop.fs.s3a.s3guard.AbstractS3GuardDynamoDBDiagnostic.bindFromCLI(AbstractS3GuardDynamoDBDiagnostic.java:147) 	at org.apache.hadoop.fs.s3a.s3guard.PurgeS3GuardDynamoTable.serviceStart(PurgeS3GuardDynamoTable.java:123) 	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) 	at org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:619) 	at org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:494) 	at org.apache.hadoop.fs.s3a.s3guard.DumpS3GuardDynamoTable.serviceMain(DumpS3GuardDynamoTable.java:517) 	at org.apache.hadoop.fs.s3a.s3guard.PurgeS3GuardDynamoTable.main(PurgeS3GuardDynamoTable.java:205) 2021-06-25 16:18:43,467 INFO util.ExitUtil: Exiting with status -1: Filesystem has no metadata store: s3a://mehakmeet-singh-data ``` Would like the reviewers to run the aws test suite once in their setup while reviewing as well.  CC: @steveloughran @mukund-thakur @bogthe","closed","","mehakmeet","2021-06-25T11:59:45Z","2021-07-02T13:14:12Z"
"","3455","HDFS-16230 Use trim() method returned value.","Strings being immutable, you need to use the trim() method return value.  ### Description of PR  Minor bug in TestStorageRestore.  ### How was this patch tested?  Issue is trivial.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","thomasleplus","2021-09-18T03:53:22Z","2021-09-19T18:39:05Z"
"","3289","HADOOP-17833. Improve Magic Committer performance","Speeding up the magic committer with key changes being  * All writes under __magic trigger marker retention   (no DELETEs after file/dir creation) * create(path, overwrite) skips all overwrite checks, including   the LIST call intended to stop files being created over dirs * thread pool used for more parallelism in task commit. * open() of .pending/pendingset files will skip HEAD calls if going straight from dir listing; will save 1 HEAD/task in job commit  This is still WiP as it needs * cost tests to verify the optimisations are active * testing through spark  Lots of changes in the tests because the committer has added a CommitContext class which manages the lifecycle of the thread pool and a set of thread local JSON serializers; this is now what is passed around in internal committer methods, so breaking tests calling in to them.  It is a better design (one we should have done from the start); manifest committer is even better as all its operations ""stages"" are modular. Just means that a lot of tests stopped compiling. And as usual, mock tests played up.  Removed the injection/handling of inconsistent S3 from the committer tests. Not needed, and simply complicating the code needlessly.   1. Incremental listings of directories with processing as the pages of results come in 1. Maximised parallelism parsing and processing files. 1. per thread json serializers; no sharing or repeated creation. 1. when creating files under __magic paths, all safety checks (overwriting files, overwriting dirs) are skipped. This benefits parquet files even more than most as it seems to be saving files with overwrite=false 1. We also skip deleting marker paths even if the FS isn't configured to do this, knowing that in job commit all these markers will be purged. This behaviour is also possible through the createFile() API, which is used when saving the intermediate manifests and the _SUCCESS file.   The write optimisations should provide significant benefits when writing files: at least one LIST per creation, for parquet a HEAD too, and O(depth) delete calls which just generates write load, risk of throttling etc. There is still more IO taking place for each magic file than writing a small simple file (it is always multipart and we have to add a marker file too). Spark also has to add an extra getXAttrs call for each file when building its intermediate stats.   Also of note  * The committer can write the summary _SUCCESS file to the path fs.s3a.committer.summary.report.directory, which can be in a different file system/bucket if desired, with the jobid as the filename. This can be used to collect all statistics of jobs even though switch over right at the same directory tree. This is the same as the Manifest Committer -that just collects more stats on its operations. * There's reuse of the hadoop common code and statistic names from the ManifestCommitter. * The hadoop-aws maven build blocks all import of mapreduce code in the production source except in the s3a.committer source tree, and even there we are selective and exclude all classes we know get referenced elsewhere.","closed","fs/s3,","steveloughran","2021-08-10T18:37:12Z","2022-06-20T16:52:00Z"
"","3147","HADOOP-17776. The java agent args should be passed as the VM arguments","Some java agents or maven plugins want works well, must pass the agent arguments as the VM arguments, such as JaCoCo etc.  PS. Use the JaCoco to generate the code coverage data and put into SonarQube.  See also: https://issues.apache.org/jira/browse/HADOOP-17776","open","","jiwq","2021-06-26T10:46:21Z","2021-07-06T01:19:39Z"
"","3086","HDFS-16039. RBF: Some indicators of RBFMetrics count inaccurately","Solve the inaccurate statistics of metrics for getNumLiveNodes, getNumDeadNodes, getNumDecommissioningNodes, getNumDecomLiveNodes, getNumDecomDeadNodes, getNumInMaintenanceLiveDataNodes, getNumInMaintenanceDeadDataNodes, getNumEnteringMaintenanceDataNodes","open","","zhuxiangyi","2021-06-09T07:16:22Z","2021-06-25T05:08:33Z"
"","2749","HADOOP-17567 fix a typo in MagicCommitTracker","Small log typo fix","closed","","phymbert","2021-03-06T12:26:03Z","2021-03-10T15:48:51Z"
"","3054","HDFS-15916. DistCp: Backward compatibility: Distcp fails from Hadoop 3 to Hadoop 2 for snapshotdiff. (#2863). Contributed by Ayush Saxena.","Signed-off-by: Wei-Chiu Chuang  (cherry picked from commit c6539e3289711d29f508930bbda40302f48ddf4c)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java","closed","","jojochuang","2021-05-26T08:31:45Z","2021-06-09T02:32:53Z"
"","3552","YARN-10976. fix resource leak due to Files.walk","see https://issues.apache.org/jira/browse/YARN-10976","closed","","lujiefsi","2021-10-14T05:23:05Z","2021-10-18T06:24:15Z"
"","2921","HDFS-15980: Fix tests for HDFS-15754 Create packet metrics for DataNode","See https://issues.apache.org/jira/browse/HDFS-15980   HDFS-15754 introduces 4 new metrics in DataNodeMetrics.  However the test associated with the patch has some bugs.  This issue is to fix those bugs in the tests.  Please note that the non-test code of HDFS-15754 worked fine without any bugs.  There are 3 issues: 1. The metric names in the tests were incorrect; 2. The tests only checked the metrics of DataNode 0, but one of the metrics, PacketsSlowWriteToMirror,  is only updated on 2 of the 3 DataNodes, so this created an indeterministic failure. 3. The metric PacketsSlowWriteToOsCache was not updated in the test due to the fact that the size of the file was smaller than BlockReceiver.CACHE_DROP_LAG_BYTES  All of them are fixed in this patch.","open","","uzshao","2021-04-16T17:23:12Z","2021-07-13T17:26:33Z"
"","3261","HADOOP-17834. Bump aliyun-sdk-oss to 3.13.0","See description of https://issues.apache.org/jira/browse/HADOOP-17834.  Confirmed that with this change, jdom 1.1 is gone from the dependency tree:  ``` $ mvn dependency:tree | grep jdom                                   [INFO] |  +- org.jdom:jdom2:jar:2.0.6:provided [INFO] |  +- org.jdom:jdom2:jar:2.0.6:compile [INFO] |     +- org.jdom:jdom2:jar:2.0.6:compile [INFO] |     +- org.jdom:jdom2:jar:2.0.6:compile ```","closed","dependencies,","smengcl","2021-08-03T20:10:02Z","2022-06-06T01:52:56Z"
"","3561","YARN-10980:fix CVE-2020-8908","see  https://www.cvedetails.com/cve/CVE-2020-8908/     A temp directory creation vulnerability exists in all versions of Guava, allowing an attacker with access to the machine to potentially access data in a temporary directory created by the Guava API com.google.common.io.Files.createTempDir(). By default, on unix-like systems, the created directory is world-readable (readable by an attacker with access to the system). The method in question has been marked @Deprecated in versions 30.0 and later and should not be used. For Android developers, we recommend choosing a temporary directory API provided by Android, such as context.getCacheDir(). For other Java developers, we recommend migrating to the Java 7 API java.nio.file.Files.createTempDirectory() which explicitly configures permissions of 700, or configuring the Java runtime's java.io.tmpdir system property to point to a location whose permissions are appropriately configured.","closed","","lujiefsi","2021-10-18T10:36:39Z","2021-10-18T11:07:29Z"
"","3506","HADOOP-17817. HADOOP-17823. S3A to raise IOE if both S3-CSE and S3Guard enabled (#3239)","S3A S3Guard tests to skip if S3-CSE are enabled (#3263)          Follow on to         * HADOOP-13887. Encrypt S3A data client-side with AWS SDK (S3-CSE)          If the S3A bucket is set up to use S3-CSE encryption, all tests which turn         on S3Guard are skipped, so they don't raise any exceptions about         incompatible configurations.      Contributed by Mehakmeet Singh","closed","","mehakmeet","2021-10-01T08:17:59Z","2021-10-05T11:13:16Z"
"","2742","HADOOP-16721. Improve S3A rename resilience","S3A Rename to  1. switch from verifying dest parent path is present and a directory to simply verifying that it isn't a file. 2. avoids race condition where thread 1 has deleted the last subdir/file but not yet recreated a parent marker 3. don't downgrade FileNotFoundException to 'false' if source doesn't exist 4. raise FileAlreadyExistsException if dest path exists or parent is a file.  If thread/process 1 deleted the subdir `dest/subdir1` one and there are no sibling subdirectories, then then the dir `dest` would not exist until `maybeCreateFakeParentDirectory()` had performed a `LIST` and, if needed, a `PUT` of a marker. This creates a window where thread/process 2, trying to rename `staging/subdir2` into `dest` could fail ""parent does not exist"".  And guess what: 1. Hive still thinks renaming directories from staging to the production dataset is a good idea. 1. It can spawn many threads to do this parallel (maybe even to compensate for the slow rename()) performance of S3. 1. On a sufficiently overloaded system (lots of threads already doing the parallel filenames) the window of parent-dir-not-found can last long enough for things to fail.     Prior to S3 being consistent this wouldn't have been an issue * rename()'s need to list everything underneath the source dir wasn't safe to use with delayed list consistency * so S3Guard was effectively mandatory. * and delayed LIST consistency may even have reduced the window size.  The fix: go from verifying parent dir exists to simply making sure that it isn't a file is a weakening of the requirement ""parent dir must exist"" -but file:// already doesn't require that.   Consequences 1. You can rename under a path which doesn't exist. file:// lets you do that already; contract tests have a switch for it.    (FS contract test has switch for this) 2. Exceptions get raised when false would be returned and hive lose details of what went wrong. This    isn't consistent with HDFS, but is with other stores (FS contract test has switch for this) 3. You can rename a few layers under a file, but then `create()` lets you do that already,   and nobody has noticed in production.     If directory marker retention was enabled there's more likelihood that an empty dir marker existed -if it did then the race condition wouldn't exist. But as there are no guarantees that the marker will be there, that's not safe to rely on.","closed","","steveloughran","2021-03-04T17:50:20Z","2021-10-15T19:49:14Z"
"","2872","HDFS-15953. fix DUMMY_CLIENT_ID verification","rpcClientId != RpcConstants.DUMMY_CLIENT_ID in FSEditLogOp#hasRpcIds()  // code placeholder public boolean hasRpcIds() {   return rpcClientId != RpcConstants.DUMMY_CLIENT_ID       && rpcCallId != RpcConstants.INVALID_CALL_ID; }   determine that two arrays are equal should use ""Arrays.equals""","closed","","WeisonWei","2021-04-06T12:33:15Z","2021-04-07T10:22:21Z"
"","3589","HADOOP-16905 branch 3.2","Revive PR #1973. This time, with HADOOP-15775 added to make it compile.   ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2021-10-27T04:21:09Z","2021-10-27T04:21:24Z"
"","3590","HADOOP-16905 branch 3.2","Revive PR #1973. This time, with HADOOP-15775 added to make it compile.","closed","","jojochuang","2021-10-27T04:22:14Z","2021-10-29T02:29:23Z"
"","3045","HADOOP-16752. assertion failure in ITestAzureBlobFileSystemFileStatus","Reviewing the patch, the error message needs to print the expected/actual values so if this isn't the fix, we have some idea of what is wrong  Change-Id: Iacdb90839136e77b2e2769f3b93049d4bfef0d0e   ---  tested, Azure cardiff `-Dparallel-tests=abfs -DtestsThreadCount=5 -Dscale`","closed","","steveloughran","2021-05-24T09:24:30Z","2021-10-15T19:42:59Z"
"","3583","HADOOP-17978. Exclude ASF license check for pkg-resolver JSON","Reviewed-by: Viraj Jasani  Reviewed-by: Masatake Iwasaki  Reviewed-by: Akira Ajisaka  Reviewed-by: Gautham B A     ### Description of PR Backporting this fix to branch 2.10  ### How was this patch tested? No testing required.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-24T08:27:21Z","2021-10-26T11:07:10Z"
"","2929","HADOOP-17608. Fix TestKMS failure (#2880)","Reviewed-by: Masatake Iwasaki  (cherry picked from commit 2bd810a5075e6b9c15fde22e43ac9bcf2a6c22f9) (cherry picked from commit 77315abe474ad3a17507bfc67956b85327f487e6)   Conflicts: 	hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java","closed","","jojochuang","2021-04-19T03:20:53Z","2021-04-19T05:13:24Z"
"","3626","Revert ""HDFS-16300. Use libcrypto in Windows for libhdfspp""","Reverts apache/hadoop#3617","closed","","GauthamBanasandra","2021-11-07T07:54:56Z","2021-11-07T07:58:51Z"
"","3489","Revert ""Fix winutils typos""","Reverts apache/hadoop#3484","closed","","goiri","2021-09-27T16:39:54Z","2021-09-27T16:42:54Z"
"","3443","Revert ""HADOOP-17195. OutOfMemory error while performing hdfs CopyFromLocal to ABFS""","Reverts apache/hadoop#3406","closed","","steveloughran","2021-09-15T21:27:39Z","2021-09-15T21:27:55Z"
"","3173","Revert ""HADOOP-17778. CI for Centos 8""","Reverts apache/hadoop#3151","closed","","GauthamBanasandra","2021-07-03T05:12:45Z","2021-07-04T06:58:00Z"
"","2900","Revert ""HDFS-15423 RBF: WebHDFS create shouldn't choose DN from all sub-clusters""","Reverts apache/hadoop#2605","closed","","goiri","2021-04-13T03:44:18Z","2021-04-13T03:44:31Z"
"","3227","HDFS-12920. HDFS default value change (with adding time unit) breaks old version MR tarball work with Hadoop 3.x","Revert ""HDFS-10845. Change defaults in hdfs-site.xml to match timeunit type. Contributed by Yiqun Lin""  This reverts commit b6d839a60ceed733bfacb791fc5ed06116720dd0.   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/main/resources/hdfs-default.xml  JIRA: HDFS-12920","closed","","aajisaka","2021-07-24T06:34:00Z","2021-07-25T16:53:50Z"
"","3009","HDFS-16024: RBF: Rename data to the Trash should be based on src loca…","Rename data to the Trash should be based on src locations","closed","","zhuxiangyi","2021-05-12T15:29:57Z","2021-05-26T12:51:04Z"
"","2819","HDFS-15923. RBF: Authentication failed when rename accross sub clusters.","Rename accross subcluster with RBF and Kerberos environment. Will encounter the following two errors:  Save Object to journal. Precheck try to get src file status So, we need use Proxy UGI doAs create DistcpProcedure and TrashProcedure and submit Job.  In patch i use proxy ugi doAs above method. It worked.  But there are another strange thing and this patch not solve:  Router use ugi itself to submit the Distcp job. But not user ugi or proxy ugi. This may cause excessive distcp permissions.","closed","","zhengzhuobinzzb","2021-03-26T02:15:40Z","2021-05-18T07:44:57Z"
"","2777","HADOOP-17476. ITestAssumeRole.testAssumeRoleBadInnerAuth failure.","Removes string match so change in AWS S3 error message doesn't  cause the test to fail","closed","","steveloughran","2021-03-15T16:37:33Z","2021-10-15T19:49:17Z"
"","2890","HADOOP-17630. [JDK 15] TestPrintableString fails due to Unicode 13.0 support.","Remove U+30000 from the test.  JIRA: HADOOP-17630","closed","","aajisaka","2021-04-10T15:53:44Z","2021-04-13T08:09:07Z"
"","2734","HADOOP-17559. S3guard import OOM.","Remove all tracking of files from DDB AncestorState; dirs in import tool.  Reduces size of the cache to O(dirs).  Test change is to reduce brittleness to clock skew on loaded test runs; removes an intermittent failure where the existence assert was triggering a s3guard update -which then broke the assert about the number of writes  Change-Id: I9251f64beb0fec225b0b4ba71bc16f3e116bc758","closed","fs/s3,","steveloughran","2021-03-02T15:08:12Z","2021-10-15T16:42:11Z"
"","3406","HADOOP-17195. OutOfMemory error while performing hdfs CopyFromLocal to ABFS","Region: US-West-2 `mvn -Dparallel-tests=abfs -DtestsThreadCount=8 -Dscale clean verify`  ### UT: ``` [INFO] Results: [INFO]  [WARNING] Tests run: 105, Failures: 0, Errors: 0, Skipped: 2 ``` ### IT: ``` [ERROR] Errors:  [ERROR]   ITestAzureBlobFileSystemLease.testFileSystemClose:301->lambda$testFileSystemClose$5:303 » TestTimedOut [INFO]  [ERROR] Tests run: 559, Failures: 0, Errors: 1, Skipped: 75 ``` This error is due to how we are closing ThreadPool in AbfsStore and not in AbfsOutputStream like we used to. If we close Filesystem before we close the stream but after we have written the block, the ThreadPool is in a ""Terminated"" state and we are not able to submit the task. In this test, we are supposed to go through with the submit and receive LeaseNotPresent from the append task.  ### IT-scale: ``` [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek:66->testReadWriteAndSeek:101 [Retry was required due to issue on server side] expected:<[0]> but was:<[1]> [INFO]  [ERROR] Tests run: 259, Failures: 1, Errors: 0, Skipped: 40 ``` This test is failing due to an assert in Validating the TracingHeaders, in the request we have retryNum as ""0"", but we are retrying once. In the trunk, this is failing with ""Out of Memory"" exception for me, so not sure if this is actually passing in the trunk as well. Would appreciate it if someone can run this test in their setup.","closed","","mehakmeet","2021-09-08T16:19:08Z","2021-09-15T21:28:55Z"
"","3239","HADOOP-17817. Throw an exception if S3 client-side encryption is enabled on S3Guard enabled bucket","Region: ap-south-1.  All tests initializing S3AFS failed.  Follow up of #2706.","closed","","mehakmeet","2021-07-27T11:56:03Z","2021-07-29T03:58:27Z"
"","3462","HADOOP-17922. Lookup old S3 encryption configs for JCEKS","Region: ap-south-1. `mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale` Used long and short bucket property names for encryption properties for tests.  `Tests run: 575, Failures: 0, Errors: 0, Skipped: 5` `Tests run: 1477, Failures: 0, Errors: 2, Skipped: 636` `Tests run: 151, Failures: 0, Errors: 1, Skipped: 28` CSE: `Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.081 s - in org.apache.hadoop.fs.s3a.ITestS3AClientSideEncryptionKms`","closed","","mehakmeet","2021-09-20T13:12:10Z","2021-11-05T12:57:50Z"
"","3292","HADOOP-13887. Support S3 client side encryption (S3-CSE) using AWS-SDK","Region: ap-south-1. `mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale`  ### CSE ON, S3Guard ON ``` [INFO] Results: [INFO]  [WARNING] Tests run: 575, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [WARNING] Tests run: 1476, Failures: 0, Errors: 0, Skipped: 1277 ``` ``` [INFO] Results: [INFO]  [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 125 ```  ### CSE ON, S3Guard OFF  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 575, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1476, Failures: 0, Errors: 2, Skipped: 641 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 1, Errors: 1, Skipped: 28 ```  ### CSE OFF, S3Guard ON  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 575, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AFileSystemContract>FileSystemContractBaseTest.testLSRootDir:835->FileSystemContractBaseTest.assertListFilesFinds:850 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1476, Failures: 0, Errors: 3, Skipped: 405 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir.testListEmptyRootDirectory:82->AbstractContractRootDirectoryTest.testListEmptyRootDirectory:196 » TestTimedOut [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:265 » TestTimedOut [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive:101 » TestTimedOut [INFO]  [ERROR] Tests run: 11, Failures: 0, Errors: 3, Skipped: 0 ```  ### CSE OFF, S3Guard OFF  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 575, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1476, Failures: 0, Errors: 2, Skipped: 467 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 1, Errors: 1, Skipped: 28 ```","closed","","mehakmeet","2021-08-11T04:59:34Z","2021-12-16T09:37:05Z"
"","3412","HADOOP-17871. S3A CSE: minor tuning","Region: ap-south-1 `mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale`  ### UT: ``` [INFO] Results: [INFO]  [WARNING] Tests run: 575, Failures: 0, Errors: 0, Skipped: 5 ``` ### IT: ``` [INFO] Results: [INFO]  [ERROR] Tests run: 1478, Failures: 0, Errors: 2, Skipped: 466 ``` ### IT-scale: ``` [INFO]  [ERROR] Tests run: 151, Failures: 0, Errors: 1, Skipped: 28 ```  ### CSE-test: ``` [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 36.17 s - in org.apache.hadoop.fs.s3a.ITestS3AClientSideEncryptionKms ```","closed","","mehakmeet","2021-09-09T12:07:54Z","2021-09-15T21:29:23Z"
"","3525","HADOOP-17953. S3A: Tests to lookup global or per-bucket configuration for encryption algorithm","Region: ap-south-1  ### CSE: `[WARNING] Tests run: 588, Failures: 0, Errors: 0, Skipped: 5` ```[ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:391->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:391->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1471, Failures: 0, Errors: 2, Skipped: 636 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 1, Errors: 1, Skipped: 28 ```  ### normal: `[WARNING] Tests run: 588, Failures: 0, Errors: 0, Skipped: 5` ``` [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:391->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:391->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1471, Failures: 0, Errors: 2, Skipped: 466 ``` ``` [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 1, Errors: 1, Skipped: 28 ```  ### s3guard `[WARNING] Tests run: 588, Failures: 0, Errors: 0, Skipped: 5` `[INFO]  [ERROR] Tests run: 1471, Failures: 1, Errors: 3, Skipped: 387` ` [INFO]  [ERROR] Tests run: 11, Failures: 0, Errors: 3, Skipped: 0 `  ### CSE-S3Guard `[WARNING] Tests run: 588, Failures: 0, Errors: 0, Skipped: 5` `[WARNING] Tests run: 1471, Failures: 0, Errors: 0, Skipped: 1271` `[WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 92`","closed","","mehakmeet","2021-10-06T13:25:53Z","2021-10-19T09:58:27Z"
"","3263","HADOOP-17823. S3A Tests to skip if S3Guard and S3-CSE are enabled","Region: ap-south-1  ### CSE ON, S3Guard ON ``` [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 9 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1465, Failures: 0, Errors: 2, Skipped: 1266 ``` ``` [INFO] Results: [INFO]  [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 125 ```  ### CSE ON, S3Guard OFF  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 9 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1465, Failures: 0, Errors: 2, Skipped: 641 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 0, Errors: 1, Skipped: 28 ```  ### CSE OFF, S3Guard ON  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AFileSystemContract>FileSystemContractBaseTest.testLSRootDir:833->FileSystemContractBaseTest.assertListFilesFinds:848 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1465, Failures: 0, Errors: 3, Skipped: 405 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir.testListEmptyRootDirectory:82->AbstractContractRootDirectoryTest.testListEmptyRootDirectory:196 » TestTimedOut [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:265 » TestTimedOut [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRmEmptyRootDirNonRecursive:101 » TestTimedOut [INFO]  [ERROR] Tests run: 11, Failures: 0, Errors: 3, Skipped: 0 ```  ### CSE OFF, S3Guard OFF  ``` [INFO] Results: [INFO]  [WARNING] Tests run: 574, Failures: 0, Errors: 0, Skipped: 5 ``` ``` [INFO] Results: [INFO]  [ERROR] Errors:  [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [ERROR]   ITestS3AMiscOperationCost.testGetContentSummaryRoot:96->AbstractS3ACostTest.verifyMetrics:376->lambda$testGetContentSummaryRoot$1:96->getContentSummary:140 » TestTimedOut [INFO]  [ERROR] Tests run: 1465, Failures: 0, Errors: 2, Skipped: 467 ``` ``` [ERROR] Errors:  [ERROR]   ITestS3AContractRootDir>AbstractContractRootDirectoryTest.testRecursiveRootListing:267 » TestTimedOut [INFO]  [ERROR] Tests run: 151, Failures: 0, Errors: 1, Skipped: 28 ```","closed","","mehakmeet","2021-08-04T02:47:44Z","2021-08-05T10:46:17Z"
"","2877","MAPREDUCE-7334. TestJobEndNotifier fails.","Reduced the sleep time from 1000 to 3 seconds to shut down HttpServer2 gracefully.","closed","","aajisaka","2021-04-08T05:55:02Z","2021-04-09T07:07:17Z"
"","3162","fix state in applicationhistory web page","Recently, when i deploy  hadoop 3.3 , after submit a task and finished running ,then from application history web page(8188),  I see the state column is valid.    ![image](https://user-images.githubusercontent.com/7209497/124079444-b6f0d080-da7b-11eb-966d-f6f096b505fb.png)   after debug from source code   found  7,8,9 should be 6, 7, 8 ` `   From below you can see appsTableData is 13 column, and 6, 7, 8 is relate timestamp(Date)  `appsTableData [Array(13)] 0: Array(13) 0: ""application_1625053883552_0001"" 1: “XXXXXXXXXX"" 2: “XXXXXXXXXXXXX"" 3: ""YARN"" 4: ""rtt"" 5: ""0"" 6: ""1625053927116"" 7: ""1625064679564"" 8: ""1625064933490"" 9: ""FINISHED"" 10: ""SUCCEEDED"" 11: ""    "" 12: ""History"" length: 13 __proto__: Array(0) length: 1 __proto__: Array(0)`  after fix ![image](https://user-images.githubusercontent.com/7209497/124079459-beb07500-da7b-11eb-996b-4417e6404890.png)","open","","zhuxiong","2021-07-01T06:53:35Z","2021-07-01T09:53:52Z"
"","3499","HADOOP-11867. Add a high performance vectored read API to file system.","Rebased work on top of https://github.com/apache/hadoop/pull/1830   Conflicts: 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/BufferedFSInputStream.java 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataInputStream.java 	hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java 	pom.xml    ### Description of PR Adding support for multiple ranged read async api in PositionedReadable. The default iterates through the ranges to read each synchronously, but the intent is that FSDataInputStream subclasses can make more efficient readers especially object stores implementation.  ### How was this patch tested? Added benchmarks.  Added UT's Added new contract tests for new API spec.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","fs,","mukund-thakur","2021-09-29T11:48:01Z","2022-05-04T12:18:03Z"
"","2992","HDFS-16015. RBF:  Read and write data through router select dn according to real user ip","Read and write data through router select dn according to real user ip","open","","zhuxiangyi","2021-05-08T11:24:18Z","2021-05-18T15:29:35Z"
"","3285","HADOOP-17156. Purging the buffers associated with input streams during close()","Ran all abfs tests. Don't see any failures.   Please treat this as initial patch to get a basic understanding for the thought process.","closed","","mukund-thakur","2021-08-10T09:14:41Z","2021-09-07T09:43:36Z"
"","3179","HDFS-16114. the balancer parameters print error","public String toString() { return String.format(""%s.%s [%s,"" + "" threshold = %s,"" + "" max idle iteration = %s,"" + "" #excluded nodes = %s,"" + "" #included nodes = %s,"" + "" #source nodes = %s,"" + "" #blockpools = %s,"" + "" run during upgrade = %s,"" + "" hot block time interval = %s]"" + "" sort top nodes = %s"", Balancer.class.getSimpleName(), getClass().getSimpleName(), policy, threshold, maxIdleIteration, excludedNodes.size(), includedNodes.size(), sourceNodes.size(), blockpools.size(), runDuringUpgrade, sortTopNodes, hotBlockTimeInterval); }  print error.","closed","","JiaguodongF","2021-07-06T10:18:40Z","2021-07-07T08:44:13Z"
"","3035","HADOOP-17714 ABFS: testBlobBackCompatibility, testRandomRead & WasbAbfsCompatibility tests fail when triggered with default configs","PR to fix  abfs to wasb url conversion when alwaysusehttps is enabled.","closed","","snehavarma","2021-05-21T06:35:22Z","2021-06-13T18:22:29Z"
"","2813","HDFS-15160. ReplicaMap, Disk Balancer, Directory Scanner and various FsDatasetImpl methods should use datanode readlock","PR to backport HDFS-15160 to branch-3.3. This is already committed on trunk some time back.","closed","","sodonnel","2021-03-24T11:54:01Z","2021-04-09T09:25:15Z"
"","3028","HADOOP-17715 ABFS: Append blob tests with non HNS accounts fail","PR to add new configuration of appendblob with HNS accounts.  Testing done: It was ensured that the xml gets generated with the append blob config and runs successfully.","closed","","snehavarma","2021-05-20T11:07:35Z","2021-06-09T06:44:17Z"
"","3440","HADOOP-17912. ABFS: Support for Encryption Context","PR introduces use of different customer-provided keys per encrypted file, superseding the global key use in HADOOP-17536.  Adding ABFS driver support for an EncryptionContextProvider plugin to retrieve encryption information, the implementation for which should be provided by the client. When encryption is activated for an account, file creation will involve ABFS driver fetching an encryption context and encryption key from the provider. These will be sent as request headers to the server, which handles encryption/decryption. The server will store the encryption context as system metadata for a file. Any subsequent REST calls to the server to access data or user metadata will require sending the encryption key headers. The encryption context of a file can be obtained through response headers of a GetPathStatus call, and then used to fetch the encryption key from the encryption provider.  New configs: `fs.azure.encryption.encoded.client-provided-key`: Server side encryption key encoded in Base6format `fs.azure.encryption.encoded.client-provided-key-sha`: SHA256 hash of encryption key encoded in Base64format `fs.azure.encryption.context.provider.type`: Custom EncryptionContextProvider type","open","","sumangala-patki","2021-09-15T16:51:14Z","2022-07-08T08:50:39Z"
"","3253","HADOOP 17290 to 17811. ABFS Backports","PR contains cherry-pick of the following ABFS driver commits:  - https://github.com/apache/hadoop/pull/2520 - https://github.com/apache/hadoop/pull/3153 - https://github.com/apache/hadoop/pull/3221","closed","","sumangala-patki","2021-08-02T06:32:13Z","2021-08-03T04:46:59Z"
"","3341","HADOOP-17873. ABFS: Fix transient failures in ITestAbfsStreamStatistics and ITestAbfsRestOperationException","PR addresses transient failures in the following test classes:  - ITestAbfsStreamStatistics: Uses a filesystem level static instance to record read/write statistics, which also tracks these operations in other tests running parallelly. To be marked for sequential-only run to avoid transient failure - ITestAbfsRestOperationException: The use of a static member to track retry count causes transient failures when two tests of this class happen to run together. Switch to non-static variable for assertions on retry count","closed","","sumangala-patki","2021-08-26T15:26:04Z","2021-11-08T10:01:21Z"
"","3234","HDFS-16145. CopyListing fails with FNF exception with snapshot diff.","please see https://issues.apache.org/jira/browse/HDFS-16145.","closed","","bshashikant","2021-07-26T10:53:16Z","2021-07-28T04:59:01Z"
"","3188","HDFS-16121. Iterative snapshot diff report can generate duplicate records for creates, deletes and Renames.","Please refer https://issues.apache.org/jira/browse/HDFS-16121 for details","closed","","bshashikant","2021-07-08T07:36:28Z","2021-11-09T19:24:43Z"
"","2881","HDFS-15961. standby namenode failed to start ordered snapshot deletion is enabled while having snapshottable directories","Please refer https://issues.apache.org/jira/browse/HDFS-15961.","closed","","bshashikant","2021-04-09T06:15:37Z","2021-04-27T03:45:07Z"
"","3340","HDFS-16187. SnapshotDiff behaviour with Xattrs and Acls is not consistent across NN restarts with checkpointing","Please check https://issues.apache.org/jira/browse/HDFS-16187","closed","","bshashikant","2021-08-26T15:17:21Z","2021-09-13T21:04:02Z"
"","3271","HDFS-16155: Allow configurable exponential backoff in DFSInputStream refetchLocations","Per https://issues.apache.org/jira/browse/HDFS-16155, we would like the ability to customize the backoff strategy when BlockMissingException occurs. This can happen when the balancer moves blocks, and in low latency clusters the existing backoff is too conservative. Drastically reducing the existing window base config would help but expose the namenode to a potential DDOS if many blocks became missing, because the current backoff would grow slowly.  Adding a configurable exponential component allows for aggressive early retries that back off quickly enough to mitigate stampeding herds. We make the backoff configurable by adding two new configs:  - `dfs.client.retry.window.multiplier`: defaults to 1 to preserve existing behavior. Increasing this can result in a steeper backoff curve when desired - `dfs.client.retry.window.max`: defaults to Int.MAX to preserve existing behavior. Decreasing this can help put a ceiling on exponential backoffs that could quickly grow to effectively unlimited levels.  As described, the default behavior is maintained and I've added a test case to verify that. Someone looking for a more aggressive initial retry that backs off quickly in case of continuous failure could try setting `window.base` to 10, `window.multiplier` to 5, and `window.max` to 10000. This would result in a quick initial retry of max 50ms, but quickly backoff to a few seconds within 3 retries.  In order to improve the testability of this feature, I pulled out the existing refetchLocations retry configs into a FetchBlockLocationsRetryer class. I also improved the readability of the comment describing the backoff strategy, and fully tested the new retryer in TestFetchBlockLocationsRetryer.","open","","bbeaudreault","2021-08-05T18:03:29Z","2022-03-04T15:21:14Z"
"","3272","HADOOP-17837: Add unresolved endpoint value to UnknownHostException","Per https://issues.apache.org/jira/browse/HADOOP-17837, adds `endpoint.toString() to the UnknownHostException. UnresolvedAddressException cannot have a message, so the current behavior is a UHE with no message as well. Adding the endpoint to the exception will make it easier to debug when these arise.","closed","","bbeaudreault","2021-08-05T19:39:05Z","2021-08-06T16:11:42Z"
"","3098","[HADOOP-17758][HDFS][FOLLOWUP]NPE and excessive warnings after HADOOP-17728","NPE after HADOOP-17728.  If something is added to the queue, ReferenceQueue.enqueue will be called, i.e. lock.notifyAll be called, That will not wait forerver.  more details: [HADOOP-17758](https://issues.apache.org/jira/browse/HADOOP-17758)","closed","","yikf","2021-06-11T05:01:30Z","2021-06-11T08:10:05Z"
"","3691","HDFS-16341. Add block placement policy desc","Now, we have six block placement policies supported, we can keep the doc updated.No test cases needed.","open","","GuoPhilipse","2021-11-20T11:38:57Z","2022-02-11T08:30:30Z"
"","2948","HDFS-15993. INodesInPath#toString() will throw AssertionError","NodesInPath#toString() with snapshot will throw AssertionError","open","","langlaile1221","2021-04-23T13:04:02Z","2021-05-25T20:56:43Z"
"","3685","HADOOP-18016. Make certain methods LimitedPrivate in S3AUtils.java","No tests included since it's just an annotation change. Ran the test suite after compiling successfully. Region: `ap-south-1`  Tests: `[WARNING] Tests run: 588, Failures: 0, Errors: 0, Skipped: 5` `[ERROR] Tests run: 1471, Failures: 0, Errors: 2, Skipped: 466`(my bucket specific failures, ignore) `[ERROR] Tests run: 151, Failures: 1, Errors: 1, Skipped: 28`(my bucket specific failures, ignore)","closed","","mehakmeet","2021-11-19T11:01:38Z","2021-11-24T08:02:59Z"
"","3466","HADOOP-17922. move to fs.s3a.encryption.algorithm - JCEKS integration","New unit test suite for propagation  * adds TestBucketConfiguration for conf file propagation (succeeds) * one for jecks (fails; needs to sync up with the other patch) * moves over some test cases from ITestS3AConfiguration which   don't need an FS.  Change-Id: Ie1b6e0d1c655d00fc6d47ce054a1aeba01c71044    ### Description of PR    ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?  new test and modified original yes: s3 london","closed","","steveloughran","2021-09-21T15:02:51Z","2021-10-01T07:53:37Z"
"","3082","[Do not commit] Exclude JSON files from RAT check","Need to exclude the JSON files from RAT check since we can't add ASF license header as a comment.","closed","","GauthamBanasandra","2021-06-08T06:43:10Z","2021-06-09T23:26:45Z"
"","2937","HADOOP-17641. ITestWasbUriAndConfiguration failing.","Moves the mock account name --which is required to never exist-- from mockAccount to an account with a UUID.  Dynamic UUID generation would seem overkill.  Contributed by Steve Loughran.","closed","","steveloughran","2021-04-20T09:29:11Z","2021-04-20T14:32:07Z"
"","2977","HADOOP-17631. Configuration ${env.VAR:-FALLBACK} to eval FALLBACK when  restrictSystemProps=true","Moves the checking on restricted access to when env vars and system props are retrieved, rather than the actual parsing of the string.  With tests to verify this.  I've looked at this carefully to see if I'm introducing any security risks. I don't see any.  Notable that if a system property or env var is set, you can't override it in the config file, because the property/var eval comes first. i.e. If I set a property user.name the system property user.name would win. That is not new in this patch.","closed","","steveloughran","2021-05-05T16:47:26Z","2021-10-15T19:43:03Z"
"","3237","HADOOP-16128 Fix FileSystem.newInstance resource leaks in unit tests","Motivation -  Fix resource leak created in unit tests that instantiate FileSystem resource.   Result -  Fixed the resource leaks in the unit test for the S3A module. Run all unit tests locally successfully.","open","","amitvc","2021-07-26T22:54:08Z","2021-08-02T14:43:07Z"
"","3071","HADOOP-17742. fix distcp fail when copying to ftp filesystem","more information: https://issues.apache.org/jira/browse/HADOOP-17742#","open","","zhaomin1423","2021-06-03T12:22:47Z","2021-10-07T07:51:23Z"
"","3070","HADOOP-17742. fix distcp fail when copying to ftp filesystem","more information https://issues.apache.org/jira/browse/HADOOP-17742#","closed","","zhaomin1423","2021-06-03T12:17:46Z","2021-06-03T14:01:01Z"
"","3069","HADOOP-17742. fix distcp fail when copying to ftp filesystem","more information https://issues.apache.org/jira/browse/HADOOP-17742#","closed","","zhaomin1423","2021-06-03T08:06:18Z","2021-06-03T12:20:45Z"
"","2698","HADOOP-17527. ABFS: Fix boundary conditions in InputStream seek and skip","Modify AbfsInputStream seek method to throw EOF exception on seek to contentLength for a non-empty file","closed","","sumangala-patki","2021-02-13T10:48:21Z","2021-03-05T09:55:05Z"
"","2852","MAPREDUCE-7287. Distcp will delete exists file , If we use ""-delete …","MAPREDUCE-7287. Distcp will delete exists file , If we use ""-delete and -update"" options and distcp file.  hdfs://ns1/tmp/a is an existing file, hdfs://ns2/tmp/a is also an existing file.  When I run this command,  ``` hadoop distcp -delete -update hdfs://ns1/tmp/a hdfs://ns2/tmp/a ``` I Found hdfs://ns2/tmp/a is deleted unpectectedly.  Issue link: https://issues.apache.org/jira/browse/MAPREDUCE-7287","closed","","zhengchenyu","2021-04-02T03:44:06Z","2021-05-28T19:21:38Z"
"","3153","HADOOP-17765. ABFS: Use Unique File Paths in Tests","Many of ABFS driver tests use common names for file paths (e.g., ""/testfile""). This poses a risk of errors during parallel test runs when static variables (such as those for monitoring stats) affected by file paths are introduced.  Using unique test file names will avoid possible errors arising from shared resources during parallel runs.","closed","","sumangala-patki","2021-06-28T05:45:39Z","2021-07-27T17:50:58Z"
"","3122","HADOOP-17765. ABFS: Use Unique File Paths in Tests","Many of ABFS driver tests use common names for file paths (e.g., ""/testfile""). This poses a risk of errors during parallel test runs when static variables (such as those for monitoring stats) affected by file paths are introduced.  Using unique test file names will avoid possible errors arising from shared resources during parallel runs.","closed","","sumangala-patki","2021-06-19T11:08:44Z","2021-07-09T12:58:37Z"
"","2914","HDFS-15977. Call explicit_bzero only if it is available.","Manually tested with CentOS 7.9.  JIRA: HDFS-15977","closed","","aajisaka","2021-04-15T08:04:10Z","2021-04-16T04:27:21Z"
"","2952","HADOOP-17661. mvn versions:set fails to parse pom.xml.","Manual test: 1. verified that mvn clean build passed 2. verified that mvn versions:set completes.","closed","","jojochuang","2021-04-24T05:48:37Z","2021-04-26T06:05:07Z"
"","3066","HDFS-16053. Make the way of get heartbeat interval from conf consiste…","Make the way of get heartbeat interval from conf consistent between Balancer and TestBalancer  ## NOTICE JIRA: https://issues.apache.org/jira/browse/HDFS-16053","open","","wzhallright","2021-06-02T09:20:44Z","2021-06-02T12:32:15Z"
"","2735","HADOOP-11452 make rename/3 public","make rename/3 public. Rollup of #743 commits rebased to trunk","open","","steveloughran","2021-03-02T15:28:15Z","2022-03-11T17:40:10Z"
"","3385","HADOOP-17891. Exclude snappy-java and lz4-java from relocation in shaded hadoop client libraries","lz4-java is a provided dependency. So in the shaded Hadoop libraries, e.g. hadoop-client-api, if we don't exclude lz4 dependency from relocation, the downstream will still see the exception even they include lz4 dependency.  ``` [info]   Cause: java.lang.ClassNotFoundException: org.apache.hadoop.shaded.net.jpountz.lz4.LZ4Factory [info]   at java.net.URLClassLoader.findClass(URLClassLoader.java:382) [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:418) [info]   at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:351) [info]   at org.apache.hadoop.io.compress.lz4.Lz4Compressor.(Lz4Compressor.java:66) [info]   at org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119) [info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152) [info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168)  ```  Currently snappy-java is included and relocated in Hadoop shaded client libraries. But as it includes native methods, it should not be relocated too due to JNI method resolution. The downstream will see the exception: ``` [info]   Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I [info]   at org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)                                                                                                  [info]   at org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)                                                                                                         [info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282) [info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210) ```","closed","","viirya","2021-09-04T21:34:12Z","2021-09-14T19:07:03Z"
"","3142","YARN-10820: Handle yarn node list synchronization issue","LINK to JIRA : https://issues.apache.org/jira/browse/YARN-10820  When we apply ""yarn node list "" command, it fails intermittently with this error :   2021-06-13 11:26:42,316 WARN client.RequestHedgingRMFailoverProxyProvider: Invocation returned exception: java.lang.ArrayIndexOutOfBoundsException: 1 on [resourcemanager-1], so propagating back to caller. Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 1  at java.util.ArrayList.add(ArrayList.java:465)  at org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto$Builder.addAllNodeStates(YarnServiceProtos.java:28009)  at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.mergeLocalToBuilder(GetClusterNodesRequestPBImpl.java:124)  at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.mergeLocalToProto(GetClusterNodesRequestPBImpl.java:82)  at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.getProto(GetClusterNodesRequestPBImpl.java:56)  at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:329)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)  at com.sun.proxy.$Proxy8.getClusterNodes(Unknown Source)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler$1.call(RequestHedgingRMFailoverProxyProvider.java:159)  at java.util.concurrent.FutureTask.run(FutureTask.java:266)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.run(FutureTask.java:266)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)  at java.lang.Thread.run(Thread.java:748)    2021-06-13 11:27:58,415 WARN client.RequestHedgingRMFailoverProxyProvider: Invocation returned exception: java.lang.UnsupportedOperationException on [resourcemanager-0], so propagating back to caller. Exception in thread ""main"" java.lang.UnsupportedOperationException         at java.util.Collections$UnmodifiableCollection.add(Collections.java:1057)         at org.apache.hadoop.yarn.proto.YarnServiceProtos$GetClusterNodesRequestProto$Builder.addAllNodeStates(YarnServiceProtos.java:28009)         at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.mergeLocalToBuilder(GetClusterNodesRequestPBImpl.java:124)         at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.mergeLocalToProto(GetClusterNodesRequestPBImpl.java:82)         at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl.getProto(GetClusterNodesRequestPBImpl.java:56)         at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:329)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:498)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)         at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)         at com.sun.proxy.$Proxy8.getClusterNodes(Unknown Source)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:498)         at org.apache.hadoop.yarn.client.RequestHedgingRMFailoverProxyProvider$RMRequestHedgingInvocationHandler$1.call(RequestHedgingRMFailoverProxyProvider.java:159)         at java.util.concurrent.FutureTask.run(FutureTask.java:266)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.run(FutureTask.java:266)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748)   Making this thread safe by adding synchronization.","closed","","swathic95","2021-06-25T06:25:24Z","2021-06-27T11:34:53Z"
"","3298","MAPREDUCE-7361. Clean shared state pollution to avoid flaky tests in class TestTaskProgressReporter","Link to issue: [https://issues.apache.org/jira/browse/MAPREDUCE-7361](https://issues.apache.org/jira/browse/MAPREDUCE-7361) ## What is the purpose of this change This PR is to clean the polluted shared status among the 3 tests:  ``` Test1: org.apache.hadoop.mapred.TestTaskProgressReporter.testBytesWrittenRespectingLimit Test2: org.apache.hadoop.mapred.TestTaskProgressReporter.testScratchDirSize Test3: org.apache.hadoop.mapred.TestTaskProgressReporter.testTaskProgress ``` - Test1 and Test2 pollute the shared status with Test3, which can make test3 fail.  - It may be better to clean state pollutions so that some other tests won't fail in the future due to the shared state polluted by this test. ## Reproduce the test failures Run the tests in the same JVM in the following orders: - Test1 and Test3 - Test2 and Test3 ## Expected result The tests should run successfully when multiple tests that use this shared state are run in the same JVM. ## Actual result - Run Test1 and Test3, Test3 fails: ``` [ERROR]   TestTaskProgressReporter.testTaskProgress:267 expected:<[2]> but was:<[3]> ``` - Run Test2 and Test3, Test3 fails: ``` [ERROR]   TestTaskProgressReporter.testTaskProgress:267 expected:<[2]> but was:<[11]> ```  ## Fix Clean the value of `statusUpdateTimes` to 0 at the start of Test3 to clean the pollution from Test1 and Test2.","open","","LALAYANG","2021-08-11T17:45:32Z","2021-08-11T17:45:32Z"
"","3300","MAPREDUCE-7360. Clean shared state pollution to avoid flaky tests","Link to issue: [https://issues.apache.org/jira/browse/MAPREDUCE-7360](https://issues.apache.org/jira/browse/MAPREDUCE-7360) ## What is the purpose of this change This PR is to clean shared state pollution between tests: ``` Test1: org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier.testNotificationOnLastRetryShutdownWithRuntimeException Test2: org.apache.hadoop.mapreduce.v2.app.TestMRAppMaster.testMRAppMasterShutDownJob ``` - Test1 can pollute the shared state with Test2, which can lead Test2 to fail after running Test1. - It may be better to clean state pollutions so that some other tests won't fail in the future due to the shared state polluted by this test.  ## Reproduce test failure First run Test1, then run Test2 in the same JVM  ## Expected result The tests should run successfully when multiple tests that use this state are run in the same JVM.  ## Actual result Run Test1 and Test2, Test2 fails: ``` [ERROR]   TestMRAppMaster.testMRAppMasterShutDownJob:530 Expected shutDownJob to exit with status code of 0. expected:<0> but was:<1> ```  ## Fix Reset FirstExitException at the start of Test2 to clean the pollution from Test1.","open","","LALAYANG","2021-08-12T06:01:44Z","2021-08-12T07:28:59Z"
"","2986","YARN-9279. Remove the old hamlet package.","JIRA: YARN-9279  Remove the old hamlet package that is not actually used since HADOOP-11875 (3.0.0-beta1).","closed","","aajisaka","2021-05-06T23:33:22Z","2021-05-21T04:05:25Z"
"","3680","YARN-11007. Correct words in YARN documents","JIRA: YARN-11007  correct words to improve doc quality","closed","","GuoPhilipse","2021-11-18T12:07:50Z","2021-11-25T11:58:33Z"
"","2960","YARN-10756. Remove additional junit 4.11 dependency from javadoc.","JIRA: YARN-10756","closed","","aajisaka","2021-04-27T05:26:49Z","2021-05-06T22:53:13Z"
"","3053","MAPREDUCE-7348. TestFrameworkUploader#testNativeIO fails.","JIRA: MAPREDUCE-7348.  The behavior of `FileUtils#deleteDirectory` is still changed even in commons-io 2.8. Use `FileUtils#forceDelete` instead of `FileUtils#deleteDirectory` to clean up the test dir.","closed","","aajisaka","2021-05-26T05:04:10Z","2021-05-26T06:48:15Z"
"","3592","YARN-10991. Fix to ignore the grouping ""[]"" for resourcesStr in parse…","Jira: https://issues.apache.org/jira/browse/YARN-10991 Fix to ignore the grouping ""[]"" for resourcesStr in parseResourcesString method. Currently, if ""]"" is present at the end of resourcesStr, it is not ignored.","closed","","ashutoshcipher","2021-10-27T06:06:20Z","2021-11-24T09:41:38Z"
"","3591","YARN-10991. Fix to ignore the grouping ""[]"" properly in parseResource…","Jira: https://issues.apache.org/jira/browse/YARN-10991  Fix to ignore the grouping ""[]"" properly in parseResourcesString method","closed","","ashutoshcipher","2021-10-27T05:50:32Z","2021-10-27T07:39:49Z"
"","2733","YARN-10662. [JDK 11] TestTimelineReaderWebServicesHBaseStorage fails","JIRA: https://issues.apache.org/jira/browse/YARN-10662  - Upgrade jmockit to the latest version. - Set argLine (reference: https://jmockit.github.io/tutorial/Introduction.html#runningTests)  Manually tested with AdoptOpenJDK 11.0.9.","open","","aajisaka","2021-03-02T08:56:54Z","2021-03-02T08:56:54Z"
"","2725","YARN-10656. Parsing error in CapacityScheduler.md","JIRA: https://issues.apache.org/jira/browse/YARN-10656  Manually tested.","closed","","aajisaka","2021-02-26T18:08:07Z","2021-02-27T03:46:39Z"
"","2775","MAPREDUCE-7329: HadoopPipes task has failed because of the ping timeout exception","jira: https://issues.apache.org/jira/browse/MAPREDUCE-7329  **Hadoop Pipes Ping implement has a bug**.  Recently, we upgrade linux kernel version from 3.x to 4.x. And we find hadoop pipe task exit with connect timeout which is implemented by PingThread in HadoopPipes.cc.  ![image](https://user-images.githubusercontent.com/7459785/111252624-814ee580-864c-11eb-8475-2d0648c22d8f.png)  After a deep research, we finally find that current ping server won't accept ping client created socket, which may cause critical problem:  - it will cause tcp accept queue full(default 50) - when client close socket, server socket won't call close method, which will leave too many CLOSE_WAIT socket fd existed(default 2h), and accept queue never cleared. - Even worse, in 4.x linux kernel version, it will cause tcp drop packet directly which makes ping client connect time out. While In 3.x linux kernel version, when accept queue full, client can also make half connection till sync queue full (default 2048), so from client side, ping will aslo work till sync queue full. And after 3 hours, task will also exit with connect timeout exception.  To fix this bug, we introduced a PingSocketCleaner thread, which will continuously accept ping socket connection from ping client. When socket close from client,  cleaner thread will detecte closed inputStream through `read` method returns `-1`, then finally close socket from sever side.  Refrenced by linux kernel patch: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=5ea8ea2cb7","closed","","lichaojacobs","2021-03-15T06:52:45Z","2021-04-09T02:59:06Z"
"","2712","MAPREDUCE-7323. Remove job_history_summary.py.","JIRA: https://issues.apache.org/jira/browse/MAPREDUCE-7323  The script does not support Python 3 and is outdated.","closed","","aajisaka","2021-02-22T06:57:23Z","2021-02-22T14:15:16Z"
"","3586","HDFS-16284: Unboxing long value in old version's block report cause full GC","jira: https://issues.apache.org/jira/browse/HDFS-16284","open","","Cosss7","2021-10-26T06:20:13Z","2021-12-25T10:53:52Z"
"","2854","HDFS-15945. DataNodes with zero capacity and zero blocks should be decommissioned immediately.","JIRA: https://issues.apache.org/jira/browse/HDFS-15945","closed","","tasanuma","2021-04-02T08:14:06Z","2021-05-17T04:53:30Z"
"","2830","HDFS-15931. Fix non-static inner classes for better memory management","Jira: https://issues.apache.org/jira/browse/HDFS-15931","closed","","virajjasani","2021-03-29T06:33:26Z","2021-04-02T06:26:56Z"
"","2823","HDFS-15926 : Remove duplicate dependency of hadoop-annotations","Jira: https://issues.apache.org/jira/browse/HDFS-15926","closed","","virajjasani","2021-03-27T09:41:50Z","2021-03-29T18:10:33Z"
"","2772","HDFS-15895 : Remove redundant String#format in DFSAdmin#printOpenFiles","Jira: https://issues.apache.org/jira/browse/HDFS-15895","closed","","virajjasani","2021-03-14T11:54:04Z","2021-03-17T06:18:38Z"
"","2743","HDFS-15873. Add namenode address in logs for block report","JIRA: https://issues.apache.org/jira/browse/HDFS-15873  Add namenode address in logs for block report. It's easier to track when the block report was sent to ANN or SNN.","closed","","tomscut","2021-03-05T01:50:09Z","2021-03-09T01:07:56Z"
"","2739","HDFS-15870. Remove unused configuration dfs.namenode.stripe.min","JIRA: https://issues.apache.org/jira/browse/HDFS-15870  Remove unused configuration dfs.namenode.stripe.min.","closed","","tomscut","2021-03-03T07:26:44Z","2021-03-08T03:19:42Z"
"","2718","HDFS-15854. Make some parameters configurable for SlowDiskTracker and…","JIRA: https://issues.apache.org/jira/browse/HDFS-15854  Make some parameters configurable for SlowDiskTracker and SlowPeerTracker. Related to the previous issue: HDFS-15814.","closed","","tomscut","2021-02-23T13:19:24Z","2021-03-01T15:55:00Z"
"","2714","HDFS-15845. RBF: Router fails to start due to NoClassDefFoundError for hadoop-federation-balance.","JIRA: https://issues.apache.org/jira/browse/HDFS-15845","closed","","tasanuma","2021-02-22T08:33:01Z","2021-02-23T06:12:35Z"
"","2702","HDFS-15836. RBF: Fix contract tests after HADOOP-13327","JIRA: https://issues.apache.org/jira/browse/HDFS-15836  Fix the following tests: - TestRouterHDFSContractCreate - TestRouterHDFSContractCreateSecure - TestRouterWebHDFSContractCreate","closed","","aajisaka","2021-02-15T11:33:09Z","2021-02-17T11:18:51Z"
"","2696","HDFS-15834. Remove the usage of org.apache.log4j.Level","JIRA: https://issues.apache.org/jira/browse/HDFS-15834  There are still some usages of org.apache.log4j.Level. They cannot simply be removed because they are used with Log4J1 Appender API and so on. I think they can be removed when upgraded to Log4j2.","closed","","aajisaka","2021-02-12T07:35:05Z","2021-02-17T07:02:55Z"
"","3639","HADOOP-17999: Do not initialize all target FileSystems for setWriteChecksum and setVerifyChecksum","JIRA: https://issues.apache.org/jira/browse/HADOOP-17999    ### Description of PR All the target filesystems corresponding to mount points are initialized in setWriteChecksum and setVerifyChecksum methods. This PR initializes the filesystems lazily  ### How was this patch tested? Added unit tests. Also for large number of mount points, hadoop cli commands such as get, put or copyFromLocal were throwing OOM. With the fix, the commands were successful.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","abhishekdas99","2021-11-09T23:42:35Z","2022-01-31T22:34:02Z"
"","2766","HADOOP-17582. Replace GitHub App Token with GitHub OAuth token","JIRA: https://issues.apache.org/jira/browse/HADOOP-17582  Use the OAuth Token used before upgrading to Yetus 0.13.0.","closed","","aajisaka","2021-03-12T05:54:37Z","2021-03-12T09:04:12Z"
"","2764","HADOOP-17581. Fix reference to LOG is ambiguous after HADOOP-17482.","JIRA: https://issues.apache.org/jira/browse/HADOOP-17581","closed","","xiaoyuyao","2021-03-12T03:10:09Z","2021-03-15T20:02:55Z"
"","2745","HADOOP-17570. Apply YETUS-1102 to re-enable GitHub comments","JIRA: https://issues.apache.org/jira/browse/HADOOP-17570","closed","","aajisaka","2021-03-05T04:47:24Z","2021-03-11T12:49:04Z"
"","2776","HADOOP-17544. Mark KeyProvider as Stable.","JIRA: https://issues.apache.org/jira/browse/HADOOP-17544","closed","","aajisaka","2021-03-15T08:23:22Z","2021-08-30T00:56:12Z"
"","2708","HADOOP-17534. Update Jackson to 2.10.5 and Jackson databind to 2.10.5.1","JIRA: https://issues.apache.org/jira/browse/HADOOP-17534","closed","","aajisaka","2021-02-19T10:17:39Z","2021-02-22T08:33:46Z"
"","2753","HADOOP-16870. Use spotbugs-maven-plugin instead of findbugs-maven-plugin","JIRA: https://issues.apache.org/jira/browse/HADOOP-16870  In addition to #2454, upgraded SpotBugs to 4.2.2 and spotbugs-maven-plugin to 4.2.0 to fix https://github.com/spotbugs/spotbugs/issues/1161","closed","","aajisaka","2021-03-09T06:58:03Z","2021-03-11T01:56:23Z"
"","2709","HADOOP-16748. Migrate to Python 3 and upgrade Yetus to 0.13.0 (branch-3.3)","JIRA: https://issues.apache.org/jira/browse/HADOOP-16748  Test the precommit job.","closed","","aajisaka","2021-02-20T10:08:31Z","2021-02-22T06:19:13Z"
"","3302","HDFS-16173.Improve CopyCommands#Put#executor queue configurability.","JIRA: HDFS-16173  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-08-13T12:01:56Z","2021-08-27T03:41:44Z"
"","3104","HDFS-16068. WebHdfsFileSystem has a possible connection leak in connection with HttpFS","JIRA: HDFS-16068","closed","","tasanuma","2021-06-15T01:30:46Z","2021-06-15T08:20:36Z"
"","3079","HDFS-16050. Some dynamometer tests fail.","JIRA: HDFS-16050  After HDFS-15915, MiniDFSCluster requires mockito-core (Mockito 2.x) dependency. Added.","closed","","aajisaka","2021-06-07T01:56:56Z","2021-06-07T06:20:56Z"
"","3060","HDFS-16046. TestBalancerProcedureScheduler and TestDistCpProcedure timeout.","JIRA: HDFS-16046","closed","","aajisaka","2021-05-28T07:48:41Z","2021-05-29T14:05:02Z"
"","2989","HDFS-16001. TestOfflineEditsViewer.testStored() fails reading negative value of FSEditsLogOpCodes (branch-3.3)","JIRA: HDFS-16001  Backport of #2980","closed","","aajisaka","2021-05-07T16:33:25Z","2021-05-08T05:31:41Z"
"","2980","HDFS-16001. TestOfflineEditsViewer.testStored() fails reading negative value of FSEditsLogOpCodes","JIRA: HDFS-16001  - Updated the edits version to -67 (The latest version in trunk). - Built Hadoop and created the editsStored by `bin/hdfs oev -i editsStored.xml -o editsStored -p binary`  Since the edits version (-67) is not supported in branch-3.3, we need to create a separate PR for the branch.","closed","","aajisaka","2021-05-05T19:27:22Z","2021-05-07T15:24:24Z"
"","2970","HDFS-15952. TestRouterRpcMultiDestination#testProxyGetTransactionID and testProxyVersionRequest are flaky.","JIRA: HDFS-15952  Manually ran the flaky unit tests 5 times, and all passed.  The fix is based on #2860","closed","","aajisaka","2021-05-03T06:54:45Z","2021-05-03T15:25:46Z"
"","3190","HADOOP-17794. Add a sample configuration to use ZKDelegationTokenSecretManager in Hadoop KMS","JIRA: HADOOP-17794  Documented the following parameters: ``` hadoop.kms.authentication.zk-dt-secret-manager.enable hadoop.kms.authentication.zk-dt-secret-manager.kerberos.keytab hadoop.kms.authentication.zk-dt-secret-manager.kerberos.principal hadoop.kms.authentication.zk-dt-secret-manager.zkConnectionString hadoop.kms.authentication.zk-dt-secret-manager.znodeWorkingPath hadoop.kms.authentication.zk-dt-secret-manager.zkAuthType ``` They are required when setting up multiple KMS instances.","closed","","aajisaka","2021-07-08T15:42:10Z","2021-07-09T14:53:34Z"
"","3099","HADOOP-17760. Delete hadoop.ssl.enabled and dfs.https.enable from docs and core-default.xml","JIRA: HADOOP-17760","closed","","tasanuma","2021-06-11T08:20:37Z","2021-06-17T00:59:17Z"
"","3090","HADOOP-17756. Increase precommit job timeout from 20 hours to 24 hours.","JIRA: HADOOP-17756","closed","","tasanuma","2021-06-10T05:34:34Z","2021-06-14T01:18:06Z"
"","3083","HADOOP-17750. Fix asf license errors in newly added files by HADOOP-17727","JIRA: HADOOP-17750","closed","","tasanuma","2021-06-08T09:12:00Z","2021-06-09T08:21:00Z"
"","2983","HADOOP-17685. Fix junit deprecation warnings in hadoop-common module.","JIRA: HADOOP-17685  Clean up the use of JUnit API.","closed","","aajisaka","2021-05-06T10:43:55Z","2021-05-13T05:22:52Z"
"","2974","HADOOP-17683. Update commons-io to 2.8.0","JIRA: HADOOP-17683  Bumps commons-io from 2.5 to 2.7.   [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=commons-io:commons-io&package-manager=maven&previous-version=2.5&new-version=2.7)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","java,","dependabot[bot]","2021-05-04T08:42:06Z","2021-05-20T11:23:54Z"
"","2962","HADOOP-17674. Use spotbugs-maven-plugin in hadoop-huaweicloud.","JIRA: HADOOP-17674  Manually tested: ``` mvn spotbugs:spotbugs -pl hadoop-cloud-storage-project/hadoop-huaweicloud (snip) [INFO] --- spotbugs-maven-plugin:4.2.0:spotbugs (default-cli) @ hadoop-huaweicloud --- [INFO] Fork Value is true [INFO] Done SpotBugs Analysis.... [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  10.349 s [INFO] Finished at: 2021-04-27T19:45:10+09:00 [INFO] ------------------------------------------------------------------------ ```","closed","","aajisaka","2021-04-27T10:47:10Z","2021-04-28T01:03:52Z"
"","3025","HADOOP-17663. Remove useless property hadoop.assemblies.version in pom file.","JIRA: HADOOP-17663","closed","","aajisaka","2021-05-19T06:12:00Z","2021-05-20T01:47:48Z"
"","3274","HADOOP-17370. Upgrade commons-compress to 1.21","JIRA: HADOOP-17370","closed","","aajisaka","2021-08-06T08:34:21Z","2021-08-08T02:24:23Z"
"","3187","HADOOP-12665. Document hadoop.security.token.service.use_ip.","JIRA: HADOOP-12665  The description is based on https://issues.apache.org/jira/browse/HADOOP-12665?focusedCommentId=15088151&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15088151. Thank you @mattf-apache","closed","","aajisaka","2021-07-08T07:21:05Z","2021-07-12T01:17:07Z"
"","3689","HDFS-16339. Show the threshold when mover threads quota is exceeded","JIRA: [HDFS-16339](https://issues.apache.org/jira/browse/HDFS-16339).  Show the threshold when mover threads quota is exceeded in DataXceiver#replaceBlock and DataXceiver#copyBlock.","closed","","tomscut","2021-11-20T04:36:35Z","2021-11-26T11:08:41Z"
"","3682","HDFS-16337. Show start time of Datanode on Web","JIRA: [HDFS-16337](https://issues.apache.org/jira/browse/HDFS-16337).  Show start time of Datanode on Web.  ![image](https://user-images.githubusercontent.com/55134131/142522685-714cae02-841a-4ba2-9979-1a44141d3112.png)","closed","","tomscut","2021-11-19T01:08:45Z","2021-11-23T08:54:40Z"
"","3681","HDFS-16335. Fix HDFSCommands.md","JIRA: [HDFS-16335](https://issues.apache.org/jira/browse/HDFS-16335).  Fix HDFSCommands.md.","closed","","tomscut","2021-11-18T13:42:07Z","2021-11-22T08:54:38Z"
"","3679","HDFS-16333. fix balancer bug when transfer an EC block","JIRA: [HDFS-16333](https://issues.apache.org/jira/browse/HDFS-16333)  We set the EC policy to (6+3) and we also have nodes that were decommissioning when we executed balancer.  With the balancer running, we find many error logs as follow. ![image](https://user-images.githubusercontent.com/2844826/142404814-7c6e76c0-2891-40a5-8b91-5cfadee5ef9c.png)  Node A wants to transfer an EC block to node B, but we found that the block is not on node A. The FSCK command to show the block status as follow ![image](https://user-images.githubusercontent.com/2844826/142404846-14e5c40c-b109-40ba-85c4-9971d133c5f4.png)  In the dispatcher. getBlockList function ![image](https://user-images.githubusercontent.com/2844826/142404867-e13fe829-17b0-47ef-b468-b9185b669e4a.png)  Assume that the location of the an EC block in storageGroupMap look like this indices:[0, 1, 2, 3, 4, 5, 6, 7, 8] node:[a, b, c, d, e, f, g, h, i]  after decommission operation, the internal block on indices[1] were decommission to another node. indices:[0, 1, 2, 3, 4, 5, 6, 7, 8] node:[a, j, c, d, e, f, g, h, i] the location of indices[1] change from node b to node j.     When the balancer get the block location and check it with the location in storageGroupMap. If a node is not found in storageGroupMap, it will not be add to block locations. In this case, node j will not be added to the block locations, while the indices is not updated. Finally, the block location may look like this,  indices:[0, 1, 2, 3, 4, 5, 6, 7, 8] block.location:[a, c, d, e, f, g, h, i] the location of the nodes does not match their indices    Solution: we should update the indices and match with the nodes indices:[0, 2, 3, 4, 5, 6, 7, 8] block.location:[a, c, d, e, f, g, h, i]","closed","","liubingxing","2021-11-18T10:24:40Z","2021-12-09T08:08:10Z"
"","3676","HDFS-16331. Make dfs.blockreport.intervalMsec reconfigurable","JIRA: [HDFS-16331](https://issues.apache.org/jira/browse/HDFS-16331).  We have a cold data cluster, which stores as EC policy. There are 24 disks on each node and each disk is 7 TB.   Recently, many nodes have more than `10 million blocks`, and the interval of `FBR` is `6h` as default. Frequent FBR caused great pressure on NN.  We want to increase the interval of `FBR`, but have to rolling restart the DNs, this operation is very heavy. In this scenario, it is necessary to make `dfs.blockreport.intervalMsec` reconfigurable.  ![image](https://user-images.githubusercontent.com/55134131/142335617-6340aaa9-8c04-4a76-9bd6-29079d4e1ef4.png) ![image](https://user-images.githubusercontent.com/55134131/142335628-489aacc4-1858-466a-afc9-bbb8b5c7d17c.png)","closed","","tomscut","2021-11-18T01:49:56Z","2022-01-19T09:56:25Z"
"","3670","HDFS-16329. Fix log format for BlockManager","JIRA: [HDFS-16329](https://issues.apache.org/jira/browse/HDFS-16329).  Fix log format for BlockManager.","closed","","tomscut","2021-11-17T12:28:12Z","2021-11-19T00:49:53Z"
"","3663","HDFS-16326. Simplify the code for DiskBalancer","JIRA: [HDFS-16326](https://issues.apache.org/jira/browse/HDFS-16326).  Simplify the code for DiskBalancer.","closed","","tomscut","2021-11-16T00:47:29Z","2021-11-19T02:37:35Z"
"","3643","HDFS-16315. Add metrics related to Transfer and NativeCopy for DataNode","JIRA: [HDFS-16315](https://issues.apache.org/jira/browse/HDFS-16315).  Datanodes already have `Read`, `Write`, `Sync` and `Flush` metrics. We should add `NativeCopy` and `Transfer` as well.  Here is a partial look after the change: ![image](https://user-images.githubusercontent.com/55134131/141215328-9403c3d9-406b-417a-83ab-f93b19aa4bb7.png)","closed","","tomscut","2021-11-11T00:31:47Z","2021-11-16T06:36:48Z"
"","3637","HDFS-16312. Fix typo for DataNodeVolumeMetrics and ProfilingFileIoEvents","JIRA: [HDFS-16312](https://issues.apache.org/jira/browse/HDFS-16312).  Fix typo for DataNodeVolumeMetrics and ProfilingFileIoEvents.","closed","","tomscut","2021-11-09T12:41:07Z","2021-11-10T10:22:48Z"
"","3636","HDFS-16311. Metric metadataOperationRate calculation error in DataNod…","JIRA: [HDFS-16311](https://issues.apache.org/jira/browse/HDFS-16311).  Metric metadataOperationRate calculation error in DataNodeVolumeMetrics#addFileIoError, causing MetadataOperationRateAvgTime is very large in some cases.  ![image](https://user-images.githubusercontent.com/55134131/140923975-a60f6e25-a3af-4669-9e09-fe2801ae5a4b.png)","closed","","tomscut","2021-11-09T12:26:11Z","2021-11-12T01:32:28Z"
"","3635","HDFS-16310. RBF: Add client port to CallerContext for Router","JIRA: [HDFS-16310](https://issues.apache.org/jira/browse/HDFS-16310).  We mentioned in [#3538](https://github.com/apache/hadoop/pull/3538#issuecomment-955108462) that adding the client port to the CallerContext of the Router.","closed","","tomscut","2021-11-09T11:20:07Z","2021-11-18T05:26:18Z"
"","3616","HDFS-16299. Fix bug for TestDataNodeVolumeMetrics#verifyDataNodeVolum…","JIRA: [HDFS-16299](https://issues.apache.org/jira/browse/HDFS-16299)  Fix bug for TestDataNodeVolumeMetrics#verifyDataNodeVolumeMetrics.","closed","","tomscut","2021-11-04T07:35:12Z","2021-11-09T12:46:01Z"
"","3615","HDFS-16298. Improve error msg for BlockMissingException","JIRA: [HDFS-16298](https://issues.apache.org/jira/browse/HDFS-16298)  When the client fails to obtain a block, a BlockMissingException is thrown. To analyze the issues, we can add the relevant location information to error msg of BlockMissingException.  ![image](https://user-images.githubusercontent.com/55134131/140273974-f5c726e3-02e7-4260-8149-4d68c510fc53.png)","closed","","tomscut","2021-11-04T07:29:33Z","2021-11-10T11:08:28Z"
"","3574","HDFS-16281. Fix flaky unit tests failed due to timeout","JIRA: [HDFS-16281](https://issues.apache.org/jira/browse/HDFS-16281)  I found that this unit test `TestViewFileSystemOverloadSchemeWithHdfsScheme` failed several times due to timeout. Can we change the timeout for some methods from `3s` to `30s` to be consistent with the other methods?  ``` [ERROR] Tests run: 19, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 65.39 s <<< FAILURE! - in org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeWithMountTableConfigInHDFS [ERROR] testNflyRepair(org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeWithMountTableConfigInHDFS)  Time elapsed: 4.132 s  <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 3000 milliseconds 	at java.lang.Object.wait(Native Method) 	at java.lang.Object.wait(Object.java:502) 	at org.apache.hadoop.util.concurrent.AsyncGet$Util.wait(AsyncGet.java:59) 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577) 	at org.apache.hadoop.ipc.Client.call(Client.java:1535) 	at org.apache.hadoop.ipc.Client.call(Client.java:1432) 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242) 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129) 	at com.sun.proxy.$Proxy26.setTimes(Unknown Source) 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setTimes(ClientNamenodeProtocolTranslatorPB.java:1059) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:431) 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) 	at com.sun.proxy.$Proxy27.setTimes(Unknown Source) 	at org.apache.hadoop.hdfs.DFSClient.setTimes(DFSClient.java:2658) 	at org.apache.hadoop.hdfs.DistributedFileSystem$37.doCall(DistributedFileSystem.java:1978) 	at org.apache.hadoop.hdfs.DistributedFileSystem$37.doCall(DistributedFileSystem.java:1975) 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) 	at org.apache.hadoop.hdfs.DistributedFileSystem.setTimes(DistributedFileSystem.java:1988) 	at org.apache.hadoop.fs.FilterFileSystem.setTimes(FilterFileSystem.java:542) 	at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.setTimes(ChRootedFileSystem.java:328) 	at org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream.commit(NflyFSystem.java:439) 	at org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream.close(NflyFSystem.java:395) 	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77) 	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106) 	at org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme.writeString(TestViewFileSystemOverloadSchemeWithHdfsScheme.java:685) 	at org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme.testNflyRepair(TestViewFileSystemOverloadSchemeWithHdfsScheme.java:622) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```","closed","","tomscut","2021-10-21T12:19:55Z","2021-10-26T06:54:47Z"
"","3568","HDFS-16280. Fix typo for ShortCircuitReplica#isStale","JIRA: [HDFS-16280](https://issues.apache.org/jira/browse/HDFS-16280)  Fix typo for ShortCircuitReplica#isStale.","closed","","tomscut","2021-10-20T06:31:28Z","2021-10-22T05:09:03Z"
"","3564","HDFS-16279. Print detail datanode info when process first storage report","JIRA: [HDFS-16279](https://issues.apache.org/jira/browse/HDFS-16279)  Print detail datanode info when process first storage report.  ![image](https://user-images.githubusercontent.com/55134131/137911057-41c5eea3-d36f-40d1-a910-22274380c682.png)","closed","","tomscut","2021-10-19T12:41:42Z","2021-10-29T01:30:58Z"
"","3554","HDFS-16274. Improve error msg for FSNamesystem#startFileInt","JIRA: [HDFS-16274](https://issues.apache.org/jira/browse/HDFS-16274)  When the blocksize of a file is smaller than dfs.namenode.fs-limits.min-block-size, an IOE will be thrown. In current exception messages, it is easy to confuse the value of blocksize with the value of dfs.namenode.fs-limits.min-block-size.   Before the change: ![image](https://user-images.githubusercontent.com/55134131/137355106-fb0ce6cb-7378-49da-b425-f41fa2bb2834.png)  After the change: ![image](https://user-images.githubusercontent.com/55134131/137355124-b79e638c-a3a3-47af-ad7f-b339cd39c426.png)","closed","","tomscut","2021-10-14T16:06:41Z","2021-10-15T20:25:09Z"
"","3538","HDFS-16266. Add remote port information to HDFS audit log","JIRA: [HDFS-16266](https://issues.apache.org/jira/browse/HDFS-16266)  In our production environment, we occasionally encounter a problem where a user submits an abnormal computation task, causing a sudden flood of requests, which causes the queueTime and processingTime of the Namenode to rise very high, causing a large backlog of tasks.  We usually locate and kill specific Spark, Flink, or MapReduce tasks based on metrics and audit logs. Currently, IP and UGI are recorded in audit logs, but there is no port information, so it is difficult to locate specific processes sometimes. Therefore, I propose that we add the port information to the audit log, so that we can easily track the upstream process.  Currently, some projects contain port information in audit logs, such as Hbase and Alluxio. I think it is also necessary to add port information for HDFS audit logs.  Before: ![before-hdfs-audit-log](https://user-images.githubusercontent.com/55134131/136699770-e5c07e90-0046-43ba-8c1c-a0e94a02657d.jpg)   After: ![hdfs-audit-log](https://user-images.githubusercontent.com/55134131/136699624-13bb3375-398b-473f-9f8a-212bdd5ec765.jpg)","closed","","tomscut","2021-10-10T14:23:10Z","2021-11-04T01:24:36Z"
"","3456","HDFS-16232. Fix java doc for BlockReaderRemote#newBlockReader","JIRA: [HDFS-16232](https://issues.apache.org/jira/browse/HDFS-16232)  Add some parameter descriptions for BlockReaderRemote#newBlockReader.","closed","","tomscut","2021-09-20T01:47:28Z","2021-09-24T06:08:00Z"
"","3428","HDFS-16225. Fix typo for FederationTestUtils","JIRA: [HDFS-16225](https://issues.apache.org/jira/browse/HDFS-16225)  Fix typo for FederationTestUtils.","closed","","tomscut","2021-09-13T11:16:34Z","2021-09-14T08:59:05Z"
"","3378","HDFS-16209. Add description for dfs.namenode.caching.enabled","JIRA: [HDFS-16209](https://issues.apache.org/jira/browse/HDFS-16209)  **Namenode config:** dfs.namenode.write-lock-reporting-threshold-ms=50ms dfs.namenode.caching.enabled=true (default)  In fact, the caching feature is not used in our cluster, but this switch is turned on by default(dfs.namenode.caching.enabled=true), incurring some additional write lock overhead.   **We count the number of write lock warnings in a log file, and find that the number of rescan cache warnings reaches about **32%,** which greatly affects the performance of Namenode.** ![namenode-write-lock](https://user-images.githubusercontent.com/55134131/131950567-e18606dd-9c48-4219-b3c1-142424821f50.jpg)  **We should set 'dfs.namenode.caching.enabled' to false by default and turn it on when we wants to use it.**","closed","","tomscut","2021-09-03T04:28:55Z","2021-09-08T05:00:44Z"
"","3366","HDFS-16203. Discover datanodes with unbalanced block pool usage by th…","JIRA: [HDFS-16203](https://issues.apache.org/jira/browse/HDFS-16203)  **Discover datanodes with unbalanced volume usage by the standard deviation.**  **In some scenarios, we may cause unbalanced datanode disk usage:** 1. Repair the damaged disk and make it online again. 2. Add disks to some Datanodes. 3. Some disks are damaged, resulting in slow data writing. 4. Use some custom volume choosing policies.  In the case of unbalanced disk usage, a sudden increase in datanode write traffic may result in busy disk I/O with low volume usage, resulting in decreased throughput across datanodes.  We need to find these nodes in time to do diskBalance, or other processing. Based on the volume usage of each datanode, we can calculate the standard deviation of the volume usage. The more unbalanced the volume, the higher the standard deviation.  **We can display the result on the Web of namenode, and then sorting directly to find the nodes where the volumes usages are unbalanced.**  **This interface is only used to obtain metrics and does not adversely affect namenode performance.**  ![image](https://user-images.githubusercontent.com/55134131/131662801-5e31d50d-e647-477c-a6a1-ade4a0f69c58.png)   **There are two NS in our test cluster. NS1 supports this feature, and NS2 does not. In this case, the Router UI is shown as follows:** ![image](https://user-images.githubusercontent.com/55134131/133259138-ea2fccc8-787c-4770-98cc-557d23e6fe70.png)","closed","","tomscut","2021-09-01T11:22:11Z","2021-09-16T01:06:49Z"
"","3354","HDFS-16194. Simplify the code with DatanodeID#getXferAddrWithHostname","JIRA: [HDFS-16194](https://issues.apache.org/jira/browse/HDFS-16194)  Simplify the code with DatanodeID#getXferAddrWithHostname.","closed","","tomscut","2021-08-29T03:26:34Z","2021-09-05T06:19:02Z"
"","3313","HDFS-16179. Update loglevel for BlockManager#chooseExcessRedundancySt…","JIRA: [HDFS-16179](https://issues.apache.org/jira/browse/HDFS-16179)  ``` private void chooseExcessRedundancyStriped(BlockCollection bc,     final Collection nonExcess,     BlockInfo storedBlock,     DatanodeDescriptor delNodeHint) {   ...   // cardinality of found indicates the expected number of internal blocks   final int numOfTarget = found.cardinality();   final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(       bc.getStoragePolicyID());   final List excessTypes = storagePolicy.chooseExcess(       (short) numOfTarget, DatanodeStorageInfo.toStorageTypes(nonExcess));   if (excessTypes.isEmpty()) {     LOG.warn(""excess types chosen for block {} among storages {} is empty"",         storedBlock, nonExcess);     return;   }   ... } ```  IMO, here is just detecting excess StorageType and setting the log level to debug has no effect.   We have a cluster that uses the EC policy to store data. The current log level is WARN here, and in about 50 minutes, 286,093 logs are printed, which can cause other important logs to drown out.  ![image](https://user-images.githubusercontent.com/55134131/130005007-ea4cd5d5-f18a-4a06-855e-ba1c98a93fcb.png)  ![image](https://user-images.githubusercontent.com/55134131/130005037-57bc2116-68de-48cd-a81c-52f866336751.png)","open","","tomscut","2021-08-19T03:54:37Z","2022-08-03T08:55:33Z"
"","3310","HDFS-16177. Bug fix for Util#receiveFile","JIRA: [HDFS-16177](https://issues.apache.org/jira/browse/HDFS-16177)  The time to write file was miscalculated in Util#receiveFile.  ![image](https://user-images.githubusercontent.com/55134131/129893185-50226ae5-a9de-4066-adfa-53812c8d967b.png)","closed","","tomscut","2021-08-18T11:46:59Z","2021-08-19T04:30:39Z"
"","3291","HDFS-16160. Improve the parameter annotation in DatanodeProtocol#send…","JIRA: [HDFS-16160](https://issues.apache.org/jira/browse/HDFS-16160)  Improve the parameter annotation in DatanodeProtocol#sendHeartbeat.","closed","","tomscut","2021-08-11T01:31:07Z","2021-08-19T01:53:01Z"
"","3288","HDFS-16158. Discover datanodes with unbalanced volume usage by the st…","JIRA: [HDFS-16158](https://issues.apache.org/jira/browse/HDFS-16158)  Discover datanodes with unbalanced volume usage by the standard deviation  In some scenarios, we may cause unbalanced datanode disk usage: 1. Repair the damaged disk and make it online again. 2. Add disks to some Datanodes. 3. Some disks are damaged, resulting in slow data writing. 4. Use some custom volume choosing policies.  In the case of unbalanced disk usage, a sudden increase in datanode write traffic may result in busy disk I/O with low volume usage, resulting in decreased throughput across datanodes.  In this case, we need to find these nodes in time to do diskBalance, or other processing. Based on the volume usage of each datanode, we can calculate the standard deviation of the volume usage. The more unbalanced the volume, the higher the standard deviation.  To prevent the namenode from being too busy, we can calculate the standard variance on the datanode side, transmit it to the namenode through heartbeat, and display the result on the Web of namenode. We can then sort directly to find the nodes on the Web where the volumes usages are unbalanced.  ![namenode-web](https://user-images.githubusercontent.com/55134131/128893408-14685fac-25bf-4661-9f28-6978052baaba.jpg)","closed","","tomscut","2021-08-10T15:18:34Z","2021-09-01T00:37:53Z"
"","3211","HDFS-16131. Show storage type for failed volumes on namenode web","JIRA: [HDFS-16131](https://issues.apache.org/jira/browse/HDFS-16131)  To make it easy to query the storage type for failed volumes,  we can display them on namenode web.","closed","","tomscut","2021-07-16T12:40:07Z","2021-07-26T06:26:23Z"
"","3191","HDFS-16122. Fix DistCpContext#toString()","JIRA: [HDFS-16122](https://issues.apache.org/jira/browse/HDFS-16122)","closed","","tomscut","2021-07-09T11:51:36Z","2021-07-10T15:53:21Z"
"","3174","HDFS-16110. Remove unused method reportChecksumFailure in DFSClient","JIRA: [HDFS-16110](https://issues.apache.org/jira/browse/HDFS-16110)  Remove unused method reportChecksumFailure and fix some code styles by the way in DFSClient.","closed","","tomscut","2021-07-04T13:54:32Z","2021-07-06T02:41:17Z"
"","3172","HDFS-16109. Fix flaky some unit tests since they offen timeout","JIRA: [HDFS-16109](https://issues.apache.org/jira/browse/HDFS-16109)  Increase timeout for TestBootstrapStandby, TestFsVolumeList and TestDecommissionWithBackoffMonitor since they offen timeout.  TestBootstrapStandby: ``` [ERROR] Tests run: 8, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 159.474 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby [ERROR] testRateThrottling(org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby)  Time elapsed: 31.262 s  <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 30000 milliseconds 	at java.io.RandomAccessFile.writeBytes(Native Method) 	at java.io.RandomAccessFile.write(RandomAccessFile.java:512) 	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.tryLock(Storage.java:947) 	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:910) 	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:699) 	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:642) 	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:387) 	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:243) 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1224) 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:795) 	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:673) 	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:760) 	at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:1014) 	at org.apache.hadoop.hdfs.server.namenode.NameNode.(NameNode.java:989) 	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1763) 	at org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:2261) 	at org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:2231) 	at org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby.testRateThrottling(TestBootstrapStandby.java:297) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```  TestFsVolumeList: ``` [ERROR] Tests run: 12, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 190.294 s <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList [ERROR] testAddRplicaProcessorForAddingReplicaInMap(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList)  Time elapsed: 60.028 s  <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 60000 milliseconds 	at sun.misc.Unsafe.park(Native Method) 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429) 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) 	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList.testAddRplicaProcessorForAddingReplicaInMap(TestFsVolumeList.java:395) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```  TestDecommission: ``` [ERROR] Tests run: 28, Failures: 0, Errors: 2, Skipped: 1, Time elapsed: 676.729 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor [ERROR] testDecommissionWithCloseFileAndListOpenFiles(org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor)  Time elapsed: 180.686 s  <<< ERROR! org.junit.runners.model.TestTimedOutException: test timed out after 180000 milliseconds 	at java.lang.Thread.sleep(Native Method) 	at org.apache.hadoop.hdfs.AdminStatesBaseTest.waitNodeState(AdminStatesBaseTest.java:346) 	at org.apache.hadoop.hdfs.AdminStatesBaseTest.waitNodeState(AdminStatesBaseTest.java:333) 	at org.apache.hadoop.hdfs.TestDecommission.testDecommissionWithCloseFileAndListOpenFiles(TestDecommission.java:912) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:288) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:282) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```","closed","","tomscut","2021-07-03T01:50:18Z","2021-07-04T23:14:38Z"
"","3168","HDFS-16106. Fix flaky unit test TestDFSShell","JIRA: [HDFS-16106](https://issues.apache.org/jira/browse/HDFS-16106)  This unit test occasionally fails.  The value set for dfs.namenode.accesstime.precision is too low, result in the execution of the method, accesstime could be set many times, eventually leading to failed assert.  IMO, dfs.namenode.accesstime.precision should be greater than or equal to the timeout(120s) of TestDFSShell#testCopyCommandsWithPreserveOption(), or directly set to 0 to disable this feature.  ```[ERROR] Tests run: 52, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 106.778 s <<< FAILURE! - in org.apache.hadoop.hdfs.TestDFSShell [ERROR] testCopyCommandsWithPreserveOption(org.apache.hadoop.hdfs.TestDFSShell)  Time elapsed: 2.353 s  <<< FAILURE! java.lang.AssertionError: expected:<1625095098319> but was:<1625095099374> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption(TestDFSShell.java:2282) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748)  [ERROR] testCopyCommandsWithPreserveOption(org.apache.hadoop.hdfs.TestDFSShell)  Time elapsed: 2.467 s  <<< FAILURE! java.lang.AssertionError: expected:<1625095192527> but was:<1625095193950> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption(TestDFSShell.java:2323) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748)  [ERROR] testCopyCommandsWithPreserveOption(org.apache.hadoop.hdfs.TestDFSShell)  Time elapsed: 2.173 s  <<< FAILURE! java.lang.AssertionError: expected:<1625095196756> but was:<1625095197975> 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:647) 	at org.junit.Assert.assertEquals(Assert.java:633) 	at org.apache.hadoop.hdfs.TestDFSShell.testCopyCommandsWithPreserveOption(TestDFSShell.java:2303) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748)```","closed","","tomscut","2021-07-02T01:27:19Z","2021-07-02T07:35:49Z"
"","3160","HDFS-16104. Remove unused parameter and fix java doc for DiskBalancerCLI","JIRA: [HDFS-16104](https://issues.apache.org/jira/browse/HDFS-16104)  Remove unused parameter and fix java doc for DiskBalancerCLI.","closed","","tomscut","2021-06-30T10:27:39Z","2021-07-01T14:46:35Z"
"","3146","HDFS-16089. EC: Add metric EcReconstructionValidateTimeMillis for Striped…","JIRA: [HDFS-16089](https://issues.apache.org/jira/browse/HDFS-16089)  Add metric EcReconstructionValidateTimeMillis for StripedBlockReconstructor, so that we can count the elapsed time for striped block reconstructing.","closed","","tomscut","2021-06-26T07:59:38Z","2021-06-29T10:34:36Z"
"","3140","HDFS-16088. Standby NameNode process getLiveDatanodeStorageReport req…","JIRA: [HDFS-16088](https://issues.apache.org/jira/browse/HDFS-16088)  As with [HDFS-13183](https://issues.apache.org/jira/browse/HDFS-13183) , NameNodeConnector#getLiveDatanodeStorageReport() can also request to SNN to reduce the ANN load.  There are two points that need to be mentioned: 1. FSNamesystem#getDatanodeStorageReport() is OperationCategory.UNCHECKED, so we can access SNN directly. 2. We can share the same UT(testBalancerRequestSBNWithHA) with NameNodeConnector#getBlocks().","closed","","tomscut","2021-06-25T03:03:06Z","2021-07-08T06:14:03Z"
"","3136","HDFS-16086. Add volume information to datanode log for tracing","JIRA: [HDFS-16086](https://issues.apache.org/jira/browse/HDFS-16086)  To keep track of the block in volume, we can add the volume information to the datanode log.","closed","","tomscut","2021-06-24T01:27:03Z","2021-07-01T06:23:43Z"
"","3134","HDFS-16085. Move the getPermissionChecker out of the read lock","JIRA: [HDFS-16085](https://issues.apache.org/jira/browse/HDFS-16085)  Move the getPermissionChecker out of the read lock in NamenodeFsck#getBlockLocations() since the operation does not need to be locked.","closed","","tomscut","2021-06-23T01:57:55Z","2021-06-25T06:03:03Z"
"","3120","HDFS-16079. Improve the block state change log","JIRA: [HDFS-16079](https://issues.apache.org/jira/browse/HDFS-16079)  Improve the block state change log. Add readOnlyReplicas and replicasOnStaleNodes.","closed","","tomscut","2021-06-18T12:24:24Z","2021-06-21T01:23:37Z"
"","3119","HDFS-16078. Remove unused parameters for DatanodeManager.handleLifeli…","JIRA: [HDFS-16078](https://issues.apache.org/jira/browse/HDFS-16078)  Remove unused parameters (blockPoolId, maxTransfers) for DatanodeManager.handleLifeline().","closed","","tomscut","2021-06-18T11:39:00Z","2021-06-20T05:38:47Z"
"","3117","HDFS-16076. Avoid using slow DataNodes for reading by sorting locations","JIRA: [HDFS-16076](https://issues.apache.org/jira/browse/HDFS-16076)  After sorting the expected location list will be: live -> slow -> stale -> staleAndSlow -> entering_maintenance -> decommissioned. This reduces the probability that slow nodes will be used for reading.","closed","","tomscut","2021-06-17T15:38:16Z","2021-06-24T02:29:53Z"
"","3084","HDFS-16057. Make sure the order for location in ENTERING_MAINTENANCE …","JIRA: [HDFS-16057](https://issues.apache.org/jira/browse/HDFS-16057).  We use compactor to sort locations in getBlockLocations(), and the expected result is: live -> stale -> entering_maintenance -> decommissioned.  But the networktopology.SortByDistance() will disrupt the order. We should also filtered out node in sate  AdminStates.ENTERING_MAINTENANCE before networktopology.SortByDistance().  org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager#sortLocatedBlock() ``` DatanodeInfoWithStorage[] di = lb.getLocations(); // Move decommissioned/stale datanodes to the bottom Arrays.sort(di, comparator);  // Sort nodes by network distance only for located blocks int lastActiveIndex = di.length - 1; while (lastActiveIndex > 0 && isInactive(di[lastActiveIndex])) {   --lastActiveIndex; } int activeLen = lastActiveIndex + 1; if(nonDatanodeReader) {   networktopology.sortByDistanceUsingNetworkLocation(client,       lb.getLocations(), activeLen, createSecondaryNodeSorter()); } else {   networktopology.sortByDistance(client, lb.getLocations(), activeLen,       createSecondaryNodeSorter()); } ```","closed","","tomscut","2021-06-08T09:22:03Z","2021-06-11T08:48:06Z"
"","3062","HDFS-16048. RBF: Print network topology on the router web","JIRA: [HDFS-16048](https://issues.apache.org/jira/browse/HDFS-16048)  In order to query the network topology information conveniently, we can print it on the router web. It's related to [HDFS-15970](https://issues.apache.org/jira/browse/HDFS-15970).","closed","","tomscut","2021-05-30T09:22:38Z","2021-06-08T07:31:28Z"
"","2933","HDFS-15991. Add location into datanode info for NameNodeMXBean","JIRA: [HDFS-15991](https://issues.apache.org/jira/browse/HDFS-15991)","closed","","tomscut","2021-04-19T15:04:04Z","2021-04-26T11:09:48Z"
"","2907","HDFS-15975. Use LongAdder instead of AtomicLong","JIRA: [HDFS-15975](https://issues.apache.org/jira/browse/HDFS-15975)  When counting some indicators, we can use LongAdder instead of AtomicLong to improve performance. The long value is not an atomic snapshot in LongAdder, but I think we can tolerate that.","closed","","tomscut","2021-04-14T14:29:57Z","2021-04-17T15:05:02Z"
"","2940","HDFS-15975. Use LongAdder instead of AtomicLong for branch-3.3","JIRA: [HDFS-15975](https://issues.apache.org/jira/browse/HDFS-15975)","closed","","tomscut","2021-04-21T04:13:38Z","2021-04-28T01:39:04Z"
"","2936","HDFS-15975. Use LongAdder instead of AtomicLong","JIRA: [HDFS-15975](https://issues.apache.org/jira/browse/HDFS-15975)","closed","","tomscut","2021-04-20T04:18:25Z","2021-04-21T01:44:15Z"
"","2930","HDFS-15975. Use LongAdder instead of AtomicLong","JIRA: [HDFS-15975](https://issues.apache.org/jira/browse/HDFS-15975)","closed","","tomscut","2021-04-19T03:48:17Z","2021-04-20T04:13:17Z"
"","2896","HDFS-15970. Print network topology on the web","JIRA: [HDFS-15970](https://issues.apache.org/jira/browse/HDFS-15970)  In order to query the network topology information conveniently, we can print it on the web.","closed","","tomscut","2021-04-12T13:10:45Z","2021-04-19T14:35:22Z"
"","2859","HDFS-15951. Remove unused parameters in NameNodeProxiesClient","JIRA: [HDFS-15951](https://issues.apache.org/jira/browse/HDFS-15951)  Remove unused parameters in org.apache.hadoop.hdfs.NameNodeProxiesClient.","closed","","tomscut","2021-04-03T12:22:06Z","2021-04-05T15:16:09Z"
"","2855","HDFS-15946. Fix java doc in FSPermissionChecker","JIRA: [HDFS-15946](https://issues.apache.org/jira/browse/HDFS-15946)  Fix java doc for org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker#hasAclPermission.","closed","","tomscut","2021-04-02T09:18:03Z","2021-04-03T03:33:02Z"
"","2837","HDFS-15938. Fix java doc in FSEditLog","JIRA: [HDFS-15938](https://issues.apache.org/jira/browse/HDFS-15938)  Fix java doc in org.apache.hadoop.hdfs.server.namenode.FSEditLog#logAddCacheDirectiveInfo.","closed","","tomscut","2021-03-30T11:20:07Z","2021-04-02T02:38:20Z"
"","2788","HDFS-15906. Close FSImage and FSNamesystem after formatting is complete","JIRA: [HDFS-15906](https://issues.apache.org/jira/browse/HDFS-15906)  We should close FSImage and FSNamesystem after formatting is complete.","closed","","tomscut","2021-03-19T11:27:08Z","2021-03-22T15:34:47Z"
"","2770","HDFS-15892. Add metric for editPendingQ in FSEditLogAsync","JIRA: [HDFS-15892](https://issues.apache.org/jira/browse/HDFS-15892)  To monitor editPendingQ in FSEditLogAsync, we add a metric  and print log when the queue is full.","closed","","tomscut","2021-03-12T16:39:48Z","2021-04-02T01:34:02Z"
"","2754","HDFS-15884. RBF: Remove unused method getCreateLocation in RouterRpcS…","JIRA: [HDFS-15884](https://issues.apache.org/jira/browse/HDFS-15884)  Remove unused method `org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer#getCreateLocation`.","closed","","tomscut","2021-03-09T15:18:34Z","2021-03-14T02:25:48Z"
"","2752","HDFS-15883. Add a metric BlockReportQueueFullCount","JIRA: [HDFS-15883](https://issues.apache.org/jira/browse/HDFS-15883)  Add a metric that reflects the number of times the block report queue is full","closed","","tomscut","2021-03-07T15:12:04Z","2021-03-13T07:28:05Z"
"","2928","HDFS-15879. Exclude slow nodes when choose targets for blocks","JIRA: [HDFS-15879](https://issues.apache.org/jira/browse/HDFS-15879)  This feature has been merge into the trunk. This PR is provided for branch-3.3.","closed","","tomscut","2021-04-19T02:15:15Z","2022-04-29T09:08:43Z"
"","2748","HDFS-15879. Exclude slow nodes when choose targets for blocks","JIRA: [HDFS-15879](https://issues.apache.org/jira/browse/HDFS-15879)  Previously, we have monitored the slow nodes, related to [HDFS-11194](https://issues.apache.org/jira/browse/HDFS-11194)  We can use a thread to periodically collect these slow nodes into a set. Then use the set to filter out slow nodes when choose targets for blocks.  This feature can be configured to be turned on when needed.","closed","","tomscut","2021-03-06T01:44:55Z","2021-03-27T11:06:57Z"
"","3021","HDFS-15814. Make some parameters configurable for DataNodeDiskMetrics…","JIRA: [HDFS-15814](https://issues.apache.org/jira/browse/HDFS-15814)  This change was fixed in trunk (3.4.0). The commit does not apply cleanly to branch-3.3, so I create a new PR.","closed","","tomscut","2021-05-18T03:04:58Z","2021-06-08T07:32:49Z"
"","3647","HADOOP-18005. Correct log format for LdapGroupsMapping","JIRA: [HADOOP-18005](https://issues.apache.org/jira/browse/HADOOP-18005).","closed","","tomscut","2021-11-12T01:26:10Z","2021-11-12T22:11:20Z"
"","3644","HADOOP-18003. Add a method appendIfAbsent for CallerContext","JIRA: [HADOOP-18003](https://issues.apache.org/jira/browse/HADOOP-18003).  As we discussed here [#3635](https://github.com/apache/hadoop/pull/3635#discussion_r746873078).  In some cases, when we need to add a `key:value` to the `CallerContext`, we need to check whether the key already exists in the outer layer, which is a bit of a hassle. To solve this problem, we can add a new method `CallerContext#appendIfAbsent`.","closed","","tomscut","2021-11-11T01:51:51Z","2021-11-15T15:56:41Z"
"","3485","HADOOP-17938. Print lockWarningThreshold in InstrumentedLock#logWarni…","JIRA: [HADOOP-17938](https://issues.apache.org/jira/browse/HADOOP-17938)  Print lockWarningThreshold in InstrumentedLock#logWarning and InstrumentedLock#logWaitWarning.  Before: ![before](https://user-images.githubusercontent.com/55134131/134805447-8f0b966a-c351-4ce6-bc2a-e4bfc302ce2d.jpg)  After: ![image](https://user-images.githubusercontent.com/55134131/134941500-c6b656e8-ea12-4bd1-939a-f4518e1fb4dc.png)","closed","","tomscut","2021-09-26T11:16:15Z","2021-10-03T04:03:02Z"
"","3436","HADOOP-17914. Print RPC response length in the exception message","JIRA: [HADOOP-17914](https://issues.apache.org/jira/browse/HADOOP-17914)  To facilitate problem tracking, we can print RPC Response Length in the exception message.","closed","","tomscut","2021-09-15T11:46:07Z","2021-09-17T06:49:38Z"
"","3189","HADOOP-17793. Better token validation","JIRA: [HADOOP-17793](https://issues.apache.org/jira/browse/HADOOP-17793)  Replaced `Arrays.equals()` with `MessageDigest.isEqual()` in several methods that validate tokens.","closed","","artem-smotrakov","2021-07-08T07:43:02Z","2021-07-10T08:46:06Z"
"","3089","HADOOP-17754. Remove lock contention in overlay of Configuration","JIRA: [HADOOP-17754](https://issues.apache.org/jira/browse/HADOOP-17754)","open","","liangxs","2021-06-09T09:43:01Z","2021-06-09T12:50:45Z"
"","3085","HADOOP-17752. Remove lock contention in REGISTRY of Configuration","JIRA: [HADOOP-17752](https://issues.apache.org/jira/browse/HADOOP-17752)","open","","liangxs","2021-06-09T04:11:46Z","2021-06-11T10:03:44Z"
"","3080","HADOOP-17749. Remove lock contention in SelectorPool of SocketIOWithTimeout","JIRA: [HADOOP-17749](https://issues.apache.org/jira/browse/HADOOP-17749)","closed","","liangxs","2021-06-07T05:03:27Z","2021-07-06T01:43:46Z"
"","3328","HDFS-16183. RBF: Delete  unnecessary calls of proxyOpComplete in routerRpcClient","JIRA:  https://issues.apache.org/jira/browse/HDFS-16183","open","","wzhallright","2021-08-24T12:22:48Z","2021-09-03T02:29:25Z"
"","2744","HDFS-15874: Extend TopMetrics to support callerContext aggregation.","jira: [HDFS-15874](https://issues.apache.org/jira/browse/HDFS-15874)","closed","","zhuqi-lucas","2021-03-05T04:00:02Z","2021-05-14T07:11:42Z"
"","2803","YARN-10708. Remove NULL checks before instanceof","JIRA ISSUE: [https://issues.apache.org/jira/browse/YARN-10708](https://issues.apache.org/jira/browse/YARN-10708)","open","","TJJack","2021-03-23T06:38:51Z","2021-04-25T07:19:57Z"
"","2805","HDFS-15913. Remove useless NULL checks before instanceof","JIRA ISSUE: [https://issues.apache.org/jira/browse/HDFS-15913](https://issues.apache.org/jira/browse/HDFS-15913)","closed","","TJJack","2021-03-23T07:47:16Z","2021-03-23T15:52:56Z"
"","2810","HADOOP-17600. Replace Time.now() with Time.monotonicNow() for measuring time durations","JIRA ISSUE: [https://issues.apache.org/jira/browse/HADOOP-17600](https://issues.apache.org/jira/browse/HADOOP-17600)","open","","TJJack","2021-03-24T03:39:23Z","2021-03-24T15:19:53Z"
"","2804","HADOOP-17599. Remove NULL checks before instanceof","JIRA ISSUE: [https://issues.apache.org/jira/browse/HADOOP-17599](https://issues.apache.org/jira/browse/HADOOP-17599)","closed","","TJJack","2021-03-23T07:22:24Z","2021-03-23T15:46:11Z"
"","3217","HADOOP-17542. IPV6 support in Netutils#createSocketAddress","IPV6 support in Netutils#createSocketAddress","closed","","prasad-acit","2021-07-20T07:36:01Z","2021-08-10T10:50:52Z"
"","3171","HADOOP-17788. Replace IOUtils#closeQuietly usages by Hadoop's own utility","IOUtils#closeQuietly is deprecated since 2.6 release of commons-io without any replacement. Since we already have good replacement available in Hadoop's own IOUtils, we should use it.","closed","","virajjasani","2021-07-02T09:24:14Z","2021-07-08T07:04:30Z"
"","3110","HADOOP-17250 Lot of short reads can be merged  with readahead.","Introducing fs.azure.readahead.range parameter which can be set by user. Data will be populated in buffer for random reads as well which leads to lesser remote calls. This patch also changes the seek implementation to perform a lazy seek. Actual seek is done when a read is initiated and data is not present in buffer else date is returned from buffer thus reducing the number of remote calls.  Rebased with trunk. Base patch is https://github.com/apache/hadoop/pull/2307 Ran all tests including scale ones using us-east my bucket. All good.","closed","","mukund-thakur","2021-06-16T13:02:44Z","2021-07-05T10:45:13Z"
"","3012","HADOOP-17380 ITestS3AContractSeek.teardown closes FS before superclas…","Inside `ITestS3AContractSeek` the filesystem was getting closed before the tests can execute their `teardown` method leading to the output log being spammed with `IOException` messages.  Moving the order of operations around in `ITestS3AContractSeek.teardown` so that `teardown` happens before closing the filesystem.  [HADOOP-17380](https://issues.apache.org/jira/browse/HADOOP-17380)","closed","","bogthe","2021-05-13T16:39:30Z","2021-05-27T11:29:11Z"
"","3350","HADOOP-17877. BuiltInGzipCompressor header and trailer should not be static variables","In the newly added BuiltInGzipCompressor, we should not let header and trailer as static variables as they are modifiable from different instances at the same time.  I prepared a test. But as this is like race-condition, so it is not definitely causing the issue. Just post the test here for reference.  ```java @Test   public void testGzipCompressorsInSameJVM() throws InterruptedException {     // This test makes sure multiple BuiltInGzipCompressor can work in same JVM.      // don't use native libs     ZlibFactory.setNativeZlibLoaded(false);     Configuration conf = new Configuration();     CompressionCodec codec = ReflectionUtils.newInstance(GzipCodec.class, conf);      class CompressionThread extends Thread {       Compressor compressor;       byte[] b;       int inputSize;        boolean failed = false;       Exception exception;        CompressionThread(CompressionCodec codec, int inputSize, byte[] b) {         compressor = codec.createCompressor();         assertEquals(""Compressor is not a BuiltInGzipCompressor"", BuiltInGzipCompressor.class, compressor.getClass());          this.b = b;         this.inputSize = inputSize;       }        public void run() {         compressor.setInput(b,0,  b.length);         compressor.finish();          byte[] output = new byte[inputSize + 1024];         int outputOff = 0;          try {           while (!compressor.finished()) {             byte[] buf = new byte[2];             int compressed = compressor.compress(buf, 0, buf.length);             System.arraycopy(buf, 0, output, outputOff, compressed);             outputOff += compressed;           }         } catch (IOException e) {           failed = true;           exception = e;         }          DataInputBuffer gzbuf = new DataInputBuffer();         gzbuf.reset(output, outputOff);          try {           Decompressor decom = codec.createDecompressor();           assertNotNull(""Got a null decompressor"", decom);           assertEquals(""Decompressor is not a BuiltInGzipDecompressor"", BuiltInGzipDecompressor.class, decom.getClass());           InputStream gzin = codec.createInputStream(gzbuf, decom);            DataOutputBuffer dflbuf = new DataOutputBuffer();           dflbuf.reset();           IOUtils.copyBytes(gzin, dflbuf, 4096);           byte[] dflchk = Arrays.copyOf(dflbuf.getData(), dflbuf.getLength());           assertArrayEquals(""decompressed data must be the same as original input"", b, dflchk);         } catch (IOException e) {           failed = true;           exception = e;         }       }     }      Random r = new Random();     while (true) {       long seed = r.nextLong();       r.setSeed(seed);        int inputSize = r.nextInt(128 * 1024) + 1;        byte[] b1 = new byte[inputSize];       r.nextBytes(b1);        CompressionThread p1 = new CompressionThread(codec, inputSize, b1);        // Tweak the input bytes a bit. So their crc values will be different.       byte[] b2 = new byte[inputSize];       System.arraycopy(b1, 0, b2, 0, inputSize);       b2[0] = (byte) r.nextInt();       CompressionThread p2 = new CompressionThread(codec, inputSize, b2);       p1.start();       p2.start();       p1.join();       p2.join();       if (p1.failed) {         fail(""Failure happens during two compression threads running: "" + p1.exception.getMessage());       }       if (p2.failed) {         fail(""Failure happens during two compression threads running: "" + p2.exception.getMessage());       }     }   } ```","closed","","viirya","2021-08-28T06:03:15Z","2021-08-29T17:05:27Z"
"","2729","HADOOP-17548. ABFS: Toggle Store Mkdirs request overwrite parameter","In HDFS, mkdirs on an existing directory path is supposed to be a success response. To achieve this, the store backend call attempts mkdirs with overwrite=true. But on the backend, additional set properties operations also gets executed (such as LMT update) which is not a HDFS requirement and leads to unnecessary metadata update traffic.  In this PR, an option to have mkdirs executed with overwrite=false is introduced and is controlled over a config. Default is retained to overwrite=true until a related backend deployment is complete.  This PR also addresses a bug where mkdirs on an existing file path was returning success instead of throwing exception.  New config: `fs.azure.enable.mkdir.overwrite` [true by default]","closed","","sumangala-patki","2021-03-01T09:23:34Z","2021-04-11T04:38:18Z"
"","2700","HADOOP-17529. Upgrade os-maven-plugin to 1.7.0","In favor of supporting RISC-V architecture.  CC @trustin  https://issues.apache.org/jira/browse/HADOOP-17529","open","","advancedwebdeveloper","2021-02-14T22:39:55Z","2021-02-22T09:10:19Z"
"","3631","HDFS-16307. Improve HdfsBlockPlacementPolicies docs readability","imrove docs readability","closed","","GuoPhilipse","2021-11-08T17:35:53Z","2021-11-25T11:57:42Z"
"","3657","YARN-11007.Improve doc","improve docs, no test cases needed.","closed","","GuoPhilipse","2021-11-14T06:28:33Z","2021-11-18T12:48:56Z"
"","3135","YARN-10829. Support getApplications API in FederationClientInterceptor","Implementing getApplications API in FederationClientInterceptors. Unit tests are running successfully on dev machine.","closed","","akshatb1","2021-06-23T16:42:13Z","2021-07-31T11:28:57Z"
"","3022","YARN-10776. Make ConfiguredRMFailoverProxyProvider select ResourceMan…","I think we need to select resourcemanager or router randomly.  In federation mode, ConfiguredRMFailoverProxyProvider always connect first router in the configuration, it is better to shuffle the router list.   We need random the router or rm list like hdfs since HDFS-6648.","open","","zhengchenyu","2021-05-18T10:26:08Z","2022-06-28T11:56:09Z"
"","2727","HADOOP-17552. Change ipc.client.rpc-timeout.ms from 0 to 120000 by default to avoid potential hang","I proposed a fix for [HADOOP-17552](https://issues.apache.org/jira/browse/HADOOP-17552).","closed","","functioner","2021-03-01T02:04:22Z","2021-03-06T18:34:25Z"
"","2878","HDFS-15957. The ignored IOException in the RPC response sent by FSEditLogAsync can cause the HDFS client to hang","I propose a fix for [HDFS-15957](https://issues.apache.org/jira/browse/HDFS-15957). And probably we should make `RESPONSE_SEND_RETRIES` configurable.","open","","functioner","2021-04-08T22:20:16Z","2021-04-19T22:24:12Z"
"","2821","HDFS-15925. The lack of packet-level mirrorError state synchronization in BlockReceiver can cause the HDFS client to hang","I propose a fix for [HDFS-15925](https://issues.apache.org/jira/browse/HDFS-15925).","open","","functioner","2021-03-27T04:12:58Z","2021-03-29T01:48:02Z"
"","2737","HDFS-15869. Network issue while FSEditLogAsync is executing RpcEdit.logSyncNotify can cause the namenode to hang","I propose a fix for [HDFS-15869](https://issues.apache.org/jira/browse/HDFS-15869).","open","","functioner","2021-03-03T05:25:16Z","2021-04-20T01:22:01Z"
"","2966","HDFS-16004.startLogSegment and journal in BackupNode lack Permission …","I have some doubt when i configurate secure HDFS.  I know we have Service Level Authorization  for protocols like NamenodeProtocol,DatanodeProtocol and so on. But i do not find such Authorization   for JournalProtocol after reading the code in HDFSPolicyProvider.  And if we have, how can i configurate such Authorization?   Besides  even NamenodeProtocol has Service Level Authorization, its methods still have Permission check. Take startCheckpoint in NameNodeRpcServer who implemented NamenodeProtocol  for example:   public NamenodeCommand startCheckpoint(NamenodeRegistration registration)       throws IOException {     String operationName = ""startCheckpoint"";     checkNNStartup();     namesystem.checkSuperuserPrivilege(operationName); ......   I found that the methods in  BackupNodeRpcServer who implemented JournalProtocol  lack of such  Permission check. See below:         public void startLogSegment(JournalInfo journalInfo, long epoch,         long txid) throws IOException {       namesystem.checkOperation(OperationCategory.JOURNAL);       verifyJournalRequest(journalInfo);       getBNImage().namenodeStartedLogSegment(txid);     }       @Override     public void journal(JournalInfo journalInfo, long epoch, long firstTxId,         int numTxns, byte[] records) throws IOException {       namesystem.checkOperation(OperationCategory.JOURNAL);       verifyJournalRequest(journalInfo);       getBNImage().journal(firstTxId, numTxns, records);     }   Do we need add Permission check for them?   Please point out my mistakes if i am wrong or miss something.","open","","lujiefsi","2021-04-30T03:32:52Z","2021-05-17T11:43:30Z"
"","3213","YARN-10858. [UI2] YARN-10826 breaks Queue view.","https://issues.apache.org/jira/browse/YARN-10858","closed","","iwasakims","2021-07-19T12:59:18Z","2021-07-20T03:54:51Z"
"","3128","YARN-10826. [UI2] Upgrade Node.js to v12.22.1.","https://issues.apache.org/jira/browse/YARN-10826  I just manually tested the UI by running `node $YARNJS start` described in the [README.md](https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/README.md) with pseudo-distributed cluster.","closed","","iwasakims","2021-06-21T14:22:56Z","2021-06-23T12:25:31Z"
"","2946","YARN-10747. Bump YARN CSI protobuf version to 3.7.1","https://issues.apache.org/jira/browse/YARN-10747","closed","","smengcl","2021-04-22T05:10:36Z","2021-04-28T07:10:25Z"
"","2791","YARN-10706. Upgrade com.github.eirslett:frontend-maven-plugin to 1.11.2","https://issues.apache.org/jira/browse/YARN-10706","closed","yarn,","liuml07","2021-03-20T08:05:54Z","2021-03-22T04:12:30Z"
"","3214","YARN-10630. [UI2] Ambiguous queue name resolution","https://issues.apache.org/jira/browse/YARN-10630","closed","","9uapaw","2021-07-19T15:58:28Z","2021-07-21T02:06:46Z"
"","2995","MAPREDUCE-7343. Increase the job name max length in mapred job -list.","https://issues.apache.org/jira/browse/MAPREDUCE-7343","closed","","ayushtkn","2021-05-10T13:12:53Z","2021-05-13T18:45:47Z"
"","3492","HDFS-16240. Replace unshaded guava in HttpFSServerWebServer.","https://issues.apache.org/jira/browse/HDFS-16240  [HDFS-16129](https://issues.apache.org/jira/browse/HDFS-16129) added use of com.google.common.annotations.VisibleForTesting to HttpFSServerWebServer. It is replaced by replace-guava replacer of [HADOOP-17288](https://issues.apache.org/jira/browse/HDFS-17288) on every build.","closed","","iwasakims","2021-09-28T04:12:50Z","2021-09-28T05:41:16Z"
"","3353","HDFS-16192: ViewDistributedFileSystem#rename wrongly using src in the place of dst.","https://issues.apache.org/jira/browse/HDFS-16192  ### Description of PR ViewDistributedFileSystem#rename with options API using src args wrongly in the place of dst. Corrected the same and also provided to access rename with options API via Util, so that we don't need to cast to distributed file system class. The reason is, if any other file system provides the implementation to that API, we will be able to use it.  ### How was this patch tested?  Added a test to verify the behavior.  ### For code changes:  same as explained in description.","closed","","umamaheswararao","2021-08-29T00:45:48Z","2021-08-31T05:11:08Z"
"","3374","HDFS-16091. WebHDFS should support getSnapshotDiffReportListing.","https://issues.apache.org/jira/browse/HDFS-16091  When there are millions of diffs between two snapshots, the old getSnapshotDiffReport() isn't scalable. This PR adds  GETSNAPSHOTDIFFLISTING op to NameNode WebHDFS API.  WebHdfsFileSystem#getSnapshotDiffReport is fixed to leverage getSnapshotDiffReportListing in the same way as DistributedFileSystem#getSnapshotDiffReport.","closed","","iwasakims","2021-09-02T14:15:02Z","2021-10-31T00:29:34Z"
"","3078","HDFS-16055. Quota is not preserved in snapshot INode","https://issues.apache.org/jira/browse/HDFS-16055  Quota feature is not preserved during snapshot creation, this causes `INodeDirectory#metadataEquals` to ALWAYS return `true`. Therefore, `hdfs snapshotDiff` will always return the snapshot root as modified, even if the quota is set before the snapshot creation:  ```bash $ hdfs snapshotDiff /diffTest s0 . Difference between snapshot s0 and current directory under directory /diffTest: M	. ```","closed","","smengcl","2021-06-04T22:24:28Z","2021-06-14T17:48:23Z"
"","2998","HDFS-16016. BPServiceActor to provide new thread to handle IBR","https://issues.apache.org/jira/browse/HDFS-16016","closed","","virajjasani","2021-05-11T15:39:39Z","2021-11-17T09:41:29Z"
"","2958","HDFS-15997. Implement dfsadmin -provisionSnapshotTrash -all","https://issues.apache.org/jira/browse/HDFS-15997","closed","","smengcl","2021-04-26T16:30:39Z","2021-05-10T19:41:07Z"
"","2860","HDFS-15988. Stabilise HDFS Pre-Commit.","https://issues.apache.org/jira/browse/HDFS-15988","closed","","ayushtkn","2021-04-03T14:23:48Z","2021-05-15T19:15:23Z"
"","2927","HDFS-15982. Deleted data using HTTP API should be saved to the trash","https://issues.apache.org/jira/browse/HDFS-15982","closed","","virajjasani","2021-04-18T13:16:45Z","2021-06-01T03:14:55Z"
"","2863","HDFS-15916. DistCp: Backward compatibility: Distcp fails from Hadoop 3 to Hadoop 2 for snapshotdiff.","https://issues.apache.org/jira/browse/HDFS-15916","closed","","ayushtkn","2021-04-04T14:13:43Z","2021-05-26T17:44:39Z"
"","2866","HDFS-15900. RBF: empty blockpool id on dfsrouter caused by UNAVAILABLE NameNode.","https://issues.apache.org/jira/browse/HDFS-15900  Backporting https://github.com/apache/hadoop/pull/2787 to branch-3.2.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hdaikoku","2021-04-05T05:03:45Z","2021-04-06T07:08:26Z"
"","2787","HDFS-15900. RBF: empty blockpool id on dfsrouter caused by UNAVAILABLE NameNode.","https://issues.apache.org/jira/browse/HDFS-15900  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hdaikoku","2021-03-19T01:42:09Z","2021-03-29T03:43:16Z"
"","2784","HDFS-15850. Superuser actions should be reported to external enforcers","https://issues.apache.org/jira/browse/HDFS-15850","closed","","vivekratnavel","2021-03-17T21:35:30Z","2021-04-15T21:45:51Z"
"","3065","HDFS-13671. Namenode deletes large dir slowly caused by FoldedTreeSet#removeAndGet","https://issues.apache.org/jira/browse/HDFS-13671","closed","","AlphaGouGe","2021-06-01T11:18:15Z","2021-06-18T06:56:56Z"
"","3562","HADOOP-17965. Fix documentation build failure using JDK 7 on branch-2.10.","https://issues.apache.org/jira/browse/HADOOP-17965  `mvn site` by JDK 7 fails due to error related to spotbugs which does not support Java 7.  ``` $ ./start-build-env.sh ubuntu@2a660195e110:~/hadoop$ mvn clean install -DskipTests ubuntu@2a660195e110:~/hadoop$ mvn site ...(snip) [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  2.138 s] [INFO] Apache Hadoop YARN UI .............................. SUCCESS [  0.044 s] [INFO] Apache Hadoop YARN Project ......................... FAILURE [  0.700 s] ...(snip) [ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.5:site (default-site) on project hadoop-yarn-project: Execution default-site of goal org.apache.maven.plugins:maven-site-plugin:3.5:site failed: An API incompatibility was encountered while executing org.apache.maven.plugins:maven-site-plugin:3.5:site: java.lang.UnsupportedClassVersionError: org/codehaus/mojo/spotbugs/SpotbugsViolationCheckMojo : Unsupported major.minor version 52.0 ```  Since we are using JDK 7 in the Dockerfile, dev-support/bin/create-release is affected too.  The cause is obsolete  entries of pom.xml for already resolved [MSITE-443](https://issues.apache.org/jira/browse/MSITE-443).  We can safely remove the  entry. `mvn site` generates same contents without the entry. spotbugsXml.xml are generated regardless of it (on `mvn compile spotbugs:spotbugs` using JDK 8).","closed","","iwasakims","2021-10-18T13:31:37Z","2021-10-19T09:09:04Z"
"","3545","HADOOP-17964. Increase Java heap size for running Maven in Dockerfile of branch-2.10.","https://issues.apache.org/jira/browse/HADOOP-17964  I got OOM on running create-release script in branch-2.10.  ``` $ ./dev-support/bin/create-release --docker --dockercache $ tail -n 14 patchprocess/mvn_install.log --------------------------------------------------- Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space         at java.util.concurrent.CopyOnWriteArrayList.iterator(CopyOnWriteArrayList.java:961)         at org.apache.maven.cli.MavenCli.execute(MavenCli.java:877)         at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288)         at org.apache.maven.cli.MavenCli.main(MavenCli.java:199)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:607)         at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)         at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)         at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)         at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356) ```  I got no issue when I build the RC of 2.10.1 using the Dockerfile. JDK migration to Zulu by [HADOOP-17572](https://issues.apache.org/jira/browse/HADOOP-17572) might be related.","closed","","iwasakims","2021-10-12T15:18:41Z","2021-10-12T15:53:18Z"
"","3509","HADOOP-17925. BUILDING.txt should not encourage to activate docs profile on building binary artifacts.","https://issues.apache.org/jira/browse/HADOOP-17925  If `-Pdocs` is activated, hadoop-client depends on xerces. The dependency was added by [HADOOP-14835](https://issues.apache.org/jira/browse/HADOOP-14835) to fix jdiff error. We are avoiding unwanted dependency by [building documentations alone in the second pass after building binary artifacts](https://github.com/apache/hadoop/blob/rel/release-3.3.1/dev-support/bin/create-release#L568-L606). The BUILDING.txt just should not encourage to activate `-Pdocs` on building binary artifacts.","closed","","iwasakims","2021-10-01T10:31:49Z","2021-10-11T08:56:03Z"
"","3447","HADOOP-17916. Fix compilation error of ITUseHadoopCodecs with -DskipS…","https://issues.apache.org/jira/browse/HADOOP-17916  Compilation of ITUseHadoopCodecs was failed due to issue of dependency declaration.  ``` $ mvn clean install -DskipTests -DskipShade ... [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project hadoop-client-integration-tests: Compilation failure: Compilation failure: [ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[34,28] cannot find symbol [ERROR]   symbol:   class RandomDatum [ERROR]   location: package org.apache.hadoop.io [ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[100,16] package RandomDatum does not exist [ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[100,54] package RandomDatum does not exist [ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[103,7] cannot find symbol [ERROR]   symbol:   class RandomDatum [ERROR]   location: class org.apache.hadoop.example.ITUseHadoopCodecs [ERROR] /home/centos/srcs/hadoop/hadoop-client-modules/hadoop-client-integration-tests/src/test/java/org/apache/hadoop/example/ITUseHadoopCodecs.java:[104,7] cannot find symbol [ERROR]   symbol:   class RandomDatum [ERROR]   location: class org.apache.hadoop.example.ITUseHadoopCodecs ```","closed","","iwasakims","2021-09-16T07:56:47Z","2021-09-16T16:44:57Z"
"","3433","HADOOP-17908. Add missing RELEASENOTES and CHANGELOG to upstream.","https://issues.apache.org/jira/browse/HADOOP-17908  RELEASENOTES and CHANGELOG of 2.10.1, 3.1.4 and 3.3.0 are missing in trunk.  While CHANGES\* were renamed to CHANGELOG\* by [HADOOP-14671](https://issues.apache.org/jira/browse/HADOOP-14671), there are some CHANGES files left. I would like to rename those to CHNAGELOG here for consistency.  ``` $ find . -name 'CHANGES*' ./hadoop-common-project/hadoop-common/src/site/markdown/release/2.10.0/CHANGES.2.10.0.md ./hadoop-common-project/hadoop-common/src/site/markdown/release/3.1.2/CHANGES.3.1.2.md ./hadoop-common-project/hadoop-common/src/site/markdown/release/3.1.3/CHANGES.3.1.3.md ```  hadoop-common-project/hadoop-common/src/site/markdown/release/2.10.0 contains both CHANGES.2.10.0.md and CHANGELOG.2.10.0.md. Since the contents of current CHANGELOG.2.10.0.md is invalid, I replaced it with the contents of current CHANGES.2.10.0.md (and removed the CHANGES.2.10.0.md).  HTML files under ""Changelog and Release Notes"" pages (hadoop-project/hadoop-project-dist/hadoop-common/release/\*) are generated by `mvn site` regardless of the file names.","closed","","iwasakims","2021-09-14T06:33:41Z","2021-10-20T04:54:59Z"
"","3399","HADOOP-17899. Avoid using implicit dependency on junit-jupiter-api.","https://issues.apache.org/jira/browse/HADOOP-17899  Compilation of branch-3.3 fails due to lack of transitive dependency existing in trunk.  ``` [ERROR] /home/centos/srcs/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[29,29] package org.junit.jupiter.api does not exist [ERROR] /home/centos/srcs/hadoop/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/http/TestHttpFileSystem.java:[47,4] cannot find symbol   symbol:   class BeforeEach   location: class org.apache.hadoop.fs.http.TestHttpFileSystem [INFO] 2 errors ```  junit-jupiter-api is transitive dependency of curator-test-5.2.0 in trunk. branch-3.3 using curator-test-4.2.0 does not have it. Existing tests of Hadoop are using JUnit 4. hadoop-common should avoid using implicit dependency on junit-jupiter-api until migration to JUnit 5 is done.","closed","","iwasakims","2021-09-08T05:39:52Z","2021-09-08T11:44:23Z"
"","3394","HADOOP-17897. Allow nested blocks in switch case in checkstyle settings.","https://issues.apache.org/jira/browse/HADOOP-17897  Nested blocks in switch case are used in existing code and checkstyle warning are just ignored. It should be allowed in project checkstyle settings.","closed","","iwasakims","2021-09-07T09:50:49Z","2021-09-08T04:56:12Z"
"","3308","HADOOP-17850. Upgrade ZooKeeper to 3.4.14 in branch-3.2.","https://issues.apache.org/jira/browse/HADOOP-17850  Upgrade ZooKeeper 3.4.14 to fix CVE-2019-0201 (https://zookeeper.apache.org/security.html).","closed","","iwasakims","2021-08-16T11:02:21Z","2021-08-17T04:36:26Z"
"","3305","HADOOP-17849. Exclude spotbugs-annotations from transitive dependencies on branch-3.2.","https://issues.apache.org/jira/browse/HADOOP-17849  Building Hadoop in dist profile with ZooKeeper 3.4.14 fails on hadoop-client-check-test-invariants. Excluding com.github.spotbugs:spotbugs-annotation from transitive dependencies should fix this for users needing zookeeer-3.4.14. Since the dependency is provided/optional on ZooKeeper 3.5.x, branch-3.3 and above are not affected.","closed","","iwasakims","2021-08-16T08:03:57Z","2021-08-16T12:46:20Z"
"","3275","HADOOP-17840: Backport HADOOP-17837 to branch-3.2","https://issues.apache.org/jira/browse/HADOOP-17840","closed","backport,","bbeaudreault","2021-08-06T11:35:58Z","2021-08-08T05:23:00Z"
"","3182","HADOOP-17775. Remove JavaScript package from Docker environment.","https://issues.apache.org/jira/browse/HADOOP-17775  for branch-3.3.","closed","","iwasakims","2021-07-07T04:56:04Z","2021-07-07T08:49:20Z"
"","3183","HADOOP-17775. Remove JavaScript package from Docker environment.","https://issues.apache.org/jira/browse/HADOOP-17775  for branch-3.2.","closed","","iwasakims","2021-07-07T05:10:39Z","2021-07-07T08:49:41Z"
"","3184","HADOOP-17775. Remove JavaScript package from Docker environment.","https://issues.apache.org/jira/browse/HADOOP-17775  for branch-2.10.","closed","","iwasakims","2021-07-07T05:14:00Z","2021-07-07T08:50:00Z"
"","3137","HADOOP-17775. Remove JavaScript package from Docker environment.","https://issues.apache.org/jira/browse/HADOOP-17775  As described in the [README of yarn-ui](https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/README.md), required javascript modules are automatically pulled by frontend-maven-plugin. We can leverage them for local testing too.  While hadoop-yarn-ui and hadoop-yarn-applications-catalog-webapp is using node.js, the version of node.js does not match. JavaScript related packages of the docker environment is not sure to work.  * https://github.com/apache/hadoop/blob/fdef2b4ccacb8753aac0f5625505181c9b4dc154/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/pom.xml#L170-L212 * https://github.com/apache/hadoop/blob/fdef2b4ccacb8753aac0f5625505181c9b4dc154/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-catalog/hadoop-yarn-applications-catalog-webapp/pom.xml#L264-L290","closed","","iwasakims","2021-06-24T05:58:38Z","2021-07-07T02:07:11Z"
"","3116","HADOOP-17746. compatibility table in directory_markers.md doesn't render right.","https://issues.apache.org/jira/browse/HADOOP-17746  Doxia can not recognize the table as table due to dashes (--) at the bottom.","open","","iwasakims","2021-06-17T12:04:21Z","2021-06-17T13:06:34Z"
"","3157","HADOOP-17719. DistCp with snapshot diff should support file systems supporting getSnapshotDiffReport.","https://issues.apache.org/jira/browse/HADOOP-17719  This PR allows (third party) file systems other than DistributedFileSystem or WebHdfsFileSystem to used for `distcp -update -diff`.  The is intended to be intermediate fix before moving snapshot related classes to hadoop-common as part of FileSystem specification.","open","","iwasakims","2021-06-29T15:02:51Z","2021-08-23T11:53:52Z"
"","3030","HADOOP-17719. DistCp with snapshot diff should support file systems other than DistributedFileSystem.","https://issues.apache.org/jira/browse/HADOOP-17719  `distcp -diff` currently supports only DistributedFileSystem. Other file system could be used if getSnapshotDiffReport is supported.","closed","","iwasakims","2021-05-20T14:38:40Z","2021-06-26T21:11:23Z"
"","2939","HADOOP-17650. Bump solr to unblock build failure with Maven 3.8.1","https://issues.apache.org/jira/browse/HADOOP-17650","closed","","virajjasani","2021-04-20T15:28:47Z","2021-09-24T12:58:48Z"
"","2931","HADOOP-17635. Update the ubuntu version in the build instruction.","https://issues.apache.org/jira/browse/HADOOP-17635  The setup procedure of Ubuntu 18.04 is defferent from 20.04. Uses of Ubuntu 18.04 need to install gcc-9 from supplemental repository and cmake >= 3.19 from source code. Leaving current instruction for Ubuntu 18.04 could be useful until majority of users move to 20.04.  Oracle Java(JDK) 8 is not available now. I replaced it with OpenJDK 8. https://launchpad.net/~webupd8team/+archive/ubuntu/java  I tested the procedure on fresh Unbuntu 18.04 environment (on EC2 using ami-0ef85cf6e604e5650). ``` $ uname -a Linux ip-172-31-18-242 5.4.0-1038-aws #40~18.04.1-Ubuntu SMP Sat Feb 6 01:56:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux  $ cat /etc/os-release NAME=""Ubuntu"" VERSION=""18.04.5 LTS (Bionic Beaver)"" ID=ubuntu ID_LIKE=debian PRETTY_NAME=""Ubuntu 18.04.5 LTS"" VERSION_ID=""18.04"" HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic  $ mvn clean install -DskipTests -DskipShade -Drequire.isal -Drequire.pmdk -Pnative -Pdist  $ ./hadoop-dist/target/hadoop-3.4.0-SNAPSHOT/bin/hadoop checknative 2021-04-19 04:23:17,621 INFO bzip2.Bzip2Factory: Successfully loaded & initialized native-bzip2 library system-native 2021-04-19 04:23:17,625 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library 2021-04-19 04:23:17,758 INFO nativeio.NativeIO: The native code was built with PMDK support, and PMDK libs were loaded successfully. Native library checking: hadoop:  true /home/ubuntu/hadoop/hadoop-dist/target/hadoop-3.4.0-SNAPSHOT/lib/native/libhadoop.so.1.0.0 zlib:    true /lib/x86_64-linux-gnu/libz.so.1 zstd  :  true /usr/lib/x86_64-linux-gnu/libzstd.so.1 bzip2:   true /lib/x86_64-linux-gnu/libbz2.so.1 openssl: true /usr/lib/x86_64-linux-gnu/libcrypto.so ISA-L:   true /usr/lib/libisal.so.2 PMDK:    true /usr/local/lib/libpmem.so.1.0.0 ```","closed","","iwasakims","2021-04-19T04:25:30Z","2021-04-19T07:45:28Z"
"","2861","HADOOP-17620. DistCp: Use Iterator for listing target directory as well.","https://issues.apache.org/jira/browse/HADOOP-17620","closed","","ayushtkn","2021-04-04T10:19:04Z","2021-04-23T17:18:15Z"
"","2846","HADOOP-17619. Fix DelegationTokenRenewer#updateRenewalTime java doc error.","https://issues.apache.org/jira/browse/HADOOP-17619","closed","","zhuqi-lucas","2021-03-31T14:43:32Z","2021-04-04T09:22:03Z"
"","3019","HADOOP-17609. Make SM4 support optional for OpenSSL native code.","https://issues.apache.org/jira/browse/HADOOP-17609  This replaces #2847.  After HDFS-15098, OpensslCipher does not work with OpenSSL >= 1.1.1 without SM4 support. RHEL/CentOS 8 provides such openssl package. The OpensslCipher on such environment should be usable if users do not need SM4 feature.  ``` $ rpm -q openssl-devel openssl-devel-1.1.1g-12.el8_3.x86_64  $ bin/hadoop checknative 2>/dev/null Native library checking: hadoop:  true /home/centos/dist/hadoop-3.4.0-SNAPSHOT/lib/native/libhadoop.so.1.0.0 zlib:    true /lib64/libz.so.1 zstd  :  true /lib64/libzstd.so.1 bzip2:   true /lib64/libbz2.so.1 openssl: false Cannot find AES-CTR/SM4-CTR support, is your version of Openssl new enough? ISA-L:   true /lib64/libisal.so.2 PMDK:    false The native code was built without PMDK support. ```","open","","iwasakims","2021-05-17T11:24:37Z","2022-07-19T22:58:23Z"
"","2847","HADOOP-17609. Make SM4 support optional for OpenSSL native code.","https://issues.apache.org/jira/browse/HADOOP-17609  After [HDFS-15098](https://issues.apache.org/jira/browse/HDFS-15098), OpensslCipher does not work with OpenSSL >= 1.1.1 without SM4 support. RHEL/CentOS 8 provides such openssl package. The OpensslCipher on such environment should be usable if users do not need SM4 feature.  ``` $ rpm -q openssl-devel openssl-devel-1.1.1g-12.el8_3.x86_64  $ bin/hadoop checknative 2>/dev/null Native library checking: hadoop:  true /home/centos/dist/hadoop-3.4.0-SNAPSHOT/lib/native/libhadoop.so.1.0.0 zlib:    true /lib64/libz.so.1 zstd  :  true /lib64/libzstd.so.1 bzip2:   true /lib64/libbz2.so.1 openssl: false Cannot find AES-CTR/SM4-CTR support, is your version of Openssl new enough? ISA-L:   true /lib64/libisal.so.2 PMDK:    false The native code was built without PMDK support. ```  Not throwing error on OpensslCipher#initIDs even if symbols for SM4 is not available could be fix. The constructor of OpensslSm4CtrCryptoCodec should throw exception if SM4 is not available in order to fall back to Java impl (JceSm4CtrCryptoCodec).","closed","","iwasakims","2021-03-31T15:00:04Z","2021-05-17T11:23:25Z"
"","2834","HADOOP-17603. Upgrade tomcat-embed-core to 7.0.108","https://issues.apache.org/jira/browse/HADOOP-17603  Upgrade tomcat-embed-core to 7.0.108 on branch-2.10  [CVE-2021-25329](https://nvd.nist.gov/vuln/detail/CVE-2021-25329) critical severity. Impact: [CVE-2020-9494](https://nvd.nist.gov/vuln/detail/CVE-2020-9494) 7.0.0-7.0.107 are all affected by the vulnerability.","closed","","amahussein","2021-03-29T22:20:05Z","2021-04-12T17:11:39Z"
"","2802","HADOOP-17598. Fix java doc issue introduced by HADOOP-17578.","https://issues.apache.org/jira/browse/HADOOP-17598","closed","","xiaoyuyao","2021-03-22T23:59:16Z","2021-03-23T04:55:15Z"
"","2786","HADOOP-17594. DistCp: Expose the JobId for applications executing through run method","https://issues.apache.org/jira/browse/HADOOP-17594","closed","","ayushtkn","2021-03-18T12:27:54Z","2021-03-19T09:10:40Z"
"","2769","HADOOP-17586. Upgrade org.codehaus.woodstox:stax2-api to 4.2.1.","https://issues.apache.org/jira/browse/HADOOP-17586","closed","","ayushtkn","2021-03-12T13:09:00Z","2021-03-13T23:33:03Z"
"","2762","HADOOP-17578. Improve UGI debug log to help troubleshooting TokenCach…","https://issues.apache.org/jira/browse/HADOOP-17578  Additional Debug Log for troubleshooting TokenCache related issues.","closed","","xiaoyuyao","2021-03-11T22:10:30Z","2021-03-23T00:01:22Z"
"","2758","HADOOP-17573. Fix compilation error of OBSFileSystem in trunk.","https://issues.apache.org/jira/browse/HADOOP-17573","closed","","iwasakims","2021-03-10T10:14:12Z","2021-03-10T16:26:20Z"
"","2886","HADOOP-17569. Building native code fails on Fedora 33.","https://issues.apache.org/jira/browse/HADOOP-17569  `strerror` of glibc >= 2.32 is thread safe. We can just use it. We still need existing code for supporting old glibc. https://sourceware.org/pipermail/libc-announce/2020/000029.html > * The deprecated symbols sys_errlist, _sys_errlist, sys_nerr, and _sys_nerr >   are no longer available to newly linked binaries, and their declarations >   have been removed from from .  They are exported solely as >   compatibility symbols to support old binaries.  All programs should use >   strerror or strerror_r instead. >  > * Both strerror and strerror_l now share the same internal buffer in the >   calling thread, meaning that the returned string pointer may be invalided >   or contents might be overwritten on subsequent calls in the same thread or >   if the thread is terminated.  It makes strerror MT-safe.  In addition, I added `-fcommon` to CFLAGS for building code of hadoop-hdfs-native-client since `-fno-common` (default from GCC 10.0) break the build. https://gcc.gnu.org/bugzilla/show_bug.cgi?id=85678","closed","","iwasakims","2021-04-09T11:24:00Z","2021-04-15T12:33:31Z"
"","2716","HADOOP-17538. Add kms-default.xml and httpfs-default.xml to site index.","https://issues.apache.org/jira/browse/HADOOP-17538  While there are links to *-default.xml in the site index, kms-default.xml and httpfs-default.xml are not included. Adding them could be useful as a quick reference.  Since HTMLs are generated by XSL for hadoop-kms and hadoop-hdfs-httpfs on `mvn site`, just adding link to the .html should work.","closed","","iwasakims","2021-02-22T13:27:43Z","2021-02-23T22:46:29Z"
"","2732","HADOOP-17531. DistCp: Reduce memory usage on copying huge directories.","https://issues.apache.org/jira/browse/HADOOP-17531","closed","","ayushtkn","2021-03-02T07:30:57Z","2021-03-23T21:06:26Z"
"","3316","HADOOP-14693. Test PR #2: Ran rewrite plugin on hadoop-hdfs module to upgrade to JUnit 5","https://issues.apache.org/jira/browse/HADOOP-14693  See https://github.com/apache/hadoop/pull/3304 for more context.  - Test PR: Ran rewrite plugin - Fix TestHDFSContractPathHandle. - Fix TestDiskBalancerRPC. - Fix TestDistributedFileSystem. - Fix TestBlockReaderLocal. - [Check line removal] Fix TestDFSClientFailover. - Fix TestScrLazyPersistFiles. - Fix TestShortCircuitCache. - Fix TestErasureCodingPolicyWithSnapshot. - [Check static addition] Fix TestFSMainOperationsWebHdfs. - Fix TestViewFileSystemHdfs. - Fix TestFileStatusWithDefaultECPolicy. - Fix TestStartSecureDataNode. - Fix TestGlobPaths. - Fix TestSaslDataTransfer. - Fix TestKeyManager. - Fix TestEncryptionZones. - Fix TestParallelShortCircuitReadNoChecksum. - Fix TestSecureNameNodeWithExternalKdc. - Fix TestShortCircuitLocalRead. - Fix TestParallelShortCircuitReadUnCached. - Fix TestViewFileSystemOverloadSchemeHdfsFileSystemContract. - Fix TestDFSInputStream. - Fix TestParallelShortCircuitRead. - Fix TestHDFSFileContextMainOperations. - Fix TestParallelUnixDomainRead. - Fix TestBlockReaderFactory. - Fix TestWebHDFS. - [Check static addition] Fix TestFileStatus. - Fix TestPermissionSymlinks. - Fix TestDiskBalancerWithMockMover.","open","","smengcl","2021-08-20T06:17:24Z","2021-08-23T07:37:38Z"
"","2798","MAPREDUCE-7330. Add access check for getJobAttempts","Hi, i have found an inconsistency, all the methods who get job infomation have access check, except getJobAttempts.","open","","lujiefsi","2021-03-22T14:14:28Z","2021-04-01T13:02:20Z"
"","3559","HDFS-16277 improve decision in AvailableSpaceBlockPlacementPolicy","Hi  In product environment,we may meet two or more datanode usage  reaches nealy 100%,for exampe 99.99%,98%,97%. if we configure `AvailableSpaceBlockPlacementPolicy` , we also have the chance to choose the 99.99%(assume it is the highest usage),for we treat the two choosen datanode as the same usage if their storage usage are different within 5%. but this is not what we want, so i suggest we can improve the decision.","closed","","GuoPhilipse","2021-10-16T14:59:43Z","2021-11-25T11:57:30Z"
"","3164","Fix NPE in Find.java","Hello, Our static analyzer found a following potential NPE. We have checked the feasibility of this execution trace. It is necessary to defend this vulnerability to improve the code quality. We have provided the patch for you. Please check and confirm it.  Here is the bug trace.  1. Select the false branch at this point (expressionClass==null is true), and null assigned to instance https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java#L129-L133  2. Return instance to caller, which can be null (The return value can be null) https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java#L133  3. Function createExpression executes and stores the return value to expr (expr can be null) https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Find.java#L113  4. Function add executes and primaries contains null value https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Find.java#L117  5. Function next executes and stores the return value to expr (expr can be null) https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Find.java#L139  6. expr is passed as the this pointer to function getUsage (expr can be null), which will leak to null pointer dereference https://github.com/apache/hadoop/blob/986d0a4f1d5543fa0b4f5916729728f78b4acec9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/find/Find.java#L140   Commit: 986d0a4f1d5543fa0b4f5916729728f78b4acec9     ContainerAnalyzer","open","","containerAnalyzer","2021-07-01T08:12:38Z","2021-09-21T18:16:25Z"
"","3595","HDFS-16283: RBF: improve renewLease() to call only a specific NameNod…","HDFS-16283: RBF: improve renewLease() to call only a specific NameNode rather than make fan-out calls  Currently renewLease() is called against all the target name services on router while the file is only available on one name service and we only need to renew lease against that name service.  The patch adds name service id (nsId) when a new file is created and it's stored on the DFSClient project. A new API to renew lease against a particular nsId is implemented so the router renews lease for that name service.  ### How was this patch tested? unit tests and cluster testing.","open","","aihuaxu","2021-10-27T21:26:54Z","2021-11-03T17:56:38Z"
"","3476","HDFS-16236. Example command for daemonlog is not correct","HDFS-16236. Example command for daemonlog is not correct","closed","","prasad-acit","2021-09-23T19:41:29Z","2021-09-27T00:43:14Z"
"","3375","HDFS-16207. Remove NN logs stack trace for non-existent xattr query","HDFS-16207 Remove NN logs stack trace for non-existent xattr query  - [X] Does the title or this PR starts with the corresponding JIRA issue - [X] unit tests:  ``` mvn test -Dtest=TestDFSShell,TestEncryptionZones,TestFileContextXAttr,TestNameNodeXAttr,TestWebHDFSXAttr ```","closed","","amahussein","2021-09-02T21:04:11Z","2021-09-09T04:21:17Z"
"","3373","HDFS-16193. Append & Rename APIs to support FGL.","HDFS-16193. Append & Rename APIs to support FGL. Switch the global lock with FGL for Append & Rename API.","open","","prasad-acit","2021-09-02T12:29:24Z","2022-05-03T16:53:06Z"
"","3351","HDFS-16191. [FGL] Fix FSImage loading issues on dynamic partitions","HDFS-16191. [FGL] Fix FSImage loading issues on dynamic partitions","open","","prasad-acit","2021-08-28T09:01:13Z","2021-09-08T19:37:02Z"
"","3280","HDFS-16171. De-flake testDecommissionStatus","HDFS-16171","closed","","virajjasani","2021-08-09T12:01:05Z","2021-08-16T05:54:52Z"
"","3232","HDFS-16141. Address permission related issues with File / Directory","HDFS-16141. Address permission related issues with File / Directory","open","","prasad-acit","2021-07-25T18:02:36Z","2021-08-13T19:08:02Z"
"","3244","HDFS-16138. BlockReportProcessingThread exit doesnt print the acutal stack","HDFS-16138. BlockReportProcessingThread exit doesnt print the acutal stack","closed","","prasad-acit","2021-07-28T04:52:04Z","2021-09-05T09:07:59Z"
"","3205","HDFS-16130. Implement CREATE File with FGL","HDFS-16130. Implement CREATE File with FGL","open","","prasad-acit","2021-07-14T18:57:27Z","2021-08-13T19:06:51Z"
"","3112","HDFS-16072. TestBlockRecovery is failing consistently on branch-2.10","HDFS-16072 TestBlockRecovery fails consistently on Branch-2.10  `TestBlockRecovery` was failing because of the side effects of the unit tests especially when the `DN.recover()` is being called.","closed","","amahussein","2021-06-16T21:36:59Z","2021-06-21T17:41:55Z"
"","3095","HDFS-16061. DFTestUtil.waitReplication can produce false positives","HDFS-16061 DFTestUtil.waitReplication can produce false positives  While checking the intermittent failure in `TestBalancerRPCDelay#testBalancerRPCDelayQpsDefault` described in HDFS-15146, I found that the implementation of waitReplication is incorrect. In the last iteration, when correctReplFactor is false, the thread sleeps for 1 second, then a TimeoutException is thrown without check whether the replication was complete in the last second.","closed","","amahussein","2021-06-10T14:45:59Z","2021-06-20T05:40:15Z"
"","2982","HDFS-16007. Deserialization of ReplicaState should avoid throwing ArrayIndexOutOfBoundsException","HDFS-16007","closed","","virajjasani","2021-05-06T09:16:13Z","2021-05-12T15:18:38Z"
"","3036","HDFS-15998. Fix NullPointException In listOpenFiles","HDFS-15998. Fix NullPointException In listOpenFiles","closed","","haiyang1987","2021-05-21T06:35:54Z","2021-06-01T07:26:47Z"
"","2941","HDFS-15963. Unreleased volume references cause an infinite loop. (#2889) Contributed by Shuyan Zhang.","HDFS-15963. Unreleased volume references cause an infinite loop. (#2889) Contributed by Shuyan Zhang.  Reviewed-by: Wei-Chiu Chuang  Reviewed-by: He Xiaoqiao","closed","","zhangshuyan0","2021-04-21T04:38:04Z","2021-04-21T10:32:56Z"
"","2902","HDFS-15940. Fixing and refactoring tests specific to Block recovery.","HDFS-15940 for branch-3.2","closed","","jojochuang","2021-04-13T08:16:50Z","2021-04-15T03:20:23Z"
"","3013","TestDFSMkdir fails with multiple partitions","HDFS-15703 TestDFSMkdirs fails with multi-partitions.","closed","","prasad-acit","2021-05-14T09:20:08Z","2021-07-19T07:02:38Z"
"","3097","HDFS-15671. testBalancerRPCDelayQpsDefault fails intermittently","HDFS-15671 TestBalancerRPCDelay#testBalancerRPCDelayQpsDefault fails on Trunk  Some blocks cannot be replicated on the existing nodes which causes the unit test to fail during the creation of the file and waiting for the replications.  Setting `dfs.namenode.redundancy.considerLoad` to false.","closed","","amahussein","2021-06-10T20:23:29Z","2021-06-11T16:00:42Z"
"","2979","HADOOP-17665","Handle properly missing SSL key/trust store configurations when monitoring file changes in HttpServer2.","closed","","bolerio","2021-05-05T18:22:34Z","2021-05-10T20:31:48Z"
"","2978","HADOOP-17665","Handle properly missing SSL key/trust store configurations when monitoring file changes in HttpServer2.","closed","","bolerio","2021-05-05T18:14:42Z","2021-05-05T21:15:28Z"
"","3294","HADOOP-17845. Support IPv6 for ChecksumFileSystem","HADOOP-17845. Support IPv6 for ChecksumFileSystem","open","","prasad-acit","2021-08-11T10:56:45Z","2021-08-12T05:38:50Z"
"","3299","HADOOP-17844. Upgrade JSON smart to 2.4.7","HADOOP-17844. Upgrade JSON smart to 2.4.7","closed","","prasad-acit","2021-08-11T18:00:03Z","2021-08-17T07:52:42Z"
"","3131","HADOOP-17769. Upgrade JUnit to 4.13.2. fixes TestBlockRecovery","HADOOP-17769 Upgrade JUnit to 4.13.2 JUnit 4.13.1 has a bug that is reported in Junit issue-1652 Timeout ThreadGroups should not be destroyed.  The bug has been fixed in Junit-4.13.2  Backporting PR-3130 to branch-2.10","closed","","amahussein","2021-06-21T18:18:54Z","2021-06-23T14:35:45Z"
"","3130","HADOOP-17769. Upgrade JUnit to 4.13.2. fixes TestBlockRecovery","HADOOP-17769 Upgrade JUnit to 4.13.2 JUnit 4.13.1 has a bug that is reported in Junit issue-1652 _Timeout ThreadGroups should not be destroyed._  The bug has been fixed in Junit-4.13.2","closed","","amahussein","2021-06-21T18:12:27Z","2021-06-24T14:38:09Z"
"","3037","HADOOP-17723. [build] fix the Dockerfile for ARM","HADOOP-17723  verified manually by running  `dev-support/bin/create-release --asfrelease --docker --dockercache` on an ARM machine.","closed","","jojochuang","2021-05-21T09:18:24Z","2021-05-24T11:11:31Z"
"","2850","HADOOP-17614. Bump netty to the latest 4.1.61.","HADOOP-17614","closed","","jojochuang","2021-04-01T16:47:36Z","2021-04-05T00:36:57Z"
"","2774","HADOOP-17589 -  Support Custom Endpoint For Hadoop Azure working with Storage Emulator","HADOOP-17589 -  Support Custom Endpoint For Hadoop Azure working with Storage Emulator  Tested and worked with Azurite","open","","iryaniv","2021-03-14T12:48:28Z","2021-03-14T14:02:03Z"
"","3543","HADOOP-17123. remove guava Preconditions from Hadoop-common-project modules","HADOOP-17123. remove guava Preconditions from Hadoop-common-project modules  - all modules only required changing import lines except for hadoop-auth - hadoop-auth required removing guava calls manually since hadoop.common.util is not within the scope.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17123. Your PR title ...')?","closed","","amahussein","2021-10-11T21:40:09Z","2021-10-14T10:04:23Z"
"","2959","HADOOP-17669. Backport HADOOP-17079, HADOOP-17505 to branch-3.3","HADOOP-17079 in branch-3.3 has a number of trivial conflicts. HADOOP-17505 is a clean cherrypick.","closed","","jojochuang","2021-04-27T03:00:34Z","2021-05-18T01:57:46Z"
"","2880","HADOOP-17608. Fix TestKMS failure","HADOOP-16524 removed the reloader thread, so the related tests in TestKMS fail. Remove these.","closed","","aajisaka","2021-04-09T05:59:28Z","2021-04-12T03:54:01Z"
"","2715","HADOOP-17537. ABFS: Correct assertion reversed in HADOOP-13327","HADOOP-13327 introduces, among other changes, functions to simplify assert for checking stream capabilities. This change fixes (originally) assertFalse statements whose logic has been reversed to assertTrue by the [PR](https://github.com/apache/hadoop/pull/2587/files).","closed","","sumangala-patki","2021-02-22T09:27:45Z","2021-02-22T11:45:59Z"
"","3331","HADOOP-17862. ABFS: Fix unchecked cast compiler warning for AbfsListStatusRemoteIterator","Hadoop yetus run shows a java compiler warning for unchecked casting of `Object` to `Iterator` in a method of AbfsListStatusRemoteIterator class. This is resolved by introducing a new class to hold the iterator and exception thrown when applicable.  This PR also adds a potential fix and enables logging to facilitate better investigation of the occasional transient failure in the corresponding test class, tracked in HADOOP-17797. The additional logging introduced here affects only the test class for this functionality.","closed","","sumangala-patki","2021-08-25T05:51:10Z","2021-11-05T12:52:11Z"
"","3208","HADOOP-17796. Upgrade jetty version to 9.4.43.v20210629","Hadoop 17796. Upgrade jetty version to 9.4.43.v20210629","closed","","prasad-acit","2021-07-15T12:03:55Z","2021-07-22T08:15:01Z"
"","2945","HADOOP-17653. Do not use guava's Files.createTempDir().","Guava's Files.createTempDir() is deprecated. The recommendation is to use JDK's java.nio.file.Files.createTempDirectory() instead.  Some of the code follows the internal implementation of Guava createTempDir() https://github.com/google/guava/blob/master/guava/src/com/google/common/io/Files.java#L423","closed","","jojochuang","2021-04-21T12:22:26Z","2021-05-02T06:41:27Z"
"","2906","YARN-10736. Fix GetApplicationsRequest JavaDoc. Contributed by Miklos Gergely.","getName and setName javadoc comments are mixed up","closed","","miklosgergely","2021-04-14T13:03:49Z","2021-04-14T15:16:30Z"
"","3565","HADOOP-17953. S3A: Tests to lookup global or per-bucket configuration for encryption algorithm (#3525)","Followup to S3-CSE work of HADOOP-13887  Contributed by Mehakmeet Singh","closed","","mehakmeet","2021-10-19T17:14:01Z","2021-10-21T11:03:50Z"
"","2949","HADOOP-17657: implement StreamCapabilities in SequenceFile.Writer and fall back to flush, if hflush is not supported","Following exception is thrown whenever we invoke ProtoMessageWriter.hflush on S3 from Tez, which internally calls org.apache.hadoop.io.SequenceFile$Writer.hflush ->  org.apache.hadoop.fs.FS DataOutputStream.hflush -> S3ABlockOutputStream.hflush which is not implemented and throws java.lang.UnsupportedOperationException.    bdffe22d96ae [mdc@18060 class=""yarn.YarnUncaughtExceptionHandler"" level=""ERROR"" thread=""HistoryEventHandlingThread""] Thread Thread[HistoryEventHandlingThread, 5,main] threw an Exception.^Mjava.lang.UnsupportedOperationException: S3A streams are not Syncable^M at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.hflush(S3ABlockOutputStream.java:657)^M at org.apache.hadoop.fs.FS DataOutputStream.hflush(FSDataOutputStream.java:136)^M at org.apache.hadoop.io.SequenceFile$Writer.hflush(SequenceFile.java:1367)^M at org.apache.tez.dag.history.logging.proto.ProtoMessageWriter.hflush(ProtoMessageWr iter.java:64)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.finishCurrentDag(ProtoHistoryLoggingService.java:239)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.han dleEvent(ProtoHistoryLoggingService.java:198)^M at org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService.loop(ProtoHistoryLoggingService.java:153)^M at java.lang.Thread.run(Thread.java:748)^M  In order to fix this issue we should implement StreamCapabilities in SequenceFile.Writer. Also, we should fall back to flush(), if hflush() is not supported.","closed","","kishendas","2021-04-23T21:14:47Z","2021-05-04T08:20:57Z"
"","3446","HADOOP-17195. OutOfMemory error while uploading huge files to ABFS","Follow-up to #3406. Region: East US. `mvn -Dparallel-tests=abfs -DtestsThreadCount=8 -Dscale clean verify`  ### UT: ``` [INFO] Results: [INFO]  [WARNING] Tests run: 105, Failures: 0, Errors: 0, Skipped: 2 ```  ### IT: ``` [INFO] Results: [INFO]  [WARNING] Tests run: 564, Failures: 0, Errors: 0, Skipped: 76 ``` ## IT scale: ``` [INFO] Results: [INFO]  [ERROR] Failures:  [ERROR]   ITestAbfsReadWriteAndSeek.testReadAndWriteWithDifferentBufferSizesAndSeek:66->testReadWriteAndSeek:92 [Retry was required due to issue on server side] expected:<[0]> but was:<[1]> [INFO]  [ERROR] Tests run: 259, Failures: 1, Errors: 0, Skipped: 40 ```  New Tests: ``` [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 83.984 s - in org.apache.hadoop.fs.azurebfs.ITestAbfsHugeFiles [INFO] [INFO] Results: [INFO] [INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0  [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.443 s - in org.apache.hadoop.fs.store.TestDataBlocks [INFO] [INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 ```","closed","","mehakmeet","2021-09-16T06:34:46Z","2021-09-21T13:47:40Z"
"","3180","HADOOP-17402. Add GCS config to the core-site (#2638)","Follow up from: https://github.com/apache/hadoop/pull/2638#issuecomment-874308502","closed","","ravwojdyla","2021-07-06T18:09:56Z","2021-07-07T21:46:09Z"
"","3249","HADOOP-17822. fs.s3a.acl.default not working after S3A Audit feature","Fixes the regression caused by HADOOP-17511 by moving where the cannedACL properties are inited -so guaranteeing that they are valid before the RequestFactory is created.  Adds  * A unit test in TestRequestFactory to verify the ACLs are set   on all file write operations * A new ITestS3ACannedACLs test which verifies that ACLs really   do get all the way through.","closed","","steveloughran","2021-07-30T12:50:31Z","2021-10-15T19:49:43Z"
"","3197","HDFS-16125: fix the iterator for PartitionedGSet","fixed the issue that iterator will visit the first partition twice.   also fixed a typo in readTopdUnlock and removed the unused rootDir Inode in PartitionedGSet constructor.  added TestPartitionedGSet.java.   `mvn test -Dtest=TestPartitionedGSet ` passed.","closed","","xinglin","2021-07-13T00:39:48Z","2021-07-16T23:14:48Z"
"","3514","HADOOP-16532: Fix TestViewFsTrash to use the correct homeDir.","Fixed the homeDir issue reported in HADOOP-16532. We are finally able to pass this test for ViewFS.","closed","","xinglin","2021-10-03T14:31:36Z","2021-10-19T08:24:33Z"
"","3548","HDFS-16272. Fix int overflow in computing safe length during EC block recovery","Fix the int overflow problem.","closed","","cndaimin","2021-10-13T07:45:50Z","2021-10-18T10:55:55Z"
"","2730","HADOOP-17560. Fix some spelling errors","Fix some spelling errors  JIRA: https://issues.apache.org/jira/browse/HADOOP-17560","closed","","JiaguodongF","2021-03-01T13:26:00Z","2021-03-03T02:41:33Z"
"","3519","HADOOP-17932. Distcp file length comparison have no effect","Fix invocation in RetriableFileCopyCommand that's broken","closed","","adol001","2021-10-05T02:48:21Z","2021-10-18T10:08:13Z"
"","3181","HDFS-16116. Fix Hadoop federationBanance markdown bug.","Fix Hadoop federationBanance markdown bug.","closed","","xiaoxiaopan118","2021-07-07T00:59:53Z","2021-08-26T08:00:49Z"
"","2957","YARN-10752. Shaded guava not found when compiling with profile hbase2.0.","Fix a classpath typo.  Verified by running `mvn clean install -Dhbase.profile=2.0 -DskipTests -DskipITs`","closed","","jojochuang","2021-04-26T13:00:25Z","2021-04-26T15:42:14Z"
"","3690","HDFS-16340. Add exception message in DiskBlancer","during disk balance ,when we cannot get json from item ,only `Unable to get json from Item.` will be recorded, but no exception message printed, we can improve it.","closed","","GuoPhilipse","2021-11-20T09:25:00Z","2021-11-21T10:08:45Z"
"","3400","HADOOP-17896. ABFS: Stabilize OpenFile withStatus","DRAFT -- PR in Progress  Add support for OpenFile to handle more FileStatus types  - allows missing etag in other file types - will be modified if and when version is added as a member of org.apache.hadoop.fs.FileStatus","open","","sumangala-patki","2021-09-08T06:02:13Z","2022-04-12T16:30:46Z"
"","3376","HDFS-16208. Support Delete API with FGL.","Delete File / Directory should use FGL instead of global write lock.","open","","prasad-acit","2021-09-02T22:18:59Z","2022-07-29T03:20:01Z"
"","2845","HADOOP-17618. ABFS: Partially obfuscate SAS object IDs in Logs","Delegation SAS tokens are created using various parameters for specifying details such as permissions and validity. The requests are logged, along with values of all the query parameters. This change will partially mask values logged for the following object IDs representing the security principal: `skoid`, `saoid`, `suoid`","closed","","sumangala-patki","2021-03-31T12:57:16Z","2021-08-17T20:29:27Z"
"","3077","[Do not commit] Modularize docker images - debug","Debugging why the CI gets stuck.","closed","","GauthamBanasandra","2021-06-04T17:03:50Z","2021-06-07T07:32:43Z"
"","3641","HADOOP-18002. abfs rename idempotency broken -remove recovery","Cut modtime-based rename recovery as it doesn't work. Applications will have to use etag API of HADOOP-17979 and implement it themselves.  Why not do the HEAD and etag recovery in ABFS client? Cuts the IO capacity in half so kills job commit performance. The manifest committer of MAPREDUCE-7341 will do this recovery and act as the reference implementation of the algorithm.  Change-Id: I071fb31967ae63e0247e2f328d9cfd0e2423b2bf   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2021-11-10T15:31:30Z","2021-11-16T11:17:44Z"
"","3250","HADOOP-17825. Add BuiltInGzipCompressor","Currently, GzipCodec only supports BuiltInGzipDecompressor, if native zlib is not loaded. So, without Hadoop native codec installed, saving SequenceFile using GzipCodec will throw exception like ""SequenceFile doesn't work with GzipCodec without native-hadoop code!""  Same as other codecs which we migrated to using prepared packages (lz4, snappy), it will be better if we support GzipCodec generally without Hadoop native codec installed. Similar to BuiltInGzipDecompressor, we can use Java Deflater to support BuiltInGzipCompressor.","closed","","viirya","2021-07-31T04:35:50Z","2021-08-27T06:52:33Z"
"","3176","MAPREDUCE-7351 - CleanupJob during handle of SIGTERM signal","Currently MR CleanupJob happens when the job is either successful or fail. But during kill, it is not handled. This leaves all the temporary folders under the output path.  JIRA: https://issues.apache.org/jira/browse/MAPREDUCE-7351  During JobKill or Jvm shutdown, this patch deletes the temporary files in hdfs.  Testing done: 1. Killed the running job in map0%reduce0% , map0%reduce100%, it deleted the temporary files with this change.","closed","","shubhamod","2021-07-05T05:01:31Z","2021-07-07T03:38:15Z"
"","3124","HADOOP-17767. ABFS: Updates test scripts","Current test run scripts need manual update across all combinations in runTests.sh for account name and is working off a single azure-auth-keys.xml file. While having to test across accounts that span various geo, the config file grows big and also needs a manual change for configs such as fs.contract.test.[abfs/abfss] which has to be uniquely set. To use the script across various combinations, dev to be aware of the names of all the combinations defined in runTests.sh as well.  This PR updates the test scripts to address above concerns. An option to delete old test containers on the account is also added. Sample run output:  ```bash Choose action: [Note - SET_ACTIVE_TEST_CONFIG will help activate the config for IDE/single test class runs] 1) SET_ACTIVE_TEST_CONFIG	 3) CLEAN_UP_OLD_TEST_CONTAINERS 2) RUN_TEST #? 2 Enter parallel test run thread count [default - 8]: 4   Set the active test combination to run the action: 1) HNS-OAuth		 3) nonHNS-SharedKey	  5) All 2) HNS-SharedKey	 4) AppendBlob-HNS-OAuth  6) Quit #? 1   Combination specific property setting: [ key=fs.azure.account.auth.type , value=OAuth ]   Activated [src/test/resources/abfs-combination-test-configs.xml] - for account: ""storeaccountname"" for combination ""HNS-OAuth"" Running test for combination HNS-OAuth on account storeaccountname[ThreadCount=4] Result can be seen in dev-support/testlogs/2021-06-21_22-28-22/Test-Logs-HNS-OAuth.txt ``` This commit also includes 2 test fixes for test methods testCheckAccessForAccountWithoutNS and testAbfsStreamOps which were failing in nonHNS + SharedKey and HNS + AppendBlob mode respectively.","open","","snvijaya","2021-06-21T07:30:11Z","2022-05-12T03:32:41Z"
"","3268","YARN-10878. move TestNMSimulator off com.google","Converting from a class to a lambda expression removes all need to reference the google stuff","closed","","steveloughran","2021-08-04T15:29:55Z","2021-10-15T19:49:40Z"
"","2781","HADOOP-17548. ABFS: Toggle Store Mkdirs request overwrite parameter (#2729)","Contributed by Sumangala Patki.  (cherry picked from commit fe633d473935fe285a12821fb70b19cfc9aa9b8c)","closed","","sumangala-patki","2021-03-17T11:49:49Z","2021-05-10T06:20:01Z"
"","3106","HADOOP-17596. ABFS: Change default Readahead Queue Depth from num(processors) to const","Contributed by Sumangala Patki.  (cherry picked from commit 76d92eb2a22c71b5fcde88a9b4d2faec81a1cb9f)","closed","","sumangala-patki","2021-06-15T10:09:25Z","2021-07-10T09:39:59Z"
"","3258","HADOOP-17290. ABFS: Add Identifiers to Client Request Header (#2520)","Contributed by Sumangala Patki.  (cherry picked from commit 35570e414a6ab9130da8cac60e563168ee0d4793)","closed","","sumangala-patki","2021-08-03T04:45:13Z","2021-09-21T15:46:40Z"
"","2885","HADOOP-17576. ABFS: Disable throttling update for auth failures","Contributed by Sumangala Patki  (cherry picked from commit 6f640abbaf14efa98d6c599e5fff95647730ad42)","closed","","sumangala-patki","2021-04-09T10:46:49Z","2021-04-28T05:31:19Z"
"","3273","HADOOP-17618. ABFS: Partially obfuscate SAS object IDs in Logs (#2845)","Contributed by Sumangala Patki  (cherry picked from commit 3450522c2f5a4cf9b54dce4c25a71f1b4b98c446)","closed","","sumangala-patki","2021-08-06T06:14:18Z","2021-09-09T13:04:13Z"
"","3133","HADOOP-17771. S3AFS creation fails ""Unable to find a region via the region provider chain.""","Contributed by Steve Loughran.  Change-Id: I94284178d27a48947e7c0942a7c8565379de7e9b","closed","fs/s3,","steveloughran","2021-06-22T12:26:26Z","2021-06-24T15:40:23Z"
"","2920","HADOOP-17535. ABFS: ITestAzureBlobFileSystemCheckAccess test failure if no oauth key.","Contributed by Steve Loughran.","closed","","steveloughran","2021-04-16T13:32:19Z","2021-04-21T15:06:06Z"
"","3262","YARN-9875. Improve fair scheduler configuration store on HDFS.","Contributed by Prabhu Joseph  (cherry picked from commit 155864da006346a500ff35c2f6b69281093195b1)   Conflicts: 	hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/conf/TestFSSchedulerConfigurationStore.java  Backport YARN-9875 to branch-3.2 to fix TestFSSchedulerConfigurationStore.","closed","backport,","aajisaka","2021-08-04T02:45:11Z","2021-08-08T05:24:13Z"
"","2953","HADOOP-17112. S3A committers can't handle whitespace in paths.","Contributed by Krzysztof Adamski.   This is the fix from the JIRA with a space added to one of the test files to replicate the problem and then to verify the fix works.","closed","","steveloughran","2021-04-24T17:58:22Z","2021-04-25T17:33:55Z"
"","3597","HADOOP-17981 Support etag-assisted renames in FileOutputCommitter","Contains MAPREDUCE-7367. #2971 Parallelize file moves in FileOutputCommitter v1 job commit  Contributed by Rajesh Balamohan   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","steveloughran","2021-10-28T15:48:28Z","2021-11-03T14:14:05Z"
"","3113","HDFS-13671. Namenode deletes large dir slowly caused by FoldedTreeSet#removeAndGet","Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-06-17T07:28:45Z","2021-06-18T06:41:02Z"
"","3611","HADOOP-17981. resilient commit through etag validation","Compared to  #3597 this moves the recovery handling into the application code.  draft commit message  ---- Uses a new EtagSource API in hadoop-common (and implemented supported in ABFS) to implement recovery from failures during job commit of the form ""java.io.IOException: Failed to rename VersionedFileStatus""  After a rename() call fails, the etag of the file at the destination path is compared with that of the source file as determined when listing the parent directory. If the two match and the source file is no longer present, the rename is considered successful.  This algorithm relies on * the etag of files being different on different uploads   (at a minimum: when different data is uploaded to the same 	destination) * etag values persisted across renames.   (true for ADLS Gen2, not for S3) * directory list API calls returning the etags of all files.  To enable users MUST set mapreduce.fileoutputcommitter.algorithm.version.v1.experimental.parallel.rename.recovery to ""true""  users SHOULD set fs.azure.rename.raises.exceptions to true  That is not mandatory, but it gives better error reporting on all forms of rename failures, and when handling recoverable errors in job commits, reduces the number of IO requests by one, so reducing load in an IO-intensive phase of the job.  ---  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2021-11-02T18:16:51Z","2022-03-04T15:23:42Z"
"","2800","HDFS-15906. Close FSImage and FSNamesystem after formatting is complete for branch-3.2","Close FSImage and FSNamesystem after formatting is complete. For branch-3.2.","closed","","tomscut","2021-03-22T15:31:59Z","2022-05-14T00:10:24Z"
"","2894","HDFS-15966. Empty the statistical parameters when emptying the redundant queue","Clear the two indicators highestPriorityLowRedundancyReplicatedBlocks and highestPriorityLowRedundancyReplicatedBlocks when emptying the redundant queue.","closed","","langlaile1221","2021-04-12T09:26:09Z","2021-09-01T11:37:35Z"
"","2812","YARN-10712. Fix word errors in class comments","Class of LocalityMulticastAMRMProxyPolicy’s class comment has error word “thsi”   Part of the comment：  Rack localized {@link ResourceRequest}s are forwarded to the RMs that owns  the corresponding rack. Note that in some deployments each rack could be striped across multiple RMs. Thsi policy respects that. If the  {@link SubClusterResolver} cannot resolve this rack we default to forwarding  the {@link ResourceRequest} to the home sub-cluster.","closed","","ChaosJu","2021-03-24T09:34:57Z","2021-03-30T11:22:17Z"
"","3088","[Do not commit] CI for Centos 7","Checking to see if Jenkins can use the Centos 7 docker image.","closed","","GauthamBanasandra","2021-06-09T08:41:21Z","2021-06-16T17:20:40Z"
"","3240","HADOOP-17628. Distcp contract test is really slow with ABFS and S3A; timing out.","Changes  * subclass can declare whether or not -direct should be default * all tests then switch to that * major cut back on default depth/width of directories * if you set the file size of ""scale.test.distcp.file.size.kb"" to 0   the large file test case is skipped. * aggressive cutback on all needless mkdir() calls.  S3A suite * declares -direct always  ABFS suites * deletes superfluous ITestAbfsFileSystemContractSecureDistCp * uses abfs scale test timeout * only runs with -Dscale  All these changes bring execution time down from 4 min to 2 min against each store.","closed","test,","steveloughran","2021-07-27T14:14:17Z","2021-08-02T10:36:44Z"
"","3578","YARN-10757. jsonschema2pojo-maven-plugin version is not defined.","Change-Id: Ieacdcfe2d1cf005eebd6f9af2cc635ba5ae1fcab    ### Description of PR   ### How was this patch tested?  No warning message after the fix: ``` mvn clean install -DskipTests 2>&1 | grep ""jsonschema2pojo-maven-plugin"" [INFO] --- jsonschema2pojo-maven-plugin:1.1.1:generate (default) @ hadoop-yarn-server-resourcemanager --- ```  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-10-22T12:06:03Z","2021-12-07T13:57:35Z"
"","3572","HADOOP-17974. Import statements in hadoop-aws trigger clover failures.","Change-Id: I9a612dde6a717ac585e8d33a6b79f2600bfb9f7a    ### Description of PR   ### How was this patch tested?  ``` mvn -Pclover clean install -DskipTests --projects '!hadoop-client-modules/hadoop-client-check-invariants,!hadoop-client-modules/hadoop-client-check-test-invariants'  [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 19:53 min [INFO] Finished at: 2021-10-21T10:18:01+02:00 [INFO] ------------------------------------------------------------------------ ```   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-10-21T09:01:26Z","2021-10-21T17:31:28Z"
"","3457","YARN-10961. TestCapacityScheduler: reuse appHelper where feasible.","Change-Id: I859c9731e916d70a648fc724d9ce0c5a0db13a01    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-20T08:03:00Z","2021-09-21T14:13:05Z"
"","3002","Bump yargs-parser from 5.0.0 to 5.0.1 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [yargs-parser](https://github.com/yargs/yargs-parser) from 5.0.0 to 5.0.1.  Release notes Sourced from yargs-parser's releases.  yargs-parser v5.0.1 Bug Fixes  security: address GHSA-p9pc-299p-vxgp (#362) (1c417bd)     Changelog Sourced from yargs-parser's changelog.  5.0.0 (2017-02-18) Bug Fixes  environment variables should take precedence over config file (#81) (76cee1f)  BREAKING CHANGES  environment variables will now override config files (args, env, config-file, config-object)   5.0.1 (2021-03-10) Bug Fixes  security: address GHSA-p9pc-299p-vxgp (#362) (1c417bd)  4.2.1 (2017-01-02) Bug Fixes  flatten/duplicate regression (#75) (68d68a0)   4.2.0 (2016-12-01) Bug Fixes  inner objects in configs had their keys appended to top-level key when dot-notation was disabled (#72) (0b1b5f9)  Features  allow multiple arrays to be provided, rather than always combining (#71) (0f0fb2d)   4.1.0 (2016-11-07)   ... (truncated)   Commits  eab6c03 chore: release 5.0.1 (#363) 1c417bd fix(security): address GHSA-p9pc-299p-vxgp (#362) e93a345 chore: mark release in commit history (#361) ee15863 chore: push new package version 4774207 fix: back-porting prototype fixes for really old version (#271) See full diff in compare view    Maintainer changes This version was pushed to npm by oss-bot, a new releaser for yargs-parser since your current version.     [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=yargs-parser&package-manager=npm_and_yarn&previous-version=5.0.0&new-version=5.0.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-05-12T03:01:13Z","2021-05-19T03:15:09Z"
"","2843","Bump y18n from 3.2.1 to 3.2.2 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [y18n](https://github.com/yargs/y18n) from 3.2.1 to 3.2.2.  Commits  See full diff in compare view    Maintainer changes This version was pushed to npm by oss-bot, a new releaser for y18n since your current version.     [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=y18n&package-manager=npm_and_yarn&previous-version=3.2.1&new-version=3.2.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-03-31T06:49:42Z","2021-04-10T16:32:11Z"
"","3001","Bump websocket-extensions from 0.1.3 to 0.1.4 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [websocket-extensions](https://github.com/faye/websocket-extensions-node) from 0.1.3 to 0.1.4.  Changelog Sourced from websocket-extensions's changelog.  0.1.4 / 2020-06-02  Remove a ReDoS vulnerability in the header parser (CVE-2020-7662, reported by Robert McLaughlin) Change license from MIT to Apache 2.0     Commits  5ea0b42 Bump version to 0.1.4 29496f6 Remove ReDoS vulnerability in the Sec-WebSocket-Extensions header parser 4a76c75 Add Node versions 13 and 14 on Travis 44a677a Formatting change: {...} should have spaces inside the braces f6c50ab Let npm reformat package.json 2d211f3 Change markdown formatting of docs. 0b62083 Update Travis target versions. 729a465 Switch license to Apache 2.0. See full diff in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=websocket-extensions&package-manager=npm_and_yarn&previous-version=0.1.3&new-version=0.1.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","javascript,","dependabot[bot]","2021-05-12T03:00:36Z","2022-02-01T02:34:16Z"
"","2997","Bump underscore from 1.9.1 to 1.13.1 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [underscore](https://github.com/jashkenas/underscore) from 1.9.1 to 1.13.1.  Commits  943977e Merge branch 'umd-alias', tag 1.13.1 release 5630f88 Add version 1.13.1 to the change log 5aa5b52 Update the bundle sizes 76c8d8a Bump the version to 1.13.1 9cda0b0 Add some build clarifications to the documentation (#2923) 8b5928c Revert .gitignore underscore.js from 57a4a0e (fix #2923) 7054a54 Update generated sources and tag 1.13.0 release 37dc52a Merge pull request #2921 from jgonggrijp/prepare-1.13.0 5511d12 Add version 1.13.0 to the change log efe5fbf Bump the version to 1.13.0 Additional commits viewable in compare view    Maintainer changes This version was pushed to npm by jgonggrijp, a new releaser for underscore since your current version.     [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=underscore&package-manager=npm_and_yarn&previous-version=1.9.1&new-version=1.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-05-11T05:49:46Z","2021-05-13T12:54:17Z"
"","3004","YARN-10777. Bump node-sass from 4.13.0 to 4.14.1 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [node-sass](https://github.com/sass/node-sass) from 4.13.0 to 4.14.1.  Release notes Sourced from node-sass's releases.  v4.14.1 Community  Add GitHub Actions for Alpine CI (@​nschonni, #2823)  Fixes  Bump sass-graph@2.2.5 (@​xzyfer, #2912)  Supported Environments    OS Architecture Node     Windows x86 & x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14   OSX x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14   Linux* x86 & x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8**, 9**, 10**^, 11**^, 12**^, 13**^, 14**^   Alpine Linux x64 6, 8, 10, 11, 12, 13, 14   FreeBSD i386 amd64 10, 12, 13    *Linux support refers to Ubuntu, Debian, and CentOS 5+ ** Not available on CentOS 5 ^ Only available on x64 v4.14.0 Features  Add Node 14 support (@​nschonni, #2895)  Fixes  Reporting wrong LibSass version (@​saper, #2621)  Supported Environments    OS Architecture Node     Windows x86 & x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14   OSX x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14   Linux* x86 & x64 0.10, 0.12, 1, 2, 3, 4, 5, 6, 7, 8**, 9**, 10**^, 11**^, 12**^, 13**^, 14**^   Alpine Linux x64 6, 8, 10, 11, 12, 13, 14   FreeBSD i386 amd64 10, 12, 13    *Linux support refers to Ubuntu, Debian, and CentOS 5+ ** Not available on CentOS 5 ^ Only available on x64 v4.13.1 Community  Fix render example syntax (@​ZoranPandovski , #2787)    ... (truncated)   Changelog Sourced from node-sass's changelog.  v4.14.0 https://github.com/sass/node-sass/releases/tag/v4.14.0 v4.13.1 https://github.com/sass/node-sass/releases/tag/v4.13.1    Commits  0d6c3cc 4.14.1 1cc6263 Bump sass-graph@2.2.5 (#2915) aa193f6 chore: Add GitHub Actions for Alpine CI eac343c 4.14.0 bbeb78c Update changelog 1210aab Fix #2621: Report libsass version 3.5.5 (#2769) 5a4a48a feat: Add Node 14 support b54053a Update changelog 01db051 4.13.1 338fd7a Merge pull request from GHSA-f6rp-gv58-9cw3 Additional commits viewable in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=node-sass&package-manager=npm_and_yarn&previous-version=4.13.0&new-version=4.14.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-05-12T03:02:08Z","2021-05-19T03:14:34Z"
"","3000","Bump ini from 1.3.5 to 1.3.8 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [ini](https://github.com/isaacs/ini) from 1.3.5 to 1.3.8.  Commits  a2c5da8 1.3.8 af5c6bb Do not use Object.create(null) 8b648a1 don't test where our devdeps don't even work c74c8af 1.3.7 024b8b5 update deps, add linting 032fbaf Use Object.create(null) to avoid default object property hazards 2da9039 1.3.6 cfea636 better git push script, before publish instead of after 56d2805 do not allow invalid hazardous string as section name See full diff in compare view    Maintainer changes This version was pushed to npm by isaacs, a new releaser for ini since your current version.     [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","javascript,","dependabot[bot]","2021-05-12T03:00:10Z","2021-05-19T04:23:41Z"
"","2891","Bump http-proxy from 1.18.0 to 1.18.1 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [http-proxy](https://github.com/http-party/node-http-proxy) from 1.18.0 to 1.18.1.  Changelog Sourced from http-proxy's changelog.  v1.18.1 - 2020-05-17 Merged  Skip sending the proxyReq event when the expect header is present [#1447](https://github.com/http-party/node-http-proxy/issues/1447) Remove node6 support, add node12 to build [#1397](https://github.com/http-party/node-http-proxy/issues/1397)     Commits  9b96cd7 1.18.1 335aeeb Skip sending the proxyReq event when the expect header is present (#1447) dba3966 Remove node6 support, add node12 to build (#1397) See full diff in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=http-proxy&package-manager=npm_and_yarn&previous-version=1.18.0&new-version=1.18.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-04-10T16:35:14Z","2021-07-25T17:16:24Z"
"","2996","YARN-10778. Bump handlebars from 3.0.7 to 4.7.7 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [handlebars](https://github.com/wycats/handlebars.js) from 3.0.7 to 3.0.8.  Changelog Sourced from handlebars's changelog.  v3.0.8 - February 23rd, 2020 Bugfixes:  backport some (but not all) of the security fixes from 4.x - 156061e  Compatibility notes:  The properties __proto__, __defineGetter__, __defineSetter__ and __lookupGetter__ have been added to the list of ""dangerous properties"". If a property by that name is found and not an own-property of its parent, it will silently evaluate to undefined. This is done in both the compiled template and the ""lookup""-helper. This will prevent Remote-Code-Execution exploits that have been published in npm advisories 1324 and 1316. The check for dangerous properties has been changed from ""propertyIsEnumerable"" to ""hasOwnProperty"", as it is now done in Handlebars 4.6.0 and later.  Security issues resolved:  npm advisory 1324 npm advisory 1316 npm advisory 1325 npm advisory 1164  Commits    Commits  16bd606 v3.0.8 90ad8d9 Update release notes 156061e backport fixes from 4.x 8ba9159 update package-lock.json See full diff in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=handlebars&package-manager=npm_and_yarn&previous-version=3.0.7&new-version=3.0.8)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","open","javascript,","dependabot[bot]","2021-05-11T05:49:24Z","2021-05-19T04:34:09Z"
"","3003","Bump acorn from 5.7.3 to 5.7.4 in /hadoop-yarn-project/hadoop-yarn/hadoop-yarn-ui/src/main/webapp","Bumps [acorn](https://github.com/acornjs/acorn) from 5.7.3 to 5.7.4.  Commits  6370e90 Mark version 5.7.4 fbc15b1 More rigorously check surrogate pairs in regexp validator See full diff in compare view      [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=acorn&package-manager=npm_and_yarn&previous-version=5.7.3&new-version=5.7.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)  Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.  [//]: # (dependabot-automerge-start) [//]: # (dependabot-automerge-end)  ---   Dependabot commands and options   You can trigger Dependabot actions by commenting on this PR: - `@dependabot rebase` will rebase this PR - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it - `@dependabot merge` will merge this PR after your CI passes on it - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it - `@dependabot cancel merge` will cancel a previously requested merge and block automerging - `@dependabot reopen` will reopen this PR if it is closed - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself) - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself) - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language  You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/hadoop/network/alerts).","closed","javascript,","dependabot[bot]","2021-05-12T03:01:36Z","2021-05-13T12:51:52Z"
"","3207","HADOOP-17803. Remove WARN logging from LoggingAuditor when executing a request outside an audit span","Built successfully with `mvn -T 4 clean install -DskipTests`.","closed","","mehakmeet","2021-07-15T11:26:32Z","2021-07-16T10:59:36Z"
"","2796","HDFS-15911 : Provide blocks moved count in Balancer iteration result","branch-3.3 backport PR of #2794","closed","","virajjasani","2021-03-22T12:45:57Z","2021-03-24T05:51:21Z"
"","2942","HDFS-15989. Split TestBalancer and De-flake testMaxIterationTime()","branch-3.3 backport of PR #2923   Signed-off-by: Akira Ajisaka  Signed-off-by: Takanobu Asanuma","closed","","virajjasani","2021-04-21T07:29:35Z","2021-04-23T03:57:47Z"
"","2797","HDFS-15911 : Provide blocks moved count in Balancer iteration result","branch-3.2 backport PR of #2794","closed","","virajjasani","2021-03-22T13:49:05Z","2021-03-24T17:13:59Z"
"","2943","HDFS-15989. Split TestBalancer and De-flake testMaxIterationTime()","branch-3.2 backport of PR #2923   Signed-off-by: Akira Ajisaka  Signed-off-by: Takanobu Asanuma","closed","","virajjasani","2021-04-21T07:37:45Z","2021-04-23T03:58:16Z"
"","2799","HDFS-15911 : Provide blocks moved count in Balancer iteration result","branch-3.1 backport PR of #2794","closed","","virajjasani","2021-03-22T15:18:26Z","2021-03-24T17:19:57Z"
"","2944","HDFS-15989. Split TestBalancer and De-flake testMaxIterationTime()","branch-3.1 backport of PR #2923   Signed-off-by: Akira Ajisaka  Signed-off-by: Takanobu Asanuma","closed","","virajjasani","2021-04-21T08:10:09Z","2021-04-23T04:27:41Z"
"","3265","YARN-10809. Missing dependency causing NoClassDefFoundError in TestHBaseTimelineStorageUtils","Backport YARN-10809 to branch-3.2. Maybe we need to backport to branch-2.10 as well.","closed","backport,","aajisaka","2021-08-04T08:21:46Z","2021-08-08T05:23:24Z"
"","2817","HADOOP-17222. Create socket address leveraging URI cache","Backport to branch-3.3.  The  only conflict was on the imports on:  hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java","closed","","sodonnel","2021-03-24T17:47:35Z","2021-03-30T10:59:45Z"
"","2815","HADOOP-13551. AWS metrics wire-up (#2778)","Backport of trunk #2778 to 3.3; had to include another patch to avoid merge conflict.  Not yet done any s3 tests","closed","","steveloughran","2021-03-24T16:57:12Z","2021-03-27T15:46:19Z"
"","2814","HADOOP-13551. AWS metrics wire-up (#2778)","Backport of trunk #2778 to 3.3; had to include another patch to avoid merge conflict.  Not yet done any s3 tests","closed","","steveloughran","2021-03-24T16:55:43Z","2021-03-24T16:57:20Z"
"","3264","HADOOP-16768. SnappyCompressor test cases wrongly assume that the compressed data is always smaller than the input data.","Backport HADOOP-16768 to fix TestSnappyCompressorDecompressor and TestCompressorDecompressor.  https://ci-hadoop.apache.org/job/hadoop-qbt-branch-3.2-java8-linux-x86_64/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt  ``` [ERROR] Failures:  [ERROR]   TestCompressorDecompressor.testCompressorDecompressor:69  Expected to find 'testCompressorDecompressor error !!!' but got unexpected exception: java.lang.NullPointerException 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:877) 	at com.google.common.base.Joiner.toString(Joiner.java:452) 	at com.google.common.base.Joiner.appendTo(Joiner.java:109) 	at com.google.common.base.Joiner.appendTo(Joiner.java:152) 	at com.google.common.base.Joiner.join(Joiner.java:195) 	at com.google.common.base.Joiner.join(Joiner.java:185) 	at com.google.common.base.Joiner.join(Joiner.java:211) 	at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:329) 	at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:135) 	at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressor(TestCompressorDecompressor.java:66) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)  [ERROR]   TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit:92  Expected to find 'testCompressorDecompressorWithExeedBufferLimit error !!!' but got unexpected exception: java.lang.NullPointerException 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:877) 	at com.google.common.base.Joiner.toString(Joiner.java:452) 	at com.google.common.base.Joiner.appendTo(Joiner.java:109) 	at com.google.common.base.Joiner.appendTo(Joiner.java:152) 	at com.google.common.base.Joiner.join(Joiner.java:195) 	at com.google.common.base.Joiner.join(Joiner.java:185) 	at com.google.common.base.Joiner.join(Joiner.java:211) 	at org.apache.hadoop.io.compress.CompressDecompressTester$CompressionTestStrategy$2.assertCompression(CompressDecompressTester.java:329) 	at org.apache.hadoop.io.compress.CompressDecompressTester.test(CompressDecompressTester.java:135) 	at org.apache.hadoop.io.compress.TestCompressorDecompressor.testCompressorDecompressorWithExeedBufferLimit(TestCompressorDecompressor.java:89) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)  [ERROR] Errors:  [ERROR]   TestSnappyCompressorDecompressor.testSnappyCompressDecompress:192 Â» Internal C... [ERROR]   TestSnappyCompressorDecompressor.testSnappyCompressDecompressInMultiThreads:409 Â» Runtime ```","closed","backport,","aajisaka","2021-08-04T03:16:54Z","2021-08-08T05:23:42Z"
"","3666","Backport HDFS-16315 for branch-3.3","Backport [HDFS-16315](https://issues.apache.org/jira/browse/HDFS-16315) for branch-3.3.","closed","","tomscut","2021-11-16T06:31:57Z","2021-11-17T05:03:57Z"
"","2951","MAPREDUCE-7339. Making ALREADY_SPECULATING more priority","ASF Jira: https://issues.apache.org/jira/browse/MAPREDUCE-7339  Change: DefaultSpeculator returns TOO_LATE_TO_SPECULATE or PROGRESS_IS_GOOD speculation value even if task is ALREADY SPECULATING. This PR targets to fix that","open","","mudit-97","2021-04-24T05:33:30Z","2021-05-25T14:01:54Z"
"","3377","HADOOP-17887. Remove the wrapper class GzipOutputStream","As we provide built-in gzip compressor, we can use it in compressor stream. The wrapper `GzipOutputStream` can be removed now.  BTW, I did a microbenchmark by running 10 times of compressing/decompresing random data.  The average time:  After: 12.93s Before: 13.52s  It is pretty close.","closed","","viirya","2021-09-03T01:44:47Z","2021-09-09T04:23:41Z"
"","2965","HADOOP-17675 LdapGroupsMapping$LdapSslSocketFactory ClassNotFoundException","As stated in this article: https://www.infoworld.com/article/2077344/find-a-way-out-of-the-classloader-maze.html  A native thread has its context classloader set to null by default. If the context classloader which is used internally by JNDI to load a class is null, then the bootstrap classloader is used, according to the apidoc here: https://docs.oracle.com/javase/8/docs/api/java/lang/Class.html#forName-java.lang.String-boolean-java.lang.ClassLoader-  JNDI uses this form with the context classloader as can be seen here: https://github.com/openjdk/jdk11u/blob/master/src/java.naming/share/classes/com/sun/jndi/ldap/VersionHelper.java#L107 or here: https://github.com/openjdk/jdk8u/blob/master/jdk/src/share/classes/com/sun/jndi/ldap/VersionHelper12.java#L72  In Impala this call happens from a Thread created in native space, so in that case, the System/Application classloader loads LdapSslSocketFactory fine in LdapGroupsMapping.getDirContext() while creating the environment, but then InitialDirContext constructor gets to instantiation the LdapSslSocketFactory inside JNDI with the help of the linked VersionHelper impelmentations, and fails to load the class with the bootstrap classloader as context classloader is null.  In order to solve this problem, we can safely use the classloader of the LdapGroupsMapping class, as it had to load the LdapSslSocketFactory class before.  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","fapifta","2021-04-29T23:59:05Z","2021-05-04T10:33:59Z"
"","3221","HADOOP-17811: ABFS ExponentialRetryPolicy doesn't pick up configuration values","Allow ExponentialRetryPolicy to be fully configured from the hadoop configuration. Properties are already defined for this, so ensure that they are passed along when the retry policy is constructed and that documentation has been updated.","closed","","brianloss","2021-07-21T20:50:36Z","2021-08-02T10:38:45Z"
"","3096","[HADOOP-17758][HDFS][FOLLOWUP] NPE and excessive warnings after HADOOP-17728","After HADOOP-17728, `StatisticsDataReferenceCleaner` may appear the following questions, That's beacuse `ReferenceQueue.remove` might return `null`.  2021-06-09 21:51:12,334 WARN  [org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner] fs.FileSystem (FileSystem.java:run(4025)) - Exception in the cleaner thread but it will continue to run java.lang.NullPointerException 	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:4020) 	at java.lang.Thread.run(Thread.java:748)","closed","","yikf","2021-06-10T15:41:35Z","2021-06-10T15:53:14Z"
"","3312","HADOOP-17851 Support user specified content encoding for S3A","Adds support for user specified content encoding for S3A in the option `fs.s3a.content.encoding`","closed","","holdenk","2021-08-19T03:47:35Z","2022-03-09T22:12:49Z"
"","3216","YARN-10862. Add new proto changes required for NodeHealthDetails in NodeHealthStatus","Adds new proto changes required for NodeHealthDetails.   Linked Jira: [YARN-10862](https://issues.apache.org/jira/browse/YARN-10862)","open","","cyrus-jackson","2021-07-19T23:12:06Z","2021-07-26T11:14:54Z"
"","2723","HADOOP-13126 Add BrotliCodec based on Brotli4j library","Adds BrotliCodec - a compression codec based on [Google Brotli](https://github.com/google/brotli)  This PR is a continuation on the work done by @rdblue at https://issues.apache.org/jira/browse/HADOOP-13126 In his patches it was based on [jbrotli](https://github.com/MeteoGroup/jbrotli) library but this library is not maintained since few years. My PR uses [Brotli4j](https://github.com/hyperxpro/Brotli4j)","open","","martin-g","2021-02-25T15:02:20Z","2022-06-24T11:07:46Z"
"","2801","HADOOP-17597. Optionally downgrade on S3A Syncable calls","Adds a new option fs.s3a.downgrade.syncable.exceptions Which converts calls to Syncable hsync/hflush on S3A output streams to  * log once at warn (for entire process life, not just the stream) * increment IOStats with the relevant operation counter  With the downgrade not enabled (default) * IOStats also incremented * The UnsupportedOperationException current raised includes a link to this   JIRA","closed","","steveloughran","2021-03-22T15:52:54Z","2021-04-26T12:02:56Z"
"","3467","HADOOP-17195. ABFS: OutOfMemory error while uploading huge files (#3446)","Addresses the problem of processes running out of memory when there are many ABFS output streams queuing data to upload, especially when the network upload bandwidth is less than the rate data is generated.  ABFS Output streams now buffer their blocks of data to ""disk"", ""bytebuffer"" or ""array"", as set in ""fs.azure.data.blocks.buffer""  When buffering via disk, the location for temporary storage is set in ""fs.azure.buffer.dir""  For safe scaling: use ""disk"" (default); for performance, when confident that upload bandwidth will never be a bottleneck, experiment with the memory options.  The number of blocks a single stream can have queued for uploading is set in ""fs.azure.block.upload.active.blocks"". The default value is 20.  Contributed by Mehakmeet Singh.","closed","","mehakmeet","2021-09-21T17:03:41Z","2021-09-22T10:19:16Z"
"","3276","HADOOP-17837: Add unresolved endpoint value to UnknownHostException (ADDENDUM)","Addresses a comment made in https://github.com/apache/hadoop/pull/3272.  The other comment about concatenating the UnresolvedAddressException message is unnecessary. In all versions of java it's impossible to add a message to an UnresolvedAddressException.  See [java11](https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/nio/channels/UnresolvedAddressException.html), there is only a no-args constructor and no `setMessage()` method.","closed","","bbeaudreault","2021-08-06T16:19:18Z","2021-08-08T12:28:13Z"
"","3056","HDFS-15916. Addendum. DistCp: Backward compatibility: Distcp fails from Hadoop 3 to Hadoop 2 for snapshotdiff.","Addendum, fixing log to DFSClient.LOG","closed","","ayushtkn","2021-05-26T17:43:16Z","2021-06-09T02:34:46Z"
"","2820","HADOOP-17531.Addendum: DistCp: Reduce memory usage on copying huge directories.","Addendum Patch https://issues.apache.org/jira/browse/HADOOP-17531","closed","","ayushtkn","2021-03-26T13:00:59Z","2021-03-26T21:31:41Z"
"","3653","HDFS-16319. Add metrics doc for ReadLockLongHoldCount and WriteLockLo…","Add metrics doc for ReadLockLongHoldCount and WriteLockLongHoldCount. See [HDFS-15808](https://issues.apache.org/jira/browse/HDFS-15808).","closed","","tomscut","2021-11-12T22:27:27Z","2021-11-14T14:47:52Z"
"","2975","HADOOP-17682. ABFS: Support FileStatus input to OpenFileWithOptions() via OpenFileParameters","ABFS `open` methods require certain information (contentLength, eTag, etc) to create an InputStream for the file at the given path. This information is retrieved via a `GetFileStatus` request to backend.  However, client applications may often have access to the FileStatus prior to invoking the open API. Providing this FileStatus to the driver through the OpenFileParameters argument of `openFileWithOptions()` can help avoid the call to Store for FileStatus.  This PR adds handling for the FileStatus instance (if any) provided via the `OpenFileParameters` argument.","closed","","sumangala-patki","2021-05-04T12:08:39Z","2021-09-08T00:40:36Z"
"","3662","HADOOP-18011. ABFS: Configurable HTTP connection and read timeouts","ABFS driver has a default connection timeout and read timeout value of 30 secs. For jobs that are time sensitive, preference would be quick failure and have shorter HTTP connection and read timeout.  This change enables 2 configs: fs.azure.http.connection.timeout and fs.azure.http.read.timeout that allows custom values to be configured for default HTTP connection and read timeout.   All the integration tests were run over ABFS accounts (pasted in PR conversation tab). New checks are added to the tests for config update and socket timeout.","open","","snvijaya","2021-11-15T18:10:25Z","2022-05-12T01:02:07Z"
"","2848","YARN-10493: RunC container repository v2","A new version of the image container repository. This aligns with the format that is used in the new Java CLI tool in YARN-10494.","open","","mbsharp","2021-04-01T14:21:35Z","2021-08-11T19:27:34Z"
"","2789","YARN-10493: RunC container repository v2","A new version of the image container repository.  This aligns with the format that is used in the new Java CLI tool in YARN-10494.","closed","","mbsharp","2021-03-19T15:41:38Z","2021-03-29T13:33:04Z"
"","2971","MAPREDUCE-7341. Intermediate Manifest Committer","A File output committer which uses a manifest file to pass the lists of directories to create and files to rename from task attempts to job committer; job commit does dir creation and file rename across a pool of threads. Based on lessons/code from the S3A committer,   It is faster than the V1 algorithm, does not require atomic/O(1) directory rename. It will be slower that v2 as files are renamed in job commit, but as it has prepared the lists of files to copy, directory creation and file rename (with deletes of destination artifacts) are all that is needed.  This committer works with any consistent filesystem/objects store for which file rename is fast. It does not perform any directory renames, so has no requirements there. It uses `listStatusIterator()` to walk the tree of files to commit. This makes it straightforward to also build up the list of dirs to create -but it does mean it will underperform against any store for which a recursive `listStatus(path, true)` is significantly faster. That is true for S3A, but this committer is not for use there. rename() takes too long.  It is targeted at Apache Spark and does not support job recovery. There's no fundamental reason why this could not be added by a sufficiently motivated individual.  Status:  - [x] file formats.  - [X] implementation architecture ""stages you can chain"" done  - [X] Fork of S3A ITest contract test done, with implementations for local FS *and* ABFS.  - [X] ABFS Terasort ITest  - [X] Basic spark tests with validation of _SUCCESS output  ### Features  - [X] build stage config from job/task config - [X] directory prepare/cleanup (parallelised) - [X] wire up committer - [X] progress callbacks - [X] delete all task attempts in parallel for performance/scale on gcs and abfs when oauth authenticated - [X] Always save _SUCCESS results to a report dir, even on failure - [x] each task attempt to save its preparation time to the iostats in task manifest; job stats to aggregate these for ease of measuring task commit performance.  Improvements  - [x] improve performance of PrepareDirectoriesStage - [x] RateLimiting class in hadoop common to wrap guava one; hide from modules the origin of the limiter. This could automatically update IOStats source. (oh no, it'll need a test too...)  ### Tests  Functional tests are done on a par with s3a committers, want to add better fault injection (the design lines up for this nicely as we can test each stage in isolation), and some scale tests.  I'm adding spark integration tests in https://github.com/hortonworks-spark/cloud-integration ;already (july 2021) working   - [x] job/task conf to manifest committer conf       and on to stage conf. Paths valid etc. - [X] round trip of manifest - [X] _SUCCESS - [ ] individual stages with large fake dir tree - [x] directory merge - [x] fail fast if job or task attempt directory exists - [X] abstract protocol test - [ ] Source File missing on rename - [x] Dest File is file/dir - [x] Parent entry of a directory to create is actually a file. - [ ] other job overwrites parent dir with file - [ ] two jobs overwrite same file  (will only surface in validate) - [ ] two task attempts writing to same dest - [ ] multi-task job commit with failure in one of the tasks while others are active - [ ] SPARK_WRITE_UUID picked up from job to _SUCCESS - [ ] overwrite an existing tree with new data - [x] cleanup stage: rename to trash option - [x] cleanup stage: parallel TA delete. Use iostats to count #of deletes, or return # in return value. - [x] many task attempts in a task dir - [x] wrong task ID in a loaded TA? actually, we don't check. - [ ] rate limited job commit on local fs with triggering","closed","mapreduce,","steveloughran","2021-05-03T11:22:17Z","2022-04-06T18:55:10Z"
"","3306","HDFS-16176. fix Client EC -unsetPolicy not success","A directory explicitly set ecpolicy, when use -unsetPolicy to remove the setting, it has been unsuccessful. Expect to delete the setting successfully","closed","","fuchaohong","2021-08-16T09:14:30Z","2021-08-18T08:40:03Z"
"","3655","HDFS-16321. Fix invalid config in TestAvailableSpaceRackFaultTolerantBPP","`TestAvailableSpaceRackFaultTolerantBPP` seems setting invalid param(valid in `TestAvailableSpaceBlockPlacementPolicy`), we can fix it to avoid further trouble.","closed","","GuoPhilipse","2021-11-13T12:14:51Z","2021-11-25T11:57:53Z"
"","2701","HADOOP-17528. SFTP File System: close the connection pool when closing a FileSystem","`SFTPFileSystem` leverages a connection pool which is not closed when a file system instance gets closed preventing a JVM from exiting as every established SFTP connection runs in a separate non-daemon thread. This PR fixes `SFTPFileSystem` closure.","closed","","mpryahin","2021-02-15T06:33:11Z","2021-02-23T17:03:43Z"
"","3202","HADOOP-17801. No error message reported when bucket doesn't exist in S3AFS","`mvn clean verify -Dparallel-tests -DtestsThreadCount=4 -Dscale` Region: ap-south-1 All tests ran successfully.  Changed test:  ``` [INFO] Running org.apache.hadoop.fs.s3a.TestS3AExceptionTranslation [INFO] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.367 s - in org.apache.hadoop.fs.s3a.TestS3AExceptionTranslation ```","closed","","mehakmeet","2021-07-14T08:51:12Z","2021-07-16T14:27:06Z"
"","3684","HDFS-16338. Correct fsimage error configuration message","`dfs.namenode.checkpoint.edits.dir` may be different from `dfs.namenode.checkpoint.dir` , if `checkpointEditsDirs` is null or empty, error message should warn the edit dir configuration, we can fix it.","closed","","GuoPhilipse","2021-11-19T09:24:29Z","2021-12-06T08:59:34Z"
"","3678","HDFS-16334. Correct NameNode ACL description","`dfs.namenode.acls.enabled` is set to be `true` by default after HDFS-13505 ,we can improve the desc","closed","","GuoPhilipse","2021-11-18T10:04:57Z","2021-11-25T11:58:24Z"
"","3669","HDFS-16328. Correct disk balancer desc","`dfs.disk.balancer.enabled` is enabled by default after HDFS-13153, we can improve the doc to avoid confusion","closed","","GuoPhilipse","2021-11-17T08:50:13Z","2021-11-25T11:58:11Z"
"","2899","YARN-10733. TimelineService Hbase tests failing with timeouts","[YARN-10733:](https://issues.apache.org/jira/browse/YARN-10733) TimelineService Hbase tests are failing with timeout error on branch-2.10  Timeout running `hadoop-yarn-server-timelineservice-hbase-tests`. The failure of the tests is due to test unit `TestHBaseStorageFlowRunCompaction` getting stuck. Upon checking the surefire reports, I found several Class no Found Exceptions.  ```bash Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/CanUnbuffer 	at java.lang.ClassLoader.defineClass1(Native Method) 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763) 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74) 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369) 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363) 	at java.security.AccessController.doPrivileged(Native Method) 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) 	at org.apache.hadoop.hbase.regionserver.StoreFileInfo.(StoreFileInfo.java:66) 	at org.apache.hadoop.hbase.regionserver.HStore.createStoreFileAndReader(HStore.java:698) 	at org.apache.hadoop.hbase.regionserver.HStore.validateStoreFile(HStore.java:1895) 	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:1009) 	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:2523) 	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2638) 	... 33 more Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.CanUnbuffer 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) 	... 51 more ```","closed","","amahussein","2021-04-12T20:42:43Z","2021-04-14T18:44:39Z"
"","2705","MAPREDUCE-7320. cleanup test data after ClusterMapReduceTestCase","[MAPREDUCE-7320: ClusterMapReduceTestCase does not clean directories](https://issues.apache.org/jira/browse/MAPREDUCE-7320)  Running Junits that extend ClusterMapReduceTestCase generate lots of directories and folders without cleaning them up.  This PR addresses cleaning up the clusters directories after finishing the unit tests.  - It touches `MiniYARNCluster.java` in order to change the base dir to `target/test-dir/$TEST_CLASS_NAME` - test classes affected    - TestMRJobClient,   - TestStreamingBadRecords,   - TestClusterMapReduceTestCase,   - TestBadRecords.   - TestMRCJCJobClient,   - TestJobName  I tested the TestUnits that use `MiniYARNCluster.java` such as:  ```bash Class     MiniYARNCluster          TestOSSMiniYarnCluster  (3 usages found)         TestMRTimelineEventHandling  (4 usages found)         TestJobHistoryEventHandler  (3 usages found)         TestHadoopArchiveLogs  (3 usages found)         TestHadoopArchiveLogsRunner  (3 usages found)         TestDynamometerInfra  (3 usages found)         TestDSTimelineV10         TestDSTimelineV20         TestDSTimelineV15         TestUnmanagedAMLauncher  (3 usages found)         TestApplicationMasterServiceProtocolForTimelineV2         TestFederationRMFailoverProxyProvider  (3 usages found)         TestHedgingRequestRMFailoverProxyProvider  (4 usages found)         TestNoHaRMFailoverProxyProvider  (5 usages found)         TestRMFailover  (4 usages found)         TestAMRMClient         TestAMRMClientPlacementConstraints         TestAMRMProxy  (5 usages found)         TestNMClient  (3 usages found)         TestOpportunisticContainerAllocationE2E  (3 usages found)         TestYarnClient  (3 usages found)         TestYarnClientWithReservation  (12 usages found)         TestYarnCLI  (7 usages found)         TestContainerManagerSecurity  (2 usages found)         TestDiskFailures  (2 usages found)         TestMiniYarnCluster  (9 usages found)         TestMiniYARNClusterForHA  (2 usages found)         TestMiniYarnClusterNodeUtilization  (3 usages found)         TestEncryptedShuffle ```","closed","","amahussein","2021-02-17T18:45:07Z","2021-02-24T18:00:19Z"
"","2722","MAPREDUCE-7320. organize test directories for ClusterMapReduceTestCase","[MAPREDUCE-7320: ClusterMapReduceTestCase does not clean directories](https://issues.apache.org/jira/browse/MAPREDUCE-7320)  Running Junits that extend ClusterMapReduceTestCase generate lots of directories and folders all over the place.  This PR addresses organizing the directories generated by the unit test, cleaning them up at the beginning of the execution if necessary.  - It touches `MiniYARNCluster.java` in order to change the base dir to `target/test-dir/$TEST_CLASS_NAME` - test classes affected    - TestMRJobClient,   - TestStreamingBadRecords,   - TestClusterMapReduceTestCase,   - TestBadRecords.   - TestMRCJCJobClient,   - TestJobName  I tested the TestUnits that use `MiniYARNCluster.java` such as:  ```bash Class     MiniYARNCluster          TestOSSMiniYarnCluster  (3 usages found)         TestMRTimelineEventHandling  (4 usages found)         TestJobHistoryEventHandler  (3 usages found)         TestHadoopArchiveLogs  (3 usages found)         TestHadoopArchiveLogsRunner  (3 usages found)         TestDynamometerInfra  (3 usages found)         TestDSTimelineV10         TestDSTimelineV20         TestDSTimelineV15         TestUnmanagedAMLauncher  (3 usages found)         TestApplicationMasterServiceProtocolForTimelineV2         TestFederationRMFailoverProxyProvider  (3 usages found)         TestHedgingRequestRMFailoverProxyProvider  (4 usages found)         TestNoHaRMFailoverProxyProvider  (5 usages found)         TestRMFailover  (4 usages found)         TestAMRMClient         TestAMRMClientPlacementConstraints         TestAMRMProxy  (5 usages found)         TestNMClient  (3 usages found)         TestOpportunisticContainerAllocationE2E  (3 usages found)         TestYarnClient  (3 usages found)         TestYarnClientWithReservation  (12 usages found)         TestYarnCLI  (7 usages found)         TestContainerManagerSecurity  (2 usages found)         TestDiskFailures  (2 usages found)         TestMiniYarnCluster  (9 usages found)         TestMiniYARNClusterForHA  (2 usages found)         TestMiniYarnClusterNodeUtilization  (3 usages found)         TestEncryptedShuffle ```   MAPREDUCE-7320. fix TestEncryptedShuffle paths MAPREDUCE-7320. move cleaning up root directory to setup","closed","","amahussein","2021-02-24T18:17:03Z","2021-02-26T19:42:34Z"
"","2695","MAPREDUCE-7141. Allow KMS generated spill encryption keys","[MAPREDUCE-7141: Allow KMS generated spill encryption keys](https://issues.apache.org/jira/browse/MAPREDUCE-7141) Add KMS support to generate key for the encryption of the spilled data on disk. The feature improves fault tolerance to AM failures/re-runs and also gives another option to the client on how it wants the keys to be created. The current implementation assumed that the KMS key can be retrieved from the DFS.","open","","amahussein","2021-02-09T16:00:18Z","2021-02-09T20:09:13Z"
"","3557","HDFS-16276. RBF: Remove the useless configuration of rpc isolation in md","[HDFS-16276](https://issues.apache.org/jira/browse/HDFS-16276) ### Description of PR The dfs.federation.router.fairness.enable configuration is not used in the code, but there is it in md, we should delete it.","closed","","zhuxiangyi","2021-10-15T09:08:14Z","2021-10-18T04:53:42Z"
"","3063","HDFS-16043. Add markedDeleteBlockScrubberThread to delete blocks asynchronously","[HDFS-16043](https://issues.apache.org/jira/browse/HDFS-16043)","closed","","zhuxiangyi","2021-05-31T10:02:36Z","2022-01-13T11:53:23Z"
"","3058","HDFS-16042. DatanodeAdminMonitor scan should be delay based","[HDFS-16042](https://issues.apache.org/jira/browse/HDFS-16042) DatanodeAdminMonitor scan should be delay based.  In DatanodeAdminManager.activate(), the Monitor task is scheduled with a fixed rate, ie. the period is from start1 -> start2.  It should be a fixed delay so it's end1 -> start1.","closed","","amahussein","2021-05-26T20:27:01Z","2021-06-09T15:20:41Z"
"","2981","HDFS-16008. RBF: Tool to initialize ViewFS Mapping to Router","[HDFS-16008](https://issues.apache.org/jira/browse/HDFS-16008)","open","","zhuxiangyi","2021-05-06T03:23:55Z","2021-05-24T03:05:33Z"
"","2964","HDFS-16000. Rename performance optimization","[HDFS-16000](https://issues.apache.org/jira/browse/HDFS-16000)","open","","zhuxiangyi","2021-04-28T12:05:30Z","2021-09-27T09:43:16Z"
"","2919","HDFS-15979. Move within EZ fails and cannot remove nested EZs. Contri…","[HDFS-15979](https://issues.apache.org/jira/browse/HDFS-15979) Move within EZ fails and cannot remove nested EZs  The changes were contributed by @daryn-sharp and we have our internal clusters running on those changes with hadoop-2.8 and hadoop-2.10. I made some modifications in order to handle the conflict since our internal branch has a feature that is not merged yet into the community (HDFS-13009).","open","","amahussein","2021-04-16T12:15:38Z","2021-10-07T15:32:00Z"
"","2915","HDFS-15974. RBF: Unable to display the datanode UI of the router","[HDFS-15974](https://issues.apache.org/jira/browse/HDFS-15974)","closed","","zhuxiangyi","2021-04-16T02:59:43Z","2021-04-23T01:33:04Z"
"","3371","HADOOP-17886. Upgrade ant to 1.10.11","[HADOOP-17886](https://issues.apache.org/jira/browse/HADOOP-17886) Upgrade ant to 1.10.11  Vulnerabilities reported in org.apache.ant:ant 1.10.9      CVE-2021-36374 moderate severity     CVE-2021-36373 moderate severity","closed","","amahussein","2021-09-01T20:16:40Z","2021-09-02T21:11:07Z"
"","3370","HADOOP-17885. Upgrade JSON smart to 1.3.3 on branch-2.10","[HADOOP-17885](https://issues.apache.org/jira/browse/HADOOP-17885) Upgrade JSON smart to 1.3.3 on branch-2.10  Currently branch-2.10 is using JSON Smart 1.3.1 version which is vulnerable to CVE-2021-27568.  We can upgrade the version to 1.3.1.","closed","","amahussein","2021-09-01T19:36:53Z","2021-09-02T21:40:06Z"
"","3040","HADOOP-17458. S3A to treat ""SdkClientException: Data read has a different length than the expected"" as EOFException","[HADOOP-17458](https://issues.apache.org/jira/browse/HADOOP-17458)  Some network exceptions would cause `SdkClientException` to fail with a `Data read has a different length than the expected` message. These should be recoverable.  - Modifying `translateException` to recover from the above exception;  Tested with `eu-west-1` and `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify` ``` [INFO] Results: [INFO] [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 87 ```","closed","","bogthe","2021-05-21T11:23:28Z","2021-07-27T12:41:41Z"
"","3051","HADOOP-17547 Magic committer to downgrade abort in cleanup if list uploads fails with access denied","[HADOOP-17415](https://issues.apache.org/jira/browse/HADOOP-17547)  Caller without `s3:ListBucketMultipartUploads` permissions would have any committers throw an `AccessDenied` exception whenever the committer would try to cleanup any pending multipart uploads.  Changes: - Swallowing exception and printing a log instead during cleanup;  Tested with `eu-west-1` and `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify` ``` [INFO] Results: [INFO] [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 87 ```","closed","","bogthe","2021-05-25T16:28:28Z","2021-06-12T16:45:12Z"
"","3363","HADOOP-17223. backport to branch-2.10 bumping httpclient and httpcore","[HADOOP-17223](https://issues.apache.org/jira/browse/HADOOP-17223) update org.apache.httpcomponents:httpclient to 4.5.13 and httpcore to 4.4.13  There is CVE-2020-13956 moderate severity  Apache HttpClient versions prior to version 4.5.13 and 5.0.3 can misinterpret malformed authority component in reques... pom.xml update suggested: org.apache.httpcomponents:httpclient ~> 4.5.13   this PR is to backport the changes into branch-2.10 bumping httpcomponents:httpclient to 4.5.13","closed","","amahussein","2021-08-31T19:57:42Z","2021-09-03T01:24:37Z"
"","3260","HADOOP-17198 Support S3 AccessPoint","[HADOOP-17198](https://issues.apache.org/jira/browse/HADOOP-17198)  This change aims to add support for S3 AccessPoints. To use S3 object level APIs for an AccessPoint, one has to use the AccessPoint (AP) ARN.  Hence the following have been added: - a new property to set the AccessPoint ARN; - S3a parsing and using the new property with appropriate exceptions; - initial documentation update for AccessPoints;  What this PR enables: - If `apname` is the name of an AccessPoint you have for created bucket then S3a now allows you to use paths like `s3a://apname/` IF the new `s3a.accesspoint.arn` is set to the AccessPoint ARN e.g. `arn:aws:s3:eu-west-1:123456789101:accesspoint/apname`;  There's one thing I'm not sure about with this initial implementation so am looking for feedback if and how I should tackle it:  `S3a` bucket now has 2 ""meanings"" it can be a bucket name or an Access Point ARN. From the point of view of interacting with the SDK, they are interchangeable and internal string parsing logic is used to create the request for the right endpoint. However, I think it would be nicer to have a clearer abstraction for bucket names or access point ARNs that S3a operations can work with. This abstraction comes with the cost of doing a refactor which I'm not sure it's worth it right now. Even by doing a quick search on `.getHost()` there's quite a few places where the bucket name is deduced from the `host`.","closed","","bogthe","2021-08-03T17:07:15Z","2021-09-29T09:54:18Z"
"","3101","HADOOP-17139 Re-enable optimized copyFromLocal implementation in S3AFileSystem","[HADOOP-17139](https://issues.apache.org/jira/browse/HADOOP-17139)  S3a `copyFromLocalFile` was not using the optimized `innerCopyFromLocalFile` because the optimized version was not handling directory copies appropriately.  This change fixes `innerCopyFromLocalFile` to handle directories.  Tested in `eu-west-1` with `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify`  ``` [INFO] Results: [INFO] [WARNING] Tests run: 151, Failures: 0, Errors: 0, Skipped: 87 ```","closed","","bogthe","2021-06-11T15:51:41Z","2021-07-30T18:45:56Z"
"","2703","HADOOP-17109. add guava BaseEncoding to illegalClasses","[HADOOP-17109: Replace Guava base64Url and base64 with Java8+ base64](https://issues.apache.org/jira/browse/HADOOP-17109)  The hadoop source code relies on `org.apache.commons.` for Base64.  This PR is to add the `com.google.common.io.BaseEncoding` to illegal classes in order to prevent using the guava import in future commits.  - This PR only touches the checkstyle configuration. - There are no occurrences of `com.google.common.io.BaseEncoding` in the code.","closed","","amahussein","2021-02-15T16:53:01Z","2021-02-17T01:15:58Z"
"","2697","HADOOP-16810. Increase entropy on precommit Linux VMs","[HADOOP-16810: Increase entropy to improve cryptographic randomness on precommit Linux VMs](https://issues.apache.org/jira/browse/HADOOP-16810) In [my comment on MAPREDUCE-7079](https://issues.apache.org/jira/browse/MAPREDUCE-7079?focusedCommentId=17013234&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17013234) This test case has been failing for ever.  - When it timeout, MRAppMaster and some YarnChild processes remain running in the background. Therefore, the JVM running the tests fail due to OOM. No one notices that this unit test case has failed because the QA reports the unit tests that failed, but not timeout. - It works for Mac OS X, but never works for Linux running on a virtual Box. It only works on the latter by disabling MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA.  In this PR:  - the `DOCKER_EXTRAARGS` are added to `hadoop.sh` to pass the random mount - ~~the version 0.10.0 is not on the release page. So, this is upgrading the Yetus to a released version 0.13.0.~~ - adding the mount parameter to start `start-build-env.sh`","open","","amahussein","2021-02-12T23:29:29Z","2021-02-22T14:48:35Z"
"","2999","HDFS-15912. Allow ProtobufRpcEngine to be extensible","@## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hchaverri","2021-05-11T21:46:35Z","2021-05-17T07:37:07Z"
"","3357","HDFS-16197.Simplify getting NNStorage in FSNamesystem.","JIRA: HDFS-16197","closed","","jianghuazhu","2021-08-31T03:38:16Z","2021-09-12T11:58:54Z"
"","3482","HADOOP-17936. Fix test failure after reverting HADOOP-16878","HADOOP-17936 ### Description of PR  After reverting HADOOP-16878 from trunk, test `TestLocalFSCopyFromLocal.testDestinationFileIsToParentDirectory` started to fail since it depends on the behavior change in the JIRA. This fixed the test.  ### How was this patch tested?  Existing tests.","closed","","sunchao","2021-09-26T05:10:35Z","2021-09-27T20:56:26Z"
"","3478","HADOOP-17936. Fix test failure after reverting HADOOP-16878 from branch-3.3","HADOOP-17936 ### Description of PR  After reverting HADOOP-16878 from branch-3.3, test `TestLocalFSCopyFromLocal.testDestinationFileIsToParentDirectory` started to fail since it depends on the behavior change in the JIRA. This fixed the test.  ### How was this patch tested?  Existing tests.","closed","","sunchao","2021-09-24T03:05:09Z","2021-09-27T20:56:58Z"
"","3493","HDFS-16241.  Standby close reconstruction thread","### Description of PR When the ""Reconstruction Queue Initializer"" thread of the active namenode has not stopped, switch to standby namenode. The ""Reconstruction Queue Initializer"" thread should be closed ### How was this patch tested? After namenode becomes active, switch immediately to see if the ""Reconstruction Queue Initializer"" thread is alive  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","langlaile1221","2021-09-28T06:40:56Z","2021-10-11T08:11:31Z"
"","3318","YARN-10884: Handle empty owners to parse log files","### Description of PR Wasb FileSystem sets owner as empty during append operation.  ATS1.5 fails to read such files with below error  ```  java.lang.IllegalArgumentException: Null user         at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1271)         at org.apache.hadoop.security.UserGroupInformation.createRemoteUser(UserGroupInformation.java:1258)         at org.apache.hadoop.yarn.server.timeline.LogInfo.parsePath(LogInfo.java:141)         at org.apache.hadoop.yarn.server.timeline.LogInfo.parseForStore(LogInfo.java:114)         at org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs.parseSummaryLogs(EntityGroupFSTimelineStore.java:701)         at org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$AppLogs.parseSummaryLogs(EntityGroupFSTimelineStore.java:675)         at org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore$ActiveLogParser.run(EntityGroupFSTimelineStore.java:888)         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)         at java.util.concurrent.FutureTask.run(FutureTask.java:266)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)         at java.lang.Thread.run(Thread.java:748) ``` The owner information is used only when the ACL is enabled and not used when ACL is disabled.  Hence, making the owner ```anonymous``` , only when the ACL is disabled so that the append operation is successful.   ### How was this patch tested? The patch was tested in a device which uses wasb storage and made sure that the exception is not seen post patching.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","swathic95","2021-08-22T03:39:52Z","2021-09-07T16:07:52Z"
"","3627","Revert ""HDFS-16300. Use libcrypto in Windows for libhdfspp (#3617)""","### Description of PR This reverts commit 1032724aa3a1b90a63b2eded6de913c96b689016.   ### How was this patch tested? CI validation.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-11-07T09:04:43Z","2021-11-07T18:46:48Z"
"","3656","HADOOP-18009. Reset UserGroupInformation after tests run","### Description of PR This pull request aims to reset shared state between TestSecureRegistry and TestRegistryOperationUtils, where TestRegistryOperationUtils (namely testUsernameExtractionEnvVarOverrride) fails when run afterwards in the same JVM. While currently test classes are run in separate JVMs, they can be run faster when run in the same JVM, and the shared state can be reset with the proposed patch in this pull request.  ### How was this patch tested? Before this patch, if the two test classes TestSecureRegistry and TestRegistryOperationUtils are run in this order within the same JVM (Surefire configuration reuseForks=true), TestRegistryOperationUtils fails. After the patch, the test passes in the order.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","odrepa","2021-11-13T22:53:17Z","2021-11-14T01:55:13Z"
"","3349","HADOOP-17880. Build 2.10.x with docker","### Description of PR this pr support build hadoop 2.10.x with docker.  ### How was this patch tested? test on mac x86_64  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id? HADOOP-17880 - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ZhendongBai","2021-08-28T06:02:52Z","2021-10-08T15:48:14Z"
"","3356","YARN-10928. Support default queue config for minimum-user-limit-percent/user-limit-factor","### Description of PR There are many user cases that one user owns many queues in his organization's cluster for different business usages in practice. These queues often share the same properties, such as minimum-user-limit-percent and user-limit-factor. Users have to write one property for every queue they use if they want to use customized these shared properties. Adding default queue properties for these cases will simplify capacity scheduler's configuration file and make it easy to adjust queue's common properties.    CHANGES: Add two properties as queue's default value in capacity scheduler's configuration: `yarn.scheduler.capacity.minimum-user-limit-percent` `yarn.scheduler.capacity.user-limit-factor`  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zheng-weihao","2021-08-30T08:57:35Z","2021-09-06T08:27:20Z"
"","3389","YARN-10928. Support default queue config for minimum-user-limit-percent/user-limit-factor","### Description of PR There are many user cases that one user owns many queues in his organization's cluster for different business usages in practice. These queues often share the same properties, such as minimum-user-limit-percent and user-limit-factor. Users have to write one property for every queue they use if they want to use customized these shared properties. Adding default queue properties for these cases will simplify capacity scheduler's configuration file and make it easy to adjust queue's common properties.  CHANGES: Add two properties as queue's default value in capacity scheduler's configuration: `yarn.scheduler.capacity.minimum-user-limit-percent` `yarn.scheduler.capacity.user-limit-factor`  ### How was this patch tested? Add unit test in capacity scheduler.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zheng-weihao","2021-09-06T08:33:16Z","2021-09-13T03:06:53Z"
"","3542","HDFS-16267. Make hdfs_df tool cross platform","### Description of PR The source files for hdfs_df uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make this cross platform.  ### How was this patch tested? Ran `hdfs_df` locally and verified that it's giving the right output - ![image](https://user-images.githubusercontent.com/10280768/136835904-d784ba05-ffcd-4122-8366-842e10f7ec0b.png)  Also, the unit tests ran fine - ![image](https://user-images.githubusercontent.com/10280768/136835965-a4f7feb6-e213-4ae7-8b30-3c3854e8a135.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-11T18:16:51Z","2021-10-13T16:46:58Z"
"","3532","HDFS-16260. Make hdfs_deleteSnapshot tool cross platform","### Description of PR The source files for hdfs_deleteSnapshot uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make this cross platform.  ### How was this patch tested? Tested it by running locally. 1. I created a snapshot first by running - ```bash $ ./hdfs_createSnapshot -n my-snapshot /mydir ``` ![image](https://user-images.githubusercontent.com/10280768/136544490-5f75d284-df7f-4d06-87d8-a6a3be65baf9.png)  2. Then deleted the snapshot by running - ```bash $ ./hdfs_deleteSnapshot /mydir my-snapshot ``` ![image](https://user-images.githubusercontent.com/10280768/136544581-41c84697-76db-4392-b14c-04c6fdd025c8.png)  Commands - ![image](https://user-images.githubusercontent.com/10280768/136544612-9104f93a-3bd2-4497-b0fd-bf290da013d9.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-08T11:19:50Z","2021-10-12T04:23:18Z"
"","3563","HDFS-16278. Make HDFS snapshot tools cross platform","### Description of PR The source files for `hdfs_createSnapshot`, `hdfs_disallowSnapshot` and `hdfs_renameSnapshot` uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make these tools cross platform.  ### How was this patch tested? The details on testing of each of the above tool is as follows - ## hdfs_createSnasphot I was able to create snapshots with both the default and custom names - ![image](https://user-images.githubusercontent.com/10280768/137968139-a31af228-46b5-452b-b962-38eaea06b830.png)  UI - ![image](https://user-images.githubusercontent.com/10280768/137968174-4af8efa2-0307-44e3-93cc-2b44e23811fd.png)   ## hdfs_renameSnapshot I renamed one of the snapshots that was created previously (`s20211019-232643.399`) as `another-snapshot` - ![image](https://user-images.githubusercontent.com/10280768/137968518-118a263e-f4e2-4dbe-8595-c107f614aa6b.png)  UI - ![image](https://user-images.githubusercontent.com/10280768/137968577-615c5441-7840-49d0-a183-de78d14be968.png)   ## hdfs_disallowSnapshot I cleared all the previously created snapshots under `/mydir` and verified `hdfs_disallowSnapshot` on it - ![image](https://user-images.githubusercontent.com/10280768/137968737-40a3a373-2ad8-4a2a-ad74-44d9ee0e8dd4.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-18T18:34:48Z","2021-10-21T17:36:15Z"
"","3588","HDFS-16285. Make HDFS ownership tools cross platform","### Description of PR The source files for **hdfs_chown**, **hdfs_chmod** and **hdfs_chgrp** uses getopt for parsing the command line arguments. getopt is available only on Linux and thus, isn't cross platform. We need to replace getopt with boost::program_options to make these tools cross platform.  ### How was this patch tested? I ran the above tools on my Hadoop cluster setup locally. The details of testing are as follows - ## hdfs_chgrp Initially - ![Initial](https://user-images.githubusercontent.com/10280768/144712274-41f54fe8-b014-47cb-a31c-4aa6f83eaff5.jpg)  Changed the group association of a single file from `hadoop` to `yetus` group - ![Single file](https://user-images.githubusercontent.com/10280768/144712292-7cc5d7ae-0f3f-441d-80d2-37159af8d8a4.jpg)  Performed the same operation with the recursive option - ![Recursive](https://user-images.githubusercontent.com/10280768/144712302-1558cc4e-24cd-4eda-811c-c02e5fd81635.jpg)  Final state of the files viewed through browser - ![Browser](https://user-images.githubusercontent.com/10280768/144712356-be462434-07ed-4621-82b5-8662d7f6725d.jpg)   ## hdfs_chmod Initially - ![Initial](https://user-images.githubusercontent.com/10280768/144712388-1d560f15-a1e1-455f-a4d5-574eb3deb11e.jpg)  Changed the permissions of a single file to `777` - ![Single file](https://user-images.githubusercontent.com/10280768/144712414-f1dca8bd-0083-4a4d-9ee7-01246e9ea977.jpg)  Performed the same operation with the recursive option - ![Recursive](https://user-images.githubusercontent.com/10280768/144712439-ed9d17f8-65a5-4a05-9787-3aa8d08ae764.jpg)  Final state of the files viewed through browser - ![Browser](https://user-images.githubusercontent.com/10280768/144712462-e114d6ba-f3b2-45a7-a09b-9b1846ad9e56.jpg)   ## hdfs_chown Initially - ![Initial](https://user-images.githubusercontent.com/10280768/144712492-8510b6f5-fbeb-418d-b577-1dae94b1826c.jpg)  Changed the ownership of a single file from `hduser` to `gautham` - ![Single file](https://user-images.githubusercontent.com/10280768/144712509-5e2760ec-c5a3-47c6-a0f1-7190c36c2cc6.jpg)  Performed the same operation with the recursive option - ![Recursive](https://user-images.githubusercontent.com/10280768/144712517-64ab0092-e478-4905-abcc-ee7bdfd8f6db.jpg)  Final state of the files viewed through browser - ![Browser](https://user-images.githubusercontent.com/10280768/144712527-081c2618-1301-402a-a568-d80c02291ed1.jpg)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-26T16:31:13Z","2021-12-09T04:47:54Z"
"","3624","HADOOP-17993. Disable JIRA plugin for YETUS on Hadoop","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","backport,","GauthamBanasandra","2021-11-05T18:24:28Z","2021-11-12T14:26:45Z"
"","3623","HADOOP-17992. Disable JIRA plugin for YETUS on Hadoop","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","backport,","GauthamBanasandra","2021-11-05T18:11:09Z","2021-11-10T18:44:08Z"
"","3609","HADOOP-17987. Disable JIRA plugin for YETUS on Hadoop","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-11-02T18:03:39Z","2021-11-05T18:28:28Z"
"","3608","HADOOP-17986. Disable JIRA plugin for YETUS on Hadoop","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-11-02T17:56:21Z","2021-11-05T18:29:19Z"
"","3601","HADOOP-17985. Disable JIRA plugin for Yetus run","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? In progress.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-29T14:06:10Z","2021-10-31T18:26:47Z"
"","3610","HADOOP-17988. Disable JIRA plugin for YETUS on Hadoop","### Description of PR The jira-json goes missing all of a sudden and we get the following error in the Jenkins CI run - ``` [2021-10-27T17:52:58.787Z] Processing: https://github.com/apache/hadoop/pull/3588 [2021-10-27T17:52:58.787Z] GITHUB PR #3588 is being downloaded from [2021-10-27T17:52:58.787Z] https://api.github.com/repos/apache/hadoop/pulls/3588 [2021-10-27T17:52:58.787Z] JSON data at Wed Oct 27 17:52:55 UTC 2021 [2021-10-27T17:52:58.787Z] Patch data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:58.787Z] Diff data at Wed Oct 27 17:52:56 UTC 2021 [2021-10-27T17:52:59.814Z] awk: cannot open /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-3588/centos-7/out/jira-json (No such file or directory) [2021-10-27T17:52:59.814Z] ERROR: https://github.com/apache/hadoop/pull/3588 issue status is not matched with ""Patch Available"". [2021-10-27T17:52:59.814Z] ```  The hadoop-multibranch pipeline doesn't use ASF JIRA, thus, we're disabling the jira plugin to fix this issue.  ### How was this patch tested? CI build doesn't fail with this error anymore.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","backport,","GauthamBanasandra","2021-11-02T18:10:13Z","2021-11-09T16:35:14Z"
"","3625","HDFS-16304. Locate OpenSSL libs for libhdfspp","### Description of PR The Docker build using start-build-env.sh is failing since commit 1032724. It can be reproduced with a minimal Ubuntu image and by running mvn clean install -Pnative -DskipTests -DskipShade -Dmaven.javadoc.skip=true inside the Docker container via start-build-env.sh.   ### How was this patch tested? By building locally and validation through CI.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","build,","GauthamBanasandra","2021-11-06T18:22:54Z","2021-11-09T16:31:12Z"
"","3677","HDFS-16332. Handle invalid token exception in sasl handshake","### Description of PR See https://issues.apache.org/jira/browse/HDFS-16332 description for the detail.  Due to missing handling of invalid token exception in sasl handshake, token refresh isn't triggered and all datanode is considered as dead nodes. This causes retry of refetchLocations with sleep and we got bad hbase's response time.  ### How was this patch tested? - Tested by the intergration test - Applied this patch to our hadoop and hbase cluster   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","bitterfox","2021-11-18T04:44:06Z","2021-12-03T14:30:28Z"
"","3587","HADOOP-17374. support listObjectV2","### Description of PR OSS supports ListObjectsV2(https://help.aliyun.com/document_detail/187544.html?spm=a2c4g.11186623.6.1589.e0623d9fE1b64S) to optimize versioning bucket list. We should support this feature in AliyunOSS module.  ### How was this patch tested?  New unit tests; cloud store tests  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (HADOOP-17374)? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","wujinhu","2021-10-26T11:16:18Z","2022-06-25T11:35:37Z"
"","3448","HDFS-11045. Make TestDirectoryScanner#testThrottling() not based on timing","### Description of PR Make TestDirectoryScanner#testThrottling() not based on timing. [https://issues.apache.org/jira/browse/HDFS-11045](https://issues.apache.org/jira/browse/HDFS-11045)  ### How was this patch tested? Local UT.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-11045. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","HanleyYang","2021-09-16T09:16:03Z","2022-02-10T21:54:23Z"
"","3417","HDFS-16220.[FGL]Configurable INodeMap#NAMESPACE_KEY_DEPTH&NUM_RANGES_STATIC.","### Description of PR INodeMap#NAMESPACE_KEY_DEPTH&NUM_RANGES_STATIC is configurable.   ### How was this patch tested? INodeMap#NAMESPACE_KEY_DEPTH and NUM_RANGES_STATIC can be successfully obtained after changing the values of INodeMap#NAMESPACE_KEY_DEPTH and NUM_RANGES_STATIC in the Configuration.  jira:HDFS-16220","open","","jianghuazhu","2021-09-10T17:07:56Z","2021-09-23T02:00:04Z"
"","3497","HDFS-16244.Add the necessary write lock in Checkpointer#doCheckpoint().","### Description of PR In Checkpointer#doCheckpoint(), when you need to execute reloadFromImageFile(), you need to add the necessary write lock. Details: HDFS-16244  ### How was this patch tested? Need to verify the correctness of Checkpointer#doCheckpoint().","closed","","jianghuazhu","2021-09-29T08:33:30Z","2021-10-13T09:50:18Z"
"","3380","HDFS-16211.Complete some descriptions related to AuthToken.","### Description of PR In AuthToken, some description information is missing. The purpose of this jira is to complete some descriptions related to AuthToken.  ### How was this patch tested? This is an improvement related to the documentation and does not require excessive testing.  More details:HDFS-16211","open","","jianghuazhu","2021-09-03T09:54:48Z","2022-02-01T03:49:39Z"
"","3368","HDFS-16204.Improve FSDirEncryptionZoneOp related parameter comments.","### Description of PR Improve some unclear comments in FSDirEncryptionZoneOp. This is the purpose of this pr.  ### How was this patch tested? This is an improvement related to the documentation and does not require excessive testing.   More details:HDFS-16204","closed","","jianghuazhu","2021-09-01T12:48:57Z","2021-09-06T10:06:13Z"
"","3480","YARN-10970. Standby RM should expose prom endpoint","### Description of PR https://issues.apache.org/jira/browse/YARN-10970 description detail:  HADOOP-16398 exports Hadoop metrics to Prometheus.   However, standby RM redirects prom metrics to the Active.  We need to separate out prom metrics displayed so the Standby RM can also be export metrics to prometheus.  ### How was this patch tested? change TestRMFailover$testRMWebAppRedirect test   ### For code changes: add `/prom` endpoint  in RMWebAppFilter$NON_REDIRECTED_URIS","closed","","Neilxzn","2021-09-24T09:22:41Z","2021-09-29T06:47:17Z"
"","3415","HDFS-16216. RBF: Wrong path when get mount point status","### Description of PR https://issues.apache.org/jira/browse/HDFS-16216  ### How was this patch tested? UT.  TestRouterMountTable.testGetMountPointFileStatus  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zhengzhuobinzzb","2021-09-10T02:20:43Z","2021-10-01T17:15:48Z"
"","3320","HDFS-16182.numOfReplicas is given the wrong value in BlockPlacementPolicyDefault$chooseTarget can cause DataStreamer to fail with Heterogeneous Storage.","### Description of PR https://issues.apache.org/jira/browse/HDFS-16182  ### How was this patch tested? add  TestBlockStoragePolicy.testAddDatanode2ExistingPipelineInSsd  ### For code changes:","closed","","Neilxzn","2021-08-23T07:30:16Z","2021-11-16T10:21:50Z"
"","3426","HADOOP-17893. Improve PrometheusSink for Namenode TopMetrics","### Description of PR https://issues.apache.org/jira/browse/HADOOP-17893  ### How was this patch tested? add test testTopMetricsPublish  ### For code changes: Parse TopMetrics in PrometheusMetricsSink","closed","","Neilxzn","2021-09-13T06:40:00Z","2021-09-21T01:44:01Z"
"","3535","HADOOP-17880. Build Hadoop on Centos 7","### Description of PR Getting Hadoop to build on Centos 7 will greatly benefit the community. Here, we aim to provide a Dockerfile that builds out the image with all the dependencies needed to build Hadoop on Centos 7.  ### How was this patch tested? Tested on mac x86_64, CI runs.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","backport,","ZhendongBai","2021-10-08T15:42:38Z","2021-11-08T12:15:13Z"
"","3315","HDFS-16180.FsVolumeImpl.nextBlock should consider that the block meta file has been deleted","### Description of PR FsVolumeImpl.nextBlock should consider that the block meta file has been deleted https://issues.apache.org/jira/browse/HDFS-16180 In my cluster,  we found that when VolumeScanner run, sometime dn will throw some error log below  ```   2021-08-19 08:00:11,549 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1020175758-nnip-1597745872895 blk_1142977964_69237147 URI file:/disk1/dfs/data/current/BP-1020175758- nnip-1597745872895/current/finalized/subdir0/subdir21/blk_1142977964 2021-08-19 08:00:48,368 ERROR org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl: nextBlock(DS-060c8e4c-1ef6-49f5-91ef-91957356891a, BP-1020175758- nnip-1597745872895): I/O error java.io.IOException: Meta file not found, blockFile=/disk1/dfs/data/current/BP-1020175758- nnip-1597745872895/current/finalized/subdir0/subdir21/blk_1142977964 at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil.findMetaFile(FsDatasetUtil.java:101) at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl.nextBlock(FsVolumeImpl.java:809) at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:528) at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:628) 2021-08-19 08:00:48,368 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/disk1/dfs/data, DS-060c8e4c-1ef6-49f5-91ef-91957356891a): nextBlock error on org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl$BlockIteratorImpl@7febc6b4 ```  When VolumeScanner scan block  blk_1142977964,  it has been deleted by datanode,  scanner can not find the meta file of blk_1142977964, so it throw these error log.     Maybe we should handle FileNotFoundException during nextblock to reduce error log and nextblock retry times.   ### How was this patch tested? no new test.   ### For code changes: FsVolumeImpl.nextBlock hanlde FileNotFoundException","closed","","Neilxzn","2021-08-19T09:46:57Z","2021-08-24T03:15:48Z"
"","3571","HDFS-7612. Fix default cache directory in TestOfflineEditsViewer.","### Description of PR Fixes JIRA: https://issues.apache.org/jira/browse/HDFS-7612  The default path is incorrect, but maven sets the property during the build.   ### How was this patch tested?  *Automated* `mvn test -Dtest=TestOfflineEditsViewer`  `mvn clean install -Pdist -Dtar -Ptest-patch`  *Manual* Cleared `test.cache.data` property and confirmed the test worked.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","mkuchenbecker","2021-10-20T21:56:43Z","2021-12-04T08:37:35Z"
"","3369","HADOOP-17804. Expose prometheus metrics only after a flush and dedupe with tag values","### Description of PR Fixes a bug with the Prometheus metrics sink where metrics were deduped on their name alone, and didn't include the tag values for deduplication purposes. Prometheus metrics are uniquely identified by their name and labels, so several metrics were just getting dropped. Specifically things like RPC metrics were only including one of the servers/ports per metric type, and Yarn queue metrics only included metrics for one queue.   Additionally, because of the ""push"" nature of Hadoop metrics, this would end up creating a lot of extra metrics for things where the tags can change over time but they still actually mean the same thing. For example, the `hastate` of namenode metrics can change, but you really only want the most recent one. To address this, I changed it to only expose metrics after a `flush` call, and to start fresh after each `flush` call. This prevents old metrics from hanging around and constantly being exposed until the service is restarted.  There are still some ""bad"" tags that are exposed which can lead to multiple Prometheus series being created when really they are the same thing. However, these can be dealt with on the Prometheus side, ignoring certain labels, rather than trying to hard code all the bad tags on the Hadoop side.  I don't _think_ there should be any threading/race conditions with publishing metrics, since the publish metrics methods are synchronized.  Also adds the help line to the output.  ### How was this patch tested? New unit tests.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] ~~Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?~~ - [ ] ~~If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)?~~ - [ ] ~~If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?~~","closed","","Kimahriman","2021-09-01T14:04:17Z","2021-09-09T07:49:41Z"
"","3490","HADOOP-17931. Fix typos in usage message in winutils.exe","### Description of PR Fixed typos in winutils.exe  ### How was this patch tested? This is just a typo fix, no testing is needed.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-09-27T16:48:06Z","2021-09-28T16:57:04Z"
"","3484","Fix winutils typos","### Description of PR Fixed some typos in winutils.   ### How was this patch tested? Not needed.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-09-26T10:30:03Z","2021-09-27T16:50:38Z"
"","3359","HDFS-16198. Short circuit read leaks Slot objects when InvalidToken exception is thrown","### Description of PR Fix leakage of short circuit read Slot when InvalidToken exception  ### How was this patch tested? A new test case is added  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","EungsopYoo","2021-08-31T11:07:19Z","2021-12-17T21:58:10Z"
"","3582","HADOOP-17977  FileOutputCommitter make PENDING_DIR_NAME configurable","### Description of PR FileOutputCommitter enable concurrent writes by making PENDING_DIR_NAME configurable  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ismailsimsek","2021-10-23T20:44:23Z","2021-10-26T16:17:33Z"
"","3567","HADOOP-17971. Exclude IBM Java security classes from being shaded/relocated.","### Description of PR Exclude IBM Java security classes located in com.ibm.security* from being shaded.   ### How was this patch tested? Manual injection of UserGroupInformation class files (which contain references to com.ibm.security classes) that were compiled manually.  I followed bug fixes for similar issues. IBM java isn't actually used in source code except being referenced when it is the running JVM.","closed","","n-marion","2021-10-19T20:08:27Z","2021-10-20T05:26:12Z"
"","3536","HDFS-16265. Refactor HDFS tool tests for better reuse","### Description of PR Currently, the test cases written in hdfs-tool-test.h isn't easy to reuse. Primarily because the expectations are different for each HDFS tool. I realized this while I was creating the PR for [HDFS-16260](https://issues.apache.org/jira/browse/HDFS-16260). For instance, passing more than one argument is erroneous to hdfs_allowSnapshot while it's the only valid scenario for hdfs_deleteSnapshot.  Thus, it won't be possible to reuse the test cases without decoupling the expectations from the test case definitions. The solution here is to move the expectations to the corresponding mock classes and invoke the call to set them up in the test cases after the creation of mock instances.  ### How was this patch tested? Unit tests ran successfully -  ![image](https://user-images.githubusercontent.com/10280768/136606319-79da272d-264c-431f-83ea-763f3bc56481.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-08T18:24:06Z","2021-10-16T16:46:55Z"
"","3617","HDFS-16300. Use libcrypto in Windows for libhdfspp","### Description of PR Currently, eay32 is the library that's used in libhdfspp for Windows. Whereas, we use libcrypto for the rest of the platforms. As per the following mail thread, the OpenSSL library was renamed from eay32 to libcrypto from OpenSSL version 1.1.0 onwards - https://mta.openssl.org/pipermail/openssl-dev/2016-August/008351.html.  Thus, we need to use libcrypto on Windows as well to ensure that we standardize the version of the OpenSSL libraries used across platforms.  ### How was this patch tested? Build passed on Windows (using stubs for those parts that don't compile yet) - https://issues.apache.org/jira/secure/attachment/13035697/build-log-hdfs-nacl-windows-10.log  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","hdfs,","GauthamBanasandra","2021-11-04T12:53:14Z","2021-11-04T17:25:16Z"
"","3414","YARN-10938. Support reservation scheduling enabled switch for capacity scheduler","### Description of PR Currently the CapacityScheduler uses reservations in order to handle requests for large containers and the fact there might not currently be enough space available on a single host. But this algorithm is not suitable for small cluster which only have very limited resources. So we can add a switch property in capacity scheduler's configuration to avoid reservation scheduling in these use cases.  CHANGES:  Add ""yarn.scheduler.capacity.reservation.enabled"" in capacity scheduler's configuration.  ### How was this patch tested? Add a unit test for switch in resourcemanager.scheduler.capacity.TestReservations  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zheng-weihao","2021-09-09T14:45:12Z","2021-09-10T05:28:40Z"
"","3445","HADOOP-15566 Opentelemetry changes using java agent","### Description of PR Changes in the PR are related to enabling tracing using opentelemetry. I am using opentelemetry-javaagent to initialize the Tracer object  ### How was this patch tested? I created a build locally and got single node cluster setup and tested the changes.   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","kiran-maturi","2021-09-16T06:09:36Z","2022-04-12T15:01:04Z"
"","3383","LAKE-14976","### Description of PR Backport of :   For libhdfs -> https://github.com/criteo-forks/hadoop-common/pull/95/files For OOPS -> https://github.com/criteo-forks/hadoop-common/pull/107/files  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","younes-b","2021-09-03T18:08:28Z","2021-09-21T08:46:46Z"
"","3431","HADOOP-17907. FileUtil#fullyDelete deletes contents of sym-linked directory when symlink cannot be deleted because of local fs fault","### Description of PR As discussed in HADOOP-6536, FileUtil#fullyDelete should not delete the contents of the sym-linked directory when we pass a symlink parameter. Currently we try to delete the resource first by calling deleteImpl, and if deleteImpl is failed, we regard it as non-empty directory and remove all its contents and then itself. This logic behaves wrong when local file system cannot delete symlink to a directory because of faulty disk, local system's error, etc. When we cannot delete it in the first time, hadoop will try to remove all the contents of the directory it pointed to and leave an empty dir.  So, we should add an isSymlink checking before we call fullyDeleteContents to prevent such behavior.  ### How was this patch tested? Add a unit test in TestFileUtil  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zheng-weihao","2021-09-14T03:14:57Z","2021-09-15T17:37:21Z"
"","3325","YARN-10893. Adding metrics for getClusterMetrics and getApplications APIs in FederationClientInterceptor","### Description of PR Adding metrics for getClusterMetrics and getApplications APIs in FederationClientInterceptor  ### How was this patch tested? Added tests for RouterMetrics and validated existing test cases for FederationClientInterceptor APIs. Unit test are running successfully on dev machine.  Verified on a test cluster.","closed","","akshatb1","2021-08-24T06:36:50Z","2021-09-09T16:20:57Z"
"","3539","YARN-10955. Add health check mechanism to improve troubleshooting skills for RM","### Description of PR Add health check mechanism to improve troubleshooting skills for RM just like Kubernetes health endpoints for API server. This feature will improve our usalibity for troubleshooting and performance profiling.  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","zheng-weihao","2021-10-11T06:15:29Z","2021-10-11T13:18:57Z"
"","3391","HADOOP-17895. Add the test to reproduce the failure of `RawLocalFileSystem.mkdir` with unicode filename","### Description of PR A sample PR to reproduce the failure for https://issues.apache.org/jira/projects/HADOOP/issues/HADOOP-17895?filter=allopenissues  ### How was this patch tested? Run the test with NativeIO enabled (it's failing)  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","majdyz","2021-09-06T14:48:04Z","2021-11-05T11:44:30Z"
"","3628","YARN-11001. Add docs on removing node-to-labels mapping","### Description of PR 1. Add one line to describe how to remove node-to-labels mapping from a node. 2. Fix grammar and punctuation in configuring node-to-labels mapping.  ### How was this patch tested? This only changes doc.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","documentation,","manuzhang","2021-11-08T02:46:37Z","2021-11-15T15:03:35Z"
"","3437","HADOOP-17913. Filter deps with release labels","### Description of PR * This PR adds an option --release   to resolve.py which provides the   ability to filter the dependencies   listed in packages.json file based   on the specified release label. * This is helpful for maintaining   dependencies across different   releases for a platform.  ### How was this patch tested? I tested it locally for the following cases - * Ensured that the current functionality   is maintained. * Added a release entry and verified that   it was filtered correctly. * Verified that the filtering worked correctly   when an incorrect release label was given.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-09-15T12:56:05Z","2021-09-16T18:08:47Z"
"","3311","HDFS-16178. Make recursive rmdir in libhdfs++ cross platform","### Description of PR * The TempDir class in libhdfs++ is currently   using nftw API provided by ftw.h, which is   only present in Linux and not on Windows. * This PR uses the remove_all API from C++17   std::filesystem to make this cross platform   in an equivalent manner.   ### How was this patch tested? The API used here was tested with standalone program - https://github.com/GauthamBanasandra/x-platform/commit/87e68a626548aac87c4e0bdcff8a33d2d769dc69  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-08-18T19:04:40Z","2021-08-20T18:31:36Z"
"","3523","HDFS-16251. Make hdfs_cat tool cross platform","### Description of PR * The source files for hdfs_cat   uses getopt for parsing the   command line arguments.   getopt is available only on   Linux and thus, isn't cross   platform. * We need to replace getopt   with boost::program_options   to make this cross platform.  ### How was this patch tested? I verified it by running the tool locally - ![image](https://user-images.githubusercontent.com/10280768/136075794-ba053461-8402-44af-b226-72f2b530ca95.png)  Also, the unit tests ran successfully - ![image](https://user-images.githubusercontent.com/10280768/136075873-7b1e38ee-9619-403b-8714-4c2cde79b792.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-05T17:48:15Z","2021-10-07T18:13:50Z"
"","3388","HDFS-16205. Make hdfs_allowSnapshot tool cross platform","### Description of PR * The source files for hdfs_allowSnapshot   uses getopt for parsing the command line   arguments. * getopt is available only on Linux and thus,   isn't cross platform. We need to replace   getopt with boost::program_options to   make this cross platform.   ### How was this patch tested? I've added unit tests that cover the parameter passing.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-09-05T18:38:38Z","2021-09-23T07:49:41Z"
"","3518","HDFS-16254. Cleanup protobuf on exit of hdfs_allowSnapshot","### Description of PR * Moved the call   google::protobuf::ShutdownProtobufLibrary()   to main method instead of   AllowSnapshot::HandlePath   since we want the clean-up   tasks to run only when the   program exits.  ### How was this patch tested? Unit tests ran fine. Also, verified by running the tool locally, as shown below -  ![image](https://user-images.githubusercontent.com/10280768/135907030-368dafb6-ff61-4095-a25a-2c379cf0e490.png)  The directory was marked as snapshot-able without any errors - ![image](https://user-images.githubusercontent.com/10280768/135907086-d73fdd87-d951-483e-8fa7-0a7ee7ea16ec.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-04T18:43:13Z","2021-10-08T10:45:13Z"
"","3319","HADOOP-17854. Run junit in Jenkins only if surefire reports exist","### Description of PR * junit command throws exception   if there are no xml files under   surefire-reports. * Thus we need to check if the xml   files exist before running the   junit command.   ### How was this patch tested? I created a commit https://github.com/GauthamBanasandra/hadoop/commit/81f3e70141ca9be4fcf1fe2c9dbcd44e65b11c42 which generates a dummy xml file and prints log messages about whether the file exists or not.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-08-22T17:56:16Z","2021-08-24T17:26:52Z"
"","3408","HADOOP-17902. Fix Hadoop build on Debian 10","### Description of PR * Debian testing has unstable list of   packages. Hence, switching to bullseye   which is a standard Debian release.  ### How was this patch tested? The Hadoop CI has been able to build and run the tests, confirming that there aren't any issues with this change.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-09-08T17:58:44Z","2021-09-20T05:05:01Z"
"","3513","HDFS-16250. Refactor AllowSnapshotMock using GMock","### Description of PR * Currently, the implementation   of AllowSnapshotMock is quite   basic. Need to replace this with   GMock so that we can tap into   the benefits offered by GMock.   ### How was this patch tested? The unit tests ran successfully.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-03T09:23:30Z","2021-10-05T16:32:14Z"
"","3531","HDFS-16263. Add CMakeLists for hdfs_allowSnapshot","### Description of PR * Currently, hdfs_allowSnapshot                                                                                                                                                                                                                                                               is built in it's parent directory's                                                                                                                                                                                                                                                         CMakeLists.txt.                                                                                                                                                                                                                                                                       * Need to move this into a separate                                                                                                                                                                                                                                                           CMakeLists.txt file under                                                                                                                                                                                                                                                                   hdfs-allow-snapshot so that it's                                                                                                                                                                                                                                                            more modular.  ### How was this patch tested? Unit tests ran successfully.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","GauthamBanasandra","2021-10-07T19:27:50Z","2021-10-08T15:49:57Z"
"","3327","YARN-10895. ContainerIdPBImpl objects still can be leaked in RMNodeImpl.completedContainers","### Description of PR  YARN-10467 fixed ContainerIdPBImpl Object Leakage in RMNodeImpl.completedContainers.  After applying YARN-10467 patch and operating cluster with large number of nodes, we found similar heap leakage still exists.  In heap dump which are dumped after failover, (so it is not active RM) about 4.5G is used by ContainerIDPBImpl on RMNodeImpl.completedContainers.  There are two cases.    1. Apps with 'KeepContainersAcrossApplicationAttempts'  is not cleared when they are failed  Even though 'KeepContainersAcrossApplicationAttempts' is set, we should clear RMAppAttemptImpl.justFinishedContainers.  If app attempt is failed and retried by next attempt, we may not need to clear RMAppAttemptImpl.justFinishedContainers because related information will be handed over to next attempts and eventually cleared.  However, when app is failed, there is no next attempt and heap leakage occur.  (We found this case when Yarn Service Application failed over multiple attempts because of OOM in AM)    2. Apps is killed explicitly by user  When app is killed by user by 'yarn application -kill' CLI interface or WebUI interface,  RMAppAttemptImpl.amContainerFinished is not called because app and app attempt state is already changed.  To handle this, we added sendFinishedContainersToNMs for each RMAppAttemptImpl.finishedContainersSentToAm, RMAppAttemptImpl.justFinishedContainers when Attempt is set to 'KILLED'    We found and patched our cluster on 3.1.2 but it seems trunk still has the same problem.  I attached patch based on the trunk.  Thanks!   ### How was this patch tested?  We reproduced cases on our test clusters and dumped heap memory for each case. Comparing heap dump on before/after patching, we checked heap leakage is cleared.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","acedia28","2021-08-24T09:59:58Z","2021-08-24T13:02:35Z"
"","3620","HADOOP-17990. Fix failing concurrent FS.initialize commands when fs.azure.createRemoteFileSystemDuringInitialization is enabled.","### Description of PR  When fs.azure.createRemoteFileSystemDuringInitialization is enabled, the filesystem will create a container if it does not already exist inside the initialize method. The current flow of creating the container will fail in the case of concurrent initialize methods being executed simultaneously (only one request can create the container, the rest will fail instead of moving on). This PR is fixing this issue by also catching org.apache.Hadoop.fs.FileAlreadyExistsException generated by the createFilesystem command.  ### How was this patch tested?  A new test in ITestAzureBlobFileSystemInitAndCreate is introduced which was breaking before the fox.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","majdyz","2021-11-04T17:04:44Z","2021-11-10T10:14:29Z"
"","3558","YARN-10978. Fixing ApplicationClassLoader to Correctly Expand Glob for Windows Path","### Description of PR  Using File.separator instead of ""/"" to make  glob expansion OS agnostic. Fixes following bug: https://issues.apache.org/jira/browse/YARN-10978 ### How was this patch tested? Verified on a Windows test cluster. Yarn Auxiliary service starts correctly when the configuration is provided.","open","","akshatb1","2021-10-16T14:52:02Z","2021-10-21T04:16:29Z"
"","3488","HADOOP-17940. Upgrade Kafka to 2.8.1","### Description of PR  Upgrade Kafka to 2.8.1  ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tasanuma","2021-09-27T08:58:28Z","2021-09-28T05:52:21Z"
"","3323","HADOOP-17859. Fix to avoid connection timed out when UDP DNS Query on DNS Registry","### Description of PR  UDP DNS query on YARN Registry DNS got connection timed out at times.  After checking logs on registry dns, we found the cause that size of UDP Response is greater than Preallocated Byte buffer(4096)  We added failsafe case by returning input query with TC flag so that client can know they should retried using TCP protocol.  Apache Jira URL: https://issues.apache.org/jira/browse/HADOOP-17859  ### How was this patch tested?  We tested this case after patching and operating our cluster. Connection timeout occured 10+ cases every week. After patching, 0 case reported  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","acedia28","2021-08-24T02:16:31Z","2021-08-24T04:53:07Z"
"","3612","WIP. HADOOP-17124. Support LZO Codec using aircompressor","### Description of PR  This patch adds LZO Codec from `aircompressor` which includes `LzoCompressor` and `LzoDecompressor` (WIP).  See https://issues.apache.org/jira/browse/HADOOP-17124 for details.  The mostly famous usage of `aircompressor` is [trino](https://trino.io/). Trino uses the library for its Lz4Codec, LzoCodec, SnappyCodec, etc. The code link is:  https://github.com/trinodb/trino/blob/fe608f2723842037ff620d612a706900e79c52c8/lib/trino-rcfile/src/main/java/io/trino/rcfile/AircompressorCodecFactory.java  ### How was this patch tested?  Unit test.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","viirya","2021-11-03T02:32:41Z","2021-11-18T22:43:54Z"
"","3326","HADOOP-17861. improve YARN Registry DNS Server qps","### Description of PR  There are some points to improve the performance of YARN Registry DNS Server.  - Do not print unnecessary logs (It just needs change log4j.properties)   - log4j.logger.org.apache.hadoop.registry.server.dns=WARN - Change some loglevels at points which can affect performance degradation. - Use ""newFixedThreadPool"" instead of ""newCachedThreadPool"" to prevent OOM - Use Blocking on TCP handler. Using non-blocking and sleeping some time(""Thread.sleep(500)"") is meaningless.  In our environment, QPS of original yarn dns server is about 5000 (UDP), 100 (TCP). Now, QPS of our improved yarn dns server is about 47000 (UDP), 500 (TCP).   I will make a pull request at https://github.com/apache/hadoop soon.     ### How was this patch tested?  Our team members have tested this manually in our cluster.    ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","eubnara","2021-08-24T07:59:18Z","2021-08-25T10:43:29Z"
"","3654","HDFS-16320. Datanode retrieve slownode information from NameNode","### Description of PR  The current information of slownode is reported by reportingNode, and stored in NameNode.  This ticket is to let the slownode retrieve the information from NameNode, so that it can do other performance improvement actions based on this information.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16320  Document: https://docs.google.com/document/d/10-qrEJ6n-wVCSKlJXsykvKzcS3UBOi6EkuKFm4Z-k40/edit?usp=sharing  ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-11-13T04:31:37Z","2021-11-22T14:51:52Z"
"","3304","HADOOP-14693. Test PR: Ran rewrite plugin on hadoop-hdfs module to upgrade to JUnit 5","### Description of PR  Test rewrite-maven-plugin https://docs.openrewrite.org/tutorials/migrate-from-junit-4-to-junit-5  ### How was this patch tested?  Not tested  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","aajisaka","2021-08-14T14:02:33Z","2021-08-20T06:18:22Z"
"","3324","YARN-10892. YARN Preemption Monitor got java.util.ConcurrentModificationException when three or more partitions exists","### Description of PR  On our cluster with a large number of NMs, preemption monitor thread consistently got java.util.ConcurrentModificationException when specific conditions met. (And preemption doesn't work, of course)  What We found as conditions are as follow. (All 4 conditions should be met)  1. There are at least two non-exclusive partitions except default partition (let me call the partitions as X and Y partition) 2. app1 in the queue belonging to default partition (let me call the queue as 'dev' queue) borrowed resources from both X, Y partitions  3. app2, app3 submitted to queues belonging to each X, Y partition is 'PENDING' because resources are consumed by app1 4. Preemption monitor can clear borrowed resources from X or Y when the container of app1 is preempted.    Main problem is that FifoCandiatesSelector.selectCandidates tried to remove HashMap key(partition name) while iterating HashMap.  Logically, it is correct because we didn't traverse the same partition again on this 'selectCandidates'. However HashMap structure does not allow modification while iterating.  I made test case to reproduce the error case(testResourceTypesInterQueuePreemptionWithThreePartitions).  We found and patched our cluster on 3.1.2 but it seems trunk still has the same problem.  I attached patch based on the trunk.  Apache jira URL: https://issues.apache.org/jira/browse/YARN-10892  Thanks!  >> {{2020-09-07 12:20:37,105 ERROR monitor.SchedulingMonitor (SchedulingMonitor.java:run(116)) - Exception raised while executing preemption checker, skip this run..., exception= java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextNode(HashMap.java:1437) at java.util.HashMap$KeyIterator.next(HashMap.java:1461) at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.FifoCandidatesSelector.selectCandidates(FifoCandidatesSelector.java:105) at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:489) at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:320) at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:99) at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PolicyInvoker.run(SchedulingMonitor.java:111) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)}}  ### How was this patch tested?  I added new testcase to reproduce the problem. new testcase will be failed without this patch.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","acedia28","2021-08-24T05:56:14Z","2021-08-24T08:55:54Z"
"","3621","HDFS-16302. RBF: RouterRpcFairnessPolicyController record requests accepted by each nameservice","### Description of PR  In HDFS-16296, we added metrics to record rejected permits for each namespace, and it would be also valuable to record the handled requests for each namespace.  This ticket is to also record the handled requests by each namespace.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16302  ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-11-05T10:03:56Z","2021-11-08T01:52:17Z"
"","3397","HADOOP-17898. Upgrade BouncyCastle to 1.69 on branch-2.10","### Description of PR  HADOOP-17898 . BouncyCastle to 1.69  - CVEs are reported for releases lower than 1.66  - Branch-2.10 was still running on bcprov-jdk16 which is very old  ### How was this patch tested?  - build locally succeeded - `mvn dependency:tree` - Looked into linked Jiras of HADOOP-15832 and reviewed the dependencies affected by the upgrade - Some of the the tests below are already timing out on the default branch. However, I verified that they have no class errors as reported in YARN-8919 and YARN-8899 ```bash mvn test -Dtest=TestFileArgs,TestMultipleCachefiles,TestStreamingBadRecords,\ TestSymLink,TestMultipleArchiveFiles,TestGridmixSubmission,TestDistCacheEmulation,\ TestLoadJob,TestSleepJob,TestDistCh,TestCleanupAfterKIll ```  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id - [X] Updated README.txt","closed","","amahussein","2021-09-07T18:33:41Z","2021-09-08T12:09:00Z"
"","3405","HADOOP-17563. Upgrade BouncyCastle to 1.69","### Description of PR  HADOOP-17563 . BouncyCastle to 1.69  - CVEs are reported for releases lower than 1.66   ### How was this patch tested?  - build locally succeeded `mvn clean install -Pdist -Dtar -DskipTests -Dmaven.javadoc` - `mvn dependency:tree` - Looked into linked Jiras of HADOOP-15832 and reviewed the dependencies affected by the upgrade - Some of the the tests below are already timing out on the default branch. However, I verified that they have no class errors as reported in YARN-8919 and YARN-8899 ```bash mvn test -Dtest=TestFileArgs,TestMultipleCachefiles,TestStreamingBadRecords,\ TestSymLink,TestMultipleArchiveFiles,TestGridmixSubmission,TestDistCacheEmulation,\ TestLoadJob,TestSleepJob,TestDistCh,TestCleanupAfterKIll ```  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id - [X] Updated README.txt","open","","amahussein","2021-09-08T13:19:45Z","2022-02-10T20:44:06Z"
"","3460","YARN-10963. Split TestCapacityScheduler by test categories","### Description of PR  Depends on YARN-10960 YARN-10691 YARN-10692  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-20T08:12:06Z","2021-12-16T22:39:04Z"
"","3638","HDFS-16313. Add metrics for each subcluster","### Description of PR  Currently we have metrics to track the operations for Router to all nameservices, like ""FederationRPCMetrics"", but we don't have metrics for Router to each nameservices.  This ticket is to add metrics for each nameservice to better track the performance of each sub cluster.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16313  ### How was this patch tested?  unit test   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-11-09T15:39:26Z","2021-11-16T01:47:54Z"
"","3613","HDFS-16296. RouterRpcFairnessPolicyController add rejected permits for each nameservice","### Description of PR  Currently RouterRpcFairnessPolicyController has a metric of ""getProxyOpPermitRejected"" to show the total rejected invokes due to lack of permits.  This ticket is to add the metrics for each nameservice to have a better view of the load of each nameservice.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16296  ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-11-03T04:42:54Z","2021-11-05T03:03:29Z"
"","3459","YARN-10962. Do not extend from CapacitySchedulerTestBase when not needed.","### Description of PR   Depends on YARN-10960.   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-20T08:08:55Z","2021-10-07T14:21:10Z"
"","3401","YARN-10917. Investigate and simplify CapacitySchedulerConfigValidator#validateQueueHierarchy","### Description of PR   ### How was this patch tested?  This was just a refactor, existing tests should cover this method.  I checked the if - elif statements, fortunately they did not utilise the previous if branch, proof:  ```   // if (oldQueue instanceof ParentQueue && !(oldQueue instanceof ManagedParentQueue) && newQueue instanceof ManagedParentQueue)   // a: oldQueue instanceof ParentQueue   // b: oldQueue instanceof ManagedParentQueue   // c: newQueue instanceof ManagedParentQueue   // a && !b && c                  --> a && !b && c    // else if (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))   // !(a && !b && c) && (b && !c)  --> b && !c   //                               --> (oldQueue instanceof ManagedParentQueue && !(newQueue instanceof ManagedParentQueue))    // else if (oldQueue instanceof LeafQueue && newQueue instanceof ParentQueue)   // d: oldQueue instanceof LeafQueue   // !(!(a && !b && c) && (b && !c)) && (d && c)  --> c && d   //                                              --> newQueue instanceof ManagedParentQueue && oldQueue instanceof LeafQueue     // else if (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   // e: newQueue instanceof LeafQueue   // !(!(!(a && !b && c) && (b && !c)) && (d && c)) && (a && e)  --> (a && !c && e) || (a && !d && e)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && !(oldQueue instanceof LeafQueue) && newQueue instanceof LeafQueue)   //                                                             --> (oldQueue instanceof ParentQueue && !(newQueue instanceof ManagedParentQueue) && newQueue instanceof LeafQueue) || (oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue)   //                                                             --> oldQueue instanceof ParentQueue && newQueue instanceof LeafQueue ```   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-09-08T09:12:36Z","2021-09-08T12:10:00Z"
"","3314","YARN-10891. Extend QueueInfo with max-parallel-apps in CS.","### Description of PR   ### How was this patch tested?   ``` hadoop-3.4.0-SNAPSHOT/bin/yarn queue --status root.default 2>/dev/null Queue Information : Queue Name : default Queue Path : root.default 	State : RUNNING 	Capacity : 50.00% 	Current Capacity : .00% 	Maximum Capacity : 100.00% 	Weight : -1.00 	Maximum Parallel Apps : 2147483647 	Default Node Label expression :  	Accessible Node Labels : * 	Preemption : disabled 	Intra-queue Preemption : disabled ```  ``` hadoop-3.4.0-SNAPSHOT/bin/yarn queue --status root.users 2>/dev/null Queue Information : Queue Name : users Queue Path : root.users 	State : RUNNING 	Capacity : 50.00% 	Current Capacity : .00% 	Maximum Capacity : 100.00% 	Weight : -1.00 	Maximum Parallel Apps : 42 	Default Node Label expression :  	Accessible Node Labels : * 	Preemption : disabled 	Intra-queue Preemption : disabled ```  ``` curl ""http://localhost:8088/ws/v1/cluster/scheduler?user.name=tdomok"" -s | jq | grep -B 10 -i parallel             ""queuePath"": ""root.default"",             ""capacity"": 50,             ""usedCapacity"": 0,             ""maxCapacity"": 100,             ""absoluteCapacity"": 50,             ""absoluteMaxCapacity"": 100,             ""absoluteUsedCapacity"": 0,             ""weight"": -1,             ""normalizedWeight"": 0,             ""numApplications"": 0,             ""maxParallelApps"": 2147483647, --             ""queuePath"": ""root.users"",             ""capacity"": 50,             ""usedCapacity"": 0,             ""maxCapacity"": 100,             ""absoluteCapacity"": 50,             ""absoluteMaxCapacity"": 100,             ""absoluteUsedCapacity"": 0,             ""weight"": -1,             ""normalizedWeight"": 0,             ""numApplications"": 0,             ""maxParallelApps"": 42, ```   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tomicooler","2021-08-19T06:18:32Z","2021-08-27T21:09:55Z"
"","3692","HDFS-16317 : Backport HDFS-14729 for branch-3.2","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","AnanyaSingh2121","2021-11-20T14:08:42Z","2021-11-30T12:31:54Z"
"","3551","YARN-10904. Investigate: Remove unnecessary fields from AbstractCSQueue or group fields by feature if possible","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-10-13T23:02:18Z","2021-10-27T17:03:46Z"
"","3528","HADOOP-17955. Bump netty to the latest 4.1.68.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tasanuma","2021-10-07T10:05:32Z","2021-10-08T01:35:08Z"
"","3419","YARN-10911. AbstractCSQueue: Create a separate class for usernames and weights that are travelling in a Map","### Description of PR   ### How was this patch tested?   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-09-10T22:24:08Z","2021-09-20T14:48:06Z"
"","3665","pull-request-creator 1","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ardier","2021-11-16T06:08:17Z","2021-11-16T14:16:02Z"
"","3650","YARN-9853. Add number of paused containers in NodeInfo page.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-11-12T09:16:52Z","2022-01-20T04:34:53Z"
"","3618","YARN-11000. Replace queue resource calculation logic in updateClusterResource","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","9uapaw","2021-11-04T15:00:08Z","2021-11-22T13:15:04Z"
"","3614","YARN-10999. Make NodeQueueLoadMonitor pluggable in ResourceManager","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","minni31","2021-11-03T07:21:02Z","2021-11-03T11:31:09Z"
"","3607","YARN-8859. Add audit logs for router service","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","minni31","2021-11-02T08:58:03Z","2022-01-03T05:08:59Z"
"","3606","YARN-10997. Revisit allocation and reservation logging","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-11-02T08:26:17Z","2021-11-12T14:43:59Z"
"","3604","YARN-10996. Fix race condition of User object acquisitions","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-11-02T08:01:15Z","2021-11-12T14:41:09Z"
"","3603","[YARN-10998] Add YARN_ROUTER_HEAPSIZE to yarn-env for routers","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","minni31","2021-11-02T07:57:14Z","2021-11-11T17:49:29Z"
"","3581","YARN-10924. Clean up CapacityScheduler#initScheduler","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-10-23T15:01:50Z","2021-10-27T15:17:01Z"
"","3580","Branch 2.10.1","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","bhalchandrap","2021-10-22T17:34:23Z","2021-10-22T17:35:09Z"
"","3575","HADOOP-17946. Upgrade commons-lang to 3.12.0","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","prasad-acit","2021-10-21T21:31:29Z","2021-10-26T01:16:51Z"
"","3573","HADOOP-17972 : Backport HADOOP-17683 for branch-3.2","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","AnanyaSingh2121","2021-10-21T12:03:23Z","2021-10-28T02:46:28Z"
"","3570","YARN-10985. Add some tests to verify ACL behaviour in CapacitySchedulerConfiguration","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-10-20T13:11:50Z","2021-10-28T14:20:36Z"
"","3560","YARN-10958. Use correct configuration for Group service init in CSMappingPlacementRule","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-10-16T15:30:18Z","2021-10-20T08:48:42Z"
"","3550","YARN-10907. Minimize usages of AbstractCSQueue#csContext","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-10-13T20:53:23Z","2021-12-13T20:58:31Z"
"","3549","YARN-10916. Investigate and simplify GuaranteedOrZeroCapacityOverTimePolicy#computeQueueManagementChanges","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-10-13T17:27:09Z","2021-10-20T13:52:38Z"
"","3495","Custom/3.2.1","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","rsassi","2021-09-28T16:25:59Z","2021-09-28T16:30:23Z"
"","3470","YARN-10965. Centralize queue resource calculation based on CapacityVectors","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","9uapaw","2021-09-23T11:22:28Z","2022-04-01T17:35:59Z"
"","3464","Filter Changes for Bigtable","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","divyapandian5","2021-09-21T06:39:47Z","2021-09-22T13:58:46Z"
"","3450","YARN-10937. Fix log message arguments in LogAggregationFileController","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ghost","2021-09-17T07:46:59Z","2021-09-19T12:36:08Z"
"","3449","YARN-10936. Log typo corrected.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","ghost","2021-09-16T11:40:42Z","2021-09-17T06:31:44Z"
"","3444","HDFS-16218 RBF: Use HdfsConfiguration for passing in Router principle","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","fengnanli","2021-09-15T23:36:11Z","2021-09-16T05:58:02Z"
"","3430","YARN-10942. Move AbstractCSQueue fields to separate objects that are tracking usage","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-09-13T20:55:18Z","2021-10-19T10:24:59Z"
"","3420","YARN-10913. AbstractCSQueue: Group preemption methods and fields into a separate class","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","szilard-nemeth","2021-09-10T22:38:14Z","2021-09-19T11:11:58Z"
"","3396","YARN-10872. Replace getPropsWithPrefix calls in AutoCreatedQueueTemplate","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-09-07T16:46:22Z","2021-09-10T15:33:04Z"
"","3392","YARN-10852. Optimise CSConfiguration getAllUserWeightsForQueue","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-09-06T16:42:49Z","2021-09-10T14:59:46Z"
"","3358","YARN-10930. Introduce universal capacity resource vector","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-08-31T10:41:18Z","2021-10-22T15:41:07Z"
"","3346","HDFS-16188. RBF: Router to support resolving monitored namenodes with DNS","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","LeonGao91","2021-08-27T20:57:54Z","2021-09-10T23:42:09Z"
"","3342","YARN-10897. Introduce QueuePath class","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","9uapaw","2021-08-26T15:34:42Z","2021-09-21T14:08:24Z"
"","3334","HDFS-16186 Datanode kicks out hard disk logic optimization","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","singer-bin","2021-08-26T06:32:25Z","2021-10-13T02:02:14Z"
"","3330","YARN-10522. Document for Flexible Auto Queue Creation in Capacity Scheduler.","### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","brumi1024","2021-08-24T15:48:48Z","2021-09-08T14:58:56Z"
"","3277","HADOOP-17799. Improve the GitHub pull request template","### Description of PR  Improve the GitHub PR template for developers.  ### How was this patch tested?  N/A  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","documentation,","aajisaka","2021-08-08T05:13:32Z","2021-08-14T12:16:57Z"
"","2955","HDFS-15624. fix the function of setting quota by storage type","1. puts NVDIMM to the end of storage type enum to make sure compatibility.     2. adds check to make sure the software layout version is satisfied      Co-authored-by: su xu      Co-authored-by: huangtianhua      Co-authored-by: YaYun-Wang <34060507+YaYun-Wang@users.noreply.github.com>      Signed-off-by: Mingliang Liu      Signed-off-by: Ayush Saxena      Signed-off-by: Vinayakumar B   This is the same PR as (#2377) except the layout version was updated to avoid conflict with HDFS-15666.","closed","","jojochuang","2021-04-26T03:59:55Z","2021-04-29T06:54:40Z"
"","2755","HADOOP-17572. [branch-2.10] Docker image build fails due to the removal of openjdk-7-jdk package","- Use Zulu OpenJDK 7. - Enable multijdk option to compile with both Java 7 and 8 in GitHub PR.  JIRA: https://issues.apache.org/jira/browse/HADOOP-17572","closed","","aajisaka","2021-03-10T07:44:53Z","2021-03-11T14:08:21Z"
"","2717","HADOOP-16748. Migrate to Python 3 and upgrade Yetus to 0.13.0 (branch-3.2)","- Upgrade Yetus to 0.13.0 to support Python 3 for the release scripts. - Removed determine-flaky-tests-hadoop.py. - Temporarily disabled shelldocs check due to YETUS-1099.  Reviewed-by: Inigo Goiri  Reviewed-by: Mingliang Liu  (cherry picked from commit b9b49ed956e6fa9b55758f3d2c1b92ae2597cdbb)   Conflicts: 	dev-support/Jenkinsfile 	dev-support/docker/Dockerfile  (cherry picked from commit ff8b79f890d65c4215c5e19cdf45cc2a4f0a348e)  JIRA: https://issues.apache.org/jira/browse/HADOOP-16748","closed","","aajisaka","2021-02-22T14:27:10Z","2021-02-24T02:06:16Z"
"","3439","YARN-10957. Using invokeConcurrent Overload with Collection in getClusterMetrics","- In #3135, we added a new overload of invokeConcurrent to avoid ArrayList initialization at multiple places. Using the same in getClusterMetrics. - Verified the unit tests for FederationClientInterceptor locally.","closed","","akshatb1","2021-09-15T16:23:35Z","2021-09-28T16:51:19Z"
"","3200","HDFS-15160. branch-3.2. ReplicaMap, Disk Balancer, Directory Scanner and various FsDatasetImpl methods should use datanode readlock.","- Backporting HDFS-15160 to branch-3.2    - First: [HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O'Donnell](https://issues.apache.org/jira/browse/HDFS-15150)      - `git cherry-pick -x d7c136b9ed6d99e1b03f5b89723b3a20df359ba8`      - main conflicts:        - `FsDatasetImpl.java.deepCopyReplica`        - `FoldedTreeSet` is not used in branch-3.2      - testing:        - ```          mvn test -Dtest=TestWriteToReplica,TestReplicaMap,\                  TestProvidedImpl,TestFsVolumeList,TestInterDatanodeProtocol          ```    - Second: [HDFS-15160](https://issues.apache.org/jira/browse/HDFS-15160)      - `git cherry-pick -x 2a67e2b1a0e3a5f91056f5b977ef9c4c07ba6718`      - main conflicts:        - `ReplicaMap.java`        - `FsDatasetImpl.java.deepCopyReplica`        - `FsDatasetImpl.java.addVolume` different code. I passed the red and write locks to `tempVolumeMap`        - `DirectoryScanner.scan()` conflict in the entire loop.           - [HDFS-15415. Reduce locking in Datanode DirectoryScanner](https://issues.apache.org/jira/browse/HDFS-15415) Which is already committed to branch-2.10 removes what [HDFS-15160](https://issues.apache.org/jira/browse/HDFS-15160) has added. So, I skipped that.     - Third: [HDFS-15457. TestFsDatasetImpl fails intermittently (#2407)](https://issues.apache.org/jira/browse/HDFS-15457) and [HDFS-15818. Fix TestFsDatasetImpl.testReadLockCanBeDisabledByConfig (#2679)](https://issues.apache.org/jira/browse/HDFS-15818)       - `git cherry-pick -x 98097b8f19789605b9697f6a959da57261e0fe19 9434c1eccc255a25ea5e11f6d8c9e1f83996d6b4`       - main conflicts:         - Java imports","closed","backport,","amahussein","2021-07-13T16:02:21Z","2021-09-14T07:18:52Z"
"","3252","YARN-10829. Follow up: Adding null checks before merging ResourceUsage Report","- Adding null checks before merging ResourceUsage Report. Follow up from previous PR: https://github.com/apache/hadoop/pull/3135 - Unit tests are running successfully on dev machine.","closed","","akshatb1","2021-08-01T07:28:22Z","2021-09-08T17:36:56Z"
"","3093","[YARN-10817][Client] Avoid NPE when use yarn command like yarn queue -status","**What changes were proposed in this pull request?** Currently, We will encounter NullPointerException when we use some yarn shell commands, As follow:  yarn queue -status ``` Missing argument for options usage: queue  -help                  Displays help for all commands.  -status    List queue information about given queue. Exception in thread ""main"" java.lang.NullPointerException 	at org.apache.hadoop.yarn.client.cli.YarnCLI.stop(YarnCLI.java:75) 	at org.apache.hadoop.yarn.client.cli.QueueCLI.main(QueueCLI.java:51) ``` Like this, and  ``` yarn cluster yarn queue yarn node ```  This is beacuse we dont init the `YarnClient` when missing argument for options, This PR will fix the issue.","closed","","yikf","2021-06-10T12:15:22Z","2021-06-10T14:01:16Z"
"","3630","HADOOP-17995. Stale record should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson","**Description of PR** As HADOOP-16947 problem with description. Stale SumAndCount also should be remove when DataNodePeerMetrics#dumpSendPacketDownstreamAvgInfoAsJson. Ensure the DataNode JMX get SendPacketDownstreamAvgInfo Metrics is accurate.  Details: HADOOP-17995","closed","","haiyang1987","2021-11-08T12:48:26Z","2021-11-23T10:04:31Z"
"","3511","HDFS-16247. RBF: Fix the ProcessingAvgTime and ProxyAvgTime code comments and document metrics describe ms unit","**Description of PR**  RBF: Fix the ProcessingAvgTime and ProxyAvgTime code comments and document metrics describe ms unit Details: HDFS-16247","closed","","haiyang1987","2021-10-01T12:05:36Z","2021-10-04T15:52:53Z"
"","3166","HADOOP-17786. Parallelize stages in Jenkins","* We now build Hadoop on multiple platforms    for validation. We need to parallelize them    for faster validation.","open","","GauthamBanasandra","2021-07-01T10:46:09Z","2021-08-03T09:39:52Z"
"","3238","HADOOP-17816. Run optional CI for changes in C","* We need to ensure that we run   the CI for all the platforms   when there are changes in C files.","closed","","GauthamBanasandra","2021-07-27T07:48:49Z","2021-08-05T12:13:34Z"
"","2856","HDFS-15947. Replace deprecated protobuf APIs","* Used the appropriate Protobuf APIs to   replace those which are on the path to   deprecation.","closed","","GauthamBanasandra","2021-04-02T18:08:05Z","2021-04-05T16:52:49Z"
"","2873","HDFS-15948. Fix test4tests for libhdfspp","* Updated YETUS_VERSION which contains   the fix for test4tests.","closed","","GauthamBanasandra","2021-04-07T09:18:39Z","2021-04-07T10:06:25Z"
"","3014","HDFS-16026. Restore cross platform mkstemp","* This reverts commit aed13f0f42fefe30a53eb73c65c2072a031f173e.","closed","","GauthamBanasandra","2021-05-15T08:57:42Z","2021-05-23T17:00:43Z"
"","3044","Revert ""Revert ""HDFS-15971. Make mkstemp cross platform (#2898)""""","* This reverts commit   aed13f0f42fefe30a53eb73c65c2072a031f173e. * Verified by building locally on Centos 7 that   Hadoop builds fine with this PR. * Build log -   https://issues.apache.org/jira/secure/attachment/13025814/build-log.zip   Reverted commit -   https://issues.apache.org/jira/secure/attachment/13025815/commit-details.txt   Dockerfile_centos_7 -   https://issues.apache.org/jira/secure/attachment/13025816/Dockerfile_centos_7","closed","","GauthamBanasandra","2021-05-23T16:36:49Z","2021-05-27T05:14:41Z"
"","3129","HADOOP-17766. CI for Debian 10","* This PR adds a stage in the CI pipeline    for Debian 10, which is optional. * The CI runs for Debian 10 only if there    are any changes in C++ files or C++    build system or the platform. It makes    use of the existing CI setup with YETUS    and posts a comment on the PR about    the run summary. * CI always runs for Ubuntu Focal. * This PR makes it extensible to onboard    CI for new environments.","closed","","GauthamBanasandra","2021-06-21T14:24:53Z","2021-06-23T17:02:34Z"
"","2857","HDFS-15949. Fix integer overflow","* There are some instance where   the overflow is deliberately   done in order to get the lower   bound. This results in an   overflow warning. * We fix this by using the min   value directly.","closed","","GauthamBanasandra","2021-04-03T06:17:33Z","2021-04-06T09:22:55Z"
"","2875","HDFS-15955. Make explicit_bzero cross platform","* The function explicit_bzero isn't available   in Visual C++. Need to make this cross platform.","closed","","GauthamBanasandra","2021-04-07T17:21:11Z","2021-04-08T16:44:47Z"
"","3151","HADOOP-17778. CI for Centos 8","* The CI will now for Dockerfile_centos_8   if there are any changes in C++ files/   C++ build files/platform related files.","closed","","GauthamBanasandra","2021-06-27T14:59:52Z","2021-07-01T11:18:14Z"
"","2827","HDFS-15935. Use memcpy for copying non-null terminated string.","* strncpy reports a warning if the   destination string isn't null   terminated. Since we append a   custom character at the end ourselves,   the warning reported by stnrcpy is   valid, but not relevant. * Thus, we use memcpy to avoid this   warning.","closed","","GauthamBanasandra","2021-03-28T11:21:18Z","2021-03-29T17:50:11Z"
"","2818","HDFS-15922. Use memcpy for copying non-null terminated string in jni_helper.c","* strncpy reports a warning if the   destination string isn't null   terminated. * The scenario here is that the string   is deliberately not null terminated   since we want to imperatively suffix   a PATH_SEPARATOR at the end. * Thus, the warning reported by strncpy   even though valid, isn't applicable. * Hence we replace strncpy with memcpy   which doesn't worry if the string   is null terminated or not.","closed","","GauthamBanasandra","2021-03-25T16:11:17Z","2021-03-27T12:00:39Z"
"","2883","HDFS-15962. Make strcasecmp cross platform","* strcasecmp isn't available on Visual C++.   Need to make this cross platform.","closed","","GauthamBanasandra","2021-04-09T07:59:52Z","2021-04-09T16:01:33Z"
"","2923","HDFS-15989. Split TestBalancer and De-flake testMaxIterationTime()","* Split TestBalancer into two test classes. TestBalancerLongRunningTasks has fewer but long running tests whereas TestBalancer has more tests with short duration. * Fix flaky test: testMaxIterationTime()","closed","","virajjasani","2021-04-17T10:14:44Z","2021-04-21T10:47:11Z"
"","2793","HDFS-15910. Improve security with explicit_bzero","* Replacing bzero with explicit_bezero since it   is more secure. * explicit_bzero guarantees that the buffer is   cleared irrespective of the compiler optimization,   unlike bzero.","closed","","GauthamBanasandra","2021-03-21T07:08:19Z","2021-03-23T16:58:20Z"
"","2826","HDFS-15929. Replace RAND_pseudo_bytes in util.cc","* RAND_pseudo_bytes is deprecated in OpenSSL   1.1.1 and must be replaced by RAND_bytes. * Refactored usages of GetRandomClientName where   it now returns the client name wrapped in a   a shared_ptr to provide the ability to do   null check. We use a null check as a means to   get the functionality similar to monads in   functional programming.","closed","","GauthamBanasandra","2021-03-28T09:06:36Z","2021-07-30T12:05:14Z"
"","2825","HDFS-15928. Replace RAND_pseudo_bytes in rpc_engine.cc","* RAND_pseudo_bytes is deprecated in OpenSSL   1.1.1 and must be replaced by RAND_bytes. * Refactored RpcEngine where client_id is wrapped in   a unique_ptr to provide the ability to do null check.   We use a null check as a means to get the   functionality similar to monads.","closed","","GauthamBanasandra","2021-03-27T17:00:54Z","2021-03-30T17:37:11Z"
"","2811","HDFS-15918. Replace deprecated RAND_pseudo_bytes","* RAND_pseudo_bytes is deprecated in OpenSSL   1.1.1 and must be replaced by RAND_bytes.","closed","","GauthamBanasandra","2021-03-24T07:50:03Z","2021-03-27T12:02:49Z"
"","3633","HADOOP-17979. Add Interface EtagSource to allow FileStatus subclasses to provide etags","* Pull out from HADOOP-17981/#3611 * Add S3A support * Add requirement that located status API calls MUST also support the API, and do this for ABFS.  s3a code retains now deprecated getETag entries. lots of references in the s3guard code which I am leaving alone.   ### How was this patch tested?  new integration tests for abfs and s3a  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","steveloughran","2021-11-08T18:40:30Z","2021-11-24T17:33:12Z"
"","3231","HADOOP-17815. Run CI for Centos 7","* Need to run the CI for Centos 7 platform   since it's a supported platform. * The CI will run on this platform only when   there's C++ file/C++ build/platform related   changes.","closed","","GauthamBanasandra","2021-07-25T12:04:06Z","2021-07-30T02:42:30Z"
"","2853","HDFS-15944. Prevent truncation by snprintf","* Need to ensure that the destination   buffer size is greater the source content.   Truncation could happen otherwise. * snprintf returns the size to be written   when we pass NULL for destination buffer   and 0 for the size of the destination   buffer. * We will use this information to ensure   that the source content doesn't exceed   the destination buffer's size before   making the call to write.","closed","","GauthamBanasandra","2021-04-02T07:48:45Z","2021-04-02T17:55:22Z"
"","3199","[Do not commit] Use exclusive src for each platform","* Need to ensure that each platform build stage   runs on its own copy of the source directory.   This is needed to parallellize the multi-platform   build.","closed","","GauthamBanasandra","2021-07-13T14:55:24Z","2021-07-17T14:36:15Z"
"","2824","HDFS-15927. Catch polymorphic type by reference","* Need to catch polymorphic exception types by   reference in order to realize the polymorphic   usage. Otherwise, the functionality of the   caught object is restricted to only that of   the base class.","closed","","GauthamBanasandra","2021-03-27T11:56:26Z","2021-03-30T20:35:46Z"
"","3167","HADOOP-17787. Refactor fetching of credentials in Jenkins","* Moved fetching of username and password    to a function.","closed","","GauthamBanasandra","2021-07-01T14:50:04Z","2021-08-06T18:56:28Z"
"","2783","HDFS-15903. Refactor X-Platform lib","* Moved C API to a separate folder   which currently contains the   syscall API. * Dropped _utils_ from the X-Platform   object library target names since it   doesn't contain only utils anymore.   It's home for syscall implementations   too.","closed","","GauthamBanasandra","2021-03-17T17:46:11Z","2021-03-22T15:40:54Z"
"","2898","HDFS-15971. Make mkstemp cross platform","* mkstemp isn't available in Visual C++.   This PR implements the necessary   cross platform implementation for   mkstemp.","closed","","GauthamBanasandra","2021-04-12T17:53:19Z","2021-04-13T15:56:03Z"
"","2908","HDFS-15976. Make mkdtemp cross platform","* mkdtemp is used for creating temporary   directory, adhering to the given pattern.   It's not available on Visual C++. Need   to make this cross platform.","closed","","GauthamBanasandra","2021-04-14T14:57:08Z","2021-08-10T16:25:13Z"
"","3007","[Do not commit] Test building on Centos 7 Docker","* Just to test, do not commit.","closed","","GauthamBanasandra","2021-05-12T10:06:44Z","2021-05-13T05:21:51Z"
"","3281","HADOOP-17836. Improve logging on ABFS network errors","* include masked URL and nested exceptions in wrapping IOEs * wrapping IOEs now path IOEs * avoid scaring people by printing full stack trace on a recoverable retry loop,   logging summary at WARN and full stack only @ debug * Fix another log entry which accidentally used %s over {} for insertion   point  Change-Id: I92c893834cbe65c2dbeb1d4fd2081c9fa71c4d0d  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","steveloughran","2021-08-09T13:02:10Z","2021-08-18T10:40:17Z"
"","3303","HDFS-16174. Refactor TempFile and TempDir in libhdfs++","* Implemented TempFile and   TempDir in cc files for avoiding   duplicate compilation of these   classes. * Wrapped these classes in a   TestUtils namespace. * Created an object library for   TestUtils and added this as a   target as necessary.","closed","","GauthamBanasandra","2021-08-14T08:07:51Z","2021-08-18T03:12:34Z"
"","2858","HDFS-15950. Remove unused hdfs.proto import","* hdfs.proto is imported in inotify.proto   and is unused. Thus, removing it.","closed","","GauthamBanasandra","2021-04-03T08:12:14Z","2021-04-05T02:42:05Z"
"","2935","HADOOP-17651. Backport to branch-3.1 HADOOP-17371, HADOOP-17621, HADO…OP-17625 to update Jetty to 9.4.39.","* HADOOP-17371. Bump Jetty to the latest version 9.4.34. Contributed by Wei-Chiu Chuang. (#2453)  (cherry picked from commit 66ee0a6df0dc0dd8242018153fd652a3206e73b5) (cherry picked from commit 6340ac857b7ff3f73bbcf95b59b98aac134f33af)   Conflicts: 	hadoop-client-modules/hadoop-client-minicluster/pom.xml  Change-Id: I673ac136922740cb1d426cb9593aa1bd3e9acd32  * HADOOP-17621. hadoop-auth to remove jetty-server dependency. (#2865)  Reviewed-by: Akira Ajisaka  (cherry picked from commit dac60b8282013d7776667415a429e7ca35efba66) (cherry picked from commit 1110b03752b45bc4695baaa6d9655e18de67303a)  * HADOOP-17625. Update to Jetty 9.4.39. (#2870)  Reviewed-by: cxorm  (cherry picked from commit 6040e86e99aae5e29c17b03fddb0a805da8fcae8) (cherry picked from commit 7f7535534d5541e10b6b14dee3aa38a00058f201) (cherry picked from commit 8ff61f9b076a7022a1dfc067b9f94434756b64a7)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java  Change-Id: I465f6003b6f4c5df9c41c83eac3738bac56403e1  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jojochuang","2021-04-20T03:46:03Z","2021-04-20T13:13:33Z"
"","2792","HDFS-15909. Make fnmatch cross platform","* fnmatch.h isn't available on Visual C++   compiler.This PR makes it cross   platform by using fnmatch on Linux and   PathMatchSpecA on Windows appropriately.","closed","","GauthamBanasandra","2021-03-20T16:03:20Z","2021-04-06T16:23:21Z"
"","3210","HADOOP-17807. Use separate src dir for platform builds","* Currently, each platform build run on the same   checkout of the source code and YETUS deletes   the git branch information. This is affecting   the subsequent (optional) platform builds. * Need to ensure that each platform build stage   runs on its own copy of the source directory. * Here's how the directory structure looked like   after the code was checked out in the CI, prior to   this PR -     .   └── workdir       ├── out       └── src * Here's how the directory structure will look like   with this PR -     .   └── workdir       ├── centos-8       │   ├── out       │   └── src       ├── debian-10       │   ├── out       │   └── src       ├── out       ├── src       └── ubuntu-focal           ├── out           └── src","closed","","GauthamBanasandra","2021-07-16T07:39:45Z","2021-07-27T04:54:31Z"
"","2710","HDFS-15843. Make write cross-platform","* Currently write from unistd.h is used   which isn't cross-platform. * We need to use std::cout.write instead.","closed","","GauthamBanasandra","2021-02-20T12:17:05Z","2021-03-16T16:06:51Z"
"","3393","HADOOP-17894. CredentialProviderFactory.getProviders() recursion loading JCEKS file from S3A","* CredentialProviderFactory to detect and report on recursion. * S3AFS to remove incompatible providers. * ITest for this.  Tested: s3a london; new test done, full test in progress","closed","","steveloughran","2021-09-06T18:15:04Z","2021-09-07T14:29:38Z"
"","3043","HADOOP-17727. Modularize docker images","* Created pkg-resolver which resolves   the package dependencies needed   for building Hadoop on the various   platforms. * This is helpful for keeping track of   the dependencies on the all the   different platforms for which we   support building Hadoop on.","closed","","GauthamBanasandra","2021-05-22T12:46:40Z","2021-06-08T03:50:08Z"
"","3006","HADOOP-17693. Dockerfile for building on Centos 8","* Adding a Dockerfile for building on Centos 8    since some folks in the community are using it.","closed","","GauthamBanasandra","2021-05-12T09:52:02Z","2021-05-14T10:42:42Z"
"","2967","HADOOP-17678. Add Dockerfile for Centos 7","* Adding a Dockerfile for building on Centos 7   since some folks in the Hadoop community   are using Centos 7.","closed","","GauthamBanasandra","2021-04-30T16:26:11Z","2021-05-10T17:13:14Z"
"","3038","HADOOP-17724. Add Dockerfile for Debian 10","* Adding a Dockerfile for building on   Debian 10 since there are a lot of   users in the community using this   distro.","closed","","GauthamBanasandra","2021-05-21T10:10:50Z","2021-06-17T17:20:35Z"
"","2807","HADOOP-17511. Add audit/telemetry logging to S3A connector","(Squashed and rebased from #2675)  Notion of AuditSpan which is created for a given operation; goal is to pass it along everywhere.  It's thread local per FS-instance; store operations pick this up in their constructor from the StoreContext.  The entryPoint() method in S3A FS has been enhanced to initiate the spans. For this to work, internal code SHALL NOT call those entry points (Done) and all public API points MUST be declared as entry points.   This is done, with a marker attribute `@AuditEntryPoint` to indicate this.  The audit span create/deactivate sequence is ~the same as the duration tracking so the operation is generally merged: most of the metrics S3AFS collects are now durations  Part of the isolation into spans means that there's explicit operations for mkdirs() and getContentSummary()  The auditing is intended to be a plugin point; currently there is  the LoggingAuditor which * logs at debug * adds an HTTP ""referer"" header with audit tracing * can be set to raise an exception if the SDK is handed an AWS Request and there's no active span (skipped for the multipart upload part and complete calls as TransferManager in the SDK does that out of span).  NoopAuditor which:  *does nothing  A recent change is that we want every span to have a spanID (string, unique across all spans of that FS instance); even the no-op span has unique IDs.","closed","fs/s3,","steveloughran","2021-03-23T16:23:28Z","2021-10-15T19:42:54Z"
"","3501","HADOOP-17945. JsonSerialization raises EOFException reading JSON data stored on google GCS","(lifted from MAPREDUCE-7341)   ### How was this patch tested?  * updated tests * rerunning s3a committer suite * In MAPREDUCE-7341; testing against azure abfs too    - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","steveloughran","2021-09-29T14:53:26Z","2021-10-19T10:03:38Z"
"","2882","HDFS-15815. if required storageType are unavailable, log the failed reason during choosing Datanode. Contributed by Yang Yun.","(cherry picked from commit e391844e8e414abf8c94f7bd4719053efa3b538a)","closed","","jojochuang","2021-04-09T06:21:07Z","2021-04-13T06:55:32Z"
"","2869","HDFS-15759. EC: Verify EC reconstruction correctness on DataNode (#2585)","(cherry picked from commit 95e68926750b55196cf9da53c25359c98ef58a4f) (cherry picked from commit 6cfa01c905ca422c3c6b2d1d735dd16ee08b4fb4)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java  Change-Id: Id5eb928b8155f3fe836f0e28754021f908128a75 (cherry picked from commit ef2c589b25397a5c1571c0740c32117620962c70)  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jojochuang","2021-04-06T05:36:32Z","2021-04-19T00:53:36Z"
"","2868","HDFS-15759. EC: Verify EC reconstruction correctness on DataNode (#2585)","(cherry picked from commit 95e68926750b55196cf9da53c25359c98ef58a4f) (cherry picked from commit 6cfa01c905ca422c3c6b2d1d735dd16ee08b4fb4)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedReconstructor.java  ## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jojochuang","2021-04-06T05:33:49Z","2021-04-13T11:05:10Z"
"","2711","Backport HADOOP-16054 (Update Dockerfile to use Bionic) to branch-3.3","(cherry picked from commit 81d8b71534645a2109a037115fb955351edfbf64)  JIRA: https://issues.apache.org/jira/browse/HADOOP-16054","closed","","aajisaka","2021-02-22T02:55:09Z","2021-02-22T04:50:59Z"
"","2713","HADOOP-16054. Update Dockerfile to use Bionic (branch-3.2)","(cherry picked from commit 81d8b71534645a2109a037115fb955351edfbf64)   Conflicts: 	dev-support/docker/Dockerfile  JIRA: https://issues.apache.org/jira/browse/HADOOP-16054  Backport #1966 to branch-3.2.","closed","","aajisaka","2021-02-22T07:15:40Z","2021-02-22T08:08:29Z"
"","3125","HADOOP-17715 ABFS: Append blob tests with non HNS accounts fail (#3028)","(cherry picked from commit 4c039fafebfe7b4d68b60c5ec6075d889ab1c40b)","closed","","snehavarma","2021-06-21T07:44:29Z","2021-07-12T06:21:42Z"
"","3640","HDFS-16187. SnapshotDiff behaviour with Xattrs and Acls is not consistent across NN restarts with checkpointing (#3340)","(cherry picked from commit 356ebbbe80aef991d564a6140e341ddd76176416)   Conflicts: 	hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java  Change-Id: If76fd0d77fafc90fe2f2c19ab1d0c43a58510f6b    ### Description of PR   ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2021-11-10T05:20:34Z","2021-11-12T06:32:03Z"
"","3138","HADOOP-17769. Upgrade JUnit to 4.13.2. branch-3.3","(#3130). Contributed by Ahmed Hussein.  Signed-off-by: Ayush Saxena  (cherry picked from commit 581f43dce15d5753deb33f72ec275d0ea67b1403)  backport upgrading Junit-4.13.2 to branch-3.3  @ayushtkn","closed","","amahussein","2021-06-24T16:04:28Z","2021-06-25T17:18:56Z"
"","3139","HADOOP-17769. Upgrade JUnit to 4.13.2. branch-3.2","(#3130). Contributed by Ahmed Hussein.  Signed-off-by: Ayush Saxena  (cherry picked from commit 581f43dce15d5753deb33f72ec275d0ea67b1403)  backport upgrading Junit-4.13.2 to branch-3.2  @ayushtkn","closed","","amahussein","2021-06-24T16:52:19Z","2021-06-25T17:20:04Z"
"","3498","HADOOP-17851. Support user specified content encoding for S3A","#3312 by @holdenk rebased & with some final wrap-up tweaks.  * ""fs.s3a.object.content-encoding"" declares the content encoding * it's not set for directories. * it's documented this is a verification code text messages for some online account   I changed the key name to line up for adding the rest of the standard headers as/when we choose; `fs.object.*` would cover the set.   Tested, s3 london, no s3guard. One test failure, the usual intermittent `ITestPartialRenamesDeletes.testRenameDirFailsInDelete`  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","steveloughran","2021-09-29T09:35:02Z","2021-09-29T12:59:13Z"
"","2934","YARN-10744. add more doc for yarn.federation.policy-manager-params","### What is the purpose of the change  add more doc for yarn.federation.policy-manager-params","open","","ChaosJu","2021-04-20T00:56:38Z","2021-04-21T02:26:48Z"
"","3042","HADOOP-17728. HDFS-16033. Fix issue of the StatisticsDataReferenceCleaner cleanUp","### What changes were proposed in this pull request? In `StatisticsDataReferenceCleaner`, Cleaner thread will be blocked if we remove reference from ReferenceQueue unless the queue.enqueue` called but no now.  As shown below, We call ReferenceQueue.remove() now while cleanUp, Call chain as follow： `StatisticsDataReferenceCleaner#queue.remove()  ->  ReferenceQueue.remove(0)  -> lock.wait(0)`  lock.wait(0) will waitting perpetual unless lock.notify/notifyAll be called, But, lock.notifyAll is called when queue.enqueue only, so Cleaner thread will be blocked.  **ThreadDump:** ``` ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007f7afc088800 nid=0x2119 in Object.wait() [0x00007f7b00230000]    java.lang.Thread.State: WAITING (on object monitor)         at java.lang.Object.wait(Native Method)         - waiting on <0x00000000c00c2f58> (a java.lang.ref.Reference$Lock)         at java.lang.Object.wait(Object.java:502)         at java.lang.ref.Reference.tryHandlePending(Reference.java:191)         - locked <0x00000000c00c2f58> (a java.lang.ref.Reference$Lock)         at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153) ```","closed","","yikf","2021-05-21T15:11:58Z","2021-06-11T07:49:11Z"
"","3648","HADOOP-18006. maven-enforcer-plugin's execution of banned-illegal-imports gets overridden in child poms","### Description of PR When we specify any maven plugin with execution tag in the parent as well as child modules, child module plugin overrides parent plugin. For instance, when banned-illegal-imports is applied for any child module with only one banned import (let’s say Preconditions), then only that banned import is covered by that child module and all imports defined in parent module (e.g Sets, Lists etc) are overridden and they are no longer applied. As of today, hadoop-hdfs module will not complain about Sets even if i import it from guava banned imports but on the other hand, hadoop-yarn module doesn’t have any child level banned-illegal-imports defined so yarn modules will fail if Sets guava import is used. So going forward, it would be good to replace guava imports with Hadoop’s own imports module-by-module and only at the end, we should add new entry to parent pom banned-illegal-imports list.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-12T05:37:36Z","2021-11-15T13:57:49Z"
"","3544","HDFS-16269. [Fix] Improve NNThroughputBenchmark#blockReport operation.","### Description of PR When using NNThroughputBenchmark to test blockReport, some exceptions will be prompted. E.g: ![image](https://user-images.githubusercontent.com/6416939/136931038-f5a271b8-c905-4464-94cb-1b8b4e9bbe21.png) The purpose of this issue is to solve this problem.  ### How was this patch tested? When using the same command, the same exception will not occur.","closed","","jianghuazhu","2021-10-12T09:35:32Z","2021-11-01T15:56:42Z"
"","3671","MAPREDUCE-7368. DBOutputFormat.DBRecordWriter#write must throw except…","### Description of PR When the exception is caught and printed (instead of propagated) the consumer has no way to tell write failed. This hides the failure and leads to problems which are difficult to debug and at the same time can cause data corruption.  More details in the JIRA MAPREDUCE-7368.  ### How was this patch tested? Writing a well-contained test is not easy at the same time the change is straightforward so it may not be necessary.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","zabetak","2021-11-17T16:19:56Z","2021-12-08T11:10:11Z"
"","3600","HDFS-16290.Make log more standardized when executing verifyAndSetNamespaceInfo().","### Description of PR When the DataNode starting, BPOfferService#verifyAndSetNamespaceInfo() will be executed. If the namenode being connected is in Active state, the following information will be recorded: ' 2021-10-27 18:08:36,242 [50867]-INFO [Thread-33:BPOfferService@376]-Acknowledging ACTIVE Namenode during handshakeBlock pool BP-597961518-xxx.xxx.xxx.xxx-1534758275943 (Datanode Uuid 9b2aedc9-f8b2 -4ee2-b99f-877bc6e42c87) service to xxxx.xxx.xxx.org/xxxx.xxx.xxx.xxx:8021 ' Here, the connection between'handshake' and'Block pool' is too tight, and the readability is not good.  Details: HDFS-16290  ### How was this patch tested? This is related to the log, and there is less information to change. For testing, there is not much pressure.","closed","","jianghuazhu","2021-10-29T03:29:22Z","2021-10-31T03:18:53Z"
"","3629","HDFS-16305.Record the remote NameNode address when the rolling log is triggered.","### Description of PR When StandbyNN triggers the rolling log, it sends a request to the remote NameNode. But there is no way to know the specific information of the NameNode. Here are some log information: ![image](https://user-images.githubusercontent.com/6416939/140687250-d064ae4d-62a5-42da-8fba-ff1ab710930f.png)  We can try to record the specific address. This is the purpose of this pr.  ### How was this patch tested? Some log information has been changed here, which is not very stressful for testing.","closed","","jianghuazhu","2021-11-08T05:08:16Z","2021-11-12T01:52:29Z"
"","3469","HDFS-16234.Improve DataNodeMetrics to initialize IBR more reasonable.","### Description of PR When sending IBR, the timing for DataNodeMetrics to initialize IBR should be triggered reasonably. This is what this jira wants to solve.  Details: HDFS-16234","open","","jianghuazhu","2021-09-23T08:06:49Z","2022-02-01T05:26:36Z"
"","3547","HDFS-16270.Improve NNThroughputBenchmark#printUsage() related to block size.","### Description of PR When NNThroughputBenchmark#printUsage() is executed, some configurations are verified first, and the correct prompts are printed.  ### How was this patch tested? This is to print some information, the test pressure is not great.","closed","","jianghuazhu","2021-10-13T06:04:46Z","2021-10-26T02:23:26Z"
"","3512","HDFS-16245.Record the number of INodeDirectory when loading the FSImage file.","### Description of PR When loading the FSImage file, record the number of loaded INodeDirectory. Details: HDFS-16245  ### How was this patch tested? When the FSImage file is loaded, we need to compare the number of INodeDirectory that has been loaded.","open","","jianghuazhu","2021-10-02T11:35:45Z","2022-07-23T08:32:05Z"
"","3452","HDFS-16219. RBF: Set default map tasks and bandwidth in RouterFederationRename","### Description of PR When dfs.federation.router.federation.rename.option is set to DISTCP, if dfs.federation.router.federation.rename.map and dfs.federation.router.federation.rename.bandwidth are not provided with default values, DFSRouter fails to launch.  ### How was this patch tested? Local dev testing.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-09-17T08:32:01Z","2021-09-22T06:34:52Z"
"","3434","HDFS-16217. RBF: Set default value of hdfs.fedbalance.procedure.scheduler.journal.uri by adding appropriate config resources","### Description of PR When dfs.federation.router.federation.rename.option is set to DISTCP and hdfs.fedbalance.procedure.scheduler.journal.uri is not set, DFSRouter fails to launch. ``` 2021-09-08 15:39:11,818 ERROR org.apache.hadoop.hdfs.server.federation.router.DFSRouter: Failed to start router     java.lang.NullPointerException     at java.base/java.net.URI$Parser.parse(URI.java:3104)     at java.base/java.net.URI.(URI.java:600)     at org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.initRouterFedRename(RouterRpcServer.java:444)     at org.apache.hadoop.hdfs.server.federation.router.RouterRpcServer.(RouterRpcServer.java:419)     at org.apache.hadoop.hdfs.server.federation.router.Router.createRpcServer(Router.java:391)     at org.apache.hadoop.hdfs.server.federation.router.Router.serviceInit(Router.java:188)     at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)     at org.apache.hadoop.hdfs.server.federation.router.DFSRouter.main(DFSRouter.java:69) ``` hdfs.fedbalance.procedure.scheduler.journal.uri is hdfs://localhost:8020/tmp/procedure by default, however, the default value is not used in DFSRouter.  ### How was this patch tested? Local dev testing. After applying fix for HDFS-16219, we can reproduce above error. And the fix provides default value to hdfs.fedbalance.procedure.scheduler.journal.uri.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-09-14T13:01:57Z","2021-09-17T07:39:41Z"
"","3479","HDFS-16237.Record the BPServiceActor information that communicates with Standby.","### Description of PR When BPServiceActor interacts with StandbyNN, we should record the information of BPServiceActor.  Details: HDFS-16237","closed","","jianghuazhu","2021-09-24T04:33:23Z","2021-09-28T05:08:18Z"
"","3579","HADOOP-17975 Fallback to simple auth does not work for a secondary DistributedFileSystem instance.","### Description of PR When an ipc.Client.Connection is created for the first time, it is added to the connections pool based on ConnectionId, and then it is there until the connection is closed. If someone instantiates two clients (eg. when fs.hdfs.impl.disable.cache is true), then the same connection is shared between the two DfsClient due to this pooling.  Connection.setupIOStreams is responsible to set the fallbackToSimpleAuth AtomicBoolean, but when a connection is accessed the second time, setupIOStreams returns without setting the AtomicBoolean properly. This leads to read failures in the second DfsClient, when the client is running with a secure configuration, but connects to a SIMPLE auth cluster with ipc.client.fallback-to-simple-auth-allowed set to true unless it is created after the first one is closed.  The fix is to properly set the fallbackToSimpleAuth AtomicBoolean even though the socket is already created. In this case the authMethod is not null, but there is a sanity check for that, so if somehow we get there with an authMethod equals null, we just do not touch the AtomicBoolean, similarly if the AtomicBoolean itself is null.  ### How was this patch tested? In a real environment with different iterations of the code that is shown in the JIRA ticket. Event orders that were tested: Get two DfsClient with cache enabled - pass (passed before patch) Get one DfsClient with cache disabled, then get on DfsClient with cache enabled - pass (failed before patch) Get two DfsClient with cache disabled - pass (failed before patch) Next tests were with cache disabled: Get one DfsClient close it get the second DfsClient - pass (passed before patch) Get one DfsClient close it after the second DfsClient is created - pass (failed before patch) Get two DfsClient without deliberately closing any of them - pass (failed before patch)","closed","","fapifta","2021-10-22T13:28:34Z","2021-11-24T10:44:58Z"
"","3533","HDFS-16264.When adding block keys, the records come from the specific Block Pool.","### Description of PR When adding block keys, the records come from the specific block pool.  ### How was this patch tested? This is related to logs, and unit testing is less stressful.","closed","","jianghuazhu","2021-10-08T13:19:06Z","2021-10-11T11:50:29Z"
"","3496","HADOOP-17941. Update xerces to 2.12.1","### Description of PR Update xerces due to CVE-2012-0881  ### How was this patch tested? Manually","closed","","warrenzhu25","2021-09-28T18:01:31Z","2021-09-29T16:14:20Z"
"","3516","HADOOP-17951 AccessPoint verifyBucketExistsV2 always returns false","### Description of PR Turns out there was a small issue with the previous implementation that always returned `false` for `verifyBucketExistsV2`. The more ""clearer"" way to check is for an error message when there's a 403. If that error message is present then the AP doesn't exist (the only case when a 403 is returned for an AP), otherwise 404 means definite no.  ### How was this patch tested? Ran `mvn -Dparallel-tests -DtestsThreadCount=32 clean verify` on `eu-west-1` with access points enabled (i.e. bucket pointing to AP ARN).  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","bogthe","2021-10-04T11:19:05Z","2021-10-04T19:58:23Z"
"","3338","HADOOP-17870 Make Http Filesystem allow non-absolute uri based paths.","### Description of PR This PR makes the Http Filesystem consistent with the other filesystems in how the open behaves. It first qualifies the path before making a URI and subsequently URL out of it.  ### How was this patch tested?  Has modified the existing unit test to cover this case too.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","yellowflash","2021-08-26T08:09:38Z","2021-08-31T12:55:53Z"
"","3416","HDFS-16210. Addendum: Add usage for refreshCallQueue","### Description of PR This is an addendum PR for HDFS-16210 which added the usage for refreshCallQueue.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16210  ### How was this patch tested? test via CLI ![Usage refreshCallQueue](https://user-images.githubusercontent.com/14933944/132831228-bf1fbf42-68a2-4ad6-8c8f-86e2dae0f97c.png)   ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-09-10T09:18:18Z","2021-09-11T03:25:45Z"
"","3322","HDFS-6874. Add GETFILEBLOCKLOCATIONS operation to HttpFS.","### Description of PR This is a rebase of the patch file 11 attached to HDFS-6874.  The GETFILEBLOCKLOCATIONS is HCFS compatible. Add support of it to httpfs to makes it possible for more applications to run directly against HttpFS server.  Add GETFILEBLOCKLOCATIONS op support for httpfs server (HttpFSServer). Add the same for httpfs client (HttpFSFileSystem) Let webhdfs client (WebHdfsFileSystem ) tries the new GETFILEBLOCKLOCATIONS op if the server supports it. Otherwise, fall back to the old GET_FILE_BLOCK_LOCATIONS op. The selection is cached so the second invocation doesn't need to trial and error again.  ### How was this patch tested? Unit tests.","open","","jojochuang","2021-08-23T09:33:08Z","2021-09-02T01:17:06Z"
"","3435","HDFS-16228.[FGL]Improve safer PartitionedGSet#size.","### Description of PR The purpose of this jira is to make the operation PartitionedGSet#size safer.  Details: HDFS-16228","open","","jianghuazhu","2021-09-15T07:45:24Z","2021-09-30T02:18:40Z"
"","3463","HADOOP-17921. DistCp derives if XAttr is supported wrongly","### Description of PR The exception handling in distcp code during the check seems improper. In a FileSystem implementation which doesn't override getXAttrs(), it would throw UnsupportedOperationException(). However, in distCp while handling the code, it tries to catch Exception class. That would result in normal IOException also treated as if the feature is not supported. This will lead to wrong behavior.  ### How was this patch tested? :eyes:  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","pkumarsinha","2021-09-20T13:46:55Z","2021-10-01T20:31:43Z"
"","3429","HDFS-16227. De-flake TestMover#testMoverWithStripedFile","### Description of PR TestMover#testMoverWithStripedFile fails intermittently with stacktrace: ``` [ERROR] testMoverWithStripedFile(org.apache.hadoop.hdfs.server.mover.TestMover)  Time elapsed: 48.439 s  <<< FAILURE! java.lang.AssertionError: expected: but was: 	at org.junit.Assert.fail(Assert.java:89) 	at org.junit.Assert.failNotEquals(Assert.java:835) 	at org.junit.Assert.assertEquals(Assert.java:120) 	at org.junit.Assert.assertEquals(Assert.java:146) 	at org.apache.hadoop.hdfs.server.mover.TestMover.testMoverWithStripedFile(TestMover.java:965) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ``` Example of this flaky behaviour: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-3386/6/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt  ### How was this patch tested? Introduced wait until Namenode reports correct storage type for all blocks of given file as there might be some delay in the reporting.","closed","","virajjasani","2021-09-13T12:22:02Z","2021-09-18T11:03:13Z"
"","3386","HDFS-16213. Flaky test TestFsDatasetImpl#testDnRestartWithHardLink","### Description of PR TestFsDatasetImpl#testDnRestartWithHardLink is flapper: ``` [ERROR] testDnRestartWithHardLink(org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl)  Time elapsed: 7.768 s  <<< FAILURE! java.lang.AssertionError 	at org.junit.Assert.fail(Assert.java:87) 	at org.junit.Assert.assertTrue(Assert.java:42) 	at org.junit.Assert.assertTrue(Assert.java:53) 	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testDnRestartWithHardLink(TestFsDatasetImpl.java:1344) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299) 	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.lang.Thread.run(Thread.java:748) ```  ### How was this patch tested? Unit testing. The current flaky behaviour is easy to reproduce by running the test code twice as part of same test. The resolution is to disable the detection as well as deletion of duplicate finalized replica by BlockPoolSlice instance.  When Datanode comes up, BPServiceActors handshakes to Namenode and tries to initialize Block pool and in the process, it tries to get VolumeMap using BlockPoolSlice instance. While doing so, reading replicas from cache fails and hence, the thread tries to add Finalized and RBW replicas to addReplicaThreadPool fork-join pool in order to build the map. This process also tries to identify if there exists any duplicate replica. For this particular test, sometimes this process can detect duplicate replica on /data2 while processing finalized replica of /data1. Hence, before we can confirm newReplicaInfo.getBlockURI() exists, finalized replica on /data2 might get deleted (rare and flaky case). Although the probability for the thread processing the identification and deletion of duplicate finalized replica to be faster than main thread is less, it cannot be avoided. Hence, we disable adding Finalized and RBW replicas to addReplicaThreadPool in BlockPoolSlice here and re-enable it only after we confirm the existence of newReplicaInfo on ""/data2"" ARCHIVE storage.","closed","","virajjasani","2021-09-05T07:29:03Z","2021-09-21T01:12:09Z"
"","3686","HDFS-16336. De-flake TestRollingUpgrade#testRollback","### Description of PR Test TestRollingUpgrade#testRollback is flaky. It keeps failing intermittently.   ### How was this patch tested? The rolling upgrade related metrics are sometimes not null because of other tests, however having null metrics mean is prerequisite for this test, hence we need to introduce wait.   ### For code changes: * [x]  Does the title or this PR starts with the corresponding JIRA issue id (e.g. '[HADOOP-17799](https://issues.apache.org/jira/browse/HADOOP-17799). Your PR title ...')?","closed","","virajjasani","2021-11-19T18:12:32Z","2021-12-09T02:24:35Z"
"","3329","HDFS-16184. De-flake TestBlockScanner#testSkipRecentAccessFile","### Description of PR Test TestBlockScanner#testSkipRecentAccessFile is flaky: ``` [ERROR] testSkipRecentAccessFile(org.apache.hadoop.hdfs.server.datanode.TestBlockScanner)  Time elapsed: 3.936 s  <<< FAILURE! java.lang.AssertionError: Scan nothing for all files are accessed in last period. 	at org.junit.Assert.fail(Assert.java:89) 	at org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testSkipRecentAccessFile(TestBlockScanner.java:1015) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) ```  ## Reason for failure: Thread in VolumnScanner keeps scanning blocks. Before block scan, it executes `TestScanResultHandler#setup` and it keeps waiting on `info` object until main thread in `testSkipRecentAccessFile()` notifies `info` object and sets it's `shouldRun` to true for VolumnScanner thread to successfully return from `TestScanResultHandler#setup`. Now in main test, we try to assert that `info.blocksScanned` stays 0 which is only possible if VolumnScanner's block scanner thread does not reach `info.blocksScanned++` point, which cannot be guaranteed always and hence this test is flaky.  ## Fix: In order to fix this, the main thread in `testSkipRecentAccessFile()` should initialize `info.sem` as Semaphore (permitting only single thread to take a lock, Mutex) and take a lock by main thread so that until released, block scan thread cannot reach point `info.blocksScanned++` and hence the test never fails. Before the block scanner thread reach the point of incrementing blocksScanned count, it has to acquire Semaphore and it gets blocked here until main thread releases the lock.  ### How was this patch tested? Unit tests","closed","","virajjasani","2021-08-24T13:27:10Z","2021-08-30T07:32:26Z"
"","3321","HADOOP-17858. Avoid possible class loading deadlock with VerifierNone initialization","### Description of PR Superclass Verifier has a static initializer VERIFIER_NONE that initializes sub-class VerifierNone. This reference can result in deadlock during class loading as per https://docs.oracle.com/javase/specs/jls/se8/html/jls-12.html#jls-12.4.2.  As of today, only RpcProgram use this instance and hence it is safe but if more clients start using this (specifically static ones), it has potential to bring deadlock. We should break this referencing before it is late.  ### How was this patch tested? TestRPC* unit tests  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-08-23T08:27:43Z","2021-08-25T07:13:08Z"
"","3594","HDFS-16261 Configurable grace period around deletion of invalidated blocks","### Description of PR See https://issues.apache.org/jira/browse/HDFS-16261 for more details.  The most straightforward way to introduce a grace period for replaced blocks is in the InvalidateBlocks datastructure. Work is periodically pulled from this class to be divvied up to DataNodes. This class is also reported on with JMX metrics, so one can easily see how many blocks are currently pending deletion.  I achieved the grace period by adding a new `pollNWithFilter` method to `LightWeightHashSet`. InvalidateBlocks are added to the LightWeightHashSet with a calculated `readyForDeleteAt` time. When `getBlocksToInvalidateByLimit` is called, a filter is passed which only includes those blocks whose `readyForDeleteAt` is expired.  The default grace period for blocks added to InvalidateBlocks is 0, which effectively makes all blocks immediately ready for deletion per existing behavior. When a grace period is configured, it applies only to blocks deleted through the `delHintNode` passed in RECEIVED_BLOCK messages. This minimizes the impact of this feature on blocks deleted for other reasons (i.e. if a file is deleted or through other ongoing namenode auditing).  ### How was this patch tested?  I've added tests for the relevant pieces, and have also been running this on 2 clusters internally.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","bbeaudreault","2021-10-27T15:56:11Z","2021-10-27T23:33:46Z"
"","3347","YARN-10902. Release reserved container on blacklist node for an application","### Description of PR Resources on application blacklisted node with reserved container can not allocate to other applications. In this case, the reserved container should be cancelled.   ### How was this patch tested? A unit test `testReleaseReservedContainerOnBlacklistedNode()` is added into `TestCapacityScheduler`.  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","JackWangCS","2021-08-28T03:44:58Z","2021-08-28T06:49:06Z"
"","3569","YARN-10948. Rename SchedulerQueue#activeQueue to activateQueue","### Description of PR Rename SchedulerQueue#activeQueue to activateQueue  ### How was this patch tested? Unit tests  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","adamantal","2021-10-20T09:09:48Z","2021-10-23T15:22:30Z"
"","3451","HDFS-16229.Remove the use of obsolete BLOCK_DELETION_INCREMENT.","### Description of PR Remove the use of obsolete BLOCK_DELETION_INCREMENT.  Details: HDFS-16229  ### How was this patch tested? This jira is mainly to solve and comment-related, so there is no need for excessive testing.","closed","","jianghuazhu","2021-09-17T08:05:18Z","2021-09-18T05:37:09Z"
"","3382","YARN-10919. Remove LeafQueue#scheduler field","### Description of PR Remove LeafQueue#scheduler field, as it is the same object as AbstractCSQueue#csContext (from parent class).  ### How was this patch tested? Only remove duplicated filed, no tests added.  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","JackWangCS","2021-09-03T14:20:14Z","2021-09-08T14:19:29Z"
"","3361","YARN-10929. Refrain from creating new Configuration object in AbstractManagedParentQueue#initializeLeafQueueConfigs","### Description of PR Refrain from creating new Configuration object and remove the unneccessary configuration sorting in AbstractManagedParentQueue#initializeLeafQueueConfigs.   ### How was this patch tested? This patch just refactor code and should be covered by exists test.  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","JackWangCS","2021-08-31T16:10:51Z","2021-12-14T11:58:23Z"
"","3505","HADOOP-17947. Provide alternative to Guava VisibleForTesting","### Description of PR Provide alternative to Guava VisibleForTesting so that we can get rid of Guava dependency to represent testing scope for a class/method/field.","closed","","virajjasani","2021-10-01T06:44:31Z","2021-10-15T16:43:19Z"
"","3362","HDFS-16199. Resolve log placeholders in NamenodeBeanMetrics","### Description of PR NamenodeBeanMetrics has some missing placeholders in logs. This patch attempts to add them.","closed","","virajjasani","2021-08-31T16:52:13Z","2021-09-04T10:50:08Z"
"","3438","YARN-10951. Move async scheduling fields to a new AsyncSchedulingConfiguration","### Description of PR Move async scheduling logic to a new class  ### How was this patch tested?   ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","JackWangCS","2021-09-15T16:06:27Z","2021-12-14T22:12:03Z"
"","3404","HADOOP-17900. Move ClusterStorageCapacityExceededException to Public from LimitedPrivate.","### Description of PR Makes ClusterStorageCapacityExceededException Public, Wanted to get this used for a retry logic to fail fast in Hive, But the LimitedPrivate Annotation doesn't let anyone else use it. I don't feel it is something which can harm if made Public, So, atleast in Future it can be used.  ### How was this patch tested? 👀  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","ayushtkn","2021-09-08T10:48:18Z","2021-09-13T17:21:15Z"
"","3599","HADOOP-17982. OpensslCipher initialization error should log a WARN message.","### Description of PR log a WARN message if OpensslCipher fails to initialize.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2021-10-28T20:50:11Z","2021-12-20T10:30:34Z"
"","3658","HADOOP-17975 Fallback to simple auth does not work for a secondary DistributedFileSystem instance","### Description of PR In the JIRA there is a long description about the problem, the TLDR is that if there are two DFSClient instance at the same time on the client side, the second one can not properly fall back to SIMPLE auth due to how the SaslRpcClient handles the fallback via an AtomicBoolean created by the DfsClient, and set by the hadoop.ipc.Client.  The initial idea/solution was posted as #3579 where we had a long discussion with @symious about this solution and that solution, and the tradeoffs. After a while, I think I was convinced that this is a proper solution, and fits into the current system, so I am adding this PR to finally fix the issue.  The solution is to add the AtomicBoolean fallbackToSimpleAuth which controls how/if the SaslRpcClient falls back to simple auth or not, to the ConnectionId class, and with that distinguish the connections of the two DfsClient based on the fallbackToSimpleAuth AtomicBoolean instance. If that is different, from any other connections' AtomicBoolean instance, then we will distinguish the connection, and with that we will initialize the value of fallbackToSimpleAuth properly.  ### How was this patch tested? JUnit test added. The problem manifested itself as a failing HBase ExportSnapshot job, between two small clusters, one using Kerberos auth one using Simple auth. With this fix the ExportSnapshot job was able to run as well.","closed","","fapifta","2021-11-14T15:37:06Z","2021-11-22T12:39:45Z"
"","3602","HDFS-16291.Make the comment of INode#ReclaimContext more standardized.","### Description of PR In the INode#ReclaimContext class, there are some irregular comments. The purpose of this pr is to make them more standardized. Details: HDFS-16291  ### How was this patch tested? Here is just some work on the documentation, the test pressure is not great.","closed","","jianghuazhu","2021-10-31T13:18:53Z","2021-11-04T01:52:43Z"
"","3622","HDFS-16301.Improve BenchmarkThroughput#SIZE naming standardization.","### Description of PR In the BenchmarkThroughput#run() method, there is a local variable: SIZE. This variable is used in the local scope, and it may be more appropriate to change it to a lowercase name. This is the purpose of this PR. Details: HDFS-16301  ### How was this patch tested? This is just the change of the variable name, which is not very stressful for the test.","closed","","jianghuazhu","2021-11-05T15:12:05Z","2021-11-10T02:33:13Z"
"","3483","HDFS-16238.Improve comments related to EncryptionZoneManager.","### Description of PR In EncryptionZoneManager, there are some missing The description of the relevant comment. The purpose of this jira is to perfect them.  ### How was this patch tested? This PR is more related to the document, and for the corresponding test, it does not require too much pressure.","closed","","jianghuazhu","2021-09-26T08:34:34Z","2021-10-01T02:30:42Z"
"","3503","HADOOP-17952. Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-common-project modules","### Description of PR In an attempt to reduce the dependency on Guava, we should remove VisibleForTesting annotation usages as it has very high usage in our codebase. This PR is to provide Hadoop's own alternative and use it in hadoop-common-project modules. hadoop-auth doesn't have dependency on hadoop-common, hence we attempt to replace VFT annotation with simple comments.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-09-30T09:59:17Z","2021-10-10T17:44:54Z"
"","3661","HDFS-16324. Fix error log in BlockManagerSafeMode","### Description of PR if `recheckInterval` was set as invalid value, there will be warning log output, but the message seems not that proper ,we can improve it.  ### How was this patch tested? No need test cases, just update warning log.","closed","","GuoPhilipse","2021-11-15T13:05:44Z","2021-12-08T12:22:06Z"
"","3317","HDFS-16181. [SBN Read] Fix metric of RpcRequestCacheMissAmount can't display when tailEditLog form JN","### Description of PR I found the JN turn on edit cache, but the metric of rpcRequestCacheMissAmount can not display.  JIRA: https://issues.apache.org/jira/browse/HDFS-16181","closed","","wzhallright","2021-08-20T10:51:53Z","2021-09-15T17:11:54Z"
"","3494","HDFS-16242. JournalMetrics should add JournalId MetricTag.","### Description of PR https://issues.apache.org/jira/browse/HDFS-16242 JournalMetrics should add JournalId MetricTag to distinguish different nameservice journal metrics  when JournalNode use ganglia sink or  prometheus sink.  JMX  Before this patch, journal' jmx : ``` // jmx json     {     ""name"" : ""Hadoop:service=JournalNode,name=Journal-nntest1"",     ""Syncs60sNumOps"" : 0,     ...     },     {    ""name"" : ""Hadoop:service=JournalNode,name=Journal-nntest2"",    ""Syncs60sNumOps"" : 0,    ...     }  ```  After this patch, journal' jmx :    ``` // jmx json     {     ""name"" : ""Hadoop:service=JournalNode,name=Journal-nntest1"",     ""tag.JournalId"" : ""nntest1"",  // add this tag     ""Syncs60sNumOps"" : 0,    ...     },     {    ""name"" : ""Hadoop:service=JournalNode,name=Journal-nntest2"",      ""tag.JournalId"" : ""nntest2"",    ""Syncs60sNumOps"" : 0,   ...      }  ```  PrometheusSink  Before this patch, journal' prometheus export : ``` journal_node_syncs60s_num_ops{context=""dfs"",hostname=""host""} 2 ``` After this patch, journal' prometheus export : ``` journal_node_syncs60s_num_ops{context=""dfs"",journalid=""nntest2"",hostname=""host""} 2 journal_node_syncs60s_num_ops{context=""dfs"",journalid=""nntest1"",hostname=""host""} 75 ``` ### How was this patch tested? add test testJournalMetricTags  ### For code changes: add JournalMetrics$getJournalId","closed","","Neilxzn","2021-09-28T10:33:18Z","2021-10-01T07:36:46Z"
"","3472","HDFS-16235. Deadlock in LeaseRenewer for static remove method","### Description of PR Have Deadlock when backend LeaseRenewer thread call `LeaseRenewer.Factory.remove` and  task thread call `LeaseRenewer.remove` ![image](https://user-images.githubusercontent.com/46485123/134507597-739a0459-7cdf-4ea2-9ac4-86e977ec7d44.png)   ### How was this patch tested?   ### For code changes:","closed","","AngersZhuuuu","2021-09-23T12:36:37Z","2021-09-25T10:18:16Z"
"","3352","YARN-10903. Fix the headroom check in ParentQueue and RegularContainerAllocator for DRF","### Description of PR Fix the headroom check in ParentQueue and RegularContainerAllocator for DRF.  The headroom check in  `ParentQueue.canAssign` and `RegularContainerAllocator#checkHeadroom` does not consider the DRF cases. This will cause a lot of ""Failed to accept allocation proposal"" when a queue is near-fully used.  ### How was this patch tested? Add a unit test `TestLeafQueue.testHeadroomCheckWithDRF()` to test the patch  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","JackWangCS","2021-08-28T15:47:32Z","2021-09-13T02:54:11Z"
"","3343","HADOOP-17874. ExceptionsHandler to add terse/suppressed Exceptions in thread-safe manner","### Description of PR Even though we have explicit comments stating that we have thread-safe replacement of terseExceptions and suppressedExceptions, in reality we don't have it. As we can't guarantee only non-concurrent addition of Exceptions at a time from any Server implementation, we should make this thread-safe.  ### How was this patch tested? Local dev testing with running unit test to add Exceptions concurrently. Verified read-after-write consistency.","closed","","virajjasani","2021-08-26T19:51:39Z","2021-09-03T01:25:34Z"
"","3659","HDFS-16323. DatanodeHttpServer doesn't require handler state map while retrieving filter handlers","### Description of PR DatanodeHttpServer#getFilterHandlers use handler state map just to query if the given datanode httpserver filter handler class exists in the map and if not, initialize the Channel handler by invoking specific parameterized constructor of the class. However, this handler state map is never used to upsert any data.   ### How was this patch tested? Local testing with mini cluster.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-11-14T18:46:46Z","2021-11-17T13:17:22Z"
"","3556","HDFS-16275. Enable considerLoad for localWrite","### Description of PR Currently when client is on the same machine of a datanode, it will try to write to the local machine regardless of the load of the datanode, that is the xceiverCount.  In our production cluster, datanode and Nodemanager are running on the same server, so when there are heavy jobs running on a labeled queue, the corresponding datanodes will have higher xceiverCounts than other datanodes. When other clients are trying to write, the exception of ""could only be replicated to 0 nodes"" would be thrown.  This ticket is to enable considerLoad to avoid the hot local write.  Jira Ticket:  https://issues.apache.org/jira/browse/HDFS-16275  ### How was this patch tested? unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","symious","2021-10-15T08:52:33Z","2021-10-15T16:04:58Z"
"","3651","HDFS-16314. Support to make dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled reconfigurable","### Description of PR Consider that make dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled reconfigurable and rapid rollback in case this feature HDFS-16076 unexpected things happen in production environment  Details: [HDFS-16314](https://issues.apache.org/jira/browse/HDFS-16314)","closed","","haiyang1987","2021-11-12T10:14:56Z","2021-11-16T11:32:24Z"
"","3454","YARN-10950. Code cleanup in QueueCapacities","### Description of PR Code cleanup. See jira  ### How was this patch tested? Unit tests  ### For code changes: - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","adamantal","2021-09-17T14:21:57Z","2021-09-19T12:42:02Z"
"","3468","HDFS-16233. Do not use exception handler to implement copy-on-write for EnumCounters.","### Description of PR Async profiler finds the COW of EnumCounters is expensive. Propose a new implementation to reduce the cost.  ### How was this patch tested? Performance optimization. No functional change so use existing unit tests.  ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","jojochuang","2021-09-22T06:10:51Z","2021-09-24T16:05:52Z"
"","3605","HDFS-16294.Remove invalid DataNode#CONFIG_PROPERTY_SIMULATED.","### Description of PR As early as when HDFS-2907 was resolved, SimulatedFSDataset#CONFIG_PROPERTY_SIMULATED was removed. Replaced by: SimulatedFSDataset#Factory and DFSConfigKyes#DFS_DATANODE_FSDATASET_FACTORY_KEY. However, the introduction to CONFIG_PROPERTY_SIMULATED is still retained in the DataNode. Details: HDFS-16294  ### How was this patch tested? This pr mainly solves the changes related to annotations, and the test pressure is not great.","closed","","jianghuazhu","2021-11-02T08:15:07Z","2021-11-04T08:50:02Z"
"","3529","HADOOP-17956. Replace all default Charset usage with UTF-8","### Description of PR As discussed on PR #3515, creating this sub-task to replace all default charset with UTF-8 as default charset has some potential problems (e.g. HADOOP-11379, HADOOP-11389).","closed","","virajjasani","2021-10-07T13:00:09Z","2021-10-14T04:07:39Z"
"","3584","HADOOP-17968 Migrate checkstyle module illegalimport to maven enforcer banned-illegal-imports","### Description of PR As discussed on PR #3503, we should migrate existing imports provided in IllegalImport tag in checkstyle.xml to maven-enforcer-plugin's banned-illegal-imports enforcer rule so that build never succeeds in the presence of any of the illegal imports.   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","virajjasani","2021-10-25T09:24:31Z","2021-10-28T06:58:03Z"
"","3345","HADOOP-17878: add config for aliyun oss cname support","### Description of PR As described in jira [HDFS-16189](https://issues.apache.org/jira/browse/HDFS-16189), cname support need to be closed if we don't use cname","open","","jackylee-ch","2021-08-27T09:05:05Z","2021-10-01T09:03:43Z"
"","3387","HADOOP-17892. Add Hadoop code formatter in dev-support","### Description of PR Adding Hadoop code formatter in dev-support. Once done, we should also update our Wiki to refer to this link directly.","closed","","virajjasani","2021-09-05T08:03:19Z","2021-09-23T06:27:04Z"
"","3360","YARN-10909.  AbstractCSQueue: Annotate all methods with VisibleForTesting that are only used by test code","### Description of PR AbstractCSQueue: some methods added for test code but not annotated with VisibleForTesting.  ### How was this patch tested? Just code style fix, no new tests needed  ### For code changes:  - [Yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [No] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [No] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [No] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","JackWangCS","2021-08-31T15:09:39Z","2021-10-23T12:49:16Z"
"","3481","HADOOP-17910. [JDK 17] TestNetUtils fails","### Description of PR ``` [INFO] Running org.apache.hadoop.net.TestNetUtils [ERROR] Tests run: 48, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.469 s <<< FAILURE! - in org.apache.hadoop.net.TestNetUtils [ERROR] testInvalidAddress(org.apache.hadoop.net.TestNetUtils)  Time elapsed: 0.386 s  <<< FAILURE! java.lang.AssertionError:   Expected to find 'invalid-test-host:0' but got unexpected exception: java.net.UnknownHostException: invalid-test-host/:0 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:592) 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:551) 	at org.apache.hadoop.net.TestNetUtils.testInvalidAddress(TestNetUtils.java:109) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.base/java.lang.reflect.Method.invoke(Method.java:568) 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) 	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) 	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) 	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) 	at org.junit.runners.ParentRunner.run(ParentRunner.java:413) 	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) 	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) 	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) 	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384) 	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345) 	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126) 	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418) ```  ### How was this patch tested? Tested with the patch locally. Works for both JDK 8 and JDK 17.  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","virajjasani","2021-09-24T19:43:46Z","2021-09-27T01:12:56Z"
"","3646","YARN-11003. Make RMNode aware of all (OContainer inclusive) allocated resources","### Description of PR * Adds a new method in RMNode: getAllocatedContainerResource, which records how much resource is allocated to containers on a node * Adds up RUNNING and NEW container resources in HB loop to keep track of resources allocated for containers  ### How was this patch tested? * Unit test * Deployment in a production cluster","closed","","afchung","2021-11-11T19:24:14Z","2021-11-23T21:20:08Z"
"","3694","HDFS-16343. Add some debug logs when the dfsUsed are not used during Datanode startup.","### Description of PR   This PR adds debug logging around the datanode startup when the cached dfsUsed is not used during datanode startup.  ### How was this patch tested?    This is a simple logging patch, tested using existing tests.  ### For code changes:    https://issues.apache.org/jira/browse/HDFS-16343","closed","","mukul1987","2021-11-21T01:42:40Z","2021-11-23T14:39:44Z"
"","3598","HDFS-16259. Catch and re-throw sub-classes of AccessControlException thrown by any permission provider plugins (eg Ranger)","### Description of PR  When a permission provider plugin is enabled (eg Ranger) there are some scenarios where it can throw a sub-class of an AccessControlException (eg RangerAccessControlException). If this exception is allowed to propagate up the stack, it can give problems in the HDFS Client, when it unwraps the remote exception containing the AccessControlException sub-class.  Ideally, we should make AccessControlException final so it cannot be sub-classed, but that would be a breaking change at this point. Therefore I believe the safest thing to do, is to catch any AccessControlException that comes out of the permission enforcer plugin, and re-throw an AccessControlException instead.  ### How was this patch tested?  New Unit test","closed","","sodonnel","2021-10-28T17:38:24Z","2021-11-02T11:14:48Z"
"","3379","HDFS-16210. RBF: Add the option of refreshCallQueue to RouterAdmin","### Description of PR  We enabled FairCallQueue to RouterRpcServer, but Router can not refreshCallQueue as NameNode does.  This ticket is to enable the refreshCallQueue for Router so that we don't have to restart the Routers when updating FairCallQueue configurations.    The option is not to refreshCallQueue to NameNodes, just trying to refresh the callQueue of Router itself.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16210  ### How was this patch tested?  Unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-09-03T09:15:28Z","2021-09-10T09:14:39Z"
"","3384","HADOOP-17679. Upgrade Protobuf to 3.17.3","### Description of PR  Updates Protobuf dependency to the latest version  This PR depends on https://github.com/apache/hadoop-thirdparty/pull/13  ### How was this patch tested?  Covered by existing tests  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","open","","medb","2021-09-04T07:12:25Z","2021-09-22T20:55:49Z"
"","3487","MAPREDUCE-7363. Rename JobClientUnitTest to TestJobClients","### Description of PR  This PR aims to rename a test class from `JobClientUnitTest` to `TestJobClients` for consistency. Although this is a minor PR on test case, it's helpful for the downstream to manage the builds and tests.  ### How was this patch tested?  Pass the CI.  ### For code changes:  - [v] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","dongjoon-hyun","2021-09-26T23:44:09Z","2021-09-28T17:27:10Z"
"","3337","HADOOP-17869. `fs.s3a.connection.maximum` should be bigger than `fs.s3a.threads.max`","### Description of PR  This PR aims to increase `fs.s3a.connection.maximum` to be bigger than `fs.s3a.threads.max`.  ### How was this patch tested?  HADOOP-15183 defined this variable like the following. - https://github.com/apache/hadoop/commit/e02eb24e0a9139418120027b694492e0738df20a#diff-268b9968a4db21ac6eeb7bcaef10e4db744d00ba53989fc7251bb3e8d9eac7dfR1216-R1221 > Controls the maximum number of simultaneous connections to S3. This must be bigger than the value of fs.s3a.threads.max so as to stop threads being blocked waiting for new HTTPS connections.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","dongjoon-hyun","2021-08-26T08:02:37Z","2021-08-30T19:32:46Z"
"","3596","HDFS-16287. Support to make dfs.namenode.avoid.read.slow.datanode reconfigurable","### Description of PR  Support to make dfs.namenode.avoid.read.slow.datanode  reconfigurable Details: HDFS-16287  ### For code changes:  - [ ] Consider that make dfs.namenode.avoid.read.slow.datanode reconfigurable and rapid rollback in case this feature HDFS-16076  unexpected things happen in production environment - [ ] DatanodeManager#startSlowPeerCollector  by parameter 'dfs.datanode.peer.stats.enabled' to control","closed","","haiyang1987","2021-10-28T09:07:34Z","2021-11-12T02:55:23Z"
"","3486","HADOOP-17939. Support building on `Apple Silicon`","### Description of PR  Since there is no `protoc` for `Apple Silicon` until 3.17.3, Apache Hadoop compilation fails to download it. - https://repo1.maven.org/maven2/com/google/protobuf/protoc/3.17.3/  This PR aims to support building on `Apple Silicon` by fallback to Intel x86_64 architecture in order to take advantage of MacOS Rosetta2.  ### How was this patch tested?  Test on Apple M1 MacMini.  **BEFORE** ``` $ mvn compile ... [INFO] --- protobuf-maven-plugin:0.5.1:compile (src-compile-protoc) @ hadoop-common --- [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary for Apache Hadoop Main 3.4.0-SNAPSHOT: [INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [  0.160 s] [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  0.517 s] [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.292 s] [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.392 s] [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.029 s] [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.028 s] [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.381 s] [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.224 s] [INFO] Apache Hadoop Auth ................................. SUCCESS [  1.018 s] [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.143 s] [INFO] Apache Hadoop Common ............................... FAILURE [  0.183 s] [INFO] Apache Hadoop NFS .................................. SKIPPED [INFO] Apache Hadoop KMS .................................. SKIPPED [INFO] Apache Hadoop Registry ............................. SKIPPED [INFO] Apache Hadoop Common Project ....................... SKIPPED [INFO] Apache Hadoop HDFS Client .......................... SKIPPED [INFO] Apache Hadoop HDFS ................................. SKIPPED [INFO] Apache Hadoop HDFS Native Client ................... SKIPPED [INFO] Apache Hadoop HttpFS ............................... SKIPPED [INFO] Apache Hadoop HDFS-NFS ............................. SKIPPED [INFO] Apache Hadoop YARN ................................. SKIPPED [INFO] Apache Hadoop YARN API ............................. SKIPPED [INFO] Apache Hadoop YARN Common .......................... SKIPPED [INFO] Apache Hadoop YARN Server .......................... SKIPPED [INFO] Apache Hadoop YARN Server Common ................... SKIPPED [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SKIPPED [INFO] Apache Hadoop YARN Timeline Service ................ SKIPPED [INFO] Apache Hadoop YARN Web Proxy ....................... SKIPPED [INFO] Apache Hadoop YARN ResourceManager ................. SKIPPED [INFO] Apache Hadoop YARN NodeManager ..................... SKIPPED [INFO] Apache Hadoop YARN Server Tests .................... SKIPPED [INFO] Apache Hadoop YARN Client .......................... SKIPPED [INFO] Apache Hadoop MapReduce Client ..................... SKIPPED [INFO] Apache Hadoop MapReduce Core ....................... SKIPPED [INFO] Apache Hadoop MapReduce Common ..................... SKIPPED [INFO] Apache Hadoop MapReduce Shuffle .................... SKIPPED [INFO] Apache Hadoop MapReduce App ........................ SKIPPED [INFO] Apache Hadoop MapReduce HistoryServer .............. SKIPPED [INFO] Apache Hadoop MapReduce JobClient .................. SKIPPED [INFO] Apache Hadoop Distributed Copy ..................... SKIPPED [INFO] Apache Hadoop Mini-Cluster ......................... SKIPPED [INFO] Apache Hadoop Federation Balance ................... SKIPPED [INFO] Apache Hadoop HDFS-RBF ............................. SKIPPED [INFO] Apache Hadoop HDFS Project ......................... SKIPPED [INFO] Apache Hadoop YARN SharedCacheManager .............. SKIPPED [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase Common .... SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase Client .... SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SKIPPED [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SKIPPED [INFO] Apache Hadoop YARN Router .......................... SKIPPED [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SKIPPED [INFO] Apache Hadoop YARN Applications .................... SKIPPED [INFO] Apache Hadoop YARN DistributedShell ................ SKIPPED [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SKIPPED [INFO] Apache Hadoop YARN Services ........................ SKIPPED [INFO] Apache Hadoop YARN Services Core ................... SKIPPED [INFO] Apache Hadoop YARN Services API .................... SKIPPED [INFO] Apache Hadoop YARN Application Catalog ............. SKIPPED [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SKIPPED [INFO] Apache Hadoop YARN Application Catalog Docker Image  SKIPPED [INFO] Apache Hadoop YARN Application MaWo ................ SKIPPED [INFO] Apache Hadoop YARN Application MaWo Core ........... SKIPPED [INFO] Apache Hadoop YARN Site ............................ SKIPPED [INFO] Apache Hadoop YARN Registry ........................ SKIPPED [INFO] Apache Hadoop YARN UI .............................. SKIPPED [INFO] Apache Hadoop YARN CSI ............................. SKIPPED [INFO] Apache Hadoop YARN Project ......................... SKIPPED [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SKIPPED [INFO] Apache Hadoop MapReduce NativeTask ................. SKIPPED [INFO] Apache Hadoop MapReduce Uploader ................... SKIPPED [INFO] Apache Hadoop MapReduce Examples ................... SKIPPED [INFO] Apache Hadoop MapReduce ............................ SKIPPED [INFO] Apache Hadoop MapReduce Streaming .................. SKIPPED [INFO] Apache Hadoop Client Aggregator .................... SKIPPED [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SKIPPED [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SKIPPED [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SKIPPED [INFO] Apache Hadoop Dynamometer Dist ..................... SKIPPED [INFO] Apache Hadoop Dynamometer .......................... SKIPPED [INFO] Apache Hadoop Archives ............................. SKIPPED [INFO] Apache Hadoop Archive Logs ......................... SKIPPED [INFO] Apache Hadoop Rumen ................................ SKIPPED [INFO] Apache Hadoop Gridmix .............................. SKIPPED [INFO] Apache Hadoop Data Join ............................ SKIPPED [INFO] Apache Hadoop Extras ............................... SKIPPED [INFO] Apache Hadoop Pipes ................................ SKIPPED [INFO] Apache Hadoop OpenStack support .................... SKIPPED [INFO] Apache Hadoop Amazon Web Services support .......... SKIPPED [INFO] Apache Hadoop Kafka Library support ................ SKIPPED [INFO] Apache Hadoop Azure support ........................ SKIPPED [INFO] Apache Hadoop Aliyun OSS support ................... SKIPPED [INFO] Apache Hadoop Scheduler Load Simulator ............. SKIPPED [INFO] Apache Hadoop Resource Estimator Service ........... SKIPPED [INFO] Apache Hadoop Azure Data Lake support .............. SKIPPED [INFO] Apache Hadoop Image Generation Tool ................ SKIPPED [INFO] Apache Hadoop Tools Dist ........................... SKIPPED [INFO] Apache Hadoop Tools ................................ SKIPPED [INFO] Apache Hadoop Client API ........................... SKIPPED [INFO] Apache Hadoop Client Runtime ....................... SKIPPED [INFO] Apache Hadoop Client Packaging Invariants .......... SKIPPED [INFO] Apache Hadoop Client Test Minicluster .............. SKIPPED [INFO] Apache Hadoop Client Packaging Invariants for Test . SKIPPED [INFO] Apache Hadoop Client Packaging Integration Tests ... SKIPPED [INFO] Apache Hadoop Distribution ......................... SKIPPED [INFO] Apache Hadoop Client Modules ....................... SKIPPED [INFO] Apache Hadoop Tencent COS Support .................. SKIPPED [INFO] Apache Hadoop OBS support .......................... SKIPPED [INFO] Apache Hadoop Cloud Storage ........................ SKIPPED [INFO] Apache Hadoop Cloud Storage Project ................ SKIPPED [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time:  5.222 s [INFO] Finished at: 2021-09-26T12:43:12-07:00 [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal org.xolstice.maven.plugins:protobuf-maven-plugin:0.5.1:compile (src-compile-protoc) on project hadoop-common: Missing: [ERROR] ---------- [ERROR] 1) com.google.protobuf:protoc:exe:osx-aarch_64:3.7.1 [ERROR] [ERROR]   Try downloading the file manually from the project website. [ERROR] [ERROR]   Then, install it using the command: [ERROR]       mvn install:install-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.7.1 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file [ERROR] [ERROR]   Alternatively, if you host your own repository you can deploy the file there: [ERROR]       mvn deploy:deploy-file -DgroupId=com.google.protobuf -DartifactId=protoc -Dversion=3.7.1 -Dclassifier=osx-aarch_64 -Dpackaging=exe -Dfile=/path/to/file -Durl=[url] -DrepositoryId=[id] [ERROR] [ERROR]   Path to dependency: [ERROR]   	1) org.apache.hadoop:hadoop-common:jar:3.4.0-SNAPSHOT [ERROR]   	2) com.google.protobuf:protoc:exe:osx-aarch_64:3.7.1 [ERROR] [ERROR] ---------- [ERROR] 1 required artifact is missing. [ERROR] [ERROR] for artifact: [ERROR]   org.apache.hadoop:hadoop-common:jar:3.4.0-SNAPSHOT [ERROR] [ERROR] from the specified remote repositories: [ERROR]   apache.snapshots.https (https://repository.apache.org/content/repositories/snapshots, releases=true, snapshots=true), [ERROR]   repository.jboss.org (https://repository.jboss.org/nexus/content/groups/public/, releases=true, snapshots=false), [ERROR]   central (https://repo.maven.apache.org/maven2, releases=true, snapshots=false) [ERROR] [ERROR] -> [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR]   mvn  -rf :hadoop-common ```  **AFTER** ``` $ $ mvn package -DskipTests ... [INFO] Reactor Summary for Apache Hadoop Main 3.4.0-SNAPSHOT: [INFO] [INFO] Apache Hadoop Main ................................. SUCCESS [  0.524 s] [INFO] Apache Hadoop Build Tools .......................... SUCCESS [  0.500 s] [INFO] Apache Hadoop Project POM .......................... SUCCESS [  0.383 s] [INFO] Apache Hadoop Annotations .......................... SUCCESS [  0.587 s] [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [  0.027 s] [INFO] Apache Hadoop Assemblies ........................... SUCCESS [  0.043 s] [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [  1.258 s] [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [  0.243 s] [INFO] Apache Hadoop Auth ................................. SUCCESS [  1.546 s] [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [  0.430 s] [INFO] Apache Hadoop Common ............................... SUCCESS [ 11.563 s] [INFO] Apache Hadoop NFS .................................. SUCCESS [  0.612 s] [INFO] Apache Hadoop KMS .................................. SUCCESS [  0.691 s] [INFO] Apache Hadoop Registry ............................. SUCCESS [  0.743 s] [INFO] Apache Hadoop Common Project ....................... SUCCESS [  0.017 s] [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [  6.956 s] [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 11.962 s] [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [  0.173 s] [INFO] Apache Hadoop HttpFS ............................... SUCCESS [  0.828 s] [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [  0.366 s] [INFO] Apache Hadoop YARN ................................. SUCCESS [  0.019 s] [INFO] Apache Hadoop YARN API ............................. SUCCESS [  4.323 s] [INFO] Apache Hadoop YARN Common .......................... SUCCESS [  3.226 s] [INFO] Apache Hadoop YARN Server .......................... SUCCESS [  0.014 s] [INFO] Apache Hadoop YARN Server Common ................... SUCCESS [  2.383 s] [INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [  1.230 s] [INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [  0.557 s] [INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [  0.462 s] [INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [  5.786 s] [INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [  3.174 s] [INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [  1.038 s] [INFO] Apache Hadoop YARN Client .......................... SUCCESS [  1.509 s] [INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [  0.122 s] [INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [  1.932 s] [INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [  1.555 s] [INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [  1.115 s] [INFO] Apache Hadoop MapReduce App ........................ SUCCESS [  1.395 s] [INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [  0.832 s] [INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [  1.643 s] [INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [  1.016 s] [INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [  0.389 s] [INFO] Apache Hadoop Federation Balance ................... SUCCESS [  0.441 s] [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [  2.282 s] [INFO] Apache Hadoop HDFS Project ......................... SUCCESS [  0.013 s] [INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [  0.436 s] [INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [  0.484 s] [INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [  0.013 s] [INFO] Apache Hadoop YARN TimelineService HBase Common .... SUCCESS [  0.579 s] [INFO] Apache Hadoop YARN TimelineService HBase Client .... SUCCESS [  0.846 s] [INFO] Apache Hadoop YARN TimelineService HBase Servers ... SUCCESS [  0.015 s] [INFO] Apache Hadoop YARN TimelineService HBase Server 1.2  SUCCESS [  0.954 s] [INFO] Apache Hadoop YARN TimelineService HBase tests ..... SUCCESS [  1.058 s] [INFO] Apache Hadoop YARN Router .......................... SUCCESS [  0.668 s] [INFO] Apache Hadoop YARN TimelineService DocumentStore ... SUCCESS [  0.387 s] [INFO] Apache Hadoop YARN Applications .................... SUCCESS [  0.013 s] [INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [  0.490 s] [INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [  0.359 s] [INFO] Apache Hadoop YARN Services ........................ SUCCESS [  0.015 s] [INFO] Apache Hadoop YARN Services Core ................... SUCCESS [  1.580 s] [INFO] Apache Hadoop YARN Services API .................... SUCCESS [  0.524 s] [INFO] Apache Hadoop YARN Application Catalog ............. SUCCESS [  0.013 s] [INFO] Apache Hadoop YARN Application Catalog Webapp ...... SUCCESS [ 18.729 s] [INFO] Apache Hadoop YARN Application Catalog Docker Image  SUCCESS [  0.024 s] [INFO] Apache Hadoop YARN Application MaWo ................ SUCCESS [  0.016 s] [INFO] Apache Hadoop YARN Application MaWo Core ........... SUCCESS [  0.401 s] [INFO] Apache Hadoop YARN Site ............................ SUCCESS [  0.016 s] [INFO] Apache Hadoop YARN Registry ........................ SUCCESS [  0.205 s] [INFO] Apache Hadoop YARN UI .............................. SUCCESS [  0.014 s] [INFO] Apache Hadoop YARN CSI ............................. SUCCESS [  2.723 s] [INFO] Apache Hadoop YARN Project ......................... SUCCESS [  0.405 s] [INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [  0.311 s] [INFO] Apache Hadoop MapReduce NativeTask ................. SUCCESS [  0.459 s] [INFO] Apache Hadoop MapReduce Uploader ................... SUCCESS [  0.351 s] [INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [  0.494 s] [INFO] Apache Hadoop MapReduce ............................ SUCCESS [  0.118 s] [INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [  0.534 s] [INFO] Apache Hadoop Client Aggregator .................... SUCCESS [  0.348 s] [INFO] Apache Hadoop Dynamometer Workload Simulator ....... SUCCESS [  0.561 s] [INFO] Apache Hadoop Dynamometer Cluster Simulator ........ SUCCESS [  0.563 s] [INFO] Apache Hadoop Dynamometer Block Listing Generator .. SUCCESS [  0.393 s] [INFO] Apache Hadoop Dynamometer Dist ..................... SUCCESS [  0.341 s] [INFO] Apache Hadoop Dynamometer .......................... SUCCESS [  0.012 s] [INFO] Apache Hadoop Archives ............................. SUCCESS [  0.270 s] [INFO] Apache Hadoop Archive Logs ......................... SUCCESS [  0.321 s] [INFO] Apache Hadoop Rumen ................................ SUCCESS [  0.549 s] [INFO] Apache Hadoop Gridmix .............................. SUCCESS [  0.531 s] [INFO] Apache Hadoop Data Join ............................ SUCCESS [  0.320 s] [INFO] Apache Hadoop Extras ............................... SUCCESS [  0.351 s] [INFO] Apache Hadoop Pipes ................................ SUCCESS [  0.013 s] [INFO] Apache Hadoop OpenStack support .................... SUCCESS [  0.317 s] [INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [  2.749 s] [INFO] Apache Hadoop Kafka Library support ................ SUCCESS [  0.294 s] [INFO] Apache Hadoop Azure support ........................ SUCCESS [  1.280 s] [INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [  0.339 s] [INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [  0.746 s] [INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [  0.421 s] [INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [  0.261 s] [INFO] Apache Hadoop Image Generation Tool ................ SUCCESS [  0.356 s] [INFO] Apache Hadoop Tools Dist ........................... SUCCESS [  0.301 s] [INFO] Apache Hadoop Tools ................................ SUCCESS [  0.012 s] [INFO] Apache Hadoop Client API ........................... SUCCESS [01:00 min] [INFO] Apache Hadoop Client Runtime ....................... SUCCESS [01:08 min] [INFO] Apache Hadoop Client Packaging Invariants .......... SUCCESS [  0.100 s] [INFO] Apache Hadoop Client Test Minicluster .............. SUCCESS [01:52 min] [INFO] Apache Hadoop Client Packaging Invariants for Test . SUCCESS [  0.072 s] [INFO] Apache Hadoop Client Packaging Integration Tests ... SUCCESS [  0.025 s] [INFO] Apache Hadoop Distribution ......................... SUCCESS [  0.114 s] [INFO] Apache Hadoop Client Modules ....................... SUCCESS [  0.012 s] [INFO] Apache Hadoop Tencent COS Support .................. SUCCESS [  0.292 s] [INFO] Apache Hadoop OBS support .......................... SUCCESS [  0.499 s] [INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [  0.219 s] [INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [  0.011 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time:  06:04 min [INFO] Finished at: 2021-09-26T12:56:05-07:00 [INFO] ------------------------------------------------------------------------ ```","closed","","dongjoon-hyun","2021-09-26T19:40:49Z","2021-09-27T16:00:37Z"
"","3566","HADOOP-17970. unguava: remove Preconditions from hdfs-projects modules","### Description of PR  Replace `guava.Preconditions` in folder `hadoop-hdfs-project` with `hadoop.util.Preconditions`  ### How was this patch tested?  Ran the tests locally  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17970. Your PR title ...')? - [X] There were 4 positions calling `checkPositionIndex`. Since there were only 4 occurrences I preferred to inline them in the code. The tradeoff is a slight change of behavior as guava.Preconditions.checkPositionIndex may throw `IllegalArgumentException`, while the inlined version only throws `IndexOutOfBoundsException`","closed","","amahussein","2021-10-19T17:56:31Z","2022-07-10T09:59:27Z"
"","3527","HDFS-16262 Async refresh of cached locations in DFSInputStream","### Description of PR  Refactor refreshing of cached block locations so that it happens as part of an async process. This new implementation only refreshes DFSInputStreams whose deadNodes list is non-empty or contains non-local blocks.   Many DFSInputStreams can be very short lived. In order to reduce unnecessary contention and work fetching locations for a stream that doesn't need it, we only register the DFSInputStream for the async refresh on the next read after the first interval. In my testing in an HBase cluster, this drastically cuts down on the amount of registering and deregistering of streams.  One can disable automatic registration altogether if they'd prefer to handle that manually. One can manually register or de-register a stream using DFSClient.addLocatedBlocksRefresh and removeLocatedBlocksRefresh.  See https://issues.apache.org/jira/browse/HDFS-16262  ### How was this patch tested?  I added a new test class TestLocatedBlocksRefresher. This has also been running on a few of our internal test clusters.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","bbeaudreault","2021-10-06T22:37:38Z","2022-01-25T13:44:00Z"
"","3477","HADOOP-17934 ABFS: Make sure the AbfsHttpOperation is non-null before using it","### Description of PR  It's possible that, after retries, we will have a null AbfsHttpOperation inside of the AbfsRestOperation. AbfsClient should be aware of this and make sure that it doesn't just assume that there's a non-null HttpOperation (e.g. try to get the HTTP status code).  ### How was this patch tested?  Just existing unit tests at this point. Haven't looked to see if I can mock a realistic new test case. Have not tried to run ABFS integration tests yet.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [x] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [x] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","joshelser","2021-09-23T20:51:14Z","2021-09-30T14:27:01Z"
"","3421","HDFS-16221. RBF: Add usage of refreshCallQueue for Router","### Description of PR  In HDFS-16210, the feature of refreshCallQueue is added to RouterAdmin, but the usageInfo is not added. So when user is typing ""hdfs dfsrouteraadmin"", the option of ""refreshCallQueue"" will not be shown.  This ticket is to add the usage information for refreshCallQueue for Router.  Jira ticket: https://issues.apache.org/jira/browse/HDFS-16221   ### How was this patch tested?  unit test  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-09-11T03:39:18Z","2021-09-12T11:53:13Z"
"","3398","HADOOP-17898. Upgrade BouncyCastle to 1.69","### Description of PR  HADOOP-17898 . BouncyCastle to 1.69  - CVEs are reported for releases lower than 1.66  [CVE-2020-26939](https://nvd.nist.gov/vuln/detail/CVE-2020-26939) moderate severity [CVE-2020-15522](https://nvd.nist.gov/vuln/detail/CVE-2020-15522) moderate severity   ### How was this patch tested?  - build locally succeeded - `mvn dependency:tree` - Looked into linked Jiras of HADOOP-15832 and reviewed the dependencies affected by the upgrade -  I verified that they have no class errors as reported in YARN-8919 and YARN-8899 ```bash mvn test -Dtest=TestFileArgs,TestMultipleCachefiles,TestStreamingBadRecords,\ TestSymLink,TestMultipleArchiveFiles,TestGridmixSubmission,TestDistCacheEmulation,\ TestLoadJob,TestSleepJob,TestDistCh,TestCleanupAfterKIll  ### For code changes:  - [X] the title or this PR starts with the corresponding JIRA issue id - [X] updated `LICENSE-binary`","closed","","amahussein","2021-09-07T19:03:26Z","2021-09-08T17:01:11Z"
"","3673","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Fixes a bug in Hadoop HDFS where if more than ""dfs.namenode.decommission.max.concurrent.tracked.nodes"" datanodes are lost while in state decommissioning, then all forward progress towards decommissioning any datanodes (including healthy datanodes) is blocked  JIRA: https://issues.apache.org/jira/browse/HDFS-16303  ### How was this patch tested?  #### Unit Testing  Added new unit tests: - TestDecommission.testRequeueUnhealthyDecommissioningNodes (& TestDecommissionWithBackoffMonitor.testRequeueUnhealthyDecommissioningNodes) - DatanodeAdminMonitorBase.testPendingNodesQueueOrdering - DatanodeAdminMonitorBase.testPendingNodesQueueReverseOrdering  All ""TestDecommission"", ""TestDecommissionWithBackoffMonitor"", & ""DatanodeAdminMonitorBase"" tests pass when run locally  Note that without the ""DatanodeAdminManager"" changes the new test ""testRequeueUnhealthyDecommissioningNodes"" fails because it times out waiting for the healthy nodes to be decommissioned  ``` > mvn -Dtest=TestDecommission#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout Timed... ```  ``` > mvn -Dtest=TestDecommissionWithBackoffMonitor#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommissionWithBackoffMonitor>TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout ```  #### Manual Testing  - create Hadoop cluster with:     - 30 datanodes initially     - custom Namenode JAR containing this change     - hdfs-site configuration ""dfs.namenode.decommission.max.concurrent.tracked.node = 10""  ``` > cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 'tracked'     dfs.namenode.decommission.max.concurrent.tracked.nodes     10 ```  - reproduce the bug: https://issues.apache.org/jira/browse/HDFS-16303     - start decommissioning over 20 datanodes     - terminate 20 datanodes while they are in state decommissioning     - observe the Namenode logs to validate that there are 20 unhealthy datanodes stuck ""in Decommission In Progress""  ``` 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  - scale-up to 25 healthy datanodes & then decommission 22 of those datanodes (all but 3)     - observe the Namenode logs to validate those 22 healthy datanodes are decommissioned (i.e. HDFS-16303 is solved)  ``` 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 18:00:14,487 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:00:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:44,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 22 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned.  2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 12 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 8 nodes which are dead while in Decommission In Progress.  2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  ### For code changes:  - [yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [no] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [no] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2021-11-17T18:56:05Z","2021-11-17T20:59:14Z"
"","3675","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Fixes a bug in Hadoop HDFS where if more than ""dfs.namenode.decommission.max.concurrent.tracked.nodes"" datanodes are lost while in state decommissioning, then all forward progress towards decommissioning any datanodes (including healthy datanodes) is blocked  JIRA: https://issues.apache.org/jira/browse/HDFS-16303  ### How was this patch tested?  #### Unit Testing  Added new unit tests: - TestDecommission.testRequeueUnhealthyDecommissioningNodes (& TestDecommissionWithBackoffMonitor.testRequeueUnhealthyDecommissioningNodes) - DatanodeAdminMonitorBase.testPendingNodesQueueOrdering - DatanodeAdminMonitorBase.testPendingNodesQueueReverseOrdering  All ""TestDecommission"", ""TestDecommissionWithBackoffMonitor"", & ""DatanodeAdminMonitorBase"" tests pass when run locally  Note that without the ""DatanodeAdminManager"" changes the new test ""testRequeueUnhealthyDecommissioningNodes"" fails because it times out waiting for the healthy nodes to be decommissioned  ``` > mvn -Dtest=TestDecommission#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout Timed... ```  ``` > mvn -Dtest=TestDecommissionWithBackoffMonitor#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommissionWithBackoffMonitor>TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout ```  #### Manual Testing  - create Hadoop cluster with:     - 30 datanodes initially     - custom Namenode JAR containing this change     - hdfs-site configuration ""dfs.namenode.decommission.max.concurrent.tracked.node = 10""  ``` > cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 'tracked'     dfs.namenode.decommission.max.concurrent.tracked.nodes     10 ```  - reproduce the bug: https://issues.apache.org/jira/browse/HDFS-16303     - start decommissioning over 20 datanodes     - terminate 20 datanodes while they are in state decommissioning     - observe the Namenode logs to validate that there are 20 unhealthy datanodes stuck ""in Decommission In Progress""  ``` 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  - scale-up to 25 healthy datanodes & then decommission 22 of those datanodes (all but 3)     - observe the Namenode logs to validate those 22 healthy datanodes are decommissioned (i.e. HDFS-16303 is solved)  ``` 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 18:00:14,487 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:00:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:44,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 22 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned.  2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 12 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 8 nodes which are dead while in Decommission In Progress.  2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  ### For code changes:  - [yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [no] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2021-11-17T22:09:32Z","2021-12-23T13:59:31Z"
"","3674","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Fixes a bug in Hadoop HDFS where if more than ""dfs.namenode.decommission.max.concurrent.tracked.nodes"" datanodes are lost while in state decommissioning, then all forward progress towards decommissioning any datanodes (including healthy datanodes) is blocked  JIRA: https://issues.apache.org/jira/browse/HDFS-16303  ### How was this patch tested?  #### Unit Testing  Added new unit tests: - TestDecommission.testRequeueUnhealthyDecommissioningNodes (& TestDecommissionWithBackoffMonitor.testRequeueUnhealthyDecommissioningNodes) - DatanodeAdminMonitorBase.testPendingNodesQueueOrdering - DatanodeAdminMonitorBase.testPendingNodesQueueReverseOrdering  All ""TestDecommission"", ""TestDecommissionWithBackoffMonitor"", & ""DatanodeAdminMonitorBase"" tests pass when run locally  Note that without the ""DatanodeAdminManager"" changes the new test ""testRequeueUnhealthyDecommissioningNodes"" fails because it times out waiting for the healthy nodes to be decommissioned  ``` > mvn -Dtest=TestDecommission#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout Timed... ```  ``` > mvn -Dtest=TestDecommissionWithBackoffMonitor#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommissionWithBackoffMonitor>TestDecommission.testRequeueUnhealthyDecommissioningNodes:1776 » Timeout ```  #### Manual Testing  - create Hadoop cluster with:     - 30 datanodes initially     - custom Namenode JAR containing this change     - hdfs-site configuration ""dfs.namenode.decommission.max.concurrent.tracked.node = 10""  ``` > cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 'tracked'     dfs.namenode.decommission.max.concurrent.tracked.nodes     10 ```  - reproduce the bug: https://issues.apache.org/jira/browse/HDFS-16303     - start decommissioning over 20 datanodes     - terminate 20 datanodes while they are in state decommissioning     - observe the Namenode logs to validate that there are 20 unhealthy datanodes stuck ""in Decommission In Progress""  ``` 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  - scale-up to 25 healthy datanodes & then decommission 22 of those datanodes (all but 3)     - observe the Namenode logs to validate those 22 healthy datanodes are decommissioned (i.e. HDFS-16303 is solved)  ``` 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 18:00:14,487 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:00:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:44,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 22 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned.  2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 12 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 8 nodes which are dead while in Decommission In Progress.  2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  ### For code changes:  - [yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [n/a] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [no] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2021-11-17T21:03:11Z","2021-11-17T22:08:36Z"
"","3667","HDFS-16303. Improve handling of datanode lost while decommissioning","### Description of PR  Fixes a bug in Hadoop HDFS where if more than ""dfs.namenode.decommission.max.concurrent.tracked.nodes"" datanodes are lost while in state decommissioning, then all forward progress towards decommissioning any datanodes (including healthy datanodes) is blocked  JIRA: https://issues.apache.org/jira/browse/HDFS-16303  ### How was this patch tested?  #### Unit Testing  Added new unit tests: - TestDecommission.testRequeueUnhealthyDecommissioningNodes - DatanodeAdminMonitorBase.testPendingNodesQueueOrdering - DatanodeAdminMonitorBase.testPendingNodesQueueReverseOrdering  All ""TestDecommission"" & ""DatanodeAdminMonitorBase"" tests pass when run locally  Note that without the ""DatanodeAdminManager"" changes the new test ""testRequeueUnhealthyDecommissioningNodes"" fails because it times out waiting for the healthy nodes to be decommissioned  ``` > mvn -Dtest=TestDecommission#testRequeueUnhealthyDecommissioningNodes test ... [ERROR] Errors:  [ERROR]   TestDecommission.testRequeueUnhealthyDecommissioningNodes:1772 » Timeout Timed... ```  #### Manual Testing  - create Hadoop cluster with:     - 30 datanodes initially     - custom Namenode JAR containing this change     - hdfs-site configuration ""dfs.namenode.decommission.max.concurrent.tracked.node = 10""  ``` > cat /etc/hadoop/conf/hdfs-site.xml | grep -A 1 'tracked'     dfs.namenode.decommission.max.concurrent.tracked.nodes     10 ```  - reproduce the bug: https://issues.apache.org/jira/browse/HDFS-16303     - start decommissioning over 20 datanodes     - terminate 20 datanodes while they are in state decommissioning     - observe the Namenode logs to validate that there are 20 unhealthy datanodes stuck ""in Decommission In Progress""  ``` 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:57:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:58:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  - scale-up to 25 healthy datanodes & then decommission 22 of those datanodes (all but 3)     - observe the Namenode logs to validate those 22 healthy datanodes are decommissioned (i.e. HDFS-16303 is solved)  ``` 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 17:59:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress.  2021-11-15 18:00:14,487 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:00:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 42 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 32 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:01:44,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 32 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:14,486 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 22 nodes decommissioning but only 10 nodes will be tracked at a time. 22 nodes are currently queued waiting to be decommissioned.  2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 12 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:02:44,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 8 nodes which are dead while in Decommission In Progress.  2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): There are 20 nodes decommissioning but only 10 nodes will be tracked at a time. 10 nodes are currently queued waiting to be decommissioned. 2021-11-15 18:03:14,485 WARN org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager (DatanodeAdminMonitor-0): dfs.namenode.decommission.max.concurrent.tracked.nodes limit has been reached, re-queueing 10 nodes which are dead while in Decommission In Progress. ```  ### For code changes:  - [yes] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [no] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [n/a] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [no] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","KevinWikant","2021-11-16T23:18:13Z","2021-11-17T18:47:50Z"
"","3664","HDFS-16314. Support to make dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled reconfigurable","### Description of PR  Consider that make dfs.namenode.block-placement-policy.exclude-slow-nodes.enabled reconfigurable and rapid rollback in case this feature HDFS-16076 unexpected things happen in production environment  Details: HDFS-16314","closed","","haiyang1987","2021-11-16T03:38:48Z","2021-12-03T17:19:56Z"
"","3683","HADOOP-18014. CallerContext should not include some characters.","### Description of PR  CallerContext should not include '\t', '\n', '=' because it could be written to audit log.  ### How was this patch tested?  I created a unit test and confirmed that it succeeded.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')?","closed","","tasanuma","2021-11-19T05:45:04Z","2021-11-25T01:58:18Z"
"","3524","HDFS-16257. Set initialCapacity for guava cache to solve performance issue","### Description of PR  Branch 2.10.1 uses guava version of 11.0.2, which has a bug which affects the performance of cache, which was mentioned in HDFS-13821.  Since upgrading guava version seems affecting too much, this ticket is to add a configuration setting when initializing cache to walk around this issue.  ### How was this patch tested?  Locally tested.  ### For code changes:  - [x] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","symious","2021-10-06T08:02:07Z","2021-10-18T11:00:01Z"
"","3668","Backport HDFS-15963 to branch-3.2","### Description of PR  Backport HDFS-15963 to branch-3.2. Cherry-picked it from branch-3.3 with fixing a minor conflict.  ### How was this patch tested?   ### For code changes:  - [ ] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [ ] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation? - [ ] If adding new dependencies to the code, are these dependencies licensed in a way that is compatible for inclusion under [ASF 2.0](http://www.apache.org/legal/resolved.html#category-a)? - [ ] If applicable, have you updated the `LICENSE`, `LICENSE-binary`, `NOTICE-binary` files?","closed","","tasanuma","2021-11-17T05:42:27Z","2021-11-18T07:32:58Z"
"","3522","HADOOP-17930. implement non-guava Precondition checkState","### Description of PR  As part In order to replace Guava Preconditions, we need to implement our own versions of the API. This Jira is to add the implementation `checkState `to the existing class `org.apache.hadoop.util.Preconditions`  ### How was this patch tested?  Extended the unit test org.apache.hadoop.util.TestPreconditions  - Added `testCheckStateWithSuccess()` - Added `testCheckStateWithFailure()`  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id: HADOOP-17930 - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","amahussein","2021-10-05T14:58:55Z","2021-10-07T01:55:30Z"
"","3473","HADOOP-17929. implement non-guava Precondition checkArgument","### Description of PR  As part In order to replace Guava Preconditions, we need to implement our own versions of the API. This Jira is to add the implementation `checkArgument `to the existing class `org.apache.hadoop.util.Preconditions`  ### How was this patch tested?  Extended the unit test org.apache.hadoop.util.TestPreconditions  - Added `testCheckArgumentWithSuccess()` - Added `testCheckArgumentWithFailure()`  ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id: HADOOP-17929 - [X] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?","closed","","amahussein","2021-09-23T13:27:40Z","2021-10-01T07:17:10Z"
"","3534","HADOOP-17409. Remove s3guard from S3A module","### Description of PR  * Remove all S3Guard code from S3A production and test modules  * CLI entry point to remove obsolete commands, downgrading to warn and fail * FS to reject instantiation if metastore set to anything other than null store   Note: this patch retains the hadoop s3guard command. This is because it is a broadly documented, used in scripts and tests. We will have to retain it. We can add another entry point ""s3a"" which would invoke the same operations. However, we still get to maintain the s3guard command and will then be left with the issue that you can only use that new command on more recent versions of Hadoop. ### How was this patch tested?  deleting tests until everything worked again   ### For code changes:  - [X] Does the title or this PR starts with the corresponding JIRA issue id (e.g. 'HADOOP-17799. Your PR title ...')? - [x] Object storage: have the integration tests been executed and the endpoint declared according to the connector-specific documentation?  TODO  - [x] all tests to complete - [x] Remove all tests related to delayed consistency, + a lot of remote file changed - [x] remove `whenGuarded()` clauses from cost tests; `withWhenRaw()` to become simply `with()` - [x] remove metadata reconciliation from Listing - [x] remove attempts to update during rename (RenameTracker) - [x] delete code to not update s3guard, especially on partial bulk delete failure (+tests) - [x] s3guard retry policy references/uses (open, copy...) - [x] Docs - [x] AssumedRole rules & docs - [X] Remove the inconsistent client? wontfix, but only retains throttling events","closed","","steveloughran","2021-10-08T13:26:13Z","2022-01-17T18:10:49Z"
"","3642","YARN-10760. Number of allocated OPPORTUNISTIC containers can dip below 0","### Description of PR  * Checks that we do not release containers that do not exist * Checks that we increment recovered allocated OPPORTUNISTIC containers on RM restart * Adds corresponding test  ### How was this patch tested?  * Unit test added * Deployment to production environment","closed","","afchung","2021-11-10T16:21:34Z","2021-11-23T21:21:52Z"
"","3645","HADOOP-17998. Allow get command to run with multi threads.","### Description JIRA https://issues.apache.org/jira/browse/HADOOP-17998  CopyFromLocal/Put is enabled to run with multi-thread with HDFS-11786 and HADOOP-14698, and make put dirs or multiple files faster.So, It's necessary to enable get and copyToLocal command run with multi-thread.  ### Tests Test case: 1 dir 240 files  13G  **1. Get with single thread.**  ``` time hadoop fs -get /tmp/data/20211101 . real    6m28.785s user    0m18.844s sys    0m44.953s ``` **2. Get with 10 threads.** ``` time hadoop fs -get -t 10 /tmp/data/20211101 . real    0m58.721s user    0m21.386s sys    0m54.066s ```","closed","","smarthanwang","2021-11-11T04:14:21Z","2021-11-22T11:37:20Z"
"","3471","HDFS-16231. Fix TestDataNodeMetrics#testReceivePacketSlowMetrics","### **Description of PR**  Fix TestDataNodeMetrics#testReceivePacketSlowMetrics Details: [HDFS-16231](https://issues.apache.org/jira/browse/HDFS-16231)","closed","","haiyang1987","2021-09-23T12:24:34Z","2021-09-28T05:46:53Z"
"","3100","HDFS-16065. RBF: Add metrics to record Router's operations","## What changes were proposed in this pull request? Currently, Router's operations are not well recorded. It would be good to have a similar metrics as ""Hadoop:service=NameNode,name=NameNodeActivity"" for NameNode, which shows the count for each operations.  Besides, some operations are invoked concurrently in Routers, know the counts for concurrent operations would help us better knowing about the cluster's state.  This ticket is to add normal operation metrics and concurrent operation metrics for Router. ## What is the link to the Apache JIRA  https://issues.apache.org/jira/browse/HDFS-16065  ## How was this patch tested?  Add unit test to test normal operation and concurrent ooperation.","closed","","symious","2021-06-11T13:45:49Z","2021-09-09T10:45:45Z"
"","3577","HADOOP-17713. Update apache/hadoop:3 docker image to 3.3.1 release","## What changes were proposed in this pull request?  1. Update to Hadoop 3.3.1 release. 2. Change to `curl` for download because `wget` thinks `apache.org` certificate is expired.  https://issues.apache.org/jira/browse/HADOOP-17713  ## How was this patch tested?  Built the new image locally:  ``` $ ./build.sh ...  => => writing image sha256:23f613502b2b883ee4978b997409ca6c24116722e67caa6a9f521aef04c0a6bc                                                                                                                   0.0s  => => naming to docker.io/apache/hadoop:3                                                                                                                                                                     0.0s ```  and tested it with the included sample Docker Compose cluster:  ``` $ docker-compose up -d --scale datanode=3 --scale nodemanager=3 $ docker-compose exec resourcemanager yarn jar \     /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 3 3 ... Job Finished in 36.514 seconds Estimated value of Pi is 3.55555555555555555556 ```","closed","","adoroszlai","2021-10-22T10:37:14Z","2021-11-08T12:00:07Z"
"","2763","#HADOOP-17580, refresh freatures description in index.md","## NOTICE refresh freatures description of HuaweiCloud OBS Adapter for Hadoop Support #HADOOP-17580  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","yuchen79","2021-03-12T01:48:59Z","2021-05-13T15:40:48Z"
"","2832","HADOOP-11245. Update NFS gateway to use Netty4","## NOTICE JIRA: HADOOP-11245  This is a draft. It passed unit tests but need functional tests to ensure things like memory leak, performance is good. Looking for additional pairs of eyes to help with the code review.","closed","","jojochuang","2021-03-29T09:22:12Z","2021-04-29T06:43:20Z"
"","3301","HADOOP-17846. Update link of wiki on README","## NOTICE [HADOOP-17846](https://issues.apache.org/jira/browse/HADOOP-17846). Update link of wiki on README","closed","","aeioulisa","2021-08-12T07:00:43Z","2021-08-12T08:00:00Z"
"","3286","YARN-10814 3.1 backport","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-08-10T09:15:19Z","2021-08-16T08:00:43Z"
"","3284","HDFS-16157. Support configuring DNS record to get list of journal nodes.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","LeonGao91","2021-08-10T06:34:34Z","2021-08-26T00:40:20Z"
"","3283","YARN-10814 3.2 backport","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-08-09T14:23:32Z","2021-08-09T23:35:19Z"
"","3259","HADOOP-15327. Upgrade MR ShuffleHandler to use Netty4","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","szilard-nemeth","2021-08-03T14:06:00Z","2022-06-30T14:00:48Z"
"","3248","YARN-10874. Refactor NM ContainerLaunch#getEnvDependencies's unit tests","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-07-30T11:09:48Z","2021-08-03T14:19:04Z"
"","3246","YARN-10848. Vcore allocation problem with DefaultResourceCalculator","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","minni31","2021-07-28T17:41:30Z","2021-07-28T20:55:05Z"
"","3245","Some changes","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","adit4455","2021-07-28T09:36:14Z","2021-07-30T15:46:30Z"
"","3242","HADOOP-17819. Add extensions to ProtobufRpcEngine RequestHeaderProto","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hchaverri","2021-07-27T21:04:20Z","2022-03-21T23:16:38Z"
"","3233","HADOOP-17770 WASB : Support disabling buffered reads in positional reads","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-07-26T03:28:49Z","2021-10-22T06:15:42Z"
"","3226","HDFS-16137.Improve the comments related to FairCallQueue#queues.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-07-23T10:04:45Z","2021-07-28T10:18:04Z"
"","3223","Addendum HADOOP-17770 WASB : Support disabling buffered reads in positional reads - Added the invalid SpotBugs warning to findbugs-exclude.xml","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-07-22T04:04:19Z","2021-07-25T07:40:27Z"
"","3220","YARN-10355. Refactor NM ContainerLaunch.java#orderEnvByDependencies","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-07-20T14:49:17Z","2021-08-04T13:27:47Z"
"","3209","HDFS-16129. Fixing the signature secret file misusage in HttpFS.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tomicooler","2021-07-15T13:37:53Z","2021-09-20T12:29:50Z"
"","3203","YARN-10833. Set the X-FRAME-OPTIONS header for the default contexts.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","brumi1024","2021-07-14T14:47:56Z","2021-07-24T03:44:22Z"
"","3178","Add Leveldb statestore metrics to NM","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ChaosJu","2021-07-06T09:56:04Z","2021-07-06T13:28:48Z"
"","3177","HADOOP-16887 WIP","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","kiran-maturi","2021-07-06T06:24:26Z","2021-12-04T08:39:10Z"
"","3170","HDFS-16107.Split RPC configuration to isolate RPC.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-07-02T07:55:40Z","2022-02-01T04:49:33Z"
"","3165","[Do not commit] Refactor creds in Jenkinsfile","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","GauthamBanasandra","2021-07-01T08:39:41Z","2021-07-17T14:35:52Z"
"","3163","HADOOP-17773. Avoid using zookeeper deprecated API and classes.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","surendralilhore","2021-07-01T07:13:50Z","2021-08-23T11:45:55Z"
"","3161","HDFS-16105. Edit log corruption due to mismatch between fileId and path","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-07-01T05:53:47Z","2021-07-02T01:05:55Z"
"","3159","HDFS-16099. Make bpServiceToActive to be volatile.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhangshuyan0","2021-06-30T06:32:47Z","2021-07-01T15:53:08Z"
"","3156","HDFS-16096. Delete useless method DirectoryWithQuotaFeature#setQuota","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhuxiangyi","2021-06-29T07:32:44Z","2021-06-30T07:12:09Z"
"","3149","HADOOP-17770 WASB : Support disabling buffered reads in positional reads","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-06-27T09:08:44Z","2021-07-22T04:05:40Z"
"","3143","YARN-10832. Log aggregation failed, but it should be kept locally for easy viewing of task error information","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","hstdream","2021-06-25T06:48:28Z","2021-08-07T04:25:21Z"
"","3132","HADOOP-17764. S3AInputStream read does not re-open the input stream on the second read retry attempt","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","majdyz","2021-06-21T21:14:26Z","2021-06-28T09:03:37Z"
"","3111","HADOOP-17764. S3AInputStream read does not re-open the input stream on the second read retry attempt","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","majdyz","2021-06-16T17:14:17Z","2021-06-16T18:57:11Z"
"","3109","HADOOP-17764. S3AInputStream read does not re-open the input stream on the second read retry attempt","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","fs/s3,","majdyz","2021-06-16T10:35:41Z","2021-07-05T19:55:56Z"
"","3108","HADOOP-17741. Service authorization adds ip-based user authentication","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","zhuxiangyi","2021-06-16T08:32:33Z","2021-06-16T14:47:36Z"
"","3103","HADOOP-17643 WASB : Make metadata checks case insensitive","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-06-13T11:43:22Z","2021-12-10T05:14:32Z"
"","3102","HADOOP-17643 WASB : Make metadata checks case insensitive","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-06-13T11:39:52Z","2021-12-09T09:34:36Z"
"","3091","HDFS-16023.Improve blockReportLeaseId acquisition to avoid repeated FBR.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-06-10T08:32:25Z","2021-06-11T13:26:52Z"
"","3064","HADOOP-17739. Use hadoop-thirdparty 1.1.1.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jojochuang","2021-06-01T01:59:52Z","2021-06-01T03:34:49Z"
"","3052","HDFS-16041. TestErasureCodingCLI fails","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-05-26T01:28:54Z","2021-05-28T09:30:15Z"
"","3023","HDFS-16028. Add a configuration item for special trash dir","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","zhengzhuobinzzb","2021-05-18T14:20:44Z","2021-08-23T12:05:29Z"
"","3016","HADOOP-17699. Remove hardcoded SunX509 usage from SSLFactory.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","xiaoyuyao","2021-05-16T16:49:20Z","2021-05-18T17:11:36Z"
"","3005","HDFS-13522. RBF: Support observer node from Router-Based Federation","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","zhengzhuobinzzb","2021-05-12T09:40:22Z","2021-06-21T09:02:30Z"
"","2988","HDFS-16012.Improve DataXceiver#copyBlock() log printing","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-05-07T12:40:33Z","2021-05-07T21:13:37Z"
"","2972","HADOOP-17643 WASB : Make metadata checks case insensitive","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","anoopsjohn","2021-05-03T11:39:07Z","2021-06-13T11:43:49Z"
"","2961","HADOOP-17672.Remove an invalid comment content in the FileContext class.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","jianghuazhu","2021-04-27T10:09:02Z","2021-07-14T07:15:03Z"
"","2954","HDFS-15561. RBF: Remove NPE when local namenode is not configured","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","fengnanli","2021-04-25T06:19:40Z","2021-04-29T13:14:19Z"
"","2917","HDFS-15985.Incorrect sorting will cause failure to load an FsImage file.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-04-16T07:49:15Z","2021-11-22T05:45:04Z"
"","2911","YARN-10737: Fix typos in CapacityScheduler#schedule.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhuqi-lucas","2021-04-15T03:44:36Z","2021-05-14T05:12:56Z"
"","2910","HDFS-15810. RBF: RBFMetrics's TotalCapacity out of bounds","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","fengnanli","2021-04-15T00:50:28Z","2021-05-02T10:19:14Z"
"","2905","HDFS-15912. Allow ProtobufRpcEngine to be extensible","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hchaverri","2021-04-13T20:06:42Z","2021-05-17T07:37:24Z"
"","2904","HADOOP-17604 Fix method metric prefix for string type","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","fengnanli","2021-04-13T19:03:35Z","2021-04-14T09:00:27Z"
"","2903","HDFS-15423 RBF: WebHDFS create shouldn't choose DN from all sub-clusters","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","fengnanli","2021-04-13T18:19:31Z","2021-04-15T23:11:48Z"
"","2901","HDFS-15912. Allow ProtobufRpcEngine to be extensible","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","hchaverri","2021-04-13T05:41:31Z","2021-04-13T19:57:30Z"
"","2893","Hdfs 15739. Empty the statistical parameters when emptying the redundant queue","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","langlaile1221","2021-04-12T09:17:39Z","2021-04-12T09:20:59Z"
"","2839","HDFS-15887.Make LogRoll and TailEdits execute in parallel.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","jianghuazhu","2021-03-30T13:39:29Z","2021-03-30T21:24:08Z"
"","2829","HDFS-15930: Fix some @param errors in DirectoryScanner.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","zhuqi-lucas","2021-03-29T02:46:45Z","2021-04-04T03:30:32Z"
"","2816","HADOOP-15566 initial changes for opentelemetry - WIP","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","kiran-maturi","2021-03-24T17:03:24Z","2022-03-23T18:53:42Z"
"","2779","HDFS-15898. Test case TestOfflineImageViewer fails","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-03-16T05:01:22Z","2021-03-17T01:11:35Z"
"","2771","HDFS-15889. Erasure Coding: Limit the number of concurrent EC Reconstruct blocks in NameNode","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","open","","haiyang1987","2021-03-13T07:27:50Z","2021-03-13T14:42:26Z"
"","2747","HDFS-15877. BlockReconstructionWork should resetTargets() before BlockManager#validateReconstructionWork return false","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","haiyang1987","2021-03-05T12:41:08Z","2021-05-20T08:47:19Z"
"","2746","HDFS-15875. Check whether file is being truncated before truncate","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-03-05T05:49:47Z","2021-03-10T06:11:47Z"
"","2740","HADOOP-17563. Update Bouncy Castle to 1.68.","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","tasanuma","2021-03-04T02:05:58Z","2021-03-05T13:57:11Z"
"","2694","HDFS-15830. Support to make dfs.image.parallel.load reconfigurable","## NOTICE  Please create an issue in ASF JIRA before opening a pull request, and you need to set the title of the pull request which starts with the corresponding JIRA issue number. (e.g. HADOOP-XXXXX. Fix a typo in YYY.) For more details, please see https://cwiki.apache.org/confluence/display/HADOOP/How+To+Contribute","closed","","ferhui","2021-02-09T09:21:28Z","2021-02-19T01:30:56Z"
"","3050","HADOOP-17126. implement non-guava Precondition checkNotNull","## NOTICE  In order to replace Guava Preconditions, we need to implement our own versions of the API. This PR is to create `checkNotNull` in a new package dubbed `unguava`. This is splitting the PR that was opened previously #2134   **The plan is as follows**  - create a new `package org.apache.hadoop.util.unguava`; - create `class Validate` - implement  `org.apache.hadoop.util.unguava.Validate`  with the following interface   - `checkNotNull(final T obj)`   - `checkNotNull(final T reference, final Object errorMessage)`   - `checkNotNull(final T obj, final String message, final Object... values)`   - `checkNotNull(final T obj,final Supplier msgSupplier)` - `guava.preconditions` used `String.lenientformat` which suppressed exceptions caused by string formatting of the exception message . So, in order to avoid changing the behavior, the implementation catches exceptions triggered by building the message (IllegalFormat, InsufficientArg, NullPointer..etc) - After merging the new class, we can replace `guava.Preconditions.checkNotNull` by `unguava.Validate.checkNotNull` - We need the change to go into trunk, 3.1, 3.2, and 3.3   Similar Jiras will be created to implement `checkState`, `checkArgument`, `checkIndex`","closed","","amahussein","2021-05-25T16:21:29Z","2021-09-17T15:47:08Z"
"","2768","HDFS-15886. Add a way to get protected dirs from a special configuration file.","## jira https://issues.apache.org/jira/browse/HDFS-15886  We used protected dirs to ensure that important data directories cannot be deleted by mistake. But protected dirs can only be configured in hdfs-site.xml.  For ease of management,  we add a way to get the list of protected dirs from a special configuration file.  How to use.  1. set the config in hdfs-site.xml  ```   fs.protected.directories /hdfs/path/1,/hdfs/path/2,file:///path/to/protected.dirs.config   ```  2.  add some protected dirs to the config file (file:///path/to/protected.dirs.config)  ```  /hdfs/path/4  /hdfs/path/5  ```  3. use command to refresh fs.protected.directories instead of FSDirectory.setProtectedDirectories(..)  ```  hdfs dfsadmin -refreshProtectedDirectories  ```   ","open","","Neilxzn","2021-03-12T12:06:38Z","2021-03-15T02:45:10Z"
"","2751","HDFS-15882. Fix incorrectly initializing RandomAccessFile based on configuration options","## ISSUE https://issues.apache.org/jira/browse/HDFS-15882  ## NOTICE  - `rw` Open for reading and writing.  If the file does not already exist then an attempt will be made to create it. - `rws` Require that every update to the file's content or metadata be written synchronously to the underlying storage device.   From the literal meaning of this variable `shouldSyncWritesAndSkipFsync`, we should use `rw` when shouldSyncWritesAndSkipFsync is true.  We use SATA disk to store the journal node's data. It's not effective for improving RPC performance whether the `shouldSyncWritesAndSkipFsync` variable is true or false. it's caused by initializing RandomAccessFile incorrectly.","closed","","lamberken","2021-03-07T08:02:47Z","2021-05-26T14:02:10Z"
"","2750","HADOOP-15880. Reduce redundant end logsegment rpc","## ISSUE https://issues.apache.org/jira/browse/HDFS-15880  ## OUTLINE  The SNN always send end logsegment rpc in cycle（dfs.ha.log-roll.period=120s）. -  - If no edit request occurs during this period, ANN will still send the RPC request to the journal node which is redundant. - The `FSEditLog#endCurrentLogSegment` method is marked synchronized, it will affetc other rpc requests.   Editlogs only contains two operations : ``` LogSegmentOp [opCode=OP_START_LOG_SEGMENT, txid=-12345] LogSegmentOp [opCode=OP_END_LOG_SEGMENT, txid=-12345] ```  ``` [dcadmin@dw-work-006 ~]$ ll /work/data/hadoop/hdfs/journalnode/nn-work/current/ total 1100 -rw-r----- 1 dcadmin dcadmin       8 Mar  7 01:02 committed-txid -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:35 edits_0000000000000000001-0000000000000000002 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:37 edits_0000000000000000003-0000000000000000004 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:39 edits_0000000000000000005-0000000000000000006 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:41 edits_0000000000000000007-0000000000000000008 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:43 edits_0000000000000000009-0000000000000000010 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:45 edits_0000000000000000011-0000000000000000012 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:48 edits_0000000000000000013-0000000000000000014 -rw-r----- 1 dcadmin dcadmin      42 Mar  7 00:50 edits_0000000000000000015-0000000000000000016 ```","closed","","lamberken","2021-03-06T17:21:15Z","2021-05-26T14:02:06Z"
"","2760","YARN-10689. Fix the finding bugs in extractFloatValueFromWeightConfig.","## Fix the finding bugs show in  [finding bugs](https://ci-hadoop.apache.org/job/PreCommit-YARN-Build/775/artifact/out/branch-spotbugs-hadoop-yarn-project_hadoop-yarn_hadoop-yarn-server_hadoop-yarn-server-resourcemanager-warnings.html ) after spot bug update","closed","","zhuqi-lucas","2021-03-11T09:07:23Z","2021-03-16T04:50:38Z"
"","2767","HDFS-15790. Make ProtobufRpcEngineProtos and ProtobufRpcEngineProtos2 Co-Exist","## Existing ClientSide available Protobuf RPC Engines. * ProtobufRpcEngine: Supports existing implementations based in protobuf 2.5.0 in both client side and server side. No code changes required in downstreams use this. * ProtobufRpcEngine2: Uses shaded protobuf of 3.7.1 version and supports client side and server side implementations based on shaded protobuf 3.7.1  # Whats Changed?  ## Server side Protobuf RPC Engines `ProtobufRpcEngine2` itself will handle both versions (2.5.0 and 3.7.1) of requests for RpcKind.PROTOCOL_BUFFERS.  `ProtobufRpcEngine2` will handover the processing to Legacy `ProtobufRpcEngine` if server side protocol implemenation found to be using older version of protobuf (2.5.0).  ## How conflict arised? Both `ProtobufRpcEngine` and `ProtobufRpcEngine2` tried to register their corresponding RPC `Server` implementations to handle requests of kind `RpcKind.PROTOCOL_BUFFERS`, although this registering is of no-use in client-JVM. Only Server side this registration is required.  Now, since `ProtobufRpcEngine2` itself can handle both versions of protobuf requests at server side, registering only `ProtobufRpcEngine2` will be sufficient. So no conflict is raised for co-existence.  ## How old clients (before 3.3.0) are affected.? Since client JVM side registering of `ProtobufRpcEngine` will not affect server-JVM (running with latest version) and server can efficiently handle both type of implementations, there will not be any issues.","closed","","vinayakumarb","2021-03-12T06:48:23Z","2021-05-24T09:51:42Z"
"","2719","YARN-10649. fix RMNodeImpl.updateExistContainers leak","## detail jira : https://issues.apache.org/jira/browse/YARN-10649  [YARN-5168](https://issues.apache.org/jira/browse/YARN-5168)  added **RMNodeImpl.updatedExistContainers**, but it didn't remove completed containers.  These objects (ContainerStatusPBImpl & ContainerIdPBImpl ) stay in **RMNodeImpl.updatedExistContainers** forever.  Because of this leak, ResourceManager in our production environment encountered  OOM issue. We found 13 million ContainerStatusPBImpl objects in the heap dump file of ResourceManager.  The patch has been applied in our production env and so far it works well.","closed","","Neilxzn","2021-02-24T03:56:05Z","2021-03-04T09:22:58Z"
"","3154","YARN-10835. Pass user defined variables of yarn.nodemanager.env-whitelist along jobs","# What changes were proposed in this pull request? The configuration of ""yarn.nodemanager.env-whitelist"" has been helpful in running multi version Hadoop on Yarn. But the pain point is that users need to use different configurations to run jobs of different versions of Hadoop.  The first idea is to have SRE prepared different versions of configurations and let users specify different environment variables of ""HADOOP_CONF_DIR"", this shifts the burden from users to SRE that they have to maintain different versions of configurations with only one or two configurations changed.  This ticket proposes to export the value of the environment variable in ""yarn.nodemanager.env-whitelist"" via the environment variable or Java properties of the client submitting jobs. So that SRE only has to maintain one version of configuration and users can easily define Hadoop version with one export in an environment variable or Java property.  # What is the link to the Apache JIRA https://issues.apache.org/jira/browse/YARN-10835  # How was this patch tested? Add unit test to pass Java properties to `ContainerLaunchContext`.","open","","symious","2021-06-28T09:11:46Z","2021-06-28T12:52:41Z"
"","2756","HDFS-15886. Add a way to read the list of protected dirs from a special config file","# jira https://issues.apache.org/jira/browse/HDFS-15886  We used protected dirs to ensure that important data directories cannot be deleted by mistake. But protected dirs can only be configured in hdfs-site.xml.  For ease of management,  we add a way to get the list of protected dirs from a special configuration file.  How to use.  1. set the config in hdfs-site.xml  ```  dfs.protected.directories.config.file.enable true   fs.protected.directories file:///path/to/protected.dirs.config  ```  2.  add some protected dirs to the config file (file:///path/to/protected.dirs.config)  ``` /1 /2/3 ```  3. done","closed","","Neilxzn","2021-03-10T08:55:33Z","2021-03-11T07:00:37Z"
"","2925","HDFS-15982. Deleted data using HTTP API should be saved to the trash","branch-3.3 backport of PR #2927","closed","","virajjasani","2021-04-18T09:14:32Z","2021-05-29T16:42:24Z"
"","3688","HADOOP-18018. unguava: remove Preconditions from hadoop-tools modules","","closed","","virajjasani","2021-11-19T18:52:37Z","2021-11-23T04:34:22Z"
"","3687","HADOOP-18017. unguava: remove Preconditions from hadoop-yarn-project modules","","closed","","virajjasani","2021-11-19T18:47:11Z","2021-11-23T04:36:38Z"
"","3672","HDFS-16330. Fix incorrect placeholder for Exception logs in DiskBalancer","","closed","","virajjasani","2021-11-17T17:44:22Z","2021-11-18T17:04:45Z"
"","3555","HADOOP-17967. Keep restrict-imports-enforcer-rule for Guava VisibleForTesting in hadoop-main pom","","closed","","virajjasani","2021-10-14T19:30:58Z","2021-10-21T07:57:16Z"
"","3546","HDFS-16268. Balancer stuck when moving striped blocks due to NPE","","closed","","LeonGao91","2021-10-12T19:01:55Z","2021-10-14T01:14:03Z"
"","3541","HADOOP-17963. Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-yarn-project modules","","closed","","virajjasani","2021-10-11T18:15:56Z","2021-10-14T09:03:14Z"
"","3540","HADOOP-17962. Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-tools modules","","closed","","virajjasani","2021-10-11T18:08:19Z","2021-10-14T08:43:47Z"
"","3537","HADOOP-17959. Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-cloud-storage-project and hadoop-mapreduce-project modules","","closed","","virajjasani","2021-10-08T18:49:45Z","2021-10-11T07:23:07Z"
"","3530","HADOOP-17957. Replace Guava VisibleForTesting by Hadoop's own annotation in hadoop-hdfs-project modules","","closed","","virajjasani","2021-10-07T13:15:57Z","2021-10-11T06:33:25Z"
"","3521","HADOOP-17947. Additional element types for VisibleForTesting (ADDENDUM)","","closed","","virajjasani","2021-10-05T13:56:25Z","2021-10-05T17:17:32Z"
"","3515","HADOOP-17950. Provide replacement for deprecated APIs of commons-io IOUtils","","closed","","virajjasani","2021-10-03T18:30:36Z","2021-10-07T12:32:51Z"
"","3432","Revert "" HDFS-15160. branch-3.2. ReplicaMap, Disk Balancer, Directory Scanner and various FsDatasetImpl methods should use datanode readlock.""","","closed","","Hexiaoqiao","2021-09-14T04:37:10Z","2021-09-14T11:04:20Z"
"","3427","HDFS-10648. Expose Balancer metrics through Metrics2","","closed","","LeonGao91","2021-09-13T06:53:34Z","2021-09-21T19:26:18Z"
"","3424","HDFS-16223. AvailableSpaceRackFaultTolerantBlockPlacementPolicy should use chooseRandomWithStorageTypeTwoTrial() for better performance.","","closed","","ayushtkn","2021-09-11T18:32:28Z","2021-09-13T09:55:20Z"
"","3422","HDFS-16222. Fix ViewDFS with mount points for HDFS only API.","","closed","","ayushtkn","2021-09-11T09:33:01Z","2021-10-03T04:32:57Z"
"","3367","HDFS-16202. Use constants HdfsClientConfigKeys.Failover.PREFIX instead of ""dfs.client.failover.""","","closed","","WeisonWei","2021-09-01T11:25:32Z","2021-09-03T06:35:54Z"
"","3296","HDFS-16163. Avoid locking entire blockPinningFailures map","","closed","","virajjasani","2021-08-11T13:12:53Z","2021-08-16T05:41:58Z"
"","3278","HADOOP-17841. Remove ListenerHandle from Hadoop registry","","closed","","virajjasani","2021-08-08T17:41:03Z","2021-08-10T02:21:58Z"
"","3267","HADOOP-17808. Avoid excessive logging for interruption (ADDENDUM)","","closed","","virajjasani","2021-08-04T15:28:20Z","2021-08-06T02:46:59Z"
"","3266","HADOOP-17835. Use CuratorCache implementation instead of PathChildrenCache / TreeCache","","closed","","virajjasani","2021-08-04T09:57:37Z","2021-08-09T09:03:17Z"
"","3241","HADOOP-17612. Upgrade Zookeeper to 3.6.3 and Curator to 5.2.0","","closed","","virajjasani","2021-07-27T14:18:36Z","2021-08-09T09:03:32Z"
"","3230","HADOOP-17814. Provide fallbacks for identity/cost providers and backoff enable","","closed","","virajjasani","2021-07-24T19:08:22Z","2021-08-09T09:03:44Z"
"","3228","HDFS-16139. Update BPServiceActor Scheduler's nextBlockReportTime atomically","","closed","","virajjasani","2021-07-24T16:26:54Z","2021-07-27T06:57:12Z"
"","3224","HADOOP-17813. Checkstyle rule should allow line length: 100","","closed","","virajjasani","2021-07-22T13:14:38Z","2021-07-23T11:44:54Z"
"","3219","HADOOP-17808. ipc.Client to set interrupt flag after catching InterruptedException","","closed","","virajjasani","2021-07-20T12:05:53Z","2021-08-04T15:22:13Z"
"","3212","YARN-9551. TestTimelineClientV2Impl.testSyncCall fails intermittent","","closed","","9uapaw","2021-07-16T15:20:02Z","2021-07-28T02:35:20Z"
"","3204","HADOOP-16272. Upgrade HikariCP to 4.0.3","","closed","","virajjasani","2021-07-14T16:53:11Z","2021-07-16T03:17:18Z"
"","3198","HADOOP-16290. Enable RpcMetrics units to be configurable","","closed","","virajjasani","2021-07-13T12:58:49Z","2021-07-20T07:45:56Z"
"","3193","MAPREDUCE-7356. Remove few duplicate dependencies from mapreduce-client's child poms","","closed","","virajjasani","2021-07-10T15:28:40Z","2021-07-13T14:00:13Z"
"","3192","HADOOP-17795. Provide fallback properties for callqueue.impl and scheduler.impl","","closed","","virajjasani","2021-07-09T16:30:39Z","2021-07-14T11:59:04Z"
"","3169","HDFS-16108. Fix incorrect log placeholders used in JournalNodeSyncer","","closed","","virajjasani","2021-07-02T07:25:44Z","2021-07-05T01:23:24Z"
"","3158","HADOOP-17779: Lock File System Creator Semaphore Uninterruptibly","","open","","belugabehr","2021-06-30T01:00:34Z","2021-09-09T13:00:26Z"
"","3150","HDFS-16092. Avoid creating LayoutFlags redundant objects","","closed","","virajjasani","2021-06-27T12:28:55Z","2021-06-29T09:31:02Z"
"","3148","HDFS-16090. Fine grained lock for datanodeNetworkCounts","","closed","","virajjasani","2021-06-26T17:06:47Z","2021-07-02T07:15:30Z"
"","3145","[Do not commit][WIP] CI for Centos 8","","closed","","GauthamBanasandra","2021-06-26T02:28:51Z","2021-06-30T16:25:24Z"
"","3127","HDFS-16082. Atomic operations on exceptionsSinceLastBalance and failedTimesSinceLastSuccessfulBalance in Balancer","","closed","","virajjasani","2021-06-21T09:03:11Z","2021-06-24T12:18:12Z"
"","3123","MAPREDUCE-7354. Use empty array constant present in TaskCompletionEvent to avoid creating redundant objects","","closed","","virajjasani","2021-06-19T11:33:31Z","2021-06-21T15:12:13Z"
"","3121","HDFS-16080. RBF: Invoking method in all locations should break the loop after successful result","","closed","","virajjasani","2021-06-18T16:28:09Z","2021-06-21T15:12:26Z"
"","3118","[Do not commit] CI for Debian 10","","closed","","GauthamBanasandra","2021-06-17T17:22:31Z","2021-06-21T17:08:11Z"
"","3115","HDFS-16075. Use empty array constants present in StorageType and DatanodeInfo to avoid creating redundant objects","","closed","","virajjasani","2021-06-17T09:29:23Z","2021-06-21T15:12:19Z"
"","3107","HDFS-16074. Remove an expensive debug string concatenation","","closed","","jojochuang","2021-06-16T06:42:43Z","2021-06-17T03:37:57Z"
"","3087","HADOOP-17753. Keep restrict-imports-enforcer-rule for Guava Lists in top level hadoop-main pom","","closed","","virajjasani","2021-06-09T07:49:28Z","2021-06-15T07:12:58Z"
"","3081","YARN-10809. Missing dependency causing NoClassDefFoundError in TestHBaseTimelineStorageUtils","","closed","","virajjasani","2021-06-08T05:53:46Z","2021-06-15T07:13:45Z"
"","3075","YARN-10805. Replace Guava Lists usage by Hadoop's own Lists in hadoop-yarn-project","","closed","","virajjasani","2021-06-03T18:04:23Z","2021-06-15T07:13:24Z"
"","3074","MAPREDUCE-7350. Replace Guava Lists usage by Hadoop's own Lists in hadoop-mapreduce-project","","closed","","virajjasani","2021-06-03T17:54:32Z","2021-06-15T07:14:05Z"
"","3073","HDFS-16054. Replace Guava Lists usage by Hadoop's own Lists in hadoop-hdfs-project","","closed","","virajjasani","2021-06-03T17:40:17Z","2021-06-15T07:13:35Z"
"","3072","HADOOP-17743. Replace Guava Lists usage by Hadoop's own Lists in hadoop-common, hadoop-tools and cloud-storage projects","","closed","","virajjasani","2021-06-03T17:20:46Z","2022-01-21T10:35:41Z"
"","3061","HADOOP-17152. Provide Hadoop's own Lists utility to reduce dependency on Guava","","closed","","virajjasani","2021-05-28T10:06:27Z","2021-06-15T07:14:22Z"
"","3059","#HDFS-13729 Removed extra space","","closed","","oojas","2021-05-27T01:48:05Z","2021-06-08T12:16:55Z"
"","3049","HADOOP-17732. Keep restrict-imports-enforcer-rule for Guava Sets in hadoop-main pom","","closed","","virajjasani","2021-05-25T11:15:37Z","2021-06-15T07:15:13Z"
"","3041","HADOOP-17725. Improve error message for token providers in ABFS","","closed","","virajjasani","2021-05-21T15:05:53Z","2021-06-15T07:14:40Z"
"","3039","HDFS-16032. DFSClient#delete supports Trash","","closed","","zhuxiangyi","2021-05-21T10:29:35Z","2021-05-31T10:30:22Z"
"","3034","HADOOP-17718. Explicitly set locale in the Dockerfile.","","closed","","jojochuang","2021-05-21T05:24:03Z","2021-05-21T16:34:41Z"
"","3033","HADOOP-17721. Replace Guava Sets usage by Hadoop's own Sets in hadoop-yarn-project","","closed","","virajjasani","2021-05-20T19:52:30Z","2021-05-25T11:16:55Z"
"","3032","HADOOP-17722. Replace Guava Sets usage by Hadoop's own Sets in hadoop-mapreduce-project","","closed","","virajjasani","2021-05-20T19:32:36Z","2021-05-25T11:17:48Z"
"","3031","HADOOP-17720. Replace Guava Sets usage by Hadoop's own Sets in hadoop-hdfs-project","","closed","","virajjasani","2021-05-20T18:50:11Z","2021-05-25T11:17:26Z"
"","3029","HADOOP-17717. Update wildfly openssl to 1.1.3.Final.","","open","","jojochuang","2021-05-20T11:44:17Z","2021-11-05T12:45:45Z"
"","3024","HADOOP-17426. Upgrade to hadoop-thirdparty-1.1.0.","","closed","","jojochuang","2021-05-18T22:22:28Z","2021-05-19T05:55:54Z"
"","3018","HDFS-16027. Replace abstract methods with default methods in JournalNodeMXBean","","closed","","jojochuang","2021-05-17T09:53:28Z","2021-05-19T02:49:01Z"
"","3017","HADOOP-17703. checkcompatibility.py errors out when specifying annotations.","","closed","","jojochuang","2021-05-17T09:36:53Z","2021-05-18T03:21:54Z"
"","3015","HADOOP-17700. ExitUtil#halt info log should log HaltException","","closed","","virajjasani","2021-05-16T07:10:29Z","2021-06-01T14:49:36Z"
"","3008","HADOOP-17689. Avoid Potential NPE in org.apache.hadoop.fs","","closed","","virajjasani","2021-05-12T11:52:17Z","2021-05-12T15:17:57Z"
"","2993","HDFS-16017. RBF: The getlisting method does not cover the result of NN","","open","","zhuxiangyi","2021-05-10T04:40:39Z","2021-08-26T17:53:01Z"
"","2990","HADOOP-17686. Avoid potential NPE by using Path#getParentPath API in hadoop-huaweicloud","","closed","","virajjasani","2021-05-08T08:53:15Z","2021-05-12T15:18:14Z"
"","2987","HDFS-16011. Support ViewFs nested mount","","open","","zhuxiangyi","2021-05-07T12:28:04Z","2021-05-14T14:03:18Z"
"","2985","HADOOP-17115. Replace Guava Sets usage by Hadoop's own Sets in hadoop-common and hadoop-tools","","closed","","virajjasani","2021-05-06T13:39:55Z","2021-05-20T18:16:54Z"
"","2976","HDFS-15982. HTTP API to consider default value of skiptrash true (ADDENDUM)","","closed","","virajjasani","2021-05-05T12:37:09Z","2022-06-14T22:07:59Z"
"","2973","HADOOP-11616. Remove workaround for Curator's ChildReaper requiring Guava 15+","","closed","","virajjasani","2021-05-04T06:59:20Z","2021-05-06T09:42:26Z"
"","2969","HADOOP-17676. Restrict imports from org.apache.curator.shaded","","closed","","virajjasani","2021-05-02T06:39:43Z","2021-05-03T16:10:38Z"
"","2938","HADOOP-17650. Exclude org.restlet.jee dependencies to unblock build failure with Maven 3.8.1","","closed","","virajjasani","2021-04-20T13:08:52Z","2021-04-20T14:33:55Z"
"","2932","HADOOP-17649. Upgrade wildfly openssl to 2.1.3.Final due to security vulnerabilities","","closed","","virajjasani","2021-04-19T08:21:16Z","2021-05-29T16:42:49Z"
"","2926","HADOOP-17645 Fix test failures in org.apache.hadoop.fs.azure.ITestOutputStreamSemantics.","","closed","","anoopsjohn","2021-04-18T11:25:35Z","2021-06-13T17:37:11Z"
"","2924","HDFS-15982. Deleted data on the Web UI must be saved to the trash","","closed","","virajjasani","2021-04-18T09:03:49Z","2021-04-18T13:18:31Z"
"","2922","HADOOP-17642. Remove appender EventCounter to avoid instantiation","","closed","","virajjasani","2021-04-16T19:21:30Z","2021-04-17T11:47:45Z"
"","2909","HADOOP-17524. Remove EventCounter and Log counters from JVM Metrics","","closed","","virajjasani","2021-04-14T17:28:00Z","2021-04-15T09:05:31Z"
"","2897","HADOOP-17611 Restore modification and access times after concat","","open","","amaroti","2021-04-12T16:15:22Z","2021-04-14T10:03:42Z"
"","2895","HADOOP-17633. Bump json-smart to 2.4.2 and nimbus-jose-jwt to 9.8.1 due to CVEs","","closed","","virajjasani","2021-04-12T12:15:56Z","2021-04-16T07:06:01Z"
"","2892","HADOOP-17611. Distcp parallel file copy should retain first chunk modifiedTime after concat","","closed","","virajjasani","2021-04-11T17:49:56Z","2021-04-13T18:25:46Z"
"","2888","HDFS-15959 : add support for digest authentication in ZKDelegationTokenSecretManager","","open","","bolerio","2021-04-10T02:04:21Z","2021-04-10T05:04:06Z"
"","2887","HDFS-15960 RBF: Router should talk to namenode with security context.","","open","","bolerio","2021-04-09T16:20:46Z","2021-06-03T04:39:51Z"
"","2879","HADOOP-17627. Backport to branch-3.2 HADOOP-17371, HADOOP-17621, HADOOP-17625 to update Jetty to 9.4.39.","","closed","","jojochuang","2021-04-09T03:06:02Z","2021-04-20T03:12:27Z"
"","2876","HDFS-15956. Provide utility class for FSNamesystem","","closed","","virajjasani","2021-04-07T19:00:18Z","2021-04-09T07:37:55Z"
"","2874","HDFS-15940. Fix TestBlockRecovery2#testRaceBetweenReplicaRecoveryAndFinalizeBlock (ADDENDUM)","","closed","","virajjasani","2021-04-07T10:06:34Z","2021-04-09T02:21:00Z"
"","2871","HADOOP-17614. Bump netty to the latest 4.1.61.","","closed","","jojochuang","2021-04-06T08:57:14Z","2021-04-12T05:55:02Z"
"","2870","HADOOP-17625. Update to Jetty 9.4.39.","","closed","","jojochuang","2021-04-06T07:45:04Z","2021-04-08T06:25:11Z"
"","2862","HADOOP-17622. Avoid usage of deprecated IOUtils#cleanup API.","","closed","","virajjasani","2021-04-04T13:12:42Z","2021-04-06T04:39:26Z"
"","2844","HDFS-15940. Fixing and refactoring tests specific to Block recovery","","closed","","virajjasani","2021-03-31T07:38:37Z","2021-04-07T10:01:59Z"
"","2840","HADOOP-17612 : Bump Zookeeper version to 3.7.0","","closed","","virajjasani","2021-03-30T15:01:15Z","2021-04-02T06:24:25Z"
"","2808","HADOOP-17531. DistCp: Reduce memory usage on copying huge directories. (#2732).","","closed","","ayushtkn","2021-03-23T21:20:05Z","2021-03-27T15:45:05Z"
"","2794","HDFS-15911 : Provide blocks moved count in Balancer iteration result","","closed","","virajjasani","2021-03-22T08:53:07Z","2021-09-24T12:53:12Z"
"","2785","HDFS-15904 : De-flake TestBalancer#testBalancerWithSortTopNodes()","","closed","","virajjasani","2021-03-18T11:33:55Z","2021-03-20T13:18:53Z"
"","2773","Test PR. Igonre.","","open","","ayushtkn","2021-03-14T12:18:05Z","2021-12-25T15:59:16Z"
"","2765","Test PR to check HADOOP-17570","","closed","","aajisaka","2021-03-12T05:23:31Z","2021-03-12T07:32:26Z"
"","2759","HADOOP-17574 : Define boundedMultipartUploadThreadPool as ExecutorService","","closed","","virajjasani","2021-03-10T10:54:25Z","2021-03-10T16:26:48Z"
"","2757","HADOOP-17571 : Bump up woodstox-core to 5.3.0 due to security concerns","","closed","dependencies,","virajjasani","2021-03-10T09:00:27Z","2021-03-11T19:57:27Z"
"","2738","HDFS-15842. HDFS mover to emit metrics.","","closed","","LeonGao91","2021-03-03T07:12:29Z","2021-06-19T22:39:47Z"
"","2728","HDFS-15865 Interrupt DataStreamer thread if no ack","","closed","","karthikhw","2021-03-01T08:29:15Z","2021-05-01T18:05:48Z"
"","2704","HDFS-15781. Add metrics for how blocks are moved in replaceBlock.","","closed","","LeonGao91","2021-02-16T07:45:48Z","2021-02-24T06:29:49Z"