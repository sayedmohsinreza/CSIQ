"#","No","Issue Title","Issue Details","State","Labels","User name","created","Updated"
"","616","HDDS-1296. Fix checkstyle issue from Nightly run. Contributed by Xiao…","…yu Yao.","closed","ozone,","xiaoyuyao","2019-03-18T05:18:43Z","2019-03-18T15:38:56Z"
"","159","HDFS-11111. Delete items in .Trash using rm should be forbidden witho…","…ut safety option","open","","LantaoJin","2016-11-11T20:08:17Z","2021-02-09T08:46:48Z"
"","458","YARN-8246 winutils - fix failure to retrieve disk and network perf co…","…unters on localized windows installlation.  PdhAddCounter expects performance counter path to be in the same language than the windows installation. With current code, the calls to PdhAddCounter will fail with error 0xc0000bb8 (PDH_CSTATUS_NO_OBJECT) on non-english windows installation.  The solution is to use PdhAddEnglishCounter function instead https://docs.microsoft.com/en-us/windows/desktop/api/pdh/nf-pdh-pdhaddenglishcounterw  ""This function provides a language-neutral way to add performance counters to the query""","closed","","pgoron","2019-01-07T14:02:39Z","2021-02-13T11:38:10Z"
"","444","YARN-9093. Remove commented code block from the beginning of TestDefa…","…ultContainerExecutor","closed","","vbmudalige","2018-12-12T06:02:58Z","2019-08-02T11:16:20Z"
"","542","HDDS-1204. Fix ClassNotFound issue with javax.xml.bind.DatatypeConver…","…ter used by DefaultProfile. Contributed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-01T18:50:50Z","2019-03-01T20:18:43Z"
"","656","HDDS-1350. Fix checkstyle issue in TestDatanodeStateMachine. Contribu…","…ted by Xiaoyu Yao.","closed","","xiaoyuyao","2019-03-28T18:53:52Z","2019-03-28T21:50:35Z"
"","665","HADOOP-16220. Add findbugs ignores for unjustified issues during upda…","…te to guava to 27.0-jre in hadoop-project","closed","","bgaborg","2019-03-29T16:26:51Z","2019-04-01T04:51:11Z"
"","598","HDDS-1254. Fix failure in TestOzoneManagerHttpServer & TestStorageCon…","…tainerManagerHttpServer. Contributed by Ajay Kumar.","closed","ozone,","ajayydv","2019-03-12T19:31:08Z","2019-03-21T22:28:42Z"
"","410","add non-empty folder when test cannot find HADOOP_CONF_DIR [https://i…","…ssues.apache.org/jira/browse/HDDS-395] - Sree Vaddi  this is to avoid throwing exception that fails cluster from starting.","open","","sreev","2018-09-04T00:36:18Z","2019-08-27T12:23:59Z"
"","683","HDDS-1377. OM failed to start with incorrect hostname set as ip addre…","…ss in CSR.","closed","ozone,","xiaoyuyao","2019-04-03T17:14:32Z","2019-04-03T18:53:51Z"
"","652","HDDS-1346. Remove hard-coded version ozone-0.5.0 from ReadMe of ozone…","…secure-mr docker-compose. Contributed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-28T01:36:34Z","2019-03-28T04:52:37Z"
"","122","HDFS-10753.Method invocation in log can be replaced by variable becau…","…se the variable's toString method contains more info","closed","","albericliu","2016-08-26T08:00:24Z","2019-07-26T11:44:20Z"
"","553","HDDS-1216. Change name of ozoneManager service in docker compose file…","…s to om. Contributed by Ajay Kumar.","closed","ozone,","ajayydv","2019-03-04T21:04:36Z","2019-03-21T22:29:16Z"
"","138","HADOOP-13309: Document S3A known limitations in file ownership and pe…","…rmission model.  Summary: - Update file system specification to describe that object stores may have a different authorization model than HDFS and traditional file systems. - Update hadoop-aws documentation to warn that S3A will return stub information for metadata related to ownership and permissions.  I wrote this information from the assumption that the HADOOP-12774 change gets finished, so that one will have to get committed first. - Also update a few cosmetic things near the part of the hadoop-aws document that I changed.","closed","","cnauroth","2016-10-10T22:55:30Z","2016-11-01T16:44:35Z"
"","62","YARN-4434.NodeManager Disk Checker parameter documentation is not cor…","…rect","closed","","bwtakacy","2015-12-08T08:07:08Z","2015-12-10T05:09:36Z"
"","58","HDFS-9459. hadoop-hdfs-native-client fails test build on Windows afte…","…r transition to ctest.  I verified that the hadoop-hdfs-native-client tests build correctly and run successfully after applying this patch.","closed","","cnauroth","2015-11-24T19:58:58Z","2016-06-23T16:07:06Z"
"","49","HDFS-9443. Disabling HDFS client socket cache causes logging message …","…printed to console for CLI commands.  This is a trivial patch to change the log statement to debug level.","closed","","cnauroth","2015-11-19T23:19:16Z","2019-01-24T10:40:18Z"
"","679","HADOOP-16230. Correct findbug ignores for unjustified issues during u…","…pdate to guava to 27.0-jre in hadoop-project","closed","","bgaborg","2019-04-02T12:38:17Z","2019-04-04T09:39:11Z"
"","517","HADOOP-16147: Allow CopyListing sequence file keys and values to be m…","…ore easily customized  [HADOOP-16147](https://issues.apache.org/jira/browse/HADOOP-16147)","closed","","noslowerdna","2019-02-25T21:58:05Z","2019-03-22T14:27:53Z"
"","235","HDFS-11978. Remove invalid '-usage' command of 'ec' and add missing c…","…ommands 'addPolicies' 'listCodecs'","closed","","wenxinhe","2017-06-15T07:23:47Z","2017-07-05T03:34:46Z"
"","587","HDDS-1245. OM delegation expiration time should use Time.now instead …","…of Time.monotonicNow. Contributed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-11T06:32:12Z","2019-03-12T04:15:41Z"
"","261","YARN-2162. add ability to optionally configure maxResources in terms …","…of percentage","closed","","flyrain","2017-08-11T23:50:41Z","2018-02-24T06:52:23Z"
"","541","HDDS-134. SCM CA: OM sends CSR and uses certificate issued by SCM. Co…","…ntributed by Ajay Kumar.","closed","ozone,","ajayydv","2019-03-01T18:11:16Z","2019-03-21T22:29:41Z"
"","172","HDFS-11234: Made the socket buffer size configurable with the config …","…node fs.hdfs.data.socket.size to be set in core-site.xml. If the node is not found, the default value is -1, so socket buffer size would not be set.","open","","subahugu","2016-12-12T07:45:29Z","2019-02-06T11:23:44Z"
"","639","HDDS-1291. Set OmKeyArgs#refreshPipeline flag properly to avoid readi…","…ng from stale pipeline. Contributed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-22T19:37:39Z","2019-03-22T22:08:28Z"
"","146","HADOOP-6801. io.sort.mb and io.sort.factor were renamed and moved to …","…mapreduce but are still in CommonConfigurationKeysPublic.java and used in SequenceFile.java","closed","","QwertyManiac","2016-10-26T07:52:30Z","2017-03-03T09:39:52Z"
"","649","HDDS-1332. Add some logging for flaky test testStartStopDatanodeState…","…Machine. Contributed by Arpit Agarwal.  Also remove an overaggressive clause in DatanodeStateMachine#isDaemonStopped,  Change-Id: I4f9dc6aeff7f4502956d160e35f2c4caadccb246","closed","","arp7","2019-03-27T00:33:05Z","2019-03-27T15:23:31Z"
"","601","HDDS-1119. DN get OM certificate from SCM CA for block token validat…","…ion. Contributed by Ajay Kumar.","closed","ozone,","ajayydv","2019-03-13T18:48:22Z","2019-03-21T22:28:10Z"
"","479","properly log the added/removed nodes from the blacklist instead of pr…","…inting the List object reference","open","","cvaliente","2019-02-06T09:02:48Z","2019-04-08T23:58:21Z"
"","596","HDDS-1087. Fix TestDefaultCertificateClient#testSignDataStream. Contr…","…ibuted by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-12T16:07:23Z","2019-03-13T18:50:54Z"
"","545","HDDS-1183. Override getDelegationToken API for OzoneFileSystem. Contr…","…ibuted by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-02T00:29:28Z","2019-03-04T18:37:27Z"
"","526","HDDS-1183. Override getDelegationToken API for OzoneFileSystem. Contr…","…ibuted by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-02-27T23:39:50Z","2019-03-05T10:32:09Z"
"","638","HDDS-1309 . change logging from warn to debug in XceiverClient. Contr…","…ibuted by Nilotpal Nandi.","closed","","nilotpalnandi","2019-03-22T10:39:48Z","2019-03-28T19:22:45Z"
"","77","Branched to archive the Avro RPC work. On trunk Avro RPC is removed a…","…fter creating this branch.  git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HADOOP-6659@1214008 13f79535-47bb-0310-9956-ffa450edef68","closed","","arif505","2016-02-19T18:20:30Z","2020-07-31T05:07:50Z"
"","513","HDDS-1120. Add a config to disable checksum verification during read …","…even though checksum data is present in the persisted data.","closed","ozone,","bharatviswa504","2019-02-21T20:30:10Z","2019-02-24T05:38:00Z"
"","620","HDDS-1205. Refactor ReplicationManager to handle QUASI_CLOSED contain…","…ers. Contributed by Nanda kumar.  Change-Id: I578ed5c196bb4d20754607cb9f4ddee7e0599077","closed","","arp7","2019-03-18T20:10:05Z","2019-12-17T17:30:08Z"
"","633","HDDS-1321. TestOzoneManagerHttpServer depends on hard-coded port numb…","…ers. Contributed by Arpit Agarwal.  Change-Id: If17c851b4aea7070064069a3596a144ad80d284c","closed","","arp7","2019-03-21T19:45:59Z","2019-03-21T21:19:33Z"
"","188","YARN-6151. FS Preemption doesn't filter out queues which cannot be pr…","…eempted.","closed","","flyrain","2017-02-06T22:16:28Z","2017-04-04T16:44:38Z"
"","572","HDDS-1235. BaseHttpServer NPE is HTTP policy is HTTPS_ONLY. Contribut…","…ed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-07T22:13:35Z","2019-03-09T17:38:02Z"
"","288","HDFS-12749. Catch IOException to schedule BlockReport correctly when …","…DN re-register. (Contributed by TanYuxin)","closed","","TanYuxin-tyx","2017-11-02T07:15:31Z","2019-07-22T02:47:21Z"
"","198","YARN-6261 - Catch user with no group when getting queue from mapping …","…definition","open","","pvillard31","2017-03-01T22:30:03Z","2019-04-08T23:57:39Z"
"","552","HDDS-1156. testDelegationToken is failing in TestSecureOzoneCluster. …","…Contributed by Ajay Kumar.","closed","ozone,","ajayydv","2019-03-04T19:26:42Z","2019-03-05T16:37:10Z"
"","580","HADOOP-16166. TestRawLocalFileSystemContract fails with build Docker …","…container running on Mac.  Also provided similar fix for Windows.  As noted in the Jira, this problem was discovered when working on a Mac, in the docker container created by `start-build-env.sh`. In that environment, this unit test fails, because it doesn't take into account that a Linux process may be mounting a Mac's case-insensitive filesystem.  This patch adds recognition of that circumstance for both Mac and Windows.  Yetus testing will not see any change.  I manually tested it in the `start-build-env.sh` environment on both Mac and Win 10 Pro.  No new unit test, as this is a fix to an existing unit test.","closed","","mattf-apache","2019-03-08T19:23:03Z","2019-03-13T17:46:22Z"
"","615","HDDS-1215. Change hadoop-runner and apache/hadoop base image to use J…","…ava8.","closed","ozone,","xiaoyuyao","2019-03-18T05:08:58Z","2019-03-19T16:55:53Z"
"","612","HDDS-1285. Implement actions need to be taken after chill mode exit w…","…ait time.","closed","ozone,","bharatviswa504","2019-03-14T23:56:01Z","2019-04-12T16:40:27Z"
"","664","[HADOOP-14951] Make the KMSACLs implementation customizable, with an …","…additional new configuration option.   Refactor the code to use KeyManagementACLs interface, with a default interface method. This help external KMS services to better re-use the code, like Ranger KMS","open","","gzsombor","2019-03-29T15:30:28Z","2019-09-03T04:38:59Z"
"","573","HDDS-1236. Fix incorrect Ozone ClientProtocol KerberosInfo annotation…","…. Contributed by Xiaoyu Yao.","closed","ozone,","xiaoyuyao","2019-03-07T22:38:32Z","2019-03-11T16:19:20Z"
"","483","HDFS-14260: Synchronized Method in BlockReceiver Can Be Replaced with…","… Atomic Value","closed","","belugabehr","2019-02-07T22:22:34Z","2019-08-08T04:21:57Z"
"","84","HADOOP-12927. Update netty-all to 4.0.34.Final","ZooKeeper 3.4.6 uses netty 3.7.0.Final","closed","","ceefour","2016-03-15T13:49:31Z","2019-01-23T07:28:09Z"
"","51","HADOOP-11149","zk failover controller test improvements -maybe this'll stop jenkins failing so often","closed","","steveloughran","2015-11-22T18:07:12Z","2015-11-23T12:31:44Z"
"","331","YARN-7773. Fix YARN Federation used Mysql as state store throw except…","YARN-7773 YARN Federation used Mysql as state store throw exception, Unknown column 'homeSubCluster' in 'field list'   #331   submitApplication appIdapplication_1516277664083_0014 try #0 on SubCluster cluster1 , queue: root.bdp_federation [2018-01-18T23:25:29.325+08:00] [ERROR] store.impl.SQLFederationStateStore.logAndThrowRetriableException(FederationStateStoreUtils.java 158) [IPC Server handler 44 on 8050] : Unable to insert the newly generated application application_1516277664083_0014 com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list' at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.Util.getInstance(Util.java:408) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013) at com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104) at com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418) at com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887) at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61) at com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java) at org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source) at org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345) at org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334) at org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196) at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218) at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070) [2018-01-18T23:25:29.326+08:00] [ERROR] server.router.RouterServerUtil.logAndThrowException(RouterServerUtil.java 55) [IPC Server handler 44 on 8050] : Unable to insert the ApplicationId application_1516277664083_0014 into the FederationStateStore org.apache.hadoop.yarn.server.federation.store.exception.FederationStateStoreRetriableException: Unable to insert the newly generated application application_1516277664083_0014 at org.apache.hadoop.yarn.server.federation.store.utils.FederationStateStoreUtils.logAndThrowRetriableException(FederationStateStoreUtils.java:159) at org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:593) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy31.addApplicationHomeSubCluster(Unknown Source) at org.apache.hadoop.yarn.server.federation.utils.FederationStateStoreFacade.addApplicationHomeSubCluster(FederationStateStoreFacade.java:345) at org.apache.hadoop.yarn.server.router.clientrm.JDFederationClientInterceptor.submitApplication(JDFederationClientInterceptor.java:334) at org.apache.hadoop.yarn.server.router.clientrm.RouterClientRMService.submitApplication(RouterClientRMService.java:196) at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:218) at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:419) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2076) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2072) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1803) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2070) Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'homeSubCluster' in 'field list' at sun.reflect.GeneratedConstructorAccessor15.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at com.mysql.jdbc.Util.handleNewInstance(Util.java:425) at com.mysql.jdbc.Util.getInstance(Util.java:408) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527) at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680) at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2484) at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079) at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013) at com.mysql.jdbc.PreparedStatement.executeLargeUpdate(PreparedStatement.java:5104) at com.mysql.jdbc.CallableStatement.executeLargeUpdate(CallableStatement.java:2418) at com.mysql.jdbc.CallableStatement.executeUpdate(CallableStatement.java:887) at com.zaxxer.hikari.pool.ProxyPreparedStatement.executeUpdate(ProxyPreparedStatement.java:61) at com.zaxxer.hikari.pool.HikariProxyCallableStatement.executeUpdate(HikariProxyCallableStatement.java) at org.apache.hadoop.yarn.server.federation.store.impl.SQLFederationStateStore.addApplicationHomeSubCluster(SQLFederationStateStore.java:547) ... 20 more","closed","","Yiran-wu","2018-01-18T16:26:22Z","2018-04-17T08:15:44Z"
"","239","YARN-6674 Add memory cgroup settings for opportunistic containers","YARN-6674 Add memory cgroup settings for opportunistic containers","closed","","szegedim","2017-06-19T16:53:32Z","2017-06-19T21:43:47Z"
"","240","YARN-6674 Add memory cgroup configurations for opportunistic containers","YARN-6674","closed","","szegedim","2017-06-19T21:32:15Z","2018-03-05T00:15:02Z"
"","232","YARN-6673 Add cpu cgroup configurations for opportunistic containers","YARN-6673 Add cpu cgroup configurations for opportunistic containers","closed","","szegedim","2017-06-15T00:07:24Z","2018-03-05T00:15:53Z"
"","227","YARN-6668","YARN-6668 Measure CPU and memory usage with cgroups in ContainersMonitorImpl","closed","","szegedim","2017-06-07T02:06:07Z","2018-03-05T00:17:08Z"
"","241","YARN-6668","YARN-6668","closed","","szegedim","2017-06-19T21:36:55Z","2017-08-08T20:41:38Z"
"","200","YARN-6302 Fail the node, if Linux Container Executor is not configured properly","YARN-6302 Fail the node, if Linux Container Executor is not configured properly","closed","","szegedim","2017-03-08T20:47:22Z","2018-03-05T00:16:22Z"
"","201","YARN-5829 Reserve containers for preemptors","YARN-5829 Reserve containers for preemptors. The change does the following. 1. When a container is selected for preemption, it stores the preemptor next to the container. 2. When a container is finally completed it creates a reservation for the preemptor application. 3. On the next node update we check first reservations, then preemption reservations. They are satisfied before any other request, even if those have higher priorities. This is used tho avoid stealing preempted containers. 4. It also disables standard reservations because I noticed that the preempted applications reserve nodes for themselves stealing back the preempted resources. 5. If a preemptor is deleted before the preemption or after the preemption, the assignment won't happen at node update. 6. If more containers were preempted to satisfy partial reservations, delete the reservation, if no more resource is needed.","closed","","szegedim","2017-03-11T01:52:38Z","2017-04-04T02:23:48Z"
"","27","Merge pull request #1 from apache/trunk","YARN-3999. RM hangs on draing events. Contributed by Jian He","closed","","mahesh-maximus","2015-08-12T06:57:32Z","2015-08-12T06:57:59Z"
"","47","YARN-3477 TimelineClientImpl swallows exceptions","YARN-3477 timeline diagnostics: add more details on why things are failing, including stack traces (at debug level sometimes)","closed","","steveloughran","2015-11-18T22:26:19Z","2021-10-15T19:46:49Z"
"","65","YARN-1564 add some basic workflow YARN services","YARN-1564 add some basic workflow YARN services","closed","","steveloughran","2016-01-08T17:39:31Z","2021-10-15T19:46:38Z"
"","163","HADOOP-13227 outputstream specification","WiP specification of output streams.  Alongside the docs, this will include tests and tightening of behaviours of output streams which don't comply with the `java.io.OutputStream` specification or other parts of the specification (i.e. the extra methods are inconsistent with HDFS). Object stores are special; their behaviours will be documented and (unreconcilable) differences with filesystems handled in tests.  Note that HDFS does not currently follow `java.io.OutputStream.write(int)`'s required behaviour: it will *not* fail on a write on a closed stream. This will have to be corrected","closed","","steveloughran","2016-11-18T12:36:06Z","2019-03-06T21:23:10Z"
"","414","Prevent OOM on array size over VM limit","When we try to read text file line over the VM limit (current Hotspot VM limit is 2^31-m where m is a small number ), the VM throws  java.lang.OutOfMemoryError: Requested array size exceeds VM limit.  This commit is to avoid this error by throwing IOException instead so  that the caller can handle.  Reference: https://plumbr.io/outofmemoryerror/requested-array-size-exceeds-vm-limit","open","","yingsu00","2018-09-10T09:51:42Z","2019-09-03T05:34:14Z"
"","330","YARN PROXYSERVER throw IOEXCEPTION","When we more than ten users simultaneously submitted to view the proxyserver, there will be stuck, and then it will throw IO exception","open","","wuzhilon","2018-01-18T09:37:51Z","2021-04-19T05:44:30Z"
"","442","YARN-9065.App's diags is too long,written zk error","When use ZKRMStateStore to store app info, App's diags is too long, written zk error.  The zk error log:  `2018-11-27 15:54:30,208 [myid:1] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@373] - Exception causing close of session 0x36753e378030000 due to java.io.IOException: Len error 8603591`","closed","","hunshenshi","2018-11-27T09:40:13Z","2019-02-04T05:56:59Z"
"","467","[HDFS-14208]fix bug that there is some a large number missingblocks after failove to active","when the cluster is very large, Standby startup takes about 1 hour. Standby starts and exits the safe mode, and failover to active, a large number of missingblocks often appear. And it takes about 6 hours before missingblocks gradually disappear.  [HDFS-14208](https://issues.apache.org/jira/browse/HDFS-14208)","closed","","ZanderXu","2019-01-16T02:41:31Z","2019-08-19T23:10:57Z"
"","106","HADOOP-11823: dont check for verifier in RpcDeniedReply","When RPC returns a denied reply, the code should not check for a verifier. It is a bug as it doesn't match the RPC protocol. (See Page 33 from NFS Illustrated book).","open","","pradeep1288","2016-06-24T22:06:45Z","2019-09-03T07:02:29Z"
"","185","Fix ""E: Unable to locate package software-properties-common"" error.","When executing `./start-build-env.sh` command, the output is:  ``` Sending build context to Docker daemon 9.216 kB Step 1/18 : FROM ubuntu:trusty trusty: Pulling from library/ubuntu c60055a51d74: Pull complete 755da0cdb7d2: Pull complete 969d017f67e6: Pull complete 37c9a9113595: Pull complete a3d9f8479786: Pull complete Digest: sha256:8f5f12335124c1b78e4cf2f8860d395f75ba279bae70a3c18dd470e910e38ec5 Status: Downloaded newer image for ubuntu:trusty  ---> b969ab9f929b Step 2/18 : WORKDIR /root  ---> 9ec52cfdcff6 Removing intermediate container f3b5b3ad6552 Step 3/18 : RUN apt-get install -y software-properties-common  ---> Running in 3311c06d0ece Reading package lists... Building dependency tree... Reading state information... E: Unable to locate package software-properties-common The command '/bin/sh -c apt-get install -y software-properties-common' returned a non-zero code: 100 ```  It's because there is no `apt-get update` before executing `apt-get install -y software-properties-common` in `dev-support/docker/Dockerfile` file:  ``` FROM ubuntu:trusty  WORKDIR /root  # Add ""RUN apt-get update"" here to fix the problem. RUN apt-get update RUN apt-get install -y software-properties-common RUN add-apt-repository -y ppa:webupd8team/java RUN apt-get update ... ```","closed","","MonsterSupreme","2017-02-04T16:32:18Z","2019-01-17T07:47:30Z"
"","111","MAPREDUCE-6729. Accurately compute the test execute time in DFSIO","When doing DFSIO test as a distributed i/o benchmark tool. Then especially writes plenty of files to disk or read from, both can cause performance issue and imprecise value in a way. The question is that existing practices needs to delete files when before running a job and that will cause extra time consumption and furthermore cause performance issue, statistical time error and imprecise throughput while the files are lots of. So we need to replace or improve this hack to prevent this from happening in the future.","closed","","zhangminglei","2016-07-07T09:06:23Z","2016-07-08T07:44:51Z"
"","439","YARN-8833 fix compute shares may  lock the scheduling process","When compute fair share, there may be a chance triggering the problem of Integer overflow, and entering an infinite loop, which blocks the scheduling process.","open","","yoelee","2018-11-15T07:43:29Z","2019-04-08T23:58:17Z"
"","33","Showing the path is empty message in ls command","When a user lists an empty folder with 'hadoop fs -ls /tmp/emptyfolder', a message will appear that says 'The path is empty'. Otherwise, nothing will appear and the user does not know it has been done or not.","open","","MobinRanjbar","2015-09-15T10:43:37Z","2022-01-31T20:50:57Z"
"","361","HADOOP-13617 : Retry requests with updated authentication","We started using hadoop's openstack filesystem (Swift) as a storage backend for Apache Flink and ran into the problem that every few hours an error would be triggered that would cause our flink jobs to get restarted.  The problem happens when the token that is used to make api calls expires. At that point the server will return a 401 status code (unauthorized).   What should happen next is that the token should be refreshed and the request should be retried with the new token.  But what happens is that while a new token is requested , the request is retried with the old token and will always fail.  The fix is a simple one, set the correct authentication header on the request before retrying  It turns out that this issue had already been reported in 2016 and a patch was provided by the original reporter.  However for some reason the patch was never merged.  Because the code has changed a bit since the patch was created, i took the liberty of adapting it so it compiles cleanly on the current trunk. I also removed the test the original author created because it basically did a sleep until  a token expired. (which usually takes hours)","open","","jelmerk","2018-04-01T16:57:58Z","2019-09-03T04:30:39Z"
"","83","HDFS-9941. Do not log StandbyException on NN, other minor logging fixes.","v1 patch includes the following fixes: # Suppress StandbyException log messages for NameNode. # {{saveAllocatedBlock}} logs the block locations (DN xfer addresses). # {{logBlockReplicationInfo}} logs to the blockStateChangeLog instead of {{DecomissionManager#LOG}}. Also added a log level guard.","closed","","arp7","2016-03-11T00:56:32Z","2019-01-24T10:35:04Z"
"","219","YARN-6457  use existing conf object as a resource for sslConf object in WebApps for the builder to use in HttpServer2","use the passed conf object as a resource in local sslConf so it can be overridden","closed","","sanjaypujare","2017-05-08T06:50:02Z","2019-07-26T12:25:21Z"
"","114","MAPREDUCE-6730 Use StandardCharsets instead of String overload","Use String.getBytes(StandardCharsets.UTF_8) instead of String.getBytes(String).","closed","","SahilKang","2016-07-11T04:36:55Z","2016-08-04T11:18:17Z"
"","312","YARN-7641. Make logs select boxes searchable","Use select2 on all the select boxes in logs page","closed","","skmvasu","2017-12-12T10:40:30Z","2017-12-13T10:25:45Z"
"","323","YARN-7750. Use local time to render dates","Use local time to render dates","open","","skmvasu","2018-01-09T06:54:40Z","2019-04-08T23:57:56Z"
"","20","YARN-3678","use 'ps' command to check the process before it is killed","closed","","Guchige","2015-05-28T01:40:27Z","2019-07-26T11:19:52Z"
"","449","HDFS-14158. Fix the Checkpointer not to ignore the configured ""dfs.namenode.checkpoint.period"" > 5 minutes","URL: https://issues.apache.org/jira/browse/HDFS-14158","closed","","tiwalter","2018-12-18T13:22:31Z","2019-02-04T04:52:46Z"
"","460","Hadoop-15994","upgrade jackson2 version to 2.9.8","closed","","lqjack","2019-01-09T10:50:05Z","2019-03-28T23:59:45Z"
"","183","HADOOP-13075 adding sse-c and sse-kms","Updating fedecz changes to work with changes in trunk.  Refactored methods to be clearer as to purpose and added more integration tests.","closed","","frozenwizard","2017-01-25T16:49:20Z","2017-02-13T15:48:27Z"
"","85","HADOOP-9991. Update netty to 3.7.1.Final to sync with zookeper","Update netty to 3.7.1.Final because hadoop-client 2.7.2 depends on zookeeper 3.4.6 which depends on netty 3.7.x. Related to https://github.com/apache/hadoop/pull/84","closed","","ceefour","2016-03-16T13:22:56Z","2019-07-26T11:18:03Z"
"","26","Merge pull request #1 from apache/trunk","update local repository","closed","","airbots","2015-08-11T18:53:19Z","2015-08-11T18:53:48Z"
"","4","Update TaskInputOutputContext.java javadoc","Update javadoc to reflect the current code structure.","open","","sebastiancadena","2014-09-25T16:37:36Z","2021-08-04T16:37:45Z"
"","112","MAPREDUCE-6729. Accurately compute the test execute time in DFSIO","Update github-side PR to works well.","closed","","zhangminglei","2016-07-08T07:23:39Z","2016-07-30T13:15:22Z"
"","189","Merge pull request #1 from apache/trunk","update from origin","closed","","jeanzhou","2017-02-07T02:07:48Z","2019-07-26T11:28:12Z"
"","488","HDDS-1114. Fix findbugs/checkstyle/accepteance errors in Ozone","Unfortunately as the previous two big commits (error handling HDDS-1068, checkstyle HDDS-1103) are committed in the same time a few new errors are introduced during the rebase.  This patch will fix the remaining 5 issues (+ a type in the acceptance test executor)   See: https://issues.apache.org/jira/browse/HDDS-1114","closed","ozone,","elek","2019-02-15T12:43:07Z","2019-02-15T19:49:53Z"
"","548","Revert ""HDDS-1072. Implement RetryProxy and FailoverProxy for OM clie…","Trunk seems to be broken by HDDS-1072.  Let's check this revert commit with jenkins, to be sure this is the problem.","closed","ozone,","elek","2019-03-04T11:20:33Z","2019-03-04T23:52:17Z"
"","184","Total Throughput Calculation Error in TestDFSIO","Total throughput should be toMB(size) * 1000.0 / ((float) execTime)","closed","","DwyaneShi","2017-01-27T21:32:00Z","2017-02-01T17:58:27Z"
"","211","Added support for BasicSessionCredentials","This version does not support AWS authentication using session token this pull request adds this functionality","closed","","martoc","2017-04-11T17:10:03Z","2019-02-16T22:05:52Z"
"","646","HADOOP-16085: use object version or etags to protect against inconsistent read after replace/overwrite","This started with [HADOOP-16085-003.patch](https://issues.apache.org/jira/secure/attachment/12962649/HADOOP-16085-003.patch) from [the JIRA](https://issues.apache.org/jira/browse/HADOOP-16085).  I'm switching over to a PR instead of using patch files attached to the JIRA.  I expect that will make review easier.  I've addressed a few things since that patch: * copy exception handling - handling 412 error on the response * addressed [Gabor's comments](https://issues.apache.org/jira/browse/HADOOP-16085?focusedCommentId=16797173&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16797173) on  TestPathMetadataDynamoDBTranslation, TestDirListingMetadata * fixed a problem I introduced around inconsistency between PathMetadata.isEmptyDir and the underlying S3AFileStatus.isEmpyDir that was manifesting in failures to clean up files after tests * increased default LocalMetadataStore cache timeout as the low 10 second default was making debugging some failing tests confusing as the outcome would depend on how quickly I went through breakpoints * fixed S3 Select test in ITestS3ARemoteFileChanged and added test for copy/rename * improved documentation  I haven't actually run all the tests again since these changes.  Also, I think there might be a couple more tests to add or alter.  For example, I don't have an explicit integration test yet to read a file that has no ETag or versionId in S3Guard.  I'll make another pass through but figured it is worthwhile to post the progress.","closed","fs/s3,","ben-roling","2019-03-26T21:58:49Z","2019-04-01T17:01:47Z"
"","514","Revert ""HDDS-1141. Update DBCheckpointSnapshot to DBCheckpoint.""","This reverts commit d33f0666f66e40ddcf453705566e64d5bfaf684a.","closed","","bharatviswa504","2019-02-21T21:37:41Z","2019-02-21T21:37:53Z"
"","56","MAPREDUCE-6475 cleanup","This rebase on trunk cleans up `TestRMContainerAllocator.testAttemptNotFoundCausesRMCommunicatorException`  MAPREDUCE-6439 has probably fixed the underlying problem; all that's left of this patch is a stop of the mock rm; minor","closed","","steveloughran","2015-11-23T12:42:57Z","2015-11-23T14:56:39Z"
"","431","HDFS-13997. Add moment.js to the secondarynamenode web UI","This pull request fix a problem on the web UI of secondarynamenode. As the UI won't show when dfs-dust.js try to call moment() and it is not included in the page.","open","","pixelart7","2018-10-22T15:50:47Z","2019-08-20T16:08:10Z"
"","585","HDDS-1138. Ozone Client should avoid talking to SCM directly","This PR is build based on @mukul1987 's initial patch to optimize write path only.  The difference are * persist pipeline info into OM keylocation metadata * support both read and write without client talking to scm * There are limitations in this PR when the pipeline changes at SCM side. I will send additional code changes to support refresh pipeline info for read. (Basic idea documented in the 2nd commits).","closed","ozone,","xiaoyuyao","2019-03-09T21:59:44Z","2019-03-15T17:41:06Z"
"","678","AppsFlyer release for Hadoop 2.7.3","This PR creates a new branch called af-release-2.7.3 (branching out from rel/release-2.7.3) which will act as a master branch for our future local changes of Hadoop 2.7.3.","closed","","nenorbot","2019-04-02T11:59:09Z","2019-04-02T12:02:57Z"
"","629","HDDS-1302. Fix SCM CLI does not list container with id 1","This PR aims to avoid the contradiction with JavaDoc after the fix in HDDS-1263.  https://issues.apache.org/jira/browse/HDDS-1302","closed","","vivekratnavel","2019-03-20T17:43:30Z","2019-03-22T22:15:02Z"
"","299","Support Log4j 2 and Logback","This patch relates to https://issues.apache.org/jira/browse/HADOOP-12956. It makes the logging implementation in hadoop common optional and provides support for event counters in Log4j 2 and Logback in addition to Log4j 1.","closed","","rgoers","2017-11-26T07:07:03Z","2019-07-19T21:04:04Z"
"","196","YARN-6194. Cluster capacity in SchedulingPolicy is updated only on allocation file reload","This patch passes the ClusterNodeTracker instead of ClusterResource into the DRF policy.","closed","","flyrain","2017-02-18T00:40:19Z","2017-04-04T16:43:21Z"
"","579","HDDS-761. Create S3 subcommand to run S3 related operations","This patch is to move all S3 related subcommands from `ozone sh s3` to `ozone s3`.  ozone sh s3 getsecret -> **ozone s3 getsecret** ozone sh bucket path `` -> **ozone s3 path ``**  https://issues.apache.org/jira/browse/HDDS-761","closed","ozone,","vivekratnavel","2019-03-08T19:21:19Z","2019-03-15T04:17:00Z"
"","136","HADOOP-12774 use UGI.currentUser for user and group of s3a objects","This patch grabs the UGI current user shortname in the FS initialize call, then uses that as the user and group for all filestatus instances generated.  ``` $ ./hadoop fs -ls s3a://hwdev-steve-ireland/ Found 1 items drwxrwxrwx   - stevel stevel          0 2016-10-07 17:29 s3a://hwdev-steve-ireland/tests ```","closed","","steveloughran","2016-10-07T16:33:18Z","2016-11-01T10:21:04Z"
"","654","HADOOP-15183 S3Guard store becomes inconsistent after partial failure of rename","This patch carefully pulls out the changes into a new class, s3a.impl.MultiObjectDeleteSupport which isolates this from everything else, and is designed for testing in isolation.      Tests are still WiP; I've pulled the partial rename failure ones out ITestAssumeRole into isolation here, because the fact that they use assumed roles is a detail to set up the failures.      It currently only does the delete failure, but I'll probably include the copy failures in here as they are so related.","closed","fs/s3,","steveloughran","2019-03-28T13:54:08Z","2019-05-29T19:10:44Z"
"","647","HADOOP-16118. S3Guard to support on-demand DDB tables.","This patch adds awareness of on-demand tables; it does not support creating them as a new SDK upgrade is needed for that. This patch is one which can be backported without the consequences of such an update.  * The diagnostics map includes the billing mode (as inferred from IO capacities) * Set capacity fails fast (and always). * The documentation discusses the mode, argues for it over autoscaling * Example output of bucket-info updated * Test that if the table is on-demand, set-capacity will fail. * If table is on-demand, The dynamo db scale tests are disabled. There's nothing to prove.  Change-Id: I77b7a6b593a2cd805376ca24d68b06bde75589c5","closed","fs/s3,","steveloughran","2019-03-26T22:50:21Z","2019-06-05T22:00:29Z"
"","402","MAPREDUCE-7121. add Abortable so that we can abort output","This patch adds abortable to mapreduce outputs  In the case of using a remote filesystem with mapreduce, upon task attempt failure we ensure the file output stream gets aborted before being closed which prevents partial results from being uploaded.","open","","bschell","2018-07-10T21:24:26Z","2019-09-03T04:09:05Z"
"","283","HADOOP-14965 s3a input stream ""normal"" fadvise mode to be adaptive","This makes the {{S3AInputStream.inputPolicy}} non-final, and on the first backwards seek on a Normal input, switches it to Random (logging @ info in the process). If seeks are forward(), it just skips forwards, as sequential input does.  The input stream instrumentation counts the #of times the policy was changed (including the first), and the current value, where it is picked up in tests (so there's no need to add a test accessor as an input stream feature itself).   The test `ITestS3AInputStreamPerformance.testRandomIONormalPolicy` broke as the instrumentation showed only 1 TCP abort, not 4. This is a success, as it shows the policy is adapting.","closed","","steveloughran","2017-10-23T20:34:44Z","2021-10-15T19:47:19Z"
"","61","YARN-4430 registry security validation can fail when downgrading to insecure would work","This makes the check -> warn, leaving ZK to decide how to handle the problem","closed","","steveloughran","2015-12-07T17:55:23Z","2021-10-15T19:46:41Z"
"","674","HADOOP-16210. Update guava to 27.0-jre in hadoop-project trunk","This is without the hadoop-tools modification (it will timeout on jenkins yetus after 5 hours, so I tested it manually.)","closed","","bgaborg","2019-04-01T14:03:59Z","2019-04-05T13:40:35Z"
"","204","HADOOP-13371 S3a Globber, WiP","This is what I'd done up to the point I stopped looking at it; copies over the FileSystem.globber and then adds an initial scale test.   1. I think we could do a great scale test against the landsat repo 1. I chose not to try and tweak the FileSystem.globber class for subclassing, so we can do much more in here, and to set things up for having a globStatus call which would return a remote iterator rather than an array","closed","","steveloughran","2017-03-18T18:35:34Z","2018-02-12T13:11:37Z"
"","316","HADOOP-15124. Improve FileSystem.Statistics performance","This is PR for https://issues.apache.org/jira/browse/HADOOP-15124","open","","medb","2017-12-17T17:25:31Z","2022-06-15T13:26:17Z"
"","685","HADOOP-16233. S3AFileStatus to declare that isEncrypted() is always true","This is needed to fix up some confusion about caching of job.addCache() handling of S3A paths; all parent dirs -the files are downloaded by the NM without  using the DTs of the user submitting the job. This means that when you submit jobs to an EC2 cluster with lower IAM permissions than the user, cached resources don't get downloaded and the job doesn't start.  Production code changes: * S3AFileStatus Adds ""true"" to the superclass's encrypted flag during construction.  Tests * Base AbstractContractOpenTest can control whether zero byte files created in tests are encrypted. Not done via an XML attribute, just a subclass point. Thoughts? * Verify that the filecache considers paths to not have the permissions which trigger reduce-privilege downloads * And extend ITestDelegatedMRJob to test a completely different bucket (open street map), to verify that cached resources do get their tokens picked up  Docs: * Advise FS developers to say all files are encrypted. It's otherwise harmless and it'll stop other people seeing impossible to debug error messages on app launch.  Contributed by Steve Loughran.  Change-Id: Ifaae4c9d735ccc5eafeebd2584b65daf2d4e5da3","closed","fs/s3,","steveloughran","2019-04-03T18:11:17Z","2019-04-03T22:03:13Z"
"","54","HADOOP-12321","This is HADOOP-12321-005-aggregated applied as is to trunk;","closed","","steveloughran","2015-11-22T18:44:46Z","2016-11-01T10:22:15Z"
"","142","HDFS-11012. Unnecessary INFO logging on DFSClients for InvalidToken.","This is for JIRA: https://issues.apache.org/jira/browse/HDFS-11012  The change merely switches the log level while also changing the string to use the SLF4J formatter to reduce the string append cost when unnecessary.","closed","","QwertyManiac","2016-10-14T07:42:14Z","2016-10-15T13:16:21Z"
"","289","[YARN-6483] Add nodes transitioning to DECOMMISSIONING state to the list of updated nodes returned by the Resource Manager as a response to the Application Master heartbeat","This is an alternative approach to https://issues.apache.org/jira/browse/YARN-3224 for notifying all affected application masters when a node transitions into the DECOMMISSIONING state. This change modifies the AllocateResponse that the YARN Resource Manager uses to respond to heartbeat request from application masters, to add any node that has transitioned to DECOMMISSIONING state since the last heartbeat to the list of NodeReport objects that is part of the AllocateResponse object. We also add a new field to each NodeReport to add the decommission timeout for DECOMMISSIONING nodes, thus covering the same functionality of the original proposal in YARN-3224.","closed","","juanrh","2017-11-07T18:59:48Z","2017-11-27T19:29:21Z"
"","40","HADOOP-11919 Testing Github integration","This is a test.","closed","","omalley","2015-10-30T17:07:03Z","2015-10-30T17:21:36Z"
"","661","HDDS-976: Parse network topology from yaml file","This is a early stage patch to support load HDFS schema from a YAML file.   It should need more unit tests.","closed","ozone,","chenjunjiedada","2019-03-29T09:39:43Z","2019-04-18T23:53:26Z"
"","48","HADOOP-12587 Hadoop auth token refused to work without a maxinactive attribute in issued token","This initial patch is not a fix. This is the patch to diagnose why I cannot submit work to a secure cluster and so identify the change that actually broke things.  I would propose that the max-inactive value is considered optional, defaulting to -1 as the code itself appears to do elsewhere.","closed","","steveloughran","2015-11-19T15:04:23Z","2021-10-15T19:46:44Z"
"","145","Steve's patch for '13680 get long bytes","This for HADOOP-13680; just naming it so that yetus does _not_ give it priority over the other PR","closed","","steveloughran","2016-10-24T10:05:27Z","2016-11-01T10:20:14Z"
"","311","[HADOOP-15096] Don't create user with lastlog","This fixes a problem where in certain cases, the useradd command can create a very large diff that can blow up the host disk size.  The reason for this is that lastlog is a sparse file, but AUFS under docker apparently doesn't deal with those well and creates a very large file.","open","","addisonj","2017-12-07T06:07:39Z","2019-04-08T23:57:53Z"
"","272","HDFS-12424 Fix sorting of the datatable on the Datanodes page in the NN UI","This fix basically comes down to three things:  - type: numeric isn't a thing -- it needs to be type num per the [datatable doc](https://datatables.net/reference/option/columns.type) - orderData was wrong in both instances; we don't want those columns to sort by some other column - version sorting by anything other than string is going to be even more confusing than it is if we sort it by string","closed","","shawnam","2017-09-12T18:08:43Z","2017-09-12T18:50:46Z"
"","568","HADOOP-15691 Add PathCapabilities to FS and FC to complement StreamCapabilities","This contains all the work to date, reapplied to trunk so that Yetus will take up the PR again.  Change-Id: Icfab71d177c88c75fd651d7eea56d3d1e8618f61","closed","","steveloughran","2019-03-07T10:52:24Z","2019-10-01T16:07:21Z"
"","81","Shell.java: fix absolute path `/bin/ls`","This commit uses `ls` from PATH instead of relying on `ls` being stored in `/bin/`. The only file according to the POSIX standard which must be stored in `/bin/` is `sh`. This fixes issues plaguing distributions like NixOS which dynamically stitch together a PATH.  This fix is loosly related to https://issues.apache.org/jira/browse/HADOOP-11935 and more specifically related to https://mail-archives.apache.org/mod_mbox/hadoop-user/201512.mbox/%3CCAH2nEUgsSeoJTJkD4T8z=D18ECnRgQ3Qvz861gU5+NPSsNgT=A@mail.gmail.com%3E","closed","","makefu","2016-02-29T08:08:23Z","2016-07-11T20:59:28Z"
"","413","Adding EC2ContainerCredentialsProviderWrapper to the credential providers","This change adds EC2ContainerCredentialsProviderWrapper to the credential providers list.  It will enable hadoop-aws library to be used on AWS fargate.","open","","nrworld","2018-09-07T19:59:46Z","2019-04-08T23:58:13Z"
"","14","YARN-3126. FairScheduler: queue's usedResource is always more than the maxResource limit.","This change add a check, for whether queue's usedResource may run over its maxResource Limit after a new resource allocation.","open","","Xia-Hu","2015-03-09T06:35:39Z","2019-04-08T23:57:24Z"
"","399","MAPREDUCE-7120. Make hadoop consider wildcard host as datalocal","This allows hadoop to treat ""*"" as data local. This allows remote filesystems to increase performance by skipping retrys for data locality by returning ""*"" as the block host when the filesystem is asked for the block location.","open","","bschell","2018-07-05T22:05:33Z","2021-07-27T16:34:41Z"
"","7","YARN-1964 Launching containers from docker","This adds a new ContainerExecutor called DockerContainerExecutor. This executor launches a container in a docker container, providing a full filesystem namespace and software isolation for the container.","closed","","ashahab-altiscale","2014-10-06T18:55:09Z","2019-01-24T10:47:12Z"
"","6","YARN-1964 Launching containers from docker","This adds a new ContainerExecutor called DockerContainerExecutor. This executor launches a container in a docker container, providing a full filesystem namespace and software isolation for the container.","closed","","ashahab-altiscale","2014-09-29T02:35:16Z","2019-01-24T10:47:27Z"
"","2","YARN-1964 Launching containers from docker","This adds a new ContainerExecutor called DockerContainerExecutor. This executor launches a container in a docker container, providing a full filesystem namespace and software isolation for the container.","closed","","ashahab-altiscale","2014-09-22T21:37:24Z","2014-09-29T02:35:34Z"
"","314","POC: replace explicit method parameters null-checks by a declarative approach","This *PR* shows an approach when explicit *null*-checks (*Preconditions.checkNotNull()*) are generated automatically by the [Traute](http://traute.oss.harmonysoft.tech/) *javac* plugin for method parameters marked by *Nonnull* annotation.    Example: consider the [FSDataOutputStreamBuilder.permission()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java#L151) - its bytecode looks like if it's compiled from the source below:    ```java public B permission(@Nonnull final FsPermission perm) {     if (perm == null) {         throw new NullPointerException(""String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it"");     }     Preconditions.checkNotNull(perm);     permission = perm;     return getThisBuilder(); } ```    Details:    ``` javap -c ./hadoop-common-project/hadoop-common/target/classes/org/apache/hadoop/fs/FSDataOutputStreamBuilder.class ...   public B permission(org.apache.hadoop.fs.permission.FsPermission);     Code:        0: aload_1        1: ifnonnull     14        4: new           #16                 // class java/lang/NullPointerException        7: dup        8: ldc           #31                 // String Argument 'perm' of type FsPermission (#0 out of 1, zero-based) is marked by @javax.annotation.Nonnull but got null for it       10: invokespecial #18                 // Method java/lang/NullPointerException."""":(Ljava/lang/String;)V       13: athrow       14: aload_0       15: aload_1       16: putfield      #3                  // Field permission:Lorg/apache/hadoop/fs/permission/FsPermission;       19: aload_0       20: invokevirtual #32                 // Method getThisBuilder:()Lorg/apache/hadoop/fs/FSDataOutputStreamBuilder;       23: areturn ```    So, the idea is to do the following:   1. Go through the project's codebase and find all places where *Preconditions.checkNotNull()* is called for method parameter 2. Ensure that target method parameter is marked by the *Nonnull* annotation (e.g. [ActiveStandbyElector.isStaleClient()](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java#L1124) is not marked by it, so, we need to add the annotation) 3. Remove *Preconditions.checkNotNull()* call    Benefits:   * the code becomes cleaner without that explicit checks * the code is better documented as it's immediately clear what method parameters must be not-*null* * IDEs highlight possible *NPE* for method parameters marked by *Nonnull* annotations  Please let me know if you like the idea, I'm fine with providing a *PR* which applies the solution to the whole project's codebase then.  TESTED: mvn clean package & ensured that resulting         bytecode has the checks","open","","denis-zhdanov","2017-12-13T08:18:04Z","2019-02-06T11:25:27Z"
"","407","MAPREDUCE-7127. Add aggregated webservice endpoints to fetch all tasks & their taskAttempts","There is a usecase to poll Hadoop for various running Tasks and display info on each individual task attempt to the user. To improve performance add additional endpoints to HS and AM webservices that will fetch aggregated data on hadoop tasks including their task attempts for a current job.","open","","bschell","2018-08-03T22:32:40Z","2019-04-08T23:58:13Z"
"","388","Minor spell mistake in comment : Line number 101","There is a minor spell mistake on line number 100, where future is misspelled as ""futute"" under ""message AclFeatureProto"" section. Here is code snippet:  message AclFeatureProto {     /**      * An ACL entry is represented by a 32-bit integer in Big Endian      * format. The bits can be divided in four segments:      * [0:2) || [2:26) || [26:27) || [27:29) || [29:32)      *      * [0:2) -- reserved for future uses.      * [2:26) -- the name of the entry, which is an ID that points to a      * string in the StringTableSection.      * [26:27) -- the scope of the entry (AclEntryScopeProto)      * [27:29) -- the type of the entry (AclEntryTypeProto)      * [29:32) -- the permission of the entry (FsActionProto)      *      */     repeated fixed32 entries = 2 [packed = true];   } message AclFeatureProto { /** * An ACL entry is represented by a 32-bit integer in Big Endian * format. The bits can be divided in four segments: * [0:2) || [2:26) || [26:27) || [27:29) || [29:32) * * [0:2) -- reserved for futute uses. * [2:26) -- the name of the entry, which is an ID that points to a * string in the StringTableSection. * [26:27) -- the scope of the entry (AclEntryScopeProto) * [27:29) -- the type of the entry (AclEntryTypeProto) * [29:32) -- the permission of the entry (FsActionProto) * */","closed","","sandeepksaini","2018-06-01T20:50:40Z","2019-08-20T20:36:24Z"
"","681","HDDS-1353 : Metrics scm_pipeline_metrics_num_pipeline_creation_failed keeps increasing because of BackgroundPipelineCreator.","There is a BackgroundPipelineCreator thread in SCM which runs in a fixed interval and tries to create pipelines. This BackgroundPipelineCreator uses IOException as exit criteria (no more pipelines can be created). In each run of BackgroundPipelineCreator we exit when we are not able to create any more pipelines, i.e. when we get IOException while trying to create the pipeline. This means that scm_pipeline_metrics_num_pipeline_creation_failed value will get incremented in each run of BackgroundPipelineCreator.","closed","ozone,","avijayanhwx","2019-04-02T17:44:40Z","2019-04-04T13:48:32Z"
"","490","HDDS-1113. Remove default dependencies from hadoop-ozone project","There are two ways to define common dependencies with maven:    1.) put all the dependencies to the parent project and inherit them   2.) get all the dependencies via transitive dependencies  TLDR; I would like to switch from 1 to 2 in hadoop-ozone  My main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.  Using transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them.   See: https://issues.apache.org/jira/browse/HDDS-1113","closed","ozone,","elek","2019-02-15T12:52:36Z","2019-03-06T20:03:49Z"
"","487","HDDS-1113. Remove default dependencies from hadoop-ozone project","There are two ways to define common dependencies with maven:    1.) put all the dependencies to the parent project and inherit them   2.) get all the dependencies via transitive dependencies  TLDR; I would like to switch from 1 to 2 in hadoop-ozone  My main problem with the first approach that all the child project get a lot of dependencies independent if they need them or not. Let's imagine that I would like to create a new project (for example a java csi implementation) It doesn't need ozone-client, ozone-common etc, in fact it conflicts with ozone-client. But these jars are always added as of now.  Using transitive dependencies is more safe: we can add the dependencies where we need them and all of the other dependent projects will use them.   see: https://issues.apache.org/jira/browse/HDDS-1113","closed","ozone,","elek","2019-02-15T09:15:53Z","2019-02-15T12:51:34Z"
"","408","HADOOP-15652. Fix typos SPENGO into SPNEGO","There are some typo words `SPENGO` which should be `SPNEGO`.  Contributed by okumin","closed","","okumin","2018-08-04T16:16:21Z","2019-03-29T12:07:20Z"
"","506","HDDS-1152. Add trace information for the client side of the datanode writes","The XCeiverClients can be traced on the client side to get some information about the time of chunk writes / block puts.  See: https://issues.apache.org/jira/browse/HDDS-1152","closed","ozone,","elek","2019-02-21T09:29:21Z","2019-03-07T09:01:30Z"
"","508","HDDS-1151. Propagate the tracing id in ScmBlockLocationProtocol","The tracing propagation is not yet enabled for the ScmBlockLocationProtocol. We can't see the internal calls between OM and SCM. We need to propagate it at least for the allocateBlock call.  See: https://issues.apache.org/jira/browse/HDDS-1151","closed","ozone,","elek","2019-02-21T10:10:21Z","2019-03-07T09:01:56Z"
"","320","YARN-7684. Fix The Total Memory and VCores display in the yarn UI is …","The Total Memory and VCores display in the yarn UI is not correct with labeled node  Use the cluster resource memory and Vcores info instead of the root queue metrics, availableMB + allocatedMB. and availableVirtualCores + allocatedVirtualCores.","closed","","zhaoyim","2017-12-28T16:03:22Z","2018-01-04T08:28:36Z"
"","164","YARN-5924 - Resource Manager fails to load state with InvalidProtocolBufferException","The solution is to catch ""InvalidProtocolBufferException"", show warning and remove application's folder that contains invalid data to prevent RM restart failure.   Additionally, I've added catch for other exceptions that can appear during recovering of the specific application, to avoid RM failure even if the only one application's state can't be loaded.","closed","","ameks94","2016-11-22T16:03:39Z","2017-11-17T13:28:25Z"
"","252","replication example revised","the placing of the third replica is confusing, is it put on the same rack as the first replica, or are the second and third put on a different replica.","closed","","yonatan-py","2017-07-24T07:51:34Z","2020-07-31T05:16:05Z"
"","37","bail out and avoid to access root in s3a","The original do while loop does not have any effect.","closed","","liufengdb","2015-10-15T20:32:22Z","2018-08-20T17:41:51Z"
"","178","Skip all erased locations","The old one only skips the first erased location and causes following XOR operation to throw NullPointerException.","closed","","DwyaneShi","2017-01-08T05:55:39Z","2017-01-15T17:28:03Z"
"","139","YARN-5721. NPE at AMRMClientImpl.getMatchingRequests","The following NPE was thrown using a Spark 2.1.0-SNAPSHOT (as client) by changing Hadoop dependency to the latest (by the time the ERROR has been generated).  {{2016-10-10 11:33:53,392 ERROR yarn.ApplicationMaster: Uncaught exception: java.lang.NullPointerException at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.getMatchingRequests(AMRMClientImpl.java:668) at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.getMatchingRequests(AMRMClientImpl.java:651) at org.apache.spark.deploy.yarn.YarnAllocator.getPendingAtLocation(YarnAllocator.scala:210) at org.apache.spark.deploy.yarn.YarnAllocator.getPendingAllocate(YarnAllocator.scala:203) at org.apache.spark.deploy.yarn.YarnAllocator.updateResourceRequests(YarnAllocator.scala:318) at org.apache.spark.deploy.yarn.YarnAllocator.allocateResources(YarnAllocator.scala:278) at org.apache.spark.deploy.yarn.ApplicationMaster.registerAM(ApplicationMaster.scala:350) at org.apache.spark.deploy.yarn.ApplicationMaster.runExecutorLauncher(ApplicationMaster.scala:418) at org.apache.spark.deploy.yarn.ApplicationMaster.run(ApplicationMaster.scala:250)}}  We've also pulled the latest code (1 hour ago) from the repository, and ran a test for getMatchingRequests. Same NPE has been encountered.  getMatchingRequests should never throw an NPE even if it has been called right after the client has been started.","closed","","szape","2016-10-11T13:41:30Z","2019-01-23T07:31:52Z"
"","300","[HADOOP-15099] YARN Federation Link fix","The fix is for YARN Federation link on [Apache Hadoop YARN page](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html).  Federation Link was `.Federation.html`, removed the `.`, hence fixing the 404 Error I saw on the site.","closed","","animenon","2017-11-26T19:14:18Z","2018-03-02T05:16:21Z"
"","362","YARN-8057 Inadequate information for handling catch clauses","The description of the problem: https://issues.apache.org/jira/browse/YARN-8057 I added stack traces information to those two logging statements, so that the full exception information can be generated to the logs.","closed","","ginolzh","2018-04-10T15:05:25Z","2019-09-09T05:20:39Z"
"","357","HDFS-13321: Inadequate information for handling catch clauses","The description of the problem: https://issues.apache.org/jira/browse/HDFS-13321 I just added stack traces information to those two logging statements, so that the exception type can be generated to the logs.","closed","","ginolzh","2018-03-23T14:54:19Z","2019-08-27T18:53:10Z"
"","255","Parallel Block Copy for DiskBalancer","The default plan executing is in sequential order,which means each step is executed one by one, because one volume can't be changed by two or more step at the same time. “Anything worth doing is, sooner or later, worth doing concurrently” However, We find a way to execute the plan in parallel,which is not only  much more efficient but also safe.The detail is here: https://github.com/liumihust/Parallel-Block-Copy-HDFS","open","","liumihust","2017-07-27T01:29:50Z","2019-04-08T23:57:45Z"
"","653","HDDS-1333. OzoneFileSystem can't work with spark/hadoop2.7 because incompatible security classes","The current ozonefs compatibility layer is broken by: HDDS-1299.  The spark jobs (including hadoop 2.7) can't be executed any more:  {code} 2019-03-25 09:50:08 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProviderTokenIssuer         at java.lang.ClassLoader.defineClass1(Native Method)         at java.lang.ClassLoader.defineClass(ClassLoader.java:763)         at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)         at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)         at java.net.URLClassLoader.access$100(URLClassLoader.java:74)         at java.net.URLClassLoader$1.run(URLClassLoader.java:369)         at java.net.URLClassLoader$1.run(URLClassLoader.java:363)         at java.security.AccessController.doPrivileged(Native Method)         at java.net.URLClassLoader.findClass(URLClassLoader.java:362)         at java.lang.ClassLoader.loadClass(ClassLoader.java:424)         at java.lang.ClassLoader.loadClass(ClassLoader.java:357)         at java.lang.Class.forName0(Native Method)         at java.lang.Class.forName(Class.java:348)         at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2134)         at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2099)         at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2193)         at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)         at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)         at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)         at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)         at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:45)         at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:332)         at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)         at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)         at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)         at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:757)         at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:724)         at org.apache.spark.examples.JavaWordCount.main(JavaWordCount.java:45)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:498)         at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)         at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)         at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)         at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)         at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)         at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)         at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:933)         at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProviderTokenIssuer         at java.net.URLClassLoader.findClass(URLClassLoader.java:382)         at java.lang.ClassLoader.loadClass(ClassLoader.java:424)         at java.lang.ClassLoader.loadClass(ClassLoader.java:357)         ... 43 more {code}  See: https://issues.apache.org/jira/browse/HDDS-1333","closed","ozone,","elek","2019-03-28T13:46:31Z","2019-04-04T21:00:54Z"
"","507","HDDS-1154. Add tracing to the client side of StorageContainerLocationProtocol and OzoneManagerProtocol","The best way to display the network performance in the tracing server is to start a new span (=save tracing information) on both of the client and and server side.  This issue is about improving the client side tracing of OzoneManager and StorageLocationManager.  The easiest way to turn on tracing for a class which implements an interface is to create a java proxy with TracingUtil.createProxy. With this utility we can execute the required tracing methods (create span/close span) around the original methods without adding any boilerplate code.  Thanks to the current design hadoop rpc calls are create by the *ClientSideTranslatorPB classes which implements the original protocol interface (eg. StorageContainerLocationProtocolClientSideTranslatorPB implements StorageContainerLocationProtocol) which means that it can easily instrumented by TracingUtil.createProxy.  The only thing what we need is to use the interface everywhere (StorageContainerLocationProtocol) instead of the implementation (ClientSideTranslator) as a client.  The only required method which is not published in the ClientSideTranslator is the close method. With adding the close method to the interface (extends Closable) we are able to use the interface everywhere which can be instrumented to send the tracing information.  See: https://issues.apache.org/jira/browse/HDDS-1154","closed","ozone,","elek","2019-02-21T09:50:32Z","2019-03-07T09:01:41Z"
"","569","HDDS-1232 : Recon Container DB service definition.","The attached patch contains the Recon container DB service definition (Interface + Impl). It uses a RocksDB instance to store the container-Key information. The interface is still evolving at this point of time, and maybe updated when the end to end work is done to read the OM DB snapshot and write the container-key data into this DB.  Unit testing done. Manual testing will be done along with HDDS-1233 and HDDS-1234.","closed","ozone,","avijayanhwx","2019-03-07T18:53:22Z","2019-03-08T21:00:20Z"
"","491","HDDS-1116. Add java profiler servlet to the Ozone web servers","Thanks to [~gopalv] we learned that [~prasanth_j] implemented a helper servlet in Hive to initialize new [async profiler|https://github.com/jvm-profiling-tools/async-profiler] sessions and provide the svg based flame graph over HTTP. (see HIVE-20202)  It seems to very useful as with this approach the profiling could be very easy.  This patch imports the servlet from the Hive code base to the Ozone code base with minor modification (to make it work with our servlet containers)   * The two servlets are unified to one  * Streaming the svg to the browser based on IOUtils.copy   * Output message is improved  By default the profile servlet is turned off, but you can enable it with 'hdds.profiler.endpoint.enabled=true' ozone-site.xml settings. In that case you can access the /prof endpoint from scm,om,s3g.   You should upload the async profiler first (https://github.com/jvm-profiling-tools/async-profiler) and set the ASYNC_PROFILER_HOME environment variable to find it.   See: https://issues.apache.org/jira/browse/HDDS-1116","closed","ozone,","elek","2019-02-15T14:42:48Z","2019-02-20T11:42:07Z"
"","24","Fix typo in TaskStatus.java","Thank you always!","closed","","khiraiwa","2015-07-21T10:50:28Z","2015-07-23T13:16:59Z"
"","358","Create REad.me","thank u","closed","","kholoud1234","2018-03-24T20:14:47Z","2019-09-11T02:19:10Z"
"","457","HDDS-965. Ozone: checkstyle improvements and code quality scripts.","Testing github PR capabilities.","closed","","elek","2019-01-07T08:41:12Z","2019-01-09T23:01:06Z"
"","459","HADOOP-16035. Jenkinsfile for Hadoop","Testing Apache Yetus master branch doing PRs for Hadoop","closed","","aw-was-here","2019-01-08T16:30:41Z","2019-02-21T15:40:14Z"
"","317","Branch 2.8.3","test","closed","","dwshmilyss","2017-12-18T07:28:13Z","2020-07-29T16:12:02Z"
"","116","Update README.txt","test","closed","","ghost","2016-07-19T09:58:13Z","2016-11-17T22:58:53Z"
"","101","test","test","closed","","K-Shinotsuka","2016-06-19T05:37:12Z","2016-06-19T05:44:53Z"
"","8","Branch 2","test","closed","","peiyuefeng","2014-11-06T06:28:05Z","2019-01-18T01:08:02Z"
"","74","Merge pull request #1 from apache/trunk","sync","closed","","2899722744","2016-02-10T06:05:52Z","2016-08-19T10:31:32Z"
"","427","YARN-6636. Add basic fairscheduler nodelabel support.","Supports unfair label scheduling and non-exclusive node labels.","open","","bschell","2018-10-15T22:17:51Z","2019-04-08T23:58:15Z"
"","134","YARN-5025. Container move (relocation) between nodes","Support for relocating containers has become a must-have requirement for most multi-service applications, since the inevitable concept-drifts make SLAs hard to be satisfied. The relocation and co-location of services (long running containers) can help to reduce bottlenecks in a multi-service cluster, especially where data-intensive, streaming applications interfere.","open","","szape","2016-10-06T09:27:06Z","2019-04-08T23:57:33Z"
"","591","HDDS-1250: IIn OM HA AllocateBlock call where connecting to SCM from OM should not happen on Ratis.","Still, need to add UT's. Added a patch to get some initial comments.","closed","ozone,","bharatviswa504","2019-03-12T05:50:04Z","2019-04-16T19:06:49Z"
"","167","HADOOP-13600","starting on parallel rename, still designing code for max parallelism. Even listing and delete calls should be in parallel threads. Really only need to be collecting at the same rate as copies, which is implicitly defined by the rate of keys added to a delete queue","closed","","steveloughran","2016-11-24T15:20:07Z","2016-11-30T11:08:43Z"
"","379","sorry","sorry","closed","","maxiaoguang64","2018-05-07T02:08:57Z","2018-05-07T02:14:51Z"
"","324","Fix NullPointerException caused by null-builder","Sometimes occurs java.lang.NullPointerException leading to app failed.","open","","xshaun","2018-01-11T08:37:06Z","2020-07-31T20:32:49Z"
"","18","HADOOP-11746-05","Some folks don't want to to this in JIRA.","closed","","aw-was-here","2015-04-01T22:30:57Z","2015-04-01T22:51:23Z"
"","152","HDFS-11115 Remove bytes2String and string2Bytes","Since StandardCharsets makes converting between (utf-8) bytes and strings trivial, let's remove the methods:     - org.apache.hadoop.hdfs.DFSUtilClient.bytes2String     - org.apache.hadoop.hdfs.DFSUtilClient.string2Bytes","open","","SahilKang","2016-11-07T03:07:16Z","2019-04-08T23:57:34Z"
"","46","Simplify logging logic","Simplify logging logic","closed","","belugabehr","2015-11-17T03:09:05Z","2019-01-11T19:08:51Z"
"","121","HADOOP-13533: Do not require user to set HADOOP_SSH_OPTS to a non-null string, allow","setting of an empty string.","closed","","chu11","2016-08-23T00:44:23Z","2016-08-25T14:12:15Z"
"","90","HDFS-10256. Use GenericTestUtils.getTestDir method in tests for temporary directories","Separated the HDFS changes from HADOOP-12984","closed","","vinayakumarb","2016-04-04T13:15:29Z","2016-06-16T11:32:13Z"
"","464","HDDS-977. Exclude dependency-reduced-pom.xml from ozone rat check.","See: https://issues.apache.org/jira/browse/HDDS-977","closed","ozone,","elek","2019-01-11T09:34:48Z","2019-01-15T00:31:14Z"
"","465","HDDS-793. Support custom key/value annotations on volume/bucket/key.","See: https://issues.apache.org/jira/browse/HDDS-793","closed","ozone,","elek","2019-01-11T16:05:19Z","2019-02-14T16:52:54Z"
"","693","HDDS-1382. Create customized CSI server for Ozone.","See: https://issues.apache.org/jira/browse/HDDS-1382 for more details..","closed","ozone,","elek","2019-04-04T12:41:15Z","2019-05-31T13:27:56Z"
"","519","HDDS-1180. TestRandomKeyGenerator fails with NPE","See: https://issues.apache.org/jira/browse/HDDS-1180","closed","ozone,","elek","2019-02-26T09:31:13Z","2019-02-27T20:41:55Z"
"","484","HDDS-1103. Fix rat/findbug/checkstyle errors in ozone/hdds projects","see: https://issues.apache.org/jira/browse/HDDS-1103","closed","ozone,","elek","2019-02-14T13:06:29Z","2019-02-15T08:50:49Z"
"","486","HDDS-1092. Use Java 11 JRE to run Ozone in containers","see: https://issues.apache.org/jira/browse/HDDS-1092","closed","ozone,","elek","2019-02-15T08:47:08Z","2019-02-15T16:27:03Z"
"","482","HDDS-1069. Temporarily disable the security acceptance tests in Ozone.","See: https://issues.apache.org/jira/browse/HDDS-1069","closed","ozone,","elek","2019-02-07T13:21:01Z","2019-02-14T16:52:10Z"
"","481","HDDS-1068. Improve the error propagation for ozone sh.","see: https://issues.apache.org/jira/browse/HDDS-1068","closed","ozone,","elek","2019-02-07T13:15:24Z","2019-02-15T08:50:28Z"
"","516","HADOOP-16146. Make start-build-env.sh safe in case of misusage of DOCKER_INTERACTIVE_RUN.","See: https://issues.apache.org/jira/browse/HADOOP-16146","open","","elek","2019-02-25T10:02:47Z","2019-09-03T05:04:43Z"
"","471","HDDS-922. Create isolated classloder to use ozonefs with any older hadoop versions","See https://issues.apache.org/jira/browse/HDDS-922","closed","ozone,","elek","2019-01-28T11:48:12Z","2019-02-14T16:52:38Z"
"","472","HDDS-1017. Use distributed tracing the indentify performance problems in Ozone.","See https://issues.apache.org/jira/browse/HDDS-1017.","closed","ozone,","elek","2019-01-28T13:04:05Z","2019-02-11T11:30:57Z"
"","636","HADOOP-16201: S3AFileSystem#innerMkdirs builds needless lists","S3AFileSystem#innerMkdirs currently does not create parent directories. The code checks for the existence of the parent directories and if they do not exist, it adds them to metadataStoreDirs list. But this list is not used and therefore the parent directories are never created. Only the given directory path is created.","closed","","lokeshj1703","2019-03-22T10:04:39Z","2019-04-03T16:48:09Z"
"","617","HADOOP-16195 MarshalledCredentials toString","S3A MarshalledCredentials.toString() doesn't print full date/time of expiry  Change-Id: I4f1bdd2be0d5760c5501dce6edb6122499108b53","closed","fs/s3,","steveloughran","2019-03-18T12:53:18Z","2019-03-28T17:18:37Z"
"","549","HDDS-1213. Support plain text S3 MPU initialization request","S3 Multi-Part-Upload (MPU) is implemented recently in the Ozone s3 gateway. We have extensive testing with using 'aws s3api' application which is passed.  But it turned out that the more simple `aws s3 cp` command fails with _405 Media type not supported error_ message  The root cause of this issue is the JAXRS implementation of the multipart upload method:  {code}   @POST   @Produces(MediaType.APPLICATION_XML)   public Response multipartUpload(       @PathParam(""bucket"") String bucket,       @PathParam(""path"") String key,       @QueryParam(""uploads"") String uploads,       @QueryParam(""uploadId"") @DefaultValue("""") String uploadID,       CompleteMultipartUploadRequest request) throws IOException, OS3Exception {     if (!uploadID.equals("""")) {       //Complete Multipart upload request.       return completeMultipartUpload(bucket, key, uploadID, request);     } else {       // Initiate Multipart upload request.       return initiateMultipartUpload(bucket, key);     }   } {code}  Here we have a CompleteMultipartUploadRequest parameter which is created by the JAXRS framework based on the media type and the request body. With _Content-Type: application/xml_ it's easy: the JAXRS framework uses the built-in JAXB serialization. But with plain/text content-type it's not possible as there is no serialization support for CompleteMultipartUploadRequest from plain/text.    See: https://issues.apache.org/jira/browse/HDDS-1213","closed","ozone,","elek","2019-03-04T14:05:33Z","2019-03-08T17:55:32Z"
"","605","HDDS-1283. Fix the dynamic documentation of basic s3 client usage","S3 gateway has a default web page to display a generic message if you open the endpoint in the browser:  http://localhost:9878/static/  It also contains a simple example to use the endpoint:  {code} This is an endpoint of Apache Hadoop Ozone S3 gateway. Use it with any AWS S3 compatible tool with setting this url as an endpoint  For example with aws-cli:  aws s3api --endpoint http://localhost:9878/static/ create-bucket --bucket=wordcount  For more information, please check the documentation.  {code}  Unfortunately the endpoint is wrong here, the static should be removed from the url.  The trivial fix is to move the ) in the js code>      See: https://issues.apache.org/jira/browse/HDDS-1283","closed","ozone,","elek","2019-03-14T15:00:27Z","2019-03-15T21:12:11Z"
"","321","Branch 3.0   operation shell script   ERROR","run  ./stop-dfs.sh    text:  Stopping namenodes on [10.50.132.145 10.50.132.146 10.50.132.147] ERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting. Stopping datanodes 10.50.132.147: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.151: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.146: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.150: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.154: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.145: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.152: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.148: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.149: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. 10.50.132.153: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS. Stopping journal nodes [10.50.132.145 10.50.132.146 10.50.132.147] ERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting. Stopping ZK Failover Controllers on NN hosts [10.50.132.145 10.50.132.146 10.50.132.147] ERROR: Both HADOOP_WORKERS and HADOOP_WORKER_NAMES were defined. Aborting.","closed","","gehaijiang","2018-01-05T14:27:26Z","2019-02-04T05:51:27Z"
"","236","HADOOP-12084: revise multiple FTPFileSystem issue","revise multiple FTPFileSystem issue  when using filesystem CACHE.","open","","RebornHuan","2017-06-15T09:44:45Z","2019-09-03T05:10:25Z"
"","544","Revert ""HDDS-1183. Override getDelegationToken API for OzoneFileSystem. Contr…""","Reverts apache/hadoop#526","closed","","ajayydv","2019-03-02T00:17:26Z","2019-08-21T18:23:26Z"
"","426","correct configuration tag in mapred-site.xml","Remove the closing and opening of configuration tag","closed","","rguillome","2018-10-11T13:34:02Z","2022-03-22T14:47:54Z"
"","592","HDFS-14361:SNN will always upload fsimage","Related to HDFS-12248.  ``` java boolean sendRequest = isPrimaryCheckPointer     || secsSinceLastUpload >= checkpointConf.getQuietPeriod(); doCheckpoint(sendRequest); ``` If sendRequest is true, SNN will upload fsimage. But isPrimaryCheckPointer always is true,  ``` java if (ie == null && ioe == null) {   //Update only when response from remote about success or   lastUploadTime = monotonicNow();   // we are primary if we successfully updated the ANN   this.isPrimaryCheckPointer = success; } ``` isPrimaryCheckPointer should be outside the if condition.  If the ANN update was not successful, then isPrimaryCheckPointer should be set to false.","open","","hunshenshi","2019-03-12T09:31:44Z","2019-04-08T23:58:25Z"
"","485","HDFS-14244. Refactor the libhdfspp cmake build files.","Refactoring the hdfs++ build scripts.  In particular: - Remove the source code from hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfspp/third_party . - Fix support for shared/static libraries. - Use the packages installed on the system when they are available. - Add support for rpath on mac os. - Run the unit tests when building stand alone. - Use add_ExternalPackage for the projects that we need to build. This means that we need to move our code into external packages also because of the life cycle of cmake builds. - Incorporate the uriparser2 wrapper into libhdfspp, but use uriparser package. Most of the linux variants have uriparser. - Add a cpack definition for libhdfspp so that you can generate a binary artifact in the standalone build. - Support newer versions of asio. (The deadline_timer needs to be replaced with the steady_timer.)  These will remove about 150kloc from Hadoop.","open","","omalley","2019-02-14T15:31:37Z","2019-09-03T04:04:51Z"
"","275","YARN-6475: Refactor long methods in hadoop-yarn-server-nodemanager","Refactor all methods in hadoop-yarn-server-nodemanager that exceed 150 lines in length. Also fixes the method length related Checkstyle violations.","closed","","soumabrata-chakraborty","2017-09-16T18:32:31Z","2019-12-09T19:40:14Z"
"","274","YARN-6475: Refactor long methods in hadoop-yarn-server-nodemanager","Refactor all methods in hadoop-yarn-server-nodemanager that exceed 150 lines in length. Also fixes the method length related Checkstyle violations.","closed","","soumabrata-chakraborty","2017-09-16T18:12:12Z","2017-09-16T18:21:05Z"
"","218","YARN-6475: Refactor long methods in hadoop-yarn-server-nodemanager","Refactor all methods in hadoop-yarn-server-nodemanager that exceed 150 lines in length.  Also fixes the method length related Checkstyle violations.","closed","","soumabrata-chakraborty","2017-04-26T17:15:14Z","2017-05-10T18:16:34Z"
"","447","recovery contaienr exit code not right","recovery contaienr exit code not right","open","","SuperbDong","2018-12-16T08:41:33Z","2019-02-06T11:27:42Z"
"","648","HDDS-1340. Add List Containers API for Recon","Recon server should support ""/containers"" API that lists all the containers","closed","ozone,","vivekratnavel","2019-03-27T00:04:31Z","2019-04-08T16:36:07Z"
"","505","HDDS-1145. Add optional web server to the Ozone freon test tool","Recently we improved the default HttpServer to support prometheus monitoring and java profiling.  It would be very useful to enable the same options for freon testing:   1. We need a simple way to profile freon and check the problems   2. Long running freons should be monitored  We can create a new optional FreonHttpServer which includes all the required servlets by default.  See: https://issues.apache.org/jira/browse/HDDS-1145","closed","ozone,","elek","2019-02-20T16:50:29Z","2019-03-07T09:01:08Z"
"","59","HADOOP-12321","Rebasing as per latest trunk, This is an aggregated patch with all changes together.","closed","","sunilgovind","2015-11-26T18:37:42Z","2019-01-24T10:43:35Z"
"","89","HADOOP-12984. Add GenericTestUtils.getTestDir method and use it for emporary directory in tests","Rebased.","closed","","vinayakumarb","2016-04-03T09:23:22Z","2016-04-07T04:49:11Z"
"","91","HADOOP-12984. Add GenericTestUtils.getTestDir method and use it for temporary directory in tests (branch-2)","Rebased against branch-2","closed","","vinayakumarb","2016-04-06T05:20:04Z","2016-04-07T04:52:35Z"
"","171","HDFS-11227: Set read timeout for peer","Read timeout is not set with peer in org.apache.hadoop.hdfs.BlockReaderFactory, so BlockReader read doesn't timeout.","open","","subahugu","2016-12-09T11:19:40Z","2019-06-22T01:33:34Z"
"","412","YARN-8747: update moment-timezone version to 0.5.1","re-sent a PR  per https://issues.apache.org/jira/browse/YARN-8747?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel","closed","","collinmazb","2018-09-06T10:55:38Z","2019-01-16T05:43:31Z"
"","68","YARN-679 service launcher","Pull-request version of YARN-679; initially the 005 patch plus corrections of javadocs and checkstyles","closed","","steveloughran","2016-01-08T21:58:54Z","2016-06-29T14:47:16Z"
"","349","Merge pull request #1 from apache/trunk","pull latest code","closed","","yonger1516","2018-03-05T12:18:08Z","2018-03-05T12:21:32Z"
"","371","HDFS-13515","provide the info when connect","open","","lqjack","2018-04-29T01:37:56Z","2019-02-06T11:26:37Z"
"","400","HADOOP-15585. Fix passing options via $HADOOP_OPTS","Prior to this change, if HADOOP_OPTS contains any arguments that include a space, the command is not parsed correctly. The only alternative appears to be to use 'eval'. Switching to use 'eval' *instead of* 'exec' also works, but it results in an intermediate bash process being left alive throughout the entire lifetime of the Java process being started. Using 'exec' prefixed by 'eval' as has been done in this commit gets the best of both worlds, in that options with spaces are parsed correctly, and you don't end up with an intermediate bash process as the parent of the Java process.","open","","bschell","2018-07-05T22:44:17Z","2019-04-08T23:58:11Z"
"","259","MAPREDUCE-6931. Remove TestDFSIO ""Total Throughput"" calculation.","Previously it failed to convert ms to seconds and thus reports aggregate throughput as 1/1000x actual numbers. Also, make all the bytes-to-mb and milliseconds-to-seconds conversions consistent in the reporting messages to help avoid this type of error in the future.","closed","","dennishuo","2017-08-03T00:43:53Z","2018-01-23T22:39:35Z"
"","168","HDFS-11182. Update DataNode to use DatasetVolumeChecker.","Preliminary patch for Jenkins runs.","closed","","arp7","2016-11-29T01:31:00Z","2019-07-26T11:29:11Z"
"","675","HADOOP-16085: use object version or etags to protect against inconsistent read after replace/overwrite","PR to replace #646 .   The content is the same, but merge squashed into a single commit on top of trunk to work around Yetus being unable to apply the patch from the previous PR after trunk was merged back in.","closed","fs/s3,","ben-roling","2019-04-01T17:00:41Z","2019-05-02T17:39:03Z"
"","662","HDDS-1207. Refactor Container Report Processing logic and plugin new Replication Manager.","PR #620 brings in new ReplicationManager, this change is to refactor ContainerReportProcessing logic in SCM so that it complements ReplicationManager and plugin the new ReplicationManager code.","closed","","nandakumar131","2019-03-29T11:19:36Z","2019-04-04T18:00:30Z"
"","294","YARN-7546. Move queue information to a fixed container - POC","PoC 1  Renders the queue information, inside a fixed container. Similar to gmails compose box. Will add minimize and close functionalities to this.     ------- PoC 2:","closed","","skmvasu","2017-11-21T08:28:47Z","2017-12-01T04:53:58Z"
"","92","HDFS-9895. Push up DataNode#conf to base class","Please kindly review the patch v001, see also https://issues.apache.org/jira/browse/HDFS-9895.","closed","","ghost","2016-04-22T23:35:37Z","2019-07-26T11:31:37Z"
"","93","Implement asynchronous rename for DistributedFileSystem","Please kindly review patch v007, thanks.","closed","","ghost","2016-04-23T00:05:32Z","2016-04-25T22:28:01Z"
"","260","YARN-6969. Remove method getMinShareMemoryFraction and getPendingCont…","Please add me as a contributor in jira so that I can assign this task to me.","closed","","LarryLo","2017-08-10T04:55:17Z","2019-07-26T18:32:18Z"
"","131","HADOOP-13655 document object store use with fs shell and distcp","Patch of filesystem shell & distcp docs to cover object stores. Also updated some references in filesystem/index.md which were out of date","closed","","steveloughran","2016-09-26T12:48:57Z","2016-11-22T21:18:53Z"
"","154","HDFS-11114. Support for running async disk checks in DataNode.","Patch for branch-2 to fix Java 7 compiler errors. Changes were minor - added final for variables referenced in anonymous inner classes and explicit type parameter in the call to _Futures.immediateFailedFuture(result.exception)_.","closed","","arp7","2016-11-08T21:58:14Z","2016-12-16T19:06:20Z"
"","588","HDDS-1247. Bump trunk ozone version to 0.5.0","ozone-0.4 branch is working, we need to update the trunk version.  See: https://issues.apache.org/jira/browse/HDDS-1247","closed","ozone,","elek","2019-03-11T09:37:20Z","2019-03-14T11:02:28Z"
"","489","HDDS-1115. Provide ozone specific top-level pom.xml","Ozone build process doesn't require the pom.xml in the top level hadoop directory as we use hadoop 3.2 artifacts as parents of hadoop-ozone and hadoop-hdds. The ./pom.xml is used only to include the hadoop-ozone/hadoop-hdds projects in the maven reactor.  From command line, it's easy to build only the ozone artifacts:  {code} mvn clean install -Phdds  -am -pl :hadoop-ozone-dist  -Danimal.sniffer.skip=true  -Denforcer.skip=true {code}  Where: '-pl' defines the build of the hadoop-ozone-dist project and '-am' defines to build all of the dependencies from the source tree (hadoop-ozone-common, hadoop-hdds-common, etc.)  But this filtering is available only from the command line.  With providing a lightweight pom.ozone.xml we can achieve the same:   * We can open only hdds/ozone projects in the IDE/intellij. It makes the development faster as IDE doesn't need to reindex all the sources all the time + it's easy to execute checkstyle/findbugs plugins of the intellij to the whole project.  * Longer term we should create an ozone specific source artifact (currently the source artifact for hadoop and ozone releases are the same) which also requires a simplified pom.  In this patch I also added the .mvn directory to the .gitignore file.  With  {code} mkdir -p .mvn && echo ""-f ozone.pom.xml"" > .mvn/maven.config"" you can persist the usage of the ozone.pom.xml for all the subsequent builds (in the same dir)  How to test?  Just do a 'mvn -f ozonze.pom.xml clean install -DskipTests'  See: https://issues.apache.org/jira/browse/HDDS-1115","closed","ozone,","elek","2019-02-15T12:49:48Z","2019-03-06T17:19:59Z"
"","277","add check compressor's byte size when finish compress","org.apache.hadoop.io.compress.BlockCompressorStream#finish is a public function,so other apps can call the method directly,such as `flume`,but when compressor.getBytesRead() == 0 then it will write a null data,and then the data after the null data will not be read as they lost. So,please add the check in the method. Thank you.","open","","famosss","2017-09-26T01:58:06Z","2022-01-31T22:39:42Z"
"","60","Add space after jobId in line 1011","One space is needed before `Job Transitioned from` in line 1011. The log output has no space between `jobId` and `Job Transitioned from`.","open","","iSultan","2015-12-02T23:20:15Z","2019-08-27T16:44:11Z"
"","52","HADOOP-9843","NPE when trying to create an error message response of RPC","closed","","steveloughran","2015-11-22T18:24:39Z","2015-11-23T10:13:08Z"
"","32","See HADOOP-12406","Note: I am not an expert at JAVA, Class loaders, or Hadoop. I am just a hacker. My solution might be entirely wrong. AbstractMapWritable.readFields throws a ClassNotFoundException when reading custom writables. Debugging the job using remote debugging in IntelliJ revealed that the class loader being used in Class.forName() is different than that used by the Thread's current context (Thread.currentThread().getContextClassLoader()). The class path for the system class loader does not include the libraries of the job jar. However, the class path for the context class loader does. The proposed patch changes the class loading mechanism in readFields to use the Thread's context class loader instead of the system's default class loader.","open","","allfro","2015-09-12T07:30:01Z","2019-04-08T23:57:26Z"
"","34","remove space before colon in decommission status","Not sure if this was intentional but can imagine it complicates things for anyone trying to parse reports","closed","","1tylermitchell","2015-09-15T16:06:17Z","2016-07-11T20:59:28Z"
"","584","HADOOP-16109. Parquet reading S3AFileSystem causes EOF - branch-3.1","Nobody gets seek right. No matter how many times they think they have.  Reproducible test from: Dave Christianson Fixed seek() logic: Steve Loughran  Cherry-picked from commit 0cbe9ad8c23; changes to S3ATestUtils also backported.  Change-Id: I20d842a6899aa181d62e9f7452ded2f09ed73040 (cherry picked from commit c52ac2afc3b1bd239d863c5e76b0875cba429bbf)","closed","","steveloughran","2019-03-09T18:49:22Z","2021-10-15T19:46:18Z"
"","583","HADOOP-16109. Parquet reading S3AFileSystem causes EOF - branch-3.2","Nobody gets seek right. No matter how many times they think they have.  Reproducible test from: Dave Christianson Fixed seek() logic: Steve Loughran  Cherry-picked from commit 0cbe9ad8c23; changes to S3ATestUtils also backported.  Change-Id: I20d842a6899aa181d62e9f7452ded2f09ed73040","closed","","steveloughran","2019-03-09T16:32:14Z","2019-03-11T12:06:07Z"
"","589","HADOOP-16109. Parquet reading S3AFileSystem causes EOF","Nobody gets seek right. No matter how many times they think they have.  Reproducible test from: Dave Christianson Fixed seek() logic: Steve Loughran  Change-Id: I39b87f3d5daa98f65de2c0a44e348821a4930573","closed","","steveloughran","2019-03-11T16:33:10Z","2019-03-28T13:42:12Z"
"","590","HADOOP-16109. Parquet reading S3AFileSystem causes EOF","Nobody gets seek right. No matter how many times they think they have.  Reproducible test from: Dave Christianson Fixed seek() logic: Steve Loughran  Change-Id: I268bfcb39ed2626b7281c793d8123f62d8d30caa","closed","","steveloughran","2019-03-11T17:09:20Z","2019-03-28T13:41:54Z"
"","31","Learning","Need support","closed","","MjAbuz","2015-09-03T06:48:31Z","2019-07-26T11:12:15Z"
"","409","Hdfs 10285","my datanode startup is so slow, when every disk has 500000+ blocks; I Get Info as :  org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2096176596-10.2.26.138-1464688125169 on volume /opt/data/9/dfs/dn/current: 147741ms So I Change Code FsDatasetUtil.java like this:   _/*        if (!path.startsWith(blockName)) {          continue;        }        if (blockFile.getCanonicalPath().equals(listdir[j].getCanonicalPath())) {          FsDatasetImpl.LOG.warn(""Block getCanonicalPath:"" + blockFile + blockFile.getCanonicalPath());          FsDatasetImpl.LOG.warn(""listdir[j].getCanonicalPath:"" + listdir[j].getCanonicalPath());          continue;        }        return Block.getGenerationStamp(listdir[j].getName());  */     /** to find block meta file by block file **/        if (path.startsWith(blockName+""_"") && path.endsWith("".meta"")) {          return Block.getGenerationStamp(path);        }_  Datanode Just cost 49000ms  after change there,so can i make chang there and without  other Negative impact ？  thks","closed","","KevinLu233","2018-08-10T07:19:33Z","2018-08-12T12:19:42Z"
"","309","MAPREDUCE-7017:Too many times of meaningless invocation in TaskAttemptImpl#resolveHosts","MRAppMaster uses TaskAttemptImpl::resolveHosts to determine the dataLocalHosts for each task when the location of data split is IP, which will call a lot of times ( taskNum * dfsReplication) of function InetAddress::getByName and most of the funcition calls are redundant. When the job has a great number of tasks and the speed of DNS resolution is not fast enough, it will take a lot of time at this stage before the job running.","open","","jiayuhan-it","2017-12-04T13:02:21Z","2020-12-23T04:47:09Z"
"","224","HADOOP-9320 Fix Hadoop native build failure on ARM hard-float","Moved the ARM JVM float ABI detection from `HadoopCommon.cmake` to `HadoopJNI.cmake`, because `${JAVA_JVM_LIBRARY}` is not available in `HadoopCommon.cmake` yet and therefore the build fails.  This commit fixes this issue.","open","","amuttsch","2017-05-21T15:49:53Z","2020-07-31T20:35:02Z"
"","397","HADOOP-15584. Move httpcomponents version to their own config","Move httpcomponent versions in pom.xml to their own variables for easy overriding","open","","bschell","2018-07-05T21:33:28Z","2019-09-03T04:16:15Z"
"","368","HADOOP-15380","move crc file to trash if exist","open","","lqjack","2018-04-27T17:34:23Z","2019-02-06T11:26:32Z"
"","347","[HADOOP-15261] Upgrade commons-io from 2.4 to 2.5","move commons-io up to 2.5 which introduced by kerb-simplekdc.","closed","","PandaMonkey","2018-02-27T01:35:21Z","2018-03-06T14:50:44Z"
"","123","MAPREDUCE-6769. Fix forgotten name conversion from ""slave"" to ""worker"" in mapred script,","most notably fixing environment variable name change and function name change.","closed","","chu11","2016-08-26T18:27:17Z","2019-01-10T18:08:21Z"
"","166","3.0.0 alpha1 kb","Mistakenly submitted a PR to the main apache repo","closed","","kbavishi","2016-11-24T00:01:34Z","2016-11-24T00:06:27Z"
"","11","Merge pull request #2 from apache/trunk","Merging latest changes to trunk","closed","","vinayrpet","2014-11-18T13:25:11Z","2014-11-18T13:39:07Z"
"","216","YARN-6457  use existing conf object as sslConf object in WebApps for the builder to use for the HttpServer2","Merged from 2.7.1 branch","closed","","sanjaypujare","2017-04-21T23:03:26Z","2017-05-08T06:48:48Z"
"","217","merge from upstream","merge from upstream","closed","","bitmybytes","2017-04-25T02:30:34Z","2017-04-25T02:31:02Z"
"","130","HADOOP-13560 block output streams","Merge commit of latest code.   Docs, XML configs up to speed scale tests only run with a -Pscale option. some props can be configured in POM","closed","","steveloughran","2016-09-23T18:59:56Z","2017-01-26T22:16:31Z"
"","392","Merge pull request #1 from apache/trunk","merge apache hadoop trunk","closed","","xbase","2018-06-10T14:52:05Z","2018-06-10T14:55:04Z"
"","102","MAPREDUCE-6721.001","mapreduce.reduce.shuffle.memory.limit.percent=0.0 should be legal to enforce shuffle to disk","closed","","gerashegalov","2016-06-22T07:25:33Z","2016-06-23T00:21:18Z"
"","455","YARN-9176.[Submarine] Repair 404 error of links in documentation","links in src/site/markdown/Examples.md and src/site/markdown/QuickStart.md will get 404, repair this links.","closed","","hddong","2019-01-04T07:58:24Z","2019-01-07T01:24:22Z"
"","474","fix some alerts raised by LGTM","LGTM has raised some alerts after analysing Hadoop (https://lgtm.com/projects/g/apache/hadoop/alerts/?mode=tree). This pull request is to fix some of the more straightforward ones. Specifically, - `ArrayIndexOutOfBounds` in KerberosName - `Contradictory type checks` in GenericExceptionHandler -`Missing format argument` in RegistrySecurity, HttpFSExceptionProvider, DatasetVolumeChecker and ServiceClient  (disclosure: I work for Semmle, the company behind LGTM).","closed","","malcolmtaylor","2019-01-30T15:58:37Z","2019-09-12T07:57:36Z"
"","663","HDDS-1357. ozone s3 shell command has confusing subcommands","Let's check the potential subcommands of ozone sh:  {code} [hadoop@om-0 keytabs]$ ozone sh Incomplete command Usage: ozone sh [-hV] [--verbose] [-D=]... [COMMAND] Shell for Ozone object store       --verbose   More verbose output. Show the stack trace of the errors.   -D, --set=    -h, --help      Show this help message and exit.   -V, --version   Print version information and exit. Commands:   volume, vol  Volume specific operations   bucket       Bucket specific operations   key          Key specific operations   token        Token specific operations {code}  This is fine, but for ozone s3:  {code} [hadoop@om-0 keytabs]$ ozone s3 Incomplete command Usage: ozone s3 [-hV] [--verbose] [-D=]... [COMMAND] Shell for S3 specific operations       --verbose   More verbose output. Show the stack trace of the errors.   -D, --set=    -h, --help      Show this help message and exit.   -V, --version   Print version information and exit. Commands:   getsecret    Returns s3 secret for current user   path         Returns the ozone path for S3Bucket   volume, vol  Volume specific operations   bucket       Bucket specific operations   key          Key specific operations   token        Token specific operations {code}  This list should contain only the getsecret/path commands and not the volume/bucket/key subcommands.  See: https://issues.apache.org/jira/browse/HDDS-1357","closed","ozone,","elek","2019-03-29T15:21:33Z","2019-03-30T17:13:30Z"
"","150","HADOOP-13773, set heap args for HADOOP_CLIENT_OPTS when HADOOP_HEAPSI…","jira url is https://issues.apache.org/jira/browse/HADOOP-13773","closed","","ferhui","2016-10-31T15:01:54Z","2019-01-24T10:32:34Z"
"","202","[YARN-6328] Update a spelling mistake","JIRA Issue: https://issues.apache.org/jira/browse/YARN-6328  Fix a spelling mistake, doesnt should be doesn't.","closed","","NJUJYB","2017-03-12T17:16:59Z","2017-03-15T09:28:11Z"
"","555","HDDS-1219. TestContainerActionsHandler.testCloseContainerAction has an intermittent failure","It's failed multiple times during the CI builds:  {code} Error Message  Wanted but not invoked: closeContainerEventHandler.onMessage(     #1,     org.apache.hadoop.hdds.server.events.EventQueue@3d3fcdb0 ); -> at org.apache.hadoop.hdds.scm.container.TestContainerActionsHandler.testCloseContainerAction(TestContainerActionsHandler.java:64) Actually, there were zero interactions with this mock. {code}  The fix is easy: we should call queue.processAll(1000L) to wait for the processing of all the events.   See: https://issues.apache.org/jira/browse/HDDS-1219","closed","ozone,","elek","2019-03-05T12:47:13Z","2019-03-05T18:19:06Z"
"","443","recovery contaienr exit code not right","It's correct exitCode when container launch nomally,but it is not correct if the container by recovery. Out of memory exitCode is -104, the exitCode had to be lost when the container was recovered by restart NodeManager.","open","","SuperbDong","2018-12-08T08:57:46Z","2019-09-03T05:05:39Z"
"","16","Adjust exception output to offer additional information","It is not a easy job to find out detailed exception messages of _db_ module under this version.","open","","weyo","2015-03-24T13:10:24Z","2022-01-31T20:35:56Z"
"","155","HDFS-11119. Support for parallel checking of StorageLocations on DataNode startup","Introduce a StorageLocationChecker class that can parallelize checking StorageLocations. It also detects stalled checks and flags such volumes as failed. The DataNode will use this class in the next Jira.","closed","","arp7","2016-11-09T00:35:41Z","2016-11-11T23:04:19Z"
"","153","HDFS-11114. Support for running async disk checks in DataNode.","Interface for running async checks on a resource.  The implementation ThrottledAsyncChecker supports throttling and result-caching. DataNode changes to use it will be done in another Jira.","closed","","arp7","2016-11-07T20:01:29Z","2016-11-08T02:47:39Z"
"","666","HADOOP-16221 add option to fail operation on metadata write failure","Initial patch.  Curious for any feedback, particular with regard to the default.  I have left the default as matching current behavior, but it doesn't feel right.","closed","fs/s3,","ben-roling","2019-03-29T16:38:11Z","2019-04-30T13:53:18Z"
"","73","HADOOP-12746. ReconfigurableBase should update the cached configuration","Initial patch to have ReconfigurableBase#ReconfigurationThread update parent's cached configuration.  The effective value may be different from the passed in newVal so this patch also updates ReconfigurableBase#reconfigurePropertyImpl to return the effective config value. This idea was suggested by [~eddyxu] on https://issues.apache.org/jira/secure/EditComment!default.jspa?id=12740232&commentId=15118434  Will likely add more unit tests for this.","closed","","arp7","2016-02-06T00:58:11Z","2016-02-12T20:44:24Z"
"","165","HDFS-11149. Support for parallel checking of FsVolumes.","Initial patch for Jenkins test run. Will add some more tests for stalled checks.","closed","","arp7","2016-11-23T19:39:56Z","2019-07-26T11:29:49Z"
"","599","HADOOP-16183. Use latest Yetus to support ozone specific build process","In YETUS-816 the hadoop personality is improved to better support ozone specific changes.  Unfortunately the hadoop personality is part of the Yetus project and not the Hadoop project: we need a new yetus release or switch to an unreleased version.  In this patch I propose to use the latest commit from yetus (but use that fixed commit instead updating all the time).   See: https://issues.apache.org/jira/browse/HADOOP-16183","closed","ozone,","elek","2019-03-13T11:00:03Z","2019-05-02T14:50:17Z"
"","94","HDFS-10382 In WebHDFS numeric usernames do not work with DataNode","In WebHDFS for cat operation, we have 2 sequential HTTP requests. The first HTTP request is handled by NN and the second one by DN. Unlike the NN, the DN is not using the suggested domain pattern from the configuration!","closed","","ramtinb","2016-05-10T00:13:56Z","2020-02-25T17:36:01Z"
"","438","YARN-9009: Fix flaky test TestEntityGroupFSTimelineStore.testCleanLogs","In TestEntityGroupFSTimelineStore, testCleanLogs fails when run after testMoveToDone.  testCleanLogs fails because testMoveToDone moves a file into the same directory that testCleanLogs cleans, causing testCleanLogs to clean 3 files, instead of 2 as testCleanLogs expects.  To fix the failure of testCleanLogs, we can delete the file after the file is moved by testMoveToDone.  Link to issue: [YARN-9009](https://issues.apache.org/jira/browse/YARN-9009)","open","","OrDTesters","2018-11-12T02:32:46Z","2021-10-07T11:44:43Z"
"","355","HDFS-13313. Fix NullPointerException in FSEditLogOp.toString().","In some subclasses of FSEditLogOp, toString() uses dot(.) operator on some fields. Those fields could be set to null by resetSubFields() and a NullPointerException would be thrown in toString(). This patch fixes this bug by calling StringBuilder.append(field). Thus ""null"" would be printed in the field=null case, and field.toString() code is properly reused to avoid repetition.","open","","glglwty","2018-03-19T19:32:48Z","2019-04-08T23:58:01Z"
"","338","MAPREDUCE-7051. Fix typo in MultipleOutputFormat","In org.apache.hadoop.mapred.lib.MultipleOutputFormat, there is a typo for the java doc of getInputFileBasedOutputFileName method.  ""the outfile name based on a given anme and the input file name"" should be ""the outfile name based on a given name and the input file name""","closed","","ywheel","2018-02-13T01:54:56Z","2020-07-30T04:01:42Z"
"","64","NNStorage does not synchronize iteration on a synchronized list","In line 839 of NNStroage.java#reportErrorsOnDirectories, the synchronized list, `sds` is iterated in an unsynchronized manner, but according to [Oracle Java 7 API specification](http://docs.oracle.com/javase/7/docs/api/java/util/Collections.html#synchronizedList%28java.util.List%29), this is not thread-safe and can lead to non-deterministic behavior. This pull request adds a fix by synchronizing the iteration on `sds`. The synchronized list is passed to method `reportErrorsOnDirectories` from [here](https://github.com/facebookarchive/hadoop-20/blob/2a29bc6ecf30edb1ad8dbde32aa49a317b4d44f4/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSImage.java#L508)","open","","emopers","2015-12-30T09:46:28Z","2019-09-03T08:28:51Z"
"","186","HDFS-11391 Numeric usernames do no work with WebHDFS FS","In HDFS-4983, a property has been introduced to configure the pattern validating name of users interacting with WebHDFS because default pattern was excluding names starting with numbers.  Problem is that this fix works only for read access. In case of write access against data node, the default pattern is still applied whatever the configuration is.  This PR fixes the web handler running in data nodes.","closed","","pvillard31","2017-02-04T21:57:41Z","2017-02-15T09:11:07Z"
"","556","HDDS-1222. Remove TestContainerSQLCli unit test stub","In HDDS-447 we removed the support the 'ozone noz' cli tool which was a rocksdb/leveldb to sql exporter.  But still we have the unit test for the tool (in fact only the skeleton of the unit test, as the main logic is removed). Even worse this unit test is failing as it calls System.exit:  {code} [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M1:test (default-test) on project hadoop-ozone-tools: There are test failures. [ERROR]  [ERROR] Please refer to /testptch/hadoop/hadoop-ozone/tools/target/surefire-reports for the individual test results. [ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream. [ERROR] ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called? {code}  I think this test can be deleted.  See: https://issues.apache.org/jira/browse/HDDS-1222","closed","ozone,","elek","2019-03-05T15:35:38Z","2019-03-05T18:15:56Z"
"","582","HDFS-14350:dfs.datanode.ec.reconstruction.threads not take effect","In ErasureCodingWorker, stripedReconstructionPool is create by  ``` java initializeStripedBlkReconstructionThreadPool(conf.getInt(     DFSConfigKeys.DFS_DN_EC_RECONSTRUCTION_THREADS_KEY,     DFSConfigKeys.DFS_DN_EC_RECONSTRUCTION_THREADS_DEFAULT));  private void initializeStripedBlkReconstructionThreadPool(int numThreads) {   LOG.debug(""Using striped block reconstruction; pool threads={}"",       numThreads);   stripedReconstructionPool = DFSUtilClient.getThreadPoolExecutor(2,       numThreads, 60, new LinkedBlockingQueue<>(),       ""StripedBlockReconstruction-"", false);   stripedReconstructionPool.allowCoreThreadTimeOut(true); } ``` so stripedReconstructionPool is a ThreadPoolExecutor, and the queue is a LinkedBlockingQueue, then the active thread is awalys 2, the dfs.datanode.ec.reconstruction.threads not take effect.","open","","hunshenshi","2019-03-09T05:42:31Z","2021-07-24T06:08:03Z"
"","468","fix bug which the iMax is not work in MutableRatesWithAggregation","IMax and IMin is not work in MutableRatesWithAggregation.","closed","","ZanderXu","2019-01-18T05:27:28Z","2022-05-27T01:32:38Z"
"","82","Disable setsid in case of Security Exceptions","If spawning an external process is not permitted by the JVM security manager, disable setsid.","open","","costin","2016-03-04T11:56:03Z","2019-02-06T11:23:13Z"
"","398","MAPREDUCE-7119. Avoid stopContainer() on dead node","If a container failed to launch earlier due to terminated instances, it has already been removed from the container hash map. Avoiding the kill() for CONTAINER_REMOTE_CLEANUP will avoid wasting 15min per container on retries/timeout.","open","","bschell","2018-07-05T21:38:54Z","2019-09-03T05:24:55Z"
"","303","YARN-7499. Application page layout changes","IA changes to YARN application page          Quicklinks menu ![Uploading Screen Shot 2017-11-20 at 2.14.04 PM.png…]() Settings menu    * Changes to Applications page layout  * Fetch yarn-app details at the parent route  * Yarn applications page layout change  * render application master info links  * Fixes issue with service param  * Render quicklinks on services page  * Implement start/stop service  * style fixes inside application body","closed","","skmvasu","2017-11-28T09:11:11Z","2017-11-28T14:24:05Z"
"","298","YARN-7499. Changes to YARN applications layout and IA","IA changes to YARN application page          Quicklinks menu ![Uploading Screen Shot 2017-11-20 at 2.14.04 PM.png…]() Settings menu    * Changes to Applications page layout  * Fetch yarn-app details at the parent route  * Yarn applications page layout change  * render application master info links  * Fixes issue with service param  * Render quicklinks on services page  * Implement start/stop service  * style fixes inside application body","closed","","skmvasu","2017-11-24T06:12:59Z","2017-11-28T09:04:11Z"
"","292","YARN-7499 Changes to Applications page layout","IA changes to YARN application page          Quicklinks menu ![Uploading Screen Shot 2017-11-20 at 2.14.04 PM.png…]() Settings menu","closed","","skmvasu","2017-11-15T09:14:17Z","2017-11-24T06:19:05Z"
"","271","YARN-2554. RM web proxy uses the client truststore specified in ssl-client.xml","I want to raise the issue again since the issue affects other application which runs on YARN. Actually, I see this problem when we run Spark app on Yarn. Spark launches Spark context web UI with custom SSL certificate when we enable SSL with ""spark.ssl.trustStore"" and ""spark.ssl.keyStore"" properties. In this case, Yarn web proxy cannot connect the Spark context web UI since the web proxy cannot verify the SSL cert (""javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed"" error is returned).  We should add an option to set SSL trust store to Yarn RM web proxy. I added an updated patch, and this patch lets web proxy use an SSL custom trust-store if it is configured in ssl-client.xml","closed","","akitanaka","2017-09-07T21:34:45Z","2018-03-10T00:31:24Z"
"","270","YARN-2554. RM webproxy uses the client truststore specified in ssl-client.xml","I want to raise the issue again since the issue affects other application which runs on YARN. Actually, I see this problem when we run Spark app on Yarn. Spark launches Spark context web UI with custom SSL certificate when we enable SSL with ""spark.ssl.trustStore"" and ""spark.ssl.keyStore"" properties. In this case, Yarn web proxy cannot connect the Spark context web UI since the web proxy cannot verify the SSL cert (""javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed""  error is returned).  We should add an option to set SSL trust store to Yarn RM web proxy. I added an updated patch, and this patch lets web proxy use an SSL custom trust-store if it is configured in ssl-client.xml","closed","","akitanaka","2017-09-07T19:59:03Z","2017-09-07T21:21:02Z"
"","63","Fix a typo","I think there is a typo unless you typed intentionally. Is `slowTaskRelativeTresholds` meant to be `slowTaskRelativeThresholds`?","open","","iSultan","2015-12-09T00:28:05Z","2019-09-03T06:41:02Z"
"","79","for archive.org","I now have access to archive.org working.  See my test case here : https://github.com/h4ck3rm1k3/hadoop-archive-org-bucket-fs  new URI(""s3://bucketname"").getHost() is returning null, and I wonder how this ever worked.","closed","","h4ck3rm1k3","2016-02-20T22:33:47Z","2016-02-22T13:13:20Z"
"","500","YARN-9238. Allocate on previous or removed or non existent application attempt","I have created the jira [YARN-9238](https://jira.apache.org/jira/browse/YARN-9238) to describe the problem. Hope for review and merge!","closed","","lujiefsi","2019-02-19T08:51:19Z","2019-02-22T07:22:49Z"
"","499","MAPREDUCE-7178. NPE happens while YarnChild shudown","I have created the jira [MAPREDUCE-7178](https://jira.apache.org/jira/browse/MAPREDUCE-7178) to describe the problem. Hope for review and merge!","closed","","lujiefsi","2019-02-19T08:42:43Z","2019-02-22T07:22:40Z"
"","498","HDFS-14216. NullPointerException happens in NamenodeWebHdfs","I have created the jira [HDFS-14216](https://jira.apache.org/jira/browse/HDFS-14216) to describe the problem. Hope for review and merge!","closed","","lujiefsi","2019-02-19T08:32:08Z","2019-02-22T02:27:36Z"
"","103","HADOOP-12345,HADOOP-11823 fixes","I have already filed the HADOOP-12345 and HADOOP-11823 jira.   The fix for HADOOP-12345 is to correctly compute the credential length which is passed as part of the NFS request. We need to round the XDR bytes to a multiple of 4 if the hostname in the credential is not a multiple of 4.  The fix for HADOOP-11823 is when RPC returns a denied reply, the code should not check for a verifier. It is a bug as it doesn't match the RPC protocol. (See Page 33 from NFS Illustrated book).  This is my first time contributing to Hadoop.","closed","","pradeep1288","2016-06-22T18:51:39Z","2016-06-23T18:04:47Z"
"","104","HADOOP-12345: Compute the correct credential length","I had to discard my earlier pull request as that included fix for the jira HADOOP-11823, hence creating a separate one for each of them.  The fix here addresses the correct credential length to be computed. We need to round of the machine name length to the next multiple of 4, else using such a credential in the NFS RPC request will result in GARBAGE_ARGS from the NFS server. See RFC-5531, page 24 and page 8","closed","","pradeep1288","2016-06-23T18:19:25Z","2016-06-30T06:29:10Z"
"","416","YARN-8470. Fix a NPE in identifyContainersToPreemptOnNode()","I encountered this issue while running 3.1.0:  ``` 2018-09-10 13:42:39,437 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container container_1536156801471_0071_01_000055 completed with event FINISHED, but corresponding RMContainer doesn't exist. 2018-09-10 13:42:39,881 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received RMFatalEvent of type CRITICAL_THREAD_CRASH, caused by a critical thread, FSPreemptionThread, that exited unexpectedly: java.lang.NullPointerException         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptOnNode(FSPreemptionThread.java:207)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptForOneContainer(FSPreemptionThread.java:161)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreempt(FSPreemptionThread.java:121)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.run(FSPreemptionThread.java:81)  2018-09-10 13:42:39,886 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Shutting down the resource manager. 2018-09-10 13:42:39,891 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: a critical thread, FSPreemptionThread, that exited unexpectedly: java.lang.NullPointerException         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptOnNode(FSPreemptionThread.java:207)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreemptForOneContainer(FSPreemptionThread.java:161)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.identifyContainersToPreempt(FSPreemptionThread.java:121)         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSPreemptionThread.run(FSPreemptionThread.java:81) ```  I'm guessing a better fix would be to synchronise the removal of applications, but this simple patch should be an improvement IMO.","open","","gg7","2018-09-11T15:49:00Z","2019-10-08T20:09:52Z"
"","173","Details for ADLS connectivity using Client Keys","I did not find the token endpoint as described in the previous version of this document, I found the URL from the ADLS REST API documentation https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-get-started-rest-api  Added dfs.adls.oauth2.access.token.provider.type, I saw errors regarding a missing dfs.adls.oauth2.access.token.provider value without this property set.","open","","slachterman","2016-12-15T21:01:53Z","2019-04-08T23:57:36Z"
"","291","HADOOP-15033. Use java.util.zip.CRC32C for Java 9 and above","I added isJava9OrAbove() flag in Shell which is statically initialized using version string parsing. If it is true, DataChecksum creates zip implementation instead of PureJavaCrc32C. It is created using constructor method handle that's looked up also if it's 9 or above. Signature check should ignore java.lang.invoke to let method handles code pass build. Performance test now outputs extra column for 9 and above with results of zip CRC32C.  Results on Cavium ThunderX server (AArch64) show ~4x improvement and are listed below. The picture is about the same on x86.  |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |    32 |  1 |     157.6 |     160.1 |   1.6% |     110.3 | -31.1% |       98.9 | -10.3% | |    32 |  2 |     171.2 |     184.2 |   7.6% |     107.0 | -41.9% |      112.9 |   5.5% | |    32 |  4 |     182.1 |     176.0 |  -3.3% |     104.0 | -40.9% |      113.3 |   8.9% | |    32 |  8 |     179.3 |     173.0 |  -3.6% |     102.1 | -41.0% |      111.5 |   9.3% | |    32 | 16 |      95.0 |      91.3 |  -3.9% |      53.4 | -41.5% |       57.5 |   7.7% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |    64 |  1 |     326.5 |     324.6 |  -0.6% |     137.4 | -57.7% |      144.7 |   5.3% | |    64 |  2 |     298.8 |     293.8 |  -1.7% |     133.5 | -54.6% |      139.5 |   4.5% | |    64 |  4 |     279.3 |     273.3 |  -2.2% |     128.4 | -53.0% |      136.5 |   6.3% | |    64 |  8 |     269.8 |     267.2 |  -0.9% |     125.9 | -52.9% |      133.6 |   6.1% | |    64 | 16 |     140.1 |     139.3 |  -0.6% |      65.6 | -52.9% |       69.3 |   5.6% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |   128 |  1 |     666.6 |     668.6 |   0.3% |     185.0 | -72.3% |      208.7 |  12.8% | |   128 |  2 |     656.3 |     616.9 |  -6.0% |     184.5 | -70.1% |      207.7 |  12.6% | |   128 |  4 |     636.7 |     599.7 |  -5.8% |     181.8 | -69.7% |      203.6 |  12.0% | |   128 |  8 |     628.3 |     592.3 |  -5.7% |     180.6 | -69.5% |      202.2 |  12.0% | |   128 | 16 |     378.1 |     345.9 |  -8.5% |      93.1 | -73.1% |      108.4 |  16.4% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |   256 |  1 |     906.4 |     776.1 | -14.4% |     207.9 | -73.2% |      239.2 |  15.0% | |   256 |  2 |     882.9 |     833.0 |  -5.6% |     206.6 | -75.2% |      238.6 |  15.5% | |   256 |  4 |     837.9 |     801.0 |  -4.4% |     204.2 | -74.5% |      235.9 |  15.5% | |   256 |  8 |     806.1 |     776.7 |  -3.7% |     201.7 | -74.0% |      225.6 |  11.8% | |   256 | 16 |     493.6 |     456.0 |  -7.6% |     107.8 | -76.4% |      121.3 |  12.5% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |   512 |  1 |    1016.3 |     850.4 | -16.3% |     214.6 | -74.8% |      248.7 |  15.9% | |   512 |  2 |     969.7 |     894.4 |  -7.8% |     212.1 | -76.3% |      241.5 |  13.8% | |   512 |  4 |     894.0 |     829.4 |  -7.2% |     207.4 | -75.0% |      232.1 |  11.9% | |   512 |  8 |     886.8 |     831.8 |  -6.2% |     205.2 | -75.3% |      238.6 |  16.3% | |   512 | 16 |     518.3 |     525.4 |   1.4% |     111.1 | -78.9% |      124.8 |  12.4% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |  1024 |  1 |    1120.0 |    1130.5 |   0.9% |     219.1 | -80.6% |      258.0 |  17.8% | |  1024 |  2 |    1054.9 |     983.9 |  -6.7% |     217.5 | -77.9% |      254.8 |  17.1% | |  1024 |  4 |     956.8 |     766.0 | -19.9% |     213.0 | -72.2% |      241.1 |  13.2% | |  1024 |  8 |     835.5 |     864.9 |   3.5% |     209.6 | -75.8% |      247.2 |  18.0% | |  1024 | 16 |     545.0 |     540.5 |  -0.8% |     110.1 | -79.6% |      131.8 |  19.7% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |  2048 |  1 |    1189.2 |    1190.5 |   0.1% |     223.3 | -81.2% |      263.3 |  17.9% | |  2048 |  2 |    1116.4 |    1027.5 |  -8.0% |     220.6 | -78.5% |      260.0 |  17.9% | |  2048 |  4 |    1009.9 |     931.0 |  -7.8% |     215.3 | -76.9% |      253.3 |  17.6% | |  2048 |  8 |     950.4 |     881.0 |  -7.3% |     213.2 | -75.8% |      251.5 |  17.9% | |  2048 | 16 |     570.7 |     521.7 |  -8.6% |     111.7 | -78.6% |      132.1 |  18.3% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |  4096 |  1 |    1264.7 |     991.2 | -21.6% |     225.7 | -77.2% |      267.6 |  18.6% | |  4096 |  2 |    1161.3 |    1023.4 | -11.9% |     222.2 | -78.3% |      263.2 |  18.5% | |  4096 |  4 |    1037.7 |     931.6 | -10.2% |     217.4 | -76.7% |      256.2 |  17.9% | |  4096 |  8 |    1010.8 |     925.2 |  -8.5% |     214.7 | -76.8% |      255.4 |  18.9% | |  4096 | 16 |     626.8 |     575.2 |  -8.2% |     115.4 | -79.9% |      129.8 |  12.5% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | |  8192 |  1 |    1264.7 |    1264.0 |  -0.1% |     225.2 | -82.2% |      269.3 |  19.6% | |  8192 |  2 |    1182.1 |    1077.8 |  -8.8% |     222.1 | -79.4% |      264.6 |  19.1% | |  8192 |  4 |    1056.2 |     968.4 |  -8.3% |     217.3 | -77.6% |      257.6 |  18.6% | |  8192 |  8 |    1015.0 |     890.3 | -12.3% |     216.0 | -75.7% |      255.1 |  18.1% | |  8192 | 16 |     616.7 |     571.2 |  -7.4% |     113.1 | -80.2% |      135.5 |  19.8% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | | 16384 |  1 |    1226.4 |    1116.2 |  -9.0% |     221.6 | -80.1% |      262.9 |  18.6% | | 16384 |  2 |    1152.5 |    1080.3 |  -6.3% |     219.0 | -79.7% |      259.1 |  18.3% | | 16384 |  4 |     895.7 |     976.4 |   9.0% |     214.5 | -78.0% |      253.1 |  18.0% | | 16384 |  8 |     985.8 |     961.2 |  -2.5% |     211.0 | -78.0% |      252.2 |  19.5% | | 16384 | 16 |     567.6 |     565.2 |  -0.4% |     110.0 | -80.5% |      133.6 |  21.5% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | | 32768 |  1 |    1107.3 |     913.8 | -17.5% |     217.2 | -76.2% |      257.1 |  18.3% | | 32768 |  2 |    1026.4 |     998.0 |  -2.8% |     214.2 | -78.5% |      252.2 |  17.7% | | 32768 |  4 |     942.1 |     908.1 |  -3.6% |     209.9 | -76.9% |      246.8 |  17.6% | | 32768 |  8 |     896.2 |     798.5 | -10.9% |     206.2 | -74.2% |      245.3 |  19.0% | | 32768 | 16 |     549.0 |     504.9 |  -8.0% |     109.6 | -78.3% |      127.7 |  16.5% | |  bpc  | #T ||      Zip ||     ZipC | % diff || PureJava | % diff || PureJavaC | % diff | | 65536 |  1 |    1101.6 |    1078.4 |  -2.1% |     217.2 | -79.9% |      256.3 |  18.0% | | 65536 |  2 |    1026.1 |     846.7 | -17.5% |     213.7 | -74.8% |      253.0 |  18.4% | | 65536 |  4 |     936.5 |     841.9 | -10.1% |     210.1 | -75.0% |      246.7 |  17.4% | | 65536 |  8 |     899.7 |     886.5 |  -1.5% |     207.9 | -76.5% |      242.9 |  16.8% | | 65536 | 16 |     497.8 |     511.7 |   2.8% |     107.1 | -79.1% |      124.2 |  16.0% |","closed","","dchuyko","2017-11-13T10:49:07Z","2018-01-11T16:09:22Z"
"","417","YARN-8785-branch-3.1.002.patch","https://jira.apache.org/jira/browse/YARN-8785","closed","","simonprewo","2018-09-21T13:33:23Z","2018-10-01T21:00:05Z"
"","332","The log about the node status is changed from debug or info to warn w…","https://issues.apache.org/jira/browse/YARN-7809","open","","zhengChina","2018-01-24T14:03:09Z","2019-04-08T23:57:58Z"
"","149","YARN-5800: Deleted LinuxContainerExecutor comment from yarn-default.xml","https://issues.apache.org/jira/browse/YARN-5800 - Removed the comment about LinuxContainerExecutor from the yarn-default.xml file","closed","","HorizonNet","2016-10-30T08:11:13Z","2016-11-05T10:14:48Z"
"","1","MAPREDUCE-6096.SummarizedJob Class Improvment","https://issues.apache.org/jira/browse/MAPREDUCE-6096  SummarizedJob class should be Improvment  When I Parse the JobHistory in the HistoryFile,I use the Hadoop System's map-reduce-client-core project org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser class and HistoryViewer$SummarizedJob to Parse the JobHistoryFile(Just Like job_1408862281971_489761-1410883171851_XXX.jhist)  and it throw an Exception Just Like  Exception in thread ""pool-1-thread-1"" java.lang.NullPointerException at org.apache.hadoop.mapreduce.jobhistory.HistoryViewer$SummarizedJob.(HistoryViewer.java:626) at com.jd.hadoop.log.parse.ParseLogService.getJobDetail(ParseLogService.java:70)  After I'm see the SummarizedJob class I find that attempt.getTaskStatus() is NULL ， So I change the order of attempt.getTaskStatus().equals (TaskStatus.State.FAILED.toString()) to  TaskStatus.State.FAILED.toString().equals(attempt.getTaskStatus())  and it works well .","open","","piaoyu","2014-09-18T09:36:59Z","2021-10-04T18:12:25Z"
"","446","HDFS-14147: Back port of HDFS-13056 to the 2.9 branch","https://issues.apache.org/jira/browse/HDFS-14147","open","","yzhou2001","2018-12-13T00:44:22Z","2019-02-06T11:27:40Z"
"","220","HADOOP-14406. Support slf4j logger log level get and set","https://issues.apache.org/jira/browse/HADOOP-14406","closed","","maobaolong","2017-05-10T09:31:53Z","2019-08-08T08:30:28Z"
"","105","HADOOP-13320. Fix arguments check in the WordCount v2.0 in Doc. Contr…","https://issues.apache.org/jira/browse/HADOOP-13320  Fixed the check on the number of parameters.","closed","","pippobaudos","2016-06-24T13:58:31Z","2016-07-01T16:35:38Z"
"","110","[HADOOP-13075] Adding support for SSE-KMS, SSE-C and SSE-S3","https://issues.apache.org/jira/browse/HADOOP-13075","closed","","fedecz","2016-07-06T06:13:22Z","2019-01-23T07:05:24Z"
"","190","Huichun/mx net","https://github.com/apache/hadoop/compare/trunk...lhcxx:huichun-MXNet?expand=1#files_bucket","open","","lhcxx","2017-02-09T04:42:34Z","2019-04-08T23:57:37Z"
"","137","HADOOP-10225","Hi Folks, This PR is an attempt to address https://issues.apache.org/jira/browse/HADOOP-10225. The patch can be tested by running  ``` mvn release:clean release:prepare -DautoVersionSubmodules=true -DdryRun=true ```  Please let me know once it has been tested and I can squash commits into one.","closed","","lewismc","2016-10-10T19:57:07Z","2019-01-10T18:35:55Z"
"","207","HADOOP-14237. S3A Support Shared Instance Profile Credentials Across All Instances","Hi @steveloughran @liuml07   Yet another patch that I made a few months back. I explained the issue at https://issues.apache.org/jira/browse/HADOOP-14237  This pull request is aiming for more open discussions rather than a complete solution. It would be great if you could offer your thoughts.","open","","kazuyukitanimura","2017-03-25T00:43:51Z","2019-04-08T23:57:39Z"
"","208","HADOOP-14239. S3A Retry Multiple S3 Key Deletion","Hi @steveloughran @liuml07   Sorry for sending may requests.  I explained the problem here https://issues.apache.org/jira/browse/HADOOP-14239  This pull requests recursively retries to delete only S3 keys that are previously failed to delete during the multiple object deletion because aws-java-sdk retry does not help. If it still fails, it will fall back to the single deletion.","closed","","kazuyukitanimura","2017-03-25T01:35:52Z","2017-04-01T17:55:24Z"
"","206","HADOOP-14235. S3A Path does not understand colon (:) when globbing","Hi @steveloughran @liuml07   I explained the issue at https://issues.apache.org/jira/browse/HADOOP-14235  This pull request fixes the issue and does not break other things as far as I know. (I also ran the unit tests).  Probably, #204 should also fix this issue. This pull request is for a short-term solution in case anyone is interested.","closed","","kazuyukitanimura","2017-03-24T23:54:28Z","2017-04-01T17:54:54Z"
"","203","HADOOP-13371. S3A globber to use bulk listObject call over recursive directory scan","Hi @steveloughran   This pull request is for fixing (mitigating) the issue of [HADOOP-13371](https://issues.apache.org/jira/browse/HADOOP-13371).  With this patch, it now passes the filter before glob happens.  I had an issue of getting OOM for globbing large s3 buckets before since it kept all possible paths and the filtering happened at the end. Now this patch prunes unnecessary paths with the filter first. I applied this patch to our production pipelines, things run flawlessly. This should be applicable to branch-2.8 as well.  Thanks in advance for reviewing this.","closed","","kazuyukitanimura","2017-03-18T00:51:34Z","2017-04-01T17:50:26Z"
"","293","YARN-7480 Renders tooltip on em-table cells","Helper for rendering tooltip on em-table cells. Helpful for columns, where the field name is large","closed","","skmvasu","2017-11-20T09:00:00Z","2017-11-28T17:12:46Z"
"","170","HDFS-10930","HDFS-10930.10","closed","","xiaoyuyao","2016-12-06T05:18:57Z","2016-12-06T22:46:27Z"
"","502","HDDS-919. Enable prometheus endpoints for Ozone datanodes","HDDS-846 provides a new metric endpoint which publishes the available Hadoop metrics in prometheus friendly format with a new servlet.  Unfortunately it's enabled only on the scm/om side. It would be great to enable it in the Ozone/HDDS datanodes on the web server of the HDDS Rest endpoint.   See: https://issues.apache.org/jira/browse/HDDS-919","closed","ozone,","elek","2019-02-19T14:26:46Z","2019-03-05T20:04:58Z"
"","550","HDDS-1214. Enable tracing for the datanode read/write path","HDDS-1150 introduced distributed for ozone components. But we have no trace context propagation between the clients and Ozone Datanodes.  As we use Grpc and Ratis on this RPC path the full tracing could be quite complex: we should propagate the trace id in Ratis and include it in all the log entries.  I propose a simplified solution here: to trace only the StateMachine operations.  As Ratis is a library we provide the implementation of the appropriate Raft elements especially the StateMachine and the raft messages. We can add the tracing information to the raft messages (in fact, we already have this field) and we can restore the tracing context during the StateMachine operations.  This approach is very simple (only a few lines of codes) and can show the time of the real write/read operations, but can't see the internals of the Ratis operations.  See: https://issues.apache.org/jira/browse/HDDS-1214","closed","ozone,","elek","2019-03-04T14:25:47Z","2019-03-12T10:36:44Z"
"","492","HDDS-1117. Add async profiler to the hadoop-runner base container image.","HDDS-1116 provides a simple servlet to execute async profiler (https://github.com/jvm-profiling-tools/async-profiler) thanks to the Hive developers.  To run it in the docker-composed based example environments we should add it to the apache/hadoop-runner base image.   Note: The size is not significant, the downloadable package is 102k.  See: https://issues.apache.org/jira/browse/HDDS-1117","closed","ozone,","elek","2019-02-15T14:47:28Z","2019-03-07T08:59:16Z"
"","496","HDDS-1129. Fix findbug/checkstyle errors in hadoop-hdds projects.","HDDS-1114 fixed all the findbug/checkstyle problems but in the mean time new patches are committed with newer error.  Here I would like to cleanup the projects again.  (Except the static field in RatisPipelineProvider which will be ignored in this patch and tracked in HDDS-1128)  See: https://issues.apache.org/jira/browse/HDDS-1129","closed","ozone,","elek","2019-02-18T17:40:18Z","2019-02-21T13:19:21Z"
"","501","HDDS-1089. Disable OzoneFSStorageStatistics for hadoop versions older than 2.8","HDDS-1033 introduced OzoneFSStorageStatistics for OzoneFileSystem. It uses the StorageStatistics which is introduced in HADOOP-13065 (available from the hadoop2.8/3.0).  Using older hadoop (for example hadoop-2.7 which is included in the spark distributions) is not possible any more even with using the isolated class loader (introduced in HDDS-922).  Fortunately it can be fixed:   *  We can support null in storageStatistics field with checking everywhere before call it.   *  We can create a new constructor of OzoneClientAdapterImpl without using OzoneFSStorageStatistics): If OzoneFSStorageStatistics is not in the method/constructor signature we don't need to load it.   * We can check the availability of HADOOP-13065 and if the classes are not in the classpath we can skip the initialization of the OzoneFSStorageStatistics  See: https://issues.apache.org/jira/browse/HDDS-1089","closed","ozone,","elek","2019-02-19T14:20:33Z","2019-03-07T09:00:05Z"
"","604","HDDS-1259. OzoneFS classpath separation is broken by the token validation","hadoop-ozone-filesystem-lib-legacy-0.4.0-SNAPSHOT.jar can't work any more together with older hadoop version, after the change of HDDS-1183.  {code} 2019-03-13 13:48:51 WARN  FileSystem:3170 - Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.ozone.OzoneFileSystem could not be instantiated 2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration 2019-03-13 13:48:51 WARN  FileSystem:3174 - java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/hdds/conf/OzoneConfiguration 	at java.lang.Class.forName0(Native Method) 	at java.lang.Class.forName(Class.java:348) 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2332) 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2297) 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393) 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3208) 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3240) 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:121) 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3291) 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3259) 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:470) 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) 	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325) 	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:245) 	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:228) 	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:103) 	at org.apache.hadoop.fs.shell.Command.run(Command.java:175) 	at org.apache.hadoop.fs.FsShell.run(FsShell.java:315) 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) 	at org.apache.hadoop.fs.FsShell.main(FsShell.java:378) Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hdds.conf.OzoneConfiguration 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) 	... 21 more {code}  And Ozone file system current jar is compatible only with hadoop 3.2 after the latest HA change. It means that ozonefs is broken everywhere where the hadoop version is older than 3.2.  See: https://issues.apache.org/jira/browse/HDDS-1259","closed","ozone,","elek","2019-03-14T14:13:54Z","2019-03-15T16:43:02Z"
"","560","HDDS-1226. ozone-filesystem jar missing in hadoop classpath","hadoop-ozone-filesystem-lib-*.jar is missing in hadoop classpath.  This PR aims at having a shaded hadoop-ozone-filesystem-lib-*.jar file in `${OZONE_HOME}/share/hadoop/ozonefs/` path","closed","ozone,","vivekratnavel","2019-03-06T01:41:11Z","2019-03-13T11:08:27Z"
"","619","HADOOP-16197 S3AUtils.translateException to map CredentialInitializationException to AccessDeniedException","HADOOP-16197 S3ATestUtils.translateException to map CredentialInitializationException to AccessDeniedException  Change-Id: Ie98ca5210bf0009f297edbcacf1fc6dfe5ea70cd","closed","fs/s3,","steveloughran","2019-03-18T18:55:57Z","2019-08-08T11:05:54Z"
"","609","HADOOP-16193. add extra S3A MPU test to see what happens if a file is created during the MPU","HADOOP-16193. add extra S3A MPU test to see what happens if a file is created during the MPU      Change-Id: I15942ff8c7a772e1bf718bccbe4a249d20fa3ef2","closed","failed,","steveloughran","2019-03-14T19:57:59Z","2021-10-15T19:46:16Z"
"","606","HADOOP-16190. S3A copyFile operation to include source versionID or etag in the copy request","HADOOP-16190. S3A copyFile operation to include source versionID or etag in the copy request  This patch adds the constraints on the request, and maps a 412 response to a RemoteFileChangedException.  No obvious test for this. The way to do it would be to get an invalid etag/version in to the request and see what happens, which would complicate the copy API a bit -but is something we will need for etag/version tracking in s3guard anyway....  Change-Id: I4b229336ba2d57018bd8b66888b807074419598e","closed","fs/s3,","steveloughran","2019-03-14T15:24:13Z","2021-10-15T19:46:11Z"
"","551","HADOOP-16162 Remove unused Job Summary Appender configurations from log4j.properties","HADOOP-16162 Remove unused Job Summary Appender configurations from log4j.properties  The Job Summary Appender (JSA) is introduced in MAPREDUCE-740 to provide the summary information of the job's runtime. And this appender is only referenced by the logger defined in `org.apache.hadoop.mapred.JobInProgress$JobSummary`. However, this class has been removed in MAPREDUCE-4266 together with other M/R1 files. This appender is no longer used and I guess we can remove it.","closed","","coder-chenzhi","2019-03-04T16:29:12Z","2019-03-05T04:43:45Z"
"","525","HADOOP-16150. ChecksumFileSystem doesn't wrap concat()","HADOOP-16150. ChecksumFileSystem doesn't wrap concat()  This intercepts concat() To throw an UnsupportedOperationException. Without this the concat() call is passes straight down to the wrapped FS, so, if the underlying FS does support concat(), concatenated files don't have checksums  It also disables the test TestLocalFSContractMultipartUploader, as the service-loader mechanism to create an MPU uploader needs to be replaced by an API call in the filesystems, as proposed by HDFS-13934  Contributed by Steve Loughran.  Change-Id: I85fc1fc9445ca0b7d325495d3bc55fe9f5e5ce52","closed","","steveloughran","2019-02-27T22:59:38Z","2021-10-15T19:50:58Z"
"","539","HADOOP-16109. Parquet reading S3AFileSystem causes EOF","HADOOP-16109. Parquet reading S3AFileSystem causes EOF  Nobody gets seek right. No matter how many times they think they have.  Reproducible test from: Dave Christianson Fixed seek() logic: Steve Loughran","closed","","steveloughran","2019-03-01T14:30:40Z","2021-10-15T19:50:53Z"
"","535","HADOOP-16109. Parquet reading S3AFileSystem causes EOF","HADOOP-16109. Parquet reading S3AFileSystem causes EOF  Nobody gets seek right. No matter how many times they think they have.  Reproducible test from:  Dave Christianson Fixed seek() logic: Steve Loughran","closed","","steveloughran","2019-02-28T19:47:07Z","2019-03-01T14:29:27Z"
"","631","HADOOP-16058. S3A tests to include Terasort. - branch-3.2","HADOOP-16058. S3A tests to include Terasort.  Contributed by Steve Loughran.  This includes  - HADOOP-15890. Some S3A committer tests don't match ITest* pattern; don't run in maven  - MAPREDUCE-7090. BigMapOutput example doesn't work with paths off cluster fs  - MAPREDUCE-7091. Terasort on S3A to switch to new committers  - MAPREDUCE-7092. MR examples to work better against cloud stores  This is the branch-3.2 patch. Testing: run the new S3A ITests","closed","","steveloughran","2019-03-21T16:23:52Z","2019-03-25T14:52:34Z"
"","565","HADOOP-16058 S3A tests to include Terasort","HADOOP-16058. Add S3A tests to run terasort for the magic and directory committers.  Contributed by Steve Loughran.  Contains:  MAPREDUCE-7090. BigMapOutput example doesn't work with paths off cluster fs  MAPREDUCE-7091. Terasort on S3A to switch to new committers  MAPREDUCE-7092. MR examples to work better against cloud stores  Bonus feature: prints the results to see which committers are faster in the specific test setup. As that's a function of latency to the store, bandwidth and size of jobs, it's not at all meaningful, just interesting.","closed","","steveloughran","2019-03-06T21:29:58Z","2021-10-15T19:50:55Z"
"","530","HADOOP-16058 S3A tests to include Terasort","HADOOP-16058. Add S3A tests to run terasort for the magic and directory committers.  Contributed by Steve Loughran.  Contains:  MAPREDUCE-7090. BigMapOutput example doesn't work with paths off cluster fs  MAPREDUCE-7091. Terasort on S3A to switch to new committers  MAPREDUCE-7092. MR examples to work better against cloud stores  Bonus feature: prints the results to see which committers are faster in the specific test setup. As that's a function of latency to the store, bandwidth and size of jobs, it's not at all meaningful, just interesting.","closed","","steveloughran","2019-02-28T11:58:22Z","2019-03-06T21:28:57Z"
"","577","HADOOP-16058 S3A to support terasort","HADOOP-16058 S3A to support terasort  Change-Id: Icd7911bd44d1382287808f60566a2405265adbd4","closed","","steveloughran","2019-03-08T14:07:38Z","2021-10-15T19:46:20Z"
"","576","HADOOP-16058 S3A tests to include Terasort","HADOOP-16058 S3A tests to include Terasort","closed","","steveloughran","2019-03-08T11:58:50Z","2019-03-28T13:38:30Z"
"","630","HADOOP-15999 S3Guard OOB: improve test resilience and probes","HADOOP-15999 improve test resilience and probes  * Add delays long enough for timestamps to be different * Add delays for S3 to stabilize after writes/deletes, so that listings and HEAD calls will get the new value, not old ones * probes for differences look for file lengths ahead of timestamps, for more tangible failures. * and they validate the raw FS status acquired after the stabiliziation delay * package private (currently) probe for S3A to verify that an FS instances considers its store to be authoritative. Currently we've been checking the config, but to really know what's happening: lets query the internal state of FS.  Change-Id: Ib0184a2aacbec1e4b316cb8cad0265bd0b579bcd  This is PR #624 with another patch applied","closed","fs/s3,","steveloughran","2019-03-20T21:31:29Z","2019-03-28T16:01:30Z"
"","602","HADOOP-15625. S3A input stream to use etags/version number to detect changed source files.","HADOOP-15625. S3A input stream to use etags/version number to detect changed source files.  Author: Ben Roling   Initial patch from Brahma Reddy Battula.  Change-Id: I2c2ef1b53d5b212f7e88f3e66d59c867c9c781d7","closed","","steveloughran","2019-03-13T22:20:53Z","2019-03-25T15:03:23Z"
"","533","HADOOP-14630  Contract Tests to verify create, mkdirs and rename under a file is forbidden","HADOOP-14630. Contract Tests to verify create, mkdirs and rename under a file is forbidden","closed","","steveloughran","2019-02-28T15:27:14Z","2020-03-09T17:07:43Z"
"","242","HADOOP-14560: Make HttpServer2 backlog size configurable","HADOOP-14560","closed","","alex-krash","2017-06-21T09:58:54Z","2017-08-17T08:06:34Z"
"","564","HADOOP-13327 Output Stream Specification","HADOOP-13327: Add OutputStream + Syncable to the Filesystem Specification  * defines what an output stream should do * And what implementations of Syncable MUST do if they declare they support the method. * Consistently declare behaviors in our streams *Including for some (S3ABlockOutputStream) state tracking: no operations once closed; if an error has occurred, future operations raise it, etc. * With some more utility classes in org.apache.hadoop.fs.impl to aid this","closed","","steveloughran","2019-03-06T21:26:30Z","2019-03-08T11:51:58Z"
"","532","HADOOP-13327: Add OutputStream + Syncable to the Filesystem Specification","HADOOP-13327: Add OutputStream + Syncable to the Filesystem Specification  * defines what an output stream should do * And what implementations of Syncable MUST do if they declare they support the method. * Consistently declare behaviors in our streams * Including for some (S3ABlockOutputStream) state tracking: no operations once closed; if an error has occurred, future operations raise it, etc. * With some more utility classes in org.apache.hadoop.fs.impl to aid this","closed","","steveloughran","2019-02-28T15:20:53Z","2019-03-06T21:18:57Z"
"","575","HADOOP-13327 Output Stream Specification","HADOOP-13327 Output Stream Specification  Change-Id: I1b6bc258a40a8bd57879d9edc3e5bb1303f0fff2","closed","","steveloughran","2019-03-08T11:54:37Z","2019-11-05T17:37:57Z"
"","96","HADOOP-13192.  org.apache.hadoop.util.LineReader match recordDelimiter has a bug","HADOOP-13192  org.apache.hadoop.util.LineReader match recordDelimiter has a bug","closed","","zhudebin","2016-05-23T11:48:17Z","2016-06-13T11:34:39Z"
"","238","Bumping up pom file hadoop version","Hadoop version was recently changed to 3.0.0-alpha4 while services-api and slider pom files were compiled against hadoop3.0.0-alpha3  This PR is bumping up hadoop version to avoid compilation issues.","closed","","pgaref","2017-06-16T20:30:12Z","2017-06-20T02:43:19Z"
"","282","HADOOP-14971 Merge S3A committers into trunk","HADOOP 13786 & MAPREDUCE-6823 code as a PR for better review","closed","","steveloughran","2017-10-20T21:52:48Z","2021-10-15T19:47:17Z"
"","387","Creating branch for hadoop-6671","git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HADOOP-6671@931593 13f79535-47bb-0310-9956-ffa450edef68","closed","","LukeZHANGZ","2018-05-17T06:00:16Z","2019-01-18T01:11:08Z"
"","69","Creating updated branch for Hadoop-6671","git-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HADOOP-6671-2@956573 13f79535-47bb-0310-9956-ffa450edef68","closed","","iqidian","2016-01-19T13:48:24Z","2020-07-29T16:15:42Z"
"","341","HADOOP-15231. Metrics sink for Wavefront.","for use with Wavefront Proxy on host and port. requires the following parameters to be configured server_host server_port metrics_prefix metrics_source  Sends wavefront compatible data format to wavefront proxy which will then be ingested by wavefront server. Data format is described [here](https://docs.wavefront.com/wavefront_data_format.html).","open","","howardyoo","2018-02-14T02:58:10Z","2021-10-20T14:03:15Z"
"","503","HDDS-1139 : Fix findbugs issues in HDDS-1085.","Fixing findbugs issues in HDDS-1085. If all the issues are solved, I will create a JIRA and attach the patch.","closed","","avijayanhwx","2019-02-19T19:39:39Z","2019-02-19T20:35:31Z"
"","504","HDDS-1139 : Fix findbugs issues caused by HDDS-1085.","Fixing findbugs issues caused by HDDS-1085.","closed","ozone,","avijayanhwx","2019-02-19T20:35:51Z","2019-02-20T00:32:48Z"
"","313","YARN-7536. Em table fix","Fixes UX issue with em-table - expands filter by default - sets minitems to filters to 1","closed","","skmvasu","2017-12-13T06:24:13Z","2017-12-13T11:17:01Z"
"","308","YARN-7536. Em table fix","Fixes UX issue with em-table - expands filter by default - sets minitems to filters to 1","closed","","skmvasu","2017-12-01T06:11:12Z","2017-12-13T06:25:06Z"
"","422","Updating insecure version of Jetty to the lattest","Fixes https://issues.apache.org/jira/browse/HADOOP-15815","closed","","borisvu","2018-10-03T14:48:32Z","2018-10-06T20:10:07Z"
"","473","HADOOP-11223. Create UnmodifiableConfiguration","Fixes https://issues.apache.org/jira/browse/HADOOP-11223 by creating an immutable Configuration class.  This class will throw UnsupportedOperationException on any mutable method calls.  I am not sure what branch or version this is appropriate to apply to so I am doing a pull request against trunk.","closed","","milleruntime","2019-01-28T19:28:24Z","2019-09-04T22:05:36Z"
"","512","HDDS-1158. TestOzoneManagerHA.testTwoOMNodesDown is failing with ratis error.","Fixes failing test TestOzoneManagerHA#testTwoOMNodesDown()","closed","ozone,","hanishakoneru","2019-02-21T20:02:01Z","2019-02-23T18:20:29Z"
"","315","YARN-7648. Fix attempts UI when app run fails","Fixes applications tab rendering when the app attempts have failed","closed","","skmvasu","2017-12-13T10:21:41Z","2019-07-02T03:06:33Z"
"","340","HADOOP-15230. fixes GraphiteSink to support point tags in correct format.","Fixed GraphiteSink to support point tags in correct format, having appeared at the end and also having no space as its name and value pair. [JIRA Ticket](https://issues.apache.org/jira/browse/HADOOP-15230)","closed","","howardyoo","2018-02-14T01:07:03Z","2021-02-07T22:29:10Z"
"","98","HADOOP-13260: Fixing broken image in markdown","Fixed broken image link in hadoop-hdfs-project/hadoop-hdfs/src/site/markdown/CentralizedCacheManagement.md file","closed","","redborian","2016-06-13T05:27:05Z","2016-06-16T04:25:52Z"
"","404","HDFS-13771. Fix enableManagedDfsDirsRedundancy typo in creating MiniDFSCluster","fix wrong parameter while calling initNameNodeConf issue","closed","","wilderchen","2018-07-26T16:33:06Z","2019-09-03T06:24:58Z"
"","141","HADOOP-13708. Fix typos in *.md documents, some of which are suggested by Andrew Wang.","Fix typos in *.md documents, some of which are suggested by Andrew Wang. Touched markdown files listed: - hadoop-tools/hadoop-archives/src/site/markdown/HadoopArchives.md.vm - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/testing.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/fsdatainputstream.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/filesystem.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/notation.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/model.md - hadoop-common-project/hadoop-common/src/site/markdown/InterfaceClassification.md - hadoop-common-project/hadoop-common/src/site/markdown/ClusterSetup.md - hadoop-common-project/hadoop-common/src/site/markdown/Compatibility.md","closed","","danix800","2016-10-13T04:46:01Z","2016-10-14T00:50:39Z"
"","432","HADOOP-16469. Typo in s3a committers.md doc","Fix typo in the committers.md file","closed","","lopes-felipe","2018-10-22T17:15:04Z","2019-07-30T11:52:09Z"
"","433","HADOOP-15870 S3AInputStream.remainingInFile should use nextReadPos","Fix the nextPos","closed","","lqjack","2018-10-23T14:39:22Z","2019-10-10T21:01:45Z"
"","394","fix the bug of the refresh disk configuration","Fix the bug of refresh disk configuration  [HDFS-13677](https://issues.apache.org/jira/browse/HDFS-13677)","closed","","ZanderXu","2018-06-14T04:52:51Z","2022-05-27T01:32:29Z"
"","140","HADOOP-13708. Fix a few typos in site *.md documents","Fix several typos in site *.md documents.    Touched documents listed: - hadoop-tools/hadoop-archives/src/site/markdown/HadoopArchives.md.vm - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/testing.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/fsdatainputstream.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/filesystem.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/notation.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/introduction.md - hadoop-common-project/hadoop-common/src/site/markdown/filesystem/model.md - hadoop-common-project/hadoop-common/src/site/markdown/InterfaceClassification.md - hadoop-common-project/hadoop-common/src/site/markdown/ClusterSetup.md - hadoop-common-project/hadoop-common/src/site/markdown/Compatibility.md","closed","","danix800","2016-10-11T18:06:36Z","2016-10-13T04:39:58Z"
"","342","Fix NULL in table-datanodes when node is down","Fix possible null-warning 'Requested unknown parameter' in WEB UI when one of datanodes is down","open","","yuriymalygin","2018-02-16T11:33:34Z","2019-08-20T18:09:09Z"
"","356","YARN-8056. Fix possible NPE when start Application Master","Fix possible NPE when yarn.resourcemanager.webapp.address or yarn.resourcemanager.webapp.https.address is not set in yarn-site.xml (incomplete configuration of HA).","closed","","yuriymalygin","2018-03-21T13:37:51Z","2022-02-25T16:28:43Z"
"","373","HADOOP-15428","fix error prompt","open","","lqjack","2018-04-30T15:44:44Z","2019-02-06T11:26:40Z"
"","385","HADOOP-15455. Incorrect debug message in KMSACL#hasAccess","Fix debug message.","closed","","phstudy","2018-05-10T18:24:08Z","2018-06-19T02:32:13Z"
"","369","HADOOP-15418","fix ConcurrentModifyException","open","","lqjack","2018-04-27T17:56:22Z","2019-02-06T11:26:34Z"
"","441","HDFS-14099","fix bug where decompressing multiple frames in ZStandardDecompressor[HDFS-14099](https://issues.apache.org/jira/browse/HDFS-14099)","closed","","ZanderXu","2018-11-26T14:13:19Z","2019-08-29T04:14:07Z"
"","99","HADOOP-13192. org.apache.hadoop.util.LineReader  match recordDelimiter has a bug","fix bug HADOOP-13192 org.apache.hadoop.util.LineReader  match recordDelimiter has a bug","closed","","zhudebin","2016-06-13T11:32:11Z","2016-06-23T14:34:20Z"
"","95","fix bug HADOOP-13192","fix bug HADOOP-13192","closed","","zhudebin","2016-05-23T11:43:32Z","2016-05-23T11:44:05Z"
"","559","SUBMARINE-41:Fix ASF warning in submarine","Fix ASF warning in submarine","closed","","dineshchitlangia","2019-03-06T01:19:28Z","2019-03-07T05:07:19Z"
"","554","SUBMARINE-41:Fix ASF License warnings in Submarine","Fix ASF License warnings in Submarine","closed","","dineshchitlangia","2019-03-05T06:24:44Z","2019-03-05T22:18:15Z"
"","212","[YARN-6478] Fix a spelling mistake","Fix a spelling mistake in FileSystemTimelineWriter.java. The ""writeSummmaryEntityLogs"" should be ""writeSummaryEntityLogs"".  https://issues.apache.org/jira/browse/YARN-6478","closed","","lingjinjiang","2017-04-13T09:12:06Z","2019-07-20T01:07:45Z"
"","87","modify docker launch script","First launch the docker container and then get the container pid","closed","","Tartarus0zm","2016-03-30T02:17:42Z","2018-07-18T10:47:09Z"
"","523","HDDS-623. On SCM UI, Node Manager info is empty","Fields like below are empty  Node Manager: Minimum chill mode nodes  Node Manager: Out-of-node chill mode  Node Manager: Chill mode status  Node Manager: Manual chill mode  Please see attached screenshot !Screen Shot 2018-10-10 at 4.19.59 PM.png!  See: https://issues.apache.org/jira/browse/HDDS-623","closed","ozone,","elek","2019-02-27T14:35:08Z","2019-03-04T22:35:01Z"
"","177","MAPREDUCE-6827. Fixed bug: the second foreach-loop was not executed","Failed to traverse `Iterable values` with `foreach` at the second time in reduce() method, because the second foreach-loop was not executed.  [JIRA MAPREDUCE-6827](https://issues.apache.org/jira/browse/MAPREDUCE-6827)  The following code is a reduce() method (of WordCount):   	public static class WcReducer extends Reducer {  		@Override 		protected void reduce(Text key, Iterable values, Context context) 				throws IOException, InterruptedException {  			// print some logs 			List vals = new LinkedList<>(); 			for(IntWritable i : values) { 				vals.add(i.toString()); 			} 			System.out.println(String.format("">>>> reduce(%s, [%s])"", 					key, String.join("", "", vals)));  			// sum of values 			int sum = 0; 			for(IntWritable i : values) { 				sum += i.get(); 			} 			System.out.println(String.format("">>>> reduced(%s, %s)"", 					key, sum));  			context.write(key, new IntWritable(sum)); 		} 	}  After running it, we got the result that the value of the variable `sum` is always 0！  After debugging, it was found that the second foreach-loop was not executed, and the root cause was the returned value of `Iterable.iterator()`, it returned the same instance in the two calls called by foreach-loop. In general, **`Iterable.iterator()` should return a new instance in each call**, such as `ArrayList.iterator()`. This patch fixed the bug.  Signed-off-by: Javeme","closed","","javeme","2016-12-30T12:10:54Z","2017-01-04T11:12:30Z"
"","17","Branch 2.5.0","extended distcp to create a file containing all files that would be / have been deleted from the target using the -delete flag","closed","","s-bolz","2015-03-30T14:46:51Z","2020-07-31T05:05:46Z"
"","55","HADOOP-9844 NPE on sasl","existing patch rebased to trunk","closed","","steveloughran","2015-11-23T10:15:03Z","2021-10-15T19:47:15Z"
"","520","HADOOP-16149 hadoop-mapreduce-client-app build not converging","Excludes the junit and hamcrest dependencies where they are out of date Tested: locally  Change-Id: If95b12b223770b057041f99f0f8fd8ba370c377f","closed","","steveloughran","2019-02-26T15:26:42Z","2019-02-27T22:10:36Z"
"","50","HADOOP-11601 FileStatus.getBlocksize() >0 for non-empty files","Enhance FS spec & tests to mandate FileStatus.getBlocksize() >0 for non-empty files","closed","","steveloughran","2015-11-22T18:06:31Z","2016-11-17T22:31:59Z"
"","3","[HADOOP-10724] better interoperation with `sort -h`","do not insert a space between number and units in StringUtils.TraditionalBinaryPrefix.long2String to work better with `sort -h`","open","","sam-s","2014-09-23T04:10:23Z","2019-04-08T23:57:22Z"
"","538","HDFS-14318:dn cannot be recognized and must be restarted to recognize the Repaired disk","dn detected that disk a has failed. After disk a is repaired, dn cannot be recognized and must be restarted to recognize     I make a patch to dn for recognize the repaired disk without restart dn","closed","","hunshenshi","2019-03-01T05:02:11Z","2019-07-17T02:25:54Z"
"","253","[ Indiscoverable bug in HDFS] FsDatasetSpi.isValidBlock() lacks null pointer check inside and neither do the callers","Dear Hadoop Developers, I'm from Alibaba, China. Recently, I meet a scenario where user want to migrate all the data in the old volumes to newly added volumes. Although HDFS now has a DiskBalancer tool, but it doesn't meet the requirement of us. So, we develop a new tool DiskMigration, which can migrate all the data in the current volumes to the new volumes and keep balance of data distribution at the same time. After introduce the work I'm doing, now we get to the point of the bug of the newest version hadoop3.0: BlockIteratorImpl.nextBlock() will look for the blocks in the source volume, if there are no blocks any more, it will return null up to DiskBalancer.getBlockToCopy(). However, the DiskBalancer.getBlockToCopy() will check whether it's a valid block. When I look into the FsDatasetSpi.isValidBlock(), I find that it doesn't check the null pointer! In fact, we firstly need to check whether it's null or not, or exception will occur. This bug is hard to find, because the DiskBalancer hardly copy all the data of one volume to others. Even if some times we may copy all the data of one volume to other volumes, when the bug occurs, the copy process has already done. However, when we try to copy all the data of two or more volumes to other volumes in more than one step, the thread will be shut down, which is caused by the bug above. The bug can fixed by two ways: 1)Before the call of FsDatasetSpi.isValidBlock(), we check the null pointer 2)Check the null pointer inside the implementation of FsDatasetSpi.isValidBlock()","closed","","liumihust","2017-07-25T06:23:40Z","2020-07-31T05:42:58Z"
"","191","Fixed location of service provider configuration file on Azure Data lake Filesystem.","Currently, the provider configuration file is in wrong location -- should be under services folder -- and ADL file system cannot be loaded without registering manually into the configuration. https://docs.oracle.com/javase/tutorial/ext/basics/spi.html#register-service-providers","closed","","jinhyukchang","2017-02-09T16:22:15Z","2019-07-26T11:36:52Z"
"","21","[HADOOP-12081] Support Hadoop on zLinux - fix JAAS authentication issue","Currently the 64 bit check in security/UserGroupInformation.java uses os.arch and checks for ""64"". s390x is returned on IBM's z platform: s390x is 64 bit. Without this change, if we try to use HDFS with Spark, we get a fatal error (unable to login as we can't find a login class). This address fixes said issue by identifying s390x as a 64 bit platform and thus allowing Spark to run on zLinux. A simple fix with very big implications!  I've tested this builds and works as expected on zLinux.","closed","","a-roberts","2015-06-10T17:45:37Z","2015-09-07T10:14:08Z"
"","493","HADOOP-16114 Ensure NetUtils#canonicalizeHost returns same canonicalized host name for a given host","Currently `NetUtils#canonicalizedHost` returns different canonicalized host name for the same host, this patch resolves it.","closed","","Praveen2112","2019-02-15T18:22:55Z","2019-03-07T11:11:00Z"
"","363","Branch 2.9.1","Creating Pull request","closed","","shashank1771","2018-04-17T04:09:04Z","2019-09-11T02:19:57Z"
"","160","HDFS-10930.07.patch","Create a PR for easy review.","closed","","xiaoyuyao","2016-11-12T15:49:31Z","2016-11-29T21:23:59Z"
"","86","HADOOP-12916","Create a github PR for code review of v04 patch on the JIRA.","closed","","xiaoyuyao","2016-03-26T04:23:37Z","2019-01-24T10:34:35Z"
"","655","HADOOP-16218. Findbugs warning of null param in Configuration with Guava update.","Contributed by Steve Loughran.  Change-Id: I461e518ce9a4730b91a8138ad55b39e9a4b0a4b8","closed","","steveloughran","2019-03-28T16:46:05Z","2021-10-15T19:46:14Z"
"","527","HDDS-1093. Configuration tab in OM/SCM ui is not displaying the correct values","Configuration tab in OM/SCM ui is not displaying the correct/configured values, rather it is displaying the default values.","closed","ozone,","vivekratnavel","2019-02-28T01:36:39Z","2019-03-07T02:24:53Z"
"","43","HDFS-9144: libhdfs++ refactoring","Code changes for HDFS-9144 as described in the JIRA.  Removing some templates and traits and restructuring the code for more modularity.","closed","","bobhansen","2015-11-06T21:27:13Z","2019-01-24T10:39:50Z"
"","41","CheckingTheChanges","check","closed","","Shubh91","2015-11-02T18:10:40Z","2020-07-29T16:17:07Z"
"","285","MAPREDUCE-6752: Bad logging practices in mapreduce","Changed the log level of the method that is only used for debugging purpose as discussed here:  https://issues.apache.org/jira/browse/MAPREDUCE-6752","open","","ggribeler","2017-10-30T02:20:37Z","2022-01-31T20:40:50Z"
"","396","classpath description is not full for hadoop cmd","Changed line number 43 where classpath description is not full and changed it to the one provided in hadoopv3:  ""prints the class path needed to get the Hadoop jar and the required libraries""","open","","sandeepksaini","2018-06-23T19:02:19Z","2019-02-06T11:26:55Z"
"","657","HADOOP-16219. Hadoop branch-2 to set java language version to 1.8","Change-Id: Id085c144dc5f71b08ec86f497c51cc494957a664","closed","","steveloughran","2019-03-28T19:37:51Z","2019-03-28T19:44:24Z"
"","162","HDFS-11148. Update DataNode to use StorageLocationChecker at startup.","Change-Id: I664e5c719921ef5e0891bc392f37ee67639a8660","closed","","arp7","2016-11-16T21:27:01Z","2019-07-26T11:21:32Z"
"","366","HADOOP-15410","change scope","closed","","lqjack","2018-04-24T16:07:50Z","2019-08-01T23:06:03Z"
"","322","HADOOP-15163. Fix S3ACommitter documentation","Change remaining mentions of `fs.s3a.committer.tmp.path` to `fs.s3a.committer.staging.tmp.path`. [Related JIRA.](https://issues.apache.org/jira/browse/HADOOP-15163)","closed","","andrioni","2018-01-08T19:33:08Z","2018-01-10T16:31:02Z"
"","365","Hadoop 15410","change log4j scope for test","closed","","lqjack","2018-04-24T16:03:51Z","2018-09-30T05:19:59Z"
"","370","HADOOP-15427","change hadoop_error to hadoop_debug","open","","lqjack","2018-04-28T16:30:45Z","2019-02-06T11:26:35Z"
"","367","HADOOP-15409","change doesBucketExistV2 to verified the acl","open","","lqjack","2018-04-25T11:08:47Z","2019-02-06T11:26:31Z"
"","389","Change of file name to md","By mistakenly while committing the file name is changed to "".md.vm"" so renaming is required to "".md"".","open","","sandeepksaini","2018-06-01T22:12:12Z","2019-04-08T23:58:09Z"
"","454","Fixing bug : https://issues.apache.org/jira/browse/MAPREDUCE-7089","Bug: When a user configures the bufferSize to be 0, the while loop in TestDFSIO$ReadMapper.doIO function hangs endlessly.  solution : check if the bufferSize is greater than 0, if not throw IllegalArgument Exception","open","","sharath-c-n","2018-12-28T18:49:40Z","2019-09-03T03:42:24Z"
"","179","Yarn 5355 branch 2","Bug:  two Name node, one for back up, one for service, if the backup take over the service, it must check the previous service is still alive?  How can you make sure that you send you ssh kill command while you state in Zookeeper is not changed? While the state is changed, and there is an ABA problem appear?   Solution: using an oracle time or sequence to mark the node timestamp","closed","","pengan1990","2017-01-14T02:25:35Z","2017-08-03T06:58:22Z"
"","393","HADOOP-15524 Bounding array size in BytesWritable to Integer.MAX_VALUE - 8","Bounding array size to Integer.MAX_VALUE - 8 to prevent OutOfMemoryError from occurring.","closed","","jsmithe","2018-06-13T15:42:56Z","2020-05-12T18:50:36Z"
"","276","Update DiskBalancer.java","BlockIteratorImpl.nextBlock() will look for the blocks in the source volume, if there are no blocks any more, it will return null up to DiskBalancer.getBlockToCopy(). However, the DiskBalancer.getBlockToCopy() will check whether it's a valid block. When I look into the FsDatasetSpi.isValidBlock(), I find that it doesn't check the null pointer! In fact, we firstly need to check whether it's null or not, or exception will occur. This bug is hard to find, because the DiskBalancer hardly copy all the data of one volume to others. Even if some times we may copy all the data of one volume to other volumes, when the bug occurs, the copy process has already done. However, when we try to copy all the data of two or more volumes to other volumes in more than one step, the thread will be shut down, which is caused by the bug above. The bug can fixed by two ways: 1)Before the call of FsDatasetSpi.isValidBlock(), we check the null pointer 2)Check the null pointer inside the implementation of FsDatasetSpi.isValidBlock()","open","","liumihust","2017-09-19T08:49:34Z","2019-02-06T11:24:52Z"
"","658","HADOOP-16219. [JDK8] Set minimum version of Hadoop 2 to JDK 8.","Based on HADOOP-11858.  Contributed by Robert Kanter.  (cherry picked from commit 4b55642b9d836691592405805c181d12c2ed7e50)  Change-Id: I18a58d5f50b84cb27e1bf1814e527a0c01e9782e","closed","","steveloughran","2019-03-28T19:52:48Z","2021-10-15T19:46:12Z"
"","30","update Shell.java to avoid OOM","avoid out of memory(such as NodeManager OOM), in case running a command which has verbose error log continuously","open","","huahuiyang","2015-08-25T06:16:46Z","2019-04-08T23:57:25Z"
"","72","YARN-4653","Attempt to document YARN security, including HADOOP_TOKEN_FILE_LOCATION propagation","closed","","steveloughran","2016-02-01T15:24:48Z","2021-10-15T19:46:40Z"
"","376","HDFS-13514. Avoid edge case where BUFFER_SIZE is 0","As reported in HDFS-13514, there is a potential bug in the following code block: ``` byte[] data = new byte[BUFFER_SIZE]; long size = 0; while (size >= 0) {   size = in.read(data); } ``` where BUFFER_SIZE is 0 I believe switching to a simple do/while can fix this.","open","","eric-maynard","2018-04-30T20:49:22Z","2021-10-10T08:46:38Z"
"","375","HDFS-13513. Avoid edge case where BUFFER_SIZE is 0","As reported in HDFS-13513, there is a potential bug in the following code block:  byte[] data = new byte[BUFFER_SIZE]; long val= 0; while (val >= 0) {   val = in.read(data); } where BUFFER_SIZE is 0 I believe switching to a simple do/while can fix this.","open","","eric-maynard","2018-04-30T20:48:45Z","2020-07-30T04:35:13Z"
"","374","HDFS-13514. Avoid edge case where BUFFER_SIZE is 0","As reported in [HDFS-13514](https://issues.apache.org/jira/browse/HDFS-13514), there is a potential bug in the following code block: ``` byte[] data = new byte[BUFFER_SIZE]; long size = 0; while (size >= 0) {   size = in.read(data); } ``` where BUFFER_SIZE is 0 I believe switching to a simple do/while can fix this.","closed","","eric-maynard","2018-04-30T20:26:58Z","2018-04-30T20:34:28Z"
"","608","HDDS-1284. Adjust default values of pipline recovery for more resilient service restart","As of now we have a following algorithm to handle node failures:  1. In case of a missing node the leader of the pipline or the scm can detected the missing heartbeats. 2. SCM will start to close the pipeline (CLOSING state) and try to close the containers with the remaining nodes in the pipeline 3. After 5 minutes the pipeline will be destroyed (CLOSED) and a new pipeline can be created from the healthy nodes (one node can be part only one pipwline in the same time).  While this algorithm can work well with a big cluster it doesn't provide very good usability on small clusters:  Use case1:  Given 3 nodes, in case of a service restart, if the restart takes more than 90s, the pipline will be moved to the CLOSING state. For the next 5 minutes (ozone.scm.pipeline.destroy.timeout) the container will remain in the CLOSING state. As there are no more nodes and we can't assign the same node to two different pipeline, the cluster will be unavailable for 5 minutes.  Use case2:  Given 90 nodes and 30 pipelines where all the pipelines are spread across 3 racks. Let's stop one rack. As all the pipelines are affected, all the pipelines will be moved to the CLOSING state. We have no free nodes, therefore we need to wait for 5 minutes to write any data to the cluster.  These problems can be solved in multiple ways:  1.) Instead of waiting 5 minutes, destroy the pipeline when all the containers are reported to be closed. (Most of the time it's enough, but some container report can be missing) 2.) Support multi-raft and open a pipeline as soon as we have enough nodes (even if the nodes already have a CLOSING pipelines).  Both the options require more work on the pipeline management side. For 0.4.0 we can adjust the following parameters to get better user experience:  {code}        ozone.scm.pipeline.destroy.timeout     60s     OZONE, SCM, PIPELINE            Once a pipeline is closed, SCM should wait for the above configured time       before destroying a pipeline.              ozone.scm.stale.node.interval     90s     OZONE, MANAGEMENT            The interval for stale node flagging. Please       see ozone.scm.heartbeat.thread.interval before changing this value.          {code}  First of all, we can be more optimistic and mark node to stale only after 5 mins instead of 90s. 5 mins should be enough most of the time to recover the nodes.  Second: we can decrease the time of ozone.scm.pipeline.destroy.timeout. Ideally the close command is sent by the scm to the datanode with a HB. Between two HB we have enough time to close all the containers via ratis. With the next HB, datanode can report the successful datanode. (If the containers can be closed the scm can manage the QUASI_CLOSED containers)  We need to wait 29 seconds (worst case) for the next HB, and 29+30 seconds for the confirmation. --> 66 seconds seems to be a safe choice (assuming that 6 seconds is enough to process the report about the successful closing)  See: https://issues.apache.org/jira/browse/HDDS-1284","closed","ozone,","elek","2019-03-14T16:05:51Z","2019-03-15T21:51:38Z"
"","529","HDDS-1191. Replace Ozone Rest client with S3 client in smoketests and docs","As it's discussed in the the parent jira the rest support for Ozone Client protocol can be removed to use S3 Rest API instead of that.  Some of the unit tests are already disabled, so it seems to be better to remove it from the documentation (and from the smoketests).  See: https://issues.apache.org/jira/browse/HDDS-1191","closed","ozone,","elek","2019-02-28T08:30:07Z","2019-03-01T08:44:12Z"
"","509","HDDS-1161. Disable failing test which are tracked by a separated jira","As I wrote in the description of the parent Jira I propose to disable (@Ignore) the unit tests which are failing while we are fixing them to get clean Jira response from the PreCommit builds.  All the tests are tracked in a separated jira.  See: https://issues.apache.org/jira/browse/HDDS-1161","closed","ozone,","elek","2019-02-21T13:16:19Z","2019-02-21T21:57:47Z"
"","452","HADOOP-16005: Add XAttr support to WASB and ABFS","As discussed in [HADOOP-16005](https://issues.apache.org/jira/browse/HADOOP-16005), this pull request implements `getXAttr` and `setXAttr` on hadoop-azure's WASB and ABFS file-systems.  The changes were tested against the following Azure storage account configurations:  - WASB: StorageV2, RA-GRS replication in East US (primary) West US (secondary). [WASB test session screenshot](https://user-images.githubusercontent.com/1086421/50362109-699f5a00-0534-11e9-97c9-e8a7cee6e6c6.png). All tests pass and the ABFS tests are skipped as expected.  - ABFS: StorageV2 with Data Lake Storage Gen2 preview enabled, RA-GRS replication in East US (primary) West US (secondary). [ABFS test session screenshot](https://user-images.githubusercontent.com/1086421/50361278-fea05400-0530-11e9-9cb4-cc23dec87cfc.png). All ABFS tests pass but the WASB tests fail since the storage account hasn't implemented the blob endpoints yet.  The test-patch script passed: [test-patch output](https://user-images.githubusercontent.com/1086421/50377952-50aaad80-05f5-11e9-8ea2-b7bf99fc7509.png).","closed","","c-w","2018-12-22T19:25:15Z","2020-05-23T01:15:46Z"
"","19","Update data on checkpoint support for 2.3+","Apparently checkpointing by interval or transaction size are both supported in at least HDFS 2.3, but the documentation does not reflect this.","closed","","mdlinville","2015-05-05T04:47:03Z","2015-05-05T16:37:49Z"
"","310","YARN-7620. Filter queues by nodelabels","Allows users to filter queues by nodelabels","closed","","skmvasu","2017-12-05T10:43:25Z","2018-01-15T07:57:08Z"
"","318","[HADOOP-15214] Make Hadoop compatible with Guava 21","After this change Hadoop could build against Guava 21.0  Justification for migration is the same as previous migrations here: https://issues.apache.org/jira/browse/HADOOP-11032","closed","","medb","2017-12-23T07:27:39Z","2018-02-08T22:50:35Z"
"","621","HADOOP-16090 S3A Client to add explicit support for versioned stores.","Adds the option fs.s3a.versioned.store to enable all policy changes for efficiently working with versioned object stores.  This (initial?) patch reverts to the original probe-then-delete mechanism of cleaning up fake empty directories; the test ITestS3AFileOperationCost.testFakeDirectoryDeletion() verifies that this reduces the number of paths queued for deletion.  Change-Id: I9708ffc3784e8f7e742b5885b48cbaf9600f6232","closed","fs/s3,","steveloughran","2019-03-18T23:23:26Z","2020-06-02T13:23:06Z"
"","403","HDFS-13734. Allow HDFS heapsizes to be configured seperately","Adds option for HDFS_NAMENODE_HEAPSIZE, HDFS_SECONDARYNAMENODE_HEAPSIZE, HDFS_JOURNALNODE_HEAPSIZE and HDFS_DATANODE_HEAPSIZE to hadoop-env.sh so that hdfs daemon JVM heapsizes can be separately configured. This matches the configuration of YARN daemon's heapsizes.","open","","bschell","2018-07-12T22:40:37Z","2019-09-03T05:08:27Z"
"","469","HADOOP-15281: Add -direct DistCp option","Adds a `-direct` option to DistCp as suggested in [HADOOP-15281](https://issues.apache.org/jira/browse/HADOOP-15281).  Tagging @steveloughran for review","closed","","noslowerdna","2019-01-22T22:51:34Z","2019-02-13T17:11:26Z"
"","125","HADOOP-13560 S3A to support huge file writes and operations -with tests","Adds  ## Scale tests for S3A huge file support; - always running at the MB size (maybe best to make optional)   -configurable to bigger sizes in the auth-keys XML or in the build `-Dfs.s3a.scale.test.huge.filesize=1000` - limited to upload, seek, read, rename, delete. The JUnit test cases are explicltly set up to run in order here. ## New scalable output stream for writing, `S3ABlockOutputStream`  -always saves in incremental blocks as writes proceed, block size == partition size. -supports Fast output stream memory buffer code (for regression testing) -supports a back end which buffers blocks in files, using RR disk allocation. As such, write/read bandwidth is limited to aggregate HDD bandwidth. -adding extra failure resilience as testing throws up failure conditions (network timeouts, no-response from server on multipart commit, etc). -adding instrumentation, including using callbacks from AWS SDK to update gauges and counters (in progress)  What we have here is essentially something that can replace the classic ""save to file, upload at the end"" stream and the fast ""store it all in RAM and hope there's space"" stream. It should offer incremental upload for faster output of larger files compared the classic file stream, with the scaleability the fast one lacks. And the instrumentation to show what's happening.","closed","","steveloughran","2016-09-08T10:32:38Z","2016-09-23T19:00:11Z"
"","254","Liumihust patch add null pointer exception in FsDatasetImpl.checkBlock()","Additional PR for the same bug","open","","liumihust","2017-07-25T09:08:14Z","2019-04-08T23:57:45Z"
"","536","HDDS-1136 : Add metric counters to capture the RocksDB checkpointing statistics.","Added metric gauges for tracking DB checkpointing statistics. The OMMetrics class will hold these guages at any instant. These can be pulled from OM by Recon.   **Testing done** Integration test for Servlet method that gets the OM DB checkpoint added. Manually verified the patch on single node Ozone cluster.","closed","ozone,","avijayanhwx","2019-02-28T23:38:27Z","2019-03-01T08:54:18Z"
"","537","HDDS-1136 : Add metric counters to capture the RocksDB checkpointing statistics.","Added metric gauges for tracking DB checkpointing statistics. The OMMetrics class will hold these guages at any instant. These can be pulled from OM by Recon.  **Testing done** Integration test for Servlet method that gets the OM DB checkpoint added. Manually verified the patch on single node Ozone cluster.","closed","ozone,","avijayanhwx","2019-03-01T03:34:07Z","2019-03-04T23:16:38Z"
"","401","YARN-3929. Add uncleaning option for local app log file","Add uncleaning option for local app log file with log-aggregation enabled  Support new config to enable/disable local app log file cleanup when log-aggregation is enabled","open","","bschell","2018-07-06T00:09:37Z","2019-08-02T05:07:27Z"
"","325","HADOOP-15166: simplify minicluster start","add minicluster subcommand to simplify its usage","closed","","gerashegalov","2018-01-11T08:48:15Z","2018-02-02T00:00:34Z"
"","88","HADOOP-12984","Add GenericTestUtils.getTestDir method and use it for temporary directory in tests","closed","","vinayakumarb","2016-04-01T05:26:59Z","2016-04-03T09:24:10Z"
"","391","Add function hdfsGetRemaining to libhdfs","Add function hdfsGetRemaining(hdfsFS fs) to libhdfs","open","","FlyGently","2018-06-06T09:48:01Z","2020-07-30T03:11:26Z"
"","158","HADOOP-13810","Add a test to verify that Configuration handles &-encoded characters","closed","","steveloughran","2016-11-11T17:33:24Z","2016-11-15T03:41:36Z"
"","566","HADOOP-15691 Add PathCapabilities to FS and FC to complement StreamCapabilities","Add a PathCapabilities interface to both FileSystem and FileContext to declare the capabilities under the path of a filesystem through both the FileSystem and FileContext APIs","closed","","steveloughran","2019-03-06T21:33:14Z","2019-03-07T10:49:35Z"
"","531","HADOOP-15961 Add PathCapabilities to FS and FC to complement StreamCapabilities","Add a PathCapabilities interface to both FileSystem and FileContext to declare the capabilities under the path of a filesystem through both the FileSystem and FileContext APIs","closed","","steveloughran","2019-02-28T14:39:31Z","2019-03-07T10:49:35Z"
"","175","HDFS-11278 Add missing @Test annotation for TestSafeMode.testSafeModeUtils","Add a missing @Test annotation for TestSafeMode.testSafeModeUtils()","closed","","lukmajercak","2016-12-28T18:32:20Z","2017-03-05T20:08:03Z"
"","669","HDDS-1330 : Add a docker compose for Ozone deployment with Recon.","Add a docker compose for Ozone deployment with Recon. Test out Recon container key service.","closed","","avijayanhwx","2019-03-29T19:28:02Z","2019-04-03T20:20:52Z"
"","100","HADOOP-13278. S3AFileSystem mkdirs does not need to validate parent path components","According to S3 semantics, there is no conflict if a bucket contains a key named `a/b` and also a directory named `a/b/c`. ""Directories"" in S3 are, after all, nothing but prefixes.  However, the `mkdirs` call in `S3AFileSystem` does go out of its way to traverse every parent path component for the directory it's trying to create, making sure there's no file with that name. This is suboptimal for three main reasons: - Wasted API calls, since the client is getting metadata for each path component  - This can cause _major_ problems with buckets whose permissions are being managed by IAM, where access may not be granted to the root bucket, but only to some prefix. When you call `mkdirs`, even on a prefix that you have access to, the traversal up the path will cause you to eventually hit the root bucket, which will fail with a 403 - even though the directory creation call would have succeeded. - Some people might actually have a file that matches some other file's prefix... I can't see why they would want to do that, but it's not against S3's rules.  [I've opened a ticket](https://issues.apache.org/jira/browse/HADOOP-13278) on the Hadoop JIRA. This  pull request is a simple patch that just removes this portion of the check. I have tested it with my team's instance of Spark + Luigi, and can confirm it works, and resolves the aforementioned permissions issue for a bucket on which we only had prefix access.  This is my first ticket/pull request against Hadoop, so let me know if I'm not following some convention properly :)","closed","","apetresc","2016-06-15T21:33:20Z","2019-10-10T18:14:13Z"
"","290","HADOOP-14128. fix renameInternal in ChecksumFs","AbstractFs.rename(source, destination, options) calls renameInternal(source, destination, overwrite)  This patch adds this method to ChecksumFs to rename the crc file in addition to the file itself to avoid crc missmatch when use for example in LocalFs.","closed","","mchataigner","2017-11-09T18:27:06Z","2019-08-09T04:01:25Z"
"","42","AltFileInputStream.java replace FileInputStream.java in apache/hadoop/HDFS","A brief description Long Stop-The-World GC pauses due to Final Reference processing are observed. So, Where are those Final Reference come from ?  1 : `Finalizer` 2 : `FileInputStream`  How to solve this problem ?  Here is the detailed description,and I give a solution on this. https://issues.apache.org/jira/browse/HDFS-8562  FileInputStream have a method of finalize , and it can cause GC pause for a long time.In our test,G1 as our GC. So,in AltFileInputStream , no finalize. A new design for a inputstream use in windows and non-windows.","open","","hash-X","2015-11-05T03:26:03Z","2019-04-08T23:57:28Z"
"","129","Update ReadMe: Intro and version added","A basic introduction and current version details added.","closed","","animenon","2016-09-23T15:53:14Z","2017-12-07T18:35:39Z"
"","390","Branch 2.7.7","a","closed","","lierdan","2018-06-04T07:43:20Z","2019-01-18T01:09:45Z"
"","29","HadoopArchives.java polishing","`HadoopArchives.java` was edited.","open","","alexVengrovsk","2015-08-16T20:19:23Z","2019-02-06T11:18:51Z"
"","436","Use print() function in both Python 2 and Python 3","__print()__ is a function in Python 3.  Also, the builtin file() was removed in Python 3 * This PR recommends replacing it with __open()__ which works in both Python 2 and Python 3.  And,  Declare __global error_count__ * The global __error_count__ can not be incremented on line 126 unless it is declared as global beforehand.  Without this change, it is an _undefined name_ that has the potential to raise __NameError__ at runtime.  [flake8](http://flake8.pycqa.org) testing of https://github.com/apache/hadoop on Python 3.7.1  $ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__ ``` ./hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/terasort/job_history_summary.py:73:61: E999 SyntaxError: invalid syntax print ""Name reduce-output-bytes shuffle-finish reduce-finish""                                                             ^ ./dev-support/determine-flaky-tests-hadoop.py:125:5: F823 local variable 'error_count' (defined in enclosing scope on line 76) referenced before assignment     error_count += 1     ^ ./dev-support/bin/checkcompatibility.py:195:10: F821 undefined name 'file'     with file(annotations_path, ""w"") as f:          ^ 1     E999 SyntaxError: invalid syntax 1     F821 undefined name 'file' 1     F823 local variable 'error_count' (defined in enclosing scope on line 76) referenced before assignment 3 ```","closed","","cclauss","2018-10-31T17:57:17Z","2019-08-27T17:17:22Z"
"","384","YARN-8256. Support pluggable node membership manager in YARN.","[YARN-8256](https://issues.apache.org/jira/browse/YARN-8256): Support pluggable node membership manager in YARN.","open","","functicons","2018-05-08T20:25:02Z","2019-04-08T23:58:08Z"
"","328","YARN-7742 - Remove duplicate entries","[YARN-7742]","open","","skmvasu","2018-01-16T11:08:42Z","2019-04-08T23:57:57Z"
"","215","MAPREDUCE-6879. TestDFSIO#sequentialTest throws java.lang.NullPointerException due to uninitialized IOStream","[https://issues.apache.org/jira/browse/MAPREDUCE-6879](https://issues.apache.org/jira/browse/MAPREDUCE-6879)  When I use -seq arg to write files, TestDFSIO#sequentialTest throws java.lang.NullPointerException due to uninitialized stream inherited from IOMapperBase:  [root@7b65290f2609 hadoop-3.0.0-alpha3-SNAPSHOT]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha3-SNAPSHOT-tests.jar TestDFSIO -seq -write 2017-04-20 15:50:43,505 INFO fs.TestDFSIO: TestDFSIO.1.8 2017-04-20 15:50:43,510 INFO fs.TestDFSIO: nrFiles = 1 2017-04-20 15:50:43,510 INFO fs.TestDFSIO: nrBytes (MB) = 1.0 2017-04-20 15:50:43,510 INFO fs.TestDFSIO: bufferSize = 1000000 2017-04-20 15:50:43,510 INFO fs.TestDFSIO: baseDir = /benchmarks/TestDFSIO 2017-04-20 15:50:43,655 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable java.lang.NullPointerException 	at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:427) 	at org.apache.hadoop.fs.TestDFSIO$WriteMapper.doIO(TestDFSIO.java:395) 	at org.apache.hadoop.fs.TestDFSIO.sequentialTest(TestDFSIO.java:722) 	at org.apache.hadoop.fs.TestDFSIO.run(TestDFSIO.java:846) 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76) 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90) 	at org.apache.hadoop.fs.TestDFSIO.main(TestDFSIO.java:731) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71) 	at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144) 	at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:136) 	at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:144) 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:498) 	at org.apache.hadoop.util.RunJar.run(RunJar.java:239) 	at org.apache.hadoop.util.RunJar.main(RunJar.java:153)","closed","","wenxinhe","2017-04-20T08:44:23Z","2017-06-07T06:24:00Z"
"","597","HDFS-3246: pRead equivalent for direct read path","[HDFS-3246](https://issues.apache.org/jira/browse/HDFS-3246): First several iterations of this patch are posted on the JIRA. This PR is a continuation of this work, it was created to make the code more reviewable. The version of the patch posted here fixes a few minor issues reported by Yetus, and added some more Javadocs to the changes in `CryptoInputStream` to make the code easier to understand.","closed","","sahilTakiar","2019-03-12T17:25:06Z","2019-04-30T21:52:34Z"
"","600","HDFS-14348: Fix JNI exception handling issues in libhdfs","[HDFS-14348](https://issues.apache.org/jira/browse/HDFS-14348) contains a list of all the JNI issues this patch fixes. All of the changes are related to proper exception handling when using the JNI library.  Some highlights:  * `hadoopRzOptionsSetByteBufferPool` was changed to add proper exception handling for `NewGlobalRef` and to cleanup some of the logic that was missed in HDFS-14321 * `get_current_thread_id` was re-written to use methods from `jni_helper.h` rather than using the JNI library directly; this fixes several issues with exception handling that were present in the code","closed","","sahilTakiar","2019-03-13T14:32:22Z","2019-03-26T23:35:24Z"
"","595","HDFS-14304: High lock contention on hdfsHashMutex in libhdfs","[HDFS-14304](https://issues.apache.org/jira/browse/HDFS-14304) describes the issue. The TL;DR is that libhdfs caches JNI `jclass` objects (which is a recommended practice, see https://www.ibm.com/developerworks/library/j-jni/index.html#notc). However, the issue is that the current implementation caches all the `jclass` objects in a global hash table. Since libhdfs is designed to be used by multiple threads concurrently, the hash table is protected by a single mutex to avoid concurrency issues. The issue is that for short lived Java method, acquiring the lock contributes to significant performance overhead.  I first considered using a RW Lock (`pthread_rwlock_t`) to fix this issue, but after some prototyping I found that it didn't fix the actual issue. Acquiring a read lock still requires going through lock acquisition, so the performance issue is still present.  Instead, after some digging, I landed on an approach similar to https://stackoverflow.com/questions/10617735/in-jni-how-do-i-cache-the-class-methodid-and-fieldids-per-ibms-performance-r  Instead of using a hash table to lazily-load and store all the `jclass` objects. This patch allocates all necessary `jclass` objects upon JVM load (the classes are loaded in `getGlobalJNIEnv`). This removes the need for the hash table and the associated locking completely.  One could argue that lazily-loading the `jclass`s (which is what the current code does) is preferable to eagerly-loading the `jclass`s (which is what this patch does). However, after doing some analysis I don't think that holds. Consider the following: * There are only 19 `jclass`s that need to be used throughout libhdfs * This is done only once per process, and is done during JVM load time (loading 19 `jclass`s should not significantly contribute to startup costs)  Another concern is the space required to store all the `jclass` objects compared to storing them in the `htable`.  Consider that the current `htable` used by `jni_helper` is initially allocated with 4096 `htable_pair`s where each `htable_pair` consists of two pointers. So the total size of the `htable` should be about 64 kilobytes.  On the other hand, a single `jclass` object should conservatively take up ~248 bytes: * [JOL](https://openjdk.java.net/projects/code-tools/jol/) shows that a single `Class` object takes up to 104 bytes (it depends on various system settings / JVM settings) * Some runtime analysis shows that a `Class` object has to store the name of the `Class` itself, which is of course variable, but is conservatively estimated to be 100 bytes     * The longest class name we store is 50 characters, which should equate to about 100 bytes in Java * Add 48 bytes for any additional overhead required for a `jclass` object to reference the Java `Class` object (I'm assuming there are some additional pointers somewhere, but I don't know enough about the JNI implementation to know what they are).  With 19 `jclass` objects that is about 4.5 kilobytes; significantly less that the size of an empty `htable`. So overall, this patch should actually decrease the memory requirement for libhdfs.  Some more details about the actual implementation: * `jclasses.h` contains all the `jclass` objects that are used throughout libhdfs * I changed `invokeMethod` in `jni_helper` to take in an existing `jclass` and deleted all the hash table lookup code * `findClassAndInvokeMethod` is equivalent to the old implementation of `invokeMethod` - it is currently just used in test code * Deleted `htable` and all associated code: test classes, mutexes, etc. * Fixed a few other checkstyle issues  While this patch looks long, most of the changes are refactoring, since every call to `invokeMethod` had to be changed. The main files to look at are: `jclasses.h`, `jclasses.c`, `jni_helper.h`, and `jni_helper.c`.","closed","","sahilTakiar","2019-03-12T15:32:25Z","2019-03-27T03:59:19Z"
"","690","HADOOP-14747: S3AInputStream to implement CanUnbuffer","[HADOOP-14747](https://issues.apache.org/jira/browse/HADOOP-14747): S3AInputStream to implement CanUnbuffer  Change Summary: * Added a contract specification for `CanUnbuffer` to `fsdatainputstream.md`     * Unlike most other interfaces the logic for `FSDataInputStream#unbuffer` is a bit different since it delegates to `StreamCapabilitiesPolicy#unbuffer`     * The one odd thing I noticed while writing up the contract specification is that the current implementation of `unbuffer` allows callers to invoke `unbuffer` on a closed file without issue; I'm not sure if that was by design or not so for now the specification allows for it and dictates that calling `unbuffer` after `close` is a no-op, but let me know if we think the behavior should be changed * `AbstractContractUnbufferTest` contains the contract tests and I added implementations for HDFS and S3; I don't see contract tests for `CryptoInputStream` so I left it alone * I added S3 specific tests: both unit tests (using mocks) and itests * The actual implementation of `unbuffer` in `S3AInputStream` just calls `closeStream`  Testing: * Ran all unit tests and S3 itests against US East (N. Virginia) using `mvn verify`     * All tests passed, `ITestS3AContractDistCp` failed once but worked when I retried it * I didn't run the scale tests, or several of the other tests that require additional configuration; let me know if I should run them","closed","fs/s3,","sahilTakiar","2019-04-04T05:03:02Z","2019-04-13T01:21:44Z"
"","9","Merge pull request #2 from apache/trunk","[apache merge] Merge from apache/hadoop, 20141108.","closed","","CaiYicong","2014-11-08T02:16:53Z","2014-11-08T02:18:26Z"
"","10","Merge pull request #2 from apache/trunk","[apache merge] Merge from apache/hadoop, 20141102.","closed","","CaiYicong","2014-11-08T02:22:21Z","2014-11-08T02:24:06Z"
"","280","MAPREDUCE-6973. Appropriate comments on creating _SUCCESS file.","@jlowe","closed","","mgarnara","2017-10-02T18:20:18Z","2019-07-26T18:30:27Z"
"","475","HDDS-956: MultipartUpload: List Parts for a Multipart upload key.","@elek I have opened a PR for HDDS-956.","closed","","bharatviswa504","2019-01-31T07:01:13Z","2019-01-31T16:17:02Z"
"","377","Fix wrong description for job4 and format JobStatus.toString()","1、Fix wrong description for job4 2、format JobStatus.toString()","closed","","maxiaoguang64","2018-05-03T07:21:07Z","2018-07-07T08:13:58Z"
"","680","HADOOP-15014: KMS should capture the originating IP address",". and log it in case of error, with a test","closed","","gzsombor","2019-04-02T15:15:46Z","2019-08-08T03:58:37Z"
"","35","Hadoop 11890",".","closed","","NikBisht","2015-10-05T05:33:57Z","2015-10-05T05:43:09Z"
"","132","HADOOP-13614 superfluous tests","- where possible give all the s3a tests the superclass of {{AbstractS3ATestBase}}, clean up children. - ITestS3AFileSystemContract has path logic reworked _so test can run in parallel_ - fixed up yarn test to work in parallel too, along with BlockingThreadPool and FastOutputStream - on the files changed, moved the copyright comment to being a simple comment rather than a javadoc one.","closed","","steveloughran","2016-09-30T17:31:33Z","2016-11-01T12:10:05Z"
"","169","typo and duplicate in `ClusterSetup.apt.vm`","- typo `varibale` -> `variable` - remove duplicate section `Operating the Hadoop Cluster`","closed","","afewnotes","2016-12-01T02:00:13Z","2017-03-07T04:03:17Z"
"","161","HADOOP-13817. Add a finite shell command timeout to ShellBasedUnixGroupsMapping.","- Tests required log capture so the LOG variable was elevated in visibility, which required changes in a few test methods unrelated to just this change. - Timeout is applied to both, the regular groups and the partial group listing check commands. - Default is left to 0 to not break current behaviour (i.e. to wait forever until groups are resolvable).","closed","","QwertyManiac","2016-11-14T12:00:47Z","2017-02-24T16:15:38Z"
"","124","YARN-5605. Preempt containers (all on one node) to meet the requirement of starved applications.","- Remove existing preemption code. - Deleted preemption tests, to be reinstated later. - Identify starved applications in the update thread when preemption is enabled. - For each starved application, identify containers to be preempted on a single node to make room for this request. - Preempt these containers after necessary warning.","closed","","kambatla","2016-08-31T21:02:20Z","2016-11-03T04:00:36Z"
"","195","HDFS-11421. Make WebHDFS' ACLs RegEx configurable. (harsh)","- Introduced a new config key `dfs.webhdfs.acl.provider.permission.pattern` with value set to existing default ACL regex - Added pattern-from-config setting points for `AclPermissionParam` to `NameNodeHttpServer`, `WebHdfsFileSystem` and `WebHdfsHandler`, akin to how `UserParam` is set today at these three places - Added setters/getters for custom pattern to `AclPermissionParam` - Added a new test for `AclPermissionParam` custom patterns that exercises numeric usernames and group names with `@` characters - Extended existing `UserParam` configurability tests to cover ACL modification with numeric usernames and group names with `@` characters over a custom pattern - Ran existing relevant tests, which continue to pass","closed","","QwertyManiac","2017-02-16T12:58:22Z","2017-02-25T02:45:01Z"
"","668","HDDS-1358 : Recon Server REST API not working as expected.","- Fixed the Guice Jersey-hk2 integration. - Added blocks to KeyMetadata - Minor fixes/improvements.   Manually tested on single node cluster.","closed","","avijayanhwx","2019-03-29T19:25:28Z","2019-04-03T21:52:06Z"
"","643","HDDS-1260. Create Recon Server lifecycle integration with Ozone.","- Create the lifecycle scripts (start/stop) for Recon Server along with Shell interface like the other components. - Verify configurations are being picked up by Recon Server on startup.","closed","","vivekratnavel","2019-03-26T03:49:39Z","2019-04-01T16:59:33Z"
"","192","YARN-6163. Preempt for multiple RRs.","- Consider multiple RRs every time an app is marked starved - Don't consider the app starved until after a configurable delay - Preemption tests use ControlledClock","closed","","kambatla","2017-02-10T05:46:11Z","2017-02-16T07:26:45Z"
"","147","HADOOP-1381. The distance between sync blocks in SequenceFiles should be configurable","- Allowed the sync interval size to be configurable via a new parameter in the Writer argument API - New default sync interval size is 200 KiB - Added tests with varied sync interval sizes, and kept a test for the older default as-is for compatibility checks - Moved some older Reader API usage in modified tests to support newer constructions","closed","","QwertyManiac","2016-10-26T14:35:13Z","2016-11-25T16:57:24Z"
"","135","HADOOP-13694. Add support for AES-192 in OpensslCipher.","- Adds equivalent support for 192-bit AES/CTR/NoPadding codec in the OpensslCipher - Adds test-cases to cover 192-bit (24-bytes) and 256-bit (32-bytes) keys to both JCE and OpenSSL crypto tests - Enhances the error message when an invalid Key or IV size is passed into OpensslCipher","open","","QwertyManiac","2016-10-06T11:19:48Z","2019-04-08T23:57:33Z"
"","515","HADOOP-16134 001- initial design of a WriteOperationsContext","*Does not compile*  This adds * a context which is passed round with writes * a parent delete policy as part of this (unused)  This PoC shows that adding a new context everywhere is overcomplex as you now need to retrofit it through the stack, even though a  (single, shared) WriteOperationsHelper is already passed in  This doesn't compile: I put it together while half-listening to an online talk, and now I've done I've learned enough to say ""not the right approach""  Better strategy: * include the WriteOperationsContext in the WriteOperationsHelper;  instantiating a new one each time. This will automatically add it to all bits of the FS code which write data * add a default/configurable delete policy to the FS, *but allow operations to explicitly overwrite this*. Example: completing all the committed work in a job commit, because we can rely on the write of the _SUCCESS file to do the work (so only do it for one file, not every file created)  We're also a bit constrained by how the MPU API of HADOOP-13186 tries to be independent of the FS instance -this is one of those cases where it complicates life even more. The FS/FC MUST be the factory for MPU instances.  Change-Id: I0de1d4b97fdf4c4f0ece1a27245ba9bb38a29559","closed","","steveloughran","2019-02-22T10:59:22Z","2019-03-28T23:38:28Z"
"","157","HADOOP-13600. S3a rename() to copy files in a directory in parallel","* Creates two separate `TransferManager`s - one for file uploads and one for file copies; this way they can be both configured separately; users may want to set the # of parallel uploads to a lower value than the # of parallel copies because uploads require actual I/O while copies do not * Main modifications are to the `innerRename` method * Instead of renaming a directory file by file, use the copy `TranferManager` to upload them in parallel * Copies are submitted to the `TransferManager` and then tracked using the returned `Copy` object * Deletes are handled via a `BlockingQueue` - once each copy is complete it adds a key to the queue, once `MAX_ENTRIES_TO_DELETE` keys need to be deleted, then the `removeKeys` method is invoked * A separate thread is spawned to read from the delete queue and issue the delete requests * A `ProgressListener` is used to track when a copy has been completed, once a copy finishes, the key to delete is added to the delete queue * Some other re-factoring to make the above possible","closed","","sahilTakiar","2016-11-10T19:54:05Z","2019-07-26T11:33:41Z"
"","223","HADOOP-14436 Minor error about a redundant colon","(https://issues.apache.org/jira/browse/HADOOP-14436) Minor mistake can led the beginner to the wrong way and getting far away from us.","closed","","maobaolong","2017-05-18T03:45:07Z","2017-07-11T08:59:13Z"
"","672","[HDDS-1360] Invalid metric type due to fully qualified class name","## What changes were proposed in this pull request?  Use simple class name for metrics name to fix `invalid metric type` error in Prometheus.  https://issues.apache.org/jira/browse/HDDS-1360  ## How was this patch tested?  Ran docker-compose in `ozoneperf` direcory (without `freon`).","closed","","adoroszlai","2019-04-01T09:44:33Z","2019-04-01T11:37:47Z"
"","671","HDDS-1322. Hugo errors when building Ozone","## What changes were proposed in this pull request?  Maven build fails to create HDDS docs due to the following hugo error in `hadoop-hdds-docs`:  ``` $ mvn -Phdds -pl :hadoop-hdds-docs clean compile ... [INFO] --- exec-maven-plugin:1.6.0:exec (default) @ hadoop-hdds-docs --- Error: unknown command ""0.5.0-SNAPSHOT"" for ""hugo"" Run 'hugo --help' for usage. ... [INFO] BUILD SUCCESS ```  `generate-site.sh` passes all arguments to `hugo`, which expects commands, not HDDS version number and target directory.  I think these two parameters are unnecessary, hence the patch removes them.  https://issues.apache.org/jira/browse/HDDS-1322  ## How was this patch tested?  With the fix docs can be generated using Maven:  ``` $ mvn -Phdds -pl :hadoop-hdds-docs clean compile ... [INFO] --- exec-maven-plugin:1.6.0:exec (default) @ hadoop-hdds-docs --- Building sites … ...                    | EN +------------------+----+   Pages            | 27   Paginator pages  |  0   Non-page files   |  0   Static files     | 18   Processed images |  0   Aliases          |  0   Sitemaps         |  1   Cleaned          |  0  Total in 49 ms ```","closed","","adoroszlai","2019-04-01T03:07:35Z","2019-04-01T16:53:32Z"
"","659","[HDDS-1351] NoClassDefFoundError when running ozone genconf","## What changes were proposed in this pull request?  Add `jaxb-core` to `hadoop-ozone-tools` dependencies to make `ozone genconf` work again.  https://issues.apache.org/jira/browse/HDDS-1351  ## How was this patch tested?  ``` $ mvn -Phdds -DskipTests -Dmaven.javadoc.skip=true -Pdist -Dtar -DskipShade -am -pl :hadoop-ozone-dist clean package $ cd $(git rev-parse --show-toplevel)/hadoop-ozone/dist/target/ozone-*-SNAPSHOT/compose/ozones3 $ docker-compose run datanode ozone genconf /tmp ozone-site.xml has been generated at /tmp ```","closed","ozone,","adoroszlai","2019-03-28T20:12:24Z","2019-04-01T02:43:53Z"
"","660","[HDDS-1351] NoClassDefFoundError when running ozone genconf","## What changes were proposed in this pull request?  Add `jaxb-core` and some `javax` artifacts to `hadoop-ozone-tools` dependencies to make `ozone genconf` work with JDK11, too.  https://issues.apache.org/jira/browse/HDDS-1351  ## How was this patch tested?  ``` $ mvn -Phdds -DskipTests -Dmaven.javadoc.skip=true -Pdist -Dtar -DskipShade -am -pl :hadoop-ozone-dist clean package $ cd $(git rev-parse --show-toplevel)/hadoop-ozone/dist/target/ozone-*-SNAPSHOT/compose/ozone $ docker-compose run datanode ozone genconf /tmp ozone-site.xml has been generated at /tmp ```","closed","","adoroszlai","2019-03-28T20:28:32Z","2019-03-29T18:24:32Z"
"","691","[HDDS-1363] ozone.metadata.dirs doesn't pick multiple dirs","## What changes were proposed in this pull request?  `ozone.metadata.dirs` is a global fallback for per-component directory configurations, eg.`ozone.om.db.dirs`, `ozone.scm.db.dirs`, etc.  All of these handle only a single location, but values with multiple comma-separated paths (eg. `/data/dir1,/data/dir2`) are treated in different ways:  * rejected for the specific configs  * used as a single value for the fallback config  The goal of this change is to reject comma-separated paths for `ozone.metadata.dirs`, too, by applying the same logic that is already used for the per-component configs.  In addition, the following minor fixes are included:  1. Fix error message in `ServerUtils#getDirectoryFromConfig`, which referenced the component name (eg. `SCM`) instead of the config item name (eg. `ozone.om.db.dirs`) as ""configuration setting"". 2. Move existing test cases for `ServerUtils` to the new `ServerUtilsTest` class from `TestHddsServerUtils` 3. Eliminate duplicated logic in `ScmUtils.getDBPath`, reuse `ServerUtils#getDirectoryFromConfig`  https://issues.apache.org/jira/browse/HDDS-1363  ## How was this patch tested?  Unit tests (new and existing).  Tested manually using `ozone` docker-compose setup.","closed","","adoroszlai","2019-04-04T08:41:26Z","2019-04-12T11:01:05Z"
"","205","Fix JobConf.getNumMapTasks() Java doc","# Problem  The original description of the `getNumMapTasks()` method in JobConf was invalid, because it referenced to the number of reducer tasks instead of the map tasks.  from: Get configured the number of reduce tasks for this job. to:     Get the configured number of map tasks for this job.  It was maybe the result of a tricky copy-paste  :-)  # Solution  Change the doc to refer to the number of map tasks.  The PR resolves the following [JIRA issue](https://issues.apache.org/jira/browse/HADOOP-14228)","closed","","joemeszaros","2017-03-24T09:50:08Z","2017-03-27T04:52:30Z"
"","611","HDDS-1265. ""ozone sh s3 getsecret"" throws Null Pointer Exception for unsecured clusters","""ozone sh s3 getsecret"" command throws a Null Pointer Exception.  This patch fixes it by showing the following message:  ``` hadoop@fa14f2633ba4:~$ ozone sh s3 getsecret This command is not supported in unsecure clusters. ```","closed","","vivekratnavel","2019-03-14T21:33:15Z","2019-03-15T00:21:07Z"
"","613","HDDS-1263. SCM CLI does not list container with id 1","""ozone scmcli list --start=1"" lists containers starting from container ID 2. There is no way to list the container with containerID 1.  This PR fixes this behavior.","closed","ozone,","vivekratnavel","2019-03-15T19:49:10Z","2019-03-15T22:39:31Z"
"","305","YARN-7546. Queue page reskin","","closed","","skmvasu","2017-11-29T06:51:18Z","2017-12-04T05:39:14Z"
"","692","HADOOP-16222. Fix new deprecations after guava 27.0 update in trunk","","closed","","bgaborg","2019-04-04T09:49:48Z","2019-07-29T13:25:17Z"
"","689","HDDS-1379. Convert all OM Volume related operations to HA model.","","closed","ozone,","bharatviswa504","2019-04-03T19:16:29Z","2019-04-05T05:29:53Z"
"","688","HDDS-1379. Convert all OM Volume related operations to HA model","","closed","ozone,","bharatviswa504","2019-04-03T18:27:14Z","2019-04-03T19:43:02Z"
"","687","HDDS-1329. Update documentation for Ozone-0.4.0 release. Contributed By Ajay Kumar.","","closed","ozone,","ajayydv","2019-04-03T18:15:54Z","2019-08-21T07:49:43Z"
"","686","HDDS-1329. Update documentation for Ozone-0.4.0 release. Contributed By Ajay Kumar.","","closed","","ajayydv","2019-04-03T18:14:21Z","2019-04-03T18:15:06Z"
"","684","HDDS-1329. Update documentation for Ozone-0.4.0 release. Contributed By Ajay Kumar.","","closed","","ajayydv","2019-04-03T18:07:24Z","2019-04-03T18:13:53Z"
"","682","HDDS-1372. getContainerWithPipeline for a standalone pipeline fails with ConcurrentModificationException.","","closed","ozone,","nandakumar131","2019-04-03T12:06:52Z","2019-04-09T09:46:07Z"
"","677","HDDS-1355. Only FQDN is accepted for OM rpc address in secure environment. Contributed by Ajay Kumar.","","closed","ozone,","ajayydv","2019-04-02T00:20:16Z","2019-05-20T16:44:12Z"
"","676","HDDS-1324. TestOzoneManagerHA tests are flaky","","closed","ozone,","hanishakoneru","2019-04-01T19:55:58Z","2019-04-03T23:02:20Z"
"","673","HDDS-1153. Make tracing instrumentation configurable","","closed","","kittinanasi","2019-04-01T12:13:34Z","2019-04-02T12:33:51Z"
"","670","HDDS-1347. In OM HA getS3Secret call Should happen only leader OM.","","closed","ozone,","bharatviswa504","2019-03-30T17:25:41Z","2019-08-22T22:00:33Z"
"","667","HADOOP-16210. guava27 trunk yetus hadoop tools trigger","","closed","","bgaborg","2019-03-29T16:57:16Z","2019-04-01T14:06:13Z"
"","651","HDDS-1339. Implement ratis snapshots on OM","","closed","ozone,","hanishakoneru","2019-03-27T23:23:33Z","2019-04-04T05:50:29Z"
"","650","HADOOP-16209. Create simple docker based pseudo-cluster for hdfs","","open","","bgaborg","2019-03-27T14:06:06Z","2020-07-31T21:01:28Z"
"","645","HADOOP-16132 Support multipart download in S3AFileSystem","","closed","fs/s3,","justinuang","2019-03-26T18:53:20Z","2019-07-10T21:44:01Z"
"","644","Update HAState.java","","closed","","20100507","2019-03-26T05:28:35Z","2019-04-03T01:19:25Z"
"","642","HADOOP-16199. KMSLoadBlanceClientProvider does not select token correctly. Contributed by Xiaoyu Yao.","","closed","","xiaoyuyao","2019-03-26T01:11:05Z","2019-03-29T04:55:32Z"
"","641","HDDS-1318. Fix MalformedTracerStateStringException on DN logs. Contributed by Xiaoyu Yao.","","closed","ozone,","xiaoyuyao","2019-03-26T00:27:50Z","2019-03-28T19:00:59Z"
"","640","HDDS-1331. In DatanodeStateMachine join check for not null.","","closed","ozone,","bharatviswa504","2019-03-22T23:46:27Z","2019-03-25T16:13:03Z"
"","637","HADOOP-15960. Update guava to 27.0-jre in hadoop-common","","closed","","bgaborg","2019-03-22T10:24:06Z","2019-04-01T12:34:00Z"
"","635","HDDS-1317. KeyOutputStream#write throws ArrayIndexOutOfBoundsExceptio…","","closed","","bshashikant","2019-03-22T09:55:37Z","2019-03-22T17:35:18Z"
"","634","HDDS-939. Add S3 access check to Ozone manager. Contributed by Ajay Kumar.","","closed","ozone,","ajayydv","2019-03-22T00:48:32Z","2019-03-26T15:59:59Z"
"","632","HDDS-1255. Refactor ozone acceptance test to allow run in secure mode. Contributed by Ajay Kumar.","","closed","ozone,","ajayydv","2019-03-21T18:29:59Z","2019-04-11T06:49:04Z"
"","628","HADOOP-16186. NPE in ITestS3AFileSystemContract teardown in DynamoDBMetadataStore.lambda$listChildren","","closed","fs/s3,","bgaborg","2019-03-20T11:04:43Z","2019-03-28T15:53:31Z"
"","627","HDDS-1299. Support TokenIssuer interface for running jobs with OzoneFileSystem.","","closed","ozone,","xiaoyuyao","2019-03-19T23:47:20Z","2019-03-25T16:17:05Z"
"","626","HDDS-1262. In OM HA OpenKey and initiateMultipartUpload call Should happen only leader OM.","","closed","ozone,","bharatviswa504","2019-03-19T22:57:56Z","2019-03-27T04:48:04Z"
"","625","HDDS-1262. In OM HA OpenKey call Should happen only leader OM.","","closed","","bharatviswa504","2019-03-19T22:08:35Z","2019-03-19T22:11:18Z"
"","624","HADOOP-15999. S3Guard: Better support for out-of-band operations","","closed","fs/s3,","bgaborg","2019-03-19T14:33:36Z","2019-04-01T12:33:01Z"
"","623","HDDS-1308. Fix asf license errors.","","closed","ozone,","bharatviswa504","2019-03-19T05:12:02Z","2019-03-19T21:10:07Z"
"","622","HDDS-1307. Test ScmChillMode testChillModeOperations failed.","","closed","ozone,","bharatviswa504","2019-03-19T05:00:00Z","2019-03-19T20:57:02Z"
"","618","MAPREDUCE-7194","","open","","belugabehr","2019-03-18T17:19:20Z","2019-08-08T07:16:48Z"
"","614","HDDS-1264. Remove Parametrized in TestOzoneShell","","closed","ozone,","vivekratnavel","2019-03-15T20:42:35Z","2019-03-27T05:07:23Z"
"","610","[MAPREDUCE-7193] Review of CombineFile Code","","open","","belugabehr","2019-03-14T20:26:43Z","2020-09-09T21:44:32Z"
"","607","HADOOP-15999. S3Guard: Better support for out-of-band operations","","closed","","bgaborg","2019-03-14T15:42:02Z","2019-03-19T14:31:49Z"
"","603","HADOOP-16026:Replace incorrect use of system property user.name","","closed","","dineshchitlangia","2019-03-14T04:36:28Z","2019-04-26T20:25:51Z"
"","594","HDDS-1246. Add ozone delegation token utility subcmd for Ozone CLI. Contributed by Xiaoyu Yao.","","closed","ozone,","xiaoyuyao","2019-03-12T14:00:51Z","2019-03-15T23:08:31Z"
"","593","HDDS-1253","","closed","ozone,","xiaoyuyao","2019-03-12T12:43:59Z","2019-03-12T16:34:19Z"
"","586","HDDS-1242. In s3 when bucket already exists, it should just return location.","","closed","ozone,","bharatviswa504","2019-03-10T04:41:33Z","2019-03-10T05:42:22Z"
"","581","HDDS-596. Add robot test for OM Block Token.","","closed","ozone,","ajayydv","2019-03-09T01:48:08Z","2019-03-11T22:01:04Z"
"","578","HDDS-1240. Fix check style issues caused by HDDS-1196.","","closed","ozone,","bharatviswa504","2019-03-08T17:29:07Z","2019-03-10T05:50:51Z"
"","574","HDDS-1119. DN get OM certificate from SCM CA for block token validation.","","closed","ozone,","ajayydv","2019-03-08T05:48:36Z","2019-03-13T18:38:51Z"
"","567","HDDS-1196. Add a ReplicationStartTimer class.","","closed","ozone,","bharatviswa504","2019-03-06T23:51:21Z","2019-03-08T16:56:53Z"
"","562","HDDS-1225. Provide docker-compose for OM HA.","","closed","ozone,","hanishakoneru","2019-03-06T06:46:55Z","2019-03-07T10:11:21Z"
"","561","HDDS-1043. Enable token based authentication for S3 api.","","closed","ozone,","ajayydv","2019-03-06T02:52:43Z","2019-05-20T16:44:25Z"
"","558","HDDS-1217. Refactor ChillMode rules and chillmode manager.","","closed","ozone,","bharatviswa504","2019-03-05T21:08:58Z","2019-03-25T20:40:13Z"
"","557","HDDS-1175. Serve read requests directly from RocksDB.","","closed","ozone,","hanishakoneru","2019-03-05T19:43:39Z","2019-03-07T03:44:56Z"
"","547","HDDS-594. SCM CA: DN sends CSR and uses certificate issued by SCM.","","closed","ozone,","ajayydv","2019-03-02T01:09:35Z","2019-03-07T22:41:53Z"
"","546","HDDS-594. SCM CA: DN sends CSR and uses certificate issued by SCM.","","closed","","ajayydv","2019-03-02T01:02:32Z","2019-03-02T02:11:04Z"
"","543","HDDS-1211. Test SCMChillMode failing randomly in Jenkins run","","closed","ozone,","bharatviswa504","2019-03-01T19:26:30Z","2019-04-03T22:03:39Z"
"","540","HADOOP-16124. Extend documentation in testing.md about endpoint constants","","closed","","adamantal","2019-03-01T14:39:01Z","2019-07-19T07:14:49Z"
"","534","HDDS-1193. Refactor ContainerChillModeRule and DatanodeChillMode rule.","","closed","ozone,","bharatviswa504","2019-02-28T19:01:10Z","2019-03-05T19:46:37Z"
"","528","HDDS-1182. Pipeline Rule where atleast one datanode is reported in the pipeline.","","closed","ozone,","bharatviswa504","2019-02-28T02:50:21Z","2019-03-01T05:41:41Z"
"","524","HDDS-1187.  Healthy pipeline Chill Mode rule to consider only pipelines with replication factor three.","","closed","ozone,","bharatviswa504","2019-02-27T22:29:21Z","2019-03-01T01:03:49Z"
"","522","YARN-9065","","closed","","hunshenshi","2019-02-27T12:01:00Z","2019-02-27T12:01:12Z"
"","521","YARN-9065","","closed","","hunshenshi","2019-02-27T11:49:36Z","2019-02-27T14:34:42Z"
"","518","HDDS-1178. Healthy pipeline Chill Mode Rule.","","closed","ozone,","bharatviswa504","2019-02-26T01:18:47Z","2019-02-27T16:56:16Z"
"","511","HDDS-1148. After allocating container, we are not adding to container DB.","","closed","ozone,","bharatviswa504","2019-02-21T19:44:13Z","2019-02-23T19:23:34Z"
"","510","HDDS-1141.Update DBCheckpointSnapshot to DBCheckpoint.","","closed","ozone,","bharatviswa504","2019-02-21T19:37:32Z","2019-02-21T21:38:27Z"
"","497","[HDFS-14295] Add CachedThreadPool for DataNode Transfers","","closed","","belugabehr","2019-02-18T21:22:33Z","2019-08-08T04:08:13Z"
"","495","[HDFS-14292] Added ExecutorService to DataXceiverServer","","open","","belugabehr","2019-02-18T15:54:44Z","2019-04-08T23:58:22Z"
"","494","HDDS-1085 : Create an OM API to serve snapshots to Recon server.","","closed","ozone,","avijayanhwx","2019-02-16T06:46:07Z","2019-02-19T19:22:13Z"
"","480","HDFS-14258: Introduce Java Concurrent Package To DataXceiverServer Class","","open","","belugabehr","2019-02-06T19:23:15Z","2019-04-08T23:58:21Z"
"","478","HDDS-987. MultipartUpload: S3API for list parts of a object.","","closed","ozone,","bharatviswa504","2019-01-31T19:25:00Z","2019-02-05T12:34:22Z"
"","477","HDDS-1025: Handle replication of closed containers in DeadNodeHanlder.","","closed","ozone,","bharatviswa504","2019-01-31T07:10:01Z","2019-02-21T22:19:26Z"
"","476","HDDS-1029: Allow option for force in DeleteContainerCommand.","","closed","ozone,","bharatviswa504","2019-01-31T07:07:00Z","2019-02-21T22:19:18Z"
"","470","[HADOOP-16073] Use JDK1.7 StandardCharsets","","open","","belugabehr","2019-01-26T00:56:03Z","2019-04-08T23:58:20Z"
"","466","HDFS-14201. Ability to disallow safemode NN to become active","","closed","","surmountian","2019-01-12T10:07:13Z","2019-07-24T23:10:26Z"
"","463","HDDS-905. Create informative landing page for Ozone S3 gateway","","closed","ozone,","elek","2019-01-10T14:03:25Z","2019-02-15T08:49:36Z"
"","462","HDDS-764. Run S3 smoke tests with replication STANDARD.","","closed","ozone,","elek","2019-01-10T10:53:05Z","2019-01-23T19:37:50Z"
"","461","Update ReconfigurationServlet.java","","closed","","20100507","2019-01-10T07:29:35Z","2019-01-14T06:29:38Z"
"","456","remove the task attempt id from earlier failed map","","open","","lqjack","2019-01-05T02:55:13Z","2019-09-03T04:43:16Z"
"","453","check the buffer size","","open","","lqjack","2018-12-27T14:08:46Z","2019-09-03T06:40:33Z"
"","451","HADOOP-16018: Fix DistCp not reassemble chunks when blocks per chunk > 0","","closed","","kai33","2018-12-20T16:33:27Z","2019-01-08T14:14:43Z"
"","450","HADOOP-16018: Fix DistCp not reassemble chunks when blocks per chunk > 0","","closed","","kai33","2018-12-20T16:20:12Z","2018-12-20T16:32:48Z"
"","448","YARN-9094. Remove unused method: NodeResourceUpdaterPlugin#handleUpdatedResourceFromRM","","closed","","alex-bodo","2018-12-16T18:54:29Z","2019-08-22T07:44:18Z"
"","445","YARN-9095. Removed Unused field from Resource: NUM_MANDATORY_RESOURCES","","closed","","vbmudalige","2018-12-12T06:41:30Z","2018-12-16T20:17:38Z"
"","440","HADOOP-15910 Fix Javadoc for LdapAuthenticationHandler#ENABLE_START_TLS","","closed","","drajakumar","2018-11-25T17:47:30Z","2019-07-30T11:40:35Z"
"","437","YARN-9203. Corrected typos for ResourceManager","","closed","","rahul3","2018-11-09T02:29:12Z","2019-01-17T07:20:11Z"
"","435","Remove extra  in documentation for Single Node Cluster","","closed","","vjsamuel","2018-10-27T17:05:48Z","2020-07-31T21:02:24Z"
"","434","HADOOP-15876: Refactoring AzureBlobFileSystemStore.java","","closed","","dedunumax","2018-10-26T04:26:40Z","2020-04-14T21:45:45Z"
"","430","合并Master修改","","closed","","ZanderXu","2018-10-18T07:46:07Z","2018-10-18T07:46:37Z"
"","429","修改更新","","closed","","ZanderXu","2018-10-18T06:39:55Z","2018-10-18T06:42:23Z"
"","428","Yarn test 1","","closed","","illyaSlobozhanin","2018-10-17T16:07:35Z","2018-10-17T16:14:11Z"
"","425","HADOOP-15818. Fix deprecated maven-surefire-plugin configuration in hadoop-kms module","","closed","","vbmudalige","2018-10-08T18:03:51Z","2018-10-09T03:57:15Z"
"","424","HDFS-13948: provide Regex Based Mount Point In Inode Tree","","closed","","JohnZZGithub","2018-10-05T23:30:51Z","2019-09-03T06:25:00Z"
"","423","YARN-6416. SIGNAL_CMD argument number is wrong","","open","","vbmudalige","2018-10-03T15:16:48Z","2019-04-08T23:58:14Z"
"","421","YARN-8788. mvn package -Pyarn-ui fails on JDK9","","closed","","vbmudalige","2018-10-02T18:20:07Z","2018-10-07T07:50:02Z"
"","420","YARN-8785-branch-3.1.002.patch","","open","","simonprewo","2018-10-01T21:05:36Z","2020-07-31T20:12:47Z"
"","419","YARN-8819. Fix findbugs warnings in YarnServiceUtils","","closed","","vbmudalige","2018-09-27T16:15:04Z","2018-10-07T16:05:07Z"
"","418","Revert 1 trunk","","closed","","yunfeil","2018-09-26T10:00:40Z","2018-09-26T10:01:31Z"
"","415","Merge pull request #1 from apache/trunk","","closed","","ayushtkn","2018-09-10T17:29:50Z","2018-09-10T17:30:10Z"
"","411","YARN-8747: update moment-timezone version to 0.5.1","","closed","","collinmazb","2018-09-05T12:07:56Z","2018-09-06T11:53:34Z"
"","406","HDFS-13775. Add a conf property to retry dns reverse lookups during datanode registration.","","open","","aniket486","2018-07-31T22:54:13Z","2019-09-03T06:34:05Z"
"","395","MAPREDUCE-7063: Update Logging to DEBUG Level","","closed","","vbmudalige","2018-06-14T15:58:14Z","2018-06-19T08:43:40Z"
"","372","add a pic","","closed","","Cocolar","2018-04-29T09:06:50Z","2019-09-11T02:20:34Z"
"","364","YARN-7668. Remove unused variables from ContainerLocalizer","","closed","","dedunumax","2018-04-23T09:57:50Z","2018-06-19T08:43:08Z"
"","360","HADOOP-14175 : NPE when ADL store URI contains underscore","","open","","keuler1","2018-04-01T01:39:39Z","2019-09-03T05:26:20Z"
"","359","Branch 3.1","","closed","","heheeeee","2018-03-29T08:36:37Z","2019-01-17T07:45:35Z"
"","354","revert-Hadoop-13707-Branch 2","","closed","","brahmareddybattula","2018-03-16T17:46:32Z","2019-02-22T14:28:46Z"
"","353","Branch 3.1","","closed","","thotamanohar","2018-03-13T13:11:37Z","2018-03-19T18:07:58Z"
"","350","Fix FileSystem.listStatus javadoc","","closed","","keith-turner","2018-03-09T21:08:06Z","2019-09-04T18:18:26Z"
"","348","add contributor","","closed","","chuqiao-wang","2018-03-04T01:43:29Z","2018-03-04T01:45:53Z"
"","346","YARN-7969 Fix confusing log messages that do not match with the method name","","open","","ginolzh","2018-02-25T22:42:50Z","2019-04-08T23:58:01Z"
"","345","YARN-7926 fix a copy-and-paste error in log messages","","closed","","ginolzh","2018-02-25T20:24:33Z","2019-08-27T14:54:31Z"
"","344","HDFS-13056. Add support for a new COMPOSITE_CRC FileChecksum which is comparable between different block layouts and between striped/replicated files","","closed","","dennishuo","2018-02-21T01:18:05Z","2019-08-08T06:03:07Z"
"","343","Add missing description of Namenode format options","","open","","yuriymalygin","2018-02-20T13:56:45Z","2019-04-08T23:58:00Z"
"","339","HADOOP-13972 Supporting per-account configuration for ADL","","closed","","ssonker","2018-02-13T10:57:55Z","2018-02-23T06:54:45Z"
"","337","Branch 3.0.1","","closed","","ycclyj","2018-02-07T05:06:25Z","2021-02-10T23:56:33Z"
"","336","This is a test","","closed","","qinchen123","2018-02-01T07:27:13Z","2019-07-26T13:08:13Z"
"","335","Branch 2.9","","closed","","Narzila92","2018-01-31T19:37:54Z","2019-01-18T01:08:34Z"
"","334","1","","closed","","Narzila92","2018-01-31T17:34:31Z","2019-07-26T13:06:06Z"
"","333","YARN-7823. add configuration to use ip as NM address rather than hostname","","closed","","xulongzhe","2018-01-26T12:09:45Z","2019-11-13T14:24:47Z"
"","329","YARN-7760. precompute master node URL","","open","","skmvasu","2018-01-18T06:35:14Z","2019-04-08T23:57:57Z"
"","327","YARN-7749. Fix GPU sidebar","","open","","skmvasu","2018-01-15T07:00:46Z","2019-04-08T23:57:57Z"
"","326","YARN-7747 use injected GuiceFilter instances","","open","","gerashegalov","2018-01-14T03:18:26Z","2019-04-08T23:57:56Z"
"","319","HADOOP-15142. Register FTP and SFTP as FS services","","open","","mmolimar","2017-12-23T21:15:17Z","2019-04-08T23:57:55Z"
"","307","Merge 5150","","closed","","skmvasu","2017-12-01T04:45:37Z","2017-12-01T04:50:34Z"
"","306","YARN-7092.  Log viewer in application page in yarn-ui-v2","","closed","","skmvasu","2017-11-29T10:25:55Z","2017-12-13T10:29:15Z"
"","304","[YARN-7578] waitForDiskHealthCheck sleep time is extended from 1000ms…","","open","","vetriselvan1187","2017-11-29T04:31:35Z","2019-04-08T23:57:53Z"
"","302","App page ia","","closed","","skmvasu","2017-11-28T09:04:38Z","2017-11-28T09:11:24Z"
"","301","Config page","","closed","","skmvasu","2017-11-28T06:17:44Z","2017-11-28T10:06:36Z"
"","297","Fix Checkstyle error, rename a argument","","open","","maobaolong","2017-11-22T02:57:26Z","2022-01-31T22:28:45Z"
"","296","Fix constants variable name","","open","","maobaolong","2017-11-22T02:50:59Z","2019-09-03T06:03:03Z"
"","295","Fix checkstyle problem","","open","","maobaolong","2017-11-22T02:39:21Z","2019-08-27T16:08:27Z"
"","287","HDFS-10323. transient deleteOnExit failure in ViewFileSystem due to close() ordering","","open","","wenxinhe","2017-11-01T09:29:55Z","2019-04-08T23:57:50Z"
"","286","Update and rename README.txt to README.md","","open","","giraugh","2017-10-30T22:52:37Z","2019-04-08T23:57:50Z"
"","284","HADOOP-14157 fix parseUrl to replace '\' for '/'","","open","","bberton-ciandt","2017-10-28T21:08:44Z","2019-08-27T18:15:06Z"
"","281","Automatic decompression of HDFS files tagged with extended attribute using Snappy","","open","","sutirupa","2017-10-17T18:07:24Z","2019-07-26T12:16:49Z"
"","279","HADOOP-14908 allow regular expressions, fixes","","closed","","johannes-altiscale","2017-09-27T18:16:59Z","2017-10-04T20:30:24Z"
"","278","(HADOOP-14908) allow for real regex patterns (and be backward compatible)","","closed","","johannes-altiscale","2017-09-27T01:56:47Z","2017-10-04T20:30:27Z"
"","273","Qinchen/gpu locality","","closed","","qinchen123","2017-09-15T01:01:29Z","2017-09-18T10:02:33Z"
"","269","HADOOP-14217. Support colon in Hadoop Path","","open","","yufeldman","2017-09-05T20:09:42Z","2019-08-27T18:32:10Z"
"","268","test pull request ,please ignore","","closed","","qinchen123","2017-08-31T06:08:50Z","2017-08-31T06:12:31Z"
"","267","Branch 2.7.4","","closed","","wangzhen11aaa","2017-08-19T01:50:37Z","2019-09-11T02:06:00Z"
"","266","HDFS-12315: Use Path instead of String to check closedFiles set","","closed","","dosoft","2017-08-17T05:35:06Z","2019-06-18T06:06:02Z"
"","265","HDFS-12314: Fixed typo in the testAddOneNewVolume()","","closed","","dosoft","2017-08-17T05:13:44Z","2019-06-18T06:11:26Z"
"","264","HDFS-12309. Fixed incorrect checkNotNull()","","closed","","dosoft","2017-08-16T21:06:40Z","2019-07-26T13:02:43Z"
"","263","MAPREDUCE-6940: Pass allSplits parameter","","closed","","dosoft","2017-08-16T13:13:39Z","2019-07-26T13:00:22Z"
"","262","YARN-7023: Fixed compareTo()","","closed","","dosoft","2017-08-16T12:22:23Z","2017-09-01T19:35:10Z"
"","258","HADOOP-14706. Adding a helper method to determine whether a log is Log4j implement.","","closed","","wenxinhe","2017-08-02T07:35:44Z","2017-08-04T05:53:10Z"
"","257","HADOOP-14706. Adding a helper method to determine whether a log is Log4j implement.","","closed","","wenxinhe","2017-08-01T10:39:33Z","2017-08-04T05:37:48Z"
"","256","Hdfs 7240","","closed","","sreenilutukurthy","2017-08-01T10:35:46Z","2017-08-01T10:41:31Z"
"","251","HADOOP-14539. Move commons logging APIs over to slf4j in hadoop-common.","","closed","","wenxinhe","2017-07-19T05:56:39Z","2017-07-24T05:19:05Z"
"","250","HADOOP-14661. Added support for AWS S3 Requester Pays Buckets","","closed","","mandusm","2017-07-14T03:18:21Z","2022-03-27T20:45:29Z"
"","249","HDFS-12125. Document the missing -removePolicy command of ec.","","closed","","wenxinhe","2017-07-13T07:28:15Z","2019-08-09T20:49:06Z"
"","248","Branch 2.6.0","","closed","","jirimutu","2017-07-12T06:22:33Z","2019-07-26T12:23:49Z"
"","247","HADOOP-14638. Replace commons-logging APIs with slf4j in StreamPumper.","","closed","","wenxinhe","2017-07-10T05:26:11Z","2017-07-11T04:31:30Z"
"","246","HADOOP-14539. Move commons logging APIs over to slf4j in hadoop-common.","","closed","","wenxinhe","2017-07-07T08:59:32Z","2017-07-19T05:53:57Z"
"","245","HADOOP-14624. Add GenericTestUtils.DelayAnswer that accept slf4j logger API","","closed","","wenxinhe","2017-07-05T09:48:48Z","2019-07-26T12:28:01Z"
"","244","HADOOP-14571. Deprecate public APIs relate to log4j1","","closed","","wenxinhe","2017-06-27T03:49:44Z","2017-07-05T03:26:29Z"
"","243","Add-Post -scheduler","","closed","","mbelenfa","2017-06-22T23:11:15Z","2017-06-22T23:12:49Z"
"","237","HDFS-11978. Remove invalid '-usage' command of 'ec' and add missing commands 'addPolicies' 'listCodecs'","","closed","","wenxinhe","2017-06-16T05:31:55Z","2017-07-05T03:34:46Z"
"","234","Yarn-Post /scheduler","","closed","","mbelenfa","2017-06-15T05:09:37Z","2017-06-15T05:09:58Z"
"","233","HDFS-11647","","closed","","wayblink","2017-06-15T05:04:35Z","2017-06-15T05:04:56Z"
"","231","HADOOP-14508. TestDFSIO throws NPE when set -sequential argument.","","open","","wenxinhe","2017-06-09T01:56:38Z","2019-09-03T08:13:06Z"
"","230","HADOOP-14208. Fix typo in the top page in branch-2.8","","closed","","wenxinhe","2017-06-07T15:06:10Z","2017-06-19T11:34:29Z"
"","229","HADOOP-14208. Fix typo in the top page in branch-2.8","","closed","","wenxinhe","2017-06-07T06:57:13Z","2017-06-19T11:33:42Z"
"","228","HADOOP-14208. Fix typo in the top page in branch-2.8","","closed","","wenxinhe","2017-06-07T06:43:13Z","2017-06-07T06:47:41Z"
"","226","HDFS-11924 : Pass FsAction to the external AccessControlEnforcer","","closed","","gzsombor","2017-06-04T12:03:19Z","2019-03-29T15:29:29Z"
"","225","HDFS-11924 : Pass FsAction to the external AccessControlEnforcer","","closed","","gzsombor","2017-06-04T11:44:42Z","2019-03-29T15:31:03Z"
"","222","YARN-6583 Hadoop-sls failed to start because of premature state of RM","","open","","scutojr","2017-05-11T03:08:01Z","2022-01-31T19:06:25Z"
"","221","Hadoop-sls failed to start because of premature state of RM","","closed","","scutojr","2017-05-10T14:29:20Z","2017-05-11T03:05:05Z"
"","214","HADOOP-14315: Python example in the rack awareness document doesn't work due to bad indentation","","closed","","sekikn","2017-04-18T03:34:07Z","2017-04-18T08:15:47Z"
"","213","YARN-6457  use existing conf object as sslConf object in WebApps for the builder to use for the HttpServer2","","closed","","sanjaypujare","2017-04-14T05:44:23Z","2019-07-26T11:34:47Z"
"","210","HADOOP-14284. Shade Guava everywhere.","","closed","","oza","2017-04-11T17:03:44Z","2019-07-26T11:35:47Z"
"","209","Test","","closed","","huafengw","2017-03-30T05:53:37Z","2017-03-30T05:53:56Z"
"","199","HDFS-11499 Decommissioning stuck because of failing recovery","","closed","","lukmajercak","2017-03-05T20:12:01Z","2017-03-08T04:33:17Z"
"","197","Hadoop 6685","","closed","","pdkluitel","2017-02-23T07:27:50Z","2020-07-29T16:18:25Z"
"","194","YARN-6175. Negative vcore for resource needed to preempt.","","closed","","flyrain","2017-02-14T20:23:25Z","2017-04-04T16:43:55Z"
"","193","YARN-6042. Fairscheduler: Dump scheduler state in log.","","closed","","flyrain","2017-02-14T19:35:07Z","2017-04-04T16:44:23Z"
"","187","HADOOP-14156: fix grammar","","closed","","adyatlov","2017-02-06T18:09:04Z","2017-03-13T07:22:30Z"
"","182","YARN-6061. Add a customized uncaughtexceptionhandler for critical threads","","closed","","flyrain","2017-01-20T00:02:59Z","2017-04-04T16:44:53Z"
"","181","YARN-4212. FairScheduler: Parent queues is not allowed to be 'Fair' policy if its children have the ""drf"" policy.","","closed","","flyrain","2017-01-19T20:20:08Z","2017-04-04T16:45:17Z"
"","180","YARN-5830. Avoid preempting AM containers.","","closed","","flyrain","2017-01-18T18:05:40Z","2017-04-04T16:45:06Z"
"","176","lastest changes from base","","closed","","yussufsh","2016-12-29T03:50:30Z","2016-12-29T03:55:31Z"
"","174","1","","closed","","wwjiang007","2016-12-28T08:05:05Z","2019-07-26T11:20:35Z"
"","156","take one of publishing","","closed","","sjrand","2016-11-09T15:42:48Z","2016-11-09T15:43:03Z"
"","151","Ozone:SCM: Add support for registerNode in datanode","","closed","","anuengineer","2016-11-03T22:59:56Z","2020-05-06T18:17:46Z"
"","148","HDFS-11060. Make DEFAULT_MAX_CORRUPT_FILEBLOCKS_RETURNED configurable","","closed","","LantaoJin","2016-10-28T09:51:04Z","2019-01-24T10:31:13Z"
"","144","HADOOP-13680. fs.s3a.readahead.range to use getLongBytes","","closed","","abmodi","2016-10-23T14:33:30Z","2016-11-01T11:28:52Z"
"","143","YARN-4597: initial commit","","closed","","xslogic","2016-10-18T01:51:36Z","2020-07-29T16:13:10Z"
"","133","HADOOP-13681. Reduce Kafka dependencies in hadoop-kafka module","","closed","","granthenke","2016-10-04T15:38:43Z","2019-06-26T01:03:31Z"
"","128","Branch 3.0.0 alpha1","","closed","","tarunvictor","2016-09-22T06:50:49Z","2016-09-22T06:51:04Z"
"","127","Branch 2.7.1","","closed","","rajeshdb","2016-09-18T01:04:46Z","2019-01-23T07:08:51Z"
"","126","HADOOP-13341: Deprecate HADOOP_SERVERNAME_OPTS; replace with (command)_(subcommand)_OPTS","","closed","","aw-was-here","2016-09-08T14:58:37Z","2016-09-12T18:18:42Z"
"","120","HADOOP-13532. Fix typo in hadoop_connect_to_hosts error message","","closed","","chu11","2016-08-22T17:17:35Z","2016-08-25T14:05:35Z"
"","119","HADOOP-13434. Add bash quoting to Shell class.","","closed","","omalley","2016-07-28T18:35:01Z","2016-12-16T19:05:45Z"
"","118","HADOOP-13434. Add bash quoting to Shell class.","","closed","","omalley","2016-07-28T18:30:27Z","2016-07-28T18:31:09Z"
"","117","Branch 2.7","","closed","","surekav","2016-07-20T10:24:59Z","2019-01-21T03:20:37Z"
"","115","Pull all changes.","","closed","","ralic","2016-07-17T08:31:47Z","2018-03-14T02:36:27Z"
"","113","[HADOOP-13075] Adding support for SSE-KMS and SSE-C","","closed","","fedecz","2016-07-11T02:34:51Z","2019-01-23T07:06:21Z"
"","109","Feature/hadoop 13320","","closed","","pippobaudos","2016-07-01T16:58:53Z","2016-07-01T16:59:45Z"
"","108","HADOOP-13320. Fix arguments check in the WordCount v2.0 in Documentation","","closed","","pippobaudos","2016-07-01T16:38:53Z","2016-07-07T22:31:24Z"
"","107","First pass at getting blocks from hdfs to ship using inotify","","closed","","ghost","2016-06-29T19:38:22Z","2016-06-29T19:38:42Z"
"","97","HADOOP-13213 - Fix documentation for hadoop-auth client.","","closed","","tellisnz","2016-05-29T13:59:52Z","2016-06-10T18:35:06Z"
"","80","void","","closed","","xystevensun","2016-02-26T18:06:54Z","2016-02-26T21:12:27Z"
"","78","HDFS-9839. Reduce verbosity of processReport logging","","closed","","arp7","2016-02-20T18:49:59Z","2016-02-21T07:23:51Z"
"","76","HADOOP-9613. [JDK8] Update jersey version to latest 1.x release.","","closed","","oza","2016-02-15T16:10:20Z","2016-06-23T12:57:39Z"
"","75","Branch 2.8","","closed","","jkotireddy","2016-02-12T20:33:30Z","2016-02-12T20:34:00Z"
"","71","fix  NullPointerException in Balancer","","closed","","rainforc","2016-02-01T08:30:14Z","2021-10-18T12:41:50Z"
"","70","Branch 2.8","","closed","","hadooping","2016-01-29T12:46:41Z","2016-02-01T23:17:42Z"
"","67","YARN-4567 javadoc failing on java 8","","closed","","steveloughran","2016-01-08T19:43:01Z","2016-01-12T16:40:32Z"
"","66","YARN-2571 RM to support YARN registry","","closed","","steveloughran","2016-01-08T18:29:04Z","2021-10-15T19:46:36Z"
"","57","[YARN-4387] Fix FairScheduler log message","","closed","","vesense","2015-11-24T08:11:53Z","2015-11-24T08:23:43Z"
"","53","HDFS-9263","","closed","","steveloughran","2015-11-22T18:37:44Z","2016-11-01T10:22:58Z"
"","45","Branch 2.7.1","","closed","","hellodengfei","2015-11-12T11:29:16Z","2016-05-25T07:41:37Z"
"","44","The `hadoop fs -ls` command and `hadoop fs -stat` command will show different time if the default timezone isn't UTC","","closed","","yxkemiya","2015-11-11T09:48:07Z","2015-11-12T06:41:14Z"
"","39","HADOOP-12527. Upgrade Avro dependency to 1.7.7.","","closed","","ejono","2015-10-29T01:15:24Z","2019-01-23T07:30:17Z"
"","38","Apache/trunk","","closed","","CaiYicong","2015-10-20T08:19:53Z","2015-10-20T10:25:53Z"
"","36","Later hadoop use flat buffer","","open","","hash-X","2015-10-10T03:29:01Z","2019-04-08T23:57:27Z"
"","28","H12257","","closed","","aw-was-here","2015-08-15T02:23:28Z","2015-08-15T02:24:14Z"
"","25","Branch 2.6.0","","closed","","omkarksa","2015-08-06T13:19:42Z","2015-12-08T09:39:09Z"
"","23","New file input stream","","closed","","hash-X","2015-07-16T14:14:15Z","2015-07-16T14:25:47Z"
"","22","Hdfs ec merge","","closed","","vinayrpet","2015-06-22T09:33:54Z","2015-09-30T15:42:12Z"
"","15","[YARN-3444] Fixed typo (capability)","","closed","","gliptak","2015-03-10T00:53:08Z","2015-04-24T17:17:46Z"
"","13","Merging from apache trunk","","closed","","vinayrpet","2014-12-22T09:37:54Z","2019-07-26T11:15:36Z"
"","12","Hdfs ec","","closed","","divyam","2014-12-03T07:12:33Z","2015-02-10T00:48:41Z"
"","5","HADOOP-11049","","closed","","sjlee","2014-09-26T19:09:37Z","2014-11-05T06:22:51Z"